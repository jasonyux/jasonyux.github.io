<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="/lectures/feed.xml" rel="self" type="application/atom+xml" /><link href="/lectures/" rel="alternate" type="text/html" /><updated>2023-01-13T02:21:48-05:00</updated><id>/lectures/feed.xml</id><title type="html">Lecture Notes</title><subtitle>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </subtitle><entry><title type="html">APPH4010 Intro to Nuclear</title><link href="/lectures/2022@columbia/APPH4010_Intro_to_Nuclear.html/" rel="alternate" type="text/html" title="APPH4010 Intro to Nuclear" /><published>2022-12-20T00:00:00-05:00</published><updated>2022-12-20T00:00:00-05:00</updated><id>/lectures/2022@columbia/APPH4010_Intro_to_Nuclear</id><content type="html" xml:base="/lectures/2022@columbia/APPH4010_Intro_to_Nuclear.html/"><![CDATA[Equations and Concepts for Intro to Nuclear

# 1. Basic Concepts

|  Name of Concept/Equation   | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :-------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
|          Isotopes           | same atomic number but different neutrons                    |                                                              |                                                              |
|        Electron Volt        | energy equal to the amount gained to accelerate rest electron through a potential difference of 1 volt |                                                              |                                                              |
|   Atomic Mass Unit/Dalton   | 1/12 of the mass of a neutral atom of $^{12}_6C$             |                                                              |                                                              |
|       Magnetic Moment       | Magnetic Dipole Moment associated with spin of nucleus       |                                                              |                                                              |
|     Spin Quantum Number     | nucleons have spin of 1/2 in units of $h / (2\pi)$           |                                                              |                                                              |
|       Mass to Energy        | $E^2 = p^2c^2 + m_0^2 c^2$                                   | $m_0$ is rest mass, and if at rest, $p=0$                    | $E_{\mathrm{electron}}=511keV$                               |
|                             |                                                              |                                                              | $E_{\mathrm{amu}}=931.5keV$                                  |
|       Binding Energy        | Energy required to separate its constituent nucleus          | using $E=mc^2$                                               |                                                              |
|                             | $B = [Z\cdot m_H + N\cdot m_N - m(A,Z)]c^2$                  | energy used for binding, so the actual atomic mass $m(A,Z)$ is smaller | consider for carbon $^{12}_6C$ we have $m(A,Z)=12$, but $m_H=1.007825$ and $m_N=1.008665$ |
|                             |                                                              | $m_H$ is mass of hydrogen so that it includes weight of electron |                                                              |
|    Binding Energy Curve     | ![image-20221013004334846](nuclear_eq_n_concepts/image-20221013004334846.png) | $B/A$ plotted because binding energy increases just as there are more proton to hold |                                                              |
|                             |                                                              | the more $A$, much much more $B$ is needed due to short range of internuclear force |                                                              |
|                             |                                                              | but the above only works until a point where there is only a fixed number of neutrons affecting a proton |                                                              |
|                             |                                                              | since higher $B$ also means more energy required to break = more energy released when making bonds, we want fission to go from right |                                                              |
|   Nuclide Stability Curve   | ![image-20221013005606999](nuclear_eq_n_concepts/image-20221013005606999.png) | more neutrons is needed for more protons, hence this stability curve in the middle |                                                              |
|                             |                                                              | so above it is more proton then needed = proton rich         |                                                              |
| Properties of Radioactivity | Decay is a random process, but the probability of occurring can be modelled, and macro quantity such as $t_{1/2}$ can be computed |                                                              |                                                              |
|                             | usually an unstable nuclide "parent" transforms into a more stable nuclide "daughter" |                                                              |                                                              |
|                             | radioactivity measured in Becquerel = 1 decay per second     |                                                              |                                                              |
|                             | often heavy nuclei gives $\alpha$, and light nuclei $\beta$  |                                                              |                                                              |
|  Alpha Emission Properties  | $(A,Z) \to (A-4, Z-2)$                                       | preferred by heavy nuclides                                  |                                                              |
|                             | $\alpha$-particle is preferred because it is **very** stable, hence net effect of decay releases energy |                                                              | (see below)                                                  |
|    Decay Energy Released    | $Q = \Delta m c^2 =  (m_{\mathrm{left}}-m_{\mathrm{right}})c^2$ | if net mass loss = net energy released = RHS more stable = RHS less actual mass | U(238)$\to$ Th(234) + He(4)                                  |
|                             | $Q_\alpha = \mathrm{KE}_{\mathrm{daughter}}+ \mathrm{KE}_{\mathrm{\alpha}}$ | for alpha decay                                              |                                                              |
|                             | $Q_\alpha = E_\alpha[1+ \frac{m_\alpha}{m_D}]$               | conservation of momentum                                     |                                                              |
|                             |                                                              | since $Q_\alpha,m_\alpha/m_D$ is known, this means KE energy spectrum for $\alpha$ will be discrete in this case |                                                              |
|    Beta Decay Properties    | for proton-rich nuclides, often see $\beta^+$ decay or electron capture to convert $P \to N$ | $p \to n + e^+ + \nu$                                        |                                                              |
|                             |                                                              | $p+e^- \to n + \nu$                                          |                                                              |
|                             | otherwise, light nuclides often have $\beta^-$, which is $N\to P$ | $n \to p + e^- + \bar{\nu}$                                  |                                                              |
|                             | ![image-20221013014759439](nuclear_eq_n_concepts/image-20221013014759439.png) | KE Spectrum of electron becomes continuous because you have three particles released |                                                              |
|    Gamma Decay Property     | occurs when excited nucleus lose energy $\Delta E$ as photons |                                                              |                                                              |
|  Rate of Radioactive Decay  | $\frac{dN}{dt} = -\lambda N$                                 | Given that macroscopically the rate is proportional to number of radioactive nuclei |                                                              |
|                             | $N(t)=N_0e^{-\lambda t}$                                     | $\lambda$ is probability per unit time that a nuclide decays |                                                              |
|                             | $A=\lambda N$                                                | since $\lambda$ is prob/time, activity is rate of decay of a sample, measured in Becquerel |                                                              |
|     Relativistic Effect     | $\Delta T' = \gamma \Delta T$                                | high energy particle emitted so that $v \approx c$, then the actual half life becomes $\Delta T'$ | $v=0.995c$ when $\nu$ is released with large energy          |
|                             | $\gamma = \frac{1}{\sqrt{1-v^2/c^2}}$                        | is Lorentz fraction                                          |                                                              |

# 5. Interaction of Radiation and Matter

Nuclear radiation normally consists of ==any particle with energy== or photons (particle radiation is the **radiation of energy by means of fast-moving subatomic particles**). Its **interactions with matter** gives us opportunity for all experimental work. Therefore, this chapter we consider:

- how charged particle interact with matter
- how uncharged particle interact with matter
- how photons interact with matter

|            Name of Concept/Equation            | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :--------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
|           Num Atoms and Atom Density           | $N = \frac{m}{M}N_A$                                         | $m$ is mass of the substance you care, $M$ is the atomic weight of it |                                                              |
|                                                | $N/V = \frac{m}{V\cdot M}N_A = \frac{\rho}{M}N_A$            | $\rho$ is the density of the substance you are               | $H_2O$ has $\rho \approx 1g/\mathrm{cm}^3$, and $M=18.0153$  |
| Heavy Charged Particles Interaction Properties | Two Ways of Interaction:<br />1) its electric field can ionize atoms in its passage<br />2) collision (e.g. with electrons) |                                                              |                                                              |
|                                                | Deflect very little from path because its heavy              |                                                              |                                                              |
|                                                | Well-defined range, depending on its energy, mass, charge, and the stopping medium |                                                              |                                                              |
|                                                | $\Delta E \approx E_\alpha(\frac{4m_e}{M_\alpha})$           | max energy lost per collision                                | incident alpha particle                                      |
|                                                | Though short range, as it can produce ionization = excited nuclei produces x-rays = need thicker $>R$ protection layer |                                                              |                                                              |
|                 Stopping Power                 | Rate at which a particle loses energy per unit path length   |                                                              |                                                              |
|                                                | ![image-20221013235440171](nuclear_eq_n_concepts/image-20221013235440171.png) | Bethe-Bloch Formula                                          |                                                              |
|      Energy Dependence of Stopping Power       | <img src="nuclear_eq_n_concepts/image-20221013235351944.png" alt="image-20221013235351944" style="zoom:50%;" /> | variation of stopping power with energy if incident proton. This general shape works for any charged ion |                                                              |
|                                                | <img src="nuclear_eq_n_concepts/image-20221025164843434.png" alt="image-20221025164843434" style="zoom: 33%;" /> |                                                              |                                                              |
|                                                | $\frac{dE}{dx} \approx \frac{\mathrm{const}}{E^k},k\approx 0.8$ | the faster the ion travels, the less chance it interacts with stopping medium, hence negative slope |                                                              |
|                                                | $R=\int_{E}^0 dE/(dE/dx) \propto E^{k+1} $                   | derived from above                                           |                                                              |
|    Projectile Dependence of Stopping Power     | $dE/dx \propto z^2 f(v)$                                     | depends on particle charge                                   |                                                              |
|                                                | $R \propto (m/z^2) F(v)$                                     | range depends on incident particle mass and charge, derived from above | if two charged particle has the same energy, then $R_1/R_2 = (m_1/m_2)(z_2/z_1)^2$ because velocity is the same |
|  Stopping Medium Dependence of Stopping Power  | $\frac{R_1}{R_2} \approx \frac{\rho_2 \sqrt{A_1}}{\rho_1 \sqrt{A_2}}$ | depends on stopping medium's density and mass number         |                                                              |
|                  Bragg Curve                   | ![image-20221014000422271](nuclear_eq_n_concepts/image-20221014000422271.png) | utilize the fact that stopping power increases when energy decreases |                                                              |
|                                                |                                                              | particle travels slower = can produce more ions per unit path |                                                              |
|                                                |                                                              | hence Brag Peak = peak stopping power = peak ionization energy | hit tumor at this distance                                   |
|                                                |                                                              | when all energy is lost, it does nothing and stops/halts     |                                                              |
|     Electrons/$\beta$-particle Properties      | Two Ways of Interaction:<br />1) its electric field can ionize atoms in its passage<br />2) collision (e.g. with electrons) |                                                              |                                                              |
|                                                | a zig-zag path                                               |                                                              |                                                              |
|                                                | a much faster speed = less stopping force                    |                                                              |                                                              |
|                                                | less well-defined range, hence uses mean-free-path           | mean free path is the average distance over which a moving particle travels before substantially changing its direction or energy |                                                              |
|                                                | you can also use max-range $R_{\max}$ as a measure, in which case$\rho  R_{\max} \propto E_{\max}$ |                                                              |                                                              |
|      Stopping Power v.s. Electron Energy       | ![image-20221014142135607](nuclear_eq_n_concepts/image-20221014142135607.png) | as electrons accelerate by rapidly changing directions, it emits a lot of Bremsstrahlung radiation meaning $-\frac{dE}{dx}_{\mathrm{rad}}$ contributes a lot to stopping power |                                                              |
|            Reaction Cross Sections             | measures area within which if you hit the particle near the target $T$, reaction will occur | can be used for both Neutron and Photon                      |                                                              |
|                                                | hence measures how strongly target $T$ reaction will occur   |                                                              |                                                              |
|           Microscopic Cross Section            | $\sigma = \sigma(E)$                                         | $E$ is energy of incident particle                           |                                                              |
|                                                |                                                              | unit is uslaly $\mathrm{cm}^2$ or in Barnes ($10^{-24}\mathrm{cm}^2$) |                                                              |
|                                                | $\sigma_{\mathrm{total}} = \sigma_{\mathrm{abs}}+\sigma_{\mathrm{sca}}+\sigma_{\mathrm{n,2n}}+...$ |                                                              |                                                              |
|                 Reaction Rate                  | $R=N \cdot \sigma \phi = \Sigma \phi$                        | $N$ is atomic density, $\phi$ is particle flux (particle per cm$^2$ per second) |                                                              |
|                                                |                                                              | $\sigma \phi$ measures number of reactions triggered per second |                                                              |
|           Macroscopic Cross Section            | $\Sigma \equiv N \sigma$                                     | is the macroscopic cross section                             |                                                              |
|        Gamma Ray Interaction Properties        | Primary ways of interaction:<br />1) photo-electric effect<br />2) Campton scattering<br />3) pair production | because gamma rays are very energetic and they are photons   |                                                              |
|                                                | we consider attenuation coefficient $\mu_m$ as a measure of **probability the photon interact with material = photon vanished** |                                                              |                                                              |
|              Photoelectric Effect              | photon absorbed by electrons (near nucleus for conservation of momentum) to be ejected | hence that photon disappeared = attenuated                   |                                                              |
|                                                | $T=E_\gamma - B_e = hv - B_e$                                | $T$ is the emitted photoelectron and $B_e$ is the binding energy of the electron |                                                              |
|                                                | secondary reactions might occur if<br />1) rest of the electrons rearrange themselves for de-excitation and release $\gamma$-ray<br />1) eject low energy electrons for de-excitation and hence **Auger** electrons  to deal with the excess energy | The Auger effect is a physical phenomenon in which the filling of an inner-shell vacancy of an atom, by an electron, is accompanied by the *emission of another electron* from the same atom *instead of releasing energy*. |                                                              |
|                                                | $\sigma_{PE} \propto z^5 / E_\gamma^{3.5}$                   | free electron = low probability of reaction                  |                                                              |
|                                                |                                                              | higher electron binding energy = more tightly bound electron has a higher chance (also for **conservation of momentum**) |                                                              |
|               Campton Scattering               | ![image-20221014150725380](nuclear_eq_n_concepts/image-20221014150725380.png) | $\gamma$ ray scattered off electron and hence we get an electron recoiling and a lower energy photon |                                                              |
|                                                | $T = E_\gamma - E_{\gamma '} = E_{\text{KE of elec}}$        |                                                              |                                                              |
|                                                | $E_{\gamma '} = E_{\text{KE of elec}} = E_e - m_ec^2$        | for $E_e$ is the total energy of the recoil electron         |                                                              |
|                                                | $E_{\gamma '} = \frac{E_\gamma}{1+(E_\gamma/mc^2)(1-\cos \theta)}$ | energy of scattered photon from conservation of energy and momentum |                                                              |
|                                                |                                                              | a problem for shielding in real life as $\gamma$ photon didn't disappear |                                                              |
|                Pair Production                 | creates an electron-positron pair (when heavy nucleus is near to conserve momentum) |                                                              |                                                              |
|                                                | $E_\gamma = 2m_ec^2 + T_- + T_+$                             | $2m_ec^2$ is the rest mass energy of positron and electron   |                                                              |
|                                                | has secondary effect when positron recombine with electron $\to$ annihilate and produce two oppositely traveling photon $\to$ can do PE or scattering |                                                              |                                                              |
|               Photon Attenuation               | from the three mechanism above, photon either disappear or scattered = not observed by detector = attenuated |                                                              |                                                              |
|                                                | ![image-20221014151515070](nuclear_eq_n_concepts/image-20221014151515070.png) | can measure and find out attenuation dependence on the three mechanism |                                                              |
|                                                | ![image-20221014151526416](nuclear_eq_n_concepts/image-20221014151526416.png) | difficult to measure                                         |                                                              |
|         Collimated Photon Attenuation          | $dI = - N\sigma I dx$                                        | $x$ is thickness of the material, $N$ is material atomic density, and $\sigma$ is interaction cross section |                                                              |
|                                                | $\mu \equiv N\sigma $                                        | linear attenuation coefficient                               |                                                              |
|                                                |                                                              | represents the probability per unit path length of a photon undergoing an interaction that would remove it from the beam |                                                              |
|                                                | $I = I_0 e^{-N\sigma x} = I_0 e^{-\mu x}$                    | intensity observed is $I$                                    |                                                              |
|                                                | $\mu_m = \mu / \rho$                                         | mass attenuation coefficient                                 |                                                              |
|      Photon Attenuation Coefficient Graph      | ![image-20221014152131327](nuclear_eq_n_concepts/image-20221014152131327.png) | again, there are three phenomenon contributing to $\sigma$ hence $\mu_m$ |                                                              |
|              Neutron Interactions              | A variety of interactions but mostly nuclear reactions:<br />1) fission if neutrons at few MeV<br />2) scattering<br />3) slowed neutrons can give neutron absorption |                                                              |                                                              |
|               Fission Fragments                | When $N$ and $_{238}U$ react, it will create two **fission fragment** and 2-3 neutrons | hence neutron attenuated                                     |                                                              |
|                                                | those neutrons can then start chain reactions                |                                                              |                                                              |
|                                                | requires low neutron energy                                  |                                                              |                                                              |
|               Neutron Moderation               | if elastic collision, can calculate energy $E$ of neutron with initial energy $E_0$ **after colliding** with some target nucleus at rest |                                                              |                                                              |
|                                                | $n = (1/\varepsilon) \ln(E_0 / E_n)$                         | $n$ is the number of collisions of neutrons                  |                                                              |
|                                                |                                                              | $E_n$ is the energy of neutron after $n$ collisions          |                                                              |
|                                                | $\varepsilon = (2/A) - (4/(3A^2))$                           |                                                              |                                                              |
|                                                | if reached low neutron energy = thermal neutron, can do fission | hence attenuated (see above)                                 |                                                              |
|            Attenuation in Neutrons             | $\sigma_T \approx \sigma_a + \sigma_s$                       | mostly scattering and absorption                             |                                                              |
|                                                | $I = I_0 e^{-N \sigma_T x}$                                  | same equation as photon attenuation                          |                                                              |
|                                                | $I = I_0 e^{-\Sigma x} = I_0 e^{- x/\lambda}$                | same $\Sigma = N\sigma_T$ as before                          |                                                              |
|                                                | $\lambda = 1 / \Sigma$                                       | mean attenuation length = **mean free path**                 |                                                              |
|                                                | $1/\lambda = (1/\lambda_a) + (1/\lambda_s)$                  | since $\Sigma = \Sigma_a + \Sigma_s$ from the first equation |                                                              |

# 6. Detectors and Instrumentation

Here, we consider the principle at systems for 

- detection radiation 
- producing controlled beams of radiation

In general, any detector gets its signal from the **interaction of radiations with matter**:

- collect charge released by ionization of gas (cased by radiation)
- excitation of electrons in semi-conductors (cased by radiation)
- observing fluorescent photons emitted due to de-excitation (cased by radiation)
- making ionization trails visible in film/solid gas (cased by radiation)

and in addition to detecting those radiations, we also want detectors to tell us more information such as **energy of the radiation**, type of the radiation, dose rate, etc.

- **dose rate** is quantity of radiation absorbed or delivered per unit time

- Dose equivalent (or effective dose) combines the amount of radiation absorbed and the medical effects of that type of radiation.

|               Name of Concept/Equation               | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :--------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
|         Observe Current by Collecting Charge         | $Q = \int_{0}^t i(t)dt$                                      | typically, radiation interact with matter and produces an electric charge (and ion) |                                                              |
|                                                      | ![image-20221014155430476](nuclear_eq_n_concepts/image-20221014155430476.png) | can collect that charge using an electric field = observe a current |                                                              |
|                                                      |                                                              | notice the **pulse like shape**                              |                                                              |
|                                                      | ![image-20221014155617597](nuclear_eq_n_concepts/image-20221014155617597.png) | In reality, since radiation is random, it is better described by Poisson statistics |                                                              |
|                                                      |                                                              | Poisson statistics = probability of a given number of events occurring in a fixed interval of time if the events occur with a known average rate and independently of the time since the last event |                                                              |
|             Modes of Detector Operation              | Pulse mode: when low radiation flux hence each radiation can be recorded as separate pulse |                                                              |                                                              |
|                                                      | Current mode: high particle flux hence average current is recorded |                                                              |                                                              |
|             Detectors Figures of Metric              | Things to check for a detector:<br />1) Energy Resolution<br />2) Detection Efficiency<br />3) Dead Time |                                                              |                                                              |
|                  Energy Resolution                   | ![image-20221014161258795](nuclear_eq_n_concepts/image-20221014161258795.png) | $E_0$ peak is the source, the Gaussian curve is the fitted energy spectrum *measured* |                                                              |
|                                                      | measures Full-Width at Half Maximum (FWHM)                   | ![image-20221014161426020](nuclear_eq_n_concepts/image-20221014161426020.png) |                                                              |
|                                                      | measured in units of energy or percent of its peak energy $E_0$ |                                                              |                                                              |
|                                                      | since it is usually first fitted to a Gaussian curve, then FWHM$=2.35\sigma$ |                                                              |                                                              |
|                                                      | ![image-20221014161543558](nuclear_eq_n_concepts/image-20221014161543558.png) | in reality due to all different effects *reaching the device* |                                                              |
|                 Detection Efficiency                 | Measures how much radiation it can capture, and if captured, how much it can record | capture = radiation reaches it<br />record = radiation reached AND recorded |                                                              |
|                                                      |                                                              | usually easy to detect charged particles $\alpha$ and $\beta$, but be careful as they have short range |                                                              |
|                                                      |                                                              | harder to deal with $\gamma, n$ because they have deposit little energy per unit path |                                                              |
|                                                      | ![image-20221014161857546](nuclear_eq_n_concepts/image-20221014161857546.png) | depends on geometry of the device                            | if device is liquid so that source submerges in it, then it is $4\pi$ full coverage |
|                                                      | ![image-20221014161905351](nuclear_eq_n_concepts/image-20221014161905351.png) | actual performance, dependent on detector material and thickness, and radiation type and energy |                                                              |
|                      Dead Time                       | due to physical/electronical problem, there will be dead-time when device becomes **unresponsive** between events |                                                              | e.g. takes time for the captured electron to travel to the cathode |
|                                                      |                                                              |                                                              | events almost overlap                                        |
|                                                      | $\tau$ =  dead time                                          |                                                              |                                                              |
|                                                      | ![image-20221014162158239](nuclear_eq_n_concepts/image-20221014162158239.png) | so non-paralyzable can recover 4                             |                                                              |
|                                                      |                                                              | paralyzable has the problem of **extended dead time**        |                                                              |
|                Dead Time Corrections                 | Paralyzable detector: $n=me^{n\tau}$                         | $n=$ true interaction rate, $m$ = recorded interaction rate, $\tau$ = deadtime |                                                              |
|                                                      | Nonparalyzable detector: $n= m/(1-m\tau)$                    |                                                              |                                                              |
|                                                      | ![image-20221014162313018](nuclear_eq_n_concepts/image-20221014162313018.png) | when true radiation $n$ is high, paralyzable can mistake it for a low $m$! |                                                              |
|             Principles of Gas Detectors              | detectors by using gas to be ionized by the radiation $\to$ record those ions |                                                              |                                                              |
|                                                      | three types of detectors on this principle<br />1) ionization chamber<br />2) proportional counter?<br />3) GM counter |                                                              |                                                              |
|                  Ionization Chamber                  | works by measuring ionization (of gas molecules) produced *solely* by incident ionizing particles (e.g. $\alpha$ radiation) |                                                              |                                                              |
|                                                      | ![image-20221014162757533](nuclear_eq_n_concepts/image-20221014162757533.png) | ionizing particles creates ionized gas $\to$ have a high enough field to prevent recombination $\to$ those ions complete the circuit by having electron goes to anode and positive ion to cathode |                                                              |
|                                                      |                                                              | but usually needs an amplifier as current produced could be small |                                                              |
|                                                      | need a certain amount of electric field applied              | see above                                                    |                                                              |
|                                                      | $A$ = #ions per sec produced $\times$ charge per ion         |                                                              | source disposing $1GeV\,s^{-1}$ energy per second and air in the chamber ionizes with $34$eV |
|                                                      |                                                              |                                                              | #ions per sec produced = $10^9 / 34$                         |
| Pulse Amplitude v.s. Voltage Applied in Gas Detector | ![image-20221014163205531](nuclear_eq_n_concepts/image-20221014163205531.png) | the fundamental reason why we have three types of gas detectors |                                                              |
|                                                      | pulse **amplitude** can give you information of the **energy** of ionizing particles! |                                                              |                                                              |
|                                                      | region I: increase $V$ means less recombination of ions      |                                                              |                                                              |
|                                                      | region II: full charge collection = no recombination         | Ionization Chamber                                           |                                                              |
|                                                      |                                                              | region where output is independent of applied voltage        |                                                              |
|                                                      |                                                              | Gas Amplification Factor (GAF) = 1                           |                                                              |
|                                                      | region III: electrons become more energized $\to$ can cause secondary ionization during collision. Those secondary ionized electrons can further produce ionizations | proportional chamber                                         |                                                              |
|                                                      |                                                              | amplification of current = Townsend Avalanche                |                                                              |
|                                                      |                                                              | GAF up to about $10^5$, but still proportional to the original ionization |                                                              |
|                                                      | region V: electrons so energized that it can excite inner electrons $\to$ UV radiation from de-excitation $\to$ ionizes other irrelevant atoms in the chamber | Geiger-Mueller Counter                                       |                                                              |
|                                                      |                                                              | can't distinguish initial input energy of those ionizing particles |                                                              |
|                                                      |                                                              | hence can only know the presence of those radiations         |                                                              |
|                 Proportional Chamber                 | electric field increased beyond region II, so that secondary ionizations occur |                                                              |                                                              |
|                                                      | pulse amplitude still tells you energy of input ionizing particles |                                                              |                                                              |
|                                                      | ![image-20221014164005297](nuclear_eq_n_concepts/image-20221014164005297.png) |                                                              |                                                              |
|                Geiger-Mueller Counter                | electric field increased so much that everything is ionized = pulse amplitude does not depend on the energy of input ionizing particles |                                                              |                                                              |
|                                                      | hence can only measure the presence of radiation             |                                                              |                                                              |
|           Scintillation Detector Mechanics           | energy of radiation $\to$ excitation of electrons of scintillation material $\to$ de-excitation/Compton scattering which emits UV/visible light |                                                              |                                                              |
|                                                      | those light photons are directed to the photosensitive surface $\to$ emit photo-**electrons** $\to$ amplified in PMT $\to$ observe pulse of current |                                                              |                                                              |
|                                                      | ![image-20221021173557853](nuclear_eq_n_concepts/image-20221021173557853.png) |                                                              |                                                              |
|               Scintillation Materials                | need high effiiency of converting energy to photons          |                                                              |                                                              |
|                                                      | linear conversion: output proportional to deposited energy from radiation |                                                              |                                                              |
|                                                      | short decay time = quick flash = short dead time             |                                                              |                                                              |
|                                                      | maximize conversion to output fluorescence                   |                                                              |                                                              |
|                                                      | transparent to its own emission (which is photon, which is *also* energy) |                                                              |                                                              |
|                 Scintillation Types                  | Fluorescence=emit visible radiation with emission time approx. 10 ns | preferred                                                    |                                                              |
|                                                      | Delayed fluorescence=above but loner emission time           |                                                              |                                                              |
|                                                      | Photofluorescence=longer wavelength and longer emission time |                                                              |                                                              |
|          Semi-Conductor Detector Mechanics           | energy of radiation creates electron-hole pair $\to$ electron move in the direction of applied field $\to$ current | works only if those e-h pairs do not recombine or get trapped in regions of impurity |                                                              |
|          Semi-Conductor Detector Properties          | only requires 3-4 $eV$ to create e-h pair, whereas to create ion pair in gas requires 30 $eV$ |                                                              |                                                              |
|                                                      | better energy resolution                                     |                                                              |                                                              |
|                                                      | faster charge collection = shorter dead time                 |                                                              |                                                              |
|            Semi-Conductor Detector Types             | diode                                                        |                                                              |                                                              |
|                                                      | high purity $Ge$                                             | see above                                                    |                                                              |
|                                                      | lithium drifted $Si$ or $Ge$                                 | alternatives to high purity, use $Li$ for drifting as dopant atoms |                                                              |
|              Thermoluminscent Detector               | operates by accumulating radiation energy and read altogether at the end |                                                              |                                                              |
|                                                      | therefore, we want crystals to de-excite as slow as possible when absorbed radiation | opposite of scintillation                                    |                                                              |
|                                                      | mechanism: electron and holes are elevated but below conduction band, hence "trapped" |                                                              |                                                              |
|             Neutron Detectors Properties             | cannot detect neutrons directly, but secondary radiation, such as $(n,p)$ |                                                              |                                                              |
|                                                      | depending on how fast the neutrons are, there are two types of neutron detectors |                                                              |                                                              |
|           Slow Neutron Detectors Mechanism           | for slow, thermal neutrons, nuclear reactions such as $(n,p),(n,\alpha), (n,f)$ have large cross section $\sigma \propto 1/v$ | basically can be triggered easily                            | $(n,p)$ means the reaction of $A+n \to B+p$                  |
|                                                      | mechanics: neutron $\to$ nuclear reactions $\to$ charged outputs (e.g. fission fragments) cause ionization $\to$ which can be measured |                                                              | $^{10} B(n,\alpha)^7 Li$                                     |
|             Slow Neutron Detector Types              | Proportional Counter                                         | $BF_3$ proportional counter performs $^{10} B(n,\alpha)^7 Li$ which a large cross section of 4010 b for thermal neutrons, and $Q=2.79MeV$ |                                                              |
|                                                      |                                                              | utilizes $(n,\alpha)$                                        |                                                              |
|                                                      | Fission Counters: coat detector with fissionable material    | utilizes $(n,f)$                                             |                                                              |
|                                                      | Activation Counter: using activation foils composed of material sensitive of neutron of different energies | utilizes neutron capture                                     |                                                              |
|                Fast Neutron Detector                 | use plastic or liquid organic scintillation material instead | $1/v$ means slow neutron detectors become not efficient      |                                                              |
|                                                      | in general, materials of rich hydrogen $\to$ recoiling protons produce energy for scintillation |                                                              |                                                              |
|               Particle Identification                | since radiation = any particle with energy, we might also want to know which particle it is |                                                              |                                                              |
|                PI: Counter Telescope                 | stack two or more detectors, and can measure $\Delta E$ between detectors and $E$ total |                                                              |                                                              |
|                                                      | $\Delta E$ can tell you stopping power                       |                                                              |                                                              |
|                                                      | $E\times \Delta E \propto mz^2$                              |                                                              |                                                              |
|                  PI: Time of Flight                  | measure time between detectors, hence determine $v$          |                                                              |                                                              |
|                                                      | if radiation is pulsed beam, then you can just measure arrival time at detector and actual beam pulse |                                                              |                                                              |
|                PI: Magnetic Analysis                 | use spectrometer with magnetic field to measure the deflection of charged particles |                                                              |                                                              |
|                                                      | $r = mv/qB$, can measure mass to charge if known velocity    | based on Lorentz Force $F=qv\times B$                        |                                                              |

# Midterm Concepts

Chapter 1

- atomic structure
  - plum pudding model, Rutherford model and its problems, Quantum Theory and its model, Bohr Model and its energy levels
  - Rutherford scattering experiment, result
- equivalence of energy and momentum
  - mass-energy equation
- uncertainty principle
- nuclear models
  - liquid drop, shell model, their differences
- electron volt, alternative mass unit
- binding energy, mass excess, and Q calculation
- binding energy curve (reproduce)
- nuclear stability curve (reproduce)
- radioactivity equation and activity
- secular equilibrium
- alpha decay
  - energy is discrete, etc
- beta decay
  - continuous energy up to a cut-off
- auger electron
- number of atoms and atomic densities calculations

Chapter 5

- stopping power
  - two components, use Bremsstrahlung
- Bethe-Bloch that 
  - stopping power goes $z^2 / v^2$
- stopping power v.s. energy curve
  - bragg peak curve
- heavy charged particles
  - how it loses energies, ionizing them and/or exciting them
- light charged particles
  - zig zag, no well-defined range
- cross sections
  - reaction rates, linear attenuation coefficient $\mu$
  - attenuation and its differential equation
- photon ways of interacting
  - three ways of how they work, but not formula
- neutrons interactions
  - three ways, elastic, inelastic, or reaction
  - thermal neutrons

Chapter 6

- detector two operation mode

- detectors figure of merit

  - equation of FWHM/$E_0$
  - efficiency, especially uncharged particles
  - definitions of absolute/intrinsic effiency

- dead time

  - paralyzable v.s. non-paralyzable
  - dead time correction equation
  - dead time curves

- will need to draw a detector

- three types of ionization chambers

- gas-field detector v.s voltage

  - its five regions and the graph itself

- scintillation detector mechanism

- semi-conductor detector mechanism

- TLDs

- Neutron Detectoros

  - slow.v.s fast needs hydrogen = moderates
  - fission counter

- particle identification

  - time-of-flight gives velocity + magnetic anlaysis 

  - none of the detector mechanisms except in the particle identification to tell us particle type

# Nuclear Structure

Aim:

- understanding what happens ***inside*** nucleus
  - e.g. understand its properties by understanding what **forces are responsible**
- no complete theory today fully describes the structure and behavior of complex nuclei

|      Name of Concept/Equation      | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :--------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
|         liquid drop model          |                                                              | nucleus regarded as a ***collection*** of neutrons and protons forming a droplet of incompressible fluid |                                                              |
|                                    |                                                              | good for **systematic behaviors** such as nucleon binding energy |                                                              |
|                                    |                                                              | discrepancies in liquid drop model = there is an ORDERED STRUCTURE within the nucleus in which neutrons and protons are arraged in stable quantum states in a potential well |                                                              |
|   Shell (Single-particle Model)    |                                                              | loosely held individual outer nucleons, which account for many of the nucleuss properties |                                                              |
|                                    |                                                              | very alike the electron shell model and how electrons arrange themselves |                                                              |
|           Nuclear Force            | binds nucleons in nucleus, n-n, p-p, p-n                     | extremely complicated, derivation from first principle       |                                                              |
|      Nuclear Force Properties      |                                                              | short range                                                  |                                                              |
|                                    | <img src="nuclear_eq_n_concepts/image-20221214025855146.png" alt="image-20221214025855146" style="zoom:50%;" /> | for very small separations, nucleons begin to repel = no clump |                                                              |
|                                    |                                                              | nuclear density approx constant for different sized nuclei = liquid drop |                                                              |
|  Nuclear Force Charge Dependency   | charge symmetric: same nuclear force for  p-p as for n-n     |                                                              |                                                              |
|   Nuclear Force Spin Dependency    | Average force for p-n > p-p or n-n by a factor of about 2    | both n and p are fermions = obey Pauli Exclusion for spin    |                                                              |
|                                    |                                                              | so for p-p and n-n, you have to have different spin, net $S=0$ |                                                              |
|                                    |                                                              | n-p can have be either anti-parallel or paralell. **Force in $S = 1$ state is stronger** than force in $S = 0$ state. Therefore, avg. force for p-n is greater than that for p-p or n-n (by about factor of 2). |                                                              |
|                                    |                                                              | Explains why can have bound n-p (deuteron), but not bound n-n or p-p (i.e., nuclear force not strong enough to bind the latter two configurations) | p-p repulsion;  n-n free particles + not strong enough force |
| Nuclear Force Spin-Orbit Coupling  | $\text{Spin-orbit Force}=L \cdot S$                          | $L,S$ being angular momentum and spin, respectively          |                                                              |
|                                    |                                                              | Force is attractive if S and L are parallel, and repulsive if they are anti-parallel |                                                              |
|                                    |                                                              | zero on average inside an atom                               |                                                              |
|    Semi-Empirical Mass Formula     | ![image-20221214030640303](nuclear_eq_n_concepts/image-20221214030640303.png) | estimating **binding energy**, which can then be used to estimate the **actual nuclear masses** for unknown nuclei (but known $A$ and $N$) |                                                              |
|                                    |                                                              | based on liquid-drop = estimates **collective properties of a nucleus** |                                                              |
|         SEMF Volume Energy         | $a_vA$ term                                                  | nucleon feels the force only from its nearest neighbors and the nucleon density is approx constant = force is constant = **B/A is approx constant** in the interior of the nucleus. |                                                              |
|         SEMF Surface Term          | $A^{2/3}$ since the first $A\propto \mathrm{Volume}$         | **reduced** by a factor proportional to the **surface area** of the nucleus |                                                              |
|                                    | ![image-20221214110550694](nuclear_eq_n_concepts/image-20221214110550694.png) | nucleons on the surface of the nucleus experience the nuclear force from nucleons inside the nucleus, but no force from the outside. hence reduced |                                                              |
|         SEMF Coulomb Term          | $\propto {Z^2}/{A^{1/3}}$                                    | Coulomb repulsion would further reduce binding energy        |                                                              |
|                                    |                                                              | protons repel each other with a long-range Coulomb force; <br />Mean radius of the nucleus is proportional to $R\propto A^{1/3}$<br />Coulomb force also $\propto Z^2$ |                                                              |
|         SEMF Symmetry Term         | $\propto (N-Z)^2$                                            | nucleus becomes more **unstable** the greater the difference between **Z and N**. Has the most effect for **light nuclei** |                                                              |
|                                    |                                                              | shell model result. The term is zero for $Z = N$ and becomes less important for heavy nuclei (high A), where $N > Z$ |                                                              |
|         SEMF Pairing Term          | $\Delta$                                                     | nucleons tend to ***couple pairwise*** into more stable configurations |                                                              |
|                                    |  > 0 if N and Z are both even                               | from Pauli Exclusion Principle                               |                                                              |
|                                    |  < 0 if N and Z are both odd                                |                                                              |                                                              |
|                                    |  = 0 if either N or Z is odd (i.e., A odd)                  |                                                              |                                                              |
|   Nuclear Fission Energy Barrier   | ![image-20221214111507480](nuclear_eq_n_concepts/image-20221214111507480.png) | barrier to fission is called the fission barrier or activation energy |                                                              |
|                                    | for **Heavy** Nuclei                                         | As s $A$ increases, the relative importance of the Coulomb repulsion term increases = it becomes energetically possible for the nucleus to split if it becomes **deformed** enough |                                                              |
|                                    |                                                              | Odd nuclei (($U_{92}^{235}$)) have very low activation energies, **since the pairing term is zero**, and can fission with low energy neutrons. |                                                              |
|          Isobaric Nuclei           | same number of total nucleons but swapped #neutrons and #protons | will affect the coulomb term in SEMF                         |                                                              |
|         Vibrational Model          | ![image-20221214112515740](nuclear_eq_n_concepts/image-20221214112515740.png) | ***SEMF assumed sphere shape,*** but in reality can deform   |                                                              |
|                                    |                                                              | vibration can actually be modelled with liquid-drop model    |                                                              |
|         Rotational States          | rotation doesnt produce any change of state                 | Collective rotational states can **only occur in non-spherical nuclei** (otherwise how do you know it is rotating?) |                                                              |
| Shell (Independent Particle) Model | aims to model energy level of nucleons                       |                                                              |                                                              |
|                                    | neutrons and protons fill energy level in the nucleus according to the Pauli Exclusion Principle | similar to electron energy levels                            |                                                              |
|                                    |                                                              | those structure like properties are not predicted by SEMF, which only deals with collective states |                                                              |
|                                    | islands of stability corresponding to closed shells, also called magic numbers, | 2 (1s), 8 (2s, 1p), 20, 28, ...                              |                                                              |
|      Nuclear Potential Energy      | predict ***energy levels*** in the nucleus, need to know $V(r)$ | arises from interactions with other nucleons.                |                                                              |
|                                    |                                                              | infinite well = simplest model                               |                                                              |
|       Woods-Saxon Potential        | $V(r) = -\frac{V_0}{1+e^{(r-R)/a}}$                          | quite a good approximate                                     |                                                              |
|       Spin-Orbital Potential       | ![image-20221214113711983](nuclear_eq_n_concepts/image-20221214113711983.png) | more complicated but **accounted for more splittings** of energy level |                                                              |
|                                    |                                                              | splittings due to spin $s$ and angular momentum $l$. If **aligned**, $s\cdot l$ positive hence binding energy is increased |                                                              |

# Nuclear Instability

Aim: Predict/explain ***why***

1. some ***half-lives are short** and others are long*, and
2. why ***certain energy transitions take place*** and others dont

In general there will be a) electromagnetic force; b) weak force; c) strong force

|       Name of Concept/Equation       | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :----------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
|       Gamma Emission Mechanism       | Excited nucleus may de-excite through $\gamma$-emission      | help understand why certain gamma decay have longer half-lives than another |                                                              |
|                                      |                                                              | gamma is everywhere in the sense that many radioactive decay accompanies gamma emission (see below, due to excited daughter nucleus) |                                                              |
|   Characteristic $\gamma$-emission   |                                                              | actually come from the transitions among the energy levels of the **daughter** nucleus. | <img src="nuclear_eq_n_concepts/image-20221218182726846.png" alt="image-20221218182726846" style="zoom:67%;" /> |
|                                      |                                                              |                                                              | the above means we have two decays:<br /><img src="nuclear_eq_n_concepts/image-20221218182834669.png" alt="image-20221218182834669" style="zoom: 15%;" />and then<img src="nuclear_eq_n_concepts/image-20221218182846700.png" alt="image-20221218182846700" style="zoom:10%;" /> |
|           Selection rules            | derived from consideration of conservation of ==angular momentum and parity==, ***specify the allowable transitions*** among energy levels | answers: "why are some gamma more likely to happen than others?" |                                                              |
|                                      | 1. Photon that carries away energy has angular momentum $L>0$ |                                                              |                                                              |
|                                      | 2. Conservation of Angular Momentum: $|l_i - l_f|\le L \le |l_i + l_f|$ | $l_i$, $l_f$ is the angular momentum of the initial and final nuclear state |                                                              |
|                                      | 3. Parity (wavefunction even or odd) is also conserved in EM transitions | whether or not a parity changed decides $\to$                | <img src="nuclear_eq_n_concepts/image-20221218204404711.png" alt="image-20221218204404711" style="zoom:50%;" /> |
|                                      |                                                              |                                                              | <img src="nuclear_eq_n_concepts/image-20221218204529542.png" alt="image-20221218204529542" style="zoom:50%;" /> |
|                                      | do ***not*** provide information on the probability of its occurrence, only if it *can* occur |                                                              |                                                              |
|    Competing Process in $\gamma$     | as gamma usually comes from de-excitaton, other processes such as de-excite with electron is competing (also comes from de-excitation) |                                                              |                                                              |
|                                      | e.g. Internal Conversion                                     | transfer energy to an orbital electron (K-shell or further out), ejecting it from the nucleus | another way to de-excite = the parent and child would be the same *as if performed gamma decay* |
|                                      |                                                              | **Single energy peak** for each orbital electron transition unlike continuous  spectrum. |                                                              |
|      $\gamma$ Transition Rates       | Weisskopf Single Particle  Transition Rates                 |                                                              | ![image-20221218204742112](nuclear_eq_n_concepts/image-20221218204742112.png) |
|                                      | $E_1$ is greatly favored                                     |                                                              | Selection rules allow E2, M3, E4, M5, and E6 transitions, but the E2 radiation is strongly favored. |
|                                      |                                                              |                                                              | ![image-20221218214350669](nuclear_eq_n_concepts/image-20221218214350669.png) |
|                                      | If transitions have *high multipolarity*, T$_{1/2}$ may be considerable | basically for E6, M6, etc above,  the probabiility is low = long half lives |                                                              |
|      Mixed $\gamma$ transitions      |                                                              | can substantially ***raise the probability*** that a particular energy -ray will be emitted |                                                              |
|                                      |                                                              | cant characterize transition probability using the single particle model |                                                              |
|         Beta Decay Equations         | ![image-20221218214832570](nuclear_eq_n_concepts/image-20221218214832570.png) |                                                              |                                                              |
|                                      | allowed reactions = conservation of ***energy/momentum/charge/lepton number*** |                                                              |                                                              |
|                                      | can find the end point of the continous $\beta$ spectrum     | recall that $Q$ gives the kinetic energy difference $Q = K_f - K_i = (m_i -m_f)c^2$ | $T_{end} = E_0 - m_e c^2$, the end point of the  spectrum. $E_0$ = total energy of the transition. |
|         Fermi's Golden Rule          | Probability of beta decay, determine the *transition rate* between an initial state (i) and a final state (f) for $\beta$ decay |                                                              |                                                              |
|                                      | ![image-20221218220413602](nuclear_eq_n_concepts/image-20221218220413602.png) | $\lambda$ measures the transition probability                |                                                              |
|           Electron Capture           | competes with +-decay when both modes are possible<img src="nuclear_eq_n_concepts/image-20221218220610791.png" alt="image-20221218220610791" style="zoom:25%;" /> | all $\beta$ are due to ==weak interaction==                  |                                                              |
| Factor affecting EC probability/rate | depends on the **overlap** of the electrons and the nucleuss wave functions |                                                              |                                                              |
|                                      | 1. electron is most likely to be captured when it is ***closer*** to the nucleus |                                                              |                                                              |
|                                      | 2. higher Z = being attracted more = size of $K$-orbit electrons orbit is smaller = being captured | EC importance over + decay increases with Z                 |                                                              |
|             Alpha Decay              | common decay mode for **heavy radionuclides**                |                                                              |                                                              |
|                                      | Shed four units of mass (two neutrons and two protons) in one decay. |                                                              |                                                              |
|    QM tunneling in Alpha Decay     | heavy nucleus like uranium has barrier of 20MeV, but emitted alpha particle from it can be *as low as 5 MeV*. now we know it is due to non-zero wave function |                                                              |                                                              |
|                                      | requires $Q>0$                                               |                                                              |                                                              |
|                                      | ![image-20221218221142073](nuclear_eq_n_concepts/image-20221218221142073.png) | explanation of QM tunneling                                  |                                                              |
|       Preformation Probability       | for alpha decay to happen, you need to first form the alpha particle in the nucleus |                                                              |                                                              |
|                                      | ![image-20221218221306606](nuclear_eq_n_concepts/image-20221218221306606.png) |                                                              |                                                              |

# Nuclear Reactions

Study a bit further on ***how/when reaction happens***

|                 Name of Concept/Equation                 | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :------------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
|            Types/Classification of Reactions             | **Elastic** scattering: a + A $\to$ a + A                    | scattering = Incident and outgoing particles are the same    |                                                              |
|                                                          | **Inelastic** scattering: a + A $\to$ a + $A^*$              | some energy goes into exciting internal levels in A, and later will go off $\gamma$ decay |                                                              |
|                                                          | **Knockout**: a particle is emitted (knocked out) from the nucleus |                                                              | e.g. stripping of a proton from a carbon nucleus             |
|                                                          | **Stripping** reaction if the transfer is from the projectile to the target. | Transfer reaction: 1 or 2 nucleons are transferred between the projectile and target. | C-12 + alpha -> C-8* + alpha*                                |
|                                                          | **Pickup** reaction if the transfer is from the target to the projectile. | Transfer reaction, i.e. target nucleus gained                | He-3 + p -> He-4* + gamma                                    |
|                                                          | **Direct** nuclear reactions: formation of a "new" nucleus typically involve the transfer of *just a few nucleons* (protons or neutrons) between the colliding nuclei. | without the creation of an intermediate compound nucleus.    | e.g. scattering                                              |
|                                                          | **Compound** nuclear reactions: results in the formation of an intermediate compound nucleus, and can result in the emission of several particles | involve the transfer of *many nucleons* between the colliding nuclei |                                                              |
|                                                          | **Resonance** reaction: incoming particle has right energy to excite an energy level in the target nucleus, greatly *increasing the cross section* |                                                              |                                                              |
|   Discern what Reaction happened with Energy Spectrum    | can distinguish different mechanism because they give rise to outgoing particles have different energy |                                                              |                                                              |
|                                                          | ![image-20221218222924675](nuclear_eq_n_concepts/image-20221218222924675.png) | **Discrete** energy peaks at high energies from direct reactions |                                                              |
|                                                          |                                                              | At lower energies, peaks correspond to more closely spaced energy levels cant be resolved |                                                              |
|                                                          |                                                              | At still lower energies, compound nuclei are formed, where neutrons and protons **share** the incoming particle energy and evaporate from the nucleus in a continuous spectrum | evaporate: formed compound nucleus $\to$ an equilibrium is reached so the compound nucleus *loses its energy slowly over time by emitting particles,* mostly protons and neutrons |
|                  Angular Distributions                   | angle of output particles relative to input particle         |                                                              |                                                              |
|        Angular Distributions for Direct Reactions        | *Direct* collisions (few nucleons take part) usually produce forward peaked reaction products | forward peak = products traveling in the **same direction** as input |                                                              |
|                                                          | *Direct* collisions = exhibit *oscillations* as a function of scattering angle due to the wave nature of the particles |                                                              |                                                              |
|       Angular Distributions for Compound Reactions       | Angular spectrum of evaporated particles from a *compound* nucleus is more *isotropic* | since the emitted particles **have no memory** of the direction of the incoming particle |                                                              |
|                                                          | ![image-20221218223426199](nuclear_eq_n_concepts/image-20221218223426199.png) | y-axis = prob of observing this                              |                                                              |
|                      Reaction rate                       | each reaction has its own cross-section = own reaction rate  |                                                              |                                                              |
|                                                          | $R =\sigma N_A \phi=\Sigma \phi$                             |  = reaction cross section<br /> = particle (e.g., neutron) flux<br />$N_A$ = the number of target atoms per unit volume |                                                              |
|                                                          |                                                              | ![image-20221218223905002](nuclear_eq_n_concepts/image-20221218223905002.png) | DD reaction cross section = higher E better because this is *fusion* |
| Classical Estimate of Reaction Cross Section Assumptions | calculate cross section itself (before we were given this)   |                                                              |                                                              |
|                                                          | assumptions = reaction happens when come close enough together for the strong nuclear force to act |                                                              |                                                              |
|        Classical Estimate of Uncharged Particles         | <img src="nuclear_eq_n_concepts/image-20221218224237587.png" alt="image-20221218224237587" style="zoom: 33%;" /> |                                                              |                                                              |
|                                                          | $\sigma= \pi (R_1 + R_2)^2 = \pi R^2$                        |                                                              |                                                              |
|         Classical Estimates of Charged Particles         | <img src="nuclear_eq_n_concepts/image-20221218224450325.png" alt="image-20221218224450325" style="zoom:150%;" /> |                                                              |                                                              |
|                                                          | Impact parameter, $b$ replaces $R$.<br />$\sigma = \pi b^2 = \pi R^2 (1-B/E)$ | means if $B>E$ reaction cannot occur                         |                                                              |
|                       QM Estimate                        | classical works when particles is more 'particle-like'       | classical approximation of reaction cross section is only decent for particles with **de Broglie wavelength less than the size of the nucleus** |                                                              |
|                                                          |                                                              | also kind of works if we are *heavy ions* = wavelength often smaller than nuclear dimensions |                                                              |
|                                                          | but if wave functions coincide, then reaction occur = can occur even if $B>E$ | in reality, high energy = looks like particle; low energy = looks like wave |                                                              |
|    Usage of Elastic Scattering for Nuclear Structure     | force causing scattering depends on the *spatial distribution of nucleus* => by analyzing the way particles scatterd = know about the size and distribution of *force field* = know about the *nucleus* |                                                              |                                                              |
|                                                          | **Electrons** make good probes of the nucleus since they are not absorbed and interact via the well-know electromagnetic force with the protons | if we take particles such as $\alpha$, then it interacts strongly once inside the nucleus = lose its identity and not reappear in the entrance channel |                                                              |
|           Compound Nuclear Reaction Mechanism            | Many nuclear reactions proceed in two or more steps          |                                                              |                                                              |
|                                                          | 1.  the *incoming* particle is absorbed by, and excites, the nucleus |                                                              |                                                              |
|                                                          | 2. the nucleus *loses* its excitation energy (decays) through one of several different, possible exit channels (decay branches or decay channels) |                                                              |                                                              |
|              Compound Nuclear Reaction Rate              | *Each decay channel* is characterized by a probability and mean lifetime, $\lambda = 1/\tau$ |                                                              |                                                              |
|                                                          | for compound reaction $A+a\to C\to B+b$<br /> reaction rate is $\sigma_{\alpha, \beta} = \sigma_c (\Gamma_\beta / \Gamma)$ | $\sigma_c$ is the cross section of forming the compound nucleus<br />$\Gamma_\beta / \Gamma$ is fractional decay width into the final channel $B+b$ |                                                              |
|                                                          |                                                              | basically, need $C$ to happen, and then also $\beta$ to happen |                                                              |
|        Energy width of Compound Nuclear Reaction         | due to the Uncertainty Principle, uncertainties in $\lambda$ and $\tau$ $\to$ *uncertainties in energies of the states* $\to$ energy spread |                                                              |                                                              |
|                                                          | ![image-20221218230238702](nuclear_eq_n_concepts/image-20221218230238702.png) |                                                              |                                                              |
|                 Cross Section Thresholds                 | ![image-20221218230654380](nuclear_eq_n_concepts/image-20221218230654380.png) | each decay mode *also has its own 'excitation energy'* required |                                                              |
|          Heavy ion particle accelerator's goal           | Heavy ion particle accelerators are used to cause reactions where the *target nuclei are blown apart* or to attempt to create *new heavy elements through absorption*. |                                                              |                                                              |

# Fission

This section will focus on **induced fission from neutron absorption**

- Some heavy transuranic (near uranium) nuclei can undergo induced fission if supplied with sufficient energy (7-8 MeV)
- about 6 MeV comes from binding of an extra ***neutron*** by the ***strong force*** and the rest from external sources
- an extra 1-2 MeV from pairing (depends on if your $N$ is odd or even)

|      Name of Concept/Equation       | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :---------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
|  Why is Fission Energetic Useful?   | <img src="nuclear_eq_n_concepts/image-20221218235727825.png" alt="image-20221218235727825" style="zoom: 33%;" /> |                                                              |                                                              |
|                                     | Fissile nuclei can fission with *low energy, thermal, neutrons* |                                                              |                                                              |
| Energy during/Mechanism of Fission  | 1. normally, the SEMF surface term provides a restoration force, like disturbing surface tension on water |                                                              |                                                              |
|                                     | 2. if sufficient energy is supplied, the shape will deform to such an extent that the Coulomb repulsion force will dominate = strong force only works locally = nucleus starts to break |                                                              |                                                              |
|                                     | 3. repulsive force will drive the (usually) 2 fission fragments apart and potential energy is *converted into kinetic energy* |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219000721638.png" alt="image-20221219000721638" style="zoom: 33%;" /> |                                                              |                                                              |
|      Fission Activation Energy      | The energy required to overcome the fission barrier = deform |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219000459450.png" alt="image-20221219000459450" style="zoom: 33%;" /> |                                                              |                                                              |
|        Properties of Fission        | Energetically preferred to have one heavy group and one light group as fission fragment | ![image-20221219000902306](nuclear_eq_n_concepts/image-20221219000902306.png) |                                                              |
|                                     | When a fissionable nucleus absorbs a neutron and forms a compound nucleus, there is a **competition between fission and gamma emission** to release excitation energy, which doesnt lead to fission | $\sigma_a = \sigma_f + \sigma_\gamma$                        |                                                              |
|        Discovery of Neutron         | James Chadwick found that if the energetic alpha particles emitted from polonium fell on certain light elements, specifically beryllium, an *unusually penetrating radiation* was produced (not gamma ray) |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219001726268.png" alt="image-20221219001726268" style="zoom:33%;" /> | this radiation's range, etc. can be measured by placing paraffin a distance away to **perform (n,p) reaction** |                                                              |
|                                     | the first 'weird' event that happened about this was: Walther Bothe and Herbert Becker = discovered that when energetic -particles from the decay of polonium impinged on certain light materials, they would ***eject high energy, very penetrating, neutral radiation***; they thought they were -rays | ![image-20221219002009283](nuclear_eq_n_concepts/image-20221219002009283.png) |                                                              |
|       'Discovery of Fission'        | Otto Hahn and Frederick Strassmann                           |                                                              |                                                              |
|                                     | Observed presence of barium and other middle-weight elements in a uranium sample bombarded by neutrons and noted *large release of energy*. |                                                              |                                                              |
|                                     | 12/2/1942  Enrico Fermi  1st controlled fission chain reaction in Chicago Pile 1 |                                                              |                                                              |
|       Start of Nuclear Weapon       | 7/16/1945  1st nuclear weapon test: ***Trinity*** in White Sands Proving Grounds, New Mexico. |                                                              |                                                              |
|                                     | called The Gadget. 20 kT (84 TJ). Pu implosion device      |                                                              |                                                              |
| Types of Nuclear Weapons Mechanism  | <img src="nuclear_eq_n_concepts/image-20221219002342388.png" alt="image-20221219002342388" style="zoom: 25%;" /> | normal **plutomium** okay, but densely pressed is not = can go over critical mass |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219002349437.png" alt="image-20221219002349437" style="zoom:25%;" /> | simply have separate halfs of sphere = explosion **combine** them = increase density = go react<br />now it is **uranium** |                                                              |
|                                     | Why no Pu+Gun Barrel? <br />1. Because plutonium is less dense than HEU, it would require a larger mass to reach criticality<br />2. even trace amounts of Pu-240 in the plutonium would release enough neutrons from **spontaneous fission** = wouldn't explode but fizzle |                                                              |                                                              |
|         Nuclear Power Plant         | 12/20/1951  **EBR-1**, Idaho National Lab, Idaho Falls, Idaho. First generation of electric power from nuclear power plant |                                                              |                                                              |
| Delayed Energy and Delayed Neutrons | nuclear reactor core and spent fuel *continue to emit considerable energy* even after the nuclear chain reaction process has *stopped*. This happens because |                                                              |                                                              |
|                                     | 1. fission fragments as a by-product are themselves every radio-active = release energy |                                                              |                                                              |
|                                     | 2. delayed neutrons play a critical role in the control of nuclear fission reactions because they are emitted at a *slower rate than prompt neutrons*, which are emitted immediately following the fission event = helpful to make power change smoother = but still are neutrons | ![image-20221219003325304](nuclear_eq_n_concepts/image-20221219003325304.png) |                                                              |
|          Neutron Economics          | each fission produces 2-3 neutrons of about 2MeV, but *prompt fission neutrons must be thermalized* to about 0.025 MeV |                                                              |                                                              |
|                                     | In the slowing down process, must avoid <br />1. absorption by other nuclei <br />2. leakage out of the system. |                                                              |                                                              |
|                                     | 3. (uncontrollable) absorbed neutron but gave non-fission reaction |                                                              |                                                              |
|           Chain Reaction            | <img src="nuclear_eq_n_concepts/image-20221219003615062.png" alt="image-20221219003615062" style="zoom:33%;" /> |                                                              |                                                              |
|    Neutron Multiplication Factor    | measure if your chain reaction is sustaining/increasing/decreasing |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219004107957.png" alt="image-20221219004107957" style="zoom: 25%;" /> |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219004140291.png" alt="image-20221219004140291" style="zoom:25%;" /> |                                                              |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219004201393.png" alt="image-20221219004201393" style="zoom:25%;" /> | recall the section on neutron economics                      |                                                              |
|       Thermal Fission Factor        | *probability of a fission event occurring* when a neutron collides with a nucleus | Not all neutrons absorbed in fuel cause fission              |                                                              |
|                                     | ![image-20221219004515942](nuclear_eq_n_concepts/image-20221219004515942.png) | basically $\sigma_f + \sigma_a = \sigma_T$                   | if 99.3% U-238 and 0.7% U-235, then the ratio is<br />$\frac{0.7*\sigma_f(^{235}U)}{(0.7\sigma_T(^{235}U)+99.3\sigma_T(^{238}U))}$ |
|                                     |                                                              |                                                              | because only $^{235}U$ can fission = Most nuclear reactors and all nuclear weapons **require higher U-235 enrichment** |
|    Methods for U-235 Enrichment     | <img src="nuclear_eq_n_concepts/image-20221219005126907.png" alt="image-20221219005126907" style="zoom: 25%;" /> | Gaseous Diffusion Enrichment: since U238 is a **little bit heavier** (cannot separate them chemically as they are the same, but physicaly not). Therefore smaller would go through more readily at the top |                                                              |
|                                     |                                                              | and you can repeat this process over and over                |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219005222054.png" alt="image-20221219005222054" style="zoom:25%;" /> | Gas Centrifuge Enrichment: again, U238 is a little bit heavier than U235, so another approach is to spin it very fast |                                                              |
|       Enrichment Definitions        | LEU  Low Enrichment Uranium: < 20% U-235                    |                                                              |                                                              |
|                                     | HEU  High Enrichment Uranium: > 20% U-235                   |                                                              |                                                              |
|                                     | Weapons Grade Uranium: > 90% U-235                           |                                                              |                                                              |
| Components in a Nuclear Power Plant | **Fuel pellets**  typically, sintered UO2 enriched to 3-5% in 235U. | Fuel pellets and fuel rods are both used to store and transport nuclear fuel in nuclear reactors. |                                                              |
|                                     | **Fuel rods**  typically, clad in Zircalloy to contain fission products and gases |                                                              |                                                              |
|                                     | **Control** rods  neutron absorbers among fuel assemblies   | to regulate the number of neutrons available to sustain the chain reaction |                                                              |
|                                     | Reactor core  typically, square cylinder shape            |                                                              |                                                              |
|                                     | Coolant/**moderator**  light water most common              | to slow down neutrons to thermal                             |                                                              |
|                                     | **Structural** material  alloys of steel (absorber/reflector) |                                                              |                                                              |
|             PWR and BWR             | <img src="nuclear_eq_n_concepts/image-20221219005748886.png" alt="image-20221219005748886" style="zoom: 33%;" /> | pressurized, transfer of heat energy                         |                                                              |
|                                     | <img src="nuclear_eq_n_concepts/image-20221219005817822.png" alt="image-20221219005817822" style="zoom: 33%;" /> | could have secondary radiation, but more efficient           |                                                              |
|            PWR Schematic            | <img src="nuclear_eq_n_concepts/image-20221219005940524.png" alt="image-20221219005940524" style="zoom:50%;" /> |                                                              |                                                              |

# Fusion

Why could fusion be useful if we had fission already?

- much less radioactive waste
- but the major problem is it is **technically harder**

|      Name of Concept/Equation       | Definition/Equation                                          | Notes                                                        | Example                                                      |
| :---------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
|   Why is Fusion Energetic Useful?   | <img src="nuclear_eq_n_concepts/image-20221219013327992.png" alt="image-20221219013327992" style="zoom:50%;" /> |                                                              |                                                              |
|    Extra Requirement for Fusion     | since nuclei are positively charged, so must impart **high kinetic energies** to the to overcome the repulsive Coulomb barrier = close enough for **strong force** to form new nucleus |                                                              |                                                              |
|                                     | 1. high temperature (i.e., kinetic energy)                   |                                                              |                                                              |
|                                     | 2. high density (close enough)                               |                                                              |                                                              |
|                                     | 3. confinement time: amount of time that the plasma must be confined in order for the reactions to occur | later on will see that density and confinement determines how much energy can be gained | <img src="nuclear_eq_n_concepts/image-20221219014048587.png" alt="image-20221219014048587" style="zoom: 50%;" /> |
|     Magnetic Confinement Basics     | confine plasma using magnetic fields = quite complicated setup | because plasma = lot of ***charged*** particles in gaseous form |                                                              |
|                                     | works based on Lorentz Force                                 | but will lose them if they move in parallel of the field!    |                                                              |
|                                     |                                                              | <img src="nuclear_eq_n_concepts/image-20221219015741263.png" alt="image-20221219015741263" style="zoom:33%;" /> |                                                              |
|        Inertial Confinement         | Heat and fuse ions so rapidly that they do not have time to escape. |                                                              |                                                              |
|                                     | typically use laser beams to compress and heat fuel pellets. |                                                              |                                                              |
|      Fusion Reaction Examples       | ![image-20221219014333643](nuclear_eq_n_concepts/image-20221219014333643.png) | Most uses isotopes of hydrogen to ==minimize repulsion==     |                                                              |
|                                     |                                                              | the highlighted ones have high cross section, and non-gamma energy release = can be more easily retrained |                                                              |
|                                     |                                                              | Helium as a product is ==very stable==, hence releases a lot of energy! |                                                              |
|  Properties of DT Fusion Reactions  | high cross section                                           |                                                              |                                                              |
|                                     | produces a lot of energy                                     |                                                              |                                                              |
|                                     | produces an alpha particle, which can be ***re-used to heat up the plasma*** |                                                              |                                                              |
|                                     | shield against neutrons, which are also very energetic       |                                                              |                                                              |
|                                     | tritium (T) don't occur naturally, so often need to be bred in the reactor factory | can be bred from Lithium                                     |                                                              |
|     Why would DT be preferred?      | 1. requires lower energy<br />2. produces more energy        | ![image-20221219015116694](nuclear_eq_n_concepts/image-20221219015116694.png) |                                                              |
|       Reaction Rate of Fusion       | Reaction prob per unit time = $n_2 \sigma v$                 | $v$ speed of the particles<br />$n_2$ is density of particle 2 |                                                              |
|                                     |                                                              | considers prob of particle 1 interacting with particle 2     |                                                              |
|                                     | total reaction rate per unit volume: $R=n_1n_2\sigma v$      |                                                              |                                                              |
|                                     | $R=n_1n_2 \lang \sigma v \rang$                              | since plasma = gas particles = velocity is defined by Boltzmann distribution |                                                              |
| Fusion Energy Output and Break-Even | Fusion energy output = $E_f=n_1n_2\lang \sigma v\rang Q \tau$ | $\tau$ is ==confinement time==!<br />$Q$ is energy released per reaction |                                                              |
|                                     |                                                              | also includes ==density $n_1,n_2$==                          |                                                              |
|                                     | Break-Even $n\tau > \frac{12kT}{\lang v \sigma \rang Q}$     | **Lawson Criteria**: need to achieve this to get more energy out than input |                                                              |
|                                     |                                                              | ![image-20221219015523087](nuclear_eq_n_concepts/image-20221219015523087.png) |                                                              |
|     Magnetic Bottle Confinement     | 1. Closed-field geometry (e.g., torus)                       |                                                              |                                                              |
|                                     | 2. **Toroidal field** (TF)  Coils external and perpendicular to toroidal containment vessel generate TF |                                                              |                                                              |
|                                     | 3. **Poloidal field** (PF)  pass current around axis of torus. Compensates for weakening TF with increasing $r$ | <img src="nuclear_eq_n_concepts/image-20221219015910587.png" alt="image-20221219015910587" style="zoom: 25%;" /> |                                                              |
|                                     | ==Tokamak== is one famous example!                           |                                                              |                                                              |
|   Inertial Confinement Mechanism    | 1. the energy from the laser or particle beam pulses is absorbed by the fuel target, heating it to extremely high temperatures and causing it to undergo a rapid **expansion**. | difficulty is to generate sufficient power to achieve this   |                                                              |
|                                     | 2. This expansion creates a shock wave that **compresses** the fuel target, increasing the density and temperature of the plasma and creating conditions suitable for fusion reactions. |                                                              |                                                              |
|                                     | therefore, requires lasers and **fuel pellets**              |                                                              |                                                              |
|     National Ignition Facility      | NIF: 192 laser beams focused onto small target, uses Indirect target |                                                              |                                                              |
|                                     | ![image-20221219020446322](nuclear_eq_n_concepts/image-20221219020446322.png) | Laser pulse vaporizes the heavy metal case, generating intense **x-rays inside the hohlraum**, which compresses and heats the DT fuel |                                                              |
|                                     |                                                              | Hohlraum = the metal cylinder                                |                                                              |

# Final 

- Nuclear sturcture
  - spin has an effect on energy
  - know every term in SEMF, what it represents
  - pairing term
  - vibrational harmnoics shapes
  - nuclear potential energy
- Nuclear Instability
  - gamma emission comes from nucleus, actually comes fro the daughter
  - selection rules for gamma emission, including the table
  - transition ratess
  - Decay schemes
    - high polarity and high half life
  - internal conversion (eject electron)
  - beta decay three possible transitions
    - tell by excess protons or neutrons whether if it goes beta plus or beta minus
  - alpha decay
    - QM consideration that as long as Q is positive it will happen
- Nuclear Reactions
  - classification
    - scattering, inelastic scattering; knockout
    - direct reactions and compound-nucleus decay
  - cross section and density equation and reaction rate
    - estimate the cross section such as assuming no coulomb interactions with $\pi R^2$; but with intearaction $\pi b^2$
    - cross section calculation for charged particles
    - no need to know partial waves
    - compound nuclear reactions equation
- Fission
  - binding energy curve and why fission is possible
  - high cross section with thermal neutrons
  - fission activation energy - when you pass the poit of this yuo can fission
  - fission history and discovery of neutron
  - chain reaction graph of uranium
    - neutron multiplication factor
    - reactors would want to stay at $k=1$, which is controlled by control rods
    - why enrichment is needed
  - nuclear reactor basics
    - fuel pellets
    - Pressurized WR and BWR designs; difference is that water doesn't boil and goes into heat exchange
- Fusion
  - why fusion is possible in binding energy curve
    - but the hard part is to have a high temperature, etc.
    - magnetic confinement v,s, inertial confinement: heating them so quickly
  - reaction rate for fusion equation with $R=n_1n_2 \sigma v$
  - triple product and lawson criterion
  - toroidal fields, know stuff like this exists
    - tokamak
  - inertial confinement
    - using lasers
- some shared topisc shuch as alpha and beta decay

- Energy calculation, very practical]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Equations and Concepts for Intro to Nuclear]]></summary></entry><entry><title type="html">ELEN6885 Reinforcement Learning part2</title><link href="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2.html/" rel="alternate" type="text/html" title="ELEN6885 Reinforcement Learning part2" /><published>2022-12-20T00:00:00-05:00</published><updated>2022-12-20T00:00:00-05:00</updated><id>/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2</id><content type="html" xml:base="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2.html/"><![CDATA[# RL packages

Now we start to discuss using DL for modeling. Some commonly used packages include

**Mainly for this course**

- openAI `gym` mainly an online environment for bench-marking for algorithm performance

**Other**

- `stable-baselines3` implementations of many modern RL algorithms
- `rllib` includes ray tuning 

> Resources
>
> - colab tutorial: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb
> - openai doc: https://www.gymlibrary.dev/
> - stable-baselines3 doc: https://stable-baselines3.readthedocs.io/en/master/

## OpenAI Gym

> - Separate the ==implementation of the environment== from the implementation of the learning algorithm.
> - Make it easy to compare various RL algorithms on the same/standardized environment.

- installation, e.g. `gym[mujoco]`

- has many environments which can be treated as **benchmark** "datasets"

  | <img src="rl/image-20221028153609045.png" alt="image-20221028153609045" style="zoom:33%;" /> | <img src="rl/image-20221028161012225.png" alt="image-20221028161012225" style="zoom: 33%;" /> | <img src="rl/image-20221028161048421.png" alt="image-20221028161048421" style="zoom:33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |

  and many more

As basically they are interactive environments, the `environment` is what is implemented, meaning your job is to **determine what action to take** (i.e. your model) given your state (e.g. history of rgb pixels of the game), and **environment gives you a reward and next state**

<img src="rl/image-20221028161310313.png" alt="image-20221028161310313" style="zoom:50%;" />

 Specifically the `environment` will give you

- **observation**: the new state simulated by the environment
- **reward**: defined and computed by the environment
- **done**: A boolean indicating if episode has finished
- **info**: additional info used only for debugging

And the most important is the ==observation & action space== for each environment:

- it will have an `action_space` specifying what are the legal actions
  - `Discrete`: A discrete space where $a(s) \in \{0,1,...,n\}$
  - `Box`: An n dimensional continuous space with optional bounds
  - `Dict`: A dictionary of space instances such as `Dict({"position": Discrete(2), "velocity": Discrete(3)})`
- it will have an `observation_space` specifying what will be returned
  - e.g. RGB pixel values of the screen

---

*Examples: Cliff Walking Environment*

The board is a 4x12 matrix, with (using NumPy matrix indexing):

- `[3, 0]` as the start at bottom-left
- `[3, 11]` as the goal at bottom-right
- `[3, 1..10]` as the cliff at bottom-center

If the agent steps on the cliff, it returns to the start. An episode terminates when the agent reaches the goal.

<img src="rl/image-20221028161926749.png" alt="image-20221028161926749" style="zoom: 50%;" />

**Reward**

- Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward.

**Action Space**: There are 4 discrete deterministic actions:

- 0: move up
- 1: move right
- 2: move down
- 3: move left

**Observation Space**: returned as a flattened index to represent the coordinate of the system

- There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal (as this results in the end of the episode). It remains all the positions of the first 3 rows plus the bottom-left cell. 
- The observation is simply the current position encoded as flattened index. You can easily convert back to coordinate system with `np.unravel_index`

---

*Example: Mujoco Humanoid*

The 3D bipedal robot is designed to **simulate a human**. It has a torso (abdomen) with a pair of legs and arms. The legs each consist of two links, and so the arms (representing the knees and elbows respectively). The goal of the environment is to walk forward as fast as possible without falling over.

![image-20221028162424416](rl/image-20221028162424416.png)

**Action space** is a `Box(-1, 1, (17,), float32)`. An action represents the **torques** applied at the hinge joints.

- action is a box has 17 different joints, each controlling torque from `[-1.0,1.0]`

- example joints you can control: `hip_1 (front_left_leg), angle_1 (front_left_leg), hip_2 (front_right_leg), right_hip_x (right_thigh), right_hip_z (right_thigh), ...`

 **Observation Space**

- Observations consist of **positional** values of different body parts of the Humanoid, followed by the **velocities** of those individual parts (their derivatives) with all the positions ordered before all the velocities

**Rewards**: the reward consists of four parts:

- `healthy_reward`: Every timestep that the humanoid is alive

- `forward_reward`: A reward of walking forward

- `ctrl_cost`: A negative reward for penalizing the humanoid if it has too large of a control force.

- `contact_cost`: A negative reward for penalizing the humanoid if the external contact force is

## Using OpenAI Gym

A minimal working example would be:

```python
import gym
env = gym.make("CartPole-v1") 
state = env.reset()
for _ in range(1000):
    env.render()  # show a window on your screen. In colab notebook more needs to be done
    action = env.action_space.sample()  # random policy
    state, reward, done, info = env.step(action) 
    if done:
        state = env.reset() 
env.close()
```

## Stable-Baselines-3

> Stable Baselines3 (SB3) is a set of reliable **implementations of reinforcement learning algorithms** in PyTorch.

Where OpenAI gym focuses on the environment, stable-baselines-3 focuses on the learning. But of course it is easily integratable with OpenAI `gym`. Smoe additional features include

- access to most popular deep reinforcement learning algorithms. Ex: DQN, PPO, A2C, DDPG
- allow you to implement deep reinforcement learning solutions in just a few lines
- ==vectorized training in parallel== (e.g. instantiate parallel copies of agents)

```python
import gym
from stable_baselines3 import PPO

env = gym.make("CartPole-v1")  # use mode='rgb_array' to return game image as state

# training
model = PPO("MlpPolicy", env, verbose=1) # multilayer perceptron + feedin environment
model.learn(total_timesteps=10_000)

# testing
obs = env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True) 
    obs, reward, done, info = env.step(action)
    env.render()
    if done:
        obs = env.reset() 
env.close()
```

# Function Approximation

Recall that all evaluation and control algorithms we discussed are **tabular methods** (e.g. $Q(s,a)$)

<img src="https://miro.medium.com/max/622/1*fe1eyEAcfj6KHWFXHUNHbQ.png" style="zoom:50%;" />

- Each update will only change the value of one state or one state-action pair, i.e., an entry in the lookup table
- The lookup table **may become unmanageable** when the number of states or state-action pairs goes up. For instance, what if we have a **continuous action space**?
- also needed a way to keep exploring, because we needed to estimate values of a lot of states to be accurate

> **Use function approximation** to treat $V(s)$ or $Q(s,a)$ as functions, i.e. we consider instead of tables, ==parameters of a function== such that:
>
> - for $V$: $S\to \mathbb{R}$
> - for $Q: S,A \to \mathbb{R}$
>
> so we consider $V(s,w)$ and $Q(s,a,w)$ for $w$ being the weights/parameters of the function

Function approximation methods has **advantages** for cases such as

- evaluate/predict $v(s)$ and $q(s, a)$ for large or high-dimension state space
- evaluate/predict $v(s)$ and $q(s, a)$ for continuing tasks (non-episodic)
- evaluate/predict $v(s)$ and $q(s, a)$ for partially observable problems
  - because we can **extrapolate** from the experienced samples (e.g. states)

and is usally trained with ==supervised training==

- e.g. given a $(s,G_t(s))$ pair, ask the model to do learn it
- use SGD to minimize the objective function such as MSE between $\hat{V}(s,w)$ and $V(s)$

## Value Function Approximation

We would like to consider $\hat{V}(s,w)$ and $\hat{Q}(s,a,w)$ parameterized by $w$:

<img src="rl/image-20221104083159491.png" alt="image-20221104083159491" style="zoom:33%;" />

where the target $V_\pi(s)$ and $Q_\pi(s,a)$ are basically predefined by the specific algorithms we want to use and our model aims to model them.
- In general we can have three types of modelling, where the last one modelling $Q$ from $V$ is not used often in practice
- for MC methods, bascially $V_\pi(s) = G_t$ is used as target, and it differs if you do TD(0), for instance
- since those are aimed to improve on tabular methods, dimensoinaltiy of $w$ is typically much smaller of states $|S|$, and highly related to how many features you want to encode in each state
- finally, notice that updating one weight will influence value for all states (cf. tabular methods)

There are **many ways** to construct your function approximator, e.g.:

- **Linear:** consider we having *three features* of a state $s=[s_1,s_2, s_3]^T$.  Then we can do:
  $$
  V(s,w) = s_1 w_w + s_2 w_2 + s_3 w_3 = [s_1, s_2,s_3]^T 
  \begin{bmatrix}w_1\\
  w_2\\
  w_3\end{bmatrix}
  $$
  or you can construct nonlinear features usch as $w_1*w_2$ 

- **non-linear:** using neural networks

- **decision tree**
- etc.

>  Here we consider ==differentiable== function approximation methods.

For instance, let $w$ be a column vector. Then we can consider
$$
J(w) = \mathbb{E}[ (\hat{V}(s,w) - V_\pi(s) )^2 ]
$$
to minimize this loss using SGD (single sample per batch) type approach to consider
$$
\Delta w = - \frac{1}{2} \alpha \nabla_w J(w)
$$
if you perform the chain rule and take derivatives you get:
$$
\Delta w = \alpha \mathbb{E}_\pi [(v_\pi(s) - \hat{v}(s,w)) \nabla_w \hat{v}(s,w)]
$$
Finally, as we are doing SGD:
$$
\Delta w = \alpha  \underbrace{(v_\pi(s) - \hat{v}(s,w))}_{\text{prediction error}} \underbrace{\nabla_w \hat{v}(s,w)}_{\text{gradient}}
$$
note that
- the above form is ==generic if you use MSE==
- $v_\pi(s)$ depends on what algorithm you use, e.g. MC means $v_\pi(s) = G_t$ is calculated at the end of episode.

---

*Example: Simple SGD updates*

Let the target be $y$, and the input features are 1-D $x$. So that you get the following dataset
$$
(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)
$$

Suppose your approximator has two parameters:
$$
\hat{V}(x) = w_1  + w_{2} x
$$

Then your MSE loss is simply:

$$
J(w) = \sum\limits_{i=1}^{n} (\hat{V}(x_i) - y_i)^2 = \sum\limits_{i=1}^{n} (w_1 + w_2 x_i - y_i)^2
$$

Therefore, in the case of SGD your per step weight update with learning rate $\eta$ is

$$
\begin{bmatrix} 
    w_1\\
    w_2 
\end{bmatrix} \gets
\begin{bmatrix} 
    w_1\\
    w_2
\end{bmatrix} - \eta \nabla_w J(w)
= \begin{bmatrix} 
    w_1\\
    w_2
\end{bmatrix} - \eta
\begin{bmatrix} 
    2(w_1 + w_2 x_i - y_i)\\
    2x_i(w_1 + w_2 x_i - y_i)
\end{bmatrix} 
$$

## RL Prediction with Value Approximation

Now, we employ VFA  methods to perform RL. Recall that in general we need two things:

- **evaluation/prediction** of $v_\pi(s)$ and $q_\pi(s,a)$
- **control/planning** to find optimal policy (usually by using $q_\pi(s,a)$)

Here, we first discuss the problem of prediction. As essentially it becomes supervised learning
1. agent interact using policy $\pi$ to collect experience/dataset
2. since the collected signals are only **rewards**, you usually need to then specify a **target** for the model to learn
3. now you have a supervised dataset, and you can use SGD to train your model

*For instance*:

- if you are using MC methos, then your computed target is simply $G_t = \sum\limits_{k=t}^{T} \gamma^{k-t} r_k$ so that your weight updates become
  $$
  \nabla w = \alpha \underbrace{( \textcolor{red}{G_t} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)
  $$
- if you are using TD methods, then ==technically your target is $R_{t+1}+\gamma V_\pi(S_{t+1})$==, but since you don't know $V_\pi$, you consider:
  $$
  \nabla w = \alpha \underbrace{( \textcolor{red}{R_{t+1}+\gamma \hat{v}(S_{t+1},w)} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)
  $$
  note that as our target is technically also parametrized by $w$ but we did not take its gradient, it is also called **semi-stochastic gradient descent**. (e.g. in DQN, we would use old $\hat{v}(S_{t+1},w^-)$ as targets and update it over time)
- for TD($\lambda$), the target is $G_t^\lambda$:
  $$
  \nabla w = \alpha \underbrace{( \textcolor{red}{G^{\lambda}_t} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)
  $$
  which is the forward-view. The **backward-view** update is shown in the section [TD($\lambda$) with Value Function Approximation](#TD($\lambda$)_with_Value_Function_Approximation)

### Linear Function Approximation

We can consider a simple case study of using linear function approximation to predict $v_\pi(s)$.

Your general pipeline would be:

1. determine your feature representation $x(s)$ for each state $s \in S$:
$$
  \bold{x}(S) = \begin{bmatrix} 
      x_1(S)\\
      \vdots\\
      x_n(S) 
  \end{bmatrix} 
$$
2. define your **model**. Here we consider a linear approximator, we consider:
   $$
   \hat{v}(S, w) = \bold{x}(S)^{T} \bold{w} = \sum\limits_{j=1}^{n} x_{j} (S)w_j
   $$
3. define your **objective**: consider given the true value $v_\pi(S)$:
   $$
   J(\bold{w}) = \mathbb{E}_\pi[(v_\pi(S) - \bold{x}(S)^{T} \bold{w} )^2]
   $$
4. perform gradient descent. Using SGD the update rule for weights become:
   $$
   \begin{align*}
    \nabla_{w} \hat{v}(S, w) &= \bold{x}(S)\\
    \Delta w &= \alpha \underbrace{(v_\pi(S) - \bold{x}(S)^{T} \bold{w} )}_{\text{prediction error}} \bold{x}(S)\\
   \end{align*}
   $$

Note that in practice: 
- very important to choose the right features and the representations
- for linear approximators, the gradient $\nabla_{w} \hat{v}(S, w) = \bold{x}(S)$ is just the feature vectors


**Special Example: modelling table look up**

We can actually use a linear approximator and ==get back tabular methods==.
- we can make feature to be onehot of states
- then weights are basically the values in the table lookup

$$
\bold{x}(S) = \begin{bmatrix} 
    1(S=s_1)\\
    \vdots\\
    1(S=s_n)
\end{bmatrix} 
$$

Then a linear approximator gives

$$
\hat{v}(S, w) = \bold{x}(S) = \begin{bmatrix} 
    1(S=s_1)\\
    \vdots\\
    1(S=s_n)
\end{bmatrix} \cdot  \begin{bmatrix} 
    w_1\\
    \vdots\\
    w_n 
\end{bmatrix} 
$$

so that essentially $\hat{v}(S_i) = w_i$ is like a table lookup.

### MC with Value Function Approximation

Recall that for MC methods, we consider a target of $G_t$. Therefore:

1. experiences $(S_i, A_i, R_i)$ and compute $(S_i, G_i)$ pairs to get training data
   $$
   (S_1, G_1), (S_2, G_2), \dots, (S_T, G_T)
   $$
2. then just perform supervised learning to train your model

<img src="rl/image-20221104092845712.png" alt="image-20221104092845712" style="zoom: 50%;" />

Note that

- **theoretically**, in order to guarantee convergence, $\alpha$ should be decreasing but needs infinite visit of all states (the usual stochastic assumption) $\sum \alpha = \infty, \sum \alpha^2 < \infty$, for instance $\alpha = \frac{1}{t}$
- but in reality people just use constant $\alpha$ and it works fine

### TD(0) with Value Function Approximation

Recall that the TD Target $R_{t+1} + \gamma \hat{v}(S_{t+1},w)$ is **biased** as we are using $\hat{v}(S_{t+1},w)$ to estimate $v\pi(S_{t+1})$. 

But you can still regardless compute the supervised signal as $(S_i, R_{i+1} + \gamma \hat{v}(S_{i+1},w))$ hence 

1. collect experience hence a dataset of
   $$
   (S_1, R_2 + \gamma \hat{v}(S_2,w)), (S_2, R_3 + \gamma \hat{v}(S_3,w)), \dots, (S_{T-1}, R_T + \gamma \hat{v}(S_T,w))
   $$
2. then again supervised training on your $\hat{v}(S,w)$ with SGD:
   $$
   \nabla \bold{w} = \alpha \underbrace{(R_{t+1} + \gamma \hat{v}(S_{t+1},w) - \hat{v}(S_t,w))}_{\text{prediction error}} \nabla \hat{v}(S_t,w)
   $$
   or sometimes people just write (as TD is used often)
   $$
   \nabla \bold{w} = \alpha \delta \nabla \hat{v}(S_t,w)
   $$

Then the rest of the algorithm is basically the same as the MC version. Note that
- again, this is not fully stochastic = semi-stochastic gradient as that target $R_{i+1} + \gamma \hat{v}(S_{i+1},w)$ is parameterized yet no gradient is taken
- examples of this type of algorithm is DQN, and active research is on improving this semi-gradient algorithm

### TD($\lambda$) with Value Function Approximation

The target for TD($\lambda$) is $G_t^\lambda$, which is a weighted sum of TD targets and is still biased of the true value $v_\pi(S_t)$.

But regardless:
1. collect experience hence a dataset of
   $$
   (S_1, G_1^\lambda), (S_2, G_2^\lambda), \dots, (S_T, G_T^\lambda)
   $$
2. then again supervised training on your $\hat{v}(S,w)$ with SGD. First the ==forward view== is simply:
   $$
   \nabla \bold{w} = \alpha \underbrace{(G_t^\lambda - \hat{v}(S_t,w))}_{\text{prediction error}} \nabla \hat{v}(S_t,w)
   $$
3. then we derive the update rule being the ==backward view==:
   $$
   \begin{align*}
   \delta_{t} &= R_{t+1} + \gamma \hat{v}(S_{t+1},w) - \hat{v}(S_t,w)\\
   E_{t} &= \gamma \lambda E_{t+1} + \nabla \hat{v}(S_t,w) \\
   \nabla \bold{w} &= \alpha \delta_{t} E_{t}
   \end{align*}
   $$

So we get the algorithm being:
<img src='rl_p2/image-20221104155114.png' style='zoom:50%;'/>

### Convergence of Prediction Algorithms

In general TD methods are fast to learn but are unstable. 

<img src='rl_p2/image-20221104155252.png' style='zoom:50%;'/>

## RL Control with Function Approximation

> How do we find the **optimal policy** once we estimated the value or action-value function using VFA?

Recall that as we are in the case of model-free control, we generally consider:
- estimate ==action-value== function $Q_\pi(S,A)$
- policy improvement using $\epsilon$-greedy policy of $Q_\pi(S,A)$

Hence the general policy iteration (still applies) basically looks like

<img src="rl/image-20221104093515112.png" alt="image-20221104093515112" style="zoom:50%;" />

> As policy improvement is based on $Q_\pi(S,A)$, all we need to do is to **estimate $Q_\pi(S,A)$ using VFA.** This essentially is the same as fitting $V_\pi(S)$ but use $Q$ instead in formulas.

Hence our model considers
$$
\hat{q}(S,A,w) \approx \hat{q}_\pi(S,A)
$$

then to fit it, we consider
$$
J(\bold{w}) = \mathbb{E}_{\pi}[(\hat{q}(S,A,w) - q_\pi(S,A))^2]
$$

and using SGD the gradient is

$$
\nabla w = \alpha \underbrace{(q_\pi(S,A) - \hat{q}(S,A,w))}_{\text{prediction error}} \nabla \hat{q}(S,A,w)
$$

and based on different algorithms you use different targets to compute $q_\pi(S,A)$

As it is basically the same as the value function case, we will quickly show:

- for MC methods, the target is $G_t$
  $$
  \nabla w = \alpha \underbrace{(\textcolor{red}{G_t} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)
  $$
- for TD(0), the target is the estimated TD target $R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w)$
  $$
  \nabla w = \alpha \underbrace{(\textcolor{red}{R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w)} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)
  $$
- for *forward-view* TD($\lambda$) the target is $G_t^\lambda$
  $$
  \nabla w = \alpha \underbrace{(\textcolor{red}{G_t^\lambda} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)
  $$
  and the **backward view update** rule is
  $$
  \begin{align*}
  \delta_{t} &= R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w) - \hat{q}(S_t,A_t,w)\\
  E_{t} &= \gamma \lambda E_{t+1} + \nabla \hat{q}(S_t,A_t,w) \\
  \nabla \bold{w} &= \alpha \delta_{t} E_{t}
  \end{align*}
  $$

the algorithms are again basically the same as the value function case.

### Convergence of Control Algorithms

Problem mostly occurs when you do TD and Non-linear function approximation.

<img src='rl_p2/image-20221104160154.png' style='zoom:40%;'/>

## Batch Method for RL Applications

Before, we are talking about SGD which uses a sample and throw it away after updates. 

> **Aim**: We can be more sample efficient by sampling a batch and perform updates. Additionally, it also provides a better estimate to solve the least square problem.

Consider a given experience dataset $D$ consisting of state-value pairs provided:

$$
\mathcal{D} = \{ (s_1, v_1^{\pi}), (s_2, v_2^{\pi}), ..., (s_T, v_T^{\pi}) \}
$$

then the best parameter that ==best fits $v_\pi$== obviously needs to perform least square on ==all of them==
$$
LS(\bold{w}) = \sum_{t=1}^T (v_t^{\pi} - \hat{v}(s_t,w))^2
= T \cdot  \mathbb{E}_{\pi}[(v^{\pi} - \hat{v}(s,w))^2]
$$
or $T$ times the average error you make. Naive SGD for each sample would not work as recall
- each piece of experience would be correlate as they will come from real experience
- but fitting VFA assumes that the samples are **independent**

therefore, they will be techniques such as experience replay to shuffle the order so data appears IID:

1. given an experience consisting of state value pairs
   $$
   \mathcal{D} = \{ (s_1, v_1^{\pi}), (s_2, v_2^{\pi}), ..., (s_T, v_T^{\pi}) \}
   $$
2. repeat
   1. sample state, value for experience
      $$
      (s_t, v_t^{\pi}) \sim \mathcal{D}
      $$
   2. update the parameters using SGD
      $$
      \Delta w = \alpha (v_t^{\pi} - \hat{v}(s_t,w)) \nabla \hat{v}(s_t,w)
      $$
3. converges to LS solution:
   $$
   \bold{w} = \arg\min_{w} LS(\bold{w})
   $$

This basically is the SGD version of **experience replay** in DQN.

---

*Brief Introduction of DQN*:

Here we will deal with non-linear function approximation and the problem of **correlation**.

> **Non-linear neural networks**: needs a static training set + uncorrelated IID data during training

<img src="rl/image-20221104090857675.png" alt="image-20221104090857675" style="zoom: 33%;" />

DQN uses ==experience replay== to remove correlations between consecutive samples of data and ==fixed Q-target== to stabilize the learning process.

The overall algorithm consists of:

1. take action $a_t$ in state $s_t$ and observe reward $r_t$ and next state $s_{t+1}$
2. store transition $(s_t, a_t, r_t, s_{t+1})$ in replay memory $D$
3. **sample random minibatch** of transitions $(s_j, a_j, r_j, s_{j+1})$ from $D$
4. compute Q-learning targets w.r.t to **old network $\bold{w}^-$** and optimize MSE:
   $$
   L(\bold{w}) = \mathbb{E}_{(s_j, a_j, r_j, s_{j+1}) \sim D}[(r_j + \gamma \max_{a'} Q(s_{j+1}, a', \bold{w^{-}}) - Q(s_j, a_j, \bold{w}))^2]
   $$
5. batch gradient descent on $L(\bold{w})$ to update $\bold{w}$

# Policy Gradient

Recall that our task is to find the best policy given some environment that we can interact with:

<img src='rl_p2/image-20221112013422.png' style='zoom:30%;'/>

The environment transit and returns new $s_{t+1}, r_{t+1}$ to the agent, **and agent needs to spit out a $a_t$ given a state $s_t$**.

The previous section considers first learnig an action-value function $Q_\theta(s,a)$ and then using it to find the optimal policy $\pi_\theta(a|s)$, e.g. doing greedy/epsilon-greedy policy, e.g. learn a $Q$ using MC, TD, SARSA, etc.

> **can we perhaps directly learn a policy $\pi(a|s)$?**
> - though we often still need some estimate of $Q$ to do policy improvement, but we can ==avoid doing $\arg\max$ over all actions to get a policy==

**Advantages**

- now action probability changes more smoothly, meaning better convergence
- easly for encoding exploratoin
- avoid the need of $\arg\max q(s,a)$ which can be costly if $|S|\times |A|$ is large

**Disadvantages**

- value-based could more efficient for a small number of states and actions
- hard to get an unbiased estimate through sampling (you will see that the gradient has an expected value term)
-  many current approaches are still based on MC, hence many has high variance and could lower convergence speed

## Policy Objective Functions

> The first thing is to design an objective function to minimize. Note that we **do not know a true policy** but only an environment
>
> - for $Q$/$V$ estimates, we can easily compute a loss from a target such as $r+\gamma Q$

Approaches: we need some measure ==(a single score $\in \mathbb{R}$)== of how good a policy then:

- in episodic environments, we can use the **value function of the start state**
   $$
   J_1(\theta) = V^{\pi_\theta}(s_0) = \mathbb{E}_{\pi_\theta}[v_1]
   $$
- in continuing environments, we can use the **expected return** (which is the average of all possible returns)
   $$
   J_{avV}(\theta) = \sum\limits_{s} d^{\pi_\theta} (s) V^{\pi_\theta}(s)
   $$
   which is basically the average of all possible returns, where $d^{\pi_\theta}(s)$ is the state distribution of the states under the policy $\pi_\theta$
- in general, we can also use an average reward per time-step:
   $$
   J_{avR}(\theta) = \sum\limits_{s} d^{\pi_\theta} (s) \mathbb{E}_{\pi_\theta}[r]
   = \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a}
   $$

and our aim is to ==maximize them==. Given this, we now need to do gradient ascent:
$$
\Delta \theta = \alpha * \nabla_\theta J(\theta)
$$
and no minus sign in front. But what is this gradient?

- to perform back-propagation, you need to have a derivative form of each operation = backed by math equation
- but considering the gradient above it is unclear what to do with the **distribution of states $d^{\pi_\theta}(s)$ is also parameterized by $\theta$**

## Policy Gradient Theorem

> **Aim**: manipulate the expression of gradient so that it does not need to do the summations

In a single step MDP problem we can fix the distribution of states:

$$
\begin{align*}
   \nabla J(\theta) 
   &= \nabla \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &= \sum\limits_{s} d^{\pi_\theta} (s) \nabla \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \nabla \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \frac{\nabla \pi_\theta(a|s) }{ \pi_\theta(a|s) } \mathcal{R}_s^{a}\\
   &= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \underbrace{\nabla \ln \pi_\theta(a|s) }_{\text{score function} } \mathcal{R}_s^{a}\\
   &= \sum\limits_{s} d^{\pi_\theta} (s) \mathbb{E}_{\pi_\theta}[\nabla_\theta \ln \pi_\theta(a|s) \mathcal{R}_s^{a}]
\end{align*}
$$

Overall (proof in the book), the theorem looks like:

> **Policy Gradient Theorem**: we can estimate the gradient for any differentiable policy $\pi_\theta$ and for any policy objective functions $J=J_1, J_{avR}$ or $\frac{1}{1-\gamma} J_{avV}$
> $$
> \begin{align*}
> \nabla_\theta J(\theta)
> &= \nabla_\theta \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) Q^{\pi_\theta}(s,a) \\
> &\propto \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s)
> =\mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
> \end{align*}
> $$
> so that ==gradient does not flow through distribution $d$==! note that the proportional up to a constant is fine because that can be absored into the learning rate $\alpha$

---

*Example*: gradient of Softmax policy using linear function approximation

Consider an naive modelling of $\pi$:
$$
\pi_\theta(s,a) = x(s,a)^T \theta
$$
but recall that $\pi \in [0,1]$, yet this does not satisfy such a constraint. Hence we can consider
$$
\pi = \mathrm{softmax}(x(s,a)^T \theta)
$$
for $\theta$ is a vector here. In this specific case, the **score function of this form** can be further computed as

<img src="rl_p2/image-20221111085101851.png" alt="image-20221111085101851" style="zoom: 33%;" />

and you can use it do the policy gradient.

However, **what if your action is continous?** For a continous probability, prob is zero given a state/action. Therefore, we need to instead model a PDF:

$$
a \sim \mathcal{N}(\mu(s), \sigma^2), \quad \mu(s) = x(s,a)^T \theta
$$

essentially modelling the mean to be modelled using VFA, and variance can either be pre-defined or also parametrized.

<img src='rl_p2/image-20221112025330.png' style='zoom:40%;'/>

where each state has its own distribution, and the x-axis is the available actions you can take. Now, we can **choose which action to take given a state we are at by sampling** from the distribution. 

Finally, in this case the score function is:
$$
\nabla_\theta \ln \pi_\theta(a|s) = \frac{a-\mu(s)}{\sigma^2} x(s,a)
$$

## MC Policy Gradient (REINFORCE)

Now we have already learned:
1. objective function for policy
2. gradient ascent methods
3. can compute gradient using gradient theorem
   - also specific forms if policy being softmax or Gaussian if continous

Recall that by policy gradient theorem:
$$
\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
$$

> **Reinforce**: MC policy gradient method considers
> - use return $G_t$ as an unbiased estimator of $Q^{\pi_\theta}(s,a)$
> - use one state $S_t$ and action $A_t$ to estimate the gradient
> - therefore stochastic gradient ascent and consider
> $$
> \Delta \theta_t = \alpha \nabla_\theta \ln \pi_\theta(A_t|S_t) G_t
> $$

and the algorithm looks like:

<img src="rl_p2/image-20221111090125607.png" alt="image-20221111090125607" style="zoom:33%;" />

## Actor-Critic Methods

> **Aim:** as MC methods using $G_t$ have a high variance, we can perhaps directly estimate $Q^{\pi_\theta}(s,a)$ and use it to estimate the gradient
> - this can also tell you how good is the new policy/improved

Use a ==critic== to estimate the action-value function:

$$
Q_w(s,a) \approx Q^{\pi_\theta}(s,a)
$$

So we can directly compute the policy gradient:

$$
\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q_w(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
$$

> Therefore **Actor-Critic methods** are:
> 1. Critic: update action-value function parameters $w$ (e.g. using TD(0) or TD($\lambda$), basically our previous section on VFA)
> 2. Actor: update policy parameters $\theta$ using policy gradient suggested by critic

Finally we approximate the policy gradient by stochastic sampling:
$$
\Delta \theta = \alpha \nabla_\theta \ln \pi_\theta(A_t|S_t) Q_w(S_t,A_t)
$$

An example using TD(0)-based estimate of Q, the Actor-Critic Algorithm looks like

<img src="rl_p2/image-20221111091952917.png" alt="image-20221111091952917" style="zoom:40%;" />

where basically:

- line 5, 6 is data collection through interaction

- line 7 is TD error and with line 9 for improving the critic (derived from gradient descent of MSE objective)

- line 8 is policy gradient update

### Compatible Function Approximation Theorem

A problem with actor-critic methods above is that approximating the policy gradient could **introduce bias**.

But it turns out that if we can choose VFA carefully, we can avoid introducing any bias

<img src="rl_p2/image-20221111092902627.png" alt="image-20221111092902627" style="zoom:40%;" />

(proof skipped)

## Policy Gradient with Advantage Function

> **Aim:** we can ==reduce the variance== of this existing policy gradient:
> $$
> \nabla_\theta J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
> $$
> because variance could come from how you estimate $Q^{\pi_\theta}(s,a)$, and the stochastic sampling of $S_t$ and $A_t$. This can be done by subtracting the baseline:
> $$
> Q^{\pi_\theta}(s,a) \to Q^{\pi_\theta}(s,a) - b(s)
> $$
> so that intuitively, picking $b(s)=V(s)$ can "re-center" all the $Q^{\pi_\theta}(s,a)$ and therefore reduce the variance

However, we could like our performance to **still be unbiased**, and it can be shown that this holds as long as baseline **$b(s)$ a function not of $a$.**

*Proof*: Consider the new gradient:

$$
\nabla \ln \pi_\theta (s,a) * [Q_w(s,a) - B(s)]
$$

we want to make sure the gradient is the same as the original one, therefore, $\mathbb{E}[\nabla \ln \pi * B(s)] = 0$ needs to be proved.

By definitino of expectation
$$
\begin{align*}
   \mathbb{E}_{s\sim \pi_\theta}[\nabla \ln \pi_\theta(s,a) * B(s)] 
   &= \sum\limits_{s} d(s) \sum\limits_{a} \pi_\theta(a|s) \nabla \ln \pi_\theta(s,a) * B(s) \\ 
   &= \sum\limits_{s} d(s) \sum\limits_{a} \pi_\theta(a|s) \frac{\nabla \pi_\theta(a|s)}{\pi_\theta(a|s)} B(s) \\
   &= \sum\limits_{s} d(s) \sum\limits_{a} \nabla \pi_\theta(a|s) B(s) \\
   &= \sum\limits_{s} d(s) B(s) \nabla \sum\limits_{a} \pi_\theta(a|s) \\
   &= 0
\end{align*}
$$

since $\sum \pi = 1$ is constant, therefore the last $\nabla$ is zero. ==Notice that this worked because $B(s)$ does not depend on action==

What is a good $B(s)$ to choose? One example is to use $V(s)$, hence the advantage funtion

> **Policy Gradient with Advantage Function**:
> the advantage function using $V(s)$ is:
> $$
> A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)
> $$
> then the policy gradient is:
> $$
> \nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} A^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
> $$

So how do we estimate the advantage function?

- estimate both $V$ and $Q$ separately, need two more set of paramters

- or we can estimate only using one (i.e. the value function)

  <img src="rl_p2/image-20221111093932628.png" alt="image-20221111093932628" style="zoom:40%;" />

  so you only need to estimate $V$

### Actor and Critic at Different Time Scales

A final nuiances is that the we can vary how **far ahead we look ahead** by changing the time scale of actor and critic.
- critic: estimating the critic can use TD($0$) until $G_t$
- actor: estimating  the actor can use different estiamte based on $V$

When estimating $Q$, we can also have actor at different time scales

<img src="rl_p2/image-20221111094240856.png" alt="image-20221111094240856" style="zoom:40%;" />

We estimating the policy gradient, if we have only a value estimate $V^{\pi_\theta}$:

<img src='rl_p2/image-20221112033819.png' style='zoom:38%;'/>

## Summary of Policy Gradient Algorithms

<img src='rl_p2/image-20221112033925.png' style='zoom:35%;'/>

# Planning and Learning

Here, we shift focus back to the problem of **planning/control**. We consider situations where:

- if you have access to a model, but your state/action space is enormous (DP is not a good idea), what is a **good planning algorithm**?

- if you have enough data and you can fit a model reasonably well, how do you use it to **further improve your planning algorithms** (such as SARSA)?

  - in reality the convergence of model could be faster than fitting value/action function because the later needs information to propagate

  - but of course, fitting a model is not an easy task

> **Model-based** methods rely on planning as their primary component, while **model-free** methods primarily rely on learning. 
>
> - Model-based: DP, and MCTS (this chapter). Also called the problem of *planning*
> - Model-free: Q-learning, SARSA, etc. Also called the problem of *learning*
>
> Of course, there are also hybrids, so that you can use a model/learned model to improve model-free algorithms, such as Dyna (this chapter)

## Rollout Algorithms and Tree Search

First, we discuss the model-based planning where model of the world is given.

For instance, consider the rule of Go game

<img src="rl_p2/image-20221118132038782.png" alt="image-20221118132038782" style="zoom:50%;" />

so that to **plan each step, we need to think ahead**. Therefore, it might make sense to consider ==simulation as part of planning==: if I take this step here, what happens next and can I win? So rollout algorithms in general involve

- try **all** potentially high reward possibilities/moves **starting from the current position**
- **estimate those moves=actions** by simulation=rollout, and sometimes in combination with some value function estimate
- pick the best move (e.g. UCB), and repeat

> Rollout algorithms are ==decision-time planning algorithms== based on Monte Carlo control applied to simulated trajectories that all ==begin at the current environment state==. 
>
> - They estimate action values for a given policy by averaging the returns of many simulated trajectories that start with each possible action and then follow the given rollout policy. 
> - When the action-value estimates are considered to be accurate enough, the action (or one of the actions) having the highest estimated value is executed, after which the process is carried out anew from the resulting next state. 

This is also important as we talk about the details of an rollout algorithm: MCTS, which is that

> It can be said the aim of a rollout algorithm (e.g. MCTS) is to ==improve upon the rollout policy; not to find an optimal policy==.
>
> - In some applications, a rollout algorithm can produce good performance even if the rollout policy is completely random. 
> - But the performance of the improved policy depends on properties of the rollout policy and the ranking of actions (e.g. Upper Confidence Bound) produced by the Monte Carlo value estimates. 
> - Intuition suggests that the **better the rollout policy and the more accurate the value estimates**, the ==better the policy produced== by a rollout algorithm (e.g. MCTS) is likely be (but see Gelly and Silver, 2007, and in AlphaGo it more human-like policy instead of better policy may be better).

### Upper Confidence Bounds

> **Upper Confidence Bound:** get a *better* balance between exploration and exploitation, besides e-greedy and softmax approach we have seen before.
>
> - e-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. 
> - It would be better to select among the non-greedy actions **according to their potential for actually being optimal**, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.

The ultimate form of the simple version of UCB looks like, for a one-step MDP problem (i.e. without states)
$$
a_t = \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln n_t}{N_t(a)}} \right]
$$
where:

- $n_t$ is the total number of actions you have taken until time $t$, and $N_t(a)$ is the total number of times an action $a$ is taken up to time $t$
- $Q_t(a)$ is your current estimated return of action $a$

- the $\arg\max$ is because we are doing the *upper confidence bound*
- essentially the left term is your **exploitation**, and right term is **exploration** trying to visit less-visited states

This form is derived from modeling the regret of each action you made, and ==minimize your regret==. 

<img src="rl_p2/image-20221118134055442.png" alt="image-20221118134055442" style="zoom: 50%;" />

What does this have to do with UCB?

- Lai and Robbins showed that the regret for the multi-armed bandit has to **grow at least logarithmically w.r.t. the number of plays $n$**
- UCB is proved to ==grow the regret logarithmically in that case== (by choosing actions from UCB), hence is optimal in that case

We can visualize what UCB is doing to be first estimating the "confidence" of your current estimate, and then pick the ones that are most promising

|                       Confidence Bound                       |                Upper Confidence Bound Action                 |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl_p2/image-20221118084108912.png" alt="image-20221118084108912" style="zoom:33%;" /> | <img src="rl_p2/image-20221118084255212.png" alt="image-20221118084255212" style="zoom:25%;" /> |

But in reality, UCB might not be easy to work with. Consider a realistic problem where you could have a large state space:
$$
a_t = \arg\max_a \left[ Q_t(s,a) + c \sqrt{\frac{\ln n_t}{N_t(s,a)}} \right]
$$
the $N_t(s,a)$ for many combinations will be zero until explored. Hence in those cases simple e-greedy of softmax could also work as well and is much simpler.

### Monte Carlo Tree Search

Now we can talk about MCTS. The objective is to do planning given a model, and specifically to improve upon a rollout policy (by significant margin, e.g. improvement in computer Go from a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015.)

> At its base, **MCTS is a rollout algorithm** as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively **direct simulations toward more highly-rewarding trajectories**.

At its core, on high level the following is happening

- MCTS is executed after encountering **each new state** to select the agents action for that state; it is **executed again to select the action for the next state, and so on**.
- MCTS incrementally extends the tree by **adding nodes representing states that look promising** based on the results of the simulated trajectories
- Outside the tree and at the leaf nodes the rollout policy is used for action selections, but at the states inside the tree something better is possible. For these states we have value estimates for at least some of the actions, so we can **pick among them using an informed policy, called the tree policy, that balances exploration (e.g. using UCB)**.

<img src="rl_p2/image-20221118135457036.png" alt="image-20221118135457036" style="zoom: 50%;" />

In more details and to explain what the above means, consider the overall algorithm iteratively performing



1. MCTS continues executing these four steps, starting each time at the trees root node, until no more time is left
   1. **Selection**. Starting at the root node, a ==tree policy (e.g. UCB) based on the action values== attached to the edges of the tree traverses the tree to ==select a leaf node==
   2. **Expansion**. The tree is expanded **from the selected leaf node** by ==adding one or more child nodes== reached from the selected node via unexplored actions
   3. **Simulation**. From the selected node, or from one of its newly-added child nodes (if any), ==simulation of a complete episode== is run with actions selected by the ==rollout policy==. 
      - The result is a Monte Carlo trial with **actions selected first by the tree policy and beyond the tree by the rollout policy**.
   4. **Backup**. The ==return== generated by the simulated episode is ==backed up to update, or to initialize, the action values== attached to the edges of the tree traversed by the tree policy in this iteration of MCTS. **No values are saved for the states and actions visited by the rollout policy** beyond the tree, as shown by the little triangle branch being dotted and discarded. Simulation result is only used to **backup the existing tree**
      - basically update my estimates of how good each action is based on simulation (+ value estimate in AlphaGO)
2. Then, finally, an **action** from the root node (=**the current state** of the environment) is **selected** according to some mechanism that depends on the accumulated statistics in the tree; for example, e.g. largest action value accumulated by UCB
3. After the environment transitions to a **new state, MCTS is run again from the first step** (often starting with a tree containing any descendants of this node left over from the tree constructed by the previous execution of MCTS)

> What does the accumulated estimate $Q(s,a)$ at each action node correspond to? In rollout algorithms where $s$ is the current state and ==$\pi$ is the rollout policy==, averaging the returns of the simulated trajectories produces ==estimates of $q_\pi(s,a)$== for each action $a$. 
>
> Then the policy that selects an action in s that maximizes these estimates and thereafter follows $\pi$ (during simulation) is a good candidate for a policy that **improves over $\pi$.** The result is ==like one step of the policy-iteration algorithm of dynamic programming==
>
> - you will see in AlphaGO we can potentially do better to estimate $q_*(s,a)$ hence choose the optimal action instead of just improvement

---

*For example*: Let us be playing a game of GO. Let us say we have some tree already, and we would like to use UCB based on action value to be our **tree policy** to select a node

1. Selection:

   <img src="rl_p2/image-20221118145758200.png" alt="image-20221118145758200" style="zoom:33%;" />

   where here, we have already made 7 simulations/rollouts hence the top node stores 7.

   - the $2/3$ at black node (at second level) means it's **black's turn to act**, but after this white action **white has won $2/3$ times** in the simulation

   - if we are computing the UCB of each node for instance at the second level:
     $$
     \mathrm{UCB}(2/3) =\frac{2}{3} + \sqrt{2} \sqrt{\frac{\ln 7}{3}}
     $$
     for using $c=\sqrt{2}$.

2. Expansion: we have selected a node and we want to explore some actions by adding a child node:

   <img src="rl_p2/image-20221118150319119.png" alt="image-20221118150319119" style="zoom:33%;" />

3. Simulation: we simulate from the **newly added node** using a rollout policy (e.g. a *fast* agent trained supervised from online GO games)

   <img src="rl_p2/image-20221118150456863.png" alt="image-20221118150456863" style="zoom:33%;" />

4. Back-Propagation: we **discard the simulation trajectory** but utilize the results to update our existing tree

   <img src="rl_p2/image-20221118150603388.png" alt="image-20221118150603388" style="zoom:33%;" />

   notice that all we need to update is the ==branch==. (Recall that the white $1/1$ means its white's turn but black has won $1/1$, hence the previous black node becomes $1/2$ as there are now in total $1+1=2$ simulations done and white lost one of them) 

But of course, no algorithm is yet perfect:

| Pros                                                         | Cons                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Tree growth focuses on more promising areas                  | Memory intensive: entire tree in memory                      |
| Can stop algorithm anytime to get search results             | For complex problems, enhancement needed for good performance |
| Avoid the problem of globally approximating an action-value function (as in DP) |                                                              |
| Convergence to minimax solution                              |                                                              |
| **High controllability = can insert your prior domain knowledge into tree policy, for instance** |                                                              |

### AlphaGO

The famous AlphaGO in essence is a MCTS algorithm, but **enhanced by several changes**, e.g. make the GO simulation computationally fast using value functions.

The overall pipeline of AlphaGO involves:

1. train a **supervised policy $p_\sigma$** cloning the plays of experts.

2. use **policy gradient** to train another agent $p_\rho$, which can consistently beats (more than 80% of the time) $p_\sigma$

3. obtain **value function $V_\rho$** from the strongest player $p_\rho$, hoping that $V_\rho \approx V_*$ of the optimal play

4. use MCTS to play by

   - tree policy is UCB

   - ==expansion is done using the prior from $p_\sigma$==, which is found to perform better than $p_\rho$ as the former is more diverse and human-like

   - ==backpropagation uses both simulation results and $V_\rho$==, so that the value for each leaf node is
     $$
     V(s_L) = (1-\lambda) v_\theta (s_L) + \lambda z_L
     $$
     and each edge/action is the mean evaluation of all simulations through that edge
     $$
     Q(s,a) = \frac{1}{N(s,a)}\sum_{i=1}^n \mathbb{1}(s,a,i)V(s_L^i)
     $$
     so that $\mathbb{1}(s,a,i)$ indicates if the edge $(s,a)$ was traversed during the $i$-th simulation.
     
   - at the end of simulation/MCTS, ==play policy== is to select action with most visited state:
     $$
     a = \arg\max_{a} N(s,a)
     $$

<img src="rl_p2/image-20221118152145169.png" alt="image-20221118152145169" style="zoom:40%;" />

more details see paper: https://www.nature.com/articles/nature16961

## Sample-Based Planning

Now we have covered planning if given model: for small spaces DP can be gone, but more often MCTS which selectively expands search space is more tractable. However, what if we don't know the model, i.e. model-free? Do we have to only use Q-learning/SARSA etc?

> If model is unknown, we can **first learn the model** and then **plan**.
>
> - once the model is fitted, we can do planning using DP, value iteration, policy iteration, MCTS, etc.
> - or, we can use the model to **generate *more* samples** to fit $Q$ functions

<img src="rl_p2/image-20221118152715124.png" alt="image-20221118152715124" style="zoom:33%;" />

Recall that for a model, we need to ==spit out next state and reward==:
$$
S_{t+1} \sim \mathcal{P}_\eta (S_{t+1}|S_t, A_t)\\
R_{t+1} \sim \mathcal{R}_\eta (S_{t+1}|S_t, A_t)\\
$$
for a model parameterized by $\eta$, and we typically can assume independence so that:
$$
\mathbb{P}(S_{t+1},R_{t+1}|S_t, A_t) = \mathbb{P}(S_{t+1}|S_t, A_t)\mathbb{P}(R_{t+1}|S_t, A_t)
$$
This then would be useful when learning a distribution model.

> Some models produce a description of all possibilities and their probabilities; these we call ==distribution models==. Other models produce just one of the possibilities, sampled according to the probabilities; these we call ==sample models==.
>
> - e.g. consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution.
>
> For algorithms such as Q-learning and SARSA, we just need a **sample model**. But for DP, we would need a **distribution model**.

It is typically easier to learn a sample model, which can be done using **supervised learning**

<img src="rl_p2/image-20221118092918058.png" alt="image-20221118092918058" style="zoom: 50%;" />

---

*For Example*: Small Table Lookup Example.

For small state spaces, you can learn a **distribution model** by simply counting:

<img src="rl_p2/image-20221118153701401.png" alt="image-20221118153701401" style="zoom: 45%;" />

So in the case of the following experience/data, you can easily fit a distribution model:

|                          Experience                          |                        Model Learned                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl_p2/image-20221118093255770.png" alt="image-20221118093255770" style="zoom: 33%;" /> | <img src="rl_p2/image-20221118093308625.png" alt="image-20221118093308625" style="zoom: 33%;" /> |

in this case we calculated the model without supervised learning but directly computing, but the idea is the same. 

Then, once we have a model, we can *sample* from it to do MC control, for instance:

<img src="rl_p2/image-20221118153938347.png" alt="image-20221118153938347" style="zoom: 40%;" />

Or of course, we can plan given a model directly by doing DP or MCTS.

## Integrating Learning and Planning

There is also an approach to try to do **learning and planning**: 

<img src="rl_p2/image-20221118154633165.png" alt="image-20221118154633165" style="zoom:33%;" />

> Within a planning agent, there are at least two roles for real experience: 
>
> - **model-learning:** it can be used to improve the model (to make it more accurately match the real environment) 
> - **direct RL:** it can be used to directly improve the value function and policy using the kinds of reinforcement learning methods we have discussed
>
> Note how experience can improve value functions and policies either directly or **indirectly via the model**. It is the latter, which is sometimes called ==indirect reinforcement learning==, that is involved in planning.

An example algorithm is Dyna-Q: which combines Q-learning and model learning as shown below

<img src="rl_p2/image-20221118095038634.png" alt="image-20221118095038634" style="zoom: 45%;" />

notice that step a)-d) is basically Q-learning, and step e)-f) further update your Q function using the model

- the intuition is that if you can explore more from your previous $(S,A)$, you could **make better decisions sooner** in the future when you met similar states, whereas just doing Q-learning makes that part slower

- For instance, as you can see here the more simulated experienced you gave to Q, it makes better decisions earlier and hence converge faster. i.e. **While walking around the world according to the experience**, I am changing dynamically/**planning at each step as well**

  <img src="rl_p2/image-20221118095318251.png" alt="image-20221118095318251" style="zoom: 40%;" />

  the idea of updating a bit sooner than until the end is like TD v.s. MC.

## Unified View of Planning and Learning

Just like how we Generalized Policy Iteration to contextualize model-free algorithms, the overall framework of planning and learning **also shares high level similarity**. The unified view we present in this chapter is that all state-space planning methods share a common structure

<img src="rl_p2/image-20221118155344361.png" alt="image-20221118155344361" style="zoom: 45%;" />

1. all state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and
2. they compute value functions by updates or backup operations applied to simulated experience.

# Deep Reinforcement Learning

Deep L can model any components in RL such as policy, model, value function, but Deep L + RL is unstable and hard to get convergence

Current approaches, modeling:

- value-based RL
  - approximating value function $Q^*$ or $V^*$ using a NN with weight $w$ or $\theta$
  - e.g. Q-learning, SARSA, MC methods, TD methods, etc.
- policy-based RL
  - search for and approximate the optimal policy function $\pi^*$
  - stochastic policy = policy model outputs a distribution of actions (e.g. by Softmax, or Gaussian distribution), and then you sample from it
  - deterministic policy = find a function that gives the action directly
- model-based RL
  - build a model (transition function and reward) from samples
  - when obtained a model, do planning

## DL for Value Functions

Here we will discuss on algorithm that has been successful and can be used for control (by simply taking $a = \arg\max_a Q(s,a)$)

### Deep Q-Network (DQN)

As we have mentioned before, this aims to model the Q network
$$
Q(s,a,w)\approx Q^\pi (s,a)
$$
and your target is simply based on the Q-learning target, which gives you a loss function:
$$
\mathcal{L}(w) = \mathbb{E}\left[ \left( \underbrace{r+\gamma \max_{a'} Q(s',a',w)}_{\text{target}} - Q(s,a,w)  \right)^2 \right]
$$
notice that you need to forward pass twice for $(s,a)$ and $(s',a')$. 

- Could there be some correlation there? Keep that in mind

- note that $a'$ will be computed by the 1) can model $Q(s,a) \to \mathbb{R}^{|A|}$ and then *index into the action $a$ to get the $Q$ value*. 2) then in this method, picking $\arg\max Q$ can be one **using only one forward pass**

then you perform gradient descent, which would look like the same descent in Q-learning
$$
\frac{\partial \mathcal{L}(w)}{\partial w} = \mathbb{E}\left[ \left( r+\gamma \max_{a'}Q(s',a',w)-Q(s,a,w) \right) \frac{\partial Q(s,a,w)}{\partial w} \right]
$$

> However, **naive Q-learning oscillates or diverges with neural nets**
>
> - data is sequential, hence there are correlations between successive samples
> - policy could change rapidly with slight change in $Q$-values
> - correlation between $Q$-value and target value during the loss computation
> - scale of rewards and $Q$-values could vary and hence gradient might be unstable

There are the core issues that DQN attempts to solve:

> DQN provides a stable solution to deep value-based RL
>
> - ==experience replay== to break correlation in *data/experience*
> - freeze ==target $Q$-network== (periodically, for example update only after 10k steps) to break correlations between $Q$-network and target, and avoid policy changes too rapidly
> - ==clip or normalize reward== so network can work adaptively to ranges of Q-values = robust gradients

Therefore the algorithm below

<img src="rl_p2/image-20221202090538120.png" alt="image-20221202090538120" style="zoom:40%;" />

## DL for Policy Functions

Here we aim to directly learn a policy, and we will cover methods such as

- Actor-critic methods: DDPG, A3C
- Optimization methods: TRPO, GPS

In general, policy gradient based methods are quite popular today, especially when continuous action spaces.

### Deterministic Policy Gradient

Recall that during stochastic policy approximation, we considered modeling the (real) policy:
$$
\pi_\theta(a|s) \to \mathbb{P}[a|s;\theta]
$$
where we could do this by taking a Softmax of the output, or modeling parameters of a Gaussian distribution for continuous action space. But here, we might consider a ==deterministic policy==:
$$
\mu_\theta(s)\to a
$$
Why does this matter? As $\mu_\theta$ is the network, and it **no longer models a probability** the stochastic policy gradient theorem we had before (using probability):
$$
\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \underbrace{\pi_\theta(a|s)}_{\text{prob.}} \right]
$$
**no longer makes sense** as using $\ln \mu_\theta(s)$ does not make sense any more. Therefore, people proved that there is a policy gradient for deterministic case:

<img src="rl_p2/image-20221202110405667.png" alt="image-20221202110405667" style="zoom:50%;" />

where the objective function in this case would be:
$$
J(\theta) = \mathbb{E}[r_1 + \gamma r_2 + \gamma^2 r_3 + ....] = \mathbb{E}_{\mu_\theta}[G]
$$
basically the expected return if we follow policy $\mu_\theta$

> But why use/prefer deterministic policy? Difference in practice?
>
> - when the variance in stochastic tends to zero, it **became the deterministic case**
>
> - stochastic needs gradient descent in $\mathbb{E} [\nabla \pi_\theta(s,a)*G_t ]$, meaning we for a single state we need action spaces combined for a good estimate v.s. deterministic case in practice **needs only to scan over a much smaller actions**
>   - therefore can outperform significantly **when action space is high dimensional**
>
> - stochastic encourages more exploration as in the end you are sampling from the distribution v.s. deterministic policy **might not explore other possibilities**. For instance
>   $$
>   a = \mu_\theta(s) + \text{Noise}
>   $$
>   so then all your exploration is the noise term

### Deep Deterministic Policy Gradient

The overall idea is basically **combining DL + DPG + Actor Critic Method** by:

- use actor-critic method to compute DPG gradient
- model your actor and critic using deep learning
- (solve instability issues with soft updates)

Therefore, first we consider modeling a policy/actor with DL:
$$
a = \pi(s,u)
$$
with weights $u$, and a critic network $Q(s,a,w)$ with weights $w$. Then the gradient updates are simply:

<img src="rl_p2/image-20221202093643427.png" alt="image-20221202093643427" style="zoom: 40%;" />

so in total here we have two networks, where the critic update is basically no $\max$ version of DQN updates, then use that to calculate gradient for actor

> Again, naive actor-critic method oscillates or diverges with neural network

Then in DDPG, we use 

- **experience replay** as in DQN for **both actor and critic**

- four networks where **soft target update** is used instead of freezing

<img src="rl_p2/image-20221202093702826.png" alt="image-20221202093702826" style="zoom:60%;" />

### Asynchronous Advantage Actor-Critic (A3C)

Again, building on top of actor-critic method but being

- **Asynchronous**: initiate multiple agents, each interact with the environment independently, and report update back to global network
- **Advantage**: using advantage function instead of $Q$ to update policy

Therefore overall it looks like:

<img src="rl_p2/image-20221202093940740.png" alt="image-20221202093940740" style="zoom: 40%;" />

So in a sense data correlation is removed automatically because **each agent has independent interactions** = no need to maintain an experience buffer.

The overall algorithm then looks like:

<img src="rl_p2/image-20221202094115629.png" alt="image-20221202094115629" style="zoom: 60%;" />

**Why A3C over other actor-critic methods?**

- naturally has **decoupled** correlation, v.s. in DQN and DDPG an experience buffer is used
- runs many agents in parallel to collect samples for updates, hence **fast** in computing/updates as well
- each agent can use **different exploration policy**, which can maximize diversity and further decorrelate
- can run on multi-core CPU threads on a single machine to reduce communication overhead

### Trust Region Policy Optimization (TRPO)

Basically a new form of loss function/objective, other than the simple ones such as $J(\theta)=\mathbb{E}_{\mu_\theta}[G]$

<img src="rl_p2/image-20221202102143093.png" alt="image-20221202102143093" style="zoom: 50%;" />

## DL for Model Functions

Now, we can also use a model to learn the world model, i.e. the transition functions
$$
p(r,s'|s,a)
$$
so that after this, we can:

- use it to collect more data and improve value function estimates
- do planning directly, such as MCTS

### AlphaGo Zero

> Recall that the first version [AlphaGO](#AlphaGO), we used
>
> - one network for roll-out policy, another for SL policy, both from **human data**
>
> - **self-play to improve policy network**: learn a RL policy network *and a value network* as well
> - **MCTS** based on the tree policy (RL network) and UCB
>   - expand using rollout policy and combine with value function
>   - pick action based on UCB

For AlphaGo Zero, the emphasis is to learn ==without human knowledge entirely==. The core idea is that it is solely based on reinforcement learning and MCTS to improve.

1. start with a **single model** producing both the move probability/policy and the value function
   $$
   f_\theta(s) = (p_\theta(a|s), V_\theta(s))
   $$

2. since the model does not care which player you are, it can self-play. But additionally, here we consider self-play using MCTS instead of just the policy $f_\theta$ because MCTS can usually result in selecting a much stronger move, hence an be treated as a **policy improvement operator**

3. Then, we have effectively gathered $(\pi,z)$, where $\pi$ is the MCTS probability and $z$ is the game winner. This game winner $z$ can be seen as a **policy evaluation operator**, which we can use to improve $V_\theta(s)$ estimate as well.

   <img src="rl_p2/image-20221207111925758.png" alt="image-20221207111925758" style="zoom: 40%;" />

4. but how exactly do we model $\pi$ probabilities in the MCTS? AlphaGo Zero models this to be:
   $$
   \pi_a \propto N(s,a)^{1/ \tau}
   $$
   so that the more popular a move is the higher the probability. Then how does it select those moves?

   <img src="rl_p2/image-20221207112607316.png" alt="image-20221207112607316" style="zoom: 40%;" />

   essentially it uses $f_\theta$ to help expand the tree so that each edge stores the $P(s,a),N(s,a),Q(s,a)$ where

   1. each simulation step ==selects== a move that maximizes upper confidence bound $Q(s,a)+U(s,a)$, where $U(s,a)\propto P(s,a)/(1+N(s,a))$ until a leaf node is encountered. This basic form is also used in Alpha Go
   2. the leaf node is then evaluated and ==expanded== only once using the prior probability and value $(P(s',\cdot), V(s') = f_\theta(s'))$. Notice no rollout here!
   3. each edge $(s,a)$ traversed is then ==updated== to increment its visit count $N(s,a)$ and its action value to be the mean evaluation $Q(s,a)=\sum V(s')/N(s,a)$.
   4. once done, a winner $z=\{+1,-1\}$ is stored, and the MCTS probability $\pi_a$ is computed

5. Finally, you have basically had a improved policy and value $(\pi,z)$, you **update your network** to match those parameters to achieve policy improvement and policy evaluation
   $$
   \mathcal{L}=(z-v_\theta)^2 - \pi^T \log p_\theta + c||\theta||^2
   $$
   where the last term is simply a regularization.

6. ==Note== that at the end of training/during actually play, you ==also perform the same MCTS procedure above== and select move based on the play policy $\pi$

> **Note that** some key difference with Alpha Go include
>
> - learn RL network + value network during **self-play from scratch** without human knowledge
>
> - modified MCTS to
>
>   - use value network **without roll-out**
>
>   - modified UCB $\mathrm{UCB}(s,a)=Q(s,a)+U(s,a)$, where $U(s,a)\propto P(s,a)/[1+N(s,a)]$
>
>   - final play policy $\pi$ becomes **stochastic** (is *not* the tree policy which is UCB)
>     $$
>     \pi(a|s) = \frac{N(s,a)^{1/\tau}}{\sum_b N(s,b)^{1/\tau}}
>     $$
>     which is then used to instruct the model $p_\theta$ to achieve policy improvement.
>

And just to re-iterate the key aspects in the paper (borrowing the author's words)

> The AlphaGo Zero selfplay algorithm can similarly be understood as an **approximate policy iteration** scheme in which MCTS is used for both policy improvement and policy evaluation. 
>
> - Policy improvement starts with a neural network policy, executes an MCTS based on that policys recommendations, and then projects the (much stronger) search policy back into the function space of the neural network. 
> - Policy evaluation is applied to the (much stronger) search policy: the outcomes of selfplay games are also projected back into the function space of the neural network. These projection steps are achieved by training the neural network parameters to match the search probabilities and selfplay game outcome respectively.

# Other RL Topics

There are many other topics of RL we haven't discussed, including:

- various other methods for **balancing exploration and exploitation**
- **Federated Reinforcement Learning**, e.g. using edge devices

- **Multi-agent Reinforcement Learning**. 
  - Recent breakthrough achieving human level performance in Stratego, "solving" incomplete information problem
  

## More on Exploration and Exploitation

Online decision has a fundamental trade-off between exploration and exploitation. Here we want to discuss various **different schemes** that you can use to balance such as trade-off

- e.g. MCTS contains search using tree policy = exploitation, but expands using **UCB**, which helps exploration

Some design principles for exploration include

- **Naive Exploration**. e.g. DDPG adding **noise** to a policy for exploration
- **Optimistic Initialization:** initialize $\hat{Q}$ to be larger than expected reward in the beginning
- **Optimistic in the face of uncertainty**: UCB as an example
  - compute uncertainty over time, and prefer actions with uncertain values for exploration
- **Probability Matching**: choose action according to some "model" you have about the reward distribution
- **Information state search**: lookahead search

> For many examples below, we will ==assume a Multi-arm Bandit== problem, meaning there will be no state for simplicity

### Optimistic Initialization

One very simple and practical idea to guarantee exploration is to **initialize $Q(a)$ to be high**(er than expected reward). So that:

- for actions that perform lower than that, we **automatically explore** other actions

In practice, we can do this by:

1. initialize high $Q$

2. Update action value by incremental MC evaluation
   $$
   \hat{Q}_t(a_t) = \hat{Q}_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat{Q}_{t-1})
   $$

However, this can **still lock onto sub-optimal actions over time**, as you will see later that both greedy/$\epsilon$-greedy + optimistic initialization will have ==linear== total regret.

### Regret

> Recall that regret aims to measure the **opportunity loss**, which can be measured by the difference between the optimal and your chosen action. 
> $$
> l_t = \mathbb [ V^* - Q(a_t)]
> $$
> then the total regret over time is
> $$
> L_t = \mathbb{E} \left[ \sum_{\tau=1}^t V^* - Q(a_\tau) \right]
> $$
> so minimizing regret = maximize cumulative reward

But regret can then be reformulated using the gap $\Delta_a = V^*-Q(a)$ for every action:
$$
\begin{align*}
L_t 
&= \mathbb{E} \left[ \sum_{\tau=1}^t V^* - Q(a_\tau) \right]\\
&= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)] (V^* - Q(a)) \\
&= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)] \Delta_a
\end{align*}
$$
So for a good algorithm, we want to 

- **have large $N$ for small $\Delta_a$, and vice versa** to minimize this (of course you need to get an accurate measure of $\Delta_a$ first)
- idea is so that you would want to choose that small $\Delta_a$ action over and over.

How does some simple approach perform in this metric? For multi-arm bandit:

<img src="rl_p2/image-20221209083335260.png" alt="image-20221209083335260" style="zoom:50%;" />

so that we would like to look for a way to get **sub-linear total regret**.

- if you never explore, your regret will increase lienearly. 

- for epsilon greedy, similar but slightly better

- sub-linear: We graduating decrease $\epsilon$ over time, having more exploitation over time.

  - decaying needs some decaying schedule. An example would be

    <img src="rl_p2/image-20221209083723566.png" alt="image-20221209083723566" style="zoom:33%;" />

    the longer time/steps we have sampled, the less exploration. Also if $d$ is big, meaning currently making **big mistakes**, we want to reduce exploration


But remember that $d$ or $\Delta_a$ is still hard to calculate in practice as we don't know $V^*$

### Upper Confidence Bound

> What is the lower bound on Multi-arm bandit in terms of regret? Maybe we can get an algorithm **if we know the lower bound?**

<img src="rl_p2/image-20221209084140380.png" alt="image-20221209084140380" style="zoom: 40%;" />

For instance, let us say we have three actions, and let us model $\hat{Q}(a)$ and we are **unsure of our estimate**:

<img src="rl_p2/image-20221209084355759.png" alt="image-20221209084355759" style="zoom:50%;" />

so that uncertainty for each action would be this breadth. The idea is ==the more uncertain we are about an action, the more important it is to explore it==

- action $a_1$ has the highest breadth/uncertainty, hence pick this first

- after picking blue, we would be less uncertain as this **distribution shifts**, and more likely to pick other actions

  <img src="rl_p2/image-20221209084614203.png" alt="image-20221209084614203" style="zoom:33%;" />

> Therefore, we are ==optimistic in the face of uncertainty== by hoping those highly uncertain actions to be potentially very beneficial

An example algorithm to do this is UCB

- $U_t(a)$, if you never tried an action $a$, then this will be very high/high uncertainty
- combine with current estimate $Q$ to combine exploration and exploitation $\hat{Q}_t(a) + \hat{U}_t(a)$

so we select action maximizing UCB
$$
a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a)
$$
But what is $\hat{U}_t$? It is motivated by the Hoeffding's Inequality

<img src="rl_p2/image-20221209085118183.png" alt="image-20221209085118183" style="zoom:40%;" />

which makes sense in the context of MAB because if we have large uncertainty, then this probability is small. Then we can use this to solve for a form of $U$:

<img src="rl_p2/image-20221209114450959.png" alt="image-20221209114450959" style="zoom:40%;" />

and we knw that this UCB1 achieves sublinear regret on on Multi-arm bandit

### Baysesian Bandits

So far made no assumptions about the reward distribution, and this methods basically **exploit prior knowledge of rewards** by specifying a prior distribution

> Have a prior to shape the reward distribution, and use **posterior** to guide exploration

So that we essentially estimate the posterior based on the (prior probability and the sample) you get:

<img src="rl_p2/image-20221209090030872.png" alt="image-20221209090030872" style="zoom:50%;" />

where $\alpha$ is number of times the arm gives you success, $\beta$ number of times if failed. Consider $\theta$ modeling reward we will get form this action. Consider a "success" being a reward $+1$ is given, failure gives $0$ reward

- in the begnning, no idea so $\alpha=\beta=1$ meaning the machine can give you any reward
- if we lost, then the average value drifts to the left
- in the end, you see the average reward is about $0.7$

Once done, now we have a distribution an action. We can repeat this to get a **distribution for each action**.

Then, how do we decide which action to pick? Since we are doing **probability matching**, we want take actions respecting those probability. For instance, say you have got three distribution with mean $0.1,0.7,1.0$

<img src="rl_p2/image-20221209115006625.png" alt="image-20221209115006625" style="zoom: 25%;" />

1. **sample** a reward from each curve, say $r_1=0.1,r_2=0.9,r_3=0.7$
2. choose an action according to the **biggest reward in the sample**. Hence action $a_2$

Other more well-fleshed idea is **Thompson Sampling**:

<img src="rl_p2/image-20221209115340909.png" alt="image-20221209115340909" style="zoom: 33%;" />

which also achieves the lower bound!

Intuitively, this way you can **naturally achieve exploration**.

> This is also ==optimistic in the face of uncertainty==, because those uncertain actions could have higher probability being $\max$ during sampling

## Federated Reinforcement Learning

> Federated reinforcement learning is a type of reinforcement learning in which multiple agents or agents **across multiple devices** learn to solve a common problem while maintaining the privacy and autonomy of their local data. 

This can be useful in situations where the data that is used to train the learning algorithm is distributed across multiple devices or agents, and where it is not feasible or desirable to centralize the data. (somewhat similar to A3C idea)

<img src="rl_p2/image-20221209092706109.png" alt="image-20221209092706109" style="zoom: 60%;" />

- **Advantages**: No need for centralized training data collection and centralized, data privacy, edge intelligence, learning quality
- **Disadvantages**: New threats and attacks to distributed participants (e.g., **backdoor attack**), **communications overhead**, varying data quality, selection of FL participant
- to some extent this is similar to A3C 

Another approach is learning on the edge:

<img src="rl_p2/image-20221209093137300.png" alt="image-20221209093137300" style="zoom: 33%;" />

Or you could also have a fully distributed workflow

<img src="rl_p2/image-20221209115720811.png" alt="image-20221209115720811" style="zoom:33%;" />

## Multi-Agent Reinforcement Learning

Categories in MARL:

- agents are **cooperative**
- agents are **competitive**
- **hybrid**, intra-group cooperation but inter-group competitive

Architectures can look like:

<img src="rl_p2/image-20221209115820150.png" alt="image-20221209115820150" style="zoom:50%;" />

An example success if DeepNash, where the there is **incomplete information** in the game as well

<img src="rl_p2/image-20221209093550846.png" alt="image-20221209093550846" style="zoom: 33%;" />

> From the paper abstract:
>
> - We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level.
> - Stratego requires long- term strategic thinking as in chess, but it also requires dealing with **imperfect information** as in poker. 
> - The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, **without search**, that learns to master Stratego through **self-play from scratch**. 

# RL Part 2 Review

- VFA: estimate $Q \gets \hat{Q}$
  - linear approach doing $\hat{Q}=x(s,a)W$
  - define loss function using MSE
  - gradient has the form $\mathbb{E}[2(\cdot ) \nabla x]$
    - if using Q-learning, then the omitted part will be $r+\max_a Q(s',a')-Q(s,a)$, etc
- Policy Gradient: score function that the gradient of the loss for improving policy is $Q\cdot\nabla \ln \pi / \mu$
  - probably a question on calculating this
- Planning and Learning: planning and learning loop, MCTS
- DRL: differences between DPG and PG, the latter being stochastic policy. More efficient since the action space don't need to integrated over
- This chapter: trade off between exploration and exploitation, and different methods]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[RL packages]]></summary></entry><entry><title type="html">ASCE1359 Intro to China</title><link href="/lectures/2022@columbia/ASCE1359_Intro_to_China.html/" rel="alternate" type="text/html" title="ASCE1359 Intro to China" /><published>2022-12-19T00:00:00-05:00</published><updated>2022-12-19T00:00:00-05:00</updated><id>/lectures/2022@columbia/ASCE1359_Intro_to_China</id><content type="html" xml:base="/lectures/2022@columbia/ASCE1359_Intro_to_China.html/"><![CDATA[Intro to East Asia: China

My timeline: https://time.graphics/line/691984

# Introduction

Order of Major Dynasty you should now:

- Republic of China

And that China has evidences of many pre-history humans living:

<img src="china_civ/image-20220908161314858.png" alt="image-20220908161314858" style="zoom: 33%;" />

# Neolithic Age (ca. 8000-2000 B.C.E)

Neolithic Age, , is discovered mostly because of the **distinct style of jar** unearthed (Neolithic Period (3300 - 2050BC):

<img src="china_civ/image-20220908161504480.png" alt="image-20220908161504480" style="zoom: 25%;" />

Additionally, features of Neolithic age except for jars include its

- **Agriculture**: the most important factor for what to plant is ==weather + water==

  <img src="china_civ/image-20220908161923132.png" alt="image-20220908161923132" style="zoom: 50%;" />

  note that already in the Neolithic age:

  - **rice** () grows in sunny + area with lots of irrigation. Hence near river
  - **wheat/millet** (to make noodles) grows in area that prefers a moderate level of irrigation. Hence more near the north
    - note that in the past (i.e. Neolithic age), the north of China has *not* been arid like now: its reasonably wet

- **Domestication of Animals** (e.g. dogs)

- **Fixed Human Settlements** (see next section)

## Major Neolithic Cultures

Around 5000 B.C.E, we have a brief map of major cultures:

| <img src="china_civ/image-20220908163052642.png" alt="image-20220908163052642" style="zoom: 33%;" /> | <img src="china_civ/image-20220908163117632.png" alt="image-20220908163117632" style="zoom: 33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |

where our focus would be the  and  culture, how they are related and different.

---

**Yangshao Culture** (5000-3000 BCE), who lives many on agriculture with meat supplies from hunting

- ==colorful==, painted pots

  | <img src="china_civ/image-20220908163446439.png" alt="image-20220908163446439" style="zoom: 25%;" /> | <img src="china_civ/image-20220908163450866.png" alt="image-20220908163450866" style="zoom: 25%;" /> | <img src="china_civ/image-20220908163454693.png" alt="image-20220908163454693" style="zoom: 25%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
  |                       3500-3000 B.C.E                        |                                                              |                                                              |

  with many emphasis on fish/animals and interesting shapes such as spirals.

- human settlement: ==Banpo==

  |                Banpo Model (c.a. 4000 B.C.E)                 |                         Second Bury                          |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220908163705259.png" alt="image-20220908163705259" style="zoom: 25%;" /> | <img src="china_civ/image-20220908163711564.png" alt="image-20220908163711564" style="zoom: 50%;" /> |

  where we notice that:

  - its various size and placement indicates a **lineage/kinship** relationship
  - there are no walls for the Banpo model. This could be indicative of them being **non-violent** and friendly people, no protection is needed from outsiders
  - already practices of **burial**: bury once to wait until decomposition, and bury again with others

---

**Longshan Culture** (3000 - 2000 B.C.E)

- ==black== potteries, distinct from Yangshuo (hence indication of a separate culture)

  <img src="china_civ/image-20220908164202703.png" alt="image-20220908164202703" style="zoom: 33%;" />

  created from rustic material, but has very sophisticated patterns while being extremely thin (0.2mm).

  - production of which surely requires high effort, hence indication of ==social stratification==/concentration of wealth
  - but it could also be a practice of certain religion, yet the previous would be more supported by other evidences

- ==Shimao== site, Longshan Settlement:

  <img src="china_civ/image-20220908164516923.png" alt="image-20220908164516923" style="zoom: 33%;" />

  note that we begin to see 

  - a structure of inner city and outer/taller city

  - presence of walls
  - has a lot of spear heads and other weapons

  indicating **social stratification** (as compared to Yangshuo) and **foreshadowing the bronze age**

---

Finally, note that it appears Longshan culture immediately proceeds Yangshuo culture, cultural interactions is always a non-linear process:

<img src="china_civ/image-20220908165024512.png" alt="image-20220908165024512" style="zoom: 33%;" />

where there are many other cultures being present, and theories explaining those in relation to the two major culture include:

- **nuclear area theory:** all starts from Yangshuo, and other cultural development radiated outwards
- **interactive sphere theory:** each culture has its own sphere, but they trade and interact, and eventually, converge.
- etc.

Without decisive evidences, it is hard to say what IS really the origin of all.

# Xia Dynasty (ca. 2100 - 1600 B.C.E)

> The Xia dynasty was the first of many ancient Chinese ruling houses, thought to exist from around 2070 B.C.E. until 1600 B.C.E. Yet **the actual existence of this dynasty and culture has been debated**.
>
> - in China, a lot of government funding has been provided to prove the existence of Xia dynasty. 
> - however, one should be careful as those evidences could be biased as well.

One major culture that are hypothesized to be related to Xia Dynasty and anyway preceeds to the Shang Dynasty is the ==Erlitou Culture== (c.a. 1750-1520 BCE), located in , 

<img src="china_civ/image-20220908165557551.png" alt="image-20220908165557551" style="zoom: 33%;" />

Some interesting aspect of the Erlitou Culture include:

- Erlitou's ==settlement== layout:

  |                        Erlitou Layout                        |                Central Palace Reconstruction                 |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220908165818245.png" alt="image-20220908165818245" style="zoom: 33%;" /> | <img src="china_civ/image-20220908165842753.png" alt="image-20220908165842753" style="zoom: 33%;" /> |

  where many scholars hypothesize that the central palace could be the **capital of Xia**

- Start of ==Bronze== vessels (proceeds Bronze Age) and some made of **Jade**

  |             Turquoise-inlaid Bronze Plaques ()             |               Turquoise Dragon and Bronze Bell               |                         Clay Vessels                         |                      Bronze ding Vessel                      |                           Yue Jade                           |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220908170113582](china_civ/image-20220908170113582.png) | ![image-20220908170118996](china_civ/image-20220908170118996.png) | ![image-20220908170123643](china_civ/image-20220908170123643.png) | ![image-20220908170129141](china_civ/image-20220908170129141.png) | ![image-20220908170135071](china_civ/image-20220908170135071.png) |

  all of which are highly likely to be produced by the workshop near the palace, found in large quantity in Erlitou. This could indicate that:

  - Erlingtou must be a central and powerful place, perhaps like that of a government
  - however, without much written records those are still speculations

- **Lack of written records**

  the possibility of Xia dynasty comes from  (), where he mentioned annals of Xia:

  |                         Shiji (91BC)                         |             Symbols Excavated from Near Erlitou              |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220908170705110.png" alt="image-20220908170705110" style="zoom: 33%;" /> | <img src="china_civ/image-20220908170725577.png" alt="image-20220908170725577" style="zoom: 25%;" /> |

  where we see some lack of written records for Xia, only left with some symbols on potteries, hence no way of confirming whether if Xia really existed.

> *According to tradition*, the Xia dynasty was established by the legendary **Yu the Great ()**, after Shun (), the last of the Five Emperors (), gave the throne to him. In traditional historiography, the Xia was later succeeded by the Shang dynasty.
>
> - is regarded as the demi-gods/emperors who significantly improved peoples lives by teaching them how to use fire and etc.

# Shang Dynasty (ca. 1766/1600 - 1045 B.C.E)

Between Shang and Xia (if it existed), there is a piece of text from the Shiji  that is worth to know. Basically:

- Xie () of Yin () was the son of Jiandi (), who (Jiandi) is a women of the Song nomad tribe
- Xie was born because Jiandi became the second consort of the Emperor Ku. This happened because "Once, when Jiandi was bathing with two other women, a **dark bird () flew past and dropped an egg down to them**. Jiandi retrieved it and swallowed it whole. Accordingly, she became pregnant"
- When Xie grew up, he **assisted Yu** (considered the founding father of ==Xia==) in taming the great flood. Thereupon, Emperor Shun charged Xie with the following orders: The hundred clans do not cleave to one another and the five ranks are not in accord. Assume the office of Governor of Conduct and attentively spread the five teachings, whose essential lesson is broad tolerance. Then Shun bestowed upon Xie **a patrician estate in ==Shang== and the surname Zi**.

Note that  is often refered to as , which is the captial of the last period of Shang. Hence,  can often be referred to as ==Shang== as well.

> ==Note==: Most of the content below show be viewed together with summaries of the book chapter  [Ch.1 Beginnigns of the Written Record](#Ch.1 Beginnigns of the Written Record).

> Even though Shang is universally considered as actually existed today, it was not in the past, until recent excavations at Anyang and Sanxingdui.
>
> - basically Shang's existence was establiashed by the large quantity of text found on **oracle bones**.
> - a lot of doubts were there for Shang because it is portrayed during the Zhou that times before were "barbaric" and "uncultured"

**Discovery of Oracle Bones:**

|                       Location of Yin                        |                   Oracle Bone Pit, Anyang                    |                       Shang Ox Scapula                       |                        Shang Plastron                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="china_civ/image-20220913215118561.png" alt="image-20220913215118561" style="zoom: 25%;" /> | <img src="china_civ/image-20220913215121937.png" alt="image-20220913215121937" style="zoom: 25%;" /> | <img src="china_civ/image-20220913215241025.png" alt="image-20220913215241025" style="zoom:25%;" /> | <img src="china_civ/image-20220913215314497.png" alt="image-20220913215314497" style="zoom:25%;" /> |

where the scripts on the oracle bones are largely reflective of the **elite life at Shang**, since using them requires the Emperor

- the word  comes from this, because its sound "Puk!" mimics that of the cracking of oracle bones. Hence  has the meaning of divination now.

- an example of text from the oracle bone:

  <img src="china_civ/image-20220911155732457.png" alt="image-20220911155732457" style="zoom: 67%;" />

  however, we are not to conclude that women are having less more than men, as there are cases where giving birth to daughters are considered auspicious according to records.

**Warefare and Polity**: consider from the extracted text:

<img src="china_civ/image-20220913220045932.png" alt="image-20220913220045932" style="zoom: 33%;" />

where Di refers to , a Shang deity; suffix of "fang" means enemies. From here we also see the role of oracle bones to warfare. But what about polity? 

- Qiang people were being slaves and used for sacrifise. In many Shang's work large labor is needed. Qiang people might be the man power.
- Concentration of power and wealth was greatly at the elite. And even artisan (e.g. making potteries) are much better off than peasants.

**The role of Women: the case of Lady Hao:**

- it is unclear what normal women's life are like at Shang due to lack of records (all oracle bones with scripts are used by/for the Emperor)
- but what is known is that elite women (e.g. Lady Hao) and elite dead ancestors (e.g. Emperor's Grandmother) had influence and power
  - e.g. Lady Hao's tomb had great amount of lavish goods
  - Lady Hao during her time was "**hard as the bronze, soft as the jade**" as she was great both at war/fighting to extend Shang's border, but also raising the kids. She was also being well respected by the people.

**Shang's Piece-mold Technique**: for more details check out the book chapter  [Ch.1 Beginnigns of the Written Record](#Ch.1 Beginnigns of the Written Record).

- very pottery based, hence explains why artisans could have a better life
- bronze casting required much material and technique. Hence it is **mostly restricted to elites' use**. Most people use stone vessels as alternatives.

**Shang's Bronze: Ritual Purposes**

- the bronze used at Shang typically had intricate patterns featuring animals such as fish, snakes, dragons, etc. This is because they believed animals to be related to Shaman, and hence are an intermediary for connecting **human and the spirits**

- among the animal motifs, one dominates: tao-tie  (a beast with insatiable greed)

  <img src="china_civ/image-20220913221801597.png" alt="image-20220913221801597" style="zoom: 25%;" />

  until today we are not entirely sure why this is preferred/appear so often on the bronze vessels. Religious purposes? Artistic purpose?

**Masks of Sanxingdui**: made of bronze but appears completely different from that of Shang. It seems to require less technical intricacy, but also very different in look.

<img src="china_civ/image-20220913222138172.png" alt="image-20220913222138172" style="zoom: 25%;" />

- what could this be used for? We don't know either.
- it is found to the south west of China (), where people at the center () sees as uncultured.
- after examination, it appears to be techniques mixed from many different regions.

**Chinese Texts**: its feature of being pictographic and having a 1) radical and 2) phonetic component is discussed in the book chapter summary  [Ch.1 Beginnigns of the Written Record](#Ch.1 Beginnigns of the Written Record).

- but it is interesting to see the evolution from past texts to today

  <img src="china_civ/image-20220913222715903.png" alt="image-20220913222715903" style="zoom:33%;" />

**Summary**:

- Oracle bones: disc. by accident, tortoise plastrons and ox scapula used in ritual divination, contain archaic form of Chinese writing and corroborate later written historical accounts, tells the Shang political and religious system
- Ritual bronzes: primarily for ritual purposes, funerary/ancestral sacrifice, taotie
- Lady Haos tomb
- Sanxingdui: another Bronze civilization in China
- Bronze age: development of city-states that control territory; **Erlitou and Yin are dense urban centers** and centers of a regional economy, ruled by royal houses (connecting Xia and Shang)

# Western Zhou (c.a. 1046 - 771 B.C.E)

On a broader picture, this is the rough timeline of how Shang relates to Western Zhou.

- Zhou Kingdom, once tributary state to the Shang, deposes the Shang in 1045 BCE

<img src="china_civ/image-20220915195207991.png" alt="image-20220915195207991" style="zoom:50%;" />

But before diving into evidences and stories of Zhou, we need to take a note of the difference between how Western Historians see Chinese histories and how Chinese scholars investigates it:

- **Western**: "wie es eigentlich gewesen ist (how it really was"). They are interested in knowing what **exactly** has happened
- **Chinese**: moralistical interpretation of the past, want to use history as a moralistic lesson to descendants (in fact many histories ARE written that way)

**Records related to Zhou**: in general there are texts that are recorded by the Zhou people/people alive near the Zhou period, and the ones written much later

- Confucian Classics
  - Shangshu   contains alleged speeches of the early W. Zhou kings
  - Shijing   Liturgical hymns (W. Zhou) and folk songs (Spring & Autumn)
  - Zhouyi   Divination manual
- Later Histories
  - Shiji  by Sima Qian  (145  c. 86 BCE)  synthetic Chinese history of first century BCE
  - Bamboo Annals (zhushu jinian )  annalistic history of bamboo slips composed in 298 BCE and rediscovered in 279 C

**Bronze Vessel = Power**: though often used for ritual purposes (still), making exquisite vessels still takes time and money

<img src="china_civ/image-20220915200737872.png" alt="image-20220915200737872" style="zoom:33%;" />

- therefore, bronze vessels is still an **embodiment of power**, making them suitable for gifts when Kings awarded to lords
- mostly concentrated and controlled by the elites. This can also be inferred from the high uniformity of style across vessels = is regulated/controlled
- many inscribed text on the bronze vessels, which can be used to study Zhou's culture (but **whether if this is a faithful** representation of the history is debatable, as those vessels are used for ritual purposes)

**From Shang to Zhou**:

<img src="china_civ/image-20220915201853489.png" alt="image-20220915201853489" style="zoom:50%;" />

- **Battle of Muye** was a battle fought between the rebel Zhou state (King Wu of Zhou ) and the defending army King Zhou () of Shang, captured their capital Yin and ended the Shang dynasty.
- Zhou then settled their captical in Xi'an, near wastern side of the Wei river.

**Settlements of Western Zhou**:

|                         Settlements                          |                       Overall "Border"                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="china_civ/image-20220915202742768.png" alt="image-20220915202742768" style="zoom: 33%;" /> | <img src="china_civ/image-20220915202924427.png" alt="image-20220915202924427" style="zoom: 33%;" /> |

- as you can see, most settlements are to the west/north of the Wei river.
- ==Chengzhou== (modern-day Luoyang) constructed soon after the Zhou conquest and served as the administrative center of the Western Zhou state on the eastern plain
  - later when Western Zhou got defeated and had to move to the east, they went to Chengzhou, captial of ==Eastern Zhou==

**Western Zhou Kings and Shiqiang pan **

|                      Western Zhou Kings                      |                         Shiqiang Pan                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="china_civ/image-20220915203525367.png" alt="image-20220915203525367" style="zoom:50%;" /> | <img src="china_civ/image-20220915203534257.png" alt="image-20220915203534257" style="zoom:50%;" /> |

- Shiqiang Pan contains two pages of texts, seemed as the "first concious attempt in China to write history"

- on the Shiqiang pan, it contained, in order: a) extol of former kings b) extols of former ancestors c) dedication of the vessel

  - but there are attemps found that recorded texts are too "good"

  <img src="china_civ/image-20220915203753379.png" alt="image-20220915203753379" style="zoom:25%;" />

**Reducing Power in Late Westerm Zhou**:

- in Lai Pan, the structure became to extol ancestors first, but not the Kings

**Invasion of the Rong** (non-chinese people)

<img src="china_civ/image-20220915204207788.png" alt="image-20220915204207788" style="zoom:33%;" />

- along with Zhou, Zhou's territory including Qin state (, which later took over and start ==Qin Dynasty==) relocated towards their other captial Chengzhou (today Luoyang)

# Eastern Zhou (770 - 256 B.C.E)

Again, a brief outline of the periods:

<img src="china_civ/image-20220920131742743.png" alt="image-20220920131742743" style="zoom:50%;" />

even though Zhou dynasty is the longest in history, ==Zhou== Kings has **declining power starting/becomes a figure head** from 771 B.C.E.

- Qin King called him self Qin Emperor, hence imperial period

Geograohically, the two period looks like:

<img src="china_civ/image-20220920132124104.png" alt="image-20220920132124104" style="zoom:50%;" />

- Western Zhou: capital in area of modern day **Xian** 
- Eastern Zhou: capital in west sacked, moved east to **Luoyang** 

And then Easter Zhou is (long hence) divided into two halves:  and 

## Spring and Autumn Period (722 - 481 B.C.E)

At the Start of , the distribution of polities (resulting from declining Zhou King influence):

<img src="china_civ/image-20220920132300350.png" alt="image-20220920132300350" style="zoom:50%;" />

- decline of Zhou power, rise of vassal states
- early china composed of a large collection of different polities

Among the several vassal states, the major ones include ,,,:

| <img src="china_civ/image-20220920132542198.png" alt="image-20220920132542198" style="zoom: 40%;" /> | <img src="china_civ/image-20220920133003347.png" alt="image-20220920133003347" style="zoom: 33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |

- these polities are also the hegemons
  - hegemon: something (such as a political state) having dominant influence or authority over others
  - this means that those states can interefere with sucecesion, hold meeting inter-states, etc.
- one important hegemon is the ====, who was in "exile" for 19 years in Di, but came back and become the Duke for Jin, and reformed Jin's military power to become one of the hegemons
- another important hegemon is the ==Qin==, who were initially non-chinese/considered by people at the central plain as less cilivilized. They were adopted as "chinese" by donaintg troops to Zhou King
  - the Qin state **absorbed cultures from two of the Four Barbarians from the west and north**, which made the other warring states see their culture in low esteem
- overall, this period is very violent, due to inceases in tension between states, insecurity, and betrayal. Yet there are some courtesy for battles and benevolence (e.g. not wipe out the entire lineage)

Besides the central states , the **four important non-chinese neighbors** are important to mention:

<img src="china_civ/image-20220920133632653.png" alt="image-20220920133632653" style="zoom: 33%;" />

where mostly the boundaries are made for geographical terms, meaning their is still culture diversity/similarity between them

- e.g. Duke Wen of Jin born by a woman in *Rong*, went to *Di* during exile
- The Master said, The Yi and Di tribes, even with their rulers, are still inferior to the Chinese states without their rulers. Analects, 3.5

453 BC Jin fell apart, partition of ==Jin== between rival families into the three states of ==Han==, ==Zhao== and ==Wei==

- this event is also the **watershed between the Spring and Autumn and Warring States periods**, refers to the division of the State of Jin between

| <img src="china_civ/image-20220920134035087.png" alt="image-20220920134035087" style="zoom:33%;" /> | <img src="china_civ/image-20220920134420548.png" alt="image-20220920134420548" style="zoom:50%;" /> | <img src="china_civ/image-20220920134431303.png" alt="image-20220920134431303" style="zoom: 33%;" /> |
| ------------------------------------------------------------ | :----------------------------------------------------------: | :----------------------------------------------------------: |

Under this turmultous condition gives birth of thinkers: Confucious (ca.551479BCE), Kong zi, Master Kong

- born in *Lu*, full of internal division within. *Lu* later becomes very weak and is absorbed by Qi

- Confucious is perhaps the author of *The Spring and Autumn Period* and *the Odes*.
- wanted to be advisor, but never succeeded, hence became a private tutor and **earn a living teaching** (a lot are aristocrates learning some courtesy)
- want students to be actively learning
- always think about "why did China become like this? How can we restore the order?"
  - believe that this is because the Zhou's ritual and order is not properly practiced by people, hence this mess
  - hence also aspires one of the first rulers in Zhou, leading the people with benevolent and virtue
  - gives rise to the ==ru class==, being people who are specialized in practicing rituals


**Analects **: collection of conversations assembled after Confucious death

- analects means fragmentsrequires reader to connect fragments.  The Chinese title, Lun Yu, means assorted conversations.

- it is *not* an essay that follows an argument structure, but structured around some key terms (e.g. *li* )

- some **most important concepts** involve

  - *li* (ritual), some aspects of which this key concept is about includes:

    <img src="china_civ/image-20220920135228733.png" alt="image-20220920135228733" style="zoom:33%;" />

    - worship: how to make offertings to gods
    - rites: how to marry; how to mourn the dead, etc.
    - daily decorum: how to interact with one's lord, parents, partners, etc.
    - morally appropriate, proper, right.

  - *ren* (benevolence, humane, etc)

    <img src="china_civ/image-20220920135607372.png" alt="image-20220920135607372" style="zoom: 33%;" />

  - *junzi*: the gentleman/noble person is not a vessel

    <img src="china_civ/image-20220920135724904.png" alt="image-20220920135724904" style="zoom:33%;" />

    he believes a gentlemen is not a person of a specific capacity not limited to one capability. Instead, the gentleman attends to the entirety and integrity of his personhe tends to his de (moral force; magnitude of character)

    - redefined that nobility does not come with birth, but through learning

  - *social roles*: 

    <img src="china_civ/image-20220920135955877.png" alt="image-20220920135955877" style="zoom:33%;" />

    against social flux, even though his teaching could be a proponent for it. 

    - not the first time there is "some" contradiction within him
    - another one mentioned in the book is his claim of 

  - *morality and legality*

    <img src="china_civ/image-20220920140105597.png" alt="image-20220920140105597" style="zoom: 33%;" />

**How did tradition**, transmitted by classicists such as Confucius, **view the first Zhou kings**?

<img src="china_civ/image-20220920140523978.png" alt="image-20220920140523978" style="zoom:33%;" />

- Duke of Zhou consolidating the foundation Western Zhou, being a very capable leader
- seems him as a very **upright** person: succeeded power from young cousin (normally father-to-son succession) and promised to gave away when he is older. In the end he made his promise.
- Confucius is inspired by Duke of Zhou (paragon of *de* (virtue) and human excellence) to define his Dao/way of teaching
- Confucius thinks we should **return** to the period of order/patrician society. But rulers at this time rejected this idea as it is too dangerous to their own establishments power. Therefore, unfortunately during his period, his teaching remain a **minority** view

**Servicemen (shi ) class during the Zhou period**

- constant warefare in Spring and Autumn experience a ==decline in aristorcracy==, with infantry + crossbows being effective army = transformed the *shi* class
  - *shi* started as the nobility/aristocrats for warriors in Western Zhou
  - early Zhou (feudal order): mounted knights; special retainers. late Zhou (Warring States): need specialists, technicians in all kinds of fields (e.g. defense)
  - Therefore, there are many social fluxes, including warriors to advisors, ministers, etc, and enables man of humble origin to climb = ***shi* class ** becomes a mix of people from non-noble classes specialized at different things (using education)
  - after ==Qin== dynasty (imperial era), especially in the ==Song==, *shi* beame officials; scholars
  
- the changing definition of *Shi* shows a ==decline of nobility== (and is later critical for the emergence of ), that throughout periods it is **talent and knowledge people are seeking**.

## Warring States (453221 B.C.E.)

**Previously in **

- If Western Zhou was "golden age" portrayed by ==Confucian== (who wants to go back to those glory Zhou period) school, Eastern Zhou is a time of **political fragmentation, incessant warfare, and moral decline**
- Confucius portrays himself as a restorer of tradition, but could be viewed as reformer; his legacy is continually contested, reinterpreted by followers
  - yet remember that his school of thought remain a minority view during his period
- key Confucian ideas captured in fragments through the Analects, compiled by disciples (bamboo strips 3rd c./books 2nd c.); emphasizing ideas of ritual (*li*), benevolence (*ren*), filial piety (*xiao*), and the virtues of the gentleman *junzi*.

**Overall situation in Warring States**

<img src="china_civ/image-20220922131924653.png" alt="image-20220922131924653" style="zoom: 33%;" />

- disintegration of ==Zhou== order and inter-state competition & warfare
- states such as Chu and ==Qin== became very powerful (most smaller, weak states are already conquered)
  - "ally your self with distant enemies, and conquer near enemies"
- at the end of the period, we know that Qin conquered all other states

**Major changes in the Warring States**

- during , battles involve using chariots and training few elite warriors; but during , concentrates on **training large infantry**

  <img src="china_civ/image-20220922132256947.png" alt="image-20220922132256947" style="zoom:33%;" />

  and the importance of **strategies** (e.g. induce the enemy to destroy themselves). In *The Art of War*, a lot of idea is about how to "outsmart" your competitor without physically engaging, hence useful in many modern situations as well

- **usage of iron** both for agriculture and weapons

- need money to maintain large army

  - increase in the practice of **taxation**
  - argiculture and commercial development: the use of **money**

  <img src="china_civ/image-20220922132826183.png" alt="image-20220922132826183" style="zoom:33%;" />

  note that Qin used the round coin. Later we found different coins scatter everywhere, meaning existance of wide trading.

- dynamic market for **specializations**

  - lots of trading, city grows in size, population grows
  - noble class no longer hold monopoly on intelligence, much change in social hierarchy (e.g. advisor, the *shi* class)

**Features of *Shi*** 

- these specialists were itinerant, traveling from state to state for employment.
  - most attractive were methods of state-strengthening and governance.
  - emphasis on governance is a common feature of Chinese thought.
- **famous specialists** with pupils were referred to with the *zi* suffix after their surname, which means the Master of a teaching

**Different schools (jia ) of thought**

<img src="china_civ/image-20220922133324153.png" alt="image-20220922133324153" style="zoom:50%;" />

- not necessarily an actual institutional schoolinstead, think of a school as a taxonomic family whose members exhibit family **resemblance**.
- many ideas are shared across groups:
  - ==filial piety== not exclusive to Classicists or followers of Confucius.
  -  Daoists teach about the Way (==dao ==)the principles of the world and how to live in itbut all masters use this term, but in different ways.
- most concern with how to ==govern==

**Mozi: ****

- most followeres are commoners/artisans
- very influential during the period (perhaps because they are specialists in artisan = defense), but died out durint Qin
- : love for all, no special regard for parents, etc. **Against confucious**, who believe that family relationship is the most importatnt.
  - hence rejection of offensive wars; military engineering and technology
- additionally, ritual and music is for ruling class, and is fundementally *selfish* since you are mourning your own lineage. Hence Mozi is against it.
- Developed systematic rules of ==logic== to persuade people
  - deep commitment to reason: argue in a rational fashion
  - Why obsession of logic: let people of different origin understand and accept their ideas

**Mencius: *Human Nature is Good***

- namd the second sage after Confucious
  - regarded himself as a follower of Confucious
  - his book *Mencius* is later used for 
  - occupied high political power in Qi
- believes that "**Human Nature is Good**"
  - everyone has the potential to become a sage (developed their moral throgh long and hard study and reflection)
  - human are born with *benevolence, rightenousness, ritual, wisdom*. But you need to cultivate them through learning.
  - **believes in every man's potential to become a sage**
- against Mozi, as Mencius believes it is impossible to show same love to others, because human beings are inclined to love their own families more

**Laozi: *Dao*** (and Daodejing)

- Dao today refers to idea from *Daodejing*, and the book *Zhuangzi*

- many messages in Daodejing is debatable and mysterious

  - Dao cannot be described

    <img src="china_civ/image-20220922135121167.png" alt="image-20220922135121167" style="zoom: 33%;" />

  - but Dao is the way of nature

- *Confucious* Dao: the way for **human beings to behave** ethically and moral. *Laozi* and *Zhuangzi's* Dao: **the way how universe/nature works.**

  - Laozi/Zhuangzi believes that Confucious exaggerated the importance of human being. Others ranging from the useful and the useless in nature should also be considered as well
  - "human is a small part of the nature", and in the end achievement of certain goals cannot be done without nature

- therefore, emphasis on *wuwei* ====

  <img src="china_civ/image-20220922135532501.png" alt="image-20220922135532501" style="zoom:33%;" />

  - non-interference, and have no excessive desires
  - believes that persuing non-intereference will have positive impacts on the society

  for example, for government you should also not interfere with people too much

  <img src="china_civ/image-20220922135712574.png" alt="image-20220922135712574" style="zoom:33%;" />

  so the ruler should stop acting when the system is inplace.

- *relativity of value*

  | <img src="china_civ/image-20220922135857820.png" alt="image-20220922135857820" style="zoom:33%;" /> | <img src="china_civ/image-20220922140019729.png" alt="image-20220922140019729" style="zoom: 33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  - advantage of the weak, strong and weak can become each other

**Zhuangzi: ** (another Daoist)

- dreamt himself of butterfly, ==questions knowledge and beliefs== we taken as granted
- the narrative style of Zhuangzi is different from other philosophers. Other philosophers teach in an opaque way, but he does it with playfulness and examples
- Confucious portrayed as a proper learner, but Zhuangzi appears eccentric and unpredictable
- both Laozi and Zhuangzi see *Dao* as the nature they must accord with, but there are differences
  - Laozi discusses about governness, and protecting the weak
  - Zhuangzi is not interested in politics (recluse), celebrated ritual freedom. Searching for conception of life

**Different thoughts of Masters**: "when your parents die, how should they be mourned?"

- **Confucious**: three year mourning period

- **Mencius**: just do what *your heart believes* is correct. 

  <img src="china_civ/image-20220922141619545.png" alt="image-20220922141619545" style="zoom:33%;" />

- **Mozi**: a limited resource should be used to the right places/==frugality==

- **Zhuangzi**: life and death is part of the natural process, it just part of the natural changes. So it would be foolish to attach yourself to changing things.

  <img src="china_civ/image-20220922141924289.png" alt="image-20220922141924289" style="zoom:33%;" />

  i.e. you should not try too hard to change things.

**Legalist: ** (very ==pragmatic==)

- Han Feizi as an example of Legalist, a student of *Xunzi (Xunzi thinks human nature is evil, and coersion, strict teaching is needed*

  - has a high noble origin
  - the only ruler took his teaching is Qin, which also become the emperor and united China
  - but note that "the **Legalist teachings**  in Qin was so repelled, by Xunzi, but they proposed a radically new way of organizing the state and its subject that allows its adherets to ==unite the realm for the first time==". So while Xunzi believes in the important of education to make the society behave, later legalist believed in coersion/law/punishmnets.

- believes that people are naturally anti-social (e.g. killed girls to favor boys at birth)

  - evilness of human nature the root of trouble = political unrest
  - so **ruler must be always on guard** even for his advisors, officials, etc

- disagree that Confucious notion that government should be based on moral and ritual, and ==emphasized laws and praticality==

  - must be easy to follow
  - must be consistent
  - must be backed by substantial reward and heavy punishment

  people should be controlled by punishments and rewards

- different interpretation of *wuwei*

  - The ruler should **conceal** his motives and desires, for then the true characters of the ministers will be plain; otherwise the ministers will polish themselves accordingly
  - Let the ministers do the work: the ruler take the credit; ministers take the blame 
  - in a sense defend the position of power. note that Han Feizi is noble.

  essentially for a ruler, **law applied to everybody except the ruler**

- but note that the aim is ==not to punish people==, but to ==improve/correct people via laws/controls==

# Qin Dynasty (221 - 207 B.C.E.)

**Previously on the Warring States**

- political disunity, lots of wars $\to$ disversity of political thought

- many thinkers (Daoist, Han Feizi, etc.) have *different idea as compared to Confucious*

  - Followers of Confucius: *Mencius* (Meng Zi) and *Xun Zi* systematize Confucian framework, elaborate on theories of governance and human nature (good/bad)
  - *Mozi* (the Mohists) known for doctrine of universal compassion
  - *Laozi* and *Zhuangzi*contemplative recluse, philosophical relativistpresents counterpoint to classic Confucian ideas
  - *Han Feizi* the rule of law, all men treated equally except the ruler
    - Diversity of philosophical approaches during Warring States is unsurpassed in Ch. hist; some schools--Legalism, Daoism--peak later; others (*Mohists*) disappear

- Repeated **cycle of union and division**, which is a basic paradigm in chinese history

  <img src="china_civ/image-20220927132049739.png" alt="image-20220927132049739" style="zoom:33%;" />

**The rise of the Qin state**

- Qin became the main power during the Warring period, but were seen as a barbaric state by other states

  <img src="china_civ/image-20220927132433325.png" alt="image-20220927132433325" style="zoom:33%;" />

- however, because this is a place easy to defend as it is on the *outskirt*, and it is easy to launch attacks

  - can weaken power of nobility, later on realized by Lord Shangyang $\to$ perform **talent recruitment**
  - western troops interaction/contact made Qin advanced in **military technology and capability**
    - later on Qin developed a system of non-hereditary ranks, where one can rise in rank purely from achievement during war
    - Qin king maintained a centralized control over all people, especially the troops

  all of which is only possible because the power of nobility is weak/no hereditary hierarchy, and contributed to the rise of Qin

- the entire conquest of Qin on other states happened during 230 - 221 BC, which is **very quick**!

**Qins foreigner minister: Lord Shang (Shang Yang)**

- in 361BC he become the **chief minister of Qin**, and wanted to make Qin a more powerful state $\to$ **Legalist State**
- changes he introduced include
  - suppressed the hereditary nobility; ranks achieved by military merit; established bureaucracy
  - country divided into county + officials, state penetration into the villages
    - i.e. local farmers no longer controlled the local nobility, but by the government
  - supervision between each other in a family unit
  - distribution of land by offering to farmers, but **land still belongs to the country** = lending ownership temporarily
  - direct taxation from people ot the state, instead of given to local nobility
  - shrunk the family into atomic family units $\to$ can tax more people; $\to$ prevent large group which could start rebellion
  - laws applied equally except for the emperor
- overall employed harsh rules, which effectively made **Qin a legalist state and laid a foundation of Qin's sucess**. But had a tragic death due to harsh rules as well.

**King Zheng (247-221BC)**

<img src="china_civ/image-20220927134150999.png" alt="image-20220927134150999" style="zoom:33%;" />

- First ==Emperor== of Unified China, King Yin Zheng. Named himself Emperor instead of King signifies his power presiding over the entire China

  - some viewed him as a passive beneficiary of smart officials and weak other states, hence sucess of reunification

  - but it could also be that King Yin Zheng is a controlling figure: failure of assassination prior to the unifcation of China

    <img src="china_civ/image-20220927134402347.png" alt="image-20220927134402347" style="zoom:33%;" />

    where in the figure, the person on the left that has a dagger is the assassin. The assassin Jin Ke in from the **Yan state** to ==fake for a peace treaty== hence surrender, but he failed and King Zheng became infuriated and speeded up the conquer later on. This might reflect King Zheng of being a powerful figure/driving force of the country as:

    - usually if ministers are the smart/driving forces, then the simples way is to bribe ministers/assassinate ministers
    - but assassinating King Zheng means he is the driving force, hence he could be powerful

- rised into power when he is age of 13

**Qin Dynasty**: lasted for only 14 years but impact is long-lasting

- established the political and law structure until the end of imperial era in 1911

- notice the difference betweem ==Fengjian and Central== system

  <img src="china_civ/image-20220927135211385.png" alt="image-20220927135211385" style="zoom:33%;" />

  notice that now the Emperor almost had a direct control over the people.

  - the Fengjian has the structure of dispersing sons/relatives with lands $\to$ let them control diverse pieces
  - While this was designed to maintain Zhou authority as it expanded its rule over a larger amount of territory, many of these became major states when the dynasty weakened.
  - Li Si opposed it, and believed the Central Bureaucracy by Shang Yang is better, as it could **prevent the dispersion of power/rebels**

- Later Li Si's plan is favored, and the following organization of the territory is used

  <img src="china_civ/image-20220927135459829.png" alt="image-20220927135459829" style="zoom:33%;" />

  now the emperor has the peak power and **everybody in this hierarchy has to listen to him**

  - the entire bureucracy is funded by the universal agricultural taxation system

  - at the same time, weaken wealthy clans/party, so no one can influence King's power. e.g. sometimes taken extreme measures such as *reallocated all powerful clans/local nobility to captical*
  - all walls for military defense is abolished, and melt down all weapons (so no more civil war)

**Economic Standardization in Qin**

- erased diversity by **unifying weights and measures**

  - produce in markets can be compared and priced equivalently
  - weights measured by weigh

  <img src="china_civ/image-20220927140044752.png" alt="image-20220927140044752" style="zoom:33%;" />

- another important standardization is **currency**: faciliates trading

  |                     Other Curreny to Qin                     |                        Qin's Currency                        |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220927140136547.png" alt="image-20220927140136547" style="zoom:33%;" /> | <img src="china_civ/image-20220927140220842.png" alt="image-20220927140220842" style="zoom:33%;" /> |

- unification of **script**. For instance the word for "horse"

  <img src="china_civ/image-20220927140308102.png" alt="image-20220927140308102" style="zoom:33%;" />

  which is advantageous for **communications** between each other/different states

  - done by Li Si, by rationalizing the Zhou script
  - symbolizes the ==cultural unity== that Qin intended

- more than standardization: ==knowledge control==

  - burying of scholars alive (uncertain): Confusious preferred Zhou's structure, but Li Si is the chancelor and is totally against it
    - Confucious hated Qin, hence people afterwards, e.g. in Han dynasty, who are followers of Confucious could be biased
  - buring of books (confirmed, 213BC): burned all Confucian classics, and whoever buys it will be executed (view this as an enemy of Qin's progress). 

  happens because Qin wants people to forget about Zhou/past, and corroberates current policies. But it does not mean to abandon all values from the past as well, e.g. lack of filiaty is still punished

- **legal standardization**: standarized rules for all (except ruler)

  - later Han dynasty described Qin rules being very harsh

  - but the truth is complicated: low ranking offical Xi's tomb in  found

    <img src="china_civ/image-20220927141158609.png" alt="image-20220927141158609" style="zoom:33%;" />

    Qin books on laws, which is **not that harsh but rather detailed and meticulous**

    |                     Logics on Punishment                     |                        Women v.s. Men                        |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |
    | <img src="china_civ/image-20220927141404034.png" alt="image-20220927141404034" style="zoom:33%;" /> | <img src="china_civ/image-20220927141535014.png" alt="image-20220927141535014" style="zoom:33%;" /> |

    additionally, we also see some **much more power of man than wives**

**Qin Transportation System:** e.g. road network (4000+ miles)

- strength economic ties between regions

- wall built during Ming Dynasty, but some foundation was laid out during Qin

  - as expression for shielding off enemies, like in Longshan period
  - wanted to **wall out non-chinese tribes to north (Xiongnu)** and wests
  - Qin recognizes its limits of power, by delineating is boundary *clearly* for the first time

  |                  Existing Walls during Qin                   |                   Great Wall we see today                    |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220927141911239.png" alt="image-20220927141911239" style="zoom:33%;" /> | <img src="china_civ/image-20220927141955179.png" alt="image-20220927141955179" style="zoom:33%;" /> |

  where essentially Qin is linking the existing walls together. Note that during this period, Qin assigned a **military official** because

  - those walls are on the outskirts, hence skirmishes would happen a lot
  - need to managed a large group of people, which is a skill more common within military

**Qin's Death: Terra-Cotta Army**

<img src="china_civ/image-20220927142246854.png" alt="image-20220927142246854" style="zoom:33%;" />

- used for Qin emperor to fight during the underworld (belief in afterlife, that military he defeated will prusue him in afterlife)

- soldiers were originally colorful

# Han Dynasty (202BC - 220AD)

**Previous on Qin Dynasty**

- 230-221 BCE: ==Qin== conquest ends the ==Warring States period==, results in unification
- 221-210 BCE: King Ying Zheng of ==Qin== proclaims himself emperor Huangdi, reigns as first emperor (Qin Shihuang)
  - **Liu Bang** (Emperor Gaozu): the founder of the Han
  - Qin state began as a politically and culturally marginal power, which is strengthened by **Shang Yang's reform**
  - then Han is corroberates by **Emperor Wu (Wudi)**
  - Shang Yangs reforms majorly included bureaucratic government, merit-based appointment, and administrative and tax reformsdirect relation to ruled
  - **Li Si,** after the conquest, builds upon Shang Yangs model, extends bureaucratic framework to standardization of weights and measures, currency, script, thought control
- **Laws of Qin** exresses legalist principles, but seen as were harsh by later people
  - portrayed as repressive and tyrannical, but recent scholars are rejecting prejudices and acknowledges its centralizing measures and its legal codes far from arbitrary

**The Collapse of the Qin Empire**

- In brief: 221BC Qin unification. 210BC Shi Huangdi died; his unpopular son, Huhai, succeeded the throne. 209BC Rebellions broke out. 207BC Qin ended by Liu Bang and Xiang Yu, both from ==Chu state== (Liu Bang won at the very end).

- Qin suffered from strong resistance on the conquered people (tax, lots of construction work), because the **restructure/unification of China within a short period** is bound to invoke dissent

  - Especially ==Chu== state had grudges, since Qin once broke alliance with Chu and masaacre a lot of people in Chu 

- Rebellions broke out. Some key rebellion leaders (both from Chu) include:

  |                Xiang Yu's Military Capabilty                 |                 Liu Bang's Social Capability                 |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20220929132146778.png" alt="image-20220929132146778" style="zoom:33%;" /> | <img src="china_civ/image-20220929132155103.png" alt="image-20220929132155103" style="zoom:33%;" /> |

  and the interesting is they have exactly different personalities and abilities

- Liu Bang won at the very end, and started ==Han dynasty==, named himself **Han Gaozu**

- Han is also divided into **two periods**, almost identical to the shift for Western Zhou and Easter Zhou

  <img src="china_civ/image-20220929132518552.png" alt="image-20220929132518552" style="zoom:33%;" />

  essentially almost identical to Zhou, and we **start with Western Han**, but later moved captial to Luoyang, hence **Eastern Han**

**Liu Bang's New Empire**

- Liu Bang's first decision was whether to keep the bureaucray of Qin or start a new one (most people associated centralization of power with tyrannay at this point, due to Qin dynasty)

- in the end, he retained most of the structure, central bureucracy, still uses commanderies, counties, appointed officials by talent, but on **had a benign implementation** (rules are more flexible) 

- decided to share power with his relatives, starting vassal states.

  - hence only western portion, about 1/3 of the entire territory is under direct control
  - easter ones are controlled by his relatives (so when he died, family still has control)

  <img src="china_civ/image-20220929132736301.png" alt="image-20220929132736301" style="zoom:33%;" />

- also didn't like Confucious, so prohibitted Confucious didactic books

- so in general, most policties are aligned with Lord Shang, with few changes:

  - relaxed the laws of Qin a bit, by relaxing the implemenation. e.g. you can plead to the judge
  - Liu Bang hated Confucious but judges of crime valued filial piety. This is no direct contraversy because the idea of filial piety is in fact common on lots of idealities, including legalism (e.g. violating filial piety will be punished)
  - but thinks Confucious scholar is useful

**Emperor Wudi** (Liu Che)

- ==strongest of all Han emperoros==: lots of achievements during his reign, especially his expansion of China
- Wudi is a **posthumouns title** due to his achievements in military, and china's territoy expansion
- had much control over vassal states
- made Confucianism state ideoligy

**Emperor Wudi: reshaping economy and politics**

- centralizing the government
  - instead of forcibly remove the large estate landowners, decreed lord's land shared with heirs (so that in the long run Emperor wins)
  - hence smaller control, and Wudi have more power
  - periodically, rich and influential family relocated to the captial to be under supervision (similar to Qin)
- significant revenues come from **state monopolies**
  - intent was to not overburden farmers
  - hence increased **taxes on private businesses**
  - monopolies on salt and iron, and later coinages as well
    - later debated if salt and iron monopoly is a good idea as they are critical for the people, but very profitable for the Emperor
    - in the long run, such monopoly disrupted the private sector of economy, and a political controversy: debate v.s. **Confucianism scholars** at that time on what is proper government

**Fight Against Xiongnu**

- Wudi puhsed out **Xiongnu** (problem since Shang, exacerbated by Zhou, e.g. making peace through gifts and peace marriage)

  <img src="china_civ/image-20220929134528429.png" alt="image-20220929134528429" style="zoom:33%;" />

  but at the same time, Xiongnu tribes fromed the first confederation

  - Xiongnu might have been ancestors of European Turkish

- also had campaigns and subdugate the various kingdoms, defeated Ferghana, got a lot of horses

  <img src="china_civ/image-20220929134933688.png" alt="image-20220929134933688" style="zoom:33%;" />

  and this campaigm **secured trade routes later to central asia**

- other campaigns aimed to make **more tribute states** hence allies to fight against Xiongnu

  <img src="china_civ/image-20220929135055258.png" alt="image-20220929135055258" style="zoom:33%;" />

  some famours campaign leaders:

  - **Huo Qubing:** defeated 5 Xiongnu tribes in 6 days, killed over 70000 Xiongnu. (but we didn't find Xiongnu's records to confirm this)
  - **Zhang Qian:** his route established later ==silk road==, and later where ==buddism== came in

- Consequences of Wudis expansions
  - Positive:
    - military security
    - established contact with the larger world - later on the silk road
    - ambassators of many tribute states $\to$ send princesses to China and returns with periodic gift to vassal states
  - Problem:
    - nearly 10% of state revenue spent on gifts
    - Stretched Chinas resources; economic well-being weakened

**Han Confucianism as State Orthodoxy**

- **Dong Zhongshu** developed Confucious that matched the mood of the time

  - **synthesizes** different ideas to Confucian metaphysics, e.g. adapted *yin-yang* and the *five forces* (fire, water, earth, metal, and wood) into Confucian metaphysics. Note that Confucious origianlly did not address supernatural
  - harmonize the contrasting idea of Mencius and Xunzi, so human's nature is good but man needs to be guided by the ruler (personification of heaven)
  - remade emperor-cnetered, hence Wudi liked it

  <img src="china_civ/image-20220929140431707.png" alt="image-20220929140431707" style="zoom:33%;" />

  - essentially, Confucianism became ==more eclectic and emperor-centered==

- he then made the proposal to Wudi, who liked his idea and adopted it

  - even though Wudi's established rule is in a way very similar to Qin's legalist principle on military and laws,
  - **ruled by legalist, but decorated/surfaced by Confucianism idea**
  - so practically, every state claimed that they follow confucianism even though rules by legalist ideality (due to Qin)

- another important change include that the government officials needed Confucianism classics as required knowledge

  - invenstion of paper in Easter Han helped expanded Confucianism
  - but the overall Confucianism has diverged some significant ways

**Emperor Lv** and **Ban Family**

<img src="china_civ/image-20220929140951519.png" alt="image-20220929140951519" style="zoom:33%;" />

- ==first woman ruler== (dowager) in imperial china, wife of Liu Bang
- power of empresses family threatened the power of emperor family, hence seen as dangerous
  - so historians such as Sima Qian cover her achievments up
- Another first woman historican: ==Ban Zhao==
  - completed the history of Western Han
  - wrote book on *Lessons for Women*, which is very popular
    - emphasized women obligations to husband, cultivation of virtues appropriate to women.
    - women and men have different status, hence can be seen as Confucious idea to stick to their role
    - but also advocated for equal education for women

**Sima Qian, the "historian"**

- our vision of ancient china is being shaped a lot from his view of it

- at that time, historian were more known as Archivist, Astrologer, and/or Scribe

  - duties may have included recording events, or keeping minutes.
  - could access palace archives, such as chronologies, confidential documents.

- *Shiji* started by his father, finished ==privately== by him (hence less biased v.s. Ban's version, which is state sponcered)

- *Shiji* narrates history from beginning to time to Sima Qian's time

  <img src="china_civ/image-20220929141604353.png" alt="image-20220929141604353" style="zoom:33%;" />

  where all the history was unearthed by him, checking archives, going to places physically, etc.

- however, there could still be biases:

  - during ruling of Han Wudi, he had a castration due to political mistake, hence could have grudge to Han
  - dislikes Emperess Lv
  - student of Dong ZHongshu, had Confucious influences

  but today seen as he tried his best to be impartial

**Wang Mang Interloper**:

- recall that Liu Bang, Empress Lv, and Empreor Wu are all classfied as early Han
- Wang Mang started a rebellion and separated out of the royal clan to start his ==Xin dynasty== (which lasted very shortly) besides ==Han==.
  - however, due to the flooding of yellow river at the time and other reasons, Wang Mang's plan is shortlived
- due to rebellion, Han people relocated to Luoyang, **restored the power**, had the ==Eastern Han==
  - Eastern Han formally began on 5 August AD 25, when Liu Xiu became Emperor Guangwu of Han, with **capital moved eastward to Luoyang**
  - still the **Liu** clan being the royalty, but the eunuchs has rose up in power

**End of Han**

- The end of the Han dynasty refers to the period of Chinese history from 189 to 220 CE, which roughly coincides with the tumultuous reign of the Han dynasty's last ruler, Emperor Xian. 
- During this period, the country was thrown into turmoil by the Yellow Turban Rebellion (184205). Meanwhile, the Han Empire's institutions were destroyed by the ==warlord Dong Zhuo== and ==fractured into regional regimes ruled by various warlords==, some of whom were **nobles and officials of the Han imperial court**. 
- One of those warlords, Cao Cao, was gradually reunifying the empire, ostensibly under Emperor Xian's rule

# Six Dynasties (220 - 589)

**Previously on Han Dynasty**

<img src="china_civ/image-20221004131407632.png" alt="image-20221004131407632" style="zoom: 50%;" />

- **Liu Bang**, or Han Gaozu, founded the Han dynasty

- ==Han== Dynasty is split into two halves, distinguished by their capitals: 
  - 206BC - 9 CE: Western/Former Han (Changan). Then problem incurred by ==Wang Mang== and forced the move of the royal family
  - 25-220 CE: Eastern Han/Later Han (Luoyang)

- under the reign of **Emperor Wu**: we had great territorial expansion, but weakened the economy of empire. Several milestones include:
  - **Han-Xiongnu War**: Han expansion creates a multilingual, multiethnic empire, enduring political and cultural ties to inner Asia; foundation of silk road and exchange of goods/ideas; but weaken the local economy due to warefare
    - note that silk road in reality are a series of *short trades*, and that it is important to know that many tradings existed *before silk road* as well. It is just that Wudi made *silk road even more trade intensive*.
  - **Dong Zhongshu** works to install Confucianism as official state ideology, but draws upon Daoist cosmology; this syncretic Confucianism legitimizes Han order
  - **Sima Qian**, court astrologer to E. Wu, writes monumental Records of the Grand Historian in the Confucian mold, considered founder of Chinese historiography
  - **Salt & Iron** monopoly debates (held 81BC) on Emperor Wus economic policies
- ==Han== governance marks continuation of Qin law and institutions, ==legalist foundation with new syncretic Confucianist state ideology laid over it==. => Called the Han Synthesis, but the Han is essentially a Legalist state.

- also recall that in the song, this missing period is essentially the topic of this section: the ==Six Dynasties==

  <img src="china_civ/image-20221004131733439.png" alt="image-20221004131733439" style="zoom: 33%;" />

**Timeline After Han**

<img src="china_civ/image-20221004131810873.png" alt="image-20221004131810873" style="zoom:50%;" />

- first split into Wu, Wei, Shu, the three kingdom period
- then north have been controlled by alien rulers, e.g. Northern Wei. The southern are controlled by aristocrats migrated from north (e.g. to ==seek refuge==)
  - recall that south had high humidity = lots of malaria
  - **oldest form of spoken Chinese in the South**, such as Fujian, because south had been "stale" while north was changing, e.g. interaction with steppe people.

- the name "Six dynasties" come from the six dynasty that had capitals in ==Nanjing== (Eastern Wu, Eastern Jin, Song, Qi, Liang, Chen)
  - it also overlapped the ==Sixteen Kingdoms== period = warring in Northern China after fall of Western Jin
  - the period of Northern Wei + Southern Song to before Sui is also called the ==Northern and Southern period== 
- Sui dynasty reunified china

**The Three Kingdoms**

<img src="china_civ/image-20221004132335086.png" alt="image-20221004132335086" style="zoom:33%;" />

- the three rulers slowly rose to power at the end of Han, by being warlords
- by 205, Cao Cao had made themselves dictator of North China. Since there are so many Wei, we added the surname of the leader to give Cao Wei
- later on **Cao Cao** also defated **Xiongnu**, but never able to reunite the other empires, which ended with the **Battle of the Red Cliff **
  - defeated by alliance between Shu and Sun, and the tiredness of his troops due to long distance marching
  - he didn't die there, but just failed to reunite
- but **eventually his Wei prevailed** as it has twice the population, and won over the other state
- near the last of Wei's heirs, Cao Shuang was executed and royal power weakened = rise and eclipsed by the **Sima family**. 
  - They eventually took over change the dynasty to ==Jin ==
  - first time a small reunion of China, but fell apart quickly by the **War of Eight Princes** (i.e. family member seeking power)
  - subsequently, in 304, civil war within and northern invasion of the ==non-Chinese tribes== collapsed
  - Therefore, the north became the ==Sixteen Kingdoms period== and the south is the ==Eastern Jin dynasty==

**Sixteen Kingdoms and Eastern Jin**

- Non-chinese joined the conflicts and made north a battle ground.

- life in the north was dangerous hence many people, especially Jin, packed up and fled south = ==Eastern Jin==

  <img src="china_civ/image-20221004133417147.png" alt="image-20221004133417147" style="zoom:50%;" />

  but since there are aboriginal people there:

  - the local populartion either pushed away or absorbed
  - essentially Eastern Jin is still under the Sima Family

- one reason why Jin so shorted lived is because its choice of Government officials. The rise of aristocracy from Han through Tang

  - ==Warring States==: on the basis of merit (*shi* class)
  - ==Qin==: merit, as a firm principle
  - ==Han==: by recommendation (rule of avoidance, so officials there are non-local) + exams
    - in reality, since officals are non-local, they basically asked people of importance **at that local area** for recommendation
    - but later this became a problem, that it still became local recommendation by locals themselves
  - ==Wei== (under Cao Cao): and the whole era of disunity: the **Nine-Grade** system
    - every family will be assigned a rank = position in government, hence hereditary in nature
    - this rank assignment depends in virtue, reputation, talent
    - **long survival of elite status**, and many of those clans persisted to the Tang dynasty
      - to preserve this, aristocrat should only marry aristocrat
  - ==Sui==: exams
    - realize that the exam is the fairest way

**Elite Aesthetics**: 

- in South China, the educated and the well-off **competed in extravagence**, instead of the virtue Confucious wanted

- these people are interested in mythical things, divination and supernatural (Seven Sages of the Bamboo Grove below)

  <img src="china_civ/image-20221004134535212.png" alt="image-20221004134535212" style="zoom:33%;" />

  for example, they loved drinking, and some also took drug.  Hence those elite people always try to be naked

  - challenged public moral at the time, of what is virtuely right
  - e.g. one was crying when an unmarried man died, but was eating and drinkg wine at his mother funeral = violated Confucious values

- but also a burst of self-expression in art and poetry

**Northwen Wei Dynasty**

- ==Tuoba==, which included tribes who spoke turkish, mongolish, etc. They invaded and controlled he nothern China after the Western Jin

  <img src="china_civ/image-20221004135051914.png" alt="image-20221004135051914" style="zoom:33%;" />

- however, Tuoba interested in the process of ==Sinicization== (to Sinify: to make Chinese. Latin Sinai=Qin=Chinese)

  - how can a minority population of Tuoba ($\approx$ 1 million) contorl the vast population of Chinese $\approx$ 20 million?
  - one approach is Sinicization, which is wierd but largely because **Emperor Xiaowen himself is admiring Chinese since childhood**
  - proposed a new system of Equi-field, which is reminiscent of Han to tax individual cultivators
    - individual family assigned land depending on how many people they have
    - this is to ensure that no-land is neglected, and that **no people wander off** (e.g. don't want merchant and nomads)
    - hence is useful for building a fiscal base
  - **Emperor Xiaowen:** new captial in Luoyang and wanted to ==transform Xianbei into Chinese==
    - used chinese surnames
    - ordered Chinese clothings in court
    - preferred Sinicized Chinese = elites of mixed ethnic backgrounds increasingly became the norm
      - Luoyang became popular, many Xianbei nobilities became versed in Chinese
      - The cultural and ethnic integration ==laid the foundation for later reunification of the North and South by the Sui==
    - however, this eventually lead to the problem that military who are mostly Xianbei are being ==treated unfairly== compared to mixed court officials. This can be said to tbe the root cause of the throw off for Emperor Xiaowen

- the **Ballad of Mulan**, when the Northern Wei (where Mulan is born) was in war with ruan-ruan

  <img src="china_civ/image-20221004135710984.png" alt="image-20221004135710984" style="zoom:33%;" />

  - in the Ode the concept of **Khan** is equivalented to the concept of **Son of Heaven**
  - indicates the sinicization of north

- Resistance of Sinicization, 

  - this acceptance of Chinese culture would lead of loss of identities, and Xianbei in military did not enjoy status that sincized Xianbei in the court had
  - since those unequal treatment made militaries unhappy, and lead to rebellion in 524

  <img src="china_civ/image-20221004135907298.png" alt="image-20221004135907298" style="zoom: 67%;" />

- but later they all suffered from sinified Chinese, that a mix of culture differences induced alot of conflict

**Buddhism**:

- provided a different way of conceving life, death, and cosmos

- Buddhism originated in India, a lot went to China from the trade routes

  <img src="china_civ/image-20221004140127938.png" alt="image-20221004140127938" style="zoom: 33%;" />

- instead of a single creed, chinese Buddhism foundd a lot of ideas and practices

  - met both ==Theravada== (early buddihism, lesser vehicle) and ==Mahayana== (later buddism, greater vehicle 'that can carry more people to salvation')
  - problem of translation then was critical

- since Buddhism is in sanskrit, early translators used Daosim concepts to explain Buddhism

  <img src="china_civ/image-20221004140458180.png" alt="image-20221004140458180" style="zoom: 33%;" />

  - in the beginning, had lots of problems such as Nirvana $\to$ Wuwei
  - then with ==Kumarajiva== and his translation project coining new terms in Chinese (e.g. Niepan, to preserve the foreign phoenic), it became much more accurate

- how did **Buddhism change Chinese culture at large**?

  - the idea of rebith, and people cycling through life is new to the Chinese
  - added the idea of **death is also suffering**, hence Nirvana is important
  - reshaped the ==relationship between the living and the ancestors==
    - ancestors become "weak" beings needing to be saved (v.s. before, ancestors are made offerings and then gave help in return)
    - how do you save them? Death can be lifted by merit, and ==merit can be transferred==, e.g. by ==Buddhist clergy==
    - remade the relationship between living and dead. Idea of ancestor are powerful has diminished
  - From the above, induced the rise of the **importance of Buddhist clergy**

- how did **China change Buddhism**?

  - Mahayna buddhism became the famous one, China magnifies Mahayana. e.g. extending the influence to Japan as well

    - e.g. Chan, or Zen in Japanese, all purely Chinese inventions

  - developed specific Buddhist schools, all Mayhayana sects

    - **Pure Land**: preach by faith, hence very easy to pratice. Only need to recite the name of Amitabha. Hence welcomed by commoners
    - **Zen buddism**: needs meditation, more attractive to literate elite

  - an example of locaization of Buddhism deity, Guan Yin of mercy and compassion.

    |                           Original                           |                           In China                           |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |
    | <img src="china_civ/image-20221004170004118.png" alt="image-20221004170004118" style="zoom:33%;" /> | <img src="china_civ/image-20221004141817604.png" alt="image-20221004141817604" style="zoom:25%;" /> |

    - initially Guan Yin is a male in India, but in China, became female
    - promoted the idea of **equal gender**: "presentation of gender is provisional, situational, audience-oriented"

**Rise of Daoism**:

- Though Han dynasty with popular with Confucious, there are still believers of Daoism and the notion of **immortality** (e.g. Lady Dai)
- during the Religious Daoism, those sporadic believers (e.g. farmers) suffered from taxations and wars. They ==wish for peaceful lives== (near the end of Han), hence grouped up and **formed a more powerful sect** and started to gain governence in their own regions = the Yellow Turban, etc.

**Religious Daoism**

- gained insights from Cihinese superstitions, and is for pursuing immortality (e.g. Qin and Emperor Wudi)

- but in 2nd century, there ermges religous Daoism: organized commual religous due to conflicts/dissent ==near end of Han== (notice that **something new happens near the end of a dynasty is pretty common by now**)

  <img src="china_civ/image-20221004142119564.png" alt="image-20221004142119564" style="zoom:50%;" />

  - Laozi is now made into a deity
  - had two sects, by Zhang Daoling (), and by Zhang Jiao (Yellow Turbans)
  - those leaders claim they obtained new revelations and gained a lot believers

- eventually the growth of Daosim and the incoming Buddhistm lead to strong competition

  <img src="china_civ/image-20221004170739423.png" alt="image-20221004170739423" style="zoom: 33%;" />

  - Daoist built temples v.s. Buddhist monasteris
  - Daoist claim that they have better spells, soceries for protection and ailness, etc.

- There have been much efforts of one religion try to use the emperor to **expel religion**: but never had a single religion to dominate

  - many western misconception therefore though Chinese religion had high tolerance, e.g. the exclusiveness of Islam and Christian
  - but that is not true, the reality is that are also trying to exclude but just never works out

# Sui and Tang (581907)

**Previously on the Age of Division**

- ==Eastern Han==: falls from socioeconomic and environmental strain, and religious (e.g. Yellow Turf), peasant movements. Fractures into short-lived, celebrated Three Kingdoms period.
- ==North-South== division remains significant; 
  - South underdeveloped, only has 1/6 of N.s population; 
  - North: had mostly **proto-Mongol or Turkic nomadic people** including 1. Xiongnu, 2. Xianbei, 3. Tuoba establish series of kingdoms which are usually brief; Northern kingdoms see a degree of ==Sinicization==, but reverse also occurs; ethnicity and culture are ==fluid==
  - Differences in geography, climate, trade routes; the rise of aristocracy
  - One of the legacies of the ==Northern Wei== (Tuoba branch of Xianbei) is patronage of Buddhism, which was translated through Daoism, (i.e. wuwei and nirvana)
  - ==Sui== had a series of measures trying to unite and resolve the difference:
    - a law code equally applied to all
    - the **Grand Canal**, which linked the Huang He and the Chang Rivers and encouraged north-south trade within China
- ==Buddhism== transforms culture; ==Chinese== culture transforms Buddhism
  - Religious Daoism: in competion with Buddhism, wanted to get exclusive patronage by the emperors but never work out

**Reunification of Sui**

- some key milestones of Sui and later

  <img src="china_civ/image-20221006132308299.png" alt="image-20221006132308299" style="zoom: 50%;" />

  - Reunification under the Sui (581-618) **Yang Jian **
  - Yang Guang, the first emperor's son when successed build the **Grand Canal** (try to keep China together) but also had extravagant rebuilding of capital at Luotang (critisized)
  - Yang Guang's campaign to Korea later was failed, and rebelliions broke out in China = emperor killed = ==Li Yuan founded Tang==
  - Early Tang: mainly ==Emperor Taizong (Li Shimin)== & empire-building+conquering  Tang Cosmopolitanism: Changan and material culture
  - ==Empress Wu (Zetian)==, who reigned for a long time, and the role of women
  - **An Lushan Rebellion** forced center to cede much power

- **rescined the ban** on Buddhism and Daoism passed by the previous Northern Zhou dynasty in 574

  - monks were already working on Buddhist text protection in Zhou: digging caves and putting Buddhist texts incised in stone

  - but Sui founder was himself a ==grand donor to Buddhism==, built a lot of monasteries
  
- mixed nordic and Chinese, a reunification of Chinese northern and southern states

  - north and southern could have became a permanent separation
  - new centralized bureucracy even stronger than in Han, and enforced it in a short periof of time (like Qin)

- later in Tang, the division seemed a much undesired situation

  - as shown by Tang, unification = longer dynasty e.g. **Han and Tang became an aim of every later ruler**

- since again, ==Sui== are doing a lot but too soon (alike Qin)

  <img src="china_civ/image-20221006132844907.png" alt="image-20221006132844907" style="zoom:33%;" />

  where

  - dig a canal to connect north and south, and to ship south resources to north
  - faciliated and **symbolizes the political reunification** of south and north

- new system by exam in ==Sui== = **intended to deprive high aristocracy of power**

  - opened to a somewhat larger population to serve in government
  - but many successful candidates **still correlates to power of rich families**

- second emperor wanted to rebuild the great wall, and build a road (much different from the first emperor of Sui, and very alike Qin)
  - he also conmissioned lots of army to vietnam, turko-mongol in the north, and korea
  - he did too much, **distrupt dometic ecnomony, and with the failure of Korea, and became overthrown**

**Tang Founders**: cosmopolitanism about elite culture

- with the disintegration of the Sui dynasty in July 617, ==Li Yuan==  urged on by his second son Li Shimin (, the eventual Emperor Taizong)  rose in rebellion.
  - Using the title of "Great Chancellor" (), Li Yuan installed a puppet child emperor, Yang You, but eventually removed him altogether and established the Tang Dynasty in 618 as Emperor. 
  - His son and successor ==Li Shimin honoured him as Gaozu ("high founder")== after his death.
  - and later ==Li Shiming become Tang Taizong==, achieved by killing his brother and forced his father to step down
    - unethical as it violated Confucian norms, and hence a "legend" arose that emperor was summoned before the King of the dead
    - the king of the dead asked why did you do it
    - the emperor was terrified, and in the end sacrificied his family for the good of the empire
    - shows the belief in afterlife and ==juristication of the courts of the dead==, which is a norm of justice at that time
- ruling in the north become multi-ethnic
  - lots of Tuoba intermariage, different families made different choices
  - already in Sui: e.g. wife of the founder of Sui was Turkish, and the crazed son of Emperor Taizong (because his father killed his homosexual lover with a palane entertainer) insist on only speaking **Turkish**. Eventually his wife Wu Zhao took over and became **Empress Wu**
  - in general, very **mixed identity in the north**
- Tang followed a lot of Sui policies
  - both had a modified form of the ==equal-field system==, which originated from Northern Wei by Dowager Empress Feng
  - both retrained militia system of nothern dynasties (very ingrained into the population)
- Tang's founder is the **Li family**
  - also mixed ethnic background, spoke both Turkish and Chinese
  - hence with **a heavy Tuoba element and steppe culture**, e.g. liked horse riding and hunting
    - steppe nomads learned to ride from a young age and were expert horsemen
  - after An Lushan rebellion, people wanted to draw a more clear ethnic lines
- The tang was an age not just for cultural openenss but of **political strength**, and had much control over its citizens
  - official supervised markets carefully (e.g. periodically monitored prices), and also kept a close watch on rural residents


**Tangs military culture: part of the elite identity**

- e.g. from Han, Wudi was a emperor of strong military

- in case of Tang, it is different from Han because

  - it is a militarized society, mixed of military **and civilist cultures**

  - the **militia system ** made almost every person had some kind of military knowledge

    - ==Sui and Tang== developed this system, which a way to privde a national army from a local level
    - based off from equal-field system, so that given land = tax and **to supply one soldier,** but that soldier is exempt from taxes

  - soldiers are serving by ==rotations== (hence no permanent soldier = less concentrated regional power)

    <img src="china_civ/image-20221006134140581.png" alt="image-20221006134140581" style="zoom:33%;" />

    - when off duty = training
    - when on duty = go to war if there are wars, otherwise patrols in captials, etc.
    - don't receive salaries, and so need to prepare their own equipmenet, horse, etc, but will have rewards from **won battles**
      - e.g. recall Mulan, who had to buy her own horse
    - won battles for rewards in land, cash, ranks, hence can be a way some family desires

  - using the above system, the cost of launching wars is minimized. Hence lead to a high degree of military knowledge penetrating throughout the population

- using this militia system, Tang got a lot of **expansion success**

  - in 630, conquered Mongols in **Mongolian**. This victory gave Taizong a title "Heavenly Khan"====

  - also campaigned western states

    <img src="china_civ/image-20221006134530315.png" alt="image-20221006134530315" style="zoom:33%;" />

    which become mordern day Xinjiang

  - but since a large part of Turkish soldier who were sent from tang's army settled there, hence they people in Xinjiang speak turkish a lot

**Established Six Protectorates**

<img src="china_civ/image-20221006134820782.png" alt="image-20221006134820782" style="zoom:33%;" />

- a protectorates has a local ruler and autonomy
- the protectorate submits to the Tang emperor as the highest sovereign.
- a Tang was to pacified te region
- the protectorates also paied tributes ot the Tang, in return, emperor gave them licenses for trade = trade relationship

**State-building at home**

- ==Tang Code== of 653 was the earliest code to survive directly, read and copied (before other code was dug up + bamboo)
  - principles of crimilal laws, etc.
  - many other countries (e.g. Japanase, Korea) adopted a lot of Tang code not for its content, but for the symbol of prosper
  - but punishment depends on social status: priviledged, commoners, and inferior people (e.g. slaves)
  - the code was also calibrated based on sex, age, and disabilities
  - **reflected Legalist on appropriate treatment, but also espoused Confucianism values** (e.g. killing own father much worse than killing a stranger)

- promoted ==Confusion education==, and expanded the ==civil exam==
  - but still Tang's offcials is only at the early state of change, most still come from wealthy family
  - only 20-30 men passes the exam per year, hence bureaucracy still mostly domoinated by aristocrates
  - the change is **most intensified during Song**
  - at the start of the Song, those great families started to fade away, perhaps rebellion targeted at aristocracy at the end of Tang = genocide.

**Tang's openness**: Changan = eternal peace

- recall that **Chengzhou=Luoyang, Changan= Xi'an**
- Chang'an was rebuilt by Sui as a political statement, and was rebuilt by the Tang for the same reason = symbol of power

<img src="china_civ/image-20221006135233371.png" alt="image-20221006135233371" style="zoom:33%;" />

- the city layout is highly planned, had 1 million living inside and approx another million just outside.

- a lot of walls for security purposes (had curfews, patrols, etc.)

- today none of the Tang outer walls remain, but only the Ming wall survived, which is approximate only around the palace city area

  <img src="china_civ/image-20221006135408226.png" alt="image-20221006135408226" style="zoom:33%;" />

- emperor palace in the north, and people lived in the rectangular units = a unit. Rich people lives closer ot the palace, and there are also two **notable markets**, where 

  - west markets are for foreign trades = most foreign merchant come form west
  - east markets are for trading local goods

- the southern gate also had a **very wide avenue: wide enough for much traffic** (gone today. but below is reconstruction)

  <img src="china_civ/image-20221006135627741.png" alt="image-20221006135627741" style="zoom:33%;" />

- Emperor Taizong is very interested by Xuanzang, a Buddhist in search of Buddhist scripture and came back

  - He is known for the epoch-making contributions to Chinese Buddhism, the travelogue of his journey to India in 629645 CE, his efforts to bring over 657 Indian texts to China, and his translations of some of these texts
  - inspired the modern novel: **The journey to the West ()**, to depict Xuanzang's journey in search of the Buddhist script

- knowledge of outside world largely **influenced by those exotic goods, central asian musicians, magician**, etc.

  - attest the love of Tang elite for cosmopolitan cultures and especially those of the central asia
  - but they had a hard time diferentiate those foreign people, **so called them "Hu"**

**Who are the Hu ?**

- general term for foreigners from the north or west. The origin might be related to homophone beard.

- North influences: steppe peoples, Xiongnu, Xianbei, Mongols.

- West influences: Sogdians, Parthians, Persians, Turkic.

  - **Sodigans** was an ancient **Iranian** civilization between the Amu Darya and the Syr Darya, and in present-day Uzbekistan, Turkmenistan, Tajikistan, Kazakhstan, and Kyrgyzstan.
  - sodigans were quite useful - important **middle-men of Tang and Byzantine empires.**
  - Sodigans are also **promininet among military** (e.g. soldiers) and merchants in Tang
  - hence have a lot of Sogdian figurines: stablemen, soldiers, musicians.

  <img src="china_civ/image-20221006140016892.png" alt="image-20221006140016892" style="zoom:33%;" />

- those ==Sogdian people also brought a major changes to Chinese music== (and dance)

  | <img src="china_civ/image-20221006140105446.png" alt="image-20221006140105446" style="zoom:33%;" /> | <img src="china_civ/image-20221006140445166.png" alt="image-20221006140445166" style="zoom:33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  - pipa was from western asia, but now is a major musical instrument in China

- before Tang, all chinese sit on the ground with the formal sitting position, but then in Tang people begins to **sit in the chair** (slowly shifts away to a new pose)

  <img src="china_civ/image-20221006140624154.png" alt="image-20221006140624154" style="zoom:33%;" />

**Tang Trade**

- some people say it is buddhist monks **brought chairs in for trade**

- Samarkand-based Sogdiana and Ferghana supplied fine **horses**

- many exotic foods

  - from Central Asia: peas, spinach, garlic.
  - from Persia: sesame, pistachios.
  - from India: black pepper, cane sugar (native sweetener: malt sugar & honey)  from Mediterranean: coriander.
  - grape wine available in Changan.
  - lychee delivered daily from warm southlands for Consort Yang.

- Tea became a national drink and a major trade item.

  <img src="china_civ/image-20221006141031697.png" alt="image-20221006141031697" style="zoom:33%;" />

  - throught the history people had their own "culture" during their time. In Tang, a specialized tea set above
  - added ingredients such as salt, citric peel, etc.

**Life of Commoners and Merchants**

- merchants much well off than commoners, who need to to menial jobs such as maintaingin gardens
- with the "Tale of Li Wa", which talked about a **high officials family background** young man came for examination $\to$ attracted to Li Wa, a prostitute $\to$ used up all his money and was abandoned $\to$ became a begger, almost died, saved by employees at funeral shop $\to$ became a famous professional mourner $\to$ father found out and beated him almost to death (assuming the status of polluted, inferior person) $\to$ saved by Li Wa $\to$ studied hard and **became a official**
- shows that ==flux of social class is not that encouraged==

**Examinations in Tang**

- exams are used to recruit men of good character
- therefore, taking examinations = regularly ==visiting examiners and giving them samples of your writing==
- but the good writings = poetry = only those brought up in a good family could be expected to know
- importance of poetry also lead to **explosion in poetry writing**
- but it ended still with majority of the court composing people from families of high social status

**Religion in Tang**

- **Buddhist** and **Daosit** are the major players, and **Confucian** central to states, but more patronized Daoism 

  - Confucianism thought more like "framework"/structure for organizing the governments, e.g. ==train candidates in Confucian classics==.

  <img src="china_civ/image-20221006141208343.png" alt="image-20221006141208343" style="zoom:33%;" />

  - more support for Daoism, because imperial famly claimed their origin can be traced back to Laozi
  - influnce of Buddism is still everywhere, a lot of monarsteries

**Tang soft power and Japan**

- properous of China attracted much foreign countries who also seek to build an empire (e.g. many **Korean and Japanese** in Chang'an)

- Japanese connection also became much deeper
  - Japan rulers were also tring to establish empire, since Tang had been so powerful
  - based their policy on Japanese pepole who travelled to China study stuff and taking back to Japan with them, and relied on a lot of Buddhists
  - the capital of Nara built in 618, and then **Kyoto built in 794 are smaller versions of Changan**: square in shape, geometrically aligned on a north-south axis; grid structure.
  - Kimono is from China, and lots of **Tang scriptures are importanted to Japanese** (who has spoken laguage but not written)

**Dynasty Names Adopted as Ethnic Term for Chinese**

- ==Han== mainly used in mainland China
  - in 20th century mainland, China's national ethnicity is constructed and called "Han"
  - national language also called "Han language"
- ==Tang==, more popular used for the Western, e.g. Tangren Jie=China Town
  - because first migration was Cantonese speaking diaspora (control of them was solidified during Tang)
  - Tang also had a much broader contact with the **rest of the world**

**Empress Wu: only woman emperor in China**

- previous empres are mostly regent, dowager
- personal name ==Wu Zetian==
  - regent, 660-690; as the Emperor of Zhou, 690-705 
  - ruled very capably, but there exists evidence her approach are sometimes ruthless
- her real look must be beutiful (praised by emperor), was borned into a wealth family
  - encouraged to read and write in family
  - married to emperor family
  - interesting story of how she came to power, e.g. was concubine of Taizong, but when Taizong died, bore the first child of Gaozong (from whom she took power from), which indicates incest
- She successfully succeeded the throne largely because lots of officials also supported her, and ==justified by using Buddism==

  - a monk wrote a commentary that told of a female goddess (hint at Empress Wu) who would be reborn as a princess in a small kingdom, etc.

- later **Xuanzong** made China the most properperous time, but was built by Empress Wu
- but still since Empress = **upset a balance of nature, hence some scholars are still concerned**
  - Confucious idea that natural diasters = emperor problem (Dong Zhongshu, a ruler who did not fullfill = nature diaster)
  - earth quake happened, and ministers took it as a problem
  - hence, some portray her as a ruthless ruler (e.g. because scribes are mane)
- avent sponsor of Buddhistm, but to user religion in her favor (e.g. justify her throne)

- in general Tang people had freeodm and under her, **women (tried) to have better freedom**

  <img src="china_civ/image-20221006142234989.png" alt="image-20221006142234989" style="zoom:33%;" />

  - e.g. shifts in beauty standards: women on horse also appealing.

- The importance to history of Wu Zetian's period of **political and military leadership includes the major expansion of the Chinese empire**, extending it far beyond its previous territorial limits, deep into Central Asia, and engaging in a series of wars on the Korean Peninsula, first allying with Silla against Goguryeo, and then against Silla over the occupation of former Goguryeo territory.

- other notable changes in value include:

  - as **wife**
    - main wife but not cocubine
    - men have shorter lifespans than women, so the wife often becomes the only head of the household.
  - as **mother**
    - should continue have influence even when sons become adults
    - cocubine is lower in status, hence concubine's son should regard main wife as mother
  - as **clan advodate**:
    - encouraged women's position in government, but still install family males to most positions especially to military
    - this is anyway too big of a change for a complete equality of gender here

> **Interesting Note** from primary sources:
>
> - Emperor Taizong's advice on effective government: land is large, need cooperation = give relatives land. Zhou is a successful model lasted several centuries
>   - Qin had zero relatives to rely on (Li Si's plan) hence failed
>   - Han enfeoffed the land too generously: the Six Kingds harbored ambitions of overthrowing throne
>   - so the best way is to enfeoff many relatives to even up their power and to have them regulate one another and share responsibilities
>   - and for government officials: 
>   - military cannot be eliminated nor used all the time
>   - emphasis of literary arts and military arts

## The Decline of Tang

**Previously on Sui and Tang**

- Sui-Tang continuity: Tang was a successor to the Sui and Northern Dynasties

- Three features of Tang culture (mostly elite)

  - cosmopolitanism: ethnic diversity, Tang ruling family (Li) as a classic example

  - importance of military pursuits and military cultures to elite identity; a more militarized society than later period

  - aristocracy remained dominant in government (the system of examination is only fully utilized for social flux at Song Dynasty)

- Tang Empire (618-907) marks **culmination of the turn toward Central Asia** that began in the Han Dynasty, and continued through age of disunity. Tang military institutions, urban culture, silk road trade and religious life look westward
  - however, in Late Tang due to much political disunity they withdrawn their troops and lost most of the control in Central Asia

- Emperor Wu Zetian is the only woman to rule under her own name; powerful and competent (some ruthless approaches), expanded empire, but disparaged by later generations because she is a woman

<img src="china_civ/image-20221011132556798.png" alt="image-20221011132556798" style="zoom: 50%;" />

**Emperor Xuanzong: the peak of the Tang**

- **Emperor Xuanzong:** early half of his reign: astute ruler, the empire expanded to the high point; patronage of the arts (especially poets)

  - Empress Wu had his daughter tried to poison her, trying to become the next empress. So there were some instability, but Emperor Xuanzong made it
  - recall that Empress Wu had a lot of her family relatives in government posts, and sponsored a lot of Buddhism, e.g. claimed that she is the incarnation of Bodhisattva. Emperor Xuanzong tried to **redress/reform** those
    - he also reformed Equal-Field system due tax problems encountered during their time
  - there were also threats from the Turks, Tibetans, so he restructured military system and give commanders great authority (among them he particularly gave **An Lushan much power**)

- change of women's clothing, don't reveal shoulders or backs

  <img src="china_civ/image-20221011132658550.png" alt="image-20221011132658550" style="zoom:33%;" />

  yet this type of dress (rightest) is not for commoners, and it is more popular for them to wear men's clothing = meaning more physical mobility. An example below are all women:

  <img src="china_civ/image-20221011132739433.png" alt="image-20221011132739433" style="zoom:33%;" />

  hence this tells something about the change of attitudes towards women

  - ==freedom of women wearing riding trousers with faces uncovered==, where as in later centuries women of equivalent status would always be veiled, ear robes, and ==be restricted to sedan chairs (Song dynasty)==

  - but unfortunately due to decline of Tang later, it reverted to conversative clothing

- however, at late ages had some problem. He fell in love with Prized-Consort Yang, and gave too much power to her

  - falling in love with , who lacked political sense, even when he is 60
  - gave too much power to Yang and her relatives, so they almost monolpolized the government
  - basically becomes a femme fatale
  - An Lushan liked by Emperor, but relative from Yang's family disliked his power, so ==arrested and executed An Lushan's supporters in court==

  - this oral history and gossip recorded in Bai Juyis A Song of Unending Sorrow

- however, Yang did show us the standard of beauty at that time is to be a little bit chubby

  <img src="china_civ/image-20221011134021391.png" alt="image-20221011134021391" style="zoom: 33%;" />

**Battle at Talas**: in modern Khazakhstan

- suprised by the sudden defection of China's tribal allies, the Chinese **lost the battle to the Islamic Abbasid caliphate**
- marked the beginning when ==region of Central Asia turned away from China towards the Islamic world==

**General An Lushans rebellion (755)**

- recall that Yang Guozhong (cousin of Prized-Consort Yang, chief minister) began to purge members of General An Lushans faction due to Yang's relative's dominance in court

- also at that time, due to malfunctioning of equal-field system, ==state relies on soldiers from alien tribes instead of collected from each field==

  - problem 1), they **don't pledge alliance to Tang**
  - problem 2), they become **long-lasting**, whereas equal-field collected soldiers will go back home
  - problem 3), An Lushan was also particularly powerful due to prior likings of Emperor Xuanzong

- General An Lushan:

  - commanded two large frontier armies in the northeast, in modern-day Beijing.
  - father was Turkish; mother was Sogdian.
  - alarmed that his supporters at court were being arrested and executed.
  - ==in 755, leads an army of 150K troops towards the capital cities==.
  - he first captures Luoyang, the eastern capital on the edge of the plains.
  - from Luoyang, the fortified Tong Pass blocked westward passage to Changan.
  - but Yang Guozhong pressed imperial forces to leave the Pass and do battleand were destroyed by An Lushan. - **frontier troops were professional, war veterans**; capital troops, unseasoned.

  <img src="china_civ/image-20221011134349910.png" alt="image-20221011134349910" style="zoom:33%;" />

- as a result, Emperor Xuanzong flees from Changan to Chengdu (modern-day Sichuan), and eventually abdicates the thrown to his son.

  - note that Chengdu is also the capital of Shu, and hence "The road to Shu is steep" from Li Bai is relevant

**Consequences of this rebellion**? A major turning point for Tang: decline of Tang stability

- Tang's forces in central asia has to be recalled for rescue. ==Tibetan Empire and Muslim Caliphate== are also encroaching those areas, so a retreat made Tang a lost of control there, which eventually caused the lost of control of the Silk Road
  - Xinjiang ceded to Tibetan empire and Muslim Caliphate
  - central asia then changed from Buddhism to Islam
  - cut-off from horse supplies due to lost of Silk Road
  - became less cosmopolitan in western eyes
  - without this buffer zone frontier, capial Changan is more exposed to incursion
- rise of regional warlordism: to quell the rebellion, Xuanzong **gave a lot of power for regional governors**.
  - then those regional governments **later** made themselves **autonomous**, hence results in political chaos
  - but also caused an even further increase in local military knowledge
- rebellion resulted from, caused, or simply coincided with
  - **collapse of the equal-field system** (for collecting tax revenue)
    - during quenching rebellions, system of household registration collapsed = does not know how many residents are there = cannot collect tax
    - because there is a shortage of land (due to population increase, aristocrats gathered too much land). Government cannot raise enough revenue.
    - lost hold of household registrar = ==lost direct control of the people==! Then then had to rely on regional governments to report those numbers, which never works out.
  - **collapse of the militia system**, since no money, hence has to relie on ==foreign solders== who pledged alligence to comamnder bu tnot dynasty
- all of those contributed to the collapse of the down fall of Tang
- this is also a period of time hard for ordinary people to survive, e.g. as deadly as first world war
  - as the battle happed in the capital, Du Fu and his family fled. This also becomes a major turning point in Du Fu's life

**Poetry: Li Bai and Du Fu**

- they are friends but differ greatly in work. Li Bai write about nature, especially wine, but ==Du Fu is more for his social consciousness== (e.g. events happening at Tang)

  <img src="china_civ/image-20221011135256440.png" alt="image-20221011135256440" style="zoom:33%;" />

- the rebellion was also a turning point of Du Fu

  - no longer a confident scholar, but become an ordinary person, hence his **writing style becomes much heavier and realistic**
  - became a helpless pawn, and wanted to write the things he wanted to say (**: , ...)

**Change of Territory**

- Tang is the first dynasty where knowledge of the outside world is really accessible for China commoners lives (e.g. in west markets, and pilgrim, merchants, caravans have been traveling back and forth). Before in Zhou dynasties, and even in Tang, those knowledges are mostly exclusive to the court

  <img src="china_civ/image-20221011140358147.png" alt="image-20221011140358147" style="zoom:33%;" />

  but now, China can no longer see himself as the only civilized nation (e.g. Zhou King once said the others are barbarians). Several other great civilizations exist, for the control of Central Asia trade routes

- Muhhamad preaches ==Islam==

  <img src="china_civ/image-20221011140615505.png" alt="image-20221011140615505" style="zoom:33%;" />

- ==Arab Caliphate== v.s. Tang, wanted Tang's submission

  <img src="china_civ/image-20221011140727895.png" alt="image-20221011140727895" style="zoom:33%;" />

  which was before An Lushan, where Tang had some initial success but suffered a decisive defeat shortly before An Lushan Rebellion. 

  - Defeat = those Chinese painters, craftsmen are taken as prisoners in Caliphate  = introduction of paper making and gave technology away

  - people argue that this also marks the end of Tang's west expansion
  - sooner or later Islam also spread in thos area

- another force was ==Tibetan Empire== in the mid-7th century

  | <img src="china_civ/image-20221011141035756.png" alt="image-20221011141035756" style="zoom:33%;" /> | <img src="china_civ/image-20221011141316127.png" alt="image-20221011141316127" style="zoom: 33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  the birth of this Empire comes from Songtsen Gampo, lead to organized state and powers

  - Tang wanted a peaceful relation, hence had consort sent to Tibetan King, and made alliance with Tibet
  - this princess is said to bring agricultural technologies, and to introduce Buddhism
  - later, Songtsen Gampo also married a Nepal princess, who had their own version of Buddhism
  - after Songtsen Gampo, (esoteric) Buddhism became Tibet's religion, but very differnt from Buddhism in China

  even though there are some peaceful times, it eventually deteriorated. So after An Lushan, **Tibetan Empire recaptured the land and tried to press on deeper in China**

  <img src="china_civ/image-20221011141657620.png" alt="image-20221011141657620" style="zoom: 50%;" />

  this also caused the direct routes for Silk Road is no longer at Chinese hands

**Examination System In Late Tang**: Bai Juyi

- even after all those territorial changes, the system of examination stayed basically the same as in Early Tang
- exams tested on
  - classics and literary ability/merit: write essays on a given topic
  - ==social ability: establish good relations with examiners==
    - sons of powerful official families often given high scores, hence still mostly elite families in offices
    - **In 800, for once the exam is just based on merit, and Bai Juyi passed**
    - a different structure in Song

**The Rise of Eunuchs**

- usually eunuchs are for serving women's quarters (guard the women's living areas), and some for messengers and some for spies on military commanders
- but after An Lushan's rebellion, they began to
  - gain control of the emperor's personal army
  - placing adopted sons into different offices, and forming factions with each other
  - ==hence eventually obtained high positions and controls==
- over the ninth century, there are eight more changes of emperor where each accession was **engineered by eunuchs** working in concert with political factions
  - hence, life at court such as Bai Juyi depended a lot on **which faction is in current power**

**Mogao Caves and Dunhuang Library**

- The 'Caves of the Thousand Buddhas' (Qianfodong), also known as ==Mogao==, are a ==magnificent treasure trove of Buddhist art==. They are located in the desert, about 15 miles south-east of the town of Dunhuang
  - By the late fourth century, the area had become a busy desert crossroads on the caravan routes of the Silk Road linking China and the West. Traders, pilgrims, and other travelers **stopped at the oasis town to secure provisions, pray** for the journey ahead or give thanks for their survival. 

- adjacent to one of the caves was the **hidden Dunhuang Library** discovered by Daoist Wang: a secret library of 40,000 documents was discovered inside cave #16 by Wang Yuanlu. The library is referred to as cave #17

  - almost half of the text where Buddhist text in Chinese or Tibetan
  - first time purely ==original documents not for public use== (compared to books written after the fact) to reveal the real life - religious and civil in Tang.
  - but because papers are ==recycled, the margins and backs display all kinds of documents==, including literacy work, contracts, government documents, etc.

- some civil and religious life revealed include

  - During the thousand years of artistic activity at Dunhuang, the **style of the wall paintings** and sculptures changed. The early caves show greater Indian and Western influence, while during the Tang dynasty (618906 C.E.) the influence of the Chinese painting styles of the imperial court is apparent. 

  - donor portraits with **Xianbei ethnic feature**

    <img src="china_civ/image-20221013215226656.png" alt="image-20221013215226656" style="zoom:33%;" />

  - **monasteries at Dunhuang** also ran a variety of money-making enterprises, including oil presses

  - Sodigan letters, Irq Bitig (8th-9th c), the only Old Turkic manuscript partially overwritten with Buddhist verses in Chinese

**The Beginning of Printing**

- the first printed book found in Dunhang library cave.

- during Tang there were a high demand for texts (e.g. religions), hence lead to invention of woodblock printing, and later in Song 

  | <img src="china_civ/image-20221011142335264.png" alt="image-20221011142335264" style="zoom: 33%;" /> | <img src="china_civ/image-20221011142345272.png" alt="image-20221011142345272" style="zoom: 33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  where the problem with the old technique is that if you made a mistake, the whole mold is useless

**Persecution of Buddhism** (840 - 846)

- many aristocrats were using land as Buddhism temples to **evade tax**
- but also a lot of conflict between Buddhism v.s. Daoism and Confucianism
- at the end of Tang **persecution of Buddhism provoked by Daoist**, but also **rationalized by Confucians**
  - sees those temples asked commoners to buy weird goods and donate money while **undermining tax revenues**
  - see Buddhism to undermine family ties, give up your own responsibility and become a monk, hence violates Confucians ideal
  - during (840-846), also as emperor favored Daoist, Buddhism was persecuted =  lots of monks and nuns have to be returned to their normal social status
  - while Buddhism did not diminish, **their impact in China never rose again to the peak in early times**

**Huang Chao Rebellion**

- ruined Chang'an, never again as China's capital

- put an end to high aristocracy in court by ==targeting the aristocrats of Tang==

- contributed to the fall of Tang in 907, and the distribution of power in China after the rebellion becomes

  <img src="china_civ/image-20221011165258535.png" alt="image-20221011165258535" style="zoom: 50%;" />

# Song (960-1276)

**Previous only the downfall of Tang** 

- Tang critically weakened by the An Lushan rebellion (755-63), allegedly caused by intrigue surrounding the Precious Consort Yang, Tang dynasty enters decline
  - retreat of forces in Central Asia lost control of it
- Eurasian Empires in the 8th century: Tang, Tibetans, Muslims, Uyghur-Turks
  - **Tibetan Empire**: Songtsen Gampo, Princess Wencheng. Eventually took over the Hexi corridor which is critical for Tang's access to Central Asia
  - **Abbasid Empire (Muslims)**: the Battle of Talas (751) between the Tang and Abbasids, signals ==Tang loss of control over silk road and central Asia==
- The **Mogao Caves** in Dunhuang: provide an abundance of vivid materials depicting Buddhist arts, religion, culture (4th-14th c), the largest body of Buddhist art in China; the **library** found there includes many secular texts that tell about politics, economics, ethnic relations of the local society in western China.
- Persecution of Buddhism: an example of conflicts between religion and state
  - Buddhist monasteries before operated oils, loans, banking services, and hold a large land being tax-exempted
  - hence the central government became annoyed especially when they don't have enough tax revenue
  - Buddhism survived the persecution, but never reached its peak status as in Tang until today
- Regional ==rise of warlords in late Tang== (just like the final breakup in the Han dynasty)
  - government became decentralized during rebellion, and many elites fleed and clustered in Chang'an and Luoyang.
  - then those elites are obliterated/almost compteletly wiped out during **Huang Chao Rebellion**
  - hence a period of disunion in 907 - 960, and the Wei river valley, ==Chang'an + Luoyang never again reach their prosper in the past==
  - therefore, this removal of stale upper-class aristocrats also **contributed to the success of the civil service exam** in Song = really based on merit = based on your ability to read, understanding Confucian classics

## An Early Modernity in Song

**China's Population throughout History**

<img src="china_civ/image-20221013132228281.png" alt="image-20221013132228281" style="zoom: 33%;" />

- inability of government in Tang might not mean bad economy. In fact, late Tang stimulated economy: rise of population, almost doubled even during Tang decline

- also a significant **shift of population from north to south**. 

  - Before North:South population is 2:1 in Early Tang
  - After An Lushan Rebellion 1:1
  - In Southern Song 1:2, when their capital in Kaifeng is sacked

- late Tang also have lots of **technological developments, such as paddle-wheel Ship and Woodblock Printing**

  <img src="china_civ/image-20221013132427835.png" alt="image-20221013132427835" style="zoom:50%;" />

**Song Territory**

<img src="china_civ/image-20221013132547104.png" alt="image-20221013132547104" style="zoom: 50%;" />

- Song's territory is the smallest among all of Tang, Song, Yuan, Ming, Qing
- also divided in two periods. 
  - Early/Northern Song rules both north and south, including Kaifeng
  - Late/Southern Song rules only capital at Hangzhou (Kaifeng is sacked by the northern enemies, i.e. the **Jurchen forces**)

- but Song in general have a ==modern economy== being industrialized and monetized

**Agricultural Leap in Song**: factors contributing to rise/improvement in agriculture

- new strains of rice from Champa/Vietnam = more rice yield as it can harvest twice a year

- mass iron production for plows and farm tools

- mobilization of bio-power (human and animal labor inputs).

- improved, intensive farming techniques of rice plots.

  - wider application of fertilizers

- dikes, canals to convert wetlands to farmland.

- ==advances in water/irrigation control==, including pump and water mills

  |                             Pump                             |                             Mill                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221013133119636.png" alt="image-20221013133119636" style="zoom:33%;" /> | <img src="china_civ/image-20221013133217766.png" alt="image-20221013133217766" style="zoom:33%;" /> |

  hence families can get more rice then needed = can sell the rest and exchange goods

- as a result, population doubles to 120 million, from 60-65 million in Tang

**Commericial Modernization in Song**

- new/more agriculture techniques + invention means more **specialization**

  - peasants can just ==exchange== for other goods instead of trying to produce it themselves

  - in late Song cottons=cash crop

- as compared to regulated trades in Chang'an in Tang, in early Song, **merchants and shops are spread out in town to intermix with residence**  

  - some even had the stock system = manager+owner of business
  - **credit systems** adopted by traders
  - an increase of ==people in middle class==

- ==encouraged maritime trade== (as compared to Tang), due to cut off of silk road

  - encourage foreign traders to come to China as well

  - to achieve maritime trade, needs improved ship technologies

    <img src="china_civ/image-20221013133731794.png" alt="image-20221013133731794" style="zoom:33%;" />

    as well as the ==invention of compass==

- as trade increase, **demands of money increased**. In addition, silk was also used as currency

  <img src="china_civ/image-20221013134014492.png" alt="image-20221013134014492" style="zoom:33%;" />

  the largest output in Tang was 320 million coins a year. But in late Song, government maintains **more than 6 billion coins**. But there was still not enough, as there is still no inflation.

  - government requires more money in circulation
  - demand has been so great and hence **issued the first paper money**, which can be created out of nothing

  |                         Paper Money                          |                     Government Logistics                     |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221013134255718.png" alt="image-20221013134255718" style="zoom:50%;" /> | <img src="china_civ/image-20221013134353502.png" alt="image-20221013134353502" style="zoom:50%;" /> |

  hence **Song's economic change to China is almost revolutionary**

**Wang Anshi vs. Sima Guang**

- despite overall economic boom, **some farmers still suffer** from interest rates and tax

- The Emperor Shenzong of Song faced **declining taxes and an increasingly heavy burden of taxation on commoners** due to the development of large estates, whose owners managed to evade paying their share of taxes.

  - Therefore, he sought for advices, and comes Wang Anshi

- Wang's primary objectives of New Policies (*xinfa*) were

  - cut government expenditure and ==strengthen the military in the north== (which requires funds)
  - To do this, Wang advocated for policies intended to ==alleviate suffering among the peasantry== and to ==prevent the consolidation of large land estates== which would deprive small peasants of their livelihood = aimed to help improve the wealth gap
  - in reality, his policies is **effectively the last effort by Song government to control the economy** = last strand
  - but this ==failed== because the local officials implemented this policy were rewarded for how much loans they gave out $\to$ local officials benefited more than the farmers

- Specifically, his policies include:

  - fixing fiscal policy: **Green Sprout reform**, which give peasants money in the beginning and ask for repay with 40% interest at harvest time (so they don't need to rent from landlords)
  - schemes of transporting money
  - pumped more currency into the economy

  recevied initial success but failed in the end

- Sima Guang (not related to Sima Qian) is one guy opposing Wang's policy: wanted incremental reform instead of all processes done at once = too fast

  - Wang Anshi almost like Legalist, as he is **making state intervening deeply to people**, hence denounced as un-Confucian
  - Wang Anshi therefore needed more support in court = reformed civil exam system to teach his thoughts
  - further exacerbated the problem

**Organization in Song**: Idealized urban life

- Qingming scroll, idealized urban life in Kaifeng (debatable), but anyway **exemplifies Song's rigor and commerce**

  |                         Full Scroll                          |                         Center Piece                         |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20221013135132538](china_civ/image-20221013135132538.png) | ![image-20221013135141379](china_civ/image-20221013135141379.png) |

  notice the overall city structure is very different than Tang: symmetrical + structure, but Song = more organic. This hints at the difference between Tang and Song: Song is moving away from society full of aristocracy to **more middle class based** society, hence different city layout, etc.

- transportation in Song: important people on horse backs and in caravan

  | <img src="china_civ/image-20221013140017927.png" alt="image-20221013140017927" style="zoom:50%;" /> | <img src="china_civ/image-20221013140025275.png" alt="image-20221013140025275" style="zoom:50%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

- but notice the lack of female figures in the scroll: if this were Tang's time, women on streets with beautiful dresses, ==why such changes==?

  - recall that Empress Wu tried to improve women's status, and during Emperor Xuanzong women can come out homes and wear men clothes
  - but in Song, there is 
    - commercialization=market economy, so that with coinage + paper money $\to$ women **became commodified again** (e.g. prostitutes, sold daughters, etc)
    - **foot binding in Song**, women having small feet = standard of beauty = object of sexual desire, because this means it feet becomes a ==private part of body== only seen by herself and husband
      - correlates with the growth of Song's commercialization, and that families sell daughters + people kidnap women 
      - women transformed into commodities who were bought and sold on the basis of appearance
      - women of good families compete with women with bound feet, hence 
  - however, in general women do exert greater autonomy at home by handling the household estate as men die earlier

- some unrealistic aspects of the scroll is

  - notice that there is no trash on street
  - no fire stations, weird because lots of houses should be made of wood
  - another modern visualization from some text include

  <img src="china_civ/image-20221013140223138.png" alt="image-20221013140223138" style="zoom:50%;" />

  notice a fire station and soldiers repairing canal

**Capital at Hangzhou**: south already contained a lot of population but then with capital shifted to Hangzhou in late Song, living in south becomes popular

- recall that in Late Song, Kaifeng is sacked, so people moved to Hanzhou
- since Late Tang people are migrating, but recall that
  - south was hard to irrigate = solved by building canals
  - north much exposed to steppe culture, and to the northern enemies, hence much more chaos = less preferred eventually

| <img src="china_civ/image-20221013140442737.png" alt="image-20221013140442737" style="zoom:33%;" /> | <img src="china_civ/image-20221013140431572.png" alt="image-20221013140431572" style="zoom: 33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |

**Urbanization in Song**

- as advancement in agriculture = food surplus = people can have other occupations

- movement of both people and good into cities
  - both Kaifeng and Hangzhou were linked by the Grand Canal (built in Sui)
  - both Kaifeng and Hangzhou had population reaching at least 1 million

- in mid Song, 5% population already urban

- however, this raid commercialization can have ==adverse effect on government==

  - technology advancement so fast that government cannot keep up with it
  - e.g. in Tang, a small elite group in government is easy to control. But now in Song there is an emerging elite class/middle class with new found wealth = push **against the centralized control Tang had**
  - this diffusion of "resources" also relates to the **"laicization" of Song religion**. Practicing religion becomes shifted away from institution but more towards individual activities any lay people can do, e.g. building their own shrines
    - very similar diffusion of governmental control, so it now the religious institutions


  therefore, the general theme of rising middle class and a =="anti-centralization" of resources as compared to Tang is also a distinct factor in Song==

**New Elite: Scholar-Officials** (*shi*, or *shidafu*)

- recall that *shi* was in Zhou as mlitary people

  - In the first half of the Chunqiu period, the feudal system was a stratified society, divided into ranks as follows: the ruler of a state; the feudal lords who served at the rulers court as ministers; the **shi (roughly translated as gentlemen)** who served at the households of the feudal lords as **stewards, sheriffs, or simply warriors**; and, finally, the commoners and slaves.
  - In Warring States period, they become more specialized forces (e.g. in defense) that includes people of humble origin (climbed up by education)
  - *shi* in Han = Confucian scholars, but *shi* still dominated by aristocrats since Han officials are appointed by local recommendations. However, there is a shift towards **more literal skills**/non-military such as poetry.

- in Song, *shi* are **scholar-officials certified through the civil service exams**

  - this new civil service exam ==aims to guarantee that elite families cannot simply occupy government==
  - success in exam = prestige in family. If passed the highest level become *jinshi*
  - without passing exam, you will just be officials of low rank
  - therefore became very competitive, 400,000 people taking but government post is very limited
  - firs time in China history that **ended large dominance of northerners in the officialdom** (e.g. since Sui and Tang, a lot of prestigious family members have mixture of northern roots and are in the government)
    - recall that Sui and Tang has a lot officials still from north + exam system are initial and social status are very fixed

- the exam curricululm includes (ultimiate aim is for governing)

  - elementary education: Analects, Classics of Filia Devotion
  - ancillary exams, such as math and legal codes
  - **core content: classics and history**
    - write eesays on policy (chief examiners can take people who side with their positions)
  - poetry composition
    - ability to allude to historical events and personages
    - easily gradable as it is standarized/ryhmed, etc.
  - hence a candidate needs to memorize a lot of classics: (word count)

  <img src="china_civ/image-20221013141330999.png" alt="image-20221013141330999" style="zoom:33%;" />

- recall that Tang candidates know their examiners, but ==Song insist on the anonymity of candidates==

  - candidates writing will be re-copied by clerks to remove hand-writings styles
  - no consideration of personality of candidate, e.g. as in Tang, and hence a very restricted version of merit

- more state and local academies are built in Song, and increased literacy made this exam popular and hence work

- the ==scholar-official (*shi*) class came to be defined by classical education and scholarly activities==

- supported also a **thriving market for books, art, and connoisseur cultures**. Therefore the invention of movable printing enable this remarkable demand

  <img src="china_civ/image-20221013141800645.png" alt="image-20221013141800645" style="zoom:33%;" />

## Cultural and Intellectual Changes in the Song

**Previously on the Song**

- The Tang-Song transition sees a major shift of Chinas population centers and economic activity southward. Population doubles, rapid **urbanization**, more trade (emphasis on the maritime)

- Social change ushers the Song **commercial revolution**. Trade facilitated by **paper currency** (technology of printing) and technological innovations in agriculture, water transport, and metallurgy. 

  - key innovations: paper money and woodblock printing

  - increased agriculture output = farmers produce more than they need = can specialize and trade

- Wang Anshi's **new policies**: set of interventionist political-economic reforms designed to improve rural welfare, solidify tax base, etc. But meets Confucian opposition because implementations are legalists
  - Qingming scroll aims to depict idealized life at Kaifeng, but we know that benefits did not apply to everyone.
- **Civil service exams** create a new bureaucracy and a new scholar-elite *shi* class as compared to before

**Crisis on the frontier: Tanguts, Khitans, Jurchens**

<img src="china_civ/image-20221018132234600.png" alt="image-20221018132234600" style="zoom: 50%;" />

- the policies from Wang was also to cover the spending to military
  - recall that Tuoba clan of the Xianbei during Six dynasties had a large army
  - Tang recruited soldiers to push out China's border, but in Song times, progressively more northern land is conquered. Specifically the Khitan, Liao, and Jurjan Jin, and Tanguts (Xi Xia). 
  - Eventually ==Yuan== ruled over: Yuan was a **Mongol-led** imperial dynasty of China

**Tanguts and China**

- Tanguts (people of Tibetan origin) after Tibetan fell apart, took Hexi corridor which is critical for the path to Central Asia

  |                      Tanguts v.s. China                      |                  Horse Farm, Hexi Corridor                   |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221018132606077.png" alt="image-20221018132606077" style="zoom: 33%;" /> | <img src="china_civ/image-20221018153346036.png" alt="image-20221018153346036" style="zoom:33%;" /> |

  however

  - has no official history because Mongol attempted to genocide the Tanguts in 1227, hence most of their text and architectures are gone
  - Tanguts had a lot of heavy calvary

- Xi Xia (Tanguts) people look lie

  |                           Costumes                           |                           Scripts                            |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221018133021480.png" alt="image-20221018133021480" style="zoom:33%;" /> | <img src="china_civ/image-20221018133140857.png" alt="image-20221018133140857" style="zoom: 33%;" /> |

  notice that

  - Xi Xia aristocrat looks very alike China, because they adopted much Chinese policies and had **Sinicization** 
  - but later their Emperor mandate their specific form of hair style to keep their culture
  - their scripts are also modelled from Chinese, but much more complicated

  - Tanguts people made beautiful Buddhist artifacts in the Mongol caves as well

    | <img src="china_civ/image-20221018133404190.png" alt="image-20221018133404190" style="zoom: 25%;" /> | <img src="china_civ/image-20221018153627755.png" alt="image-20221018153627755" style="zoom:33%;" /> |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |

**Tangut as Xi Xia** (1038 - 1227)

- In 881 they helped Tang to suppress Huang Chao rebellion

- religion fusion: Chinese + Tibetan Buddhism

- in early Song, lots of trade between Tanguts and Song, and many silk road goods went through them. But their relations with Song is volatile: in later Song they asked for equal status with Song Emperor:

  <img src="china_civ/image-20221018133734437.png" alt="image-20221018133734437" style="zoom:50%;" />

  notice that

  - before 1038, Tangut was polite and referred to itself being a vassal state. But after 1038, no justification is needed 
  - before, mentioned much culture/music as justification = imply that **Confucianism is a standard for civilization** = acceptance of Chinese' idea of civilization

**Khitans (Liao)**, 907 - 1125

- Abaoji established the ==tribal federation== = connecting many tribes together = the Liao Dynasty

<img src="china_civ/image-20221018153958544.png" alt="image-20221018153958544" style="zoom:33%;" />

- Khitans are not friendly to Tanguts either, but they posed more **threat to Song**

- they conquered and occupied the Sixteen Prefectures, ==which also included Beijing==, hence a lot of sentiment in Song

  <img src="china_civ/image-20221018134323478.png" alt="image-20221018134323478" style="zoom:33%;" />

  - many Song emperor tried to recover this land from Khitans, but failed
  - this strip had strategic North- South chokepoints between central plains and northern steppes, Manchuria.
  - was not re-captured until Yuan

- Song was defeated by Khitan, hence signed a ==Peace Treaty of Chanyuan==

  - clear boundaries, sixteen prefecture that Song wanted is kept by Liao
  - Song paid indemnity for peace, which is a **reasonably cheap price** hence gives little incentive for them to fight back
  - Song was also forced to recognize Liao as peers (etc. Khitan would address Song emperor as elder brother)
    - this is rejected by Chinese people = now you have **two sons of Heaven**
    - sense of humiliation, major source of ==strong sentiment in Song==
  - but at least this avoided wars between them. Maintained until 1125 until Song invited Jurchens to attack Liao

- government in North is a mobile civilization = nomadic culure. In the southern = 16 prefecture, so **people kept the Chinese style**

  <img src="china_civ/image-20221018135019332.png" alt="image-20221018135019332" style="zoom:33%;" />

  so essentially Chinese lived under Khitan rules

- Khitan also, used by Jurchens who conquered Khitan and later adopted their own scripts based on Chinese

  <img src="china_civ/image-20221018135138990.png" alt="image-20221018135138990" style="zoom:33%;" />

**Jurchen Jin**

- further east of the Khitans, being half nomad utilizing both hunting and farming

  <img src="china_civ/image-20221018135207765.png" alt="image-20221018135207765" style="zoom:33%;" />

- Originally they were subjects of Khitan, but later Aguda formed a ==confederation of tribes== and rebelled = Jurchan

  - nomad usually do not pose threats until formed confederation. But this also means that in them people are **not from the same tribe**
  - they named themselves ==Jin== dynasty

- Jurchen was also the reason Song became Southern Song in later periods. Since every Song wanted to **retake 16 prefectures** from Khitan but non worked, Emperor Huizong decided to be **allied with Jurchan**. In 1123, Juchen defeated Khitan, but in 1127, they turned on their Song ally, and ==Kaifeng is lost==. After that, Huizong abdicated and Song had to ==move south to Hangzhou==.

  |                                                              |                                                              |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221018135528141.png" alt="image-20221018135528141" style="zoom: 33%;" /> | <img src="china_civ/image-20221018135611298.png" alt="image-20221018135611298" style="zoom: 33%;" /> |

  - is a great painter (e.g. **three perfections, painting, poetry and calligraphy**)
  - Emperor Huizong wished to retake the Chinese territory of the Sixteen Prefectures (area including Beijing) from the Khitan Liao, but is responsible for losing the North and hence criticized by later historians for his **indulgence in aesthetic pursuits**

- hence, Jurchan moved their capital to Kaifeng. But unlike Khitan who kept a duo system, ==Jurchan went Sinicization==

  <img src="china_civ/image-20221018135811632.png" alt="image-20221018135811632" style="zoom:50%;" />

  so a lot Jurchan spoke Chinese, dress Chinese, like ==Northern Wei==

- in total, Song at this point had to sign treaties including:

  <img src="china_civ/image-20221018135943098.png" alt="image-20221018135943098" style="zoom:33%;" />

  but was actually not a big burden, as they did not exceed 2% of state avenue, because Song's modernization and is rich. However, most went to pay for military, which took up $3/4$ of the revenue for just maintaining the army.

  <img src="china_civ/image-20221018140053433.png" alt="image-20221018140053433" style="zoom: 33%;" />

**Essentials of Military Classics (*Wujing Zongyao* )**

- commissioned by Song emperor and had instructions for a broad range of weapons

  |                        Siege Ladders                         |                       Triple Crossbow                        |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221018140220582.png" alt="image-20221018140220582" style="zoom:33%;" /> | <img src="china_civ/image-20221018140230536.png" alt="image-20221018140230536" style="zoom:33%;" /> |

  even though Song had advancements in military and spent a lot on its army, it did not conquer their lost territories back:

  - Jurchan are sinicized anyway, and payment for peace was cheap
  - however, you still need to keep a strong army in case of future infringement

- this book also recorded the invention/production of **gunpowder**, which was found from Daoist alchemy accidentally

- but even with those, it didn't give much advantages because artisans are captured by enemies. e.g. those siege machines are also employed by Jurchen

**Neo-Confucianism**: in response to Chinese cultural identity crisis at borders

- "It was perverse for Chinese to forget their ancestors and abandon sacrifices to them, serving instead barbarian ghosts. Shi Jie 

<img src="china_civ/image-20221018140623628.png" alt="image-20221018140623628" style="zoom:33%;" />

- Buddhism was already in 1000 years, but now they start to **reject the religion as it is not indigenous and practiced by their enemies**

  - recall in Tang times Buddhism was wide spread. But in Song, due to changes of international politics, such as the Khitan Liao and Tanguts are buddhists, Song scholars thought of ==Buddhism as religion of enemy==

  - story about Cai Wenji who lived in Han but abducted by Xiongnu, to show the barbarian culture of the northern people as compared ot Chinese architectures

    <img src="china_civ/image-20221018140826760.png" alt="image-20221018140826760" style="zoom:33%;" />

    however, notice that here they are dressing in Khitan = used as a symbol of foreignness = barbarianism of their northern enemies

- late Song had a lots of **problem**: big armies taking much expenditures, officials not selected properly, and taxes not collected properly $\to$ scholars believe that everything ==needs reform==

  - but unlike Wang Anshi's new policy, Song scholars aims to **revitalize in Confucianism $\to$ Neo-Confucianism**

    | <img src="china_civ/image-20221018141134032.png" alt="image-20221018141134032" style="zoom:33%;" /> | <img src="china_civ/image-20221018141216014.png" alt="image-20221018141216014" style="zoom:33%;" /> |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |

    - even during Age of Division and Tang when there are a lot of Buddhist, Confucianism technically is still used as texts for exams

    - but those scholars dismissed them and said Confucianism is lost since after Zhou, that it was all about Buddhism and Taoism. Hence they considered themselves as revivalists = **reinterpret the traditional Confucianism core = becomes a dominant idea among Song intellectual**

- **Zhu Xi** (as a representative and consolidator) for Neo-Confucianism

  - Buddhism had life cycles, and hence Confucian also had Metaphysics about *li*  (principle) and *qi* (vital energy)

    - believes that nothing can exist without *li*

  - used the two concepts to explain human's good nature. Humans born with good *li* but *qi* is impure = hence people do bad things

    - sages had perfect *qi*, so people needs to work on this

  - Zhu Xi's take is that *li* is more important $\to$ how do you **investigate things**

    - rigorous study to probe *li* in things
    - your methods of study should include nurture reverence + pursue knowledge

  - the ultimate goal is **self-cultivation**, which is believed to lead to harmonious society (if everyone does it). But how to cultivate it?

    - Zhu Xi's idea is to investigate things, hence his school = ==*Li Xue* = a mainstream Neo-Classical school==

  - since Zhu Xi believed one needs to study, he helped establish private academies = rise of private schooling for transmitting new ideas

    - also wrote *Commentaries on Analects*, which is later made an official text in 1241 until 1905

    <img src="china_civ/image-20221018142312659.png" alt="image-20221018142312659" style="zoom:33%;" />

  - since Zhu Xi wanted to revive, he called himself *Daoxue* or *Lixue*, they don't called themselves Neo-Confucianism

- even though those new thoughts is a response of the time of crisis to attack Buddhism or Taoism, but they are **not immune to their influence**, 

  - e.g. still use Buddhist practices such as medication and adopted Buddhist terms in Neo-Confucianism
  - i.e. has a lot of elements drawn from them

**Song Women's Status**

- pretty complicated
  - commodification of women, selling daughters gives farmers a ton of money, to be prostitutes
  - foot-binding, which started in the sing-song class, but eventually expanded throughout = small foot had sexual allure for men
  - but women can also use that to their advantages, and some had autonomy, such as in the reading that some women had words that judges listen to

- foot binding in Song. women having small feet-only seen by herself and husband (264)
  - correlates with the growth of Song's commercialization, and that families sell daughters + people kidnap women
  - women transformed into commodities who were bought and sold on the basis of appearance
  - women of good families compete with women with bound feet, hence 

# Midterm

- three main parts
- Identifications of people, places, key concepts
  - e.g. equal field system
  - a list will be given out, also explain the historical importance of this key term
- short answer questions
  - explain the deeper meaning/significance of a passage
  - e.g. a paragraph long
  
- one long essay
  - e.g. half of the test
  - e.g. interaction with steppe normadic culture, e.g. explain *shi* class in warring states and implications in later dynasties
  - e.g. bidirectional influences between stuff
  - like three to fourth paragraphs, and you need to provide a structured, argument-based response

# Yuan Dynasty (1271 - 1368)

> Yuan was a **Mongol-led imperial dynasty of China** and a successor state to the Mongol Empire after its division. It was established by ==Kublai== (Emperor Shizu), leader of the Borjigin clan, and lasted from 1271 to 1368. 
>
> - Although ==Genghis Khan== had been enthroned with the Han-style title of Emperor in 1206 and the Mongol Empire had ruled territories including modern-day northern China for decades, it was not until 1271 that ==Kublai Khan== officially proclaimed the dynasty in the traditional Han style
>   - Genghis Khan = Chinggis Khan later in text
> - Kublai's conquest was not complete until **1279 when the Southern Song dynasty** was defeated in the Battle of Yamen. His realm was, by this point, isolated from the other Mongol-led khanates and controlled most of modern-day China and its surrounding areas, including modern-day Mongolia.
> - Mongol rule was ==cosmopolitan under Kublai Khan==. He welcomed foreign visitors to his court, such as the Venetian merchant Marco Polo, who wrote the most influential European account of Yuan China
> - In 1368, following the **defeat of the Yuan forces by the Ming dynasty**, the Genghisid rulers retreated to the Mongolian Plateau and continued to rule until 1635 when they surrendered to the Later Jin dynasty (which later evolved into the Qing dynasty). 

How did the ==Mongols== managed to conquer all China during ==Yuan==?

- they didn't have literati, didn't have language, didn't have permanent houses

**Raising Warriors**: especially women warriors

- can reveal Mongols political, cultural and social priorities
- Eurasian steppe: resource scare, nature is volatile, human is small compared to nature
  - generating only a minimal surplus, and internal disputes arises due to resource scarcity
- therefore perpetual uncertainty, **hard on both women and man, little time on inequality**
  - nomad = ==group is prized more== important than individual survival = young children were taught a universal skill set
  - has highly non-gendered training, e.g. horsemanship, women and men equal competence
  - herd and hunt typically taught by mothers, e.g. Chinggis Khan's mother
- learned bravery during hunt = **bravery in battle**
  - horsemanship, bravery, shooting skills for all
  - e.g. Alaqa Beki, a **women leader of large army**

**Groups Win Wars**: prizing group advantages

- military tactics through surprise, ambush. Hence also requires soldiers strong in acrobatic skills

- **caracole** tactic:
  
  <img src="https://qph.cf2.quoracdn.net/main-qimg-9a7d2ad9f1f7cd2e072ed4f3f53e32c4-lq" style="zoom:33%;" />
  
  - horses are faster in group
  - shoot at 40-50m = general kill zone, so every warrior can contribute to the arrows
  
- **feigned** retreat
  
  <img src="https://rud.is/khan/uploads/4/6/9/3/46935069/876421879_orig.jpg" style="zoom:33%;" />
  
  - appear to be overwhelmed and flee, to **lure** enemies to ambush location

**Climatic Changes**

- 15 year **Pluvials (increased rainfall=wet) climatic** favored Mongol
  - to being able to support a **record-holding amount of animals** (e.g. horses) due to biomass flourishing. So many perhaps more than the rest of the horse in the world combined
  - zero foot-soldiers, or supply chain, but each horseman have **4-5 well-trained horses**
  - those horses would respond to the unique whistle of the owner
- soldiers can sleep on their horses
  - place their meat underneath saddle
  - can also train blood from horses as calorie intake if needed

**Semi-private/public Ger**

<img src="https://www.youngpioneertours.com/wp-content/uploads/2020/01/Ger-construction.jpg" style="zoom:33%;" />

- Ger is a single room structure (tent)
  - often entire family lives in a single Ger. This means sometimes many people all into 100 sqrft space, for both women, men, and kids
  - hence also little *room* for inequality
- Ger is Mongol's only structure shielding from nature
  - place for everything, all resources
  - technically **owned by the female**

**Buying Loyalty**

- Chinggis dissolved conquered population to create a structure for him as head of power
  - then perform military integration = some of them become part of the army
  - each person loyal to **their direct superior**, and is recursive until the top, which is Chinggis
- so ultimately each gifted population (along with their stuff) is still owned by Chinggis
  - those owners then need to populate the armies
- so source of income is plunder or tribute

**Women in Mongols' Political Sphere**

- chiggis wealth transfer to the youngest child
- not only were women are entitled to power, men also believed in their power
  - women power has nothing to do with proximal relationship with men, as in early China (e.g. Zhou dynasty)
- women in general (including Chinese) enjoyed a more fairer status:
  - mongol named a women **Yang Miaozhen** (successful and fearful leader of army) to the position of provincial governor - probably the highest power ever to be held by a civilian women in imperial China
  - women has power in conducting family affairs (334-336). e.g. Zhaos wife, **Guan Daosheng**, has a commanding role in household management
  

**Extreme Violence Saves Lives**

- e.g. Persia killed Mongol's envoys, so Mongol went on to decimate them
- believed violence during warfare is a necessity, so they wanted to **spread their image of violence**
- so a town under their attack can
  - surrender and pay tribute (survive)
  - not useful become slaughtered (decimated)
  - the useful, such as artisans were saved to improve techniques. Or a few terrified were allowed to flee and spread this terror (survive)
- mongol invasion in south is slow
  - instead of plunder/kill farmers, tax them was proposed

**Mongol Ruling China**

- The Yuan dynasty created a =="Han Army"== () out of defected Jin troops and an army of **defected Song troops** called the "==Newly Submitted Army==" ().

- as many women (still physically strong) start to shift more to political poewr
  - initially 30% are Mongol women in army, but now forces are not even many Mongols

- Kublai wanted to also focus on governing in China
  - eventually adopting alternating policies favoring their own steppe culture and sedentary culture of the vast population of Chinese

- no evidence that Kublai converted to Buddhism, nor Confucianism
  - only a rudimentary knowledge of Chinese, hence certainly not Confucianism
  - sensitive not to rely on Chinese, but military ones in western/central asia
  - **suspended the examination system** = no group can challenge Mongols
  - only later ==emperor Ayurbarwada== **restored** the civil exam system due to his tutor teaching Confucius and he liked it
- Kublai considered **South Chinese** the least trust worthy group, hence at the bottom of four divisions 
  - Specifically, the ==four classes== of people by the descending order were Mongolian people, Semu people, Han people (in the northern areas of China) and Southern Chinese (people of the former Northern Song Dynasty)
  - perhaps because Southern Chinese are the latest to be conquered
  - prioritize economic growth, some 

- Kublai nevertheless **sought to keep good relation to both Chinese and Mongols**
  - as ruler for Mongol sought to preserve martial values, and having mongol women as concubines
  - as ruler for Chinese, established capital in China and subsidized Chinese art
  - in reality, even only for 100 years it is still **hard** to both rule sedentary China and steppe Mongol. 

**Fall of Yuan**

- However, eventually depopulation in China and increasing stress on people caused **rebellion**
  - Kublai launched huge campaign using sedentary Chinese are army = many **died**
  - increasing amount of burden + natural disasters (drought) + ineffective government policies inflicted **struggle, famine in people**

# Ming Dynasty (1368 - 1644)

**Previously on the Mongols and the Yuan** (from Chinese perspective)

- Tribal confederations became a major threat
  - Throughout the period of the Southern Song (1127-1279) Mongol confederation under Genghis Khan gaining strength in the north.
  - Mongols being effective in warfare, e.g. brutality
  - embrace new war technologies from captured artisans
- migration possibly encouraged by climate changes
  - high productivity of horses, on average each mongol solider has 3-5 horses
  - Eurasian steppe nature, hence a small population = lose control when large territory
  - i.e. problem of **minority rule**: assimilation v.s. their elite identity also needed to be balanced
- Mongol's nomadic culture + small population = both women and men trained equally
  - no need a supply chain, just plunder and adopt technology
  - but also had succession crises
- Kublai Khan defeats Southern Song and established Yuan dynasty
  - first non-Chinese who rule the entire China both north and south
  - **divided people into 4 ethnic group and favored non-Chinese** = arise Chinese ethnic consciousness and desire to regain China = eventually lead to Chinese rebellions = ==downfall of Yuan==
    - recall that they were military strong in horseback but as the creation Han Army and Newly Submitted Army made many mongols become sedentary + a much smaller population, it becomes hard for them to hold the vast population of Chinese
- Yuan was overall political unstable but flourishing time for culture
  - e.g. Yuan drama, literature, blue and white porcelain

## Ming Autocracy

**The strangeness of Early Ming**: despotism and an agrarian vision of economy and society

<img src="china_civ/image-20221027132235822.png" alt="image-20221027132235822" style="zoom: 33%;" />

- smaller than Yuan as Mongolia is not in Ming's control, and captial at ==Nanjing== (capital during Six dynasties)

  - rule both north and south from a southern city for the first time

- Founder ==Zhu Yuanzhang==, with reign title ==Hongwu== (1368-1398)

  <img src="china_civ/image-20221027132453072.png" alt="image-20221027132453072" style="zoom: 33%;" />

  - Hongwu reign year 1 to represent 1368
  - reign title chosen by himself, because of his **military achievement: Ming has expanded** and included corridor area in north and sixteen prefectures (later Beijing become capital under YongLe)

- An extraordinary life experience of **Zhu Yuanzhang**

  <img src="china_civ/image-20221027132703443.png" alt="image-20221027132703443" style="zoom:33%;" />

  - parents were poor, and was a wondering monk for several years
  - Mongols burnt down the monasteries, and he joined rebels
  - **commoner to become an emperor since Liu Bang**

- part of his experience in his own words (explains why he shaped a ==conservative== vision as government/==weird early Ming==)

  <img src="china_civ/image-20221027132909702.png" alt="image-20221027132909702" style="zoom:33%;" />

  he claimed to "restore S. Song style", but **kept many Yuan policies** (conservatives)

  - emperor to bureaucracy below him is much more hierarchical/autocratic, and strong personal subordination to emperor

    - did not hesitate to have officials beaten openly = very despotic
    - became a regular mean to **punish officials**: rarely used in Tang, but **Mongol did use it frequently**

  - paranoid for treason: frequent execution of officials

    - **great purges:** zero toleration of corruption and disloyalty. Executed over 30,000 victims
    - **didn't trust prime ministers**, made himself the chief director for large and small matters and **removed "Great Chancellor" as a rank**

    <img src="china_civ/image-20221027133324396.png" alt="image-20221027133324396" style="zoom: 33%;" />

    for example, the first and fourth is due to (he believes is) disloyalty, and the 2nd and 3rd for corruption.

- emperors are now complete autocrat. related to Mongol's style,

  - comparison of the past: officials were somewhat confident and powerful in decision making
  - In Tang they sat in chairs, in Song not required to kneel (also vowed not to execute officials even bad news/disagreement). But in **Ming, a lot more ==autocratic==**

  <img src="china_civ/image-20221027133538707.png" alt="image-20221027133538707" style="zoom:33%;" />

**Despotism**: new government structure

<img src="china_civ/image-20221027175108011.png" alt="image-20221027175108011" style="zoom:33%;" />

- this despotism in emperor dominating over bureaucracy is highly related/==originated== by

  - decline of hereditary aristocracy since Huang Chao rebellion = recruit based on merit/little family influence = earlier those **aristocrat families are well venerated** and hence easily **exerts strong influence even in court**
  -  the rise of civil service exam and the above = aristocrat "class" are flexible, can **hardly create equal influence**
  - the only **special case is during Song**, when emperor explicitly said not to execute scholars if bad news/disagreements hence those recruited by merit base can have influence
  - his peasant background also made him have dislikes of anti-intellectuals

  - though Emperor Zhu Yuanzhang did not have civil system for a decade but eventually needed help = depended on them, so a **power balance is subtle**

- since Tang to Yuan, government structure were somewhat similar. But in Ming

  |                        Tang and Song                         |                             Ming                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221027134122694.png" alt="image-20221027134122694" style="zoom: 50%;" /> | <img src="china_civ/image-20221027134246693.png" alt="image-20221027134246693" style="zoom:50%;" /> |

  - before, **grand chancellor** (developed in Song, e.g. Wang Anshi) have a high degree of influence in decision making, but now is removed
  - strong centralization of authority in the throne
    - abolished Grand Chancellor = Prime Minister
    - military bureau divided into 5, and censorate into 12. In total 23 ministries **directly reported to Emperor**
    - **grand secretariat** = mid level officials = good at writings as helpers
      - after Zhu Yuanzhang, they gain power and even reject emperor's decisions in the long run = basically functioning as prime minister
    - this induced a **large workload** for emperor, and seem only possible with Zhu Yuanzhang

**Social reorganization at the local level**

- Zhu Yuanzhang liked rural areas as being less corrupt and simple + close community of farmers

  - very different in Song time's commercialized farmers

- therefore, developed an **ideal economical life being "rural"**

  - aimed for rural self-sufficiency
  - developed the **Lijia  system** (+ registration system for direct control of people, like Qin period)
  - developed the "**Sacred Edict of Six Maxims**" (for spreading Confucius teachings)

- ==Lijia (hundred-and-tithing): village registration system==

  - assign people with land and get taxes as well. so that
    - 10 households = 1 *jia*, and 10 *jia* = 1 *li*. Hence 100 households per *li*
    - additional 10 wealthy ones in charge of managing hence 110 households
  - the idea of **self-supervision + direct report** to emperor was raised by Shangyang in Qin
  - registering households as done similarly by Mongol, which 
    - origins as early as Qin (which was for recruiting **military**). 
    - In Song, Wang Anshi also employed a a similar policy for governmental interference in **economy**
    - but Zhu Yuanzhang wanted this more for ==control== **of people** (the book yellow registers)
  - effectively transfer govern responsibility to local community. Emphasized community self-management including tax-collecting, transportation, judicial, police services, etc.
    - intent is a small government as peasants manage themselves = no need for high tax as well

- he also uses Lijia to teach people about the laws and Confucianism

  <img src="china_civ/image-20221027135538558.png" alt="image-20221027135538558" style="zoom:33%;" />

  - "sacred edict" read aloud regularly, people urged to behave with **Confucian values such as Filial Piety**
  - very effective mean of imbuing people (including illiterate) with Confucius values

**Four Categories of People**

- Mongol had this division: 1) Mongol; 2) Semu = colored-eye people, from west/central Asia; 3) Northern Chinese; 4) Southern. Reveals how *much* Mongol confederation trusted each group

  <img src="china_civ/image-20221027135758073.png" alt="image-20221027135758073" style="zoom:33%;" />

- in Ming, he also **categorized peopled base on occupation SPAM**

  - but later this ==doesn't work for economy==, and broke down with high commericialization later
  - attempted to demarcate social boundaries, e.g. merchants, farmers, must do different things. But since Song, **people are highly dynamic**, as in commercialization, many specialization and trading/work across occupations. So that was a failure of his policies

- recall that exams during Yuan was hard, so a lot of scholars changed direction and went to medicine

  - of course, getting a degree is still valuable, but as more and more people receive education and exam even more intense, a lot of **novel occupations/diversified future of scholars**
  - but still, Southern literati elites regardless held immense influence in local society = **localist turn** (started in Southern Song). e.g. story of cruel family getting hold of the entire *local* community
    - before, passing the exam = route for wealth, status and fame. But now they realize this is not the only route and can **exert influence on a local level**. (which is very related to neo-Confucianism - build society from local level) = establishment of ==local roots==
    - for them, civil exam is now just among the many exams, but just sitting for the exam still gives status.
    - so south weathered the Mongol invasion much better than the north thanks to their established local roots = as they anyway didn't rely on the civil exam to get wealth = your life stays hereditary

- again, even as merchants at the bottom category, they are often the wealthiest, e.g. Shen Wansan (a merchant)

  <img src="china_civ/image-20221027140231319.png" alt="image-20221027140231319" style="zoom:33%;" />

  since education requires money, they are at an advantage: merchant can infiltrate by marriage into gentry (shidafu) family, or just adopt their sons

**Zhu Yuanzhang's successors**

<img src="china_civ/image-20221027140532108.png" alt="image-20221027140532108" style="zoom:33%;" />

- succession crisis after death. The correct line of succession should be Zhu Yunwen, but Zhu Di = **his uncle got it** (hence seen as "illegitimate")

- ==Zhu Di== gives himself the reign title ==Yongle==, and did several important things

- one major change is Nanjing being demoted to secondary captial and ==Beijing becomes capital==

  <img src="china_civ/image-20221027140744214.png" alt="image-20221027140744214" style="zoom:33%;" />

  - he felt insecure in the south (as he is an illegitimate ruler), and his generals are all from northern areas
  - demographically it is **strategically important to protect central plain from north but not safe**
  - it is also not very developed yet, so resources needed to shift from Nanjing/the south
  - but Beijing became the capital for the rest of the 7 centuries until today

- Yong Le's achievements

  - be ready for the Mongol: rebuilt sections of ==great wall==, and **grand canal** for transportation, expanded **military**

    - successive dynasties built and maintained multiple stretches of border walls. The best-known sections of the wall were built by the Ming dynasty

  - his palace at Beijing = the ==Purple Forbidden City==

    <img src="china_civ/image-20221027183224061.png" alt="image-20221027183224061" style="zoom:33%;" />

  - also extensive **maritime voyages: Zheng He**

  - sponcered a lot of **Buddhism**, but unclear if he is loyal to Buddhism as sometimes he pursues as Daoist. Perhaps as a tool to establish trust in his subjects

    - but did forge a strong relationship with Tibet

- Yong Le's sponser of Zheng He's voyage

  <img src="china_civ/image-20221027141241577.png" alt="image-20221027141241577" style="zoom:33%;" />

  - the aim was to mask his illegitimacy of ruling (as an uncle took power from the direct grandson of emperor), by **announcing to the maritime world that he is emperor**
  - therefore, voyage was for diplomatic reasons

**Admiral Zheng He**

- the mission was diplomatic: to all tributary states that Yong Le is now the emperor. Hence Zheng He also had military force brought along

- as a by product of the voyage

  - **enlarged trading routes** 
  - **furthered Ming's tribute system** (see next subsection)

- brought back some exotic info/products, e.g. giraffe is named as 

  <img src="china_civ/image-20221027141918382.png" alt="image-20221027141918382" style="zoom:33%;" />

- some of Zheng He's fleets routes

  <img src="china_civ/image-20221027141319992.png" alt="image-20221027141319992" style="zoom:33%;" />

**The Tribute System**

- was a old system. Two mechanisms of Chinese regimes **relationships with foreign states: tribute and trade**.
- the system:
  - foreign rulers to send embassies to China bearing tribute in the form of local exotica
  - emperor in turn gives gifts of equal or greater value; bestowed titles on tributary rulers; and permitted a controlled volume of **trade**
- as it is mutually beneficial:
  - a device for **mutual recognition**, and to establish **political and cultural prestige of China internationally**
  - more **trade** opportunities
  - in most times both parties are happy

**End of Yongle Fleets**: only YongLe emperor promoted maritime trade/voyage

- expeditions cost a lot of money: in 1424, Yongle died, and new court considered the expeditions as **wasteful expenditures** that did not benefit the peoples welfare.
- eventually people switched focus to their northern land: shifted policy to ==focus on northern steppe enemies==
  - 1449 Tumu disaster: Mongol Oirats capture the boy emperor and threaten Beijingthis marks permanent shift in foreign policy away from the sea and towards the steppe.
- hence this end also signals the **ended the state-sponsored sea voyages and foreign contacts/trade overseas**. Instead of maritime trade, a lot became used for defending

## Ming and the Global Economy

**Previously on early Ming**

- **Hongwu (1368-98):** **autocracy** and inheritance of Mongol polices + social reorganization (agrarian view)
  - autocracy of emperor in court is supported by the decline of aristocracy + rise of civil exam in Song
  - social reorganization involves demarcation of people into four classes $\to$ did not work at the end as society often have boundary crossings
- **Yongle** (1402-24): usurper = deeply concerned of his ruling = moved capital from Nanjing to Beijing
  - build the forbidden city in Beijing
  - eager to pursue **tributary relations** with new states = recognize his legitimacy of ruling
  - for a similar reason above, sponsored Zheng He's voyage
- **Zheng He**'s voyage: massive expedition primarily as a diplomatic tool
  - a by-product is to encouraged **private maritime trade** (commercialization later)
  - but costly + enemies in the north made China shift focus

**Monetization in mid-Ming**: second wave of commercialization after Song dynasty

- e.g. description of the prosperity and commercialized economy in south

  <img src="china_civ/image-20221101131958629.png" alt="image-20221101131958629" style="zoom: 33%;" />

  material property + commercialization = non-agrarian society starts to come back again

  - Hangzhou was also reasonably prosperous, as previous Song capital
  - ==Jiang'nan== = south of Yangzi river = **most prosperous region** in mid-Ming = lots of records come from this area

- recall that *Lijia*  (hundred-and-tithing): village registration and self- management system + **tax system in kind (e.g. rice if produced rice)**

  - assumption that people stays in the land, and had loopholes = corruptions = less tax collected from wealthy people who try to evade

- more effective policy to **collect tax all in "cash"** = ==Single Whip Reform== (1570)

  - pushed forward by Grand Secretary Zhang (recall that grand secretary is very powerful in court)
  - complete resurvey of all cultivated land (1580) = updated information = can ask ==all taxes to be paid in copper/silver==
  - aims to simplify taxation and increase efficiency = very effective later
    - 1568-73 Imperial Treasurys annual deficit: 2-3 million taels of silver
    - 1582 The treasury has 4 million taels of silver surplus and grain reserve for a couple of years
    - critical to **help Ming dynasty to reach its peak**

**Currency of the Hongwu Reign**

- paper money is issued by the government, but they are printed/produced without backup hence people lost faith in it:

  <img src="china_civ/image-20221101133338779.png" alt="image-20221101133338779" style="zoom:33%;" />

  - for tax: payment needs to be in copper coin/silver

  - people lost faith in paper currency because ==Zhu Yuanzhang== issued paper money **without backup** $\to$ can use silver to exchange paper money but when asked to *exchange from paper money to silver, government cannot issue*

  - regardless, government encouraged paper money = wanted to restrict digging silver. But they never succeeded in controlling its currency = **ends up relying entirely on copper coin and silver ingot**

    <img src="china_civ/image-20221101133404657.png" alt="image-20221101133404657" style="zoom: 25%;" />

    (note that silver ingots are trusted more as copper can regularly debase)

- thirst for silver ingot = encouraged for ==global trade== (foreign countries using silver to trade for valuable Chinese goods)

  - lots of **foreign private trade** activities from European

    <img src="china_civ/image-20221101133810017.png" alt="image-20221101133810017" style="zoom:33%;" />

    recall that:

    - Ming government did not sponsor maritime trade (**opposite in Song's altitude = took large revenue from maritime commerce**)

    - this is perhaps because a) gov charged light tax in commerce b) agrarian view c) private maritime trade tied with piracy = political problems. Hence very suspcious with private trade outside diplomatic reasons

  - trade with ==Portuguese==: arrived and found a vigorous trade network in China

    - some maritime traders treat Chinese ships as competitors = kill them and take over their business

      | <img src="china_civ/image-20221101134216818.png" alt="image-20221101134216818" style="zoom: 25%;" /> | <img src="china_civ/image-20221101134237483.png" alt="image-20221101134237483" style="zoom: 25%;" /> |
      | :----------------------------------------------------------: | :----------------------------------------------------------: |

      this caused some problem: where China official did not support trading with Japan due to piracy and suspicions, those Europeans don't care hence those Chinese goods are illegally leaked to Japan

    - but eventually, local Chinese officials sees trade benefits and defied government decisions = Portuguese conspired with local officials to **establish a port in Macao** (a backwater at the time) to trade with Canton and Nagasaki

      <img src="china_civ/image-20221101134506378.png" alt="image-20221101134506378" style="zoom: 25%;" />

      Macao = first and last European colony in China ruled by Portuguese

- what goods did European liked from China? some trades included porcelain = Chinese artisans mass produce porcelain specifically for foreign trades:

  <img src="china_civ/image-20221101134612595.png" alt="image-20221101134612595" style="zoom: 25%;" />

  most of exported ones are not best quality, and best ones often reserved for China's home market

**Trading with Spanish and Dutch**

- results of private maritime trades = period when porcelain appearing **popular in countries such as Dutch**

  <img src="china_civ/image-20221101154259734.png" alt="image-20221101154259734" style="zoom: 25%;" />

- both Spanish and Dutch also had high trade interest with China

  - early 1570s, the ==Spanish== established a trade port in Manila (Philippines), still far from China. 
  - in 1623-62, the ==Dutch== East India Company (VOC) occupies fort in northern **Taiwan**
    - note that at that time Taiwan was not under Ming's control. 
    - Once colonization by Dutch took place, Chinese went and Taiwan become incorporated into the Qing

- **Spanish** found China was silver hungry = traded/provided **a lot of silver** as they had extraordinary level of silver from mines in its colonies

  <img src="china_civ/image-20221101135157936.png" alt="image-20221101135157936" style="zoom:25%;" />

  Spanish ship silvers to China = established a stronghold in trading = on average more than 2,300 tons of silver arrived in China from abroad

  - note that: ==since 1493 - 1898 the Americas were invaded and incorporated into the Spanish Empire==, with the exception of Brazil, British America, and some small regions of South America and the Caribbean

-  in 16th  17th c., the trade imbalance with China was filled with silver from Spains New World colonies, and from Japanese mines

  - gold flow out, silver flow in = as silver seen more important in China
  - gives many bad consequences (as well as few good ones)

- **Consequence of the influx of silver**:

  - depreciation of silver = inflation;  market speculation; .. all contributed to **Ming's collapse** in the long run

  - immediate effects = ==rise of merchant== = wealth can surpass the gentry family = destabilize the social order

    <img src="china_civ/image-20221101135859684.png" alt="image-20221101135859684" style="zoom:25%;" />

    e.g. the above is a book written for gentry/scholar class to instruct them how one *spend silver in a way to not look poor* = anxiety of Confucian gentleman facing the threat of status change from merchant class

- **Consequence of foreign trade**: 

  - increase in ==population==

    - recall that Song population grew due to new tech for agriculture + new crop from Vietnam

    - similarly, in Qing: **new crop from Amiercan by spanish colonizers**. Maize, Sweet potatoes, etc, grows in places unsuitable for millet and rice and have high yield:

      <img src="china_civ/image-20221101140025257.png" alt="image-20221101140025257" style="zoom:33%;" />

      hence also helped the life of poorer people

  - Ming fascinated by **Europeans's ballistic technology**, e.g. cannon

    <img src="china_civ/image-20221101140323059.png" alt="image-20221101140323059" style="zoom:33%;" />

    many Chinese officials realized they are lagging behind in technology, and hence some advisers: should hire European governments to help defend northern enemies

  - famous **Jesuits** in China:

    <img src="china_civ/image-20221101140717697.png" alt="image-20221101140717697" style="zoom:25%;" />

    - impersonate as Confucian scholar to blend into Chinese society at first

    - aimed to convert Ming gentry to ==Christianity==, by reconciling Christian beliefs with Chinese beliefs

    - managed to **convert some but very small numbers to Christian**, as many Chinese gentries are more interested in European tech than religion

    - contribution to the world map = **remembered as a person introducing western info + tech** much more than as a preacher

      <img src="china_civ/image-20221101140916588.png" alt="image-20221101140916588" style="zoom:25%;" />

**A new cultural form: novel**

- recall that good economy usually results in prosperity of culture

- Ming poetry becomes less important hence less poets from Ming (and Qing), **but more into (fictitious) novels: 3 out of the 4 classics of Chinese literature produced**

  <img src="china_civ/image-20221101141147687.png" alt="image-20221101141147687" style="zoom:33%;" />

  - Journey to the west: Xuanzang's trip to see Buddhist scripts during Tang, but a fantasied novel
  - Three Kingdoms: is history
  - The water margin: set in the northern Song period = how a group of 108 outlaws trying to rebel the government
  - The Plum in the Golden Vase: 16th century work focusing on the corrupt noble in China = known for erotic reasons

- educated gentleman writes prose and fiction = those are more likely survive. The lack of novels in early time means novel might be there as well 

  - however, many novels are fictitious yet poems were real. Why are people suddenly interested in unreal things? Don't know
  - some say that it is due to Wang Yangming; others believe using easier language catered to a wider audience

**Changes in gender relations**

- overall, women facing ==increasingly intense subjugation== 

  - **female infanticide**: wanted to keep small family size, hence kills girls rather than boys who can labor and continue family line
  - revised inheritance and property laws = intended to help but reverse effect for women
  - female seclusion = find themselves **confined** and even not allowed to visit temples a lot
  - chastity cult = encouraged widow to be a widow. However, this was an ideal and in reality widow remarriage is common (as they lose properties from husband, due to Ming laws)
  - foot-binding. started in Song, but in Ming becomes more extreme. Accepted by women as a way to ensure good marriage

- confined inside hence they are socializing within themselves.

  <img src="china_civ/image-20221101142217363.png" alt="image-20221101142217363" style="zoom: 25%;" />

- but there are also some ==benefits==

  - **rising level of female education** + **men sponsored female readers & writers**. Perhaps because men are exerting so much control that they think nothing can go wrong by allowing these
  - **compassionate marriage**
    - perhaps better education means men can more easily communicate with women
    - worship of personal feeling and sentiments = influence of Wang Yangming

# Qing Imperialism

**Previously in Ming and the World**

- Ming Dynasty: large-scale trends in population growth, **commercialization (again since Song)**, and **global trade (private maritime trades)**
  - much global influence from private maritime trade. Discouraged by government due to piracy + political reasons + their northern enemies

- ==Single-Whip tax reform==: simplify taxation procedure, so that almost all are paid in cash (mostly ==silver==)
  - so that tax evasion becomes harder = increased government revenue
  - but **increased demand of silver**
- large demand of silver faciliated global trade
  - export Chinese manufactured goods such as porcelain, imported silver from abroad
  - also funded the rise of Atlantic (e.g. Spanish) empires
- along with silver, **Jesuit commissionaires** also came to China aiming to convert Chinese to Roman Catholicism
  - reconciling Christian with Confucian pratices, 
  - but Chinese are more interested in European science and technology rather than religion, hence not very effective
  - introduction of **western science into China**
- **Novel as a new literary genre**: vernacular language and a wide readership
- **Women more subjugated but increased education**
  - also more women publisher

## Manchu Conquest

**Decline of Ming**: several factors (importance is unranked)

- early period we had aristocratic emperor, but later also **inattentive emperor**
  - Emperor Wanli reigned for a long time but stopped attending to court affairs for 30 years
  - ruling often needs regulation and interventions; promotion needed his approval; But he simply did not care
- climate changes and ==natural disasters==, such as **famine**, **droughts**, epidemics, **cold spell,** etc. in the 1630-1640s.
  - was also the coldest decade in Russian history = poor agriculture due to the cold as well = famine
  - Beijing had frequent epidemic and grand canal become highways to spread
- **collapse of fiscal system** due to eventual shortage of silver
  - commoners do not have much silver to pay tax; govern cannot pay soldiers defending their borders
  - 80% of counties stopped forward taxation to central government
  - Europeans became reluctant shipping silver to China as time goes on = also contributed to silver shortage
- threats from **Mongol and Jurchens**
  - Ming also needed money to supply military = attempted to increase taxes, etc. all of which are non-popular approaches for the people
- all of those combined gave **revolts**
  - the two most prominent groups led by former soldiers Li Zicheng (1606-1645) and Zhang Xianzhong (1606- 1647) eventually captured the capital Beijing

**Rise of Jurchens/Manchus**

- recall that Jurchan were powerful **during the Song**: they established ==Jin== dynasty, helped Song to defeat Khitans but turned to sack Kaifeng later
  - Jurchans established the Jin dynasty but also become much sinicized in the past and their
  - Jin dynasty 1127 - 1200s, when later Mongols sacked and took control of northern China = Yuan dynasty
  - in 1600s, they see themselves as Later Jin hence a **political legitimacy** for restoration of power

<img src="china_civ/image-20221103132855563.png" alt="image-20221103132855563" style="zoom:25%;" />

- brief overview: The Jurchens are chiefly known for producing the Jin (11151234) and Qing (1616/1636-1912) conquest dynasties on the Chinese territory. The latter dynasty, originally calling itself the Later Jin, was founded by a Jianzhou commander, **Nurhaci (r. 161626), who unified most Jurchen tribes**, incorporated their entire population into **hereditary military regiments known as the Eight Banners**, and patronized the creation of an alphabet for their language based on the Mongolian script.
- they live in China's north plain, had long engagement with Chinese in terms of trade, tribute, ally/enemy

- Jurchans where **half-sendantry** = different from Mongols
  - had settlements, but also a lot of raids, enslaved captives, hunting and fishing, etc
  - profited a lot from trade with Ming: Chinese liked **ginseng**. 25% silver went to Jurchen because of the ginseng trade = they now have money to fund military
- rise of Jurchan as **Nurhaci brought up a strong tribal confederacy**

**Nurhaci: leader of Jurchen Confederacy**

- From the lineage of **Aisin Gioro**

- Ming's divide and conquer strategy actually made his own tribe stronger

  - first subjugated to Ming and used it to **defeat other tribes** to make his tribe strong
  - in 1595 Ming awarded him the title of Dragon-Tier General
  - eventually his tribe became the strongest, so **Ming turned against him**, and ==Nurhaci allied with Mongol tribes==

- in 1616 he declared himself the ==khan of the Great Jin== (restoration of Jin dynasty as descendents)

  - captial at Mukden, modern Shenyang
  - adapt Mongolian alphabet to the Jurchen speech

  <img src="china_civ/image-20221103133520903.png" alt="image-20221103133520903" style="zoom:33%;" />

  so Shenyang was the center of their political power

- another decisive step is the **Eight Banner system**: reconstituting social structure

  <img src="china_civ/image-20221103133613612.png" alt="image-20221103133613612" style="zoom: 25%;" />

  - each banner is a permanent group also a **fighting unit + unit of residence**

    - included dependants of fighting mens includeing women children and servants

  - hence **shared identity and shared clothing within a group**

  - over time, each banner becomes somewhat ethnic groups = close relationship within the group, but not purely one ethnic within a group

    <img src="china_civ/image-20221103133752655.png" alt="image-20221103133752655" style="zoom:25%;" />

**Hong Taiji's creation of Manchu Qing**

- succeeded Nurhaci, second Khan of later Jin
- declares himself emperor of the ==Great Qing==
  - Qing because Qing = clear, hence water related > Ming = fire (and moon)
  - this new dynasty name is a clear announcement of his intention to conquer Ming
- some measures he took to rule include
  - ordered translations of Chinese culture to Jurchen so he could study them
  - moved towards Chinese-style burearcarcy centralization
  - in 1636 he creates a new identity for his people called ==Manchu==
- one key thing to realize is that ==Manchus are also not one ethnicity== = rather amalgamation of steppe nomadic people + sedentary Chinese + etc. Therefore, Manchu as an "ethnicity" more like a **political construct**

**Manchu Identity**

- Manchu identity is **not determined by birth**, but by just being a member of the Banner

  - **a tool for integration**: shared identity overcame linguistic-ethnic differences of banner families that included Jurchen, Mongols, Korean, Chinese
  - **a tool for segregation**: emphasized unique claim of Manchus to rule China.

- They also emphasized identity markers such as hair styles and clothing, to be **different from that of Han people**

  <img src="china_civ/image-20221103134331632.png" alt="image-20221103134331632" style="zoom:25%;" />

  so men hair style become political tool as well: shows who you are submissive to.

  <img src="china_civ/image-20221103134354376.png" alt="image-20221103134354376" style="zoom:25%;" />

  examples of Manchu clothing is more diverse. e.g. soldiers uses a lot of bows

  |                           Soldiers                           |                        Qing Officials                        |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221103134659635.png" alt="image-20221103134659635" style="zoom: 25%;" /> | <img src="china_civ/image-20221103134730021.png" alt="image-20221103134730021" style="zoom:25%;" /> |

- Manchu also *tried* to forbid ==women== to foot-binding

  - but that ban didn't work: foot-binding practices are already very entrenched in Han Chinese minds, and it was seen as attractiveness/object of sexual desire
  - so most Manchu women don't bind their feet, but Han people still continue the practice
  - why no enforcing this change like with mens' hairstyle? perhaps because men already submitted, no need to change women
  - but the general trend in Ming and Qing can also be viewed as **tightening controls of its people = also of women**
    - Ming being active in domestic affairs with its Lijia policy, and using morality manuals (Sacred Edict of Six Maxims)
    - prior to Ming, pre-martial sex was not proscribed. But in Ming and Qing there are a lot of much focus on chastity = **interference** from government

**Qing conquest of Korea and Japan**

- wanted to at least prevent Korea to join Ming to fight against them, so basically **conquered Korea** (but left little cultural footprint)
  - initially, Korea see Jurchens as barbarians = many had Confucius thoughts
  - Korea also wanted to help Ming because, before when Japan tried to invade Korea, Ming helped.
  - but eventually Korea surrendered to Manchu due to their military strength and became **vassal state of Qing**
    - so no choice but to join conquest to Ming
- sees themselves as **repository Confucian hence named themselves soHwa**
  - as in fact it is true: Ming is the *last dynasty ruled by Han Chinese*
- Qing conquest of Ming also shocked Japanese
  - the Japanese sees this as Ka-i Hental = transformation of China from civilized to barbarian 
  - (recall that during Tang, they see Chinese as a model for culture and civilization)
  - so after this, Japanese also think that East Asian culture superiority is no longer in China, but in Japan

**Last Years of Ming empire**: there player. Ming, the Rebels, Hong Taiji's successor

- Ming asked Manchu to help quell the rebel, but eventually they had a bigger plan and conquered the whole China

<img src="china_civ/image-20221103135745712.png" alt="image-20221103135745712" style="zoom:25%;" />

- Qing took several yeas to conquer China proper, and expanded territories

  <img src="china_civ/image-20221103135823958.png" alt="image-20221103135823958" style="zoom:25%;" />

- Qing had **remarkably capable** emperors = ==High Qing==: **Kangxi** Emperor (r. 1654-1722) **Yongzheng** Emperor (r. 1722-1735) **Qianlong** Emperor (r. 1735-1796)
  - set most of the boundaries of today's China: Xingjiang, Tibet, etc

> **Kangxi**
>
> - The Kangxi Emperor is considered one of China's greatest emperors. 
> - He suppressed the Revolt of the Three Feudatories, forced the Kingdom of Tungning in Taiwan and assorted Mongol rebels in the North and Northwest to submit to Qing rule, and blocked Tsarist Russia on the Amur River, retaining Outer Manchuria and Outer Northwest China.
> - The Kangxi Emperor's reign **brought about long-term stability and relative wealth** after years of war and chaos. He initiated the period known as the "Prosperous Era of Kangxi and Qianlong" or **"High Qing"**, which lasted for several generations after his death.
>
> **Yongzheng**
>
> - A hard-working ruler, the Yongzheng Emperor's main goal was to create an ==effective government at minimal expense==. Like his father, the Kangxi Emperor, the Yongzheng Emperor used military force to preserve the dynasty's position.
> - Although Yongzheng's reign was much shorter than that of both his father (the Kangxi Emperor) and his son (the Qianlong Emperor), the Yongzheng era was a **period of peace and prosperity**. 
> - Yongzheng Emperor cracked down on corruption and reformed the personnel and financial administration
>
> **Qianlong**: 
>
> - As a capable and cultured ruler inheriting a thriving empire, during his long reign, the Qing Empire reached its most splendid and prosperous era, boasting a large population and economy. 
> - As a military leader, he led military campaigns ==expanding the dynastic territory to the largest extent by conquering and sometimes destroying Central Asian kingdoms==. 
> - This turned around in his late years: the Qing empire began to decline with corruption and wastefulness in his court and a stagnating civil society.

**Qing Conquest of Taiwan in 1683**

- recall that Dutch conquered in 1623 and established their colony in Taiwan

- brief history of Taiwan

  - Fall of Ming 1640-1650: people from Fujian started settling there, and **Ming loyalists retreated** to Taiwan

  - **Zheng Chengong** 1662: drove out the dutch and more=100k Chinese emigrated there

  - ==Qing defeated the Zheng regime 1683==: but allow it to exist autonomously and made it a **prefecture** (district under the government of a prefect) of **Fujian**

    <img src="china_civ/image-20221103140256769.png" alt="image-20221103140256769" style="zoom: 33%;" />

  - in 1870s, Taiwan became its own province as it is a key port for martime trade

  - 1894-1895 after Sino-Japanese war and WWII: **became part of Japan**. A lot of buildings today still have Japanese style

  - at the end of 1945-1949:  being the first leader of the Kuomintang (Nationalist Party of China), lost and followers immigrated to Taiwan. 

    - Thinking they can go back and recover, but obviously never happened. These are also considered as *Waisheng* and don't speak Taiwanese (now the "ethnic" gap is much smaller)
    - since many Waisheng are governments, so they are also kind of upperclass in Taiwan

- therefore, Taiwanese very much takes over from *Mingnan* dialect from Fujian

- but again, Qing in general had loose control over newly conquered regions

**Qing conquest of Dzungar Mongols**

- western Dzungar dmoinated the west euroasia in 1630
- long batter with Dzungar Mongol, and **subdued Dzungar Mongol in 1696**
- but a lot of revolt by Dzungar Mongols = Qing launched genocide in 1750s
  - effectively **ended the northen Mongol problem**

**Qing conquest in Tibet**

- Dzungar mongols tried to expand into Tibet = hence they invaded in 1717
- so Qing invaded Tibet and ==installed a pro-Qing Dalai Lama==; Tibet became a protectorate
- but again little interference in Tibetan local governance: hence maintained Tibetan culture

**Qing creation of Xinjiang**

- in the Han dynasty, some chinese troops stationed there, but not in Tang and Song as Chinese had lost control

- finally re-acquired by the Qing = again Qianlong's troops basically **massacred Dzungar troops and completed subjudation**

  <img src="china_civ/image-20221103140836249.png" alt="image-20221103140836249" style="zoom: 25%;" />

- after this conquest, in 1768 announced the formal annexation = **Xingjing, meaning new dominion** 

  - but again, territory under control of Manchus, but **local poeple also allowed to contine their practice**
  - no restrictions as with Han chinesemen

- however, maintenance of far territory is also a **financial burden**, e.g. military spending. Therefore, perhaps most effective use of Xinjing in Qing is as a **penal colony = 10% officials in Xingjinag as punitive banishment**

**Managing large territory and diverse people**

- ==Qianlong== assumed different persona to different audiences = portray him in many different costumes

  |                         as Manjusri                          |                         as Confucian                         |                  as European-style Warrior                   |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221103141308857.png" alt="image-20221103141308857" style="zoom:33%;" /> | <img src="china_civ/image-20221103141345240.png" alt="image-20221103141345240" style="zoom:25%;" /> | <img src="china_civ/image-20221103141428175.png" alt="image-20221103141428175" style="zoom: 33%;" /> |

  e.g. as Manjusri = appeal to Laima in Tibet, or as Confucian scholar when speaking to Han Chinese

  - **acknowledges culture differences** and ethnic distinctions, also especially wanted to **preserve his Manchu ethnicity**

- so in general, Qing are interested in conquering *new* lands but **not governing those *new* land**

  - global history: Russian expanded east-ward: time of imperial land grabbing = have as much land as possible = control over resources
  - but remember those from Qing are not peaceful conquest = e.g. Xinjiang lots of people massacred due to dissent. So still insisting on their **political dominance but not really in establishing culture statues**

**Euroiserie in China**: Yuanming Yuan built near a lake outside Beijing

- imperial palace began construction in 1707, added a number of **western** style building with the aid of **Jesuit architects**

- unfortunately destroyed by British and French in 1860. Historical drawings shown below, and attempts of restoration in progress

  | <img src="china_civ/image-20221103141816162.png" alt="image-20221103141816162" style="zoom: 33%;" /> | <img src="china_civ/image-20221103141923398.png" alt="image-20221103141923398" style="zoom:33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

## The Qing in Crisis: Opium & Western Imperialism

**Previously on the Qing Conquest**

- Decline of Ming and rise of Manchu Qing
  - Ming facing adverse climate, peasant rebellion, etc
  - Peasant rebellions give opportunity to Manchus from NE China, descendants of Jin
- Qing did **really well** and controlled China for almost 3 centuries
- Qing used the **Banner System** which constructed a "political" ethnicity
  - a banner group represents a unit of a military organization/ unit of residence
  - joining to be a Bannerman = being a Manchu = an "invented" ethnic group
- Manchu Qing emphasizes that Manchus were ==not just Sinicized, but regarded Qing as a multiethnic empire==;
  - e.g. Qianlong with multiple faces, different policies, etc.
- ==High Qing:== **territory** reached the greatest extent
  - 18 century: unrivaled standard of living and invention flow more from east to west
  - 19 century: outmatched by western countries

**The Canton System**: Qing's international trade

<img src="china_civ/image-20221110132343392.png" alt="image-20221110132343392" style="zoom:33%;" />

- Qing **legalized private maritime commerce**, though restricted to Canton

  - there were many prosperous trade ports, but all abandoned by people and finally went with **Canton = most dominate place in sino-western trade**
  
  <img src="china_civ/image-20221110132453325.png" alt="image-20221110132453325" style="zoom:33%;" />

- wanted to control western influence

  - control with inflow of western foreigners: fear illegal infiltration of western missionaries
  - therefore, in 1757: Canton the sole port open for Western trade
  - 1760 court imposed **regulations on foreigners** (e.g. their mobility) in order to keep them from disturbing Chinese
    - even when trading, they need to deal with designated Qing merchants (hong)


**13 Hongs of Canton**: 

- Chinese merchants controlled business and **monitored European trades**

  - Hong = A hong  originally designates both a type of building and a type of Chinese merchant intermediary in Guangzhou (formerly known as Canton), Guangdong, China, in the 1819th century, specifically during the Canton System period.
  - foreigners can only spend a few weeks in the Hong, then return to Macao, which was still a Portuguese colony

  <img src="china_civ/image-20221110132833326.png" alt="image-20221110132833326" style="zoom:33%;" />

- Wu Bingjian  (17691843), known as Howqua: ==most powerful merchant at the time==

  <img src="china_civ/image-20221110133615919.png" alt="image-20221110133615919" style="zoom:33%;" />

  - all Chinese home merchants belong to a guild: Cohong  =  responsible for policing the trade with westerners
  - granted right to monopolize import and export trade, but in turn **guarantee to government all duties and proper behavior of western**
  - he also involved in the opium trade = people in China today therefore have a fixed feeling towards him

- during 18th century, **industrial revolution in Britain made it became leading commercial nation** with colonies (e.g. in India)

  - English trade with China monopolized by British East India Company = allowed by Queen Victoria
  - but this powerful company is still restricted by Chinese Hong in Canton

> The **Industrial Revolution** was the transition to ==new manufacturing processes in Great Britain, continental Europe, and the United States==, that occurred during the period from around 1760 to about 18201840.
>
> - This transition included going from **hand production methods to machines**, new chemical manufacturing and iron production processes, the increasing use of steam power and water power, the development of machine tools and the rise of the mechanized factory system. 
> - Output greatly increased, and a result was an **unprecedented rise in population and in the rate of population growth**.

**Macartney Mission of 1793**: 

- British **wanted to improve profits/trade footholds in China**, e.g. get tea cheaper and establish footholds to store goods 

  <img src="china_civ/image-20221110134153766.png" alt="image-20221110134153766" style="zoom:33%;" />

  - hence wanted China to abandon tribute system = towards more in how European deal with each other

  - a more direct trade. Free trade + **Adam Smith = Wealth of Nation** = justify British idea to ask China abandon Canton system

- requests include

  - open commercial warehouse in Beijing

  - sought extra territoriality:British nationals exempt from the Qing legal jurisdiction

  i.e. wanted to revise the canton system to **trade more freely in China**

- Therefore, missionaries sent to China brought some interesting European goods but Qianlong is not interested in

  <img src="china_civ/image-20221110195847636.png" alt="image-20221110195847636" style="zoom:33%;" />

  notice that in this painting westerner **tried to portray Chinese as despotic and un-understanding**, because Qianlong (in his 80s) decisively **refused all British request** in a very condescending/arrogant tone:

  <img src="china_civ/image-20221110134503496.png" alt="image-20221110134503496" style="zoom:33%;" />

- Why did Qianlong reject those requests?

  - fundamentally, those requests themselves are somewhat went overboard.
  - perhaps Qianlong is not interested in western goods? Note that they **did collected a lot of western gifts from other countries**, so it is what presented was not new = China didn't really need British manufactures
    - however, later on the major problem is his ==slight on western (military) tech==

  - perhaps because the missionaries did not kowtow? Recent research shows that obsession of the ritual does not come from Qianlong, but the missionaries

- Why was Qianlong so arrogant?

  - indeed, China was powerful as there is a huge trade imbalance: silk and porcelain, and tea became drink of choice in Britain

  - **large amount of silver flow into China**,

    | <img src="china_civ/image-20221110135252789.png" alt="image-20221110135252789" style="zoom:33%;" /> | <img src="china_civ/image-20221110135349779.png" alt="image-20221110135349779" style="zoom:33%;" /> |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |

  - so British had to have plans of **coming up alternatives commodity to trade**:

    - cotton + silver from new world
    - tried to re-sell tea to the US to get profit: **Boston Tea Party**. The target was the Tea Act of May 10, 1773, which allowed the British East India Company to sell tea from China in American colonies **without paying taxes** apart from those imposed by the Townshend Acts. Hence is unfair and Americans dumped/destroyed entire shipment of tea
    - Finally, British turned to opium

- Opium trade with China

  - opium was not new, but only used for medical purposes before
  - 17th century: smoking opium and tabacco caught on in China, just like how British took tea.
  - The **great demand of opium** then solved the trade deficit with Britain
  - 19th century: net outflow of silver from China due to opium trade

  <img src="china_civ/image-20221110135724678.png" alt="image-20221110135724678" style="zoom:33%;" />

**Opium Imports from India to China**

<img src="china_civ/image-20221110140059672.png" alt="image-20221110140059672" style="zoom:33%;" />

- opium became major product in China over time
  - EIC monopoly ended and private trade (e.g. Russel & Company) begins, even more opium (like freedom of trade)
  - e.g. lots of American gained most fortune from China trade

- then obviously this caused a lot of **problems**
  - **2 million addicts in China**
  - difficult for Chinese to enforce ban
    - attempted 1729 first opium ban; 1800, banned importation and domestic production
    - however, this **did not work** (even open trading disappeared) because there are lots of European **smuggling + corruption** from local officials and **underground networks**
  - since opium was illegal, Qing made no tax revenue from it
  - net silver flow out = silver shortage soon caused problem in money supply
    - 2 million taels deficit in 1820s and 9 million in early 1830s

**The Opium War and its Aftermath**

- opium war fundamentally changed the relationship between China and the Western
  - before, China shows **dominance** over western power, like how most civilization in centered in China and how Mongol/northern tribes conquered lots of territories, and establishing **tributary/vassal states**
  - now, difference in technology made **China lag much behind western in power**

- Emeperor appointed ==Lin Zexu== to destroy opium at Humen

  <img src="china_civ/image-20221110140939345.png" alt="image-20221110140939345" style="zoom:33%;" />

  - Charles Elliot (Chief Superintendent of British Trade in China) promised to **compensate the merchants after confiscation, but did not happen** after Lin confiscated them
  - hence English commercial interest **pushing war** with China, so Lins confiscation of opium became a justification for war

- Qing navy was ==ill prepared against British navy forces==

  - China had no significant government input in maritime tech after Zhenghe's voyage
  - in general, low tax imposed = low research fund for strong navy power
  - ended up having fully iron warship from British v.s. wood ship in China

  <img src="china_civ/image-20221110141548241.png" alt="image-20221110141548241" style="zoom:33%;" />

**Treaty of Nanjing**: defeat and humiliation

- China lost easily and is forced to **sign unequal treaties**

- those treaties include:

  - 21 million silver paid to England as punitive indemnity

  - opening of treaty ports, such as Shanghai (later, concessions)

  - gave England Hong Kong in perpetuity (later, returned in 1997)

  - pressured more opium trades

  - low tariff set for the English

  - in 1843: extra-territoriality: Qing laws did not apply to the English in Qing territory. (e.g., Englishmen could kill

    Chinese and not be subject to Qing law. )

- all of those ==undermined Qing's autonomy==

  - other treaties are also signed after Treaty of Nanjing
  - China gradually had to rely on European, **opium trade continued**, even more unequal treaties
  - Christian missionaries are allowed to preach 

**Century of Humiliation: the PRCs narrative of national history**

- this period refers to when China lost control of territories to Westerners, and starts with the Opium War (approximately 1839 - 1949)

  <img src="china_civ/image-20221110142249123.png" alt="image-20221110142249123" style="zoom:33%;" />

  emphasize national rejuvenation by the community party

- lots of **current policy is driven by the fear of China losing to western power**

  - Chinas modern-day policies are driven by this never again mentality.

## From Taiping to Sino-Japanese War

 **Previously on the Qing Opium Crisis**

- **Canton System** (1757-1839/42): regulated Qings trade with the West through Chinese hong merchants; restrictive yet worked well before the abolition of the EICs monopoly in 1834
- Mccartney Embassy (1793) is Britains attempt to revise the Canton System, it features two expansionist empires, bringing competing notions of world order; taken as a misinformed example of **British flexibility vs. Chinese rigidity and xenophobia**

- British E. India Company ships opium to Qing, in exchange for silver and goods; Qing court orders opium confiscated from British and destroyed, triggering Opium War (1839-42), **Treaty of Nanjing** is first of many **unequal treaties**

**Domestic Problems under Qing**: towards later rebellion

<img src="china_civ/image-20221115131925018.png" alt="image-20221115131925018" style="zoom:33%;" />

- recall that Qing at its best time (High Qing) had large territory and high standard of living

  - around the turn 19 century, government failures and international relations caused trouble

- now, **silver became scarce** after 1800especially due to silver outflows from China in order to purchase British EIC opium.

  - silver scarcity meant more copper cash required to **pay same taxes (in silver)**.
    - in 1750: one tael of silver needed 780 copper cash
    - in 1838: one tael of silver needed 1,650 copper cash.
    - in effect, silver scarcity meant tax burden more than doubled
  - people can't pay  taxes more burden on taxpayers  tax evasion  less governments revenue  weaker government

- source of internal unrest: **limited resource and population pressure**

  - recall end of Ming had climate change and wars, where starting from Qing 1640 increases

  <img src="china_civ/image-20221115132218907.png" alt="image-20221115132218907" style="zoom:33%;" />

  - however, as more population **amount of arable land is still fixed**

  <img src="china_civ/image-20221115132322946.png" alt="image-20221115132322946" style="zoom:33%;" />

  hence this lead to environment degradation and competition for resources

  - unstable water ecosystems due to intense land reclamation
  - deforestation = erosion, flooding, and desertification = 30,000 died in Kaifeng due to flooding. From 1645 to 1855, a flood almost every 1.89

- **talent glut** = talent grows faster than the number of jobs available = students being jobless

  - lower degree holders: 40k in 1400; 600k in 1700; over 1 million in 1800 (but gov only fixed 20k official posts)
  - a social problem, hence **causing those unemployed/underpaid people indignant**, who later contributes to rebellion (combined with factors above)

- two of the largest (many) **civil wars** that occurred in Qing in 19th century

  - **Taiping** rebellion (1850-64)
  - **Nian** rebellion (1851-68)

**Outbreaks of Many Rebellions**

- **White lotus rebellion** (1796-1804)

  <img src="china_civ/image-20221115133127394.png" alt="image-20221115133127394" style="zoom:25%;" />

  - White Lotus Society: a millenarian Buddhist secret society that first appeared in the Yuan dynasty
  - help poor people, women played leadership roles
  - political movement of grass roots society in the area shown above, and organized them into rebel movement
    - again a **result of economic strain, overpopulation, government corruption, etc**
    - impoverished peasants feeling indignant
  - took 8 year to take them out and spent 5 years of government = further stress in government

- **Eight Trigram**: another secret society

  <img src="china_civ/image-20221115133335829.png" alt="image-20221115133335829" style="zoom:25%;" />

  - in 1813, off-shoot of white lotus, again became a rebellion and many people died

- **Triads**

  - impoverished, hence people formed fraternity group = anti-Manchu and pro-Han
  - smuggling activity and armed quarrels = quite militant group of people

- **Taiping Rebellion** (more detail next)

- other rebellions such as 

  - **Nian Rebellion**
    - crushed by Generals **Zuo Zongtang ** and Li Hongzhang 
    - chef invented that General Zuo's Chicken  is from the same Hunan province, has nothing to do with Zuo Zongtang as person
  - **Panthay Rebellion**: ethnic tension between Muslim and Manchu
  - **Ya'qub Beg Khante**: turkey had Muslim establish autonomous regime, then suppressed by Zuo Zongtang

**Structural Difficulties of Managing Crisis**

- country too large and too populous
  - many ethnic groups, and population **outpaced administrative control**
  - limitations of bureaucratic efficiency before telegraph and rail (e.g. wrong info from local)
- competing national and **local** interests
  - e.g., local officials overreported rebel threats to siphon funds.

**Taiping Heavenly Kingdom**

<img src="china_civ/image-20221115134718029.png" alt="image-20221115134718029" style="zoom:33%;" />

- greatest revolution the world has yet seen

  - people are positive about the revolution as being confident in the portrayal
  - involves both peasants and failed exam takers, being indignant of the stresses

- starts from the South: where may conflicts are from locals and **Hakka** (guest, ethnically Han who fled from north to South to evade from Xiongnu during the Han dynasty)

  <img src="china_civ/image-20221115134952213.png" alt="image-20221115134952213" style="zoom:33%;" />

  as newcomers, Hakkas are seen to compete resources and not welcomed by locals

  - architecture themsleves with a hole is for defensive purpose = tension with the locals
  - can be found in Guandong, Guangxi, and Fujian

- **Hong Xiuquan = a Hakka who organized the rebellion**

  <img src="china_civ/image-20221115170953866.png" alt="image-20221115170953866" style="zoom:33%;" />

  - from a poor family,  failed examination four times.
    - after failed the 3 time = had dream vision who when he was the second son of God, and brother of Jesus.
  - then he
    - smashed shrines dedicated to **Buddhist and Confucian** worship.
    - added **anti-Manchu strain to his preaching**
    - calling people to rise up and defeat the Manchu rulers

  <img src="china_civ/image-20221115135542705.png" alt="image-20221115135542705" style="zoom:33%;" />

  eventually they expanded and included people outside of Hakka and went all the way to Nanjing, and made it a capital. There, he urges for

  - ==equalization== of land holdings, ==equality== of men and women
  - ==equal== exam but test on stuff such as translation of bibles instead Confucian classics
  - ==Utopian== view of the society

- **Western reactions to Taiping**: Taiping armies known for their puritanical zeal

  - western (both english and american) missionaries hoped Taiping will succeed
    - Hong Xiuquan claims to be related to Jesus = hope it will succeed and convert Chinese to Christian
  - European liked to because it is radial and revolutionary (devotion of equality)
  - English government: was neutral and they wanted just trade. So they took advantage of it help Chinese to quell the rebellion and in turn **take more privileges in trade** (heavily dependent in trading in China and US)
    - note that US civil war is also during 1681 = less supply/trade from US; and similarly Taiping broke out in China
    - English fear that their trade econ will collpase as they are major traders
    - so they need to interfere somewhere = decided to intervene in China

- but the largest credit of quelling Taiping still goes to ==Han Chinese officials==

  - note those Han Chinese ==gentry== (not people who failed, who usually join rebels) ==have not much anti-Manchu thougths==

  <img src="china_civ/image-20221115140518034.png" alt="image-20221115140518034" style="zoom: 25%;" />

  - he built an army from his home province = Hunan, hence Xiang Army
  - **financed through local gentry** instead of Manchu lead government
  - indicates a ==shifting (military) power to locals== from the center/government (e.g. rise of Han)

- the Taiping rebellion caused

  - nation-wide: affected 16 out of 18 provinces
  - unprecedented death toll: **at least 20 million, up to 40 million**. (by comparison, 620,000 died in the American Civil War)
  - devastated the economically important lands south of Yangzi.
  - so close to bring down the Qing dynasty, but didn't.
    - Recall that the pattern of rebellion replacing dynasty was: warlords rise in power to quell the rebellion, but after they compete and winner takes a new Dynasty
    - Now, the army still remains to be part of the imperial army. But after this local provincial army became so strong = replacing Manchu garrisons as major power = ==shift of balance of power from government to the locals==

- this rebellion in total lasted 15 years, because Qing was also busy doing something else: fighting the Europeans

**The Second Opium War** (1856-1860). Also called Arrow War

- right in the middle of Taiping Rebellion, **China was also engaging with great Britain and France** in addition to Taiping rebels

- called Arrow War as it is named by the Ship in suspicion of piracy, how ever it is layer registered with English.

  - British use this incident as **pretext to start war**, to add more unequal terms to treaty
  - at the same time, a French was to be executed, hence French joined as well

- ended with (second) defeat of Qing government to the English

  <img src="china_civ/image-20221115141216430.png" alt="image-20221115141216430" style="zoom:33%;" />

  resulted in 

  - burn down of Summer Palance (built for Qianlong and his successors and had European style architectures)
  - more unequal treaty signed (**1860 Convention of Peking**), more ports opened, and huge indemnity. Some Chinese being taken as slaves to the Americas (indentured)
    - Indentured servitude is a form of labor where an individual is under contract to work without a salary to repay an indenture or loan

**Self-Strengthening Movement** 1861-95

- Qing fear that dynasty is going to collapse soon. Most urgently they wanted to **reform the military and modernize with western tech**

- Li Hongzhang as an example:

  <img src="china_civ/image-20221115141726795.png" alt="image-20221115141726795" style="zoom:33%;" />

  - equipped his army with **imported weapons** and developed **modern** provincial armies to defeat Taiping rebels.

- this promoted Qing to

  -  set up **factories** to build western style weapons and warships
  - **education** infrastructure to study western inventions/thoughts
    - e.g. schools of science and technology in the Fuzhou shipyard and Jiangnan arsenal.
    - school of foreign languages.
    - study abroad programs in America, England, and France

- knowledge about the west gradually improved

  - e.g. chinese kids sent to educational mission to the US, including going to Yale and Columbia
  - China then also added structures such as railways

  <img src="china_civ/image-20221115142103609.png" alt="image-20221115142103609" style="zoom:33%;" />

**First Sino-Japanese War** (1894-95)

- though the self-improvement movement is doing well, until Japan defeated China

  - before, China thought Japan is a little brother
  - now, there is this humiliating defeat

- Meiji emperor during the time: **conflicted with Qing influence of Korea, and engaged into war**

  - Korea had long been Chinas most important client state, but its strategic location opposite the Japanese islands and its natural resources of coal and iron attracted Japans interest.

  <img src="china_civ/image-20221115142336795.png" alt="image-20221115142336795" style="zoom:33%;" />

- war broke out mostly at sea, and Japan had China sign a treaty after China's defeat

  <img src="china_civ/image-20221115142436096.png" alt="image-20221115142436096" style="zoom:33%;" />

  - term includes ceding Taiwan to Japan, and recognition of Korean Independence (short-lived, for it was colonized by Japan in 1910)

- Qing realized that its nation is weak, but why even after self-improvement movement?

  - some people see self-improvement movement as useless
  - others attribute to problems such as corruption + lack of training and organization/literacy of Chinese troops despite advance in technology

  regardless, the failure of the Qing in its late decades also contribute

# From Empire to Republic

**Previously on the late Qing crisis**

- Just prior to the start of the Opium War, silver shortage, ecological stress, have pushed **Qing China into general economic crisis**; peasant unrest and tax revolts proliferate

- Taiping Rebellion (1850-64): failed scholars espousing heterodox Christianity (Hong Xiuquan)
  - had motivation for equality, but also the most destructive rebellion
  - long lasting rebellions = resulting in Qing dynasty power is severely undermined
- Han **provincial gentry** come to aid of the dynasty, raised local armies to defeat Taipings
  - also came to initiate **self-strengthening movement to learn western technologies**
  - e.g. Li Hongzhang, builds western arsenals, weapons, and translate western texts.
- **Arrow War** (1856-60) at the same time, also tension with western especially the English and French
  - Qing lost again to the Anglo-French force, and have **summer palace burnt and more unequal treaties signed**

- **Sino-Japanese War** (1894-1895): Chinese defeat again, still due to technology diff or lack of military organization/coordination? Don't know.
  - defeat has deep consequence = realizes how astonishing weak the empire was = Chinese **loss of faith in Manchus**

**Hundred Day's Reform**: and later a serious of rebellions/reformed until 1911 ==Xinhai Rebellion == ended Qing in 1912

- when China lost Sino-Japan War, western realized Chinese weakness and were competing for territory.

  <img src="china_civ/image-20221117132336654.png" alt="image-20221117132336654" style="zoom: 25%;" />

  previously they were mostly occupying parts of South-East Asia. But seeing how easily China is defeated by Japan, **westerns are also lurking and China is "cut up"** during 1902-1903:

  | <img src="china_civ/image-20221117132534877.png" alt="image-20221117132534877" style="zoom: 25%;" /> | <img src="china_civ/image-20221117132748517.png" alt="image-20221117132748517" style="zoom:25%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

- How to save China? First attempt by **Hundred Day's Reform** (Wu Xu)

  - was a failed 103-day national, **cultural, political, and educational reform movement** that occurred from 11 June to 22 September 1898 during the late Qing dynasty
  - they heard the humiliating signing of Sino-Japanese defeat treaty, and became indignant
  - in 1898, they launched a reform funded by Guangxu Emperor (but ==Cixi== is power behind the throne = most powerful since Wudi)
    - Cixi's aim is to maintain her role and power, rather than help people. e.g. reform policies approved usually for self-serving purposes
    - but in 1898 she allowed Guangxu to rule on his own, and K and L are asked to help with the reform

  
  | <img src="china_civ/image-20221117133001371.png" alt="image-20221117133001371" style="zoom: 25%;" /> | <img src="china_civ/image-20221117133109537.png" alt="image-20221117133109537" style="zoom:25%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  
  - she then was afraid that Guangxu's reform would undermine she power and Manchu, so locked up Guangxu and the reform proponents fled to Japan = hence only 103 days of reform
  - Cixi = ==ended last chance of China to change peacefully==
  

**The Boxer Uprising** (1900): purge foreigners and Christians. It was an anti-foreign, anti-colonial, and anti-Christian uprising in China between 1899 and 1901, towards the end of the Qing dynasty

- Chinese blamed failing/natural disasters to be caused by foreigners: e.g. wrongly influenced Chinese culture and practices

  <img src="china_civ/image-20221117133822915.png" alt="image-20221117133822915" style="zoom:25%;" />

  note that mostly peasants, and called "Boxer" because they mostly fight with martial arts

- emerged also in 1898 after 100 days reform, and hates foreigners and converts (to follow foreign religions such as Christianity)

  <img src="china_civ/image-20221117134110487.png" alt="image-20221117134110487" style="zoom:25%;" />

  as you see it is technically "not a religion problem", but the **socio-economic problem** that whoever converted to Christian had this unfair protection and foreign backing.

- The events came to a head in June 1900 when Boxer fighters, convinced they were invulnerable to foreign weapons, **converged on Beijing with the slogan "Support the Qing government and exterminate the foreigners."**

- To counter this rebellion against foreigners, an ==Eight Nation Alliance== of American, Austro-Hungarian, British, French, German, Italian, Japanese and Russian troops moved into China to lift the siege, and afterwards **took over the palace in Beijing**

  | <img src="china_civ/image-20221117134700506.png" alt="image-20221117134700506" style="zoom:25%;" /> | <img src="china_civ/image-20221117134737638.png" alt="image-20221117134737638" style="zoom:25%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  and again Chinese' loss lead to the Boxer Protocol in 1901 = total indemnity of 450 million

- *again* spurred national debate of how China can still move forward

**Third and Last Attempt to Save China**: 1911-1912 

- so far we see efforts starting from self-strengthening, 100 days reform, boxer uprising, all failed

- **people needed *will* reformation** = Zou Rong, an anti-Manchu nationalist and published the *Revolutionary Army* journal

  <img src="china_civ/image-20221117135352207.png" alt="image-20221117135352207" style="zoom: 33%;" />

  - convinced that wipe out Manchus is necessary, and called out in the *Revolutionary Army* journal for comrades
  - advocated to establish the Chinese republic

- **Sun Yat-sen**: the most prominent anti-Manchu revolutionary and a founding member of anti-Machu

  - believes the best way to overthrow is to ***ally with the secrete society*** (e.g. the triad, etc)
  - he spent most of the time traveling to many foreign grounds = to gain financial support to fund uprisings in China
  - in 1905-12 he formed the Revolutionary Alliance = people with many different ideals such as socialist and republican, but all allied to ==overthrow the Manchu rulers and revive China==. Later this alliance is transformed into **Guomingdang (KMT)**

  <img src="china_civ/image-20221117135747597.png" alt="image-20221117135747597" style="zoom:33%;" />

- his ==three main principles== are

  - **nationalism**  = members of the country to pay highest loyalty to the nation, rather than to family, etc.
  - **democracy**  = to him it means western constitutional government
  - **livelihood of the people**  = social welfare such as food, housing, transportation, and etc.

- revolutionary broke out in 1911 (Xinhai Revolution), many provinces declared independence, and ==ended Qing in 1912==

  <img src="china_civ/image-20221117140318311.png" alt="image-20221117140318311" style="zoom:25%;" />

  - Qing gave up in only 4 month
  - "founding father" at that time was at Colorado, so some people say this title of "founding father" is overestimated
  -   On 1 January 1912, the National Assembly declared the establishment of the **Republic of China,** with Sun Yat-sen, leader of the Tongmenghui (United League), as President of the Republic.

- But Sun quickly gave away the power to ==Yuan Shikai== = no longer the Chinese coin of hole in the middle = western style coin

  - because there was this promise that: Sun would resign in favor of Yuan Shikai, who would become President of the new national government, *if Yuan could secure the abdication of the Qing emperor*
  - Even Sun had gather fund, had ideals, but he didn't have much military power. Hence Yuan Shikai, who was appointed as the leader of the powerful Beiyang Army by Qing

  <img src="china_civ/image-20221117140804031.png" alt="image-20221117140804031" style="zoom: 25%;" />

- however, Yuan Shikai died early before he could consolidate a legitimate central government before his death in 1916, led to decades of political division and ==warlordism==, including an attempt at imperial restoration = China entered ==again the unrestful warlord period (1916 - 1927)==

**Change of Beliefs and Practices in China** during 1915-1926

- a new periodical serving as a source of new ideology and heavily influenced the New Culture Movement and May 4th protests later

  - **The May Fourth Movement** was a Chinese anti-imperialist, cultural, and political movement which grew out of student protests in Beijing on May 4, 1919. Students came to protest the Chinese government's weak response to the Treaty of Versailles decision to allow Japan to retain territories in Shandong that had been surrendered to Germany after the Siege of Tsingtao in 1914.
  - The New Culture Movement (Chinese: ) was a movement in China in the 1910s and 1920s that **criticized classical Chinese ideas** and promoted a new Chinese culture based upon progressive, ==modern and western ideals like democracy and science==.

- the new periodical: *New Youth* was a Chinese literary magazine founded by ==Chen Duxiu== and published between 1915 and 1926

  <img src="china_civ/image-20221117141302470.png" alt="image-20221117141302470" style="zoom:25%;" />

  - founded by Chen Duxiu, advocate for individual freedom, and some major ideologies including rejecting Confucianism and **crave the energy of youth** (hence challenge the Confucius idea of young obey elders), promoting the young to think by themselves

  - advocated serious ideas in vernacular language v.s. in the past those serious writings tend to use classical styles

  - also had works from both ==Lu Xun== and ==Mao Zedong==

    <img src="china_civ/image-20221117171901925.png" alt="image-20221117171901925" style="zoom:25%;" />

    note that even though Japan is disliked due to its invasion, **Japan was still the first choice of over sea study** because it is a) close b) had modernized tech, and c) similar language

  - the journal also focused the ==adoption of Western ideals== of **"Mr. Science"** () and **"Mr. Democracy"** () in place of "Mr. Confucius" in order to strengthen the new nation (though science was better received)

  | <img src="china_civ/image-20221117141125330.png" alt="image-20221117141125330" style="zoom: 33%;" /> | <img src="china_civ/image-20221117141129267.png" alt="image-20221117141129267" style="zoom: 33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

- during this period, many male also came out to speak for women's rights (e.g. Lu Xun, a male and He Zhen, a female)

**May 4th Protests**, 1919

- recall that China joined allies, so when German defeated territories in China should be handed back to China, but the result was disappointing that German concessions will be handed over to Japan due to the Treaty of Versailles decision

  <img src="china_civ/image-20221117141804283.png" alt="image-20221117141804283" style="zoom:25%;" />

- students were very angry = government arrested student leaders = general consensus is reached among the students such as **nationalism, patriotism, science, and democracy**, and enemies are imperialism, warlordism, etc.

**Founding Chinese Communist Party (CCP)** in 1921

- at the same time WWI finished, where ==Marxism and Leninism is successful== as seen in soviet union 

  - also had anti-western, so liked soviet union a lot

  - Marxism and Leninism shows capable of revolution = exactly what China need

- again founded by Li Daozhao and Chen Duxiu, for Maxism (Mao Zedong also influenced by Li Daozhao)

  <img src="china_civ/image-20221117142227411.png" alt="image-20221117142227411" style="zoom:25%;" />

- as we know later, under the leadership of Mao Zedong, the **CCP emerged victorious in the Chinese Civil War against the Kuomintang**, and in ==1949 October 1st Mao proclaimed the establishment of the People's Republic of China==. 

---

**Into the Modern China**: a short review and preview of major events until 1949. In general, this is the period with **much more foreign influence** compared to past China/Dynasty changes. Another difference is that before: rebellion = restoration of past glory. But now modern = completely new motivation/ideology

- 1841 - first opium war
- 1860s - second opium war: imperial incursions and increasing christian missionaries, English, etc in China
  - christian autonomy in local society here is central to the cause of the later boxer up-rising
- 1861-95 self-strengthening movement
- 1895 - Sino-Japanese war: watershed moment that losing to Japan (long seen as vassal state) made Chinese realizes its weakness + anti-Manchu
- 1898 - Hundred Days reform
- 1900 - Boxer rebellion. Can think of as "thugs" trying to bring back Chinese ruling. Quelled by the eight-nation alliance
- 1911 - ==Xinhai Revolution==. Starting in Wushang - we've rebelled, who will join us - lots of provinces joined quickly
  - in Feb 1912 Qing abdicated the throne, and **Republic of China found 1912 Jan 1** (recognized in Taiwan), and Sun Yat-sen was the head
  - however, Sun didn't have military power but the ideals, hence collabed with Yuan Shikai = will be the new president if his military can help
- 1916 - 1927 warlord period of decentrailized china = time of disunity
  - Yuan Shikai died in 1916 - very short reign before the government is coalesced
- 1921-24: KMT allied with CCP to march north and subdue the warlords and reunified the coountry
  - In 1921, Chen Duxiu and Li Dazhao led the founding of the CCP with the help of the Far Eastern Bureau of the Communist Party of the Soviet Union and Far Eastern Secretariat of the Communist International.
  - For the first six years of its history, the CCP aligned itself with the Kuomintang (KMT) as the organized left-wing of the larger nationalist movement. 
- 1919 **May 4th movement**: territory not given back to China - highly literary movement of intellectuals such as Lu Xun and Cheng Duxiu trying to **incorporate western ideas into China governing**
- 1925: Sun Yat-sen died, who is a key figure making the alliance of CCP and KMT together, things goes wrong between CCP and KMT
  - General Chiang Kai-shek, who became the Chairman of the Kuomintang after Sun's death and subsequent power struggle in 1925, began the Northern Expedition in 1926 to overthrow the Beiyang government. 
  - In 1927, Chiang moved the nationalist government to Nanking and **purged the CCP, beginning with the ==Shanghai massacre==**. The latter event forced the CCP and KMT's left-wing into armed rebellion, marking the beginning of the ==Chinese Civil War==
  - basically KMT turned against CCP. Bear in mind that former had strongholds in cities, whereas latter whose ideal is communist only yet had supports by peasants in countrysides
- 1927 - 1949: a lot of upheaval and disunity
  - CCP fighting with KMT, and Japan is also attacking
  - The ==Second Sino-Japanese War (19371945, WWII)== or War of Resistance (Chinese term) was a military conflict that was primarily waged between the Republic of China and the Empire of Japan.
- 1949 - **CCP won** and start of People Republic of China
  - The Communists gained control of mainland China and established the People's Republic of China in 1949, forcing the leadership of the Republic of China to retreat to the island of Taiwan.

## The Nanjing Decade (1927 - 1937)

> Some key terms in this period
>
> - **Fascism** is a authoritarian and ultra-nationalist political ideology, characterized by a dictatorial leader, centralized autocracy, militarism, forcible suppression of opposition, belief in a natural social hierarchy, subordination of  individual interest for the perceived good of the nation and race, and strong regimentation of society and the economy
> - **Imperialism** is the state policy, practice, or advocacy of extending power and dominion, especially by direct territorial acquisition or by gaining political and economic control of other areas, often through employing hard power, but also soft power.
> - **Warlord Era**
>   - The **Warlord Era** was a period in the history of the **Republic of China** (not PRC) when control of the **country was divided among former military cliques of the Beiyang Army** and other regional factions from 1916 to 1928.
>   - In historiography, the Warlord Era began in 1916 upon the **death of Yuan Shikai,** the de facto dictator of China after the Xinhai Revolution overthrew the Qing dynasty and established the Republic of China in 1912. **Yuan's sudden death created a power vacuum** that spread across the country hence the warlord area.
> - The **Nanjing Decade**:
>   - The Nanjing decade (also ; ) is an informal name for the decade from 1927 (or 1928) to 1937 in the People's Republic of China. It began when Nationalist, or even "Confucian Fascist" Generalissimo **Chiang Kai-shek took Nanjing** from warlord Sun Chuanfang halfway through the Northern Expedition in 1927. Chiang declared it **to be the national capital** despite the existence of a left-wing Nationalist government in Wuhan. 
>   - The Nanjing decade was marked by both progress and frustration. 
>     - The period was far more stable than the preceding Warlord Era. There was **enough stability** to allow economic growth and the start of ambitious government projects. Entrepreneurs, educators, lawyers, doctors, and other professionals were more free to create modern institutions than at any earlier time. 
>     - However, the Nationalist government also suppressed dissent, corruption and nepotism were rampant and **revolts** broke out in several provinces; internal conflicts also perpetuated within the government. The ==Nationalists== were never able to fully pacify the ==Chinese Communist Party==, and struggled to address the widespread unrest and protests over their failure to check Japanese aggression
>   - The decade ended with the outbreak of the **Second Sino-Japanese War** in 1937 and the retreat of the Nationalist government to Wuhan.

There will be a lot on student protests, resistance, etc. 

- Racism in Shanghai, Chinese tormented by powerful imperial forces. 
- On going struggle between **KMT**, **CCP**, and the encroaching **Japanese** Imperialism

**The Warlord Era**

- In 1915 ==Yuan Shicai with his Beiyang army== has declared himself a new emperor, with the intention of finding new dynasty = Republic of China (the one with Sun Yatsen). But he died soon in 1916 so China is divided into regions ruled by separate generals and hence a **warlord era**

  <img src="china_civ/image-20221129132107080.png" alt="image-20221129132107080" style="zoom:33%;" />

- KMT revival with Sun Yat-sen's return in 1917, and Communist continued their activities

  - intense ideology struggle on both sides, nationalist v.s. communist
  - on KMT, some were believing in fascist, but also some sympathizes with communist. Same on CCP's side

- but both KMT and CCP agreed on bourgeosis revolution (as with MarxismLeninism in Soviet), hence they cooperate to create ==United Front==

  - launch northern expedition against warlords in the north

  - before the beginning of expedition, some tension: nationist turn on CCP because of its popularity

  - e.g. May 30, KMT police opened fire, leading to strikes in many cities

    <img src="china_civ/image-20221129132539074.png" alt="image-20221129132539074" style="zoom:50%;" />

    which made CCP grow even more in power/popularity.

- In general, many KMT encouraged CCP to cooperate, while some right-wing from KMT do not trust CCP = cannot co-exist

  - hence KMT into two major wings: **left wing sides with CCP, while right wing believes in militarism**

    <img src="china_civ/image-20221129222115881.png" alt="image-20221129222115881" style="zoom:33%;" />

- then in 1927 = White Terror = surprise attacks KMT arrested hundreds of CCP people and executed them = end of KMT support from people

  <img src="china_civ/image-20221129133109511.png" alt="image-20221129133109511" style="zoom:50%;" />

  - many of these purges were executed by the more extreme factions in KMT

**Shift towards an Agrarian Communism**

- believes that factory workers would be the ones for Communist revolution in China

  - Mao's emphasis of investigation = historical materialism from Marxist
  - Firmly situated ==peasants as revolutionary subjects== - and moved Chinese communism towards an agrarian focus.
  - Saw ritualistic and ==violent struggle against class enemies as critical to revolution==. This is a key difference from KMT socialists, who believed in cooperatives to support peoples livelihood 

  <img src="china_civ/image-20221129133412885.png" alt="image-20221129133412885" style="zoom:50%;" />

**End of the First United Front**

- CCP and KMT civil war, and CCP establish bases/soviets between 1927 to 1933 (the main one being Mao's base)

- worried by CCP's growing support even in urban areas, Chiang Kai-shek **launched four massive military campaigns** to encircle and annihilate the Jiangxi Soviet, all of which were repulsed by the communists with their **guerrilla tactics**

  <img src="china_civ/image-20221129133928967.png" alt="image-20221129133928967" style="zoom:33%;" />

  - however eventually CCP had to retreat, hence this long march shown above = recorded as their heroic struggle

---

**Diff of KMT v.s. CCP in Economy**

- China had econ problem such as famine before, and both parties are concerned about imperialism that Chinese cannot survive. But they differ in **how to solve this problem**
- nationalist believe
  - accumulate **national** wealth, aim to annihilate dependencies on others hence social strength/**self-sufficiency**
  - econ independennce = autarky = **national prosperity is precondition for happiness of people**
    - wanted export lead economy. but this did not work with the great depression in 1932 (Wall street crush), so silk and tea lost much markets, and Japan's influence made China grow (China exporting raw materials to Japan while Japan grows in technology)
  - solve poverty by **increasing production** = need tech advancement = gives land questions
    -  Left-wing (Wang Jingwei and others) wanted to build up land cooperatives
    - Right-wing (Chiang Kai-Shek) more concerned with military control of villages
  - but there are **conflicts with private industry,** as they needs its own profit but nationalist needs national wealth
  - does not care much about rural, but more central power

- CCP focused on **rural organization**, land reform, and **class struggle**
  - global capitalism is the problem, and being semi-colonized = both landlords needs to be overthrown and wealth redistributed
  - land reform program, redistribute to peasants
  - focus a lot on **women emancipation** = e.g. local women can diverse their husbands, can own land, etc
  - give peasants opportunity to read and write

**Diff in Military mobilization**

- Mao's Red Army: **Guerrilla warfare**
  - draw enemy into deep CCP territory such as mountains, so KMT becomes naturally split up due to landscapes
  - red army are **mostly consists of volunteers**; and hold respect to civilians
    - Eight points of attention: do not hit or swear others, be honest about buying and selling, etc
    - helped increase popularity = a lot of support from peasants + China had a large size of peasants
  - cooperate between army and people
- Nationalist
  - advance altogether in one column, traditional approach
  - more than half are **soldiers, not civilians** = Chaang Kai shek prioritized military discipline

**Factions within KMT**

- two big groups controlling the executive functions, CC Clique and Blue Shirt

- CC Clique v.s. Blue Shirts

  - CCP Clique return for Chinese past and vision for future. LHS has stuff from the past such as great wall, on the RHS tech advanced visions

    <img src="china_civ/image-20221129140336691.png" alt="image-20221129140336691" style="zoom:33%;" />

  - Blue Shirt: emphasize of strngth and strong young people

    <img src="china_civ/image-20221129140433293.png" alt="image-20221129140433293" style="zoom:33%;" />

**New Life Movement** (nationalist)

- draw on four virtues, advocate that these virtues penetrate peoples life

  <img src="china_civ/image-20221129140624061.png" alt="image-20221129140624061" style="zoom:33%;" />

  being productive at all times

- then Chiang Kai-sheks Speech incorporates ideas of **convergence of militarism and being productive in life**

  - i.e. your individual behavior can hurt the wealth of the entire nation = you should stick to your social role. If everyone perform their social role, then not chaos and uprising associated with communist

  <img src="china_civ/image-20221129140939040.png" alt="image-20221129140939040" style="zoom:50%;" />

  - but still gained a lot critique from Confucianism:
    - New Life Movement was an attempt to ***suppress individual freedom*** and to impose a rigid and authoritarian ideology on the people = the movement's emphasis on obedience and conformity was at odds with the principles of Confucianism, which emphasize personal responsibility and self-cultivation.

**Left-Wing Cultural Movement**

- Marxist texts, concerned with every day life, and experimented with new literary forms

  - aim to make literature appeal with the masses = new form of short novels

- two famous person her is **Song Dingling and Mao Dun**

- but then left wing faced cracked down by 5 prominent being executed

  <img src="china_civ/image-20221129141427721.png" alt="image-20221129141427721" style="zoom: 33%;" />

- overall had rich cultural production, both soft v.s. hard films as tools for politic education

- KMT supported tabloits and defamed Cai Chusheng (a famous women actor at that time) and her movie = committed suicide

  - impact on later communist cultural production
  - Mao: importance of appealing to the masses

**Japanese Imperialism**: belief that Japan can relieve the influence from western by conquering Asian countries and ruling them

- Japan controlled a lot of due to their control of the Manchurian Railway

  <img src="china_civ/image-20221129142010627.png" alt="image-20221129142010627" style="zoom: 33%;" />

  basically the region

- Battle ensued when plain clothes Chinese

  - eventually Japan won the right to police Shanghai
  - **Japanese established Manchukuo within that Chinese region, which enraged CCP and KMT**

- then goes to the **Second Sino-Japanese war**

  - and **Paul Robeson** and our national anthem

  <img src="china_civ/image-20221129225545886.png" alt="image-20221129225545886" style="zoom:25%;" />

## WWII and Second Sino-Japanese War (1937-1945)

**Previously on the Republican Era**

- 1911-16 The first **Republic**
  - first Sino-Japanese war and Boxer protocol are national traumas
  - 1911 revolution: replace Qing and new government, and **Sun Yat-Sen** established the first Republic of China
  - Sun soon replaced by Yuan Shicai, who had strong military. However Yuan died early in 1916, leading to warlord era
- 1916-27 The **Warlord Era**
  - Centered around Beijing, the **New Culture Movement** sees a new generation of intellectuals question the Confucian basis of social, moral, political traditions. Believe that those are the reason for Chinese'fall, and **emphasized on Mr. Science and Mr. Democracy**
  - May 4th Movement (1919) politicizes intellectuals (anti-imperialist movement)
  - **CCP founded** 1921 in Shanghai.
- 1927-37 The **Nanjing decade**
  - Chiang Kai-Shek (CKS) succeeded Sun and became KMT leader, and with (appear) allied with CCP to go on northern expedition to **re-unite China** from the warlords = **First United Front**
  - CKS with his strong military belief never trusted CCP, hence **white terror** in 1927, and CCP flee
    - retreated to Jiangxi Soviet. Stop of Soviet Union's support to KMT, and **Mao start to emerge as the leader in CCP**
    - then KMT then tried campaigns to get rid of Communist in Jiangxi, and in the 4th occasion, CCP fled despite their encirclement and went on a **long march** (34-35) all the way to Yan'an, Shaanxi
    - only a bunch of true faithful people in CCP left (a lot key figures including Mao, Zhou Enlai, Xi's father, etc). But they collected lots of supports along the way
    - CCP regroups, builds **peasant support through land reform**, uses mass movements to accomplish objectives
  - hence overall a unity after the chase, and KMT + CKS become the major government in China. CKS's view is highly militaristic and productive, that through discipline in a top-down approach we can make ourselves stronger
    - **New Life Movement** to impose discipline and morality on China's society. Inspiration drawn from the Italian fascist and German Nazi movements
    - also many cultural influence during the time: **leftist intellectuals making films**, etc.

> **Some Major Events** in this period
>
> - In 1931, the [Mukden Incident](https://en.wikipedia.org/wiki/Mukden_Incident) helped spark the Japanese invasion of Manchuria. The Chinese were defeated and Japan created a new puppet state, ==Manchukuo==; many historians cite as the beginning of the war
>
> - The **beginning of the war** is conventionally dated to the ==Marco Polo Bridge Incident == on **7 July 1937**
>   - Since the Japanese invasion of Manchuria (region) in 1931, there had been many small incidents along the rail line connecting Beijing and Tianjing. 
>   - On this occasion, a Japanese soldier was temporarily absent from his unit, and the Japanese commander **demanded the right to search the town for him**. When this was refused, other units on both sides were put on alert; with tension rising, the **Chinese Army fired on the Japanese Army, which further escalated the situation**, even though the missing Japanese soldier had returned to his lines.
> - following this incident, Japanese scored major victories, capturing Beijing, Shanghai and the Chinese capital of Nanjing in 1937, which resulted in the ==Rape/Massacre of Nanjing== () beginning on **December 13, 1937**
>   - As the Japanese approached, the Chinese army **withdrew the bulk of its forces since Nanjing was not a defensible positio**n. The civilian government of Nanjing fled
>   - The perpetrators also committed other war crimes such as **mass rape, looting, and arson**. The massacre was one of the worst atrocities committed during WWII
>   - Due to multiple factors, death toll estimates vary from 40,000 to over 300,000, with rape cases ranging from 20,000 to over 80,000 cases
> - Following the **==Sino-Soviet Treaty of 1937==, strong material support** helped the Nationalist Army of China and the Chinese Air Force continue to exert strong resistance against the Japanese offensive. By 1939, after Chinese victories in Changsha and Guangxi, and with Japan's lines of communications stretched deep into the Chinese interior, the **war reached a stalemate**.
> - in August 1940,  the ==United States supported China== through a series of increasing boycotts against Japan, culminating with cutting off steel and petrol exports into Japan by June 1941. Additionally, **American mercenaries such as the Flying Tigers** provided extra support to China directly.
> - In December 1941, Japan launched a surprise attack on ==Pearl Harbor==, and declared ==war on the United States==. The United States declared war in turn and increased its flow of aid to China

**CCP v.s. KMT:** grassroots v.s, organizations

- some degree of fluidity/flexibility within the party, but overall differ in how to restructure/rule

- CCP: socialist, **redistribution of land and peasants** can now on lands
  - appeal to peasants and grassroots
  - start revolution by helping the peasant, and note that China at that time had a lot of peasants/is rural
- KMT=nationalist=Republic of China: in a more extreme way, a bit fascist and military society. 
  - Hence new cultural movement, refining and strenghening China **in a militaristic way;** also a lot of focus on productivity and utilitarianism
- both had some influence from Soviet Union, e.g. Marxist-Leninism
  - *The Communist International* (*==Comintern==*), was a *Soviet*-controlled international organization. They sent advisers and aids to help with revolutions

**Japanese Imperialism**

- South Korea asked Japan to ban the *rising sun flag* during Olympics, which is also the war flag of imperial Japan (which occupied Korea and part of China)

  | <img src="china_civ/image-20221201132603199.png" alt="image-20221201132603199" style="zoom: 25%;" /> | <img src="china_civ/image-20221201132549883.png" alt="image-20221201132549883" style="zoom: 25%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

- recall that before, Qing has lost Korea territory and Taiwan to Japan; Japan became competitor with Russia for influence in China in the north

  <img src="china_civ/image-20221201132906262.png" alt="image-20221201132906262" style="zoom:25%;" />

  - surprisingly, they even defeated Russia in the northern territories in China = 1904-5 the **Russo-Japanese** War 
  - in 1931 the Mukden incidence and ==established Manchuko==; 
    - Japan really took this territory as its colony; made a lot of investments; therefore it could be the most prosperous area in China at that time
  - 1937-1945; Japan tried to encroach into deeper China, and hence second Sino-Japanese war

**Marco Polo Bridge Incident**: July 7, 1937

- CKS wanted to first unify Chinese = open non-resistance policy = became very non-popular. He is then kidnapped and forced to ally with CCP to fight against Japan

- hence **high tension** between Japanese and Chinese with this minor Marco Polo Bridge Incident = triggered second Sino-Japanese war

  <img src="china_civ/image-20221201133343977.png" alt="image-20221201133343977" style="zoom:33%;" />

  - after that Japan attacked Beijing, Tianjing, etc.

**First Phase of Sino-Japanese War**: Battles

- Shanghai is an main source of Nationalist revenue + strategic position = proximity to Nanjing;

- Shanghai had significant western living in there; but their policy is to not interfere at the moment

- However nationalist eventually **lost**, **Battle of Shanghai** (1937)

  - first and fiecest of the 22 major enegagements, lasted three month before Chinese failed
  - the bloody Saturday + anti-Japanese feelings

  <img src="china_civ/image-20221201133759519.png" alt="image-20221201133759519" style="zoom:33%;" />

- big defeat of China, CKS lost over 30% of elite German-trained divisions troops (later on contributed to CCP's victory during civil war)

- however, at least **pushed some unification of China** (some warlords) to face the Japan troops

**First Phase of Sino-Japanese War**: war crimes

- **Nanjing Massacre** and Rape of Nanjing. Japan failed to conquered Shanghai quickly= angered Japanese, hence when conquering Nanjing this happened

  - Japanese troops went on rampage for six weeks
  - 20,000 - 80,000 women and girls of all ages were raped, many of whom were mutilated or killed in the process.
  - 200,000 - 300,000 civilians and POWs were killed.
  - a woman writer recorded this, who turned the school into a sanctuary, and her diaries records those war and atrocities

  <img src="china_civ/image-20221201134307167.png" alt="image-20221201134307167" style="zoom:50%;" />

  up until now, they still had not apologized for this, compared against Germany = Japanese acquired reputation for cruelty

- "**Comfort Women**", i.e. sex slaves; ensure an isolated group of women to satisfy solider's sexual eneds

  - an estimate of 200,000 young women and girls were forced into this service; mostly Korea, but also Chinese, Japanese, etc
  - one women assigned for 70-80 soldiers before battle
  - again, Japanese arguments were that they were paid prostitutes; but many are not/are abducted

- **Human Experiments and use of Biological Weapons**: using diseases as weapon

  - like Nazi's, Japanese conducted experiments with human = death camp = infected healthy subjects with various diseases and observe them; operating them without anesthesia

    <img src="china_civ/image-20221201134930024.png" alt="image-20221201134930024" style="zoom:33%;" />

  - many biological weapon researchers escaped punishment and went to continue research as other countries feel worried if they are behind; questions of medical ethics = had no ethical standards setup yet

**First Phase of Sino-Japanese War**: Man-made Yellow River Flood

- nationalist government made a desperate decision to breach the dam = **cause the flood to stop advancement of Japanese troops**

  <img src="china_civ/image-20221201135255770.png" alt="image-20221201135255770" style="zoom: 33%;" />

- flood inundated a lot of land and killed over 800,000 people in those provinces (unexpected by the government) = sacrificing million of lives to stop Japanese = though only stopped them for five month, and later continued to capture Wuhan

**Second Phase: Stalemate** 1939-41

- blue is territory occupied by Japan, and nationalist government moved to interior

  <img src="china_civ/image-20221201135451696.png" alt="image-20221201135451696" style="zoom:33%;" />

- three parties, Though CCP and KMT are both fighting Japanese, they are still not "allies"

- **Japan's New Order in Asia**: Japan's belief that they can rebuild Asia and replace the western colonizers: propaganda that other countries should provide resources to Japan

<img src="china_civ/image-20221201135737880.png" alt="image-20221201135737880" style="zoom:33%;" />

- puppet regime in Nanjing (1940-45), Wang Jingwei (top in KMT) collaborated with Japan during war time, seen as a national traitor

**Third Phase: Global War**

- China alliance with United States and Britain = received military support and supplies

  - e.g. US army transportation in China, with supplies and soldiers such as Flying Tigers

  <img src="china_civ/image-20221201140212011.png" alt="image-20221201140212011" style="zoom:33%;" />

  - some sense that US rescured Chinese, but China still had important contribution to the WWII. One is that it held huge numbers of Japanese troops on its territory. Second is that China it technically the first to engage in WWII in 1937 (or even 1931), while Europeans start in 1939

  <img src="china_civ/image-20221201140543710.png" alt="image-20221201140543710" style="zoom:33%;" />

- Japan retaliated with **Attack on Pearl Harbor (1941)**

- in the end US dropped two atomic bomb and **Japan surrendered**

**Impact of the War on China**

- devastation of many parts in China; **heightened since of national identity**; a **weakened nationalist government**, and CKS lost lots of his troops

  - therefore, it set the stage for Communist success later. 
  - it can be seen that CKS fight against Japan diverted his troop and effort from eliminating CCP, and during those time CCP enjoyed more support in rural China = had more time to develop and became stronger

- China-Japan relation today: a bit of awkward handshake between two presidents

  | <img src="china_civ/image-20221201141620456.png" alt="image-20221201141620456" style="zoom:33%;" /> | <img src="china_civ/image-20221201141629826.png" alt="image-20221201141629826" style="zoom:33%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

**Chinese Civil War**: CCP v.s. KMT final showdown after the WWII. 1945 - 1949

- final showdown; US reduced their support in CKS's government due to some corruption in government. Mao didn't get full support from Stalin either

  <img src="china_civ/image-20221201141848649.png" alt="image-20221201141848649" style="zoom:33%;" />

- but we know the final result is that Mao won, established ==People's Republic of China==

- after the defeat, ==CKH fleet to Taiwan==, with 1.2 million nationalist refugees there (and always wanted to retake the mainland)

  - but in 1950, US sends the Seventh Fleet to the Taiwan Strait = **existence of US power prevented each side from attacking**
  - but still tension and lots of propaganda on-going

  <img src="china_civ/image-20221201142157579.png" alt="image-20221201142157579" style="zoom:33%;" />

- yet overall, both party **recognize Sun Yat-sen as father of modern China** (idea of a unified China during warlord era, and founder of Republic of China). However, while Mainland China supported Mao, Taiwan supports CKS

# Mao's China (1920s - 1976)

**Previously on the war years**

- 1931 **Mukden Incident**  (many historians view this as the beginning of WWII for China) touches off a well-planned invasion of Manchuria; starts the 15 years China- Japan war (1931-45). Japan establishes a puppet state of **Manchukuo** in northeast China.
- 1937-1945 **Second Sino-Japanese War** (another view of beginning of WWII). Tentative **second united frontier**. 
  -  Chinese forces suffer heavy losses, trade space for time. 
  - Japanese overrun E. China, GMD fights war of attrition, CCP preserves strength in rural areas
  - CCP have time to win popular support, partly due to Japan's invasion that KMT cannot focus on fighting CCP
- After **Pearl Harbor**, China formally allies with U.S. and Britain;
  - Japanese war crimes (e.g., the **Rape of Nanjing**, **Comfort Women**, Unit 731) remain inadequately acknowledged by the Japanese govt and victims not compensated for. 
  - Wartime collaboration exists yet remains underexamined in scholarship.
- 1945-1949 **Chinese Civil War**
  - KMD retreat to Taiwan, but both claim to be the legitimate government

**Why did China become Communist?**

- Explanation 1: just because it was the **alternative to KMT**

  - how did nationalist fail? KMT had wide spread corruption, inflation, poverty, declining purchasing power (huge inflation), warlordism

    <img src="china_civ/image-20221206132234171.png" alt="image-20221206132234171" style="zoom:50%;" />

  - CCP just happened to be there as an alternative: if KMT more successful, China might not have CCP winning = no communist

  - Japan is also a contributor, that holding KMT off so CCP can have time to develop

- Explanation 2: CCP itself aimed to gain **popularity, mobilizing peasantry, anti-Japan** idea resonated well with the large peasant-based population at that time

**Mao Zedong** 1893-1976

- in his early years

  - born to a rich peasant = wealthy farmer, education in Confucian classics; traveled to work in Peking University Library

  - attended CCP first meeting as the delegate of Hunan, working under **Li Dazhao**

    <img src="china_civ/image-20221206133027857.png" alt="image-20221206133027857" style="zoom:33%;" />

  - Mao concentrated on **rural work** and had this report on an **Investigation of the Hunan Peasant Movement** = before CCP believe in urban workers, but he advocated to mobilizing peasantry and violence as a necessary mean for revolt

- Shanghai Massacre 1927, communist hiding and fleeing from KMT = Chen Duxiu blamed as the mistake. Then Mao lead them into **Jiangxi Province**

  <img src="china_civ/image-20221206133303458.png" alt="image-20221206133303458" style="zoom:33%;" />

  for the first time, CCP is governing some area: **organized land reform** (redistribute land) and other reforms = quite successful in Jiangxi

- **Long March** in search of new base in Yan'an in Shaanxi province, after KMT attack again. Only 10% made it the whole way. Those people are seen as selected/destined for this mission = **founding myths of PRC**

  - Yan'an is like Qin's captial which is **easy to defend,** and easy to send troops
  - but living condition was quite tough: Mao lives in a cave home. But CCP are very friendly to peasants
  - important strategy of masses to the masses:
    - "all correct leadership is necessarily "==from the masses, to the masses==". This means: take the ideas of the masses (scattered and unsystematic ideas) and concentrate them (through study turn them into concentrated and systematic ideas), then go to the masses and propagate and explain these ideas until the masses embrace them as their own"
    - Mao believes **peasants are the vanguard = true masses** for this revolution v.s. Marxist called peasants a sack of potatoes

  | <img src="china_civ/image-20221206133748438.png" alt="image-20221206133748438" style="zoom:33%;" /> | <img src="china_civ/image-20221206134147004.png" alt="image-20221206134147004" style="zoom: 50%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

- Techniques in this mass movement

  - **study groups** = small group meetings like "book club" = express and criticize opinions on **socialists texts**
  - **struggle sessions** = performative displays of revolutions = 
    - encourage the peasants to come out and speak publicly accusing landlord of this crimes = directly confronting
    - this act of humiliation for landlord could be more important than the land transfer = **symbolize transfer of power from rich to poor**
  - everyone in the group has to participate in the above activities
  - really emphasize **small groups + support from locals = decentralized** idea instead of purely top-down structure to rely on center
    - e.g. land reform = abolishment of landlords and returning lands to peasants aims to support locals

- sees himself as the interpreter of Marxism-Leninism in Chinese context, and a master of guerrilla warfare

- **in 1949, Mao's proclamation of PRC**

  <img src="china_civ/image-20221206135101458.png" alt="image-20221206135101458" style="zoom:33%;" />

  this painting used more as a political tool, later on Deng Xiaoping is added, even though he is not even there

- PRC's political organization: **party supervising government** = so really it is a one party country (a feeling of authocracy)

  <img src="china_civ/image-20221206135407607.png" alt="image-20221206135407607" style="zoom: 33%;" />

**Land Reform Continued** after PRC founded

- **First phase** of land reform

  - before it is mostly north in Mao's region, now also includes south of China
  - continues struggle sessions etc; redistribution of land = broke up the older elites = CCP to introduce modern modes of agriculture

  - also used as a reward for peasants for supporting CCP
  - problem: **small land holdings is not a productive way** for high agriculture output

- **Second phase**: farmers join together = **collectivization of agriculture**

  - households to help each other and *forced* into cooperation = collectivization
  - hope to further increase agricultural output = further support modern inditurialization
  - but many felt **discontent** because again lost ownership of it = shared responsibility

  <img src="china_civ/image-20221206140000494.png" alt="image-20221206140000494" style="zoom:33%;" />

**America and Korea**: War to Resist America and Aid Korea (195053) 

- recall that US is against Soviet Union = communist. However, they know they cannot exterminate communists.
  - in order to use Japan as a base to **contain communism in Asia**
  - communist ideology is that **every one should be communist** = China joins then it is helpful = also for building alliance for Soviet

- Therefore, America now is against China/refuse to help Korea against America
  - decided to help North Korea as if it is lost, America would attack China
- Consequences of the war include
  - China more close to Soviet for econ assists
  - confirming US imperialist ambition in East Asia, as US intervention in Korea
  - suffered high death tool and called US is a paper tiger
  - difficult to get Taiwan *fully* back in control = with **US-China relationship on Taiwan** a problem until today
  - also emerges campaigns in China against foreigners
  - Mao lost the elder of his two sons; and the other son has mental trouble = lost his successor

**Five Year Plans**

- PRC's **First Five-year Plan**: won the Korean War, and land reform aimed to help industrialization as a model from soviet = **invest in heavy industry**. 

  - indistural output went up by 25%, significant number
  - quite successful, Mao very confident and listened to intellectuals outside CCP to speak up = **Hundred Flowers Campaign** = expecting only mild criticism
    - turns out this campaign brought *heavy* criticism = accused of monopoly of power 
    - then Mao retaliate = **anti-rightist campaign** to force those labeled as rightest forced for re-education

- second five year plan: the **Great Leap Forward** (1958-1960). Sense of urgency to catch up with other developed country in the west

  - e.g. surpass the UK and catch up with the US in **steelmaking**
  - a feature of this leap forward = people's commune
    - 5k households within a commune = so that if women have to go back and cook, takes time away
    - aim is to **scale** it by cooking for all = saves time = **more efficient** and productive
  - but this could give problem that peasants could eat 6 month of rice in 20 days = stock is shared in commune but not in own home = later **famine disaster**

  <img src="china_civ/image-20221206141317264.png" alt="image-20221206141317264" style="zoom:33%;" />

  - everyone setup their **backyard steel production** = again scaling;

  - but overall this divert attention to agriculture = reduce in agricultural output = great famine later

    <img src="china_civ/image-20221206141718230.png" alt="image-20221206141718230" style="zoom:33%;" />

- but peasants were very enthusiastic about it. Baby named after those concepts:

  <img src="china_civ/image-20221206141910439.png" alt="image-20221206141910439" style="zoom:33%;" />

  however, we know now it is a ==economic disaster== contributing to the great famine

**Great Famine** 1959-61

- caused by the weather and drought, but also the ==Great Leap Forward fiasco==: 30 million **starved to death**

- historians believe that the problem is not absolute lack of food but the systemic **flaws or decisions that prevent food getting to the people**

- anyway criticism within the party of the Great Leap Forward = Mao took responsibility and **resigned in 1959**

  <img src="china_civ/image-20221206142246061.png" alt="image-20221206142246061" style="zoom:33%;" />

  **more pragmatic people Liu Shaoqi and Deng Xiaoping took power** = more into Marxist materialistic base than the idealized vision

- new leaders = **economic recovery** in 1966

## Economic Reforms and Political Authoritatianism

**Previously on Mao's China**

- Peoples Republic founded 1949, CCP pursues Soviet-style socialist construction, emphasis on heavy industry, **collectivization**, reorganizing grassroots society to unlock labor power; Mao promotes rapid development $\to$ Great Leap Forward 1958
- **Great Leap** is an economic disaster; in CCP leadership reshuffle, ==pragmatists Liu Shaoqi and Deng Xiaoping rise==; Maos desire to regain influence factors in launch of the **Cultural Revolution** 1966-69 factional warfare, civil unrest, paralyze nation
  - his goal can be said partly to regain his power, but also as a socialist "ideologist", he believes in social egalitarianism and sees Deng's policy on economical and political reform **astray from the correct path** = cultural revolution

> Some key terms:
>
> - The term "**bourgeois**" has a specific meaning in Marxist theory. It refers to the middle class, which is seen as the class that controls the means of production in capitalist societies. In a Marxist framework, the bourgeoisie are seen as the **enemy of the proletariat**, or working class, because they exploit the labor of the proletariat for their own benefit.

**The Great Proletariat Cultural Revolution** 1966-1969/76

- Mao believes that:

  - the method to solve all problems is by **continuous revolution**. New contradictions/problems will always come up, and class struggles should be used as a tool to solve those new problems
  - believes in social **egalitarianism** and no elitism, that policies should be made by the people for the people
  - In Mao's view, there are "**bourgeois**" elements within the Chinese Communist Party, i.e. those who had **abandoned** the party's revolutionary ideology and had ==embraced more moderate, reformist policies==. These individuals were seen as a threat to the party's revolutionary principles
  - Therefore, party needed to be purged of what he saw as "bourgeois" elements, and he ==launched the Cultural Revolution== to achieve this goal.
  - another perhaps popular view is in his attempt to **regain power/influence** in the party

- 1966 Mao wrote a poster "**bombard the headquarters**" = making a war on the party that he established

  - this Cultural Revolution first lasted until 1969
  - again, he utilize his famous strategy of mass movement = **red guards** (young women students)
    - shows that even not in party, he still have **wide-spread support in people**
    - combined with his earlier mistakes, it can be said he is a person better at ***moving* the masses** to, e.g. revolt, than to govern
  - the goal is to remove those bourgeois elements from the party

  <img src="china_civ/image-20221208132106538.png" alt="image-20221208132106538" style="zoom:33%;" />

  notice that they are holding "==Mao's little red book==" = Quotations from Chairman Mao. This book is seen as **bible for cultural revolution**. 

  Other forms of "struggle" within this revolution include:

  - "criticize and struggle" publicly criticize/beat teachers

    - recall that during the anti-Rightest movement (after 100-flowers), many intellectuals were condemned = had a bad reputation
    - A lot of victims committed suicide to avoid humiliation
    - **Deng went to exile**, and President Xi's father was also denounced in this period as well
    - an perhaps "unexpected" chaos as Mao did not specify "who to criticize against", thereby causing ==wide-spread chaos==

  - rebels also followed Mao's call to "Smash the Four Olds": old thoughts; old culture; old customs; old habits. For exapmle Confucianism

    <img src="china_civ/image-20221208133146032.png" alt="image-20221208133146032" style="zoom: 33%;" />

  - the idea of being permitted to defy superior, **rising up again established authority** is a theme in this period

- Deng's Solution: redirect Young people's energy is to drive them ==to countryside== = 20mil displaced to country side. **Propaganda** below seeming to convince them go there

  |                          Propaganda                          |                         President Xi                         |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="china_civ/image-20221208133344893.png" alt="image-20221208133344893" style="zoom:33%;" /> | <img src="china_civ/image-20221208133652739.png" alt="image-20221208133652739" style="zoom:33%;" /> |

- ended finally when Mao passed away

**Deng Xiaoping's Period**. Whoever is the leader of the party = the man that exert **most impact**. However, the paths taken by post-Mao China were NOT solely dictated by CCP leaders such as Deng

- at the end of Mao, already some significant changes: **United States President Nixon's China visit**

  - In 1972, President Richard Nixon made a historic visit to China, where he met with Chinese leader Mao Zedong. 
  - This visit marked the beginning of a ==thaw in relations between the two countries==, and it paved the way for the normalization of diplomatic relations between them. Prior to his visit, the United States and China had been on opposite sides of the Cold War as China side with Soviet
  - One of the main reasons for the ==deterioration of relations between China and the Soviet Union== was ideological. The two countries were both communist states, but they had different interpretations of Marxism-Leninism, the ideology that guided their governments. 
    - The Soviet Union, under the leadership of Leonid Brezhnev, embraced a more moderate and pragmatic approach to communism
    - while China, under the leadership of Mao Zedong, took a **more radical and revolutionary** stance. This led to tensions between the two countries, as they competed for leadership of the international communist movement.
  - therefore, Mao begins to see Soviet as the biggest threat and now **tactical cooperation with US might be worthwhile**

- after Mao's death, three groups competing for power after Mao:

  - **the Gang of Four**, based in Shanghai and more like a clique controlled by Chairman Mao, lead by his wife Jiang Qing

    - but after Mao died, they were purged
    - Jiang escaped by viewing herself as Maos scapegoat: Im just a dog of Chairman Mao, and I bite whoever he wants me to bite!

  - **Hua Guofeng** who is a young official gaining Mao's trust

    - Two whatevers: We will resolutely uphold whatever policy decisions Chairman Mao made, and unswervingly follow whatever instructions Chairman Mao gave
    - but quickly forced out of power by Deng

  - **Deng Xiaoping**'s group, who eventually succeeded as he was a core member and had wide spread connections in the party

    <img src="china_civ/image-20221208135041294.png" alt="image-20221208135041294" style="zoom:33%;" />

    - once said: Should China one day become a superpower that bullies, invades, and exploits people everywhere, then the world should expose it, oppose it, and work together with the Chinese people to overthrow it. = was **advocating peaceful existence between countries with different ideologies** (unlike Soviet)
    - connections within the party:
      - **Long March** veteran
      - he never turned on his colleagues despite being purged twice during the Cultural Revolution
        - hence they felt grateful
    - he then successfully **drove out Mao's chosen successor** (Hua) to reform
      - reform direction: Deng distanced the CCP from the Cultural Revolution, from Mao, and from the Soviet economic model
      - Deng installed reformists in positions of power.

- In 1981 "Resolution": on failures of CCP and **enabled the party to distant itself with Mao** = ==start new goals==

  - some denunciation on Mao = cultural revolution was an error
  - but still recognize Mao had vital contributions (especially early years), still respected and beloved
    - after this little red book disappeared/decayed, etc.
  - but still not really solving any fundemental change, e.g. structural change, that could prevent this from happening again

- Deng's goal for China was to 

  - ==modernize/reform the economy== and improve living standards, while still maintaining Communist Party rule 
  - argues that poverty $\neq$ socialism, and ==poverty is problem== = very different from previous
  - hence a somewhat ==capitalist== approach

  <img src="china_civ/image-20221208135840708.png" alt="image-20221208135840708" style="zoom:23%;" />

**Rural Reform under Deng**: Household responsibility System

- recall that the Great Leap Forward was an attempt but a failed one

- reform not a top-down decision, but 18 farmers came together to signed. The idea is that land is shared but you **get to keep what you grew** (in addition to what you should grow/submit to government)

  <img src="china_civ/image-20221208140056290.png" alt="image-20221208140056290" style="zoom:33%;" />

  - After this treaty, every farmer in this village competed to produce more as compared to the Great Leap Forward
  - called the first capitalist village

**Urban Reform**: smash the iron rice bowl

- ==under Mao, there were no private enterprises==, all are controlled under the state.

  - Workers belonged to their work unit (after graduation) and had lifetime job security.

- Deng's reform: ==privatize many state-owned enterprise==s, so for workers in inefficient enterprises **gets laid off**

  <img src="china_civ/image-20221208140509562.png" alt="image-20221208140509562" style="zoom:33%;" />

  - though lead to economic development, but income **inequality comes up** as an inevitable problem

- Opening to the world: ==Special Economic Zones== (SEZ)
  - The SEZs were designed to be areas where **foreign investment and economic development** were encouraged, in order to stimulate **economic growth and create jobs**.
  - also lead to China's later entry into WTO
  - Shenzhen would be an example of successful ecnomoic zone
- by introducing this reform, it moved away from socialism towards ==capitalism==: "**Socialism with Chinese characteristics**"

**One Child Policy** (1980-2011): more people due to improvemened of public health

- rationale: China had already been burdened to lift 1 billion people out from poverty, which was Deng's goal. Hence in 1980s this comes up

  - of course had propaganda

  <img src="china_civ/image-20221208141920760.png" alt="image-20221208141920760" style="zoom:33%;" />

  - but had a lot of problem/criticism e.g. female fetus the victum
  - note that it is only targeted at Han, so not to exterminate ethnic minorities

**Political Authoritarianism**: Deng and leaders seem themselve as authoritarian

| <img src="china_civ/image-20221208142201727.png" alt="image-20221208142201727" style="zoom:33%;" /> | <img src="china_civ/image-20221208142406020.png" alt="image-20221208142406020" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |

- "Economic reform does not to imply **political liberalization**"

- Tian'anmen Protest in May 1989, ==also refered to as May 4-th Movement==. Tension/demends include

  - Freedom of the press: The protesters called for **greater freedom of the press**, so that journalists could report on political and social issues without fear of censorship or reprisal.
  - Freedom of speech: The protesters demanded the right to **express their views** and opinions without fear of retribution.
  - ==An end to corruption==: Many of the protesters were concerned about corruption in the Chinese government
    - An example is that children of elites can get into Tsinghua and Peking University easily
    - party official has the priviledge being more wealthy (result of captialism)
  - Political reform: Some of the protesters called for greater political reform, including the introduction of **democratic** institutions and the expansion of civil liberties.
  - fundementally it is caused by wide-spread discontent, result from factor such as 
    - more **lay-off**
    - before **controlled rice price**, but now it shoots up (as market is opened) and gives resource tension and inflation

- lasted for 2 month, the party become **split between liberal and conservative factions**

  <img src="china_civ/image-20221208212944976.png" alt="image-20221208212944976" style="zoom:33%;" />

  - ==liberal== = more moderate, we should talk to students, listen to their complaints and reform
    - reformist such as Zhao Ziyang argued that the economy should be opened up to market forces, and that political reforms, such as greater democracy and respect for human rights, were necessary in order to modernize China.
  - ==conservative== = repress those news to prevent revolution, very sensitive to revolution (hence also sent troops)
    - Deng Xiaoping and his supporters believed that the economy should continue to be planned and **controlled by the state**
    - but they also believed that **market forces** should play a greater role in the economy.
    - in the end, believes that the party should have leading power in decision making
  - Both believed in the importance of market forces in the economy, but differed **on the extent** to which market forces should be allowed to play a role

# Final Exam

Structure

- 8 ID, pick 5
- 6 pick 2 primary sources: what is says, how does it say it, etc
  - who, what, when, why, how = analytical
  - significance 
  - contextualize what source I am talking about
- a decent essay

Overall idea:

- Significance of each event
  - e.g. foot-binding significance = commercialization of women + the entire economy

Key terms

- **Foot-binding**: 
  - in Song, foot-binding started among low-class women who were performers/entertainers = so poor had to sell themselves; commercialization of women also during the great movement from north to south = security of women at most agile point = all people are unsure about their worth as now you are going south; women competing in courtesans desired foot-binding = mothers had to also get their daughter into the mold though it is a hard choice
  - in Qing, tried to ban foot-binding but didn't work = ethnic Hans couldn't because foot-binding became a social status = being foot-bound you don't need to work in the fields/households , and had servants whose feet were not bound = somewhat an elite status; with the fall of Qing that practice is now gone

- **Canton system**: Hongs; international trade; 
- **Qianlong Emperor**: golden age in Qing; Qing really started to expand territory, e.g. Xinjiang under Qianlong; supported some aesthetic engagement; foreign trade
- **Neo-Confucianism**: HuangChao rebellion wiped aristocrats with power; Zhu Xi
- **Yam System**: serious of outposts so that horse can gallop = allows for speedy transfer of information = how quickly info can propagate
  - The postal system, known as the Yam system, was a sort of medieval pony express with stations positioned at intervals of 20-30 miles. At each station, an "arrow messenger" would mount a fresh horse and ride to the next station at a full gallop.
- **Kurultai** group elect a new leader from a group; elder male chosen as "Kurultai" = next Khan; an election process
  - a kuriltai (also spelled *kurultai*; general assembly) of Mongol nobles was convoked in order to elect the new great khan
- **Chinngis Khan**; **Kublai Khan**
  - *Temjin* (original name) crowned as the kurultai to become Chinngis Khan; unified his steppe tribe; made a lot of invasions = feared ; disable steppe system and created autocracy in the steppe empire
  - Kublai: a switch of inward facing history of creation of empire; most people consider the founder of Yuan; invaded southern Song; sinification when ruling; differ with Chinngis as how you rule = cannot cut and paste nomadic strategy to thrive; start of *governance* = focused more on social and economic issues rather than military conquest
  - influence of nomadic culture on China
- **Columbian Exchange**
  - got plants, tomatoes, etc = nutritional = well suited for growth in China; social implications = growth of population
  - also diseases
  - deficit of silver; Zhang Juzhen = chief secretary; credited to transit from agrarian to; encourage people not to follow the Confucian; policy really benefited the main economy = increasing demand of silver ; as a side effect boosted power of merchants = 
    - 1679 Tokgawa shut down european and halted silver import but still came in = no tap silver inflow
    - made paying taxes in this form impossible = people horde their silver = copper to silver went into decline 
    - Colombian exchange also had the inflow of silver to China=the increased supply of silver led to a *devaluation* of the metal, which had previously been a scarce and valuable resource.
  - meaningful engagement with US
- **1911 revolution** ended Qing; Xinhai Rebellion
  - Sun Yat-sen needed some military backing and made agreement with Yuan
  - warlord period after Yuan died in 1916-1927
- **New Cultural Movement v.s. May 4th Movement**
  - May 4th 1919: treaty of Versailles with Shandong transferred to Japan angered students = shift focus in China towards soviet unions instead of western power = felt like westerns have abandoned them even though they are all ally in WWI
  - but May 4th Movement $\neq$ this event only, but more the idea of *shift away from traditional Confucian education* to a more modern science and democracy + vernacular literature (e.g. Lu Xun)
  - New Culture movement => New Youth periodical
  - what does it has to do with 1911: 1911 being political changes, while these are more literary/and cultural changes to China

Long Essay

1. a
2. a
3. opium war and boxer rebellion
4. a
5. a
6. e.g. May 4-th; what they advocated for = departure from confucian edu values; how does society change but still persisted from the traditional path

# Notes on The Open Empire Book

This section contains reading notes for the various related chapters of the book "The Open Empire: A History of China to 1800"

## Ch.1 Beginnigns of the Written Record

Written records were mostly for written on **tortoise-shell bottoms**, or called **plastrons** (), dating to the reign of King Wu Ding() for the ==Shang Dynasty==.

- texts were transcribed onto either plastrons or sometimes scapula of cattles. Specically, this is done by 1) preparing the bones 2) heting it up 3) the bone cracks 4) interpret the message from cracks and write the questions/answers onto it

  <img src="china_civ/image-20220911153533594.png" alt="image-20220911153533594" style="zoom: 67%;" />

  how was this discovered? This is first discovered during 1899 Beijing, during Malaria, when some people give out "dragon bones" (scapula of cattles and turtle shells dug up in large quantities from Anyang) to be grinded into powder for medicine. Then, some scholar found scripts on those dragon bones, and hence lead to realize those are actual texts.

- For King Wu Ding, **Anyang** ()was the capital

- finally, ==Shang== is succeeded by ==Zhou== by the concept of **Mandata of Heaven** ()

*How Chinese Characters Work*:

- most well-known examples of chinese characters resemble the concept they depict
- vast majority of cahracters contain a radial () to convey the general topic of the word, and a phonetic element to indicate the sound. For instance,  means elephant or image, but if added a radical , it means image.
- some people also say that chineses are inefficient, indicated by lower literarcy rate in China v.s. US. However, in places such as Hong Kong, Singapore, and Taiwan, literarcy rates are higher.
- Finally, the advent of  is largely ue to systems to trasnscribe sounds of Chinese into alphabet to be studied by the Western.

*The Advantages of Chinese Script*

- Only few scribes can read or write at that time
- since written chinese characters are decoupled from pronounciation, you can keep your spoken language but just **assign Chinese characters to preexisting words in your language**. This gives rise to the diverse dialects but still a uniform written system.

*The Content of Oracle Bones*

- Ritual related questions such as: What should be sacrifiesed? In what quantity?

- Political questions such as: Sohuld the king send this troops to a certain kingdom?

- the layout of a typical script on oracle bone consist of:

  ![image-20220911155732457](china_civ/image-20220911155732457.png)

  ![image-20220911155718282](china_civ/image-20220911155718282.png)

  where the **preface** explains which priest presided and which date, **charges** states the topic, **prognostication** gives the interpretation of the cracks (i.e. predictions) and **verification** is the actual outcome

- do not conclude from the above that Shang preferred sons to daughters, because there are cases when "good" births were of daughters.

- Found a lot of oracle bones, indicating that **divination occured daily**

- Finally, scripts on plastrons are also used for **teaching**, where we found teacher scripts with cracks along with student's messy and error-containing scripts on plastrons without cracks. This indicate that plastrons are plentiful during the time and students do not need to write on other materials first (if they did practice, then their characters would be better formed!)

*Discoveries at Anyang*

- Lady Hao, one of sixty-four consorts of the Shang King Wu Ding, has a tomb contained 3 ivory carvings, close to 500 bone hairpins, over 500 jdaes and nearly 7000 cowry shells (used as money)
- this is one of the only complete tomb excavated, withot the disturbance of gold diggers/theives

*Art of Making Bronzes*

- A large of amount of Bronze vessels found at Anyang and Erlitou, suggesting that it is a society with significant resource to produce those at such a large quantity.

  ![image-20220911160601075](china_civ/image-20220911160601075.png)

*The Antecedens of Modern Chinese Cuisine at Anyang*

- people at Anyang produced many different type of bronze veseels for cooking meat, holding cooked grain, etc.
- most cooked dishes were meat stew
- resistents of Anyang believed their **ancestors retrained their desire for food**, even when dead

*The Pyramid of Shang Society*

- This **power pyramid** can be found in the tomb: underneath the nature gods stood the ancestors of the royal family. The long-dead outranked the more recently dead, who in turn out ranked Shang king.
- Shang believed one could communicate more easily with those in nearby tires, and offerd a lot to recent dead.
- Member of the royal linage and women from the families who could marry with them constituted the aristocracy of the Shang.
- Shang believed the dead king would occupy this tomb after his death, and because he neded the service of all those who worked for him while alive, **they had to perish when he died**.

*The Nature of the Shang Polity*

- His men rounded up livestock from the residents of places the royal entourage visited, but his officials **did not collect taxes** on a regular basis from a set territory

- therefore, in what sense did Shang constitute a state? It is perhaps more like a soft state, whose stability hinged on the king's frequent visits to his subordinates and their domains, **whose bureaucracy remained divine** rather than human.

*The Illiterate Contemporaries of the Shang in Sichuan*

- the discovery of the Sanxingdui  in Sichuan illustrates the **cultural variety of Bronze Age China during Shang**
- artifacts in Sichuan different dramatically from anything at Anyang

*Mandate of Heaven*: from ==Shang== to ==Zhou==

- According to Sima Qian (most critics argue this might not be accurate), the last Shang king liked the company of owman, drnak too much, enjoyed depraved songs, etc

- Many officials left Shang to serve Zhou, and when Zhou's advisor urged him to invade Shang, he says "You don't know the **Mandate of Heaven** yet"

- Then the last Shang king killed an offical who dared to criticize him, by cutting his chest open wile alive, **Zhou king launched the invasion and won**.

- from then on, people start to believe that:

  - if emperor could not govern and is soon revolted, the Heaven has retreated support and revolter would have the new Mandate of Heaven
  - but if the revolution failed, then it is said the **Mandate of Heaven is retained**

  as a result, Mandate of Heaven is practically a 

*Zhou Conquest of the Shang*:

- once ==Zhou== had defeated the ==Shang==, they established their capital along the Wei River ().
- Until 771 B.C.E, when Zhou abandoned their capital in the Wei River valley (near ), historian then called the period ==before this Western Zhou==, and the period after such a move ==Eastern Zhou==

*Divination in the Zhou: The Book of Changes*

- Like the Shang, the Zhou also divined with help of bones and shells, but also used a **leafy herb with white flower called yarrow**, where diviners manipulated the **stalks of yarrow in group of six and decided fate based on hexagrams.**
- From , there are mentions that commoner men can also perform divination with both shells and yarrow stalks, such as for marriage

*The Book of Songs* 

- contains the first songs from anceint China, and depicts vividly the elite and the **lives of commoners**
- these recorded songs are culled from 3,000 to 306 by **Confucius** (c.a. 551-479 B.C.E)
- the ealiest songs record ritual chants, but later ones include contents about battles and eventually love

*The Agricultural Year*

- A late song details many economic exchanges between the lord of estate and his dependents

  - the lord's obligation is to **provide people with clothing**
  - commoner women's obligation is to **provide silk and thread to the lord**
  - commoner men's obligation is to provide **animal skins**

- an example would be

  <img src="china_civ/image-20220914152436952.png" alt="image-20220914152436952" style="zoom: 50%;" />

  which gives a sense of life in ==Western Zhou==:

  - people provided their lord with **a share of crop, hunt, and textile made**
  - the lord provide them with **clothings** and **invite them to feasts**

- but there are also other songs when people are **dissatisfied** with their lord, in which case they may threaten to leave the lord

*Bronzes of Western Zhou*

- besides Book of Songs, **inscriptions on bronzes also provided an outline of government and economy of Zhou**
- contents include:
  - how ==Zhou kings== also worshiped their ancestors, like ==Shang==
  - many rituals are conducted by inscribing on a **bronze bell**, so that when **struck**, the living hoped messages would be conveyed to the dead and perhaps wishes granted

*A Zhou Warrior and the Reward for his Service*

- one text described a battle when Xianyun people raided Zhou garrison, who were defeated by the contribution of the Zohu warrior Duo You.
- king then **award him lands**, with people in that land obligated to serve the lord
  - since men in the lower rank can rise, it can be seen as a **form of early bureaucracy**
- hence, Western Zhou can be seen as "**the aggregation of thousands of pieces of land woemn together by the political power of the state**"

*Ancestor Worship*

- the Book of Songs make it possible to reconstruct the **ritual context** (i.e. exactly what happened) of the Western Zhou bronzes
- basically in one song, what happened is:
  - a young boy in the clan designated as the impersonator for the spirit to enter his body
  - chief of the lineage conducts the ritual
  - an officiating invoker serves as the go-between connecting the impersonator and the pious descendant.
  - then, living family members requets good fortune and long life from the dead
  - officiating invoker then consults an oracle
  - and the "spirit" replies

*The Zhuangbei Hoard: Ritual Revolution?*

<img src="china_civ/image-20220915203850551.png" alt="image-20220915203850551" style="zoom:40%;" />

- in 771 B.C.E, Zhou is invaded and abandoned their captial near Xi'an and **fled east**. Then the Wei family, fore fled, buried seventy-five vesssels and **twenty-eigth bells** in a bit (maybe hoping to recover it when they are back)

- this pit is now found, and it is realized that objects can be categorized distinctly into ==two types==, one ealier dating to 1050 B.C.E and one later to 880 B.C.E

  - the ealier vessels were having exaggerated, delicate patterns, such as animals and tao-tie
  - the later ones become flatter and stylized (**simpler**), and the tao-tie motif beame a little bit abstract

  therefore, it is hypothesized (largely agreed) that a ==ritual revolution== occured c.a. 850 B.C.E, and she explains by:

  - ealier ones belived in shamnic travel to he world of spirits, and have no limits on how many vessels a certain person can have
  - later ones perhaps have a decline in the belief inanimal spirits, and is **only permitted to have a certain number of vessels**

*The continuity of Shang and Western Zhou*

- both shared
  - belief in oracle bones and divinations (Zhou additionally used yarrow stalks)
  - both are soft states, with incipient form of tax and uncertain borders. Gifts from subjects to rulers are the major form of income
- they differ
  - Zhou also have some written records of the commoners
  - Zhou is beginning to have bureaucracy system
  - Book of Songs described a gender split in Zhou: men could farm and hunt, but women needs to gather mulberry leaves, spun thread, dye textiles and making clothing. Also inscriptions on Zhou bells chime the prayers of the living for their sons and grandsons, **not for daughters and granddaugthers**.

## Ch.2 The Age of the Warrior and The Thinker

> **Here we are featuring:**
>
> - Spring and Autumn Period , and The Warring State Period , both of which is during the ==Eastern Zhou==
> - Chong'er (=) from the Jin family.
> - The period of Confucious  (died near the start of ), Zhuang Zi , Mencius , and Xun Zi 

*Start of Easter Zhou in. 771 B.C.E*

- in 771 B.C.E, two dependent states of Zhou made alliance with some tribal people to overthrow the Zhou King.

- The defeated ==Zhou== shifted their captial to the east **near the modern city of Luoyang**, and became more like a **symbolic head** as the competing regional leaders at far superior armies

- There were some **more than 100 polities at that time**, and non had siginificant power to conquer all its neighbors. Besides them, there are also groups of "less-civilized" people who do not have their own written records

  <img src="china_civ/image-20220917124507405.png" alt="image-20220917124507405" style="zoom:33%;" />

  the four major "non-chinese" groups are therefore the Rong, Di, Man, and Yi, whom although is less civilized, but many chinese had contact with them to learn **battling skills** and cavalry (see later)

- In the end, Easter Zhou is one with 6 centuries of warefare (770 to 221 B.C.E), and is usually considered into two halves:
  - ====: named after the book *The Spring and Autumn Annals*, but actually the book that stores most of our knowledge from the period is *The Commentary of Mr.Zuo* ()
  - ==== (the Warring States Period): when the Zhou has became so weak that everybody is fighting between themselves to give birth to the next dynasty

*The Commentary of Mr.Zuo and the Society it Describes* 

- keep in mind that this is written 2 centuries after this period

- More than 100 polities, described over 500 battles, and more than 100 civil wars within polities as well.
- At the beginning of , a hierarchy of **birth deciding which linage a man belonged=social status**
- but even at this difficult time, there are several people who fought their own way to be recognized as the "==lord protector=="
  - Duke Huan of Qi
  - Duke Wen of Jin , also called Double Ears (Chong'er)

*The Struggle of Double Ears to Win His Realm* (Jin)

- the Rong wife (of the current ruler) who bore Double Ears neber necame a favorite (e.g. because Rong is non-chinese)
- Wome at the time was also having low status:
  - because marriage = building alliance, usually ==bride + female relatives are sent so that a son can be born==
  - therefore, all mother can do it so ==angle for the success of her son==: 
    - sent her son to the territory of the Di people
    - Lady Li (mother of Double Ear) created the impression that the designated heir (for crown) sought to poison his father. But to accuse Lady Li means to offtend his father, the heir decided to kill himself.
    - Awaited the time when their father died, and thought this is the time
    - **Duke's ministers could also took** over the throne by killing her sons
    - So Double Ears were travelling (e.g. making friends with Chu) before returning to take over the Jin polity with help. To obtain the help, he made a promise to the Chu King that if they meet on battle, he **will withdraw his force a distance of three day's march**
- finally took over, and he:
  - enlarged his army to three times the size
  - met the Chu force, did the retreat as promised, but Chu still engaged. Double Ear won by ==tricking== them tht they are retreating
  - since he has shown to be a powerful ruler, he is finally appointed by the ==Zhou King== to be a **lord protector** and awarded him the new rank.

*Changes in the Art of War after Death of Double Ears*

- Double Ears (636-628 B.C.E) marked the begining which ==military strength and innovative strategy become important==
  - people were trying to have many **chariots**, with about 10-30 carrying the chariot with 3 skilled warriors on it
  - but since chariots are costly and ineffective unless on flat plains, they lost to **infantry** , which also marked an era of political shift since it means **farmers could also revolt**
  - Di people took advantage of infantry by having a lot of them, and even ==erased the social distinction== that divided those who can be ON the chariot and those who ran along. In warefare with infantry, ==anyone can rise up the ranks== to become a general, and social flux is encouraged.
  - the above is adopted by other rulers later as well
- Following a need for army is a rise in tax, literarcy rate (e.g. people can rise up to be the King's advisor), and agricultural productivity
  - started the use of **iron tools** instead of wooden ones, possibly introduced by the Europoid.
  - the rise of infantry and army size means **experts generals are needed** to take lead instead of royal lineages. They believed that the ==art of war can be taught==, and gives rises to books/teachings.
- The first book on strategems is **The Art of War ()**
  - stressed that the entire amry had to be trained to follow the orders unquestionably
  - "the highest skill is to bring the enermy's army to ==submit without combat=="
  - students should seek to **identify weakness in their army** and find the **right time to attack**
- But another new change soon emerged is the introduction of **calvary** by Zhao King (learnt from their neighbor)

*The Demand of Experts*

- rulers during the  **hire advisors on the basis of their skills**. A famous example is Feng Xuan:

  - the intermediary introduced Feng Xuan to the lord Mengchang:

    <img src="china_civ/image-20220917132734568.png" alt="image-20220917132734568" style="zoom:50%;" />

  - then, during a time when Mengchang wants to collect tax and Feng Xuan volunteered, he did not bring the taxes back but "forgive" all of them. The Lord is confused and asked why:

    <img src="china_civ/image-20220917132900022.png" alt="image-20220917132900022" style="zoom:50%;" />

- such sharp observation and fast decision is referred to as ==acumen==, being hungered for during 

*The World of Confucius*

- born to the Kong family in Qufu 
- China's first **private thinker** (i.e. not served as ministers or generals)
- The only source for understanding his teaching is The Analects (), which is a collection of conversations/teachings compiled by later students
  - again this is compiled much later after he died
  - conversations are first orally tranmitted, and then ==written down on bamboo (first source!)==
    - scripts on bamboo are written from right to left, top to bottom
  - the received text of  is compiled by having different disciples writting different sections, hence there are **sharp differences among how they perceive Kongzi and his teachings**

*The Teaching of Confucius as in the Analects*

- he likes to converse with peoplesx

- claims that he will , but did innovate by

  - how to behave virtuously and how to govern (ealier believed this skill is born with rulers)
  - the concept of ritual == is an important quality,== and if men employ this, society could be reformed
  - other ==essential qualities include humanness ==
  - during their parent's lifetimes, children should obey their wishes and after their death, children should continue for three years to conduct family affairs just as their parents had.
    - however, Confucious ==did not teach anything about spirit and supernatural==.

- Confucious seems to have ==only taught sons==. Why?

  - one possible explaination is that men's interest was to have a larger family unit, and since only men can worship their ancestors, only sons cold carry on the family line. Whereas women are interested in a uterine family  consisting themselves and their own children, there is a conflict.

- Confucious believed that one can influence the wolrd by behaving as a gentleman at home, and that with benevolence () all chaos will be checked

  <img src="china_civ/image-20220917135156728.png" alt="image-20220917135156728" style="zoom:50%;" />

  but later people did find it difficult to rule without force, and indeed Confucious was powerless when his city was attacked

- In 479 B.C.E. he died

*Ritual as shown on the Vessels of *

- many vessels from Confucious time survived until today, but it is found that they are **very different from the Western Zhou**.

  - no more animals, but human figures engaging in activities

    <img src="china_civ/image-20220917135515555.png" alt="image-20220917135515555" style="zoom: 25%;" />

- In Shang and Zhou, kings had retrained a monopoly over bronze vessels but by the time of Confucious, ==lower-ranking lords has taken over production and begin to record their own victories==

  - to them, ==ritual activities (a them in the Analects) is viewed differently== as attesting the effectiveness of power and violence, while Confucious wanted the rituals as a forum of the individual's development of benevolence.

*After Confucious*

- since the Analects is collectively edited by students, separate sections contain different views on the teachings, and hence there were a lot of disagreement = Confucians broke up into eight rival schools.
- the century after Confucious death then giave brith to some of China's liveliest and most intersting thinkers

*Mozi's Criticism* (50 years after Confucious)

- reformulated his teachings in response to Confucious
- At this period 5th century B.C.E, the fighting during the  has dwindled to **four major contenders - Qi, Jin, Chu, and Yue**.
- Mozi accepted certain basic Confucian prescriptions, but felt that:
  - bonds with acquaintances are overemphasized, e.g. thief steal from other families to benefit their own. Therefore, he proposes ==, which is that each individual had an obligation towards all other people in the society==
  - ridiculed that Confucious advocate for elaborate funerals but gives no discussion of spirits and afterlife. Hence he proposes measures for **controlling the cost of funerals**.
  - but he agrees with Confucious that gentleman had to remain engaged in the society for reformation

*The Way and Integrity Classic* 

- written by , and disciples of this school are called Daoist

- emphasizes the quality of Wuwei ==, which should be interpreted as non-interfence:==

  <img src="china_civ/image-20220917141028913.png" alt="image-20220917141028913" style="zoom:50%;" />

*Zhuangzi: The use of Paradox and Humor*

- while both  and 's teachings often addresses the ruler of the state, Zhuangzi focuses on the individual and presents a series of anecdotes

- he believes that people should ==abandon preconceived notions==

  - death should not be mourned

  - advantage of the disadvantaged

    <img src="china_civ/image-20220917141422248.png" alt="image-20220917141422248" style="zoom:50%;" />

- Zhuangzi also emphasizes on , letting things run their natural course, but does not seem to condemn deforestation

*Zhuangzi and Mencius* 

- Mencius is a contemporary of Zhuangzi

- Mencius believed that **men were born good, but the environment transformed /corrupted them**

  - an example would be deforestation, fked by the environment

    <img src="china_civ/image-20220917141929779.png" alt="image-20220917141929779" style="zoom:50%;" />

- Mendius, since believing in men, felt the people could be trusted to select a new rules, and the ==concept of Mandate of Heaven is to mean that a ruler could rule only with support of his subjects==

*Mencius and Xunzi: the Continuing Confucian Debate*

- Xunzi  is in contrary with Mencius, believing that **men were born evil but can improve via education**.
- like Confuscious, they focused on question of how to govern, but they were ==hoping more of a unification rather than restoration== (as the Zhou dynasty has became so weak)
- at the time of Xunzi, there were only three major power vieing: Qi, Chu, and Qin
  - Xunzi visited Qin just before their conquest, and saw that "They employ them (the people) harshly, terrorize them with authority, embitter them with hardship, coax them with r4ewards, and cow them with punishments"
  - hence ==predicted that Qin would not last long==

*Money Economy of the Warring States Period*

- In Shang and early Zhou, rulers used cowrie shells and cloth are currency, and their subjects exchanged goods

- In , the ==merchant class grew as trade across regions increased==, and that the economy were well-developed for trading:

  <img src="china_civ/image-20220917143052255.png" alt="image-20220917143052255" style="zoom:50%;" />

  because of the increase in trading, it leads to an ==increase in specialization==

*The Chaotic Pace of Change*

- constant fighting throught the period = large resource consumed = rulers taxing individuals

- a bunch of changes:

  - regional rulers displacing Zhou kings
  - chariot warriors displaced by infantry

  ==a great degree of social flux== also gave anciety for higher class, but also ==promoted literarcy==

- chaotic period also gave rise to all the different thinkers starting from Confuscious

- the **Legalist teachings**  in Qin was so repelled, by Xunzi, but they proposed a radically new way of organizing the state and its subject that allows its adherets to ==unite the realm for the first time==

## Ch.3 The Creation of Empire

*From the Warring States to Qin*

- Qin defeated the other riavals not because of any new technology, but because they found a new way to organize their state: Legalist ministers who advocated the **abolition of all privilegeds of nobility**
  - Results in very efficient system in Qin (in contrast to the modern view of bureaucracy)
- After the unification of China, scholars in ==Qin== and ==Han== begin to face the question: how should they **govern** talented officials? What role should the state play in the economy?
- during the Warring state, it was mostly priviledged aristocracy and the laboring masses. But after 221 B.C.E. it became mostly scholars, peasants, artisans, and merchants (merchants last because pesants and artisans produced something)
  - technically we still have slaves, doctors and religious specialists in addition
- Qin ended within 14 years of ruling, due to problems in the court and people unsatified=rebellion

*Brief history of Han*

- founded by a strong leader and succeeed by strong woman, Han enjoyed 200 years of competent ruling
- Interloper Wang Mang, tired of being regent (to young emperor), formed a ==new dynasty called Xin==
- within 15 years, other families of Han regained power and weakened the dynasty
- destruction of their capital at Changan forced them to move to Luoyang
- beginning of increased tax, and emperesses wrest control from weak emperors
- rise of enunuchs and Confucian students began protests, as well as many following Daoism
- lots of protest and rebellion, a disaster ended the dynasty

*The Legalist State*

- , who stuttered during an ear where eloquence is prized, had to record his idea on pen and paper
- believed that ruler should be detached from everyday business, and apply **unbending standard to judge officals and people**
- believed in a **law that treated everyone equally**, and that only the **systematic application of law** can control people, whose nature are evil (Xunzi)

*The Architecture of Qin's Success: Shan Yang's Reforms*

- Qin wasn't strong enough to take over all rivals until 's term in office
- Shangyang **disdain the past**, hence ==rejected Confucious values==, especially ritual, and sees it as pointless
- Shangyang greatly improved the fiscal of the Qin:
  - registration of households, and let people **supervise each other**
  - if criminal activity is not reported, chop heads
  - once a man reached 16-17, ==obliged to serve military== and pay ==taxes==
  - establishing the concept of ==ownership of land==
  - ==achievements in army gives promotion in rank== proportional to the achievement
  - the entire population of Qin is divided into ==ranks== = permitted clothing, land, slaves, households.
  - people (pesants/artisans) who contributed much grain/cloth to the state is free from tax
  - proposed rulers of small districts ==, who is in charge of organizing army==, carry out public works, collect taxes...
  - **standardized units** such as weight and length

*China's First Emperor*

- given a strong fiscal basis, King Zheng (later called ****) decided to attack all the other rivals at the time

- after destorying the other six kingdoms, King Zheng:

  - forced the royals from the six kingdoms to live near the capital (for monitoring)
  - emphasized on **farming** as the mainstay of the economy
  - **standardized coins** to be a circle with holes (so that they can be stringed together)
  - divided territories into **commanderies**, which is further divided into **counties**. 
    - Commanderies are like little central government:
    - they need to perform three tasks: civl matters, e.g. taxation, military affairs, and ==supervision of governmental officials==
  - the top officials in the central government include
    - chancellor, who heade the bureaucracy
    - imperial secratry, who drafted king's orders
    - grand commandant, in charge of military

- Qin has also done some **significant projects**

  - built **networks of roads** over 6,800 km long
  - ==dug irrigation cannals==, and some three hundred thousand men built extensive walls of pounded earch along northern borders
  - ==fearful of death==, he tried to obtain exilir for immortality but failed. Instead, he let seven hundered thousand men working for his ==tomb==, which was intested to replicate the universe as his permanet resting home in the ==afterlife==
    - for fear that workers may disclose the position of the tomb, all artisans and laborers who had worked there ==were imprisoned in the tomb== while alive
    - gives rise to the ==Terra-Cotta army== , who were thought to be troops that the emperor can fight with in the underworld
    - however, his tomb today was never excavated/found

- When Qin founder died in 210 B.C.E, he named his more popular first son to succeed, but with interloped by a powerful official named Zhao Gao

  - took over as a chancellor

- but the greatest opposition is the **rebellion of the people**. This happened because there were laborers who are delayed by the rain, and that:

  - since they cannot complete their work, they "will die" (according to Han historians)
  - if they revolt, they die as well (according to Han historians)

  therefore, maybe they should risk for their own kingdom.

*Reasons for Doubting the Historical Record*

- Win ordered a ==large-scale book burning== in 213 B.C.E destroying all dissenting point of view

  - except *The Book of Song*  and *The Book of Documents*  were banned, and only agricultral and divination books permitted
  - for books such as , most content are anyway transmitted from teacher to student, hence book burning had no effect

- Jia Yi's Confucian view point, which emphasized humanity and rightenousness, provided Han dynasty the perfect justification to overthrow Qin

- but it was found later in a tomb in  Shuihudi, where a Qin clerk lived a life as a hearing for crimal suit and took ==several legal writings with him in the tomb==

  - described a detailed judification system where poeple would often be ==beaten as a punishment instead of killed==
  - women also had work outside the typical sewing: some were working in ==lacquer production with signed vessels==

  however, it is *prescriptive* of what sohuld happen rather than *descriptive* of what actually happened, so it should also be taken with a grain of salt

*The Founding of Han*

- peace during  but as soon as he died and much succession problem happened at palace, many of the former regional states broke away
- bona-fide peasants  and low-ranking official **rebelled and started a new dynasty**
  - Liu Bang is very social but crude, hence have a lot of drinking friends at inn
  - when he won the battle against Qin, he pleged to abolish all other Qin's law but only keep a) those who kills others shall die b) he who harms others or steals should be punished
  - but in the end it is found that the ==Han dynasty took many Qin laws verbatim==
- one major depature from Qin is Han's policy on placing nobility all near captical. Instead, Han created new nobilities to his brothers and relatives and ==dispersed them==:
  - gave them titles of local Kings
  - 2/3 of the territory reined by sons and relatives, 1/3 by Liu Bang himself
  - but the core of Han is still ====
- but the first years of Liu Bang's reign was difficult, as it has to **suppress rebellions within China and defned against the powerful ** in the north
  - after Liu Bang's death, a new ruler in 209 B.C.E. lost to Xiongnu and signed a ==humiliating peace treaty== requiring China to present gifts of textiles, food, and wine, wile Xiongnu not to attack China

*The Reign of Dowager Empress Lv*

- Empress Lv  ruled for an extended period of time by placing infants on throne
- as an empress, suffered pressure even from Xiongnu (asking she to marry over)

*The World of the Regional Rulers:  finds*

- found ==Han's tomb in ==, with three of the four tombs almost intact

- most important find is ==Lday Dai's tomb==

  - Lady Dai's skin fully preserved

  - has a lot of cloths and wine vessels in the tomb

  - found much evidence of Han cuisine, e.g. meat stew and rice

  - her ==view of the afterlife== characterized by the T-shaped banner which contained three sections:

    - top: the immortal Lady Dai (spirit)
    - middle: Lady Dai existence in the underworld  
    - bottom: Lady Dai in tomb (soul)

    not a sequential event, but silmultanoues

  - migth not be literate due to no books in tomb

- other tombs are his sons, where a vast majority of books is fonud = reassess the **intellectual world of Han**

*The Han Dynasty Under Emperor Wu* 

- Emperor Wu elimited checks on his power, and then ==established a Confucian academy==, which is deeply affected by his advisor 
- Emperor Wu started the academy by 
  - first appointing five teachers who specialized in five books: ==, /, /,  ==
  - enrolling thousands of students, who **upon graduation can go into the government**
- Therefore, the procedure of hiring new officials become a) be ==recommended== and b) take the ==examinations==
- Emperor Wu also tried to battle against Xiongnu due to the unfair treaty, but neither achieved a decisive victory
  - during some of the battles, Li Ling founght hard but lost. He failed to suicide and became a slave
  - wanted to defend Li Ling, who he knows as a childhood friend, but got himself castrated instead of put to death because he thinks its even worse **if his literary work  cannot be left to posterity**
  - , unlike later books written by historians *funded by the government*, is ==self-funded and hence a more trust-worthy source==

*The Creation of Autocracy*

- Emperor Wu asserted control during his time by dismissed or sentenced to death some five chancellors, and became unchecked in power
- he also promoted  to serve as regent after his death, but unfortunately he never gave up power and just ==placed child emperor over another==
  - this cycle of regent people/empresses/enunuchs ==named themselves are regent and gained control== is common for later era

*Economic Problems in the Han*

- Since almost every measure Emperor Wu imposed, e.g. sending troops to Xiongnu and establishing academy required funds, some mechanism is needed

- solution: **government monopolies**

  - salt and iron, where the latter is immensely useful for making tools
  - later also assumed monopoly of copper, bronze, and wines

- but long years to fightings = lots of taxings = people suffering. Hence after  death investigation are sent out

  - scholars were against tradings and monopolies, while the government officials successfully defended it

- In an essay by Wang Bao, were a slave's life for a widow is mentioned, the econ for that time can be inferred:

  - there is a widespread network of small and wide markets
  - slaves are expected to travel from places to places to buy stuff
  - although a money economy existed, mostly they are used to **trade for luxuries**

  additionally, it meant the society can also be split into two types:

  - ==estate ownders==: collected large amount of wealth and became **increasinly powerful and influential**
  - ==slaves==: e.g. lost from war or seriously in debt

*Wang Mang  Regency*:

- Wang Mang wanted to ==suppress the power of large estate owners==, and founded the Xin dynasty

- Wang Mang's approach included limiting the maximum land you can have, and cessing some land from owner to poors

- However, his reign came to an end due to **flooding**, which caused

  - unsatisfactory landowners due to his policy
  - angry peasants

  hence he is killed and taken over

*The Restoration of the later Han*

- The later Han with captial in Luoyang is sometimes called ==Eastern Han== as it is east of Chang'an
- The captical has walls surrounded imperial palace, inns, canals, markest, and schools, and had a **huge amount of residents, students, and officials**
- since it is just restored, it still relies on large land owners and hence they have some power over the recommendation system and put sons into government
- the ==rise of eunuchs== in the palance

*Wang Chong's Skeptism and 's Family*

- **argued that many early arguments are made *without* logic**. For instance, people performing ceremonies to the spirit of earth is useless/unheard by the earth, just as people won't head anything about the lice on their skin.
- Ban's family is full of literate sons and daughters
  - Ban Gu, son of Ban Biao, resolved to complete the history of China written by his father was thrown into jail accused of distorting history
  - Ban Zhao , the daughter of , is one of the most famous women writers
    - *Lessons for Women*: unequal treatment of women and how she thinks married women should behave
    - later on ==sponsered by the court== to finish writing the history
- Ban Zhao's view of women:
  - four qualities women should have: "womenly virtue, womenly words, womanly bearing, and womenly work"
  - ==wife should not question her husband==, but husband should not beat wife either
  - ==plead for equal education==, as usually sons are educated by daughters not
    - also inferred difficulty of women: families abandon their daughter due to **marriage costs**
    - but powerful families prefer having a ==daughter married to emperor== so that they can ==seize power==
- Ban Biao was also served as the advisor to Emperor Deng

*Refusal to Serve in Government*

- but as the successor Emperor Huan could not bear the power of in-laws, used ==eunuchs to launch a coup and unseated the Liang family==, which is the family of his wife
- as Emperor Huan's needed eunuchs to succeed, he became very **unpopular** and people are rejecting his invite to serve. For example, one person who rejected explained:
  - holding office meant having to censure the emperor, and since critism will be punished, this is not good
  - if not censuring, then his **own "purity" as scholar is damaged**

*The Rise of the Organized Daoist Church*

- Instead of the righteous scholars, Emperor Huang tried to tap from the new Daoist
- Daoist belief at that time went under a major transformation
  - believe that == is a deity and can come to give advices==
  - split into two fractions,  and 
- the  Five Peck of Daoist movement believed that
  - instead of teaching selected adepts techniques to obtain immortality, teach a larger community
  - the leader  ==promised good health in exchange to peoples belief/rice offerings to Daoist deity==
  - the idea of ==linking illness with bad deeds== initiates here, and that people believe confessions is the cure
- the yellow turban planned for a rebellion , but
  - leaders quickly executed by military, under the lead of 
  - people quicly dissovled
- foreshadows the introduction of buddism and changes of the religious lanscape later

*The Legacy of the Han*

- much remains uncertain about the fall of Han, but popular theory suggests that the empire experienced a ==massive shortfall of funds==
- However, both Qin and Han left an **impression of unified China in peoples minds**
- during the 400 years of reign of Qin and Han
  - Chineses society assume the contours it would take today, and much market economy of the Han and government's role in regulating also persisted until today
  - women still has subordinate roles, but when opportunies arrive people such as Ban Zhao can break out of their traditional roles
  - the only siginficant change, which comes later, is spiritual: introduction of **Buddism**

## Ch.4 China's Religious Landscape

**Overview**:

- Buddhism, started as a foreign religion, won more followers than Daoism for the year 200-600
- Buddhism included life of lay people who live at home with their faimlies or monk who joined monasteries
- many reason underlay Buddihist's success: most important is the ability to **absorb elements of preexisting religions**, e.g. recruited many local deities as guardian. Therefore, you can worship both Daoism and Buddhism at the same time

- long-lasting political instability marked the centuries of China's conversion to Buddhism: for three centureies after the fall of Han, a period often called ==Six Dynasties (220 - 589)==, no regime successed in conquering more than half of China's territory (until ==Sui== dynasty that comes afterward)

**The First Buddhist in China**

- Buddhism is associated with **trade**, since the Buddha (ca 500 B.C.E) liked merchants show provided crucial financial support
- In 148 C.E. the famous missionary An Shigao from Parthia arrived in Luoyang, and began work on **translating Sanskrit texts into Chinese** (Sanskrit text is the text of Buddhist teaching)

**The Successors to the Han Dynasty**

- succeeded by the ==Three Kingdom period== (220 - 280), in which Cao Cao's **nine grade system** had a long lasting impact
  - in principle, a local official, the impartial judge, was supposed ot assign each candidate a rank from one to nine in the office
  - in reality, the pattern established during Han prevailed, recommendations were influential, hence sons of powerful families have advantage
- when Cao Cao's heirs lost power, the Sima family founded the ==Jin Dynasty==, who ruled northen China for barely half a century util broken up in the War of Eight Princes (290 - 306), so northern part becomes separated again with Jin retrained its control at Luoyang

- In 311, the **Xiongnu conquered the former capital Luoyang**

**Appeal of Buddhism to Northern Rulers**

- Shi Le, the ruler who conquered Luoyang, was once introduced to the Buddhist missinoary **Fotudeng**, who performed some miracle tricks and impressed him
- he then granted the Buddhists the right to build **monasteries**
- whether if Fotudeng can do magic tricks is tangantial here, the important thing is:
  - the news of this spreaded Buddhism
  - Buddhism offered ==non-Chinese rulers an alternative to Confucianism,== which is very Chinese
  - also, for ==women==, this offered a **new alternative than family life**: joining a nunnery. Justified by ==transfer of merit== (e.g. to family)

**The Difficulties of Translating Buddhist Concept**

- Chinese belongs to Sino-Tibetan language, while Sanskrit which has an alphabet, belongs to Indo-European
- also many different cultural assumptions: **Indians talked about sex much more freely** than Chinese, hence cautious translators dropped any mentions of kissing and hugging
- difficult to translate new terms/concept not in Chinese:
  - **early translators decided to use Daoism concepts**: Nirvana $\to$ wuwei, which is far from accurate
  - also difficult to translate ==karma==: one's next life could be reborn into an animal or person depending on behaviors; Daoism viewed a constant afterlife

- then in 344 - 413, Kumarajiva, a monk from Central Asia or from India, is sponserd by private individuas to come to Chang'an to perform **better translations**

**Buddhism in Central Asia**

- Kumarajiva is trained by a monk in Kucha, which is now part of China's Xinjiang, and many people there speak **Turkic language Uighur** instead of Chinese
- he studied Buddhist texts in Sanskirt, spoke Tokharian, and learned some Chinese from merchants who came to Kucha
- he was later kiddnapped to Gansu for 17 years, where he mastered Chinese, and he started to believe in the **Greater Vehicle**
  - the ==Lesser Vehicle== (Hinayana) held Nirvana possible only for the few who joined the Buddhist clergy
  - the ==Greater Vehicle== (Mahayana) offered salvation even to laity
- starting in 401, skilled translators and knowledgable monks also joined the translation project
- even though Buddhism has value for **celibacy**, there are many exceptions (e.g. Kumarajiva) where the practioner had family
- Bodhisattva named Avalokitesvara = nowadays ====

**Contact between India and China**

- pilgrams going to India as early as 260 on the overland trade route linking India and China
- no single ==Silk Road== strecthed all the way yfrom Rome to CHina, but most frequented routes **led to India**
  - **Chinese silk was very lucrative** as only Chinese knew how to process silk worms
  - much silk also went to European conutries such as Rome, but a lot might be made in Byzantium
  - then silk is also used as gifts to monasteries
  - Chinese imported a lot of glass,  v.s. 
- In fact, the silk road did not have a single merchant travelling on a camel to Rome, but more often:
  - **only few were long distance merchants**
  - most stayed on circuit close to home
  - other silk road travelelrs include missionaries, refugees, and envoys for Kings, etc.
- when a wave of conquests (Persian Empire) cut off the trade routes between China and Inda, the great age of SIlk Road came to an end, having lasted for over 1000 years

**The Northern Wei Dynasty**

- during early contact with India and Central Asiam the only government in north China to retrain power for over a century was establiedh by a non-Chinese people, the ==Tabgach ==, or also reffered ot as ==== (name of the tribe)
- Xianbei people speaks **both Turic and Mongolian laguages**, but had no written langue of their own
- Xianbei people had:
  - a system of tanistry: more powerful person in duel is the leader
  - women knoew how to tend to hers and hunt because men left for war. Hence ==women enjoyed more power== than most other societies
  - no stable capital, as it is defined where the ruler set camp
- slowly shifted base from Yin Shan mountain in Inner Monolia to northern Shanxi, and staretd taxing the Chinese peasantry and **adopted a Chinese law code**
- by the middle of fifth century, Northern Wei has defated most of their rivals to **take control of north China**

**Dowager Empress Feng and the Equal-Field System**

- established a new captial in Pingcheng, and some of Tabgach are drawn to Chinese ways
- Dowager Empress Feng had a chance to enter the system and **systematically appoint Chinese to office** to lessen Tabgach influence
- Empress Feng seriouly wanted to transform Tabgach to Chinese:
  - wanted to tie the Chinese **farming** population to the land, as otherwise it is mostly bureaucrats and atisans who did not grow their own food
  - this is also useful as it could serve as a source of income by taxing
- the policy to realized the above is the **Equal-Field system**:
  - apointed land to all those of tax paying age
    - since this includes slaves, this means powerful landowning families could retain control over large areas of land
  - gives two types of lands, **farmland** for growing crops, which is short term investment since lands are often quickly redistributed when farmer stopped working
  - and **mulberry tree** for silkworms, which is a long term investment as it takes a long time for the tree to mature
- all of which is to tie people to the land, but did not take effect immediately
  - the system **lasted into the eight century**, as it helped tied people to land and increased revenue

**Adoption of Chinese Ways**

- in 493, Emperor Xiaowen  () implemented a series of measures designed to make his empire Chinese
- his measures include:
  - establish a new captial in Luoyang
  - dropped their own family name of Tabgach and had surname of **Yuan**
  - also appointed the **nine-grade system**
  - advocated conversion to Daosim instead of Buddhism (may wanted to weaken Buddhist establishment for more power)
- but he died young in 499, and the dynasty entered three decades of discord, where ==Northern Wei came to a miserable end in 534==

**Growth of Buddhism under the Northern Wei**

- even during those difficult times, Buddhism received wide support from different factions
- examples of its populance include:
  - "**there is no place that does not have a Buddhist sanctuary**", by a prince in 518
  - had large buddhist monarsteries up to 300 meters high, and **built over 839 royal monasteries**, and about thirty thousand places of worship
- In 528, Erzhu tribes defeated Empress Ling, conquered Luoyang, named a new emperor, and Luoyang was laid waste in 534 from fire

**Persistance of Tabgah Identity**

- even after the fall of Northern Wei Dynasty, Tabgach did not lose their identity since the warlords could not agree on a new emperor. Hence they went back to stay in north Shanxi

- two generations after Emperor , Tabgach people **still retained their language and identity**

- however, the ==impact of their steppe culture on north Chinese is long-lasting==

  - Yan Zhitui recorde thah people in the north and south (of yellow river) behaved very differently:

    - in the south husband can be extravagent even if wife and children suffer
    - in the north, it is **usually wife who runs the household, and sometimes husband has to put up with wife's insult**

    where the latter seems related to Tabgach's culture: women had more influence

**The Move South**

- before, the south is often viewed as **uncivilized**, and the wet climate (==perfect for rice==) is thought of as a **home for illness**
- however, the fall of Luoyang prompted migration, as well as the dangers of battles
- consequently, many northern Chinese aristocrats moved to ==Nanjing==
- in south China, Daoist teachers were most active, but there were some buddhist influences as well. But there were somem dramatic changes:
  - for buddhist, the teachings of the **Greater Vehicle largely superseded those of Lesser Vehicle**
  - Daoist visionaries divided into **Lingbao  and Mao Shan **

**Religious Life in South China**

- the Liang emperor was interested in Daoism, studied it in Mao Shan Montain, hence new Daoist sect took the name Mao Shan
- the Maoshan adherents continued the practices of Five Peck Daoist, but **distance themselves from practice that had given the Doaist a bad name**, specifically certian sexual rites
- had a system of seven levels housing both divine immortals as well as the spirits of the dead
  - slightly optimistic than Chinese view of afterlife were dead would go through a series of courts
  - believed that you could **lodge suits in the underground agasint people who had wronged them**
  - however, you can rise a level if you had **revealations from certified teachers**, which in reality is a "secretive service" and costs a lot

**Religious Policies of Emperor Wu of the Liang**

- he stued Daosit, but then in 504 urged his family and officails to **give up Doaism for Buddhist teaching**

**Critics of the Buddhist Establishments**

- by the middle of 6th century, there are approx two million monks
- those clery **did not work or serve in military**, hence seen as a drain on state resource
- this caused ==ire in the emperor of Zhou dynasty== (557 - 581) who succeeded the Qi, and ordered all Buddhist monks to return to lay life and ==all Buddhist texts and statues destoryed==
  - even inside buddhism, there are criticism as they shelted criminals and engaged in loans
  - however, those records are mostly about the wealthy clergy, so we don't know the common clergy's behavior
- however, a ==short supression in 574 cuold do little to shake the hold== of either Buddhist or Daoist, both of which had history over 4 centuries]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Intro to East Asia: China]]></summary></entry><entry><title type="html">ELEN6885 Reinforcement Learning</title><link href="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning.html/" rel="alternate" type="text/html" title="ELEN6885 Reinforcement Learning" /><published>2022-12-15T00:00:00-05:00</published><updated>2022-12-15T00:00:00-05:00</updated><id>/lectures/2022@columbia/ELEN6885_Reinforcement_Learning</id><content type="html" xml:base="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning.html/"><![CDATA[Reinforcement Learning

---

The office hours will take place by default in EE dept. student lounge, **13th FL, Mudd building**. TA's may decide to host their office hours remotely. If that is the case they will post an announcement here with a link.

| TA               | Day       | Time               |
| ---------------- | --------- | ------------------ |
| Trevor Gordon    | Monday    | 1:30 PM - 3:30 PM  |
| Xin Huang        | Tuesday   | 9:30 am - 11:30 am |
| Jinxuan Tang     | Wednesday | 3:30 pm - 5:30 pm  |
| Yukai Song       | Thursday  | 9:30 am - 11:30 am |
| Gokul Srinivasan | Friday    | 3:30 PM - 5:30 PM  |

# Introduction to RL

**Instructors**:

- Chong Li (first half of the semester)
- Chonggang Wang (second half)

**Exam**:

- bi-weekly assigments 50% (first assginment out after week 2)
- midterm exam 20%
- final exam 30%

**Key aspects of RL** (as compared to other algorithms):

- no supervisor, only reward signal (and/or environment to interact with)
- feedback/reward could be delayed
- time matters, as it is a sequenitial decision making

The general process of interaction looks like:

<img src="rl/image-20220915170944706.png" alt="image-20220915170944706" style="zoom: 50%;" />

where essentially, the agent learns

- a **policy** (e.g. the decision making agent)
  $$
  s \in S \to \pi(a|s),a\in A(s)
  $$
  since the action space could be dependent on the state. Note that a policy could be **stochastic** or **determinstic**

- a **reward** indicates the ==desirability== of that state. Reward is immediate or sometimes delayed (but as long as you have a reward for the entire episode, it is fine)

- a **return** is a cumulative sequence of received rewards after a given time step
  $$
  G_t = R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^T \gamma^k R_{t+1+k}
  $$
  where usually $\gamma < 1$ if we have infinite episodes $T \to \infty$

- a **value function** (which depends on the policy)
  $$
  V_\pi(s) = \mathbb{E}_\pi [G_t|S_t=s] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+1+k} |S_t=s \right]
  $$
  given that I am currently at $s$, what is the *expected return if I follow the policy $\pi$*.

- a **action-value function**
  $$
  Q_\pi(s,a) =  \mathbb{E}_\pi [G_t|S_t=s, A_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+1+k} |S_t=s, A_t=a\right]
  $$
  given that I am currently as $s$, what is the *expected return if I take $a$ as next step and then follow policy $\pi$*. Note that if we take a "marginal" on $a$, we can get the $V_\pi$ from the $Q_\pi$:
  $$
  V_\pi(s) = \sum_{a \in A(s)} Q_\pi(s,a) \pi(a|s)
  $$
  which can be easily seen from their definitions. The reverse can be derived as well:
  $$
  Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')
  $$
  
- a **world model** (optional, so that if you know this, planning becomes much easier)
  $$
  \mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1} = s' | S_t=s , A_t=a]
  $$
  which is the transition probability
  $$
  \mathcal{R}_s^a = \mathbb{E}[R_{t+1} | S_t=s,A_t=a]
  $$
  which is the reward model. Note that it is expected when what happens next is stochastic.

> The agent's goal is to find a **policy** to **maximize the total amount of reward** it receivers over the long run.

---

*Examples*

- **Maze**

  |                           Maze Map                           |                            Reward                            |                        $V^{\pi^*}(s)$                        |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="rl/image-20220915170101082.png" alt="image-20220915170101082" style="zoom:50%;" /> | <img src="rl/image-20220915170120864.png" alt="image-20220915170120864" style="zoom:50%;" /> | <img src="rl/image-20220915170136366.png" alt="image-20220915170136366" style="zoom:50%;" /> |

  note that 

  - each step has a $-1$ reward (we designed, so that the agent finds the shortest path = max reward), and states is the agent's location

  - these values are computed given the optimal policy

- **Student Markov Chain**: consider some given stochastic *policy*:

  |                     Student Markov Chain                     |                           +Reward                            |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="rl/image-20220915170318596.png" alt="image-20220915170318596" style="zoom:50%;" /> | <img src="rl/image-20220915170339032.png" alt="image-20220915170339032" style="zoom: 67%;" /> |
  
  we can sample episodes for student markov chain starting from $S_1=C_1$ and obtain episodes such as:
  
  <img src="rl/image-20220915170437631.png" alt="image-20220915170437631" style="zoom:33%;" />

  Of course, for each episode, you can have some reward given some reward model:

  <img src="rl/image-20220915170506093.png" alt="image-20220915170506093" style="zoom:50%;" />
  
  where essentially we are just computing the expected return $G_t$. But since you can have as much episodes as you want, we can consider:
  $$
  V(S=C_1)=\mathbb{E}[G_t | S_t=C_1] = \sum_{i=1}^\infty P(\tau_i)\cdot V(\tau_i)
  $$
  where probability of a trajectory $P(\tau_i)$ we can get since we know the transition model. However, note that this is impossible to compute since we can have infinite episodes (so we need other tricks, e.g. Bellman Equations & Dynamic Programming)

---

This gives rise to the definitions:

> **Markov Decision Process** is a tuple $\lang S,A,\mathcal{P},\mathcal{R},\gamma \rang$
>
> - $\mathcal{S}$ is a finite set of states
>
> - $\mathcal{A}$ is a finite set of actions
>
> - $\mathcal{P}$ is a state trainsition probability matrix:
>   $$
>   \mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]
>   $$
>
> - $\mathcal{R}$ is a reward function, so that $\mathcal{R}_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$ for cases when reward is not deterministic
>
> - $\gamma$ being the discount factor $\gamma \in [0,1]$.

so that if the problem can be formulated into a MDP, then we can apply RL algorithms.

And finally, after we find a policy using RL algorithm/or even train a policy using RL, we need to have access to an **environment**: in this course we will use ==OpenAI Gym==. (simulation are used usually because human-invovled real environment would be too costly)

## RL History

There are two threads of development

<img src="rl/image-20220915170902678.png" alt="image-20220915170902678" style="zoom:50%;" />

So that essentially:

- control theory includes the Bellman Optimality Equations, and essentially solves the problem of how to plan, **given a known model**
- the trial and error structure gives rise to the structure of how to train an agent to learn how to plan **when we don't know the model**

# Bandit Problem and MDP

Fundemental mathematical stuff behind RL algorithms.

## Bandit Problem

One of the first problem that people look at. The problem is simple, but the solution shows principles in RL. The classical is the n-armed bandit problem

> **N-armed Bandit Problem**: suppose you are facing two machines, where both are Gaussian and
>
> - reward from the first gives $\mu=5$, with variance 1
> - reward from the second gives $\mu=7$, with variance $1$
>
> The obviously we will use the second machine. But in reality, the **above information is not available**. So what do you do? Your objective is to ==maximize the expected total reward over some time period==.

**Intuitively**, the idea should be:

1. spend some initial money to explore and estimate the epectation using average reward
2. then play towards the better machine

With this, we consider $Q_t(a)$ value being the estimated value/average reward so far of action $a$ at the $t$-th play:
$$
Q_t(a) = \frac{R_1 +R_2 + ... + R_{K_a}}{K_a}
$$
for $K_a$ is the number of times action $a$ was chosen and $R_1,...,R_{K_a}$ being the reward associated with action $a$.

Once you estimated $Q_t(a)$, we can consider:

<img src="rl/image-20220916082722457.png" alt="image-20220916082722457" style="zoom: 67%;" />

where we can test:

- **greedy** policy (or $\epsilon=0$), choosing the machine with current best $Q_t(a)$

- **$\epsilon$-greedy** means you choose a random machine with probability $\epsilon/N$ even though you know the current best. E.g. if you have, at step $t=10$, achieved $Q_{10}(a_1)=4.5,Q_{10}(a_2)=4$, then with $\epsilon=0.2$:
  - choose $a_1$ with probability $0.8+0.1=0.9$
  - choose $a_2$ with probability $0.1$
- why is $\epsilon=0$ not good? Once you have made a unlucky estimation of the average, you may be stuck at a suboptimal solution. Especially in the early stage. But with $\epsilon$-greedy, I always have chance to jump to the correct arm.
  - note that of course, there are cases where greedy policy works. But more often $\epsilon$-greedy gives more consistent performance.

- but what is the optimal solution?

> **Exploitation vs. exploration dilemma**: Should you ==exploit== the information youve learned or ==explore== new options in the hope of greater payoff?
>
> - this is what makes most RL problem hard to find the optimal

An alternative to $\epsilon$-greedy is the **Softmax policy** (to balance exploration and exploitation)

> **Softmax Policy**: essentially use softmax of the Q values to give probability
> $$
> \mathrm{Softmax}(Q(a)) = \frac{\exp(Q_t(a)/\tau)}{\sum_{a \in A} \exp(Q_t(a)/\tau)}
> $$
> note that $\tau$ has a critical effect:
>
> - if $\tau \to \infty$, the distribution will become almost uniform
> - if $\tau \to 0$, then it becomes greedy action selection

But which one is better, softmax or $\epsilon$-greedy? In practice it depends on applications.

---

**Notes on Algorithmic Implementation**

- do you need to store all rewards to estimate average reward $Q_k$ for each action? No, you can use a **moving average**
  $$
  \begin{align}
  Q_{k+1}
  &= \frac{1}{k} \sum_{t=1}^k R_t\\
  &= \frac{1}{k} \left( R_t + \sum_{i=1}^{k-1} R_i \right) \\
  &= \frac{1}{k} \left( R_t + kQ_k - Q_k \right) \\
  &= Q_k + \frac{1}{k}[R_k - Q_k]
  \end{align}
  $$
  where $Q_k$ is for the computed average reward from the previous step, and **$R_k$ is the new info**. Note that ==this form is very commonly seen in RL==:
  $$
  \text{New Estimate} \gets \text{Old Estimate} + \alpha \underbrace{(\text{Target - Old Estimate})}_{\text{innovation term}}
  $$
  where the innovation term would give you some idea of the new information. The step size is there because we know the innovation term will oscillate since $\mathrm{Target}$ might contain inaccuraries

- what is a **good step size** to choose? We want your estimate, e.g. $Q$-value to converge.

  <img src="rl/image-20220916084728218.png" alt="image-20220916084728218" style="zoom:50%;" />

  an example for this to work is $\alpha=1/k$. Essentially, the intuition behind this is that it will take past information a bit more, but you should *always have some non-trivial weight on the new information*.

- What if this is a **non-stationary** problem? i.e. the distribution for the n-arm bandit machine is *varying over time*, e.g. mean changes over time. In this case, the convergence doesn't mean anything since the "correct" mean is moving. In this case, we can choose $\alpha$ being a constant.

## Markov Decision Process

MDP formally describe an environment for reinforcement learning, where the ==environment is fully observable==. First

> **Markov Property**: the future is independent of the past given the present. i.e. I only care about present, past gives no information
> $$
> P(S_{t+1}|S_t) = P(S_{t+1}|S_1,...,S_t)
> $$
> may or may not be a good assumption in some problems. However, you can always make this a good assumption, e.g. ==if you choose $S_t = \text{everything up to time t}$, then this trivially holds==.

A recap of the definitions

> **Markov Process** is a tuple $\lang S,\mathcal{P}\rang$
>
> - $\mathcal{S}$ is a finite set of states
>
> - $\mathcal{P}$ is a state trainsition probability matrix:
>   $$
>   \mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]
>   $$

An example of the transition matrix is

<img src="rl/image-20220916085816449.png" alt="image-20220916085816449" style="zoom:50%;" />

> **Markov Reward Process** is a tuple $\lang S,\mathcal{P},\mathcal{R},\gamma \rang$
>
> - $\mathcal{S}$ is a finite set of states
>
> - $\mathcal{P}$ is a state trainsition probability matrix:
>   $$
>   \mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]
>   $$
>
> - $\mathcal{R}$ is a reward function, so that $\mathcal{R}_s = \mathbb{E}[R_{t+1}|S_t=s]$ for cases when reward is not deterministic
>
> - $\gamma$ being the discount factor $\gamma \in [0,1]$. Useful for infinite episodes

*For example:*

<img src="rl/image-20220916090117592.png" alt="image-20220916090117592" style="zoom: 50%;" />

Then from this, we can also define

- **return**: think of the $R_{t+1}$ as going from a MDP to a MRP. If you start at Class 1, then you should also include $R=-2$ in your computation, just as in MDP, the reward
  $$
  G_t = R_{t+1} + \gamma R_{t+2} += ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
  $$

- the choice of $\gamma$ affects the bahavior/goal of your algorithm

  - $\gamma=0$ leads to myopic evaluation
  - $\gamma=1$ leads to far-sighted evaluation

Additionally, we can also define **state-value function**

> **State Value Function** (for MRP): the expected return starting from state $s$ 
> $$
> V(s) = \mathbb{E}[G_t|S_t=s]
> $$

which we have discussed before, and to compute this intuitively would be:

- start from a state $s$
- compute expected reward for all possible trajectory from this state
- weight it by probability of each trajectory

Which would give this

<img src="rl/image-20220916090938392.png" alt="image-20220916090938392" style="zoom:50%;" />

> Is there an easier way than sample, weight, and sum? The **Bellman Equation**.

We can decompose the value function definition, and see what we get

<img src="rl/image-20220916091139515.png" alt="image-20220916091139515" style="zoom:50%;" />

where only the last step is tricky:

- what is $\mathbb{E}[\gamma G_{t+1}|S_t=s]$? By definition of expected value:
  $$
  \begin{align*}
  \mathbb{E}[\gamma G_{t+1}|S_t=s]
  &= \gamma \sum_g G_{t+1} P(G_{t+1}|S_t=s)\\
  &= \gamma \sum_g G_{t+1} \sum_{s'} P(G_{t+1}, S_{t+1}=s'|S_t=s)\\
  &= \gamma \sum_g G_{t+1} \sum_{s'} P(G_{t+1}|S_{t+1}=s', S_t=s)P(S_{t+1}=s'|S_t=s)\\
  &= \gamma  \sum_{s'} \left( \sum_g G_{t+1}  P(G_{t+1}|S_{t+1}=s') \right) P(S_{t+1}=s'|S_t=s)\\
  &= \gamma \sum_{s'\in S}V(s') P(S_{t+1}=s'|S_t=s)\\
  &= \mathbb{E}[\gamma V(s') | S_t=s]
  \end{align*}
  $$
  where:
  
  - the aim for the second equality is because we know the final result has $v(S_{t+1})$, so we need to introduce a next state $s'$.
  - the third equality comes from applying chain rule for joint probability
  - the fourth equality comes from the Markov Assuption, where $G_{t+1}$ does not depend on the past, only present. And that we are taking $G_{t+1}$ since it does not depend on the future state $s'$
  
- now, given the above, we essentially realize for an ealier state:
  $$
  \begin{align*}
    V(s) 
    &= \mathbb{E}[R_{t+1}|S_t=s] + \gamma \mathbb{E}[V(S_{t+1})|S_t=s] \\
    &= \mathcal{R}_s + \gamma \sum\limits_{s' \in S} \mathcal{P}_{ss'} V(s')
  \end{align*}
  $$
  for $\mathcal{R}$ is the expected reward in case reward is stochastic. And in matrix form:
  $$
  V = \mathcal{R} + \gamma \mathcal{P}V
  $$
  for instance:

  <img src="rl/image-20220916092443352.png" alt="image-20220916092443352" style="zoom:33%;" />

  so given this, **can we solve for $V$** (which is our final goal)? Simply solve it if given $P,R$:
  $$
  \begin{align*}
  V 
  &= R+\gamma P V\\
  V &=(I - \gamma \mathcal{P})^{-1} R
  \end{align*}
  $$
  An example using this to compute $V$ from the closed form solution:

  <img src="rl/image-20220916092636709.png" alt="image-20220916092636709" style="zoom: 50%;" />

  but is this all? We have real life concerns:

  - The complexity of *this approach* is $O(n^3)$ due to the inverse, for $n$ being the number of states
  - Most often in reality you are ==not given $P$==, i,e, you don't know it

> Two important results from the previous discussion for MRP:
> - Bellman's equation for $V$:
>   $$
>   V = \mathcal{R} + \gamma \mathcal{P}V
>   $$
> - and we can solve for $V$ easily in closed form:
>   $$
>   V =(I - \gamma \mathcal{P})^{-1} R
>   $$

However

> **For large MDPs, or unknown transition models:** we have the following algorithms to be our savior
>
> - Dynamic programmming (known transition, but more efficient)
> - Monte-Carlo evaluation (sampling, unknown transition)
> - Tempora-Difference learning (sampling, unknown transition)

> **Professor Li:** "Why you need algorithm if you have the closed form solution? All algorithms are there to exchange for *compute*, or more oftenly, *approximate* a solution".

Finally, we introduce our main problem:

> **Markov Decision Process** is a tuple $\lang S,A,\mathcal{P},\mathcal{R},\gamma \rang$
>
> - $\mathcal{S}$ is a finite set of states
>
> - ==$\mathcal{A}$ is a finite set of actions==
>
> - $\mathcal{P}$ is a state trainsition probability matrix:
>   $$
>   \mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'|S_t=s,\textcolor{red}{A_t=a}]
>   $$
>
> - $\mathcal{R}$ is a reward function, so that $\mathcal{R}_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$ for cases when reward is not deterministic
>
> - $\gamma$ being the discount factor $\gamma \in [0,1]$.

and remember that, most of the time, we do no have access to $P$.

Then, ==when given a policy $\pi(a|s)$, we are back to a MRP==: it becomes a process of $\left\langle \mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma \right\rangle$, where:
- the transition matrix's action is removed by:
  $$
  \mathcal{P}^{\pi}_{ss'} = \sum_a \pi(a|s) \mathcal{P}_{ss'}^a
  $$
  i.e. if I take an action $a$ at state $s$, what the the trainsition probability to $s'$? Then I take the average, to get the averae transition probability from $s\to s'$ for the policy
- the reward function's action is removed by:
  $$
  \mathcal{R}^{\pi}_s = \sum_a \pi(a|s) \mathcal{R}_s^a
  $$
  for $\mathcal{P}_{ss'}^{a}, R_s^{a}$ are given as part of the MDP. 

- when given a policy, your MDP evaluations become MRP evaluations, since now $P^\pi_{s,s'}$ is your transition matrix independent of action, and you can compute them in a closed form using our previous result.

> But now, ==value function== will depend on the ==policy I use==, as compared to the previous cases:
> $$
> V_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]
> $$
> and ==so is the action-value function==

*For instance*: consider a given policy $\pi(a|s)=0.5$ uniformly, with $\gamma=1$. Then you should get

<img src="rl/image-20220916094059606.png" alt="image-20220916094059606" style="zoom: 50%;" />

using essentially the MRP results we got, and reducing MDP given a policy to MRP.

But now, the ==Bellman equations depend on the policy==, and we have the following:

> **Bellman Expectation Equation**: notice the connection between value function and action-value function
>
> <img src='rl/image-20220916113343.png' style='zoom:30%;'/>
>
> giving the equations for:
>
> - Value function $V_\pi(s)$ being averaging over next value functions
> - Action-value function $Q_\pi(s,a)$ being averaging over next action-value functions after taking action $a$ at state $s$
> Hence we get:
> $$
> V_\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma V_\pi(S_{t+1})|S_t=s]
> $$
> $$
> Q_\pi(s,a) = \mathbb{E}_\pi [R_{t+1} + \gamma Q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]
> $$
> or we can express them as:
> $$
> V_\pi(s) = \sum_a \pi(a|s) Q_\pi(s,a)
> $$
> $$
> Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s')
> $$
> then using those relationships, I can express ==$V$ in terms of $V$==, and ==$Q$ in terms of $Q$==, which gives the Bellman Expectation Equation:
> $$
> V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s') \right)
> $$
> $$
> Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \sum_{a'} \pi(a'|s') Q_\pi(s',a')
> $$
> which is basically the ==same as the equation we had with $\mathbb{E}_\pi$==. Note that 
>
> - when transition is deterministic given an action, then $\mathcal{P}_{ss'}^a=1$ for the destined state $(s,a) \to s'$, and $\mathcal{P}_{ss'}^a=0$ otherwise.
> - similarly, the expected reward $\mathcal{R}_s^a$ is used when reward is non-deterministic given a $s,a$, 

And with this, we again have the closed form solution as implied by the above as we reduced MDP to MRP once we have a policy:

> **Closed Form Solution for $V^\pi$** in MDP: the Bellman's expectation equation can essentially be translated to the MRP version:
> $$
> V_\pi = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} V_\pi
> $$
> and the closed form solution is:
> $$
> V_\pi = (\mathbb{I} - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}
> $$
> for $\mathcal{P}^\pi, \mathcal{R}^\pi$ being the transition and reward matrix reduced to MRP implied by the policy $\pi$.

## Optimal Value Functions and Policies

Now, we understand the basics of MDP, and how we are essentially solving it by reducing to MRP, we come back to the central problem of control: ==how to find the *optimal* policy and/or the optimal value function?==

First, we need to define what they are:
> **Optimal Value Function**: the value function that maximizes the expected return for any given policy $\pi$:
> $$
> V_*(s) = \max_\pi V_\pi(s)
> $$
> and the optimal action-value function is the same:
> $$
> Q_*(s,a) = \max_\pi Q_\pi(s,a)
> $$

But what about optimal policy, which if we recall are distributions $\pi(a|s)$? We can define it as:

> **Optimal Policy**: we can define a partial ordering
> $$
> \pi > \pi' \iff V_\pi(s) \geq V_{\pi'}(s),\quad \forall s
> $$
> and note that, the two most important attribute for a problem
> - (existence) solution always exists. For any MDP, there eixts an optimal policy (e.g. see example below)
> - (uniqueness) the optimal policy *not* unique, but all optimal policies achieve the same value function/action-value functin.

For instance, we can give an optimal policy better than all other policies

$$
\pi_*(a|s) = \begin{cases}
1 & \text{if } a=\arg\max_{a\in A} Q_*(s,a)\\
0 & \text{otherwise}
\end{cases}
$$
which is optimal, meaning there always exist an optimal policy. But in reality, we ==don't know $Q_*$ yet==. So our task remains how we can find such a policy, now that we know it exists.

> **Bellman Optimality Equation**: we can again draw the graph, and derive the optimality equations from $V_*,Q_*$, which gives us some idea how we can find it
>
> <img src='rl/image-20220916113343.png' style='zoom:30%;'/>
>
> giving the equations for intuitively as:
> $$
> V_*(s) = \max_a Q_*(s,a)
> $$
> $$
> Q_* (s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_*(s')
> $$
> and we can combine them to obtain a form only including itself:
> $$
> V_*(s) = \max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_*(s') \right)
> $$
> $$
> Q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \max_{a'} Q_*(s',a')
> $$
>
> ==this is essentially the basis of all RL algorithms and what they aim to solve==

But now there is a $\max$, and we ==can no longer have a closed form solution== as the equations become nonlinear. Therefore, we need to use iterative solution algorithms to ==approximate them== (these two equations are the fundementals of the whole RL algorithm)
- Value Iteration
- Policy Iteration
- Q-learning
- SARSA
- etc.

## Extended MDP

Sometimes, this is not MDP by nature, but we can do some modification and convert them to MDP

**Partially Obseravable MDP**: for instance, the robot can only observe neighboring grids, but not the global information

<img src="rl/image-20220923081511421.png" alt="image-20220923081511421" style="zoom:33%;" />

therefore, technically:

- I do not know $s$, but I know something related.
- Solved by introducing ==belief MDP== (see wiki for more details)

> **Belief MDP**
>
> ideally, we want $\pi(a|s)$ given a state. All I have is a history $h=(A_0,O_1,...,A_{t-1},O_t,R_t)$
>
> therefore, we can consider a **belief state** (instead of state):
> $$
> b(h) = (\mathcal{P} (S_t=s^1 | H_t = h),\mathcal{P} (S_t=s^2 | H_t = h), ...,\mathcal{P} (S_t=s^n | H_t = h) )
> $$
> which is a probablity over all possible states. 
>
> - note that we ==do not have to encode a belief state like this==. In the case of a continous distirbutoin, this can be a gaussian distribution, for instance.
>
> then your policy becomes:
> $$
> \pi(a| b(h))
> $$
> notice that belife states satisfy Markov propety

---

**Continous State MDP**: what if your state is continuous

- discretization on the continous-state. 
  - but this becomes a trade-off between granuality = accurarcy v.s. 
- use a value function approximation to approximate value directly

# Model-based RL

Keep in mind that all approaches is still tightly related to the bellman optimality equations

> **Notation**: note that we *try* to capitial letter such as $V$ to represent the true/random variable, and $v$ represent *our* estimate/realization.

## Introduction of Dynamic Programming

> DP refers to a **collection of algorithms** that can be used to compute optimal policies given a ==perfect model== of the **environment** as a MDP
>
> - because at each iteration, we need some form of $\mathcal{P}_{ss'}^a V_k(s')$ to update $V_{k+1}$
> - for now, we assume a finite MDP, but DP ideas can easily be extended to cnotonus states or POMDP

However, Classical DP algorithms are of limited utility in RL. Why?

- Need perfect model
- Great computational expense

But regardless, if those can be overcome, we would like to perform two tasks in general:

- ==prediction==: given a policy $\pi$ and the MDP, evaluate how good it is by outputting its **value funciton $V_\pi$**
- ==control==: given a MDP, find the **optimal policy $\pi_*$**

## Prediction: Policy Evaluation

> **Aim**: given any policy $\pi$, we want to evaluate its value function in a MDP.

The key idea is the Bellman equation: assuming we know everything on the RHS, then that should give me the correct value on LHS:
$$
V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s') \right)
$$
using this idea of dependency=constraint, we can **iteratively estimate $V_\pi(s)$** by:
$$
V_{k+1}(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k}(s') \right)
$$
which is called a **synchronous backup** since it uses $V_k(s)$ even if we have *for some states a better estimate $V_{k+1}(s)$*

> **Synchronus Policy Evaluation**:
>
> `Repeat` until convergence:
>
> 1. store $V_k(s)$
>
> 2. for each state $s \in S$:
>
>    1. estimaet $V_{k+1}(s)$ by
>       $$
>       V_{k+1}(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k}(s') \right)
>       $$
>
> 3. use the latest update $V_{k} \gets V_{k+1}$

But do we *always* needs such an interation, as we saw in some examples in HW1?

> **Note** that in homeworks, we had this ==bottom up approach== where we found the correct value function **in one step**. For instance, we can compute the following value function of a policy:
>
> <img src="rl/image-20220923111844922.png" alt="image-20220923111844922" style="zoom:33%;" />
>
> then since $V(S=s_{t+2})=0$ for any state, we can compute $V(S=s_{t+1})$ ==without any error== using the backup equation
> $$
> V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{\pi}(s') \right)
> $$
> and all can be done over one iteration. 

But in reality, ==often your state space is not a tree==, i.e. to get to $S=s_{14}$ in the example below, there will be ==loops== in your tree (i.e. you can get to $s_{14}$ from $s_{13}$, and to get to $s_{13}$ you can also start from $s_{14}$). Therefore, since then you ==cannot estimate $V(S=s_{14})$== correctly, you will need a loop for convergence.

---

*For Example*: Small Grid World

<img src="rl/image-20220923083659494.png" alt="image-20220923083659494" style="zoom:50%;" />

where we have:

- reward of $-1$ for all non-terminal states, and there is one terminal state marked in grey
- undisconuted MDP with $\gamma =1$

- actions leading out of the state gives no changes (i.e. go left of $s_8$ is still $s_8$)

**Question**: if my policy is random $\pi(a|s)=0.25,\forall a$, what is the value of it/how good is this?

Since we also have transition model, we can use the DP algorithm. We can initialize $V_0(s)=0$:

<img src="rl/image-20220923083836193.png" alt="image-20220923083836193" style="zoom:33%;" />

for intance, to compute $V_2(s=1)$ would be:
$$
\begin{align*}
V_{k=2}(s_1) 
&= \sum_a 0.25 \cdot \left( -1 + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k=1}(s') \right) \\
&= 0.25 (-1 + 0) + 0.25 (-1 + -1) + 0.25 (-1 + -1) + 0.25 (-1 + -1) \\
&= -1.75
\end{align*}
$$
where all the $V_1(s)$ comes from our estimate from $v_1$ at the previous timestep.

**Additional Question**: what would be my control?

(This is just a lucky case that we got optimal policy in one greedy step.) We know that the value function is for a random policy, but can we get a better policy from it? Notice that by simply using the **greedy policy of $\pi$**:

| <img src="rl/image-20220923084435565.png" alt="image-20220923084435565" style="zoom:50%;" /> | <img src="rl/image-20220923084439721.png" alt="image-20220923084439721" style="zoom:50%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

notice that this is essentially performing **exactly one iteration of policy improvement**, but we luckily get the optimal policy just by doing policy improvement once

## Control: Policy Improvement and Policy Interation

Now I have evaluated a policy, I would like to improve the policy and perhaps ==find the best policy==

> **Policy Improvement**: generate a policy $\pi'$ that is better than $\pi$
>
> *one way* to guarantee improvement is to apply a greedy policy
> $$
> \pi' = \mathrm{greedy}(V_\pi)
> $$
> and we can later show that indeed improves the policy and is useful.

> **Policy Iteration**: since we can get a better policy from old value, then we can iterate:
>
> <img src="rl/image-20220923084722197.png" alt="image-20220923084722197" style="zoom:33%;" />
>
> `Repeat` until no improvement/convergence
>
> 1. (**Policy Evaluation**) evaluate current policy $\pi$
> 2. (**Policy Improvement**) improve the policy (e.g. by acting greedily) by generating $\pi' \ge \pi$

This gives the **policy iteration algorithm** in more details:

<img src="rl/image-20220923093222983.png" alt="image-20220923093222983" style="zoom:50%;" />

where essentially 

- during policy evaluation, we are taking $\Delta$ to be the largest error we encountered for each state

- we are selecting the greedy policy by selecting the best action given the value function.

---

*Proof*: We want to prove that greedy policy always improve the policy, hence we have a ==monotonic improvement==

Consider some existing deterministic policy $\pi(s)$, and we have a greedy policy version $\pi'(s)$:
$$
\pi'(s) = \mathrm{greedy}(\pi(s)) = \arg\max_{a \in A}q_\pi(s,a)
$$
Then by definition, the **action-value must have obviously improved the policy by one-step**
$$
q_\pi(s,\pi'(s)) = \max_{a \in A} q_\pi(s,a) \ge q_\pi(s,\pi(s)) = v_\pi(s)
$$
where the last step is due to the fact that we are using a **determistic policy**, so the state-value function will be the same as value function since you **always only choose one action per state**.

Finally, we just need to show that $V_{\pi'}(s) \ge V_\pi(s)$ by connecting the above

$$
\begin{align*}
v_{\pi}(s)
&\le q_\pi(s,\pi'(s)) \\
&= \mathbb{E}_{(a,r,s'|s) \sim \pi'}[ \mathcal{R}_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s ] \\
&\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s ] \\
&\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma R_{t+2} + \gamma^{2} q_\pi(S_{t+2}, \pi'(S_{t+2})) | S_t=s ] \\
&\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_t=s ] \\
&= v_{\pi'}(s)
\end{align*}
$$
where:
- the second equality is because we have are sampling the immediate next step from $\pi'$, but the rest is following $\pi$.
- third inequality relies on the fact that we are a deterministic policy and $q_\pi(s,\pi'(s)) \ge v_\pi(s)$ by construction

Notice that since the improvement is monotonic, then we have everything becomes an equality at convergence:
$$
q_\pi(s,\pi'(s)) = \max_{a \in A} q_\pi(s,a) = q_\pi(s,\pi(s)) = v_\pi(s)
$$
meaning that the Bellman optimality equation is satisfied
$$
v_\pi(s) = \max_{a \in A}q_\pi(s,a)
$$
which means the $\pi$ we get at the **end of convergence** is the optimal policy

---

> **Generalized Policy Iteration**: the general recipe include:
> 1. any policy evaluation algorithm
> 2. any (proven) policy improvement
> 3. repeat until convergence to the optimal policy

But whatever new algorthim you come up with, we want to measure/compare (note that the converged value function is optimal, hence the same):

- **convergence speed**: how fast the algorithm can converge
- **convergence performance**: same convergence speed, but *variance* during training could be big/small

## Control: Value Iteration

> **Aim**: We want to ==improve the speed of the policy iteration algorithm== because **each of its iterations involves policy evaluation**, which itself is an iterative computation through the state set. 

Intuition, we notice that at $k=3$ in the example before, just one greedy policy we already get the optimal policy ==even if the value function has not converged==

|  ![image-20220923084435565](rl/image-20220923084435565.png)  | ![image-20220923084439721](rl/image-20220923084439721.png) |
| :----------------------------------------------------------: | :--------------------------------------------------------: |
| Therefore, maybe we can directly improve before waiting for converged value function |                                                            |

**Value Iteration**:

<img src="rl/image-20220923094259901.png" alt="image-20220923094259901" style="zoom: 50%;" />

where essentially 

- I computed my $v(s')$ for one round, and took the max (greedy policy) directly afterwards

- notice that this is also the Bellman Optimality Equation
- it is also proven that this **converges to the optimal value**

> **Value Iteration is used much more often** than policy iteration due to its faster compute

## Summary: Synchronous DP

Note that all the above is synchronous DP, because at each step $k+1$ we are using the value function strictly from step $k$.

<img src="rl/image-20220923094516742.png" alt="image-20220923094516742" style="zoom:40%;" />

- **major dropback:** even for value iteration, it needs to **iterate through every state at each iteration** (which could be intractable for large state space)
- Asynchronous DP: maybe we can selectively update, or update more in real time

## Asynchronous DP

Before, we compute the new value for all states at $t=k+1$ strictly using $t=k$ values, meaning for each improvement we need to wait until all states are updated.

> **Aim**: We want to see improvement a bit faster, instead of waiting for the entire sweep of states to complete. We want our algorithm to not get locked into any hopelessly long sweep before it can make progress.
> - but note that this does not necessarily mean less compute

Three simple ideas in Asynchronous DP:
- in-place DP
- Prioritized Sweeping
- Real-time Dynamic DP

### In-place DP

> **Intuition**: since we already have a new value for some states we finished computing at $t=k+1$. we use the **new value directly** before completing the entire estimate over all states

<img src="rl/image-20220923094905916.png" alt="image-20220923094905916" style="zoom:40%;" />

note that already here, we made an improvement **for memory**. But what about state space complexity?

### Prioritized Sweeping

> **Intuition**: instead of updating each step in a fixed order, we can consider a better order to update the states so that we can perhaps converge faster. For instance, we can use the size of the Bellman error as a guidance.

For states with large Bellman error, we might want to update them first

$$
\text{Bellman Error}(s) = \left|  \max_{a\in A} \left( \mathcal{R}_s^{a} + \gamma \sum\limits_{s'\in S} \mathcal{P}_{ss'}^{a} v(s') \right) - v(s)\right|
$$

which is basically comparing the difference between a target the current estimate. But this means we need:
- first compute this error for every state, then choose which one has the largest error to update
- next, we need to **update the error table**, which can be done ==only on the affected states $s'$== (the error is only a functoin of current state and next state)
-  no guaranteed convergence to the optimal policy

### Real-Time DP

> **Intuition**: I only update the value function $V(s)$ on states that the agent **has seen**, i.e. is relevant for the agent

1. agent has a current policy $\pi$
2. agent performs some action and observe $S_t, A_t, R_{t+1}$, and is at $S_{t+1}$
3. backup the value function $V(S_t)$ using the new observation
   $$
   v(S_t) \gets \max_{a\in A} \left( \mathcal{R}_{\textcolor{red}{S_t}}^{a} + \gamma \sum\limits_{s'\in S} \mathcal{P}_{\textcolor{red}{S_t}s'}^{a} v(\textcolor{red}{s'}) \right)
   $$
   notice that this is off-policy since the value now is not about the behavior policy
4. update the policy $\pi$ using the new value function and repeat

Note that, again, there is **no guaranteed convergence to the optimal policy**

## Generalized Policy Iteration

Policy iteration consists of two simultaneous, interacting processes:

1. one making the value function consistent with the current policy (**policy evaluation**)
2. making the policy greedy with respect to the current value function (**policy improvement**). 

Up to now we have seen:

- In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary (e.g. value iteration). 
- In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement. 
- In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other. 

> **Genearalized Policy Iteration**: As long as both processes (policy evaluation and policy improvement) continue to update all states, the ultimate result is typically the sameconvergence to the optimal value function and an optimal policy.

Intuitively, the evaluation and improvement processes in GPI can be viewed as both competing and cooperating. 

- They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically **makes the value function incorrect** for the changed policy, and making the value function consistent with the policy typically causes that **policy no longer to be greedy** (i.e. has a better policy given this new value function)
- In the long run, however, these two processes interact to **find a single joint solution**: the optimal value function and an optimal policy

<img src="rl/image-20221007144344783.png" alt="image-20221007144344783" style="zoom: 33%;" />

> Therefore, why GPI holds, i.e. usually converge to optimal value/policy, can be intuitively explained as:
>
> - The value function **stabilizes only when it is consistent with the current policy**, 
> - The policy **stabilizes only when it is greedy with respect to the current value function**.
>
> Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the ==Bellman optimality equation holds==, and thus that the policy and the value function are optimal.

This overall idea of GPI is used in ==almost any RL algorithms==

# Model-free RL

Recall that Model-based RL requires knowing the transition function and the reward function. More importantly in a **Model-Based problem**:

- we *could* get a closed form solution if the state space is small in a model-based MDP (using the Bellman's equation)
- iterative algorithms, that uses **Bellman expectation** equation, are used for estimating policy value
- control algorithm (i.e. value iteration), that uses **Bellman optimality** equation, are used to find optimal value/policy

But more often in reality we ==don't know the model==. So how do we solve the control problem?

> **Aim**: we can use sampling to ==estimate== the missing transition/reward models. The key question is then ==how do we take samples== to make our model more efficient in estimating the value functions?
>
> - MC Methods
> - TD Methods

## Monte Carlo Learning

> **MC Sampling**: learns from **complete** episodes (hence finite episodes)
>
> - hence can be only used with episodic MDP
>
> - the main idea is to consider value function of a state = mean return from it
>   $$
>   V(s) = \mathbb{E}[G(s)] \approx \frac{1}{n}\sum_{i=1}^n G(s)
>   $$

Specifically, we consider the goal of learning $V_\pi$ from episodes of a policy $\pi$
$$
(S_1, A_1, R_1, ..., S_k) \sim \pi
$$
is **one episode from $\pi$**. We ca sample many episodes, and for each episode:

1. compute the total discounted reward for the future for *each state*:
   $$
   G_t^i = R_{t+1}^i + \gamma R_{t+2}^i + ... + \gamma ^{T-1} R_T^i
   $$
   for $i$-th episode

2. estimate the value function using the law of large numbers:
   $$
   v_\pi(s) = \mathbb{E}_\pi[G_t | S_t=s] = \lim_{n \to \infty}  \frac{1}{n}\sum_{i=1}^n G^i_t(s)
   $$
   for $G_t^i(s)$ means the discounted reward starting from state $s_t$ at the $i$-th episode 

> While this is the basic idea, there are many **modifications** of this:
>
> - every visit v.s. first visit MC estimate to have different convergence speed - bias tradeoff
> - collecting all episodes and computing together is *computationally expensive*, hence there are iterative version of this

### First/Every Visit MC Policy Evaluation

Consider the following two epsiodes we have sampled following policy $\pi$:
$$
(S_1, S_2, S_1, S_2, S_4, S_5) \sim \pi \\
(S_1, S_2, S_2, S_4, S_5, S_3) \sim \pi
$$
with some reward for each state, but ignored here. Suppose I want to estimate $V(s_1)$ from the two samples

> **First Visit MC Policy Evaluation**: to evaluate $V_\pi(s)$ for a state $s$
> 1. *only* for the first time step $t$ that the state is visited in an episode
> 2. count that as a sample for state $s$ and increment counter $N(s) \gets N(s) + 1$
> 3. increment total return for that state $S(s) \gets S(s) + G_t$
> 
> Then the value estimate is:
> $$
> v_\pi(s) = \frac{S(s)}{N(s)}
> $$
> and by law of large numbers, $N(s) \to \infty$ we have $v_\pi(s) \to V_\pi(s)$. Hence this is a ==consistent== estimator.

Therefore, using first visit MC policy evaluation in the previous example, we would have:
$$
v_\pi(s_1) = \frac{G^{1}_1(s_1) + G^{2}_1(s_1)}{2}
$$
where $G^{i}_t(s)$ is the t-th timestep in the i-th episode. 

> **Every Visit MC Policy Evaluation**: to evaluate $V_\pi(s)$ for a state $s$
> 1. for the *every* time step $t$ that the state is visited in an episode
> 2. count that as a sample for state $s$ and increment counter $N(s) \gets N(s) + 1$
> 3. increment total return for that state $S(s) \gets S(s) + G_t$
> 
> Then the value estimate is:
> $$
> V(s) = \frac{S(s)}{N(s)}
> $$
> and this again, by law of large numbers, this is a ==consistent== estimator.

Using every visit MC policy evaluation in the previous example, we would have:
$$
V(s_2) = \frac{G^{1}_2(s_2) + G^1_4(s_2) + G^{1}_2(s_2) + G^{1}_2(s_2)}{4}
$$

Essentially we treat every sample of even the same episode separately

| Property | First Visit | Every Visit |
| :---:  | :---: | :---: |
| Consistent | Yes | Yes |
| Biased | No | Yes |
| Convergence Speed | Slow | Fast |


where:
- *usually* every-visit has a faster convergence speed (unless in cases when state is rare in both cases, could have similar speed)
- recall that for law of large number, we required independence of samples. However, samples from every visit are ==correlated==. Therefore, this could be ==biased== even though it could converge due to large number of samples.

> *Recall* that Bias and different from Consistent:
> - Consistent estimator: as the sample size increases, the estimates (produced by the estimator) "converge" to the true value 
> - Unbiased estimator: if I feed in different sample sets, the average of the estimates should be the true value

Last but not least, implementation wise since we are computing a mean, we could use **recursive mean computations**:
$$
  \begin{align*}
  \mu_k
  &= \frac{1}{k} \sum_{i=1}^k x_i \\
  &= \frac{1}{k} \left( \sum_{i=1}^{k-1} x_i + x_k \right) \\
  &= \frac{1}{k} \left( (k-1) \mu_{k-1} + x_k \right)\\
  &= u_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})
  \end{align*}
$$
in our case, $\mu$ would be the mean hence estimation of value $V_\pi$. This gives use the usually programmatic **recursive MC updates**, where we perform updates on the fly when doing sampling:

$$
\begin{align*}
  N(S_t) &\gets N(S_t) + 1\\
  V(S_t) &\gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))
\end{align*}
$$

whenever an update is needed. This gives a brief outline of the pseudo-code for both algorithm:

| First-Visit | Every-Visit |
| :---: | :---: |
| <img src='rl/image-20220930175001.png' style='zoom:100%;'/> | <img src='rl/image-20220930175019.png' style='zoom:100%;'/> |

### MC for Non-Stationary State

However, if you have a non-stationary problem (e.g. mean of a distribution is time-dependent), then we want to have a dynamic estimator and we don't want convergence.

Hence we just want to use ==new information directly==, without doing the averaging:
$$
V(s_t) \gets V(s_t) + \alpha (G_t - V(s_t))
$$
which you can just convert to the equation of moving average
$$
V(s_t) \gets (1-\alpha) V(s_t) + \alpha G_t
$$
for a **constant step size $\alpha$**. We can intuitively see the difference as the previous form of $V(S_t) \gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$ since now this is a constant step size:
$$
\begin{align*}
V_{t+1} 
&= V(s_t) + \alpha (G_t - V(s_t)) \\
&= \alpha G_t + (1-\alpha) V_t \\
&= \alpha G_t + (1-\alpha) [ \alpha G_{t-1} + (1-\alpha)V_{t-1}]\\
&= \alpha G_t + (1-\alpha) \alpha G_{t-1} + (1-\alpha)^2 V_{t-1}\\
&= ...
\end{align*}
$$
so eventually you reach $G_1$, hence we get:
$$
v_{t+1}(s) = (1-\alpha)^t v_1(s) + \sum_{i=1}^t \alpha (1-\alpha)^{t-i}G_i
$$
meaning that for large $t$:

- the first term will become extremely small, meaning that the **old information would matter very slightly** (whereas in $V(S_t) \gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$ every sample has equal contribution)
- usually we want to put **more weight on new information**, as this is non-stationary.
- but we usually use the other formula $V(s_t) \gets V(s_t) + \alpha (G_t - V(s_t))$ for update as that is more computationally efficient

But in principle, all the MC backups essentially does:

<img src="rl/image-20220930085105526.png" alt="image-20220930085105526" style="zoom:33%;" />

which is like a **DFS** when sampling. This is later constrasted with TD approaches.

## Monte Carlo Control

First, we revisit our previous approach of generalized policy iteration

<img src="rl/image-20220930085400935.png" alt="image-20220930085400935" style="zoom: 50%;" />

where we used greedy policy for improvement and is proven to work.

Therefore, a simple idea of model-free policy improvements

- use MC policy evaluation
- use greedy policy improvement?

We realize that using greedy-policy here have two concerns:
1. if we estimateed $v_\pi$, then since greedy needs:
   $$
    \pi'(s) = \arg\max_{a \in A} Q(s,a)
   $$
   but *if we only have $V_\pi$*, then we need to compute:
   $$
    \pi'(s) = \arg\max_{a \in A} \mathcal{R}_s^{a}+ \mathcal{P}_{ss'}^{a}V'_\pi
   $$
   but we don't know $\mathcal{P}$ 
2. could get me stuck in some suboptimal action due to exploitation

> To solve the first problem, it is easy: we can instead use ==MC methods to directly estimate $Q(s,a)$==

But then second problem is a little more complicated:

> We can use **$\epsilon$-greedy policy** instead of greedy policy to allow for ==exploration== since we are estimating the world (hence do no have complete information as in the Model-based case)
> $$
> \pi(a|s) = \begin{cases}
>   (\epsilon / m) + 1 - \epsilon & \text{if } a = \arg\max_{a \in A} Q(s,a) \\
>   \epsilon / m & \text{otherwise}
> \end{cases}
> $$
> for $m = |A|$. But does this **guarantee the improvement of the policy**? This is proven to be true!

*Proof*: for all states, any -greedy policy , the -greedy policy  with respect to is an improvement, that is,

<img src="rl/image-20220930090408152.png" alt="image-20220930090408152" style="zoom:60%;" />

where a key step is shown above. There we wanted to show that $q_\pi(s,\pi'(s))$ is better than the old $v_\pi(s)$

- the second equality comes from the fact that best action has an additional $(1-\epsilon)$ probability
- the third equality comes from the fact that $\sum_x \alpha(x)f(x) \le \max_x f(x)$ for $\sum \alpha(x) = 1$. In other words, weighted average is always less than the maximum value.
- the fourth equality comes from expanding the second term and cancelling the first term, with only $\sum \pi(a|s) q_\pi(s,a)$ left

Then finally the last step to show $v_{\pi'}(s) \ge v_\pi(s)$ which we ignored basically comes from expanding the definition of value function, and substituting in our result above.

Therefore, the final MC policy iteration becomes

<img src="rl/image-20220930091516025.png" alt="image-20220930091516025" style="zoom:50%;" />

> **Note** that different from the model-based case, we cannot perform value-iteration here to simplify the computation. This is because we don't have the transition model $\mathcal{P}$ as required in the Bellman optimality equation:
> $$
> Q_*(s,a) = \mathcal{R}_s^a + \sum_{s'} \mathcal{P}_{ss'}^a \max_{a'} Q_*(s',a')
> $$

### Greedy in the Limit with Infinite Exploration

Recall that we know, for every MDP problem, there exists an optimal deterministic policy by taking the greedy policy of an optimal Q-function. But if we are doing $\epsilon$-greedy, then we are not guaranteed to converge to the optimal policy.

> **GLIE**: A learning policy is called **GLIE** (Greedy in the Limit with Infinite Exploration) if it satisfies the following two properties:
> 1. If a state is visited infinitely often, then **each action in that state is chosen infinitely often** (with probability 1):
> $$
> \lim_{k \to \infty} N_k(s,a) = \infty
> $$
> 2. In the limit (as $t \to \infty$), the learning policy is **greedy with respect to the learned Q-function** (with probability 1):
> $$
> \lim_{k \to \infty} \pi_k(a|s) = 1(a = \arg\max_{a' \in A} Q_k(s,a'))
> $$

Therefore, taking into account our policy improvement with $\epsilon$-greedy, we can ==still achieve optimal policy by satisfying the GLIE condition== with:

$$
\epsilon_{k}  = \frac{1}{k}
$$

for the $k$ th iteration we have updated the policy.

Therefore, we get essentially a change for policy improvement step:

$$
\text{Policy Improvement:}\quad \epsilon \gets \frac{1}{k}, \quad \pi'(s) \gets \epsilon\text{-greedy}(Q)
$$

## Off-Policy Evaluation

So far, all we have learned is on-policy, i.e. the **behavior policy** used to collect samples is the same as the policy you are estimating $V_\pi$

> **Off-policy evaluation**: means the **behavior policy $\mu$** you used to collect data is **not** the same policy you want to estimate $V_\pi$. So you want to learn policy $\pi$ from $\mu$. 
> - for here, we can suppose both $\mu$ and $\pi$ policy are known
> - essentially you may want to evaluate your $V_\pi$ or $Q(S_t,A_t)$ but only **using data from $\mu$**.

How do we do that? The basic methods here is importance sampling.

### Importance Sampling

> **Importance Sampling**: goal is to estimate the expected value of a function $f(x)$ (i.e. $G(s)$) under some probability distribution $p(x)$, without sampling from the distribution $p(x)$ but **using $f(x)$ sampled from $q(x)$**:
> $$
> \mathbb{E}_{x \sim p(x)}[f(x)] =?
> $$
> with data sampled from $x \sim q(x)$. It turns out we can easily show that, under few assumptions:
> $$
> \mathbb{E}_{x \sim p(x)}[f(x)] = \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]
> $$

Notice that the ==stationary distribution of states== will be different, i.e. we will be considering

<img src="rl/image-20220930093651232.png" alt="image-20220930093651232" style="zoom:33%;" />

So, if we have samples generated from $x \sim q$, and since in our case we consider $f(X) = G_t(s)$, then basically need to consider, for each trajectory $\tau_j$ and its discounted reward $G^{j}$:
$$
Q_{\tau \sim \pi}(s,a) = \mathbb{E}_{\tau \sim \mu(x)}\left[\frac{P_\pi(\tau)}{P_\mu(\tau)}G^{j}\right]
$$
But what is this ratio of probability?
$$
P_\pi(\tau) = P_\pi(S_t, A_t, ..., S_T, A_T)\\
P_\mu(\tau) = P_\mu(S_t, A_t, ..., S_T, A_T)
$$
we can essentially use chain rule to represent this as quantities we know:
$$
\begin{align*}
P_\pi(\tau) 
&= P_\pi(S_t, A_t, ..., S_T, A_T)\\
&= \pi(A_T | S_t, S_{T-1},A_{T-1}, ..., S_t,A_t) P_\pi(S_t, A_t, S_{t+1},A_{t+1},...S_T, A_T)\\
&= \pi(A_T | S_T) P_\pi(S_T | S_{T-1},A_{T-1},...S_t, A_t)P_\pi(S_{T-1},A_{T-1},...S_t, A_t)\\
&= \underbrace{\pi(A_T | S_T)}_{\mathrm{policy} } \underbrace{P_\pi(S_T | S_{T-1}, A_{t-1})}_{\text{transition model} } P_\pi(S_t, A_t, S_{t+1},A_{t+1},...S_{T-1},A_{T-1})\\
&= ...\\
&= p(S_{1}) \prod_{i=1}^{T}\underbrace{\pi(A_{i}|S_{i})}_{\mathrm{policy}} \underbrace{p(R_{i}|S_{i},A_{i})}_{\text{reward model}} \underbrace{p(S_{i+1}|S_{i},A_{i})}_{\text{transition model}}
\end{align*}
$$
where 

- the third and fourth equality comes from the fact that we are MDP, hence future **only depends on current**
- finally, the transition probability and reward probability will be cancelled when we do the ratio:
  $$
  \frac{P_\pi(\tau)}{P_\mu(\tau)} = \frac{p(S_{1}) \prod_{i=1}^{T}\pi(A_{i}|S_{i}) p(R_{i}|S_{i},A_{i}) p(S_{i+1}|S_{i},A_{i})}{p(S_{1}) \prod_{i=1}^{T}\mu(A_{i}|S_{i}) p(R_{i}|S_{i},A_{i}) p(S_{i+1}|S_{i},A_{i})} = \prod_{i=1}^{T}\frac{\pi(A_{i}|S_{i})}{\mu(A_{i}|S_{i})} \equiv w_{\mathrm{IS}}
  $$
  is called the ==importance sampling weight== for each trajectory
- can estimate *any other policy*, but given that we have the same coverage. unseen action in policy $\mu$, it has prob zero

> **Ordinary IS**: the above essentially gives the ordinary IS algorithm:
> $$
> \begin{align*}
>   V_{\pi}(s)
>   & \approx \frac{1}{n} \sum\limits_{j=1}^{n} \frac{p_\pi(T_j|s)}{p_\mu(T_j|s)} G(T_j)\\
>   &= \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{T} \frac{\pi(a_{j,i}|s_{j,i})}{\mu(a_{j,i}|s_{j,i})} \right) G(T_j)\\
>   &= \frac{1}{n} \sum\limits_{j=1}^{n} w_{\mathrm{IS}} \cdot  G(T_j)
> \end{align*}
> $$
> for practically, we are just **swapping out $G(T_j)$ with $w_{\mathrm{IS}} \cdot  G(T_j)$ in our previous on-policy algorithms**.

Now, what are the assumptions used to make IS work? You might notice the term $\frac{\pi(a_{j,i}|s_{j,i})}{\mu(a_{j,i}|s_{j,i})}$ could have gone badly, and it is exactly the case

> **Importance Sampling Assumptions**: since we are reweighing samples from $\mu$, if we have distributions that are non-overlapping, then this will obviously not work.
>
> - in particular, if we have any single case that $\mu(a|s)=0$ but $\pi(a|s)>0$, then this will not work.
> - therefore, for this to work, we want to have a large ==coverage==: so that for $\forall a,s$ such that $\pi(a|s)>0$, you want $\mu(a|s)>0$.

Intuitively, this means that if $\pi$ is not too far off from $\mu$, then the importance sampling would work reasonably.

In practice, there are still some problems with this, and hence some variants include:

- **Discounting-aware Importance Sampling**: suppose you have $\gamma =0$ and you have for long episodes. Then techinically since $\gamma=0$, your return is determined since $S_0,A_0,R_0$ and multiplying the weight by $w_{\mathrm{IS}}$ contributes significantly to ==variance==
- **Per-decision Importance Sampling**: which applies a weight to each single reward $R_T$ within the $G$.

## TD Learning

In practice, the MC is used much less because:

- we need to **wait** until the agent reaches the **terminal state** to compute $G_t$, which is our target for update equation
  - e.g. if we play go game, it needs to wait until the end of the game
- hence also requires finite episode settings
- **large variance**, because the **episode can be long**

> **Aim**: TD learning tries to resolve that issue by "estimating the future" instead of waiting and using the true future. So the questions become: is there some useful information we can do without this wait?
>
> - learns from incomptele episodes, hence bootstrapping
>
> - but because you are updating from your estimate based on your estimate, **there will be bias** (MC has no bias)
>
> - For any fixed policy $\pi$, TD(0) has been proved to converge to $v_\pi$, in the mean for a constant step-size parameter if it is sufficiently small, and with probability $1$ if the step-size prameter decreases according to the usual stochastic approximation condition:
>   $$
>   \sum_{n=1}^\infty \alpha_n(t) = \infty,\quad \text{and}\quad \sum_{n=1}^\infty \alpha^2_n(t) < \infty
>   $$
>   The first condition is required to guarantee that the steps are large enough to eventually ==overcome any initial conditions or random fluctuations==. The second condition guarantees that **eventually the steps become small enough to assure convergence**.

Recall that MC updates in general looks like
$$
V(S_t) \gets V(S_t) + \alpha (\underbrace{G_t}_{\text{MC target}} - V(S_t))
$$
which is the actual return, but in TD(0) we can update using an **estimated return**
$$
V(S_t) \gets V(S_t) + \alpha (\underbrace{(R_{t+1} + \gamma V(S_{t+1})}_{\text{TD(0) target}}) - V(S_t))so that :
$$
- essentially I am utilizing the fact that:
  $$
  G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ...= R_{t+1} + \gamma V(S_{t+1})
  $$
  hence since we can approximate $V(S_{t+1}) \approx v(s_{t+1})$, we get our TD(0) update

- intuitively, this TD(0) means that I wanted to make update on $S_t$ and I do this by:
  - taking an action $A_t$ and get to $S_{t+1}$
  - then using my trajectory $S_{t}, R_{t+1}, S_{t+1}$ to update by estimating the expected return I would get in the future

- so this means that like MC, I will still be estimating with acting and sampling experience from real environment

But since TD$(0)$ is only moving/looking one step forward at $V(S_{t+1})$, you can easily also look many steps:

<img src="rl/image-20221007111034003.png" alt="image-20221007111034003" style="zoom: 25%;" />

> **General TD$(n)$** updates: since we can also have
> $$
> G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ...= R_{t+1} + \gamma R_{t+2}+\gamma^2 V(S_{t+2})
> $$
> this becomes a TD(1) since now we are looking even one more step ahead, hence suing
> $$
> V(S_t) \gets V(S_t) + \alpha (\underbrace{(R_{t+1}+\gamma R_{t+2} + \gamma^2 V(S_{t+1})}_{\text{TD(1) target}}) - V(S_t))
> $$
> and this can go to TD$(n)$ easily.

Visually, we can compare the three algorithms discussed so far:

|                          MC Update                           |                          TD Update                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl/image-20221007082930340.png" alt="image-20221007082930340" style="zoom:33%;" /> | <img src="rl/image-20221007082958257.png" alt="image-20221007082958257" style="zoom:33%;" /> |

And just to be complete, for DP method we know all transition probabilities, henc we can take into account all possibilities

<img src="rl/image-20221007083124404.png" alt="image-20221007083124404" style="zoom:33%;" />

In this sense,  TD(0) can be seen as **taking samples over this DP backup**, which also "bootstraps"

|      | Needs Model | Sampling | Bootstrapping |
| :--: | :---------: | :------: | :-----------: |
|  DP  |     Yes     |    No    |      Yes      |
|  MC  |     No      |   Yes    |      No       |
|  TD  |     No      |   Yes    |      Yes      |

Finally, algorithmically for TD(n):

<img src="rl/image-20221007111217374.png" alt="image-20221007111217374" style="zoom: 33%;" />

## MC v.s. TD Methods

Here we compare some features of using MC v.s. TD to perform policy evaluation:

|                                            |                              MC                              |                            TD(0)                             |
| ------------------------------------------ | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Learn before/without knowing final outcome |                              No                              |                             Yes                              |
| Target for update                          |                 $G_t$ which is a real return                 |     estimate $R_{t+1} + \gamma v(S_{t+1})$ can be biased     |
| Variance                                   | High, because single branch + long episode means uncertainy builds up quickly | Low, because now depend on one random action, transition, and reward |
| Bias                                       |                           no bias                            |            not proven, has shown bias upperbound             |
| Sentitive to Initial Value                 |                              no                              |              yes, can affect convergence speed               |
| Convergence Properties                     |                          Converges                           |                          Converges                           |
| Convergence Speed                          |                             Slow                             |                             Fast                             |
| used with Function Approximation           |                        Little problem                        |                 often has problem converging                 |
| Storing Tabular Data                       |                             Yes                              |                             Yes                              |

The still remaining concern but both still requires some kind of table to store value for each state, and/or action. For example, we update $Q$ estimate by storing the values of $(s,a)$ in a ==tabular representation==: ==finite== number of state-action pair.

However, as you can imagine many real world problems have **enormous state and/or action space** so that we cannot really tabulate all possible values. So we need to somehow ==generalize== to those unknown state-actions.

> **Aim**: even if we encounter state-action pairs *not* met before, we want to make *good decisions* by past experience.

> **Value Function Approximation**: represent a (state-action/state) value function **with a parametrized function** instead of a table, so that even if we met an inexperienced state/state-action, we can get some values. (which will be discussed in the next section)

---

*For Example*: using TD for policy Evaluation

Consider the following MDP, with reward $0$ except for one state:

<img src="rl/image-20221007084804460.png" alt="image-20221007084804460" style="zoom: 50%;" />

So that essentially there are:

- two terminate states, so your optimal policy is to just move right. 
- And suppose my current policy $\pi$ is random left or right at equal probability, we want to evaluate this policy $\pi$ .

Let us use TD(0) learning and have initial values $V(S)=0.5$ for non-terminal states and $V(S_T)=0$ for those temrinal states.

- we take $\alpha=0.1$, $\gamma=1$
- then we update the estimate **per time-step** following TD(0) algorithm

With which we arrive at:

<img src="rl/image-20221007134742879.png" alt="image-20221007134742879" style="zoom: 50%;" />

so after 100 episodes seen, we see it is going to converge to the true value. 

Interestingly, from here we can ask: what is the **episode encountered** (see the line annotated by number 1) for the so that TD(0) only end up updated $V(A)\gets 0.45$?

- realize that if you calculate anything from $V(S)\to S' \neq S_T$ you will get no change in value, still $0.5$. For instance:
  $$
  V_A \gets V_A + \gamma (R + V_B - V_A) = 0.5 + 1 \cdot (0) = 0.5
  $$

- but things change only if you ended up at $V_T$ on the left, so that you get
  $$
  V_A \gets V_A + \gamma (R + V_T - V_A) = 0.5 + 0.1 \cdot (0 + 0 -0.5) = 0.45
  $$

Therefore the episode just needs to hit left end state.

In this example, we can also compare the MC and TD variance

<img src="rl/image-20221007090134530.png" alt="image-20221007090134530" style="zoom: 50%;" />

and note that MC achieves least MSE, while TD is performing MLE.

## TD Control

In the end all people care is to get the optimal policy, rarely just to evaluate. Hence we need to use the evaluation/learning and do control

> **Intuitively**: we can just apply TD to the $Q(S,A)$, and use $\epsilon$-greedy policy improvement as we have proven to work with MC methods
>
> - as long as we have some form of alternating policy evaluation and policy improvement, following [Generalized Policy Iteration](#Generalized Policy Iteration) mos algorithms typically converge to the optimal policy

### SARSA

> **SARSA**: Essentially TD(0) learning for one step followed immediately by $\epsilon$-greedy policy improvement.
>
> - intuitively, it follows the [Generalized Policy Iteration](#Generalized Policy Iteration) style
>
> - Sarsa converges with probability 1 to an optimal policy and action-value function, under the usual conditions on the step sizes $\sum \alpha^2 < \infty$, as long as all stateaction pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (GLIE, which can be arranged, for example, with $\epsilon$-greedy policies by setting $\epsilon = 1/t$).

Given a single move/step $(S,A,R,S',A')$ we can perform an update using TD(0) equation:

<img src="rl/image-20221007091739133.png" alt="image-20221007091739133" style="zoom: 33%;" />

Then, instead of waiting it to cnverge we can directly perform update, hence the algorithm becomes

<img src="rl/image-20221007091933118.png" alt="image-20221007091933118" style="zoom:50%;" />

where notice that essentially I improve my policy by picking better $A$ using $\epsilon$-greedy, and then ask my model to update accordingly.

- this is **on-policy control** (i.e. behavior policy $\pi_b$ is the same as evaluation policy $\pi_e$)

- on-policy as you already chosen your action while doing policy improvement
- and recall that for convergence to the optimal greedy policy, you will **need $\epsilon$ to decrease (i.e. the GLIE)**

### Q-Learning

> **Q-Learning**: Most important and widely used today. It is motivated by having the **learned** action-value function, $Q$, from TD updates to **directly approximates $Q_*$**, the optimal action-value function, independent of the policy being followed.
>
> - dramatically simplifies the analysis of the algorithm and enabled early convergence proofs (is proven to converge)
> - All that is required for correct convergence is that all pairs continue to be updated. Under this assumption and a variant of the usual stochastic approximation conditions (i.e. $\sum \alpha^2 < \infty$) on the sequence of step-size parameters, **$Q$ has been shown to converge with probability 1 to $Q_*$.**

Now, the algorithm is that you are not choosing $A'$ from the current policy $\pi$, but update using greedy policy $\mathrm{greedy}(\pi)$

<img src="rl/image-20221007092305741.png" alt="image-20221007092305741" style="zoom: 50%;" />

hence this is off-policy

- your update is to estimate a greedy policy (i.e. optimal policy). This could have you, for example picking leftest action
- but when I sample, I use $\epsilon$-greedy policy, meaning that I might have ended up in a differnt state. e.g. by picking the rightest action
- but it is proven that it still converges to the optimal policy quickly

Algorithm:

<img src="rl/image-20221007145704583.png" alt="image-20221007145704583" style="zoom:33%;" />

where here I improve my policy by picking better $A$ using $\epsilon$-greedy, but then ask my model to update the greedy policy. 

- This is widely used as there are early theoretical proofs to show it always converge to the optimal policy. But note that

- but SARSA + greedy is still different than Q-Learning
- so under the usual stochastic approxmation conditions with step size, and using GLIE, i.e. $\epsilon$ decreasing, **both SARSA and Q-Learning will converge to the optimal value/policy**. So what is the different between using them?

---

*For Example: Cliff Walking*

Consider the following cliff walking problem, where we want to use the SARSA and Q-learning to find the ==optimal policy given fixed $\epsilon=0.1$.== Then eventually you will see the following performance

<img src="rl/image-20221007093725043.png" alt="image-20221007093725043" style="zoom:50%;" />

however, here we notice that Q-learning is doing worse. This does **not** contradict our previous conclusions because:

- Q-learning aims to directly learn the optimal policy **without taking into account the behavioral policy** (i.e. $\epsilon=0.1$). Hence it learns the optimal path but occasionally falls off the cliff due to $\epsilon=0.1$
- SARSA aims to learn the policy **taking into account the behavioral policy** and hence picks the safer path

- but at the end of day, if we decrease $\epsilon \to 0$, then both converge to the same optimal path

But why would you want to have $\epsilon$-greedy policy used as behavioral policy when deployed?

- in cases such as automonomous driving, simulation environment might be too optimistic and you may want your model to be more **robust to random errors**, as shown in the cliff case
- so in that case SARSA could be prefered, which might not give you the optimal $Q$ values hence more robust

## DP v.s. TD Methods

We can compare the backup diagrams and realize that our update forms corresponds a lot to **DP=Bellman Equations while TD=sampling Bellman Equations**

<img src="rl/image-20221007094456708.png" alt="image-20221007094456708" style="zoom: 50%;" />

This becomes much clearer if we compare the update equation used:

<img src="rl/image-20221007094708566.png" alt="image-20221007094708566" style="zoom:50%;" />

where $x \xleftarrow{\alpha} y$ means the update $x \gets x+ \alpha (y-x)$

- so that Q-learning is **sampling Bellman Optimality from DP**
- SARSA is like **sampling from Bellman Expectation from DP**

## TD($\lambda$) and Eligibility Traces

We want to expand the basic TD(0) idea which can perform per step update (no wait for $G_t$) and MC which involves deep updates. Specifically, you will see how those relate to the **idea of TD($\lambda$)**, and eventually see a **unified view of RL algorithmic solutions**.

At the end of this section, you should

- understand what TD$(\lambda)$ and eligibility traces are

- have a picture of how existing algorithms are came up/organized


Then we can go into implementations with Deep NN in the next section.

### TD($\lambda$)

Recall that TD(0) considers the update of **one-step look ahead**

<img src="rl/image-20221015013800029.png" alt="image-20221015013800029" style="zoom:33%;" />

which basically aims to approximate the target $G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ...= R_{t+1} + \gamma V(S_{t+1})$. We mentioned that we can easily extend this idea to **$n$-step look ahead**, which can be even generalized to the MC which gets to the terminate state:
$$
\begin{align*}
G_t^{(n)} 
&=R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ... \\
&= R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \underbrace{\gamma^nV(S_{t+n})}_{\text{n-th step}}
\end{align*}
$$
Hence visually

|                            Visual                            |                  TD(n) to MC Update Target                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl/image-20221014081856031.png" alt="image-20221014081856031" style="zoom: 33%;" /> | <img src="rl/image-20221014081936837.png" alt="image-20221014081936837" style="zoom: 50%;" /> |

> **Intuition**: We can perhaps combine both MC and TD, and use **all the information in the sample**. So that instead of a single $n$-step return, we can **average/combine all the $n$-step return**.

For instance, consider you have a 2 and 4 step return computed, we can consider

|                         Data We Have                         |                            Target                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl/image-20221014082300419.png" alt="image-20221014082300419" style="zoom: 33%;" /> | $\frac{1}{2} G^2 + \frac{1}{2} G^4 = \frac{1}{2}\sum G^{(n)}$ |

So the question is what is a good way to combine those information?

> **$\lambda$-Return:** compute $G_t^\lambda$ which combines all $n$-step return $G_t^{n}$ ==using weight $(1-\lambda )\lambda^{n-1}$==, which decays each future by $\lambda$ while summing up to $1$. This means that given all returns:
> $$
> G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}, \quad G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{N}V(S_{t+N})
> $$
> and hence your update equation becomes
> $$
> V(S_t) \gets V(S_t) + \alpha (G_t^\lambda - V(S_t))
> $$
> however, since this means you are adding both long episodes (MC) and short ones (TD), you will get
>
> - ==both high bias and big variance== (from MC and TD)
> - you need a ==delay as it waits until terminal state== (based on definitino $n=T$ here is the terminal state)
> - using ==bootstrapping== because we calculate $G_t^{(n)}$ using TD method, hence ==sensitive to initialization==
>
> so even in TD(0) we can get rid of a lot of those problems. Meaning that TD($\lambda$) is not really used in practice. But this did provided the theoretical foundation for the eligibility trace, which is very useful.

Visually, the weights look like

|                    Using All Information                     |     Weights on Early $G_t^{(n)}$ decays = less important     |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="rl/image-20221014082434105.png" alt="image-20221014082434105" style="zoom:50%;" /> | <img src="rl/image-20221014083138482.png" alt="image-20221014083138482" style="zoom:33%;" /> |

> ==Note== that in the forward-view, the **weights need to sum up to $1$**. This means in case if you have a finite horizon, as shown in the figure above you need to **adjust your last weight** so that your weights sum up to $1$.
> $$
> G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t}^{(n)}+\underbrace{\lambda ^{T-t-1}G_t}_{\text{last return}}
> $$
>
> - for instance, if you have three $G_t^{(n)}$, and $\lambda = 0.5$, then the weights will be $0.5, 0.5^2, 1-(0.5+0.5^2)$

So how do we **improve this theoretical idea**. Consider viewing TD($\lambda$) as a ==forward view== as we are looking into the future

<img src="rl/image-20221014083247509.png" alt="image-20221014083247509" style="zoom: 50%;" />

> This forward view provides theory, the ==backward view== provides mechanism/practicality

I would like to fix the following problems in this algorithm:

- get a sample and can update like TD(0) per time step (instead of waiting)
- combines information like TD($\lambda$) but hopefully avoid those bias and variance

### Eligibility Trace

> **Heuristics**: what are the useful features of the forward view TD($\lambda$) update?
>
> - ==frequency heuristics==: most visited states should be more important (=need to have some kind of *memory* to know that it was visited in the past)
>   - achieved by constantly adding the history and yet decaying like TD($\lambda$). i.e. add past eligibility but scaled: $\lambda E_{t-1}$
>
> - ==recent heuristic==: most recent states should be also important
>   - add a weight of $1$ (the indicator function)
>

So the idea technically comes form TD($\lambda$), which actually does

- put more weight on the most recent state it has the highest weight
- more weight on most visited states as they will be added more than once in the $\sum$

> **Eligibility Trace**: essentially how do we ==design a weighting== that achieves the above but also allow us to do per-step update?
>
> Initialize the weight $E_0(s)=0$ for each state, and then consider 
> $$
> E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)
> $$
> for $\mathbb{1}$ is the indicator function, and it records the **"importance" for each state** when the value function is updated (see later). But why this form? It turns out that this specific form allows for an ==(offline) equivalence== of eligibility trace to TD($\lambda$) forward view.

For example, consider you are rolling out a single episode, and a state that is being visited 4 times in a row, not seen for a while, then seen twice, then seen once. Then, the eligibility trace of it looks like:

<img src="rl/image-20221014084104772.png" alt="image-20221014084104772" style="zoom:50%;" />

therefore, it satisfied what I needed:

- if a state is visited more **often**, it gets a higher eligibility trace (like in the beginning)
- if a state is visited more **recently**, it automatically have a weight of $+1$

> **Backward View** of TD($\lambda$): now we can use eligibility trace for value update.
>
> Consider the TD error $\delta_t$ we used to update when doing TD(0):
> $$
> \delta_t = \underbrace{R_{t+1} + \gamma V(S_{t+1})}_{\text{TD(0) target}} - V(S_t)
> $$
> which enabled us to do per time-step update. Then, the idea is we ==weigh== the per-time step error, such that ==if we accumulate the updates until the end of the episode==, it will result in the ==same update as the forward TD($\lambda$)==. This is also called the **offline equivalence** (see next section), but with this constraint we end up with the weight called ==eligibility trace $E_t(s)$== which we showed before:
> $$
> E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)
> $$
> and hence the update rule:
> $$
> V(s)\gets V(s) + \alpha E_t(s) \delta_t
> $$
> so that this is given to each state, e.g. if we have 10 states, we get 10 eligibility traces. Intuitively, since $E_t(s)$ depended on history, it measures how much this state matters **by looking in the past**: to see if it is seen frequently, recently.

Visually, the backward view means we are using this loss $\delta_t$ using information in the past:

<img src="rl/image-20221014084613356.png" alt="image-20221014084613356" style="zoom:33%;" />

additionally

- if $\lambda = 0$, then $E_t(s) =\mathbb{1}(S_t=s)$  and then eligibility trace becomes TD(0) update since we are then doing:
  $$
  V(s)\gets V(s) + \alpha \delta_t(s)\cdot \mathbb{1}(S_t=s)
  $$
  meaning we only update the ones we just saw

So this backward update now has the advantage of

- including in the *idea* of forward TD($\lambda$) to use **all the information we have**

- this is implementable as we don't need to look n-step beyond (but behind). Meaning we can perform **update per time step**

Finally, we can show the algorithm of using Eligibility trace:

<img src="rl/image-20221014085742951.png" alt="image-20221014085742951" style="zoom: 67%;" />

where basically the error is TD(0) error, but we are **updating all states using eligibility trace per time step**

- this means we need to have two tables, one for eligibility per state and anther store the value per state
- since now it is online (update per step), this backward is **not** equivalent to the forward view. But if we pick a step size sufficiently small, it is almost equivalent
- practically, this **online TD($\lambda$) updates** is what we use and works

> ==Note== that 
>
> - we are updating **all states per time step**, so even if you have just seen a state $S_t=B$, and $S=\{A,B\}$, you still need to update **both $S=A,S=B$** in the loop when updating $V(s)$. A hint is to notice that $\delta$ is not a function of state in the above algorithm.
>
> - when used with value function approximation, the **eligibility trace generalizes** to
>   $$
>   E_t(s) = \gamma \lambda E_{t-1}(s) + \nabla \hat{V}(S,w)
>   $$
>   and algorithm looks like
>
>   <img src="rl/image-20221205221054594.png" alt="image-20221205221054594" style="zoom: 25%;" />

### Offline Equivalent of Forward and Backward Views

Finally, here we answer the question why did people decide to use specifically the weight in the form of $E_t(s)$
$$
E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)
$$

> **Offline Equivalence**
>
> Consider having your eligibility trace being updated/accumulated all the time, but ==you only update $V$ at the end==. Recall that for the forward view, we also only update at the end. Now, you realize that
>
> <img src="rl/image-20221014085128664.png" alt="image-20221014085128664" style="zoom:33%;" />
>
> so if we were offline only learning the eligibility traces, your final updated $V$ will be the same as the forward view
>
> - so in offline mode you are essentially **summing up all the errors of each state over all time** but without updating $V$
> - so then the proof is to show that the this accumulated error is the same in forward view! But this proof is long so skipped

## TD($\lambda$) Control

In the previous section we are essentially **learning the value function**, now we care about using it to **find the optimal policy**. The basic idea is again simple: we can apply this learning style to learn an optimal $Q_*(s,a)$. 

Recall that to derive the backward eligibility trace we need to

1. derive the **forward** algorithm by considering the $n$ step look-ahead version of the TD error
2. use the eligibility trace in **backward** algorithm
3. show their **offline equivalence**

### SARSA($\lambda$)

First, we consider the forward TD($\lambda$). Consider the ==$n$-step look ahead for SARSA's target==

<img src="rl/image-20221015155935901.png" alt="image-20221015155935901" style="zoom:33%;" />

So that the $n$-step look ahead return is
$$
q_t^{n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1} R_{t+n} + \gamma^n Q(S_{t+n})
$$
Hence the forward view considers weighting those returns:
$$
q_t^{\lambda} = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}
$$
So the forward view of SARSA becomes:
$$
Q(S_t, A_t) \gets Q(S_t,A_t) + \alpha (\underbrace{q_t^{\lambda}}_{\text{TD($\lambda$) target}} - Q(S_t,A_t))
$$
and notice that using $\lambda=0$ we get back SARSA (which uses TD(0) target)

Then from it, we can **find the backward view** by building this ==eligibility trace== knowing that
$$
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)
$$
so then intuitively we can say that the eligibility trace is, with $E_0(s,a)=0$:
$$
E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{1}(S_t=s, A_t=a)
$$

so that our update rule is:

$$
Q(s,a) \gets Q(s,a) + \alpha E_t(s,a) \delta_t
$$

essentialy:

- target is stil SARSA target, but error is weighted by eligibility trace
- so again two tables, one for Q and one for eligibility trace
- and we skip the proof that this gives the same result as the forward view of SARSA if offline

Therefore, we get this SARSA($\lambda$) algorithm

<img src="rl/image-20221014091605986.png" alt="image-20221014091605986" style="zoom:50%;" />

We can visualize the differences:

<img src="rl/image-20221014091753630.png" alt="image-20221014091753630" style="zoom: 50%;" />

so that
- if SARSA, then we only updated using only one "previous" step
- if using 10-step SARSA, my last step will take into account my previous 10 steps equally
- if using SARSA($\lambda$) with $\lambda=0.9$, then we update using all history weighted by eligibilty trace

### Q($\lambda$)-Learning

There are many ways to do this, but you need to have some proof that it works. So here we use Watkins's version. First recall that

- Q-learning aims to do Bellman optimality directly each step
- it is off-policy, and like SARSA, the update is per step (no need to wait)
- both Q-learning and SARSA is proven to give you the optimal policy (under mild conditions)

Now, consider Q($\lambda$).  We first consider ==$n$-step look ahead of the target== (different from SARSA's target)
$$
\begin{cases}
\text{1-step}, & R_{t+1} + \gamma \max_a Q(S_{t+1}, a) \\
\vdots \\
\text{$n$-step}, & R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1} R_{t+n} + \gamma^n \max_a Q(S_{t+n}, a)
\end{cases}
$$
But then, you have a problem that since Q-learning is off-policy, what does the forward view look like? In Watkin's $Q(\lambda)$, we considers two terminate cases:

<img src="rl/image-20221014092301366.png" alt="image-20221014092301366" style="zoom:50%;" />

so that your last look-ahead either is
- the actual terminate state
- your behavior policy picked an action that is non-greedy

So that your backward view becomes

$$
\delta_{t} = R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)
$$
with eligibility trace
$$
E_t(s,a) = \begin{cases}
1 + \gamma \lambda E_{t-1}(s,a) & \text{if }s=s_t, \text{ and } a\sim \epsilon\text{-greedy}(Q(s,a)) = \arg\max_{a} Q(s,a) \\
0, & \text{if }a \neq \arg\max_{a} Q(s,a)\\
\gamma \lambda E_{t-1}(s,a) & \text{otherwise, i.e. not at } s_t
\end{cases}
$$
with the upadte rule being

$$
Q(s,a) \gets Q(s,a) + \alpha E_t(s,a) \delta_t
$$

Hence the algorithm looks like

<img src="rl/image-20221014092949925.png" alt="image-20221014092949925" style="zoom:50%;" />

so basically
- if action I took w.r.t. to my current $\epsilon\text{-greedy}(Q)$ is the greedy action, then we do the normal eligibility trace update
- if not, then reset all to zero, because we reached a "dummy terminal state" which we defined by next move being not greedy action (by this definition, we can still have equivalence of backward and forward view)
- so again, my error is the Belmman Optimality Error using the $A^{*}$ from the greedy policy, but my behavior policy is to do $A'$ which is $\epsilon$-greedy. Hence this is off-policy as is Q-learning.

> Recall that all algorithms we see here **needs a tabular value**. Hence for large state space/state-action space, those algorithms are slow due to this loop for all state/state-action. 
> 
> This problem is solved by using **function approximation**: the only parameters are the parmeters of the functions which can spit out those values, which we will see in the next section.

## Unified View of RL Solutions

All the basics of RL algorithms can be put in one picture, and we can understand their differences/similarities

<img src="rl/image-20221014094057829.png" alt="image-20221014094057829" style="zoom:50%;" />

where

- shallow means update only looks at 1 step, and deep is like MC (**depth**). This depth can be changed depending on n-step return and hence the $\lambda$ parameter weighting **how many future** you want to use.
- full backup means you look at all possible neighbors (**width**)
- in practice, all algorithms such as Q-learning and SARSA are based on sample backups, as the updates are faster and more efficient
- MCTS with truncation and approximations would be similar to the exhaustive search on top right]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Reinforcement Learning]]></summary></entry><entry><title type="html">COMS4995 Deep Learning part1</title><link href="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/" rel="alternate" type="text/html" title="COMS4995 Deep Learning part1" /><published>2022-12-08T00:00:00-05:00</published><updated>2022-12-08T00:00:00-05:00</updated><id>/lectures/2022@columbia/COMS4995_Deep_Learning_part1</id><content type="html" xml:base="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/"><![CDATA[# Logistics and Introduction

Office hours

- Lecturer, Iddo Drori (idrori@cs.columbia.edu), Tuesday 2:30pm, Zoom (Links to an external site.)
- CA, Anusha Misra, Wednesday 3:30-4:30pm, Zoom (Links to an external site.)
- CA, Vaibhav Goyal, Friday 3-4pm, Zoom (Links to an external site.)
- CA, Chaewon Park (cp3227@columbia.edu), Thursday 3:30-4:30PM, Zoom (Links to an external site.)
- CA, Vibhas Naik (vn2302@columbia.edu), Monday 11AM-12PM, Zoom

Grades:

- 9 Exercises (30%, 3% each, **individual**, quizzes on Canvas)
  - quizzes will timed in some of the live lectures. So attend lectures!
- Competition (30%, in pairs)
- Projects (40%, in teams of 3 students)

**Projects Timeline**

- *Feb 18*: Form Teams and Signup
- *Feb 25*: Select project
- *Mar 10-11*: Project Kick-off and proposal meetings
- *Mar 31 - Apr 1*: Milestone Meetings
- *Apr 21-11*: Final Project Meetings
- *Apr 28*: Project poster session

## Human Brain Deep Learning

In comparison of sizes of number of neurons:

<img src="DL_notes/image-20220118133414667.png" alt="image-20220118133414667" style="zoom: 67%;" />

where the left hand size are the deep learning models and the right hand size the human brain

- however, it is also said that humans are "generalized", where machines are "specialized"

What happens 

| ![image-20220118133924987](DL_notes/image-20220118133924987.png) | ![image-20220118133935376](DL_notes/image-20220118133935376.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

- Type 2 process, your pupil will dilate (slow)

Then, in AlphaGo, as well as other models, essentially it is doing:

<img src="DL_notes/image-20220118134044859.png" alt="image-20220118134044859" style="zoom:50%;" />

where the 

- neural networks DNN are doing Type 1 processes
- tree search doing Type 2. (e.g. Monte Carlo Tree Search)

## Intro: Transformers

[The paper](https://arxiv.org/abs/1706.03762) Attention Is All You Need describes transformers and what is called a **sequence-to-sequence architecture**. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the  sequence of words in a sentence, into another sequence. (Well, this  might not surprise you considering the name.)

Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A brief sketch of how it works would be:

1. Input (e.g. in English) passes through an encoder
2. Encoder takes the input sequence and maps it into a higher dimensional space (imagine translating it to some imaginary language $A$)
3. Decoder takes in the sequence in the imaginary language $A$ and turns it into an output sequence (e.g. French)

Initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.

- A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.

More details:

- https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04

- https://towardsdatascience.com/transformers-141e32e69591

### Example Application

Consider the task of trying to use AI to solve math problems (e.g. written in English)

It turns out that using language models, it doesn't work if you want to do those math questions. However, if you turn math questions into programs, then it worked!

<img src="DL_notes/image-20220118134738566.png" alt="image-20220118134738566" style="zoom: 33%;" />

Then, some example output of this using Codex would be:

![image-20220118134913140](DL_notes/image-20220118134913140.png)

notice that questions will need to be able to **rephrased/transformed** so that it is clearly a **programming task**, before putting into learning models.

- this also means a full automation would be difficult

## DL Timeline

![image-20220118135630742](DL_notes/image-20220118135630742.png)

## Supervised Deep Learning Example

Consider the following data, and we want to use a linear model to separate the data

<img src="DL_notes/image-20220118135900287.png" alt="image-20220118135900287" style="zoom:50%;" />

notice that by default, a linear transformer does not work. Hence we need to consider $x_1x_2$ as a feature

- the idea is that we may want to consider ==feature extraction/processing== before putting them into the network

However, what if we are given the following data:

![image-20220118140203431](DL_notes/image-20220118140203431.png)

where there doesn't seem to be a clear/easy solution if we stick with a linear classifier even with some single layer feature transformation. As a result, in this case you will have to use a neural network:

![image-20220118140229608](DL_notes/image-20220118140229608.png)

the upshot is that we really need to consider **extracting features and use linear classifiers** before using deep neural network, which is necessary only in some cases like the one above.

## Representations Sharing Weights

Some nowadays popular NN architectures are:

![image-20220118145207752](DL_notes/image-20220118145207752.png)

where notice that one common feature that made them successful is to **share weights $W$ between layers/neurons**:

- CNN share $W$ through space

- RNN share $W$ through time

- GNN share $W$ across neighborhoods

# Forward and Back Propagation

The basics of all the NN is a Neuron/Perceptron

<img src="DL_notes/image-20220120131919835.png" alt="image-20220120131919835" style="zoom:50%;" />

where:

1. input is a single vector
2. the $\Sigma$ represents we are summing the components of $\vec{x}$ ($w_0$ is a scalar representing bias)
3. then the scalar is passed into an activation function $f$, often non-linear

Now, remember that our aim is to ==minimize loss== with a given training label:

<img src="DL_notes/image-20220120132007260.png" alt="image-20220120132007260" style="zoom:50%;" />

where $\mathcal{L}$ is a loss function of our choice:

- this is summed over all the training $m$ samples
- $w_0$ will represent the bias, often/later absorbed into $W$
- our objective is to minimize this $J$

> *Note*
>
> Remember that for any Learning task
>
> <img src="DL_notes/image-20220120203244526.png" alt="image-20220120203244526" style="zoom:80%;" />
>
> Before the step of feature extraction and selection, you may want to pay careful attention on how to do **feature extraction/selection** to improve your model.

## Neural Networks

The simplest component is a Neuron/Perceptron, whose mathematical model was basically finding:
$$
 g(\vec{x}) = w^T\vec{x}+w_0 = 0
$$
and doing the following for classification:
$$
f(x):= 
\begin{cases}
+1 & \text{if } g(x) \ge 0\\
-1 & \text{if } g(x) < 0
\end{cases} \quad = \text{sign}(\vec{w}^T\vec{x}+w_0)
$$
However, since the output is $\text{sign}$ function which is not differentiable, in Neural Network we will use $\sigma$ sigmoid instead. This also means that, instead of using Perceptron Algorithm to learn, we can use **backpropagation** (basically a taking derivatives using chain rule backwards).

That said, a ==Neural Network== basically involves connecting a number of **neurons**:

<img src="DL_notes/image-20220120205520527.png" alt="image-20220120205520527" style="zoom: 33%;" />

where since at $\Sigma$ we are just doing $\vec{w}^T \vec{x}$, which is a linear combination, we used the $\Sigma$ symbol.

Then, combining those neurons in a **fully connected network**:

<img src="DL_notes/image-20220120132436067.png" alt="image-20220120132436067" style="zoom:67%;" />

Conceptually, since each neuron/layer $l$ basically does two things, from the book:

![image-20220120204213314](DL_notes/image-20220120204213314.png)

Hence each layer can be expressed as:
$$
z^l = W^l \vec{a}^{l-1},\quad \vec{a}^l = f(z^l)
$$
where we assumed a single data point input (i.e. a vector instead of a matrix), but note that:

- $Z^l$ is the linear transformation, $a^l$ is the $\sigma$ applied to each element if activation is $\sigma$.

  Therefore, since we are just doing a bunch of linear transformation (stretchy) and passing to a sigmoid (squash):

  |                            Start                             |                                                              |                                                              |                                                              |                                                              |
  | :----------------------------------------------------------: | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | ![image-20220120204442993](DL_notes/image-20220120204442993.png) | ![image-20220120204456452](DL_notes/image-20220120204456452.png) | ![image-20220120204624982](DL_notes/image-20220120204624982.png) | ![image-20220120205207727](DL_notes/image-20220120205207727.png) | ![image-20220120205247722](DL_notes/image-20220120205247722.png) |

  where:

  - at the start, the red will be labeled as $+1$, and blue $-1$ in a space perpendicular to the graph
  - the last one looks separable.

- This can be easily gerealized when you have $n$ data points, so that:
  $$
  Z^l = (W^{l})^T A^{l-1},\quad A^l = f(Z^l)
  $$
  basically you have now **matrices** as input and output, weights are still the same. To be more precise, a layer then looks like

  <img src="DL_notes/image-20220120210731448.png" alt="image-20220120210731448" style="zoom:50%;" />

  so that you are doing $z^l = (W^{l})^T a^{l-1}+W_0$, if again, we have a single data point input of $m$ dimension. Yet often we would have absorbed the $W_0$ by lifting, so that we have:
  $$
  \vec{a} = \begin{bmatrix}
  a_1\\
  \vdots\\
  a_m\\
  1
  \end{bmatrix} \in \mathbb{R}^{m+1}, \quad W^T \leftarrow [W^T, W_0] \in \mathbb{R}^{n \times (m+1)}
  $$

In general, if there are $n_l$ neurons in the $l$-th layer, then at the $l$-th layer:
$$
A^{l-1} = \text{input} \in \mathbb{R}^{(n_{l-1}+1)) \times m},\quad  (W^l)^T \in \mathbb{R}^{n_l \times (n_{l-1} + 1)}
$$
where $n_0 = m$ is the dimension of the data, and we absorbed in the bias.

- a quick check is to make sure that $Z^l = (W^l)^T A^{l-1}$ works
- a visualization is that data points are now aligned **vertically** in the matrix

### Neuron Layer

To make it easier to digest **mathematically**, we can think of ==each layer as a single operation==. Graphically, we convert:

<img src="DL_notes/image-20220120132436067.png" alt="image-20220120132436067" style="zoom: 50%;" />

To this, simply three "neurons" (excluding the input):

<img src="DL_notes/image-20220120212259363.png" alt="image-20220120212259363" style="zoom: 33%;" />

where notice that:

- $F(x) = f(W^Tx)$ is a shorthand for notating the entire linear and nonlinear operation. This would be *nonlinear*.

- therefore, each layer $l$ basically just does:
  $$
  F^l(A^{l-1}) = A^l
  $$
  outputting $A^l$. Hence we can do a "Markov chain" view of the NN as just doing:
  $$
  F(x) = F^3 (F^2 (F^1(x)))
  $$

- if $f$ are identities, then the output is just a linear transformation since $F$ would be *linear*.

---

*For Example*:

Consider the following NN. For brevity, I only cover the first layer:

![image-20220120212538132](DL_notes/image-20220120212538132.png)

First we do the pre-activation $z^1$ for the input of a single data point of dimension $3$:
$$
z^1 = (W^1)^T a^{0} = \begin{bmatrix}
w_{11} & w_{21} & w_{31}\\
w_{12} & w_{22} & w_{32} \\
\end{bmatrix}\begin{bmatrix}
x_{1}\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
z_{1}^1\\
z_2^1\\
\end{bmatrix}
$$
where bias would be ignored for now (otherwise there will be one more column of $[b_1, b_2^T$ for $(W^1)^T$ and a row of $1$ for $a^0$. Then the activation does:
$$
a^1 =  \begin{bmatrix}
f(z_{1}^1)\\
f(z_{2}^1)
\end{bmatrix}= \begin{bmatrix}
a_1^1\\
a_2^1
\end{bmatrix}
$$
which will be input of the second layer. Eventually:

<img src="DL_notes/image-20220120134137821.png" alt="image-20220120134137821" style="zoom: 50%;" />

the last part does not have an activation since we are doing a regression instead of classification.

### Activation Functions

The activation function always does a mapping from $f: \mathbb{R} \to \mathbb{R}$. Hence they are applied element-wise.

Common examples of activation functions include:

<img src="DL_notes/image-20220120213419561.png" alt="image-20220120213419561" style="zoom:50%;" />

where:

- a difference between Swish and ReLU is that Switch has a defined derivative at $z=0$

- In the case of ReLU, consider an input space of dimension $2$. Now fold it along an axis, returning two separately smooth planes intersecting at a fold. At this point, one of them is flat, and the other is angled at $45$ degrees. Since the input dimension is $2$, we will be folding again at the y-axis.

  - By repeating this to create many folds, we can use the ReLU function to generate an n-fold hyperplane from the smooth 2D input plane.

  Therefore, combined with linear transformation, the folds will be slanted:

  <img src="DL_notes/image-20220120220614045.png" alt="image-20220120220614045" style="zoom:67%;" />

  in the end we just have a sharded space from many folds.
  
- Swish: notice that $\lim_{\beta \to \infty}$ Swish becomes ReLU

Their derivatives are important to know since we will use them when taking derivatives during backpropagation:

<img src="DL_notes/image-20220120132903520.png" alt="image-20220120132903520" style="zoom:50%;" />

where notice that

- Sigmoid: $\sigma'$ depends on $\sigma$ means we would have ==already computed this== in the forward pass. This will save computational effort!
- ReLU: the simplest derivative among all. Saves computational effort.

Last but not least, for multiclass classification, often the **last layer** uses SoftMax, which maps $\mathbb{R}^d \to \mathbb{R}^d$. This is often used **only for the last layer if we are doing multiclass**:
$$
g^L(z^L)_i = \frac{e^{z_i^L}}{\sum_{j=1}^d e^{z_{j}^L}}
$$
where:

- $z_i^L$ basically is the pre-activatoin on the last layer $l$

- this is a generalizatoin of logistic regression because $\sum_i^d g(z)_i = 1$, i.e. elements sum up to 1.

- also pretty easy to implemnet:

  ```python
  lambda z: np.exp(z) / np.sum(np.exp(z))
  ```

yet since they add up to 1, they can also be seen as a generalization of logistic regressoin.

### Loss Functions

Now we have several choice of $f$, our final objective of minimize is:
$$
\frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)
$$
for $\hat{y}^i$ is basically the $F^3 (F^2 (F^1(\vec{x}^i)))$, for example. Therefore, a couple of different choices for $\mathcal{L}$ function:

- Mean Squared Error
  $$
  \mathcal{L}(y^i, \hat{y}^i) = (y^i - \hat{y}^i)^2
  $$

- Other power $p$ error:
  $$
  \mathcal{L}(y^i, \hat{y}^i) = |y^i - \hat{y}^i|^p
  $$
  Graphically:

  <img src="DL_notes/image-20220120214540633.png" alt="image-20220120214540633" style="zoom: 80%;" />

- Logistic Regression Loss (Cross-Entropy Loss)
  $$
  \mathcal{L}(y^i, \hat{y}^i) = -y^i \log(\hat{y}^i) - (1-y^i) \log (1-\hat{y}^i)
  $$
  If this loss is used, the objective $J$ is convex in $W$.

- There is also a Softmax version/multiclass version of the logistic loss. Checkout the book for more info.

> **Note**
>
> In general, the loss function is **not** convex with respect to $W$, therefore solving:
> $$
> \min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)=\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W))
> $$
> does not guarantee a global minimum. We therefore **use gradient descent** to find a **local minimum**.

### Regularization

What happens if we add regularization, such that we consider:
$$
\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W)) + R(W)
$$
In general, this will more or less force values inside $W$ to be small.

Now, recall that $W$ basically does the linear part/composes the pre-activation before putting into $f$:

<img src="DL_notes/image-20220120215157181.png" alt="image-20220120215157181" style="zoom:50%;" />

So we if have $f$ being a function such as sigmoid, it means that pre-activation would be most at the blue part:

<img src="DL_notes/image-20220120215433180.png" alt="image-20220120215433180" style="zoom: 80%;" />

where notice that:

- adding regularization will likely shrink the $W$, which means that $Z^l$ would be small. Hence, $A^l=f^l(Z^l)$  would likely be ==linear== (the green part)! 
- This means our activation being less complex, i.e. $f$ becomes almost a **linear** operation. Intuitively, the more we regularize, the less complicated our model

## Forward Propagation

<img src="DL_notes/image-20220120134756312.png" alt="image-20220120134756312" style="zoom: 80%;" />

where if we use the previous example:

- e.g. $L=3$ since we have 3 layers

- $T$ is the number of iterations we want to perform (e.g. number of epochs if we are doing over the entire batch)

- the ==initialization must be randomized==, so that the updates for new $W$ will not be identical/same (when doing back prop)

  - If we set all the weights to be the same, then all the the neurons in the same layer **performs the same calculation**, there by making the whole deep net useless. If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron.

    E.g.
    $$
    z^1 = (W^1)^T a^{0} = \begin{bmatrix}
    w_{11} & w_{21} & w_{31}\\
    w_{12} & w_{22} & w_{32} \\
    \end{bmatrix}\begin{bmatrix}
    x_{1}\\
    x_2\\
    x_3
    \end{bmatrix} = \begin{bmatrix}
    z_{1}^1\\
    z_2^1\\
    \end{bmatrix}
    $$
    will have $z_1^1 = z_2^1$ and updates within the same layer will be identical.

- we are picking only one sample from the set because we will be doing **stochastic gradient descent** with the backprop algorithm. This is commonly used when datasets are large so we don't want to do the entire dataset per step.

## Back Propagation

The aim of this algorithm is, for each $W^l$ at layer $l$ (assumed bias is absorbed):
$$
W^l := W^l - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}
$$
basically doing a gradient descent to minimize our loss:

- $\alpha$ would be the learning step size, which we can tune as a parameter

Now, to compute the derivative, instead of **doing it in a forward pass** so that we need to do:

1. compute $\partial \mathcal{L}/\partial \mathcal{W^1}$
2. then compute $\partial \mathcal{L}/\partial \mathcal{W^2}$
3. then compute $\partial \mathcal{L}/\partial \mathcal{W^3}$ 

if we have 3 layers. However, it turns out we can do ==achieve all the calculations== if we do it in ==a backward pass==:

1. Notice that:
   $$
   \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T
   $$
   since $Z^l = (W^l)^TA^{l-1}$. notice that $A^{l-1}$ is already computed in the forward pass. Graphically, if $l=2$, we are here:

   ![image-20220120225335839](DL_notes/image-20220120225335839.png)

   note that we basically are doing derivative of a scalar w.r.t. a vector, so ==Jacobians== would be the brute force way:

   ![image-20220120231135345](DL_notes/image-20220120231135345.png)

   yet the point is that we can save much effort using chain rule.

   - if you have a $f:\mathbb{R}^n \to \mathbb{R}^m$, then the Jacobian will be dimension $\mathbb{R}^{m \times n}$. You can image each row doing $[df_i/dx_1,...,df_i/dx_n]$.
   
2. Then, we need:
   $$
   \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
   = \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}
   =\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}
   $$
   since we know $A^l=f(Z^l)$. If we have sigmoid, then we know $\sigma'(x) = \sigma(x)\cdot(1- \sigma(x))$. 

   - Hence each element such as
     $$
     \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l_{11}}} 
     = \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l_{11}}}
     =\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{\sigma(Z^l_{11})}}{\partial \mathcal{Z^l_{11}}}
     =\sigma(Z_{11}^l)\cdot(1-\sigma(Z_{11}^l))\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
     =A_{11}^l\cdot(1-A_{11}^l)\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
     $$
     so then computing for the derivative for the entire matrix, means just doing the above for each element

   - so the ==one thing we actually have to compute== would be:
     $$
     \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
     $$
     though we know $\partial Z^l / \partial A^{l-1}=W^l$, we don't know the loss function. However, it turns out we only needed to compute this for ==one (the last) layer== (see next step)

3. Lastly, the real shortcut in back propagation is that:
   $$
   \frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
   =\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
   =W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}
   $$
   hence, the ==other thing we have to compute is==:
   $$
   \frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}
   $$
   and that from this formula, knowing $\partial L / \partial Z^{l}$ means we can compute $\partial L / \partial A^{l-1}$ of the **previous layer**!

> **In summary**
>
> To compute
> $$
> \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T
> $$
> we needed to know
> $$
> \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
> =\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}
> $$
>
> - notice that this computation is **local**: it only involves stuff from layer $l$
>
> which requires 
> $$
> \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
> $$
> which can be done in an efficient way once we are done with a layer:
> $$
> \frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
> =\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
> =W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}
> $$

Therefore, the back propagation algorithm looks like:

<img src="DL_notes/image-20220120232037290.png" alt="image-20220120232037290" style="zoom: 67%;" />

where notice that:

- we have an "extra" step here because this assumes that the *biases are not absorbed*

- the three main steps are outlined in the summary mentioned above. Due to dependency, they are done in reverse order
- the purple line shows the efficiency, that $\partial L / \partial A^{l}$ can be  computed from previous result with layer $l+1$. The only one what requires computation is the first iteration/last layer.
- this is basically the entire model! i.e. back propagation does the **training/update** of the parameters

---

*For Example*

In the first iteration, if we don't care about losses yet, we need to figure out the second step on $\partial L / \partial Z^{l}$:

<img src="DL_notes/image-20220120232507028.png" alt="image-20220120232507028" style="zoom: 67%;" />

which we are doing element-wise for clarity. This can be easily generalized and placed into a vector:

<img src="DL_notes/image-20220120232742923.png" alt="image-20220120232742923" style="zoom:67%;" />

- then we can compute easily:
  $$
  \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T
  $$

In the second iteration, we notice that we can reuse the results above:

<img src="DL_notes/image-20220120232935679.png" alt="image-20220120232935679" style="zoom:67%;" />

- which then can compute the $$

Lastly:

<img src="DL_notes/image-20220120233035309.png" alt="image-20220120233035309" style="zoom:67%;" />

which again reused the result from its higher up layer.

> **Note**
>
> If we take derivatives in the forward pass, we get only derivative of one variable. If we do it in the backward pass, we do it once and get ==all the derivatives== with little effort
>
> A big advantage of back propagation is also utilizing the fact that we are doing **local** computation (and passing on the result)
>
> <img src="DL_notes/image-20220120140038638.png" alt="image-20220120140038638" style="zoom:50%;" />
>
> which means backpropagation is a special case of **differential programming**, which can be optimized.

## Weight Initialization

Basically a uniform distribution for weights and zeros for bias

<img src="DL_notes/image-20220127141900682.png" style="zoom:50%;" />

Program

```python
for l in range(1, self.num_layers):
    # glorot init
    epa = np.sqrt(2.0 / layer_dimensions[l] + layer_dimensions[l-1])
    self.parameters["W" + str(l)] = np.random.rand(layer_dimensions[l], layer_dimensions[l-1]) * eps
    self.parameters["b" + str(l)] = np.zeros((layer_dimensions[l], 1)) + 0.01
```

## Problems with NN

Recall that our objective is to minimize:

<img src="DL_notes/image-20220120233655595.png" alt="image-20220120233655595" style="zoom: 50%;" />

This implies the following problem:

1. What if $n$ is large? Since our loss sums over all $n$ data points, it would take a long time to compute
   - **Solution**: Stochastic Gradient Descent
2. Computing derivatives/doing gradient descent of large network takes time
   - **Solution**: Backpropagation
3. For each time step/update, the gradient would be *perpendicular* to the previous one, forming a slow zig-zag pattern (slow to converge).
   - **Solution**: Adaptive gradient Descent

## Example Implementation

Consider the implementing the following architecture

<img src="DL_notes/neural_network_practice.png" style="zoom: 23%;" />

Then we would have:

```python
#Implement the forward pass
def forward_propagation(X, weights):
    Z1 = np.dot(X, weights['W1'].T)  + weights['b1']
    H = sigmoid(Z1)

    Z2 = np.dot(H, weights['W2'].T + weights['b2'])
    Y = sigmoid(Z2)    
    # Z1 -> output of the hidden layer before applying activation
    # H -> output of the  hidden layer after applying activation
    # Z2 -> output of the final layer before applying activation
    # Y -> output of the final layer after applying activation
    return Y, Z2, H, Z1
```

And backward:

```python
# Implement the backward pass
# Y_T are the ground truth labels
def back_propagation(X, Y_T, weights):
    N_points = X.shape[0]
    
    # forward propagation
    Y, Z2, H, Z1 = forward_propagation(X, weights)
    L = (1/(2*N_points)) * np.square(np.sum(Y - Y_T))
    
    # back propagation
    dLdY = 1/N_points * (Y - Y_T)
    # dLdZ2 = dLdA2 * dA2dZ2 = dLdA2 * sig(Z2)*[1-sig(1-Z2)] # broadcast multiply
    dLdZ2 = np.multiply(dLdY, (sigmoid(Z2)*(1-sigmoid(Z2)))) 
    # dLW2 = dLdA2 * dA2dZ2 * dZ2dW2 = dLdZ2 * dZ2dW2 = dLdZ2 * A1 # matrix multiply
    dLdW2 = np.dot(H.T, dLdZ2) 

     # dLb2 = dLdA2 * dA2dZ2 * dZ2db2 = dLdZ2 * dZ2db2 = dLdZ2 * 1
    dLdb2 = np.dot(dLdZ2.T, np.ones(N_points))
    
    # dLdA1 = dLdA2 * dA2dZ2 * dZ2dA1 = dLdZ2 * dZ2dA1 = dLdZ2 * W2
    dLdA1 = np.dot(dLdZ2, weights['W2']) 
    # dLdZ1 = dLdA1 * dA1dZ1 = dLdA1 * sig(A1) * [1-sig(A1)] # broadcast multiply
    dLdZ1 = np.multiply(dLdA1, sigmoid(H)*(1-sigmoid(H)))
     # dLW1 = dLdA1 * dA2dZ1 * dZ2dW1 = dLdZ1 * dZ2dW1 = dLdZ1 * A0 # matrix multiply
    dLdW1 = np.dot(X.T, dLdZ1) 

    dLdb1 = np.dot(dLdZ1.T, np.ones(N_points))
    
    gradients = {
        'W1': dLdW1,
        'b1': dLdb1,
        'W2': dLdW2,
        'b2': dLdb2,
    }
    return gradients, L
```

# Optimization

In practice, it is important to have the some understanding of the optimizer we will use (e.g. how to gradient descent), because they perform differently in different scenarios.

The ones we use in practice the ==different algorithms to find minima== can be separated into the following three classes:

- First order methods.
  - Gradient Descent (Stochastic, Mini-Batch, Adaptive)
  - Momentum Related (Adagrad, Adam, Hypergradient Descent)
- Second order methods
  - Newton's Method (*generally faster than gradient descent* because it uses Hessian/is higher order)
  - Quasi Newton's Method (SR1 update, DFP, BFGS)

- Evolution Strategies
  - Cross-Entropy Method (uses cluster of initial points as initial conditions, then descent as a group)
  - Distributed Evolution Strategies, Neural Evolution Strategies
  - Covariance Matrix Adaptation

## Overview

Again, our goal of learning this is to understand, given an optimization problem of finding best $\theta^*$:
$$
\theta^* = \arg\min_\theta J(\theta)
$$
where $J$ would be the total loss we are dealing with.

- an example for $J$ would be
  $$
  J(\theta)  = \left( \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^{(i)}, h(x^{(i)}; \theta)) \right) + \lambda R(\theta)
  $$
  where we included a regularization term $R(\theta)$ here as well.

First, let us recall that the basic gradient descent algorithm generally looks like

![image-20220125132905495](DL_notes/image-20220125132905495.png)

where $\eta$ would be tunable:

- its aim is to obvious find the $\theta^*$, but it might get **stuck at local minima**
  - in that case, we need to add some noise, or use stochastic gradient descent
- for large NN, finding $dJ/dW$ takes effort.
  - this is optimized with back-propagation algorithm
- but obviously this is not the only way to do it, as you shall see soon

> **Convex Optimization**
>
> It would be so little pain if $J$ is convex w.r.t. $W$, so that any **local minimum** is also a **global minimum**.
>
> - but often in NN, $J$ is not a convex function of $W$, so we do have the problem of stopping at local minimas.
>
> For a problem to be a convex optimization (with constraints):
>
> ![image-20211014155124853](DL_notes/image-20211014155124853.png)
>
> this problem is a convex optimization ==IFF both holds==:
>
> - the **feasible** region of output (due to the constraint) is a **convex set**
> - the **objective** function $f(\vec{x})$ is a **convex function**
>

But in reality, this is what are we are facing:

<img src="DL_notes/image-20220125133004782.png" alt="image-20220125133004782" style="zoom: 50%;" />

where recall that:

- Linear programs: objective function is linear (affine), and constraints are also linear (affine)
  - so that the feasible region is a convex set (because the feasible region is always a polygon = convex set)
- Quadratic program: objective function is quadratic, and constraints are linear (affine)
  - if constraints are quadratic, then the feasible region might not be a convex set.
- Conic Program: where constraints are a conic shaped region
- Other common solvers include: `CVX`, `SeDuMi`, `C-SALSA`,

---

*For Example*:

<img src="DL_notes/image-20220125133237058.png" alt="image-20220125133237058" style="zoom:50%;" />

where:

- LHS shows starting with different initial points yields different result, i.e. some ended up at local minimas
- RHS shows starting from different initial points yields the same result, i.e. global minimum.

## Derivative and Gradient

Most of the cases we will be dealing with $f(\vec{x})$ where $\vec{x}$ is multi-dimensional. For instance $L(W)$ with loss being dependent on weights. Then, an obvious usage of this would be in gradient descent:
$$
w_{t+1} = w_t - \alpha_t \nabla f(w_t)
$$
for us using $\alpha_t$ because it can be changing (e.g. in adaptive methods)

> *Recall*
> $$
> \nabla f(\vec{x}) = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} , ..., \frac{\partial f}{\partial x_n}  )
> $$
> being a $n$ dimensional vector:
>
> - imagine graphing $f(\vec{x})$ in a $\mathbb{R}^{n+1}$ since $\vec{x}\in \mathbb{R}^n$
> - then $\nabla f$ points at direction of steepest ascent

Another useful quantity would be the second derivative:

> **Hessian**
>
> Again, for a scalar function with vector input:
>
> <img src="DL_notes/image-20220125133408736.png" alt="image-20220125133408736" style="zoom: 33%;" />
>
> which is useful because:
>
> - $\vec{x}^*$ is a local minimum if $H\equiv \nabla^2f$ is **positive semi-definite**
>   - in the case of $x\in \mathbb{R}$, we know that $f''(x) \ge 0$ means minima. In the case of vector input space, you have $n$-directions to look at. If each direction satisfies $Hx = ax$ for $a \ge 0$, then obviously it is "concave up", and that $Hx = ax$ for all $x$ means $H$ contains only non-negative eigenvalues -> positive semidefinite

Sometimes, we may want to use **numerical calculations of derivatives** to make sure our formula put in practice is correct:
$$
f'(x) \approx \frac{f(x+\epsilon) - f(x - \epsilon)}{2 \epsilon}
$$
for small $\epsilon$.

- notice we are all using $x$ as the input variable. It might be useful in context if we think of $x \to \vec{w}$ being the weights that we need to optimize on.

- an example program would be

  ![image-20220125133621511](DL_notes/image-20220125133621511.png)



> **Note**
>
> To compute gradient using:
>
> - analytic equation: exact, fast
>   - e.g. for NN, we derive the derivatives and used the formula
> - numerical equation: slow, contains error
>   - useful for debugging, .e.g if backprop is implemented correctly

## First Order Methods

Now, we talk about first order methods: **using only first order derivative** $g_t \equiv \nabla f(x_t)$ for weight (remember we generalized weights $w \to x$ any input) at iteration $t$.

### Gradient Descent

The easiest and direct use of $\nabla f(x_t)$:

<img src="DL_notes/image-20220125133718470.png" alt="image-20220125133718470" style="zoom: 33%;" />

where note that:

- you could add an early-stopping criteria at the end

- the tunable learning rate $\alpha_t$ is critical:

  |                           Too Big                            |                          Too Small                           | Just Right                                                   |
  | :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
  | ![image-20220125133933698](DL_notes/image-20220125133933698.png) | ![image-20220125133949095](DL_notes/image-20220125133949095.png) | <img src="DL_notes/image-20220125134009975.png" alt="image-20220125134009975"  /> |
  
  but as you might have guessed, step size $\alpha_t$ could be updated automatically in some other methods
  
- However, at saddle points, it may cause the **update** be too small. Hence often a noise term will be added
  $$
  x_{t+1} = x_t - \alpha_tg_t + \epsilon_t
  $$
  for $\epsilon_t \sim N(0, \sigma)$ hoping that it goes out of local minima/saddle points. (related: [Vanishing/Exploding Gradient](#Vanishing/Exploding Gradient))

---

*For Example*

Consider logistic loss:
$$
J(x;\theta) = -\frac{1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]
$$
Then, compute the gradient:
$$
\frac{\partial J(\theta)}{\partial \theta_j}  = 
\frac{\partial}{\partial \theta_j} \,\frac{-1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]
$$
Carefully computing the derivative yields:
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m\left[h_\theta\left(x^{(i)}\right)-y^{(i)}\right]\,x_j^{(i)}
$$
Then you can update $\theta_j$ using this.

---

**Problems with Gradient Descent**

1. The above takes an entire training set for computing the loss. Takes time.

   - use mini-batch or stochastic

2. Computing derivative w.r.t weights $\theta$ takes effort if $\theta$ is high dimensional

   - use backpropagation

3. What step-size should we use? We may overshoot if too large of a stepsize.

   - adaptive learning rate

4. Gradient descent typically spend too much time in regions that is relatively flat as gradient is small

   - e.g.

     |                        Normal Region                         |                         Flat Region                          |
     | :----------------------------------------------------------: | :----------------------------------------------------------: |
     | <img src="DL_notes/image-20220125134255982.png" alt="image-20220125134255982" style="zoom:50%;" /> | <img src="DL_notes/image-20220125134221684.png" alt="image-20220125134221684" style="zoom:50%;" /> |

   - Use [Adaptive Gradient Descent](#Adaptive Gradient Descent)

#### Adaptive Step Size

There are certain options we can choose from:

- **decrease learning rate as training progresses** (learn less in the future -> prevent overfitting)

  - this can be done using either a decay factor that gets smaller over time:
    $$
    \alpha_t = \alpha_0 \frac{1}{1+t\beta}
    $$
    for $\beta$ being small

  - simply exponential decay:
    $$
    \alpha_t = \alpha_0 \gamma^t
    $$
    for some $\gamma < 1$ but close to $1$, or
    $$
    \alpha_t = \alpha_0 \exp(-\beta t)
    $$
    
    
    

- **line searches**: given some $\min_x f(x)$, and suppose we are currently at $x_t$ being our **current best guess**. We know the current gradient is $g_t = \nabla f(x)|_{x_t} = \nabla f(x_t)$. We consider some step size $\alpha_t$ we **might take**:
  $$
  \phi(\alpha_t) \equiv f(x_t + \alpha_t g_t)
  $$
  and we want to **approximately minimize $\phi(\alpha_t)$** to output a $\alpha_t$ to use. Basically we are ==sliding along the tangent line of the current point and see how far we should slide==

  - **backtrack line search**

    ![image-20220125222301156](DL_notes/image-20220125222301156.png)

    where $t^*$ is our desired $\alpha_t$

    Graphically:

    <img src="DL_notes/image-20220125225435921.png" alt="image-20220125225435921" style="zoom: 67%;" />

  - **exact line search**

    Solve the following exactly:
    $$
    \min_{\alpha_t} f(x_t + \alpha_t \hat{g}_t)
    $$
    where $\hat{g}_t = g_t  /|g_t|$. So this means we want:
    $$
    \nabla f(x_t + \alpha_t \hat{g}_t) \cdot \hat{g}_t = 0
    $$
    and we need to solve this for $\alpha_t$

    - one property of this result is that, since we know:
      $$
      \quad \hat{g}_{t+1} = \nabla f(x_t + \alpha_t \hat{g}_t)
      $$
      So we see that
      $$
      \hat{g}_{t+1} \cdot \hat{g}_t = 0
      $$
      meaning **consecutive runs gives perpendicular gradient direction**. This makes sense since we are taking the *optimal step size*, i.e. we have walked the farthest along that direction.

    - Finding $\alpha_t$ is computationally expensive as we need to solve for it, so it is rarely used
  
  - **adaptive line search**: skipped

#### Mini-Batch Gradient Descent

A common technique within gradient descent is to split your dataset into $n$ sets of size $k$, and train each set as **one step for updating the gradient**.

- so that we don't spend time computing the loss function on the entire data
- if you want, you can also parallelize this computation

This is useful because, for a sample of size $k$, the **sample mean** follows the central limit theorem:
$$
\hat{\mu} \sim N(\mu, \sigma^2/n)
$$
which can be easily seen because:

- $\text{Var}[X_i] = \sigma^2$, and using linearity:
  $$
  \text{Var}\left[\frac{1}{n}\sum X_i\right] = \frac{1}{n^2} \text{Var}\left[\sum X_i\right] = \frac{1}{n^2} \sum\text{Var}\left[ X_i\right] = \sigma^2 / n
  $$

- This is good because **standard deviation of $\hat{\mu} \propto 1/\sqrt{n}$ **

  So if using 100 samples vs 10000 samples means:

  - faster computation for factor of 100
  - but only more error of factor of 10

#### Stochastic Gradient Descent

Basically equivalent of Mini-batch of size $1$

- i.e. each update involves **taking 1 random sample**

<img src="DL_notes/image-20220125225520623.png" alt="image-20220125225520623" style="zoom: 67%;" />

Some common nomenclatures:

- **Epoch**: a pass through the entire data

Some **properties**:

- unbiased estimators of the true gradient
- early steps often converge fast towards the minimum

- very noisy -> but increases the chance of getting a global minima
  - e.g. at saddle points, it may cause the step be too small. But this is **already noisy**, so no problem.


### Adaptive Gradient Descent

Either we use normal gradient descent, or gradient descent with optimized steps, we faced the problem of taking too long to converge in **flat regions**.

An overview would be that it uses **gradients from previous steps** to compute current gradient.

- want to achieve faster convergence by move faster in dimension with low curvature, and slower in dimension with oscillations
- the more official documentation:  **AdaGrad** for short, is an **extension of the gradient** descent optimization algorithm that allows the step size in each dimension used by the optimization algorithm to be automatically adapted ==based on the gradients seen for the variable==

- however, some critics of this would say that it yields different result with gradient descent

Examples with adaptive gradients include:

- Momentum
- AdaGrad
- Adam

#### Momentum

The basic idea is that the momentum vector **accumulates gradients from previous iterations** for computing the current gradient.

**Arithmetically weighted moving average**
$$
a_t = \frac{na_t +  (n-1)a_{t-1}) + ... + a_{t-n+1}}{n+(n-1)+ ... + 1}
$$
for basically imagining $a_t \to g_t$ is the gradient

- $n$ is the **weight** which we can specify
- basically this is in a ==weighted moving average== the weights decrease ==arithmetically==, normalized by the sum of weights

In the end, we see **accumulation of gradients** because:

<img src="DL_notes/image-20220125232040667.png" alt="image-20220125232040667" style="zoom: 80%;" />

where notice that:

- $s_t = \sum_{i=t-n+1}^ta_i$ is the accumulation of past gradients, since $a_t \to g_t$

---

Alternatively, there is also an expoentially weighted version

**Exponentially Weighted Moving Average**

<img src="DL_notes/image-20220125232352096.png" alt="image-20220125232352096" style="zoom: 80%;" />

where here:

- again basically $a_t \to g_t$

- the parameter is actually $(1-\alpha) = \beta$ for convenience, and we want $\beta \in [0,1)$

- in an algorithm:

  <img src="DL_notes/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" />

  where basically $x$ would be our weights.

> **Note**
>
> A first problem with momentum is that the step sizes may not decrease once we have reached close to the minimum that may cause oscillations, which can be remedied by using Nesterov momentum (Dozat 2016) that replaces the gradient with the gradient after computing momentum (Dozat 2016):
>
> <img src="DL_notes/image-20220125232704982.png" alt="image-20220125232704982" style="zoom:80%;" />

Graphically

![image-20220125233001622](DL_notes/image-20220125233001622.png)

#### AdaGrad

This deals with the case that we **didn't talk about what to do with $\alpha_t$**:

<img src="DL_notes/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" />

So one variation, AdaGrad, ==adapts the learning rate== to the parameters, i.e. $\alpha_t$ is different for **each $\theta_i$**/parameter:

- performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features
- larger updates (i.e. high learning rates) for parameters associated with infrequent features

For this reason, it is well-suited for dealing with sparse data, and suitable for SGD.

Instead of using $\alpha_t$ for all parameters at current time, use
$$
\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha_t}{\sqrt{s_{t,i} + \epsilon}} g_{t,i}
$$
and that $s_{t,i}$ is a weighted sum of gradients of $\theta_i$ up to time $t$:
$$
s_{t,i} = \beta s_{t-1,i} + g_{t,i}^2
$$
for $g_{t,i}$ is the gradient for the $\theta_i$.

**Problem**

This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is **no longer able to acquire additional knowledge**:
$$
\lim_{s_{t,i} \to \infty} \frac{1}{\sqrt{s_{t,i} + \epsilon}} = 0
$$
This is then solved by:

- **Adadelta**

  An exponential decaying average of square updates without a learning rate, replacing

  <img src="DL_notes/image-20220125234218229.png" alt="image-20220125234218229" style="zoom:80%;" />

- **RMSProp**

  Adagrad using a weighted moving average, replacing:

  <img src="DL_notes/image-20220125234149326.png" alt="image-20220125234149326" style="zoom: 80%;" />

#### Adam

Adaptive moment estimation, or Adam (Kingma & Ba 2014), combines the best of both momentum updates and Adagrad-based methods as shown in Algorithm 6.

<img src="DL_notes/image-20220125234407385.png" alt="image-20220125234407385" style="zoom:67%;" />

where basically:

- uses momentum in the red part
- uses AdaGrad like adaptive learning rate on the yellow part
- since it combined two models, we have **two parameters to specify**. Typically $\beta_1 = 0.9, \beta_2 = 0.99$

Several improvements upon Adam include:

- **NAdam** (Dozat 2016) is Adam with Nesterov momentum

- **Yogi** (Zaheer, Reddi, Sachan, Kale & Kumar 2018) is Adam with an improvement to the second momentum term which is re-written as:

  <img src="DL_notes/image-20220125234659018.png" alt="image-20220125234659018" style="zoom:80%;" />

- **AMSGrad** (Reddi, Kale & Kumar 2018) is Adam with the following improvement

  <img src="DL_notes/image-20220125234724476.png" alt="image-20220125234724476" style="zoom:80%;" />

### Hyper-gradient Descent

Hypergradient descent (Baydin, Cornish, Rubio, Schmidt & Wood 2018) performs ==gradient descent on the learning rate== within gradient descent.

- may be applied to any adaptive stochastic gradient descent method

The basic idea is to consider $\partial f(x_t)/ \partial \alpha$, for $x \to w$
$$
\frac{\partial f(w_t)}{\partial \alpha} = \frac{\partial f(w_t)}{\partial w_t} \frac{\partial w_t}{\partial \alpha}
$$
we know that $w_t =  w_{t-1} - \alpha g_{t-1}$:
$$
\frac{\partial f(w_t)}{\partial \alpha} = g_t \cdot \frac{\partial }{\partial \alpha} ( w_{t-1} - \alpha g_{t-1})
$$
where:

- The schedule may lower the learning rate when the network gets stuck in a local minimum, and increase the learning rate when the network is progressing well.

Algorithm:

<img src="DL_notes/image-20220125234826019.png" alt="image-20220125234826019" style="zoom:80%;" />

### Vanishing/Exploding Gradient

These problem is encountered when training artificial **neural networks** with ==gradient-based learning methods== and ==backpropagation==. In such methods:

- **vanishing gradient:** during each iteration of training each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight. 
  $$
  W^l := W^l - \alpha \frac{\partial L}{\partial W^l}
  $$
  The problem is that in some cases, the **gradient will be vanishingly small**, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.

- When **activation functions** are used whose derivatives can take on larger values, one risks encountering the related **exploding gradient problem**

One example of the problem cause for vanishing gradient

- traditional **activation** functions such as the hyperbolic tangent function have gradients in the range $(0,1]$, is very small
- Since **backpropagation** computes gradients by the chain rule. This has the effect of ==multiplying $n$ of these small numbers== to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially

## Second Order Methods

First order methods are easier to implement and understand, but they are **less efficient than second order methods**.

- Second order methods use the first and ==second derivatives== of a univariate function or the gradient and Hessian of a multivariate
  function to compute the step direction
- Second order methods approximate the objective function using a **quadratic** which results in faster convergence
  - imagine basically 2nd order methods -> parabola -> go down a bowl with a bowl (2nd order); as compared to with a ruler (1st order method)
- but a problem that they need to overcome is how to deal with computing/storing **Hessian** matrix, which could be large

### Newton's Method

Basically we know that we can find the **root of an equation** using newton's method:

<img src="DL_notes/image-20220125235632561.png" alt="image-20220125235632561" style="zoom: 67%;" />

so we basically guessed $x_{t+1}$ to be the root by **fitting a line**:
$$
x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}
$$
which basically does:

- the number of steps to move being $\Delta x = f(x_t)/{f'(x_t)}$

Then, since our goal is to solve (in 1-D case):
$$
\min f(x) \to f'(x) = 0
$$
So basically we consider finding root for $f'(x)$:
$$
x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}
$$
This results in 

<img src="DL_notes/image-20220125235919309.png" alt="image-20220125235919309" style="zoom: 67%;" />

where the blue line is the "imagined function" using Newton's Method

- notice it is a **quadratic**
- therefore, it goes down the "bowl" **faster than first order methods** as mentioned before

---

The same formula can be derived using Taylor's methods as well

<img src="DL_notes/image-20220126000201474.png" alt="image-20220126000201474" style="zoom: 67%;" />

However, this is **useful** because it guides on how to deal with ==vector input functions $f(\vec{x})$== which we need to deal with:

<img src="DL_notes/image-20220126000337166.png" alt="image-20220126000337166" style="zoom: 67%;" />

where the last step is basically our new update rule.

- note that this $H^{-1}$ basically **takes place of the $\alpha_t$** we had in first order methods

- therefore the algorithm is:

  <img src="DL_notes/image-20220126000628969.png" alt="image-20220126000628969" style="zoom:67%;" />

However, some problem resides:

<img src="DL_notes/image-20220126000538635.png" alt="image-20220126000538635" style="zoom:80%;" />

notice that computing and inverting Hessian takes lots of computation, and storing Hessian takes space!

### Quasi-Newton Methods

Quasi-Newton methods, which provide an iterative **approximation** to the inverse Hessian $H^{-1}$, so that it may:

- avoid computing the second derivatives
- avoid inverting the Hessian
- may also avoid storing the Hessian matrix.

The idea is to start thinking exactly what we need to approximate. Our goal is anyway the **iterative update**:
$$
x^+ = x - B^{-1}\nabla f(x)
$$
so our goal is to ==approximate $B^{-1} \approx \nabla^2 f$==. The task is therefore find some **conditions to calculate $B$**

By definition of second derivative, we know that:
$$
\nabla f(x^k+s^k) - \nabla f(x^k) \approx B^k s^k
$$
where:

- $s^k$ is the **step size** at iteration $k$
- $B^k$ is our approximation of Hessian/second derivative at step $k$

Now, since it will be an approximation, we want to impose some constraints to make the approximation good:

1. Second equation **for next $B$** should hold eaxctly:
   $$
   \nabla f(x^{k+1}) - \nabla f(x^k) =  B^{k+1}s^k
   $$
   where $x^{k+1} = x^k+s^k$. This will be then represented as:
   $$
   B^{k+1}s^k = y^k
   $$
   for $\nabla f(x^{k+1}) - \nabla f(x^k)  \equiv y^k$, or even more simply:
   $$
   B^{+}s = y
   $$

2. We also want the following **desirable properties**

   - $B^+$ is symmetric, as Hessians are symmetric
   - $B^+$ should be close to $B$, which is the previous approximation
   - $B,B^+$ being positive definite

Now, we explore some **approximations for $B^+$** that attempts to satisfy the above constraint.

#### SR1 Update

This is the simpliest update procedure, such that $B^+$ can be close to $B$, and it will be symmetric:
$$
B^+ = B + a u u^T
$$
for some $a,u$ we will solve soon. Notice that if we let this be our **update rule for $B$** (first iteration just initialize $B=I$), and we have the ==enforcement that secant equation should hold==:
$$
y=B^+s = Bs + a u u^Ts=Bs + (au^Ts)u
$$
for $s, u, y$ all being vectors. Notice that this means:
$$
y - Bs = (au^Ts)u
$$
where:

- both sides of the equation are **vectors**! This means that $u$ is a scalar multiple of $y-Bs$.

So we can ==solve for $u,a$==, and obtain the solution and **plug back into our update rule for $B^+$**:
$$
B^+ = B + \frac{(y-Bs)(y-Bs)^T}{(y-Bs)^Ts}
$$
where at iteration $k$, we already know $B\equiv B^k, s \equiv s^k$ and $y$, so we can compute $B^+$ at iteration $k$.

---

*Just to be clear*, using the above formula our **descent algorithm** would be:

**At iteration $k$**

1. compute $(B^{k})^{-1} \nabla f(x^k)$
2. do the descent $x^{k+1} = x^k - \alpha_k (B^{k})^{-1} \nabla f(x^k)$ for some tunable parameter $\alpha_k$
3. prepare $B^{k+1}$ using the above formula.

---

Now, while this technically **computes the approximation**, we can make the algorithm even better by **directly computing $H = B^{-1}$** and its updates using the above formula for $B^+$.

Using the following theorem:

![image-20220130161458549](DL_notes/image-20220130161458549.png)

We can show that $(B^+)^{-1}$ **can be computed directly**
$$
(B^+)^{-1} = H^+ = H + \frac{(s-Hy)(s-Hy)^T}{(s-Hy)^Ty}
$$
then we just use $H$ and $H^+$ all the time instead of $B,B^+$ in the above algorithm.

#### Other Approximations

The **David-Fletcher-Powell (DFP)** correction is defined by
$$
H^+ = H+ \frac{ss^T}{y^Ts} - \frac{(Hy)(Hy)^T}{y^T(Hy)}
$$
The **Broyden-Fletcher-Goldfarb-Shannon** (BFGS) is defined by:
$$
H^+ = H+ \frac{2(Hy)s^T}{y^T(Hy)} - \left( 1 + \frac{y^T s^T}{y^T(Hy)} \right)\frac{(Hy)(Hy)^T}{y^T(Hy)}
$$
In summary, they all attempt to approximate the real Hessian, which is expensive in computation.

> **Note**
>
> These methods are similar to each other in that they all begin by initializing the inverse Hessian to the identity matrix and then iteratively update the inverse Hessian. These three update rules differ from each other in that their **convergence properties improve upon one another**.

## Evolution Strategies

In contrast to gradient descent methods which advance a single point towards a local minimum, evolution strategies update
a probability distribution, from which multiple points are sampled, lending itself to a highly efficient distributed computation

> **Useful resource**
>
> - https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#simple-gaussian-evolution-strategies

Intuition: Instead of updating a single initial point and go downhill, use **a distribution of points** to go downhill

![image-20220125140715802](DL_notes/image-20220125140715802.png)

### Simple Gaussian Evolution Strategies

at each iteration, we sample from distribution and update that distribution

![image-20220126002014142](DL_notes/image-20220126002014142.png)

### Covariance Matrix Adaptation

<img src="https://lilianweng.github.io/lil-log/assets/images/CMA-ES-algorithm.png" alt="CMA-ES Algorithm" style="zoom: 33%;" />

Example:

<img src="DL_notes/image-20220126002131825.png" alt="image-20220126002131825" style="zoom: 67%;" />

### Natural Evolution Strategies

<img src="https://lilianweng.github.io/lil-log/assets/images/NES-algorithm.png" alt="NES" style="zoom: 33%;" />

# Regularization

Regularization is a technique that helps prevent over-fitting by **penalizing the complexity** of the network. Often, we want our model to achieve **low bias and low variance** for ==test set==:

<img src="DL_notes/image-20220127131208903.png" alt="image-20220127131208903" style="zoom: 50%;" />

where

- our final aim is to have the model generalize to **unseen data**.

> Why **overfitting** happens? In general it is because your ==training dataset is not representative of all the trends in the population==, so that you could fit too much to the training data and **miss the real "trends"** in the population.
>
> - hence, it cannot generalize to test sets well

---

*Recall that*

Bias and variance are basically:

<img src="DL_notes/image-20220127164739968.png" alt="image-20220127164739968" style="zoom:50%;" />

and intuitively:

- **bias**: error introduced by approximating a complicated true model by a simpler model
- **variance**: amount by which our approximation/model would change for different training sets

e.g. an unbiased estimator would have:
$$
\mathbb{E}_{\vec{x}\sim \mathcal{D}}[\hat{\theta}(\vec{x})] = \lang \hat{\theta}(\vec{x}) \rang = \theta
$$
which is different from consistency:
$$
\lim_{n \to \infty} \hat{\theta}_n(\vec{x}) = \theta
$$
In reality, NN does better than traditional ML models such as SVM by being more complicated:

<img src="DL_notes/image-20220127131541371.png" alt="image-20220127131541371" style="zoom: 67%;" />

---

Main **regularization techniques** here include:

1. add a penalty term of the weights $W$ directly to the loss function
2. use dropouts, which randomly "disables" some neuron during training
3. augment the data

## Generalization

Training data is a sample from a population and we would like our neural network model to ==generalize well to unseen test data== drawn from the same population.

Specifically, the **definition of generalization error** would be the difference between the **empirical loss** and **expected loss**

<img src="DL_notes/image-20220127171220853.png" alt="image-20220127171220853" style="zoom: 67%;" />

where in Machine Learning, you would have seen something like this:
$$
\mathrm{err}(f) := \mathbb{P}_{(x,y)\sim \mathcal{D}}[f(x) \neq y]
$$
here:

- $f$ is a given model to test, and $ \text{err}(f)$ is the generalization error.

- so it is technically equivalent to the red highlighted box only. Yet the whole expression resembles the PAC learning criterion:
  $$
  \mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon
  $$
  where $f^*$ is the optimal predictor in the class $\mathcal{F}$, such that $f^* =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$.

  - however, notice that it is **not**, because here we are computing $ \text{err}$ which is "generalization error" for both, instead of computing sample error.

The difference between the two is **important**:

- suppose $G(f(X,W))  = 0$ for our model $f$. Then it means our empirical loss is as good as the expected loss. This only implies that our model has done the "best it could".
- but suppose $\mathrm{err}(f)=0$, this means that our model is performing perfectly on the population, which I think is a stronger statement than the above.

> The key idea is that, if **generalization error is low**, then our model is not overfitting (doing the same performance for both train and test dataset)
>
> - though this metric is ==not computable== since we don't have the population, there are various ways to "estimate" it, like doing Cross Validation with many folds. ([Cross Validation](#Cross Validation))
>
> However, it is important to remember that **generalization error** depends on ==both variance and bias== (and noise) of the model/dataset:
> $$
> \mathbb{E}[y-\hat{f}(x)]^2 =  \text{Var}[\hat{f}] + ( \text{Bias}(\hat{f}))^2 + \text{Var}[\epsilon]
> $$
> where $\text{Var}[\epsilon]$ is the variance of the **noise of the data**, i.e. $y = W^Tx + \epsilon$ if you think about regression.
>
> - therefore, this is why we want to ==reduce bias and variance==!

In practice, we see things like this:

<img src="DL_notes/image-20220127131908115.png" alt="image-20220127131908115" style="zoom:67%;" />

where usually:

- Adding more training data $(X, Y)$ increases the generalization accuracy until a limit, i.e. the best our model can do anyway $\neq$ the optimal bayes
- Unless we have sufficient data, a very complex neural network may **fit the training data very well at the expense of a poor fit to the test data**, resulting in a large gap between the training error and test error, which is over-fitting, as shown above

---

However, in Deep Learning, recent results have shown:

<img src="DL_notes/image-20220127132009196.png" alt="image-20220127132009196" style="zoom: 50%;" />

where:

- for DL it seems that we are double descending, so we may want to "overfit"

### Cross Validation

> **Cross validation** allows us to estimate the **mean and variance of the generalization error** using our limited data.
>
> - mean generalization error is the average of the generalization error over all $k$ models and is a good ==indicator for how well a model performs on unseen data==

In short, the idea is simple. We randomly split the data **into $k$ folds**

1. take the $i$-th fold to be testing, and the rest, $k-1$ folds being training data
2. learn a model $f_i$
3. compute generalization error of $f_i$
4. repeat 1-3 for $k$ times, but with a **new $i$**

This is useful because now we can compute the **mean and variance of generalization error** using the $k$ different models we trained.

- i.e. think of the evaluation metric being applied on your *model choice/architecture*, hence the need of mean/variance so it doesn't depend much on "which data is chosen to be training"

> **Note**
>
> We can also use cross validation to select features for building a model. We can build many models with **different subsets of features** and then compute their mean and variance of the generalization error to determine ==which subset performs best==.

## Regularization Methods

Our general goal is to **reduce both bias and variance**

<img src="DL_notes/image-20220127190032359.png" alt="image-20220127190032359" style="zoom:50%;" />

where:

- "better optimization" means using a better optimizer, as covered in the previous chapter
- here, our focus is on the bottom right part: **regularize**

Some main methods we will discuss

1. add a penalty term of the weights $W$ directly to the loss function, usually using the **norm**
2. use **dropouts**, which randomly "disables" some neuron during training
3. **augment** the data

> Different methods have different specific effects technically, though the overall effect is that they reduce overfitting.

### Vector Norms

We define vector norms before discussing regularization using different norms.

For all vectors $x$, $y$ and scalars $\alpha$ all vector norms **must satisfy**

1. $||x|| \ge 0$ and $||x||=0$ iff $x = 0$
2. $||x+y|| \le ||x|| + ||y||$
3. $||\alpha x|| = |\alpha| \,||x||$

> **Note**
>
> Under this definition:
>
> - $L_p$ for $p < 1$ will have non-convex shapes
> - $L_0$ does not count as a norm

The general equation is simply:
$$
||x||_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
$$
Some most common norms include:

![image-20220127191601903](DL_notes/image-20220127191601903.png)

### Regularized Loss Functions

One way to regularize it to use **regularized loss function**:

<img src="DL_notes/image-20220127132745260.png" alt="image-20220127132745260" style="zoom: 33%;" />

where:

- $\lambda$ would be tunable.
  - If $\lambda$ = 0, then no regularization occurs. 
  - If $\lambda$ = 1, then all weights are penalized equally. 
  - A value between $0$ and $1$ gives us a tradeoff between fitting complex models and fitting simple models

- Notice that here $R(w)$ refers to regularizing the **entire weight of the network $W$**.
  - if you want to have different regularization for different layers, you need to do $R_1(W^1)+R_2(W^2)$ in the final loss term.
- Common types of regularization are $L_1$ and $L_2$
  - $L_1$ regularization is a penalty on the sum of absolute weights which **promote sparsity**
  - $L_2$ regularization is a penalty on the sum of the squares of the weights which prefer **feature contribution being distributed evenly**


---

*For Example*

If we are using SGD, and we added a $L_2$ regularization would cause our **gradient update rules to change**:

<img src="DL_notes/image-20220127133353173.png" alt="image-20220127133353173" style="zoom:67%;" />

where:

- the extra term is due to regularization. so if $\lambda \to 0$, we get back to regular SGD
- also confirms that $W_t$ in general are smaller if we have regularization

#### Ridge and Lasso Regression

Recall that in machine learning, the following is **Ridge Regression**

<img src="DL_notes/image-20220127192337546.png" alt="image-20220127192337546" style="zoom: 67%;" />

The solution for the ridge ==regression== can be solved exactly:
$$
\vec{w}_{ridge}=(X^TX + \lambda I)^{-1}X^T \vec{y}
$$
which basically comes from taking the derivative of the objective and setting it to zero, and note that:

- this matrix $X^TX + \lambda I$ is exactly ==invertible== since it is now **positive definite** (because we added some positive number to diagonal)
- since $X^TX + \lambda I$ is invertible, this always result in a ==unique solution==.

> **Note**
>
> - analytic solution doesn't exist for ==DL==, since our prediction is no longer:
>   $$
>   \hat{y} = XI\beta
>   $$
>   which is for simple linear regression.
>
>   But in DL, we have a NN with many nonlinear functions nested like:
>   $$
>   \hat{y} = f_3(W^3 \, f_{2}(W^2\,f_{1}(W^1X)))
>   $$
>   where each layer $f$ are the activation functions for each layer. The solution of this is no longer analytic.

Yet, since this problem can be converted to the **constraint optimization problem**

<img src="DL_notes/image-20220127192522192.png" alt="image-20220127192522192" style="zoom: 67%;" />

using **Lagrange Method**, then, the problem basically looks like:

|                      Objective Function                      |              Contour Projection into $w$ Space               |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220127192856939.png" alt="image-20220127192856939" style="zoom:50%;" /> | <img src="DL_notes/image-20220127192905141.png" alt="image-20220127192905141" style="zoom:50%;" /> |

> *Recall*: Lagrange Penalty Method
>
> Consider the problem of:
>
> ![image-20220127192658890](DL_notes/image-20220127192658890.png)
>
> This problem will be the same as minimizing the augmented function
> $$
> L(\vec{x}, \vec{\lambda}) := f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})
> $$
> and recall that :
>
> - our aim was to minimize $f(\vec{x})$ ==such that $g_i(\vec{x}) \le 0$ is satisfied==
> - $\vec{x}$ is the original variable, called primal variable as well
> - $\lambda_i$ will be some new variable, called **Lagrange/Dual Variables**.

---

Similarly, if we use $L_1$ norm, then we have **Lasso's Regression**

<img src="DL_notes/image-20220127192950886.png" alt="image-20220127192950886" style="zoom:50%;" />

Geographically, we are looking at:

|                    Actual Aim (Sparsity)                     |                    Lasso's Approximation                     |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220127193020129.png" alt="image-20220127193020129" style="zoom:50%;" /> | <img src="DL_notes/image-20220127193026129.png" alt="image-20220127193026129" style="zoom: 67%;" /> |

Sadly,  there is **no closed form solution** even for simple regression in this case.

### Dropout Regularization

The idea is that we **randomly dropout neurons** by setting their activations to be $0$.

- this will reduce cause the training to be less accurate, but makes it more robust in overfitting as those neurons "won't fit all the time" to the data

- this is done in training only. When testing, we don't drop them out

Graphically:

|                Fully connected neural network                |                    Dropout regularization                    |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220127193519465](DL_notes/image-20220127193519465.png) | ![image-20220127193527248](DL_notes/image-20220127193527248.png) |

Since activations are randomly set to zero during training, we basically **implement it by adding a layer before after activation**
$$
a_j^l := a_j^l I_{j}^l
$$
where $I_j^l$ is like a **mask, deciding whether if it will be dropped**

<img src="DL_notes/image-20220127193726263.png" alt="image-20220127193726263" style="zoom: 67%;" />

Intuitively:

- over iterations, some neurons will be dropped -> less overfitting on those neurons. In some other cases, those neurons will need to stand in for others.
- overall, we want to keep the **same magnitudes of "neurons"** for that layer even if we dropped out, hence $1/(1-p_l)$ scale up, so that they ==stand in== for the dropped out neurons

- $p_l$ is a hyper-parameter. In some framework we can set it, in some other like `keras` it is ==automatically tuned==

To implement it in code, we use a mask:

```python
def dropout(self, A, prob):
    # a mask
    M = np.random.rand(A.shape[0], A.shape[1])
    M = (M > prob) * 1.0
    M /= (1 - prob)
    A *= M # applying the mask
    return A, M
```

note that:

- forward propagation: apply and *store the mask*
- backward propagation: *load the mask* and apply derivatives
  - since $a_j^l := a_j^l I_{j}^l$, then backpropagation equation needs to be updated as well

---

*For Examples*

<img src="DL_notes/image-20220127134329966.png" alt="image-20220127134329966" style="zoom:67%;" />

#### Least Square Dropout

Dropout is actually not completely new

<img src="DL_notes/image-20220127194559956.png" alt="image-20220127194559956" style="zoom:67%;" />

where notice that:

- the solution is **exact**

> **Note**
>
> - again, analytic solution doesn't exist for DL, since our prediction is not a simple linear regression but concatenating a bunch of nonlinear operations as well

#### Least Squares with Noise Input

<img src="DL_notes/image-20220127194543146.png" alt="image-20220127194543146" style="zoom:67%;" />

### Data Augmentation

> **Data augmentation** is the process of ==generating new data points by transforming existing ones==.
>
> - For example, if a dataset has a lot of images of cars, data augmentation might generate new images by rotating them or changing their color. Then, it is used to **train a neural network**

Data augmentation may be used to **reduce overfitting**. Overfitting occurs when a model is too closely tailored to the training data and does not generalize well to new data. Data augmentation can be used to generate new training data points that are similar to the existing training data points, but are not identical copies.

- This helps the model **avoid overfitting** and generalize better to new data.

The general idea here is that we augment the training data by replacing each example pair with a **set of pairs**
$$
(x_i, y_i) \to  \{(x_i^{*^b} , y_i)\}_{b=1}^B
$$
by, transformations including

- e.g. rotation, reflection, translation, shearing, crop, color transformation, and added noise.

#### Input Normalization

This is simply to normalize the input **in the beginning**

![image-20220127142128055](DL_notes/image-20220127142128055.png)

Then perform:
$$
\hat{x} = \frac{x- \mu}{\sigma}
$$

#### Batch Normalization

Batch normalization basically **standardizes the inputs** to a layer **for each mini-batch**. You can think of this as doing normalization for each layer, for each batch

- advantage: avoid exploding/vanishing gradients if the inputs are small!
- usually not only normalizing the input, but also for each **layer** over and over again.

So basically, **for input of next layer**:
$$
Z = g(\text{BN}(WA))
$$
where $ \text{BN}$ is doing batch normalization. In essence:

<img src="DL_notes/image-20220127195826143.png" alt="image-20220127195826143" style="zoom: 80%;" />

note that:

- this means you **backpropagation equation**/derivatives needs also to include that term.

## Uncertainty in DNNs

Sometimes we also want to measure the **confidence about the outputs** of a neural network.

- for instance, confidence in our learnt paramotor $\theta$

In theory, we want ask the question: **What is the distribution over weights $\theta$ given the data?** (from which we know the confidence of our current learnt parameter)
$$
p(\theta |x,y) = \frac{p(y|x,\theta) p(\theta)}{p(y|x)}
$$
where $p(\theta)$ would be the **prior**, and $p(\theta|x,y)$ would be the **posterior** since we see the data.

- this is not possible to compute since we don't know them.

In practice, we can roughly compute **confidence of current prediction** by "dropout masks"

<img src="DL_notes/image-20220127135347957.png" alt="image-20220127135347957" style="zoom: 40%;" />

For instance:

![image-20220127135432128](DL_notes/image-20220127135432128.png)

so we are not only **outputting predictions**, but also **outputting confidence of our predictions**

# Convolutional Neural Networks

The major aim of this section is to discuss **models that solves ML problems with images**

A brief overview of what we will discuss:

- CNN is basically a "preprocessing neural network" that replaces Linear part from $W^lA^{l-1}$ to **convolution with kernel** (which is also linear)
- problems with deeper CNN layers causes vanishing gradients, hence models such as Residual NN and DenseNet are introduced

On a high level, we should know that treating images means our input vector would be **large in size**, with $n$ dimension (after flattening) means that, if we use vanilla model, we need $O(n n_{1})$ matrix for the first layer with $n$ neurons!

Then, CNN aims to

-  deal with this storage/computation problem by using **sparse matrix (kernel)**, which are essentially matrices with ==repeated elements==.

  <img src="DL_notes/image-20220201170511507.png" alt="image-20220201170511507" style="zoom:80%;" />

  since the output is size $3$, it means we basically have **3 neurons** essentially having ==the same weight== ($k_1, k_2, k_3$). Therefore, we also call this **sharing weights across space**.

  <img src="DL_notes/image-20220201142122782.png" alt="image-20220201142122782" style="zoom: 50%;" />

  notice that in this case, our number of parameters to learn is $O(3)=O(1)$ is **constant**!

- another problem of CNN is to encode **spatial and local information**, which would be otherwise lost if we **directly flatten it** and pass it onto a normal NN.

  <img src="DL_notes/image-20220201133600948.png" alt="image-20220201133600948" style="zoom: 33%;" />

  where you will see the aim of kernels would be that they **captures features such as edges/texture/objects**, which obviously has spatial relationships in the image.

> **Note**
>
> - The fact that weights learnt in CNN preprocessing part of the architecture are constant can be thought of as the **result of the constraints we placed on those weights**: we need them to be kernels, hence we need *symmetry, sparsity, and the particular output shape* as shown above.
>
> - After using the kernel, CNN architecture then would add a bias and an activation, all of which would assemble the actions taken in **one layer**.
>
>   - essentially the linear $W^l A^{l-1}$ is replaced by convolution
>   - each filter in a layer is of effectively $O(1)$ in size, but we can learn **multiple such filters**. Finally it may look like this
>
>   ![image-20220201171758990](DL_notes/image-20220201171758990.png)
>
>   where notice that:
>
>   - the output after **several such layers** will be **piped to a normal NN**, for instance, for the final specific classification tasks.
>   - subsampling are basically techniques such as **pooling**, which will be covered later. The aim is to **reduce the dimension (width $\times$ height)**. 
>   - To reduce the number of channels, you can use $f$ number of $1\times 1\times c$ filters, which can reduce to $f$ channels.
>     - make sense since doing $1\times 1\times c$ convolution is telling how to sum the pixel on each channel into $1$ single pixel.

## Convolution

Basically it is nothing than doing:

- elementwise multiplication between a patch of a matrix and a filter/kernel
- summing them up

Therefore, convolutions in any dimension can be represented as a **matrix vector multiplication**:
$$
k * x = Kx_{\text{flatten}}
$$
where:

- $K$ is the kernel, and $x_{\text{flatten}}$ is the **flattened** version of the image $x$.
- the exact shape of $K$ would be interesting. Think about how you would realize the above equation.

### One-Dimensional Convolution

The idea is simple, if we are given a **1D vector** and a **1D kernel**:

<img src="DL_notes/image-20220201172613937.png" alt="image-20220201172613937" style="zoom:67%;" />

so essentially it is a **locally weighted sum**. To think about how we **represent this in linear algrebra**, consider that we have a $1 \times 5$ input with size $3$ kernel:

![image-20220201172859269](DL_notes/image-20220201172859269.png)

verify that the above works. 

- notice that the output is of dimension $3$. This **must be the case** because there are only $3$ unique positions to place the size $3$ filter inside the size $5$ input vector.
- this matrix is also called Koeplitz matrix

Therefore, we can reason this as:
$$
k * x = Kx = \sum_{i=1}^3 k_iS_i x
$$
where $S_i$ are the matrices where only "diagonal" entries are ones, otherwise zeros.

Now, one problem is that we noticed the **output size is smaller**, which can be bad in some cases. We can fix this by adding padding to the edges:

![image-20220201173324819](DL_notes/image-20220201173324819.png)

however:

- one problem with **zero padding** is that it introduces **discontinuities at boundaries** 
- another technique is to pad with **reflection**, i.e. replacing the top $0 \to x_1$, and bottom $0 \to x_5$.

---

*For Example*: Stride with size 2

The above all assumed a stride with size 1. We can perform the task with stride 2 by doing

![image-20220201174111406](DL_notes/image-20220201174111406.png)

this could be useful as the **output size is decreased** by a factor of $2$.

---

*For Example*: A Simple Single Conv Layer

A typical layer looks like:

<img src="DL_notes/image-20220209143859413.png" alt="image-20220209143859413" style="zoom: 67%;" />

where notice that:

1. pass the 1D image vector to filter (**replacing the linear part** to $Kx$)
   - optionally you would then also add the **bias** to the output $Kx + b$
   - notice that this linear operation has a **very sparse matrix**, $K$
2. shortened version of the vector then goes through **activation**

This particular setup in the end can detect **any block of lonely $1$** in the input.

> **Note**
>
> The fact that we are applying a kernel **everywhere the same** is so that it preserves the property that images are **translational invariant**.

---

*Other Filters*

Sharpening:

<img src="DL_notes/image-20220201174625087.png" alt="image-20220201174625087" style="zoom:50%;" />

more filters are omitted.

### Multi-Dimensional Convolution

Since our images are usually **2D** if grey scale, so we extend the convolution to a 2D kernel.

- the pattern you will see is easily generalizable to 3D inputs as well.

<img src="DL_notes/image-20220201174800911.png" alt="image-20220201174800911" style="zoom: 67%;" />

Then, if we want to **add paddings**

<img src="DL_notes/image-20220201174906311.png" alt="image-20220201174906311" style="zoom:67%;" />

But more importantly, we can **put this in a matrix vector multiplication as well**:

- flattening 2D matrix to $[x_{11}, x_{12}, ..., x_{nm}]^T$.
- the shape of kernel would be **repeatedly assembling 1D filters**

Consider the following operatoin:

<img src="DL_notes/image-20220201175143906.png" alt="image-20220201175143906" style="zoom:67%;" />

Can be done by:

![image-20220201175349503](DL_notes/image-20220201175349503.png)

which is basically:

- the lowest diagonal is the **1D Toeplitz matrix for the first row of $k$**
- the second lowest diagonal is the **1D Toeplitz matrix for the second row of $k$**
- etc.
- finally, the **output is a vector**, which can be interpreted as a flattened 2D image

> Therefore, convolution with 3D images using 3D kernels, basically is equivalent of matrix-vector multiplication with:
>
> - flattened image to 1D
> - repeatedly assembling **2D Toeplitz matrix** for the "$i$-th place" to form a 2D matrix.

---

*For Example*

To find the lonely one:

<img src="DL_notes/image-20220201135357568.png" alt="image-20220201135357568" style="zoom:33%;" />

---

#### Three Dimensional Convolution

The technique of expanding 2D Toeplitz matrix for 3D convolution basically does the following for convolution:

<img src="DL_notes/image-20220201182111825.png" alt="image-20220201182111825" style="zoom:80%;" />

which basically **outputs a single 2D matrix**.

- makes sense that the **number of channels** in both kernel and input lines up, as in the end we just do a element-wise multiplication and sum up.

However, **another way** would be to do ==two dimensional convolution on each channel==

<img src="DL_notes/image-20220201182357125.png" alt="image-20220201182357125" style="zoom:80%;" />

notice that:

- this means we would have 3 filters (may be the same), each is a **2D matrix/kernel/filter**
- the number of channels for both kernel and their respective input is $1$.
- **outputs 3 channels instead of $1$**, as compared to the previous case

### Properties of Convolution

There are several nice properties of convolution that are handy for **optimizing computation complexity**.

First, the most obvious ones are due to **convolution** are essentially matrix/vector multiplication as they are from linear algebra

- **Commutative**: $f*g = g * f$
- **Associative**: $f*(g*h) = (f*g) * h$
- **Distributive**: $f*(g + h) = f * g + f * h$
- **Differentiation**: $\frac{d}{dx} (f * g) = \frac{df}{dx}* g = f * \frac{dg}{dx}$

#### Separable Kernels

Some kernels would be separable like:

<img src="DL_notes/image-20220201135511859.png" alt="image-20220201135511859" style="zoom: 50%;" />

Then, we can use the **property that**:

<img src="DL_notes/image-20220201175857580.png" alt="image-20220201175857580" style="zoom: 80%;" />

where this is **very useful because**, if the image is size $n \times n$, and separable kernel $k \times k$

- directly convovling needs $O(n^2 k^2)$, since each of the $\approx n^2$ output pixel needs $k^2$ computation.
- if we do it with **two simpler convolutions**, then $O(2n^2k)=O(n^2k)$ which is better

#### Composition

Since we know convolutions is basically matrix-vector multiplication: Repeated convolutions with a small kernel are equivalent to a single convolution with a large kernel

<img src="DL_notes/image-20220201180550990.png" alt="image-20220201180550990" style="zoom:80%;" />

where this is useful because it is more efficient.

## Convolutional Layers

Now, we discuss what happens in convolutional layers in a NN such as the following

![image-20220201171758990](DL_notes/image-20220201171758990.png)

Notice that we know:

<img src="DL_notes/image-20220201182741514.png" alt="image-20220201182741514" style="zoom:50%;" />

- convolution of $n \times n \times 3$ with a **single kernel $k \times k \times 3$** produces $n \times n$ (if we have padding)

- if prepare $4$ different $k \times k \times 3$ kernel and we do this ==separately for $4$ times==, we get $n \times n \times 4$ output

  <img src="DL_notes/image-20220201182937454.png" alt="image-20220201182937454" style="zoom: 67%;" />

Therefore, since in general **a single layer would have many different filters**, if we have $f$ filters, we would produce $n \times n \times f$ as our output size.

- however, before we put this through activation, notice that having $n \times n \times f$ is pretty large. (in practice the performance gain is not too huge, so it doesn't matter if we put it before or after activation)

So, before activation, we would use **pooling techniques** to reduce the dimension.

### Pooling

Pooling is an operation that reduces the dimensionality of the input. Some simple and common ones are

> **Max pooling** takes the maximum over image ==patches==.
>
> - for example over $2 \times 2$ grids of neighboring pixels $m =\max\{x_1, x_2, x_3, x_4\}$, hence **reducing dimensionality in half** in each spatial dimension as shown in Figure 5.18.
>
>   <img src="DL_notes/image-20220201183419024.png" alt="image-20220201183419024" style="zoom:67%;" />
>
> - notice that this **does not reduce the number of channels!**

To reduce the number of channels, it is essentially saying that **how do we want to sum pixels in different channel**? Therefore, it makes sense that we can use a **one-dimensional convolution** to solve this

> **One dimensional convolution** with $f$ filters also allows reducing the number of channels to $f$ as shown in Figure 5.19.
>
> <img src="DL_notes/image-20220201183701494.png" alt="image-20220201183701494" style="zoom:80%;" />
>
> - which makes sense as if $f=1$, essentially you summed over all channels, collapsing all channels to $1$ channel.

### Simple Convolutional Layer

Putting everything above together, a typical convolutional layer involves **three operations**:

- convolution with kernel (linear)
- pooling
- activation (nonlinear)

An example would be:

<img src="DL_notes/image-20220201184433823.png" alt="image-20220201184433823" style="zoom: 80%;" />

where, In this example, the input is a $28 \times 28$ grayscale image and the output is one of ten classes, such as the digits $0-9$

- The first convolutional layer consists of $32$ filters, such as $5 \times 5$ filters, which are applied to the image with padding which yields a $28 \times 28 \times 32$ volume.
- Next, a non-linear function, such as the ReLU, is applied pointwise to each element in the volume
- The first convolution layer of the network shown above is followed by a $2 \times 2$ max pooling operation which reduces dimensionality in half in each spatial dimension, to $14 \times 14 \times 32$.
- then, go from $14 \times 14 \times 32$ to $14 \times 14 \times 64$, you would have **64 filters** of size $5 \times 5 \times 32$, for example

## Architectures

Now we talked about some of the **modern architectures** that builds up on the basic CNN we discussed before.

- in fact, now as **Vision Transformers** are out, processing with images have now been mostly done using that as Transformers itself is quite a generic model

### CNN

A basic example of CNN is shown in this example

<img src="DL_notes/image-20220203202818491.png" alt="image-20220203202818491" style="zoom: 67%;" />

- A deeper network of **eight layers may resemble the cortical visual pathways** in the brain (Cichy, Khosla, Pantazis, Torralba & Oliva 2016). 
- note that we are doing activation **then pooling here**
  - max-pooling and monotonely increasing non-linearities commute. This means that $ \text{MaxPool(Relu(x)) = Relu(MaxPool(x))}$ for any input. 
  - So the result is the same in that case. Technically it is better to first subsample through max-pooling and then apply the non-linearity (if it is costly, such as the sigmoid). **In practice it is often done the other way round** - it doesn't seem to change much in performance.

- the second Convolution Layer comes from applying 64 filters of size $k \times k \times 32$. Usually this can be **abbreviated** to say applying $k \times k$ dimension filters.
- Many early implementations of CNN architectures were **handcrafted for specific image classification tasks**. These include 
  - LeNet (LeCun, Kavukcuoglu & Farabet 2010), 
  - AlexNet (Krizhevsky, Sutskever & Hinton 2012)
  - VGGNet (Simonyan & Zisserman 2014), 
  - GoogLeNet (Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich et al. 2015)
  - Inception (Szegedy, Vanhoucke, Ioe, Shlens & Wojna 2016).

(but now, vision transformers are of big focus due to its **generality**)

### ResNet

The **deep residual neural network** (ResNet) architecture (He, Zhang, Ren & Sun 2016a), (He, Zhang, Ren & Sun 2016b), introduced ==skip connections== between consecutive layers as shown below

|                         Architecture                         |                           Details                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220201190948331.png" alt="image-20220201190948331" style="zoom:80%;" /> | <img src="DL_notes/image-20220201191036841.png" alt="image-20220201191036841" style="zoom:80%;" /> |

The idea is simple, the **pre-activation of a layer** $z^l$ now has a **residual term from previous layer**
$$
z^{l+1} = f(W^l,a^l)+a^l;\quad a^{l+1} = g(z^{l+1})
$$
instead of $z^{l+1} = f(W^l,a^l)$.

- adding those skip connections/residual terms allow training deeper neural networks by **avoiding vanishing gradients**. The ResNet architecture enables training very deep neural networks with hundreds of layers.
- Adding a new layer to a neural network with a skip connection **does not reduce its representation power**. Adding a residual layer results in the network being able to represent all the functions that the network was able to represent before adding the layer plus additional functions, thus **increasing the space of functions**.

For instance, a three layer network composition **originally** would look like
$$
F(x)=f(f(f(x)))
$$
Now it becomes, if each layer has a residual:

![image-20220201191454941](DL_notes/image-20220201191454941.png)

### DenseNet

A DenseNet (Huang, Liu, van der Maaten & Weinberger 2017) layer **concatenates** the input $x$ and output $f(x)$ of each layer to form the next layer $[f(x), x]$.

- because it is concatenating it, the input in the first layer also **directly appears in input to any further layers**
- in the ResNet, input in the first layer **indirectly appears** as they are absorbed in, such as  $f(f(x)+x)$

Therefore, graphically:

<img src="DL_notes/image-20220201191751076.png" alt="image-20220201191751076" style="zoom:80%;" />

And the formula composition for three layers look like
$$
F(x) =  f(f([f(x),x]),[f(x),x]), f([f(x),x]),[f(x),x]
$$

## Understanding CNNs

Consider the model ImageNet, which has the following architecture

<img src="DL_notes/image-20220201192419470.png" alt="image-20220201192419470" style="zoom:80%;" />

When trained on a large dataset, since we also **gradient descent to learn kernel/filters** in CNN, we can look at the **learnt kernels** for different layers.

- In the end the kernel is just a **matrix with some constraints**

- therefore, we can impose those constraints only and let **back propagation** to learn those weights

<img src="DL_notes/image-20220201192533577.png" alt="image-20220201192533577" style="zoom: 67%;" />

which, interestingly, coincides with many of the handcrafted ones we had before

<img src="DL_notes/image-20220201140058415.png" alt="image-20220201140058415" style="zoom:33%;" />

> **Note**
>
> - now we learn filters, but we need to specify the architecture
> - this is now superseded with vision transformer, which learns **both the architecture** and the kernel

However, we are interested in knowing ==what patterns do each layer learn==. How do we do that?

### Input Maximizing Activation

Consider transferring the above to an **optimization problem**: given trained network with weights $W$, ==find input $x$ which maximizes activation==.
$$
\arg\max_x a^l_{i}(W, x)
$$
which we can find by **gradient ascent**.

- e.g. given some kernel, doing gradient ascent gives:

  <img src="DL_notes/image-20220201192921549.png" alt="image-20220201192921549" style="zoom: 67%;" />

  where the first steps are basically initializing with **random noise**

Applying this technique to multiple layers, and we find that

![image-20220201193034483](DL_notes/image-20220201193034483.png)

so basically:

- first layers learn the edges
- then textures
- then objects

Alternatively, you can also use this technique to find out **what patch of images** this kernel is bad at:

![image-20220201193141397](DL_notes/image-20220201193141397.png)

### Transfer Learning

The fact that those CNN learn fundamental concepts such as edges and textures means we can do **transfer learning**

Task 1: learn to recognize animals given many (10M) examples which are not horses

Task 2: learn to recognize horses given a few (100) examples

- Keep layers from task 1, re-train on **last layer**

<img src="DL_notes/image-20220201193339131.png" alt="image-20220201193339131" style="zoom:50%;" />

# Sequence Models

Applications using sequence models include machine translation, protein structure prediction, DNA sequence analysis, and etc. All of which needs some **representation that remembers previous data/state**.

An overview of what will be discussed

- **Sequence models**
- **Recurrent neural networks (RNNs)**: when unrolled, basically modelling Finite State Machine
- **Backpropagation through time**: Updating the shared/same weights across states
- **GRU and LSTM**: fixing vanishing/exploding gradient in RNN
- Word embeddings, beam search, encoder-decoder attention
- Transformers

In general, to reduce computational complexity through this deep network, **weights are shared/same across time**. i.e. shared weights when unrolled.

<img src="DL_notes/image-20220203131557649.png" alt="image-20220203131557649" style="zoom: 50%;" />

Another interesting thing to know is the **timeline** of model development w.r.t. sequence models:

<img src="DL_notes/image-20220203131802058.png" alt="image-20220203131802058" style="zoom:33%;" />

where:

- for simple deep models like RNN, there were problems of vanishing gradients. So LSTM and GRU are essentially variants to solve this problem in RNN.

## Natural Language Models

Representing language requires a natural language model which may be a **probability distribution over strings**

Some basic methods for **treating input text data** include

- Bag of words + Classifier
- Feature Vector + Classifier
- Markov Model

### Bag of Words

> **Bag of words** are text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but **keeping multiplicity**/count.

> **Term Frequency**: In a bag of words we count how prevalent each term $x$ is in a single document $d$ which is the term frequency $TF(x, d)$.
>
> - we assume words are commonly normalized to lowercase, stemmed by removing their suffixes, and common stopwords (such as a, an, the, etc.) are removed.

However, sometimes we also want to get more weight/focus for words that are rare. Therefore, we may want to consider the entropy of the word using **inverse document frequency**:
$$
IDF(x) = 1 + \log( \frac{ \text{total number of documents}}{ \text{number of documents containing $x$} })
$$

> **TF-IDF**: Then we can *boost* the frequency of rare words by considering the product of term frequency and inverse document frequency:
> $$
> TFIDF(x,d) = TF(x,d) \times IDF(x)
> $$

Finally, we use some **classifier models**, e.g. ML or DL models for classification using the mentioned above features.

**Problem**

- however, such a representation does not preserve order information
  $$
  \text{Alice sent a message to Bob} \quad \text{v.s.} \quad \text{Bob sent a message to Alice}
  $$
  would have the same score/vector representation

### Feature Vector

In contrast to a bag of words, using a feature vector to represent a sentence **preserves order information**. However, the problem is that sentences that have the **same meaning** could have a different word order:
$$
\text{Alice sent a message on Sunday} \quad \text{v.s.} \quad \text{On Sunday Alice sent a message}
$$
would need **different feature vectors** even if same information.

- hence, there will be a lot of redundancy

### N-Gram Model

Here we basically model **probability distribution of n-grams**

- A Markov model is a 2-gram or bi-gram model where:
  $$
  P(x_ n | x_1 ,....,x_{n-1}) \approx p(x_n | x_{n-1})
  $$

- but we want **long term dependencies**

  - using large/high-order n-gram model requires **large corpus**
  - hence not effective at capturing long term dependencies

## RNN

> **Recurrent Neural Network** both maintain word order and model long term dependencies by sharing parameters across time.
>
> - also allows for inputs and outputs of different length
> - model both forward and backward sequence dependencies
>   - using bidirectional RNN
> - but generally *difficult to train*: backpropagation causes gradients to explode/vanish
>   - hence need LSTM or GRU
>
> Its behavior basically is:
>
> 1. process the input sequence one word at a time
> 2. attempting to predict the next word from the current word and the previous hidden state $h_{t-1}$. 
>    - RNNs don't have the limited context problem that n-gram models have, since the hidden state can in principle **represent information about all of the preceding words** all the way back to the beginning of the sequence
> 3. output $y_t=f(Vh_t)$ at time $t$ where $V$ will be shared and $f$ is an activation function of your choice.
>    - exactly when it outputs can be tuned/changed in algorithm, which leads to different architectures such as many-to-one (outputting $y$ only at the last time step)

The basic foundation of a RNN is a **finite state machine**

### State Machine

In a state machine, we have:

- $S$: possible states
- $X$: possible inputs
- $f: S \times X \to S$: **transitions**
- $Y$: possible outputs
- $g: S \to Y$: mapping from state to outputs

-  $S_0$ initial state

<img src="DL_notes/image-20220203205139875.png" alt="image-20220203205139875" style="zoom:50%;" />

The key idea is that **a new state** comes from **both the previous state and an input**
$$
s_t = f(s_{t-1},x_t)
$$
This idea will be the ==same in RNN==.

Then, for a **sequence of inputs $x_t$**, the output $y_t$ would be:

<img src="DL_notes/image-20220203205331091.png" alt="image-20220203205331091" style="zoom:67%;" />

Essentially depends on all previous state/input.

- **Recurrent neural networks are state machines** with specific definitions of transition function $f$ and mapping $g$, in which the states, inputs, and outputs are vectors.

### Recurrent Neural Network

Given some sequence of data: $x_1, ..., x_n$, we consider some **hidden state $h_1, ..., h_n$** that will be used to form **output** $y_1, ..., y_t$. This is done by **sharing weights $U,W,V$** across time:

<img src="DL_notes/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" />

where essentially we see that RNN is modelling **state transitions and outputting stuff**, which is like a FSM

- hidden state $h_t$ at time $t$ is computed by:
  $$
  h_t = g(Wh_{t-1} + Ux_t)
  $$
  which is a **nonlinear function** on **previous state and current input**. ($x_t, h_{t-1}$ would be vectors)

  <img src="DL_notes/image-20220203132556812.png" alt="image-20220203132556812" style="zoom:33%;" />

- the output then is:
  $$
  y_t = V h_t
  $$
  which **can be followed by an activation**, in that case we say:
  $$
  y_t = f(z_t) = f(Vh_t)
  $$
  where $f$ would be a nonlinear function such as sigmoid.

-  notice that all weights $V,W,U$ **are the same**!
  - this means that we only need to update it once for each matrix!

Additionally, since to get to the new hidden state $h_t$ we need $Wh_{t-1} + Ux_t$, this can be computed in one shot by:
$$
[W,U] \begin{bmatrix}
h_{t-1}\\
x_t
\end{bmatrix} = Wh_{t-1} + Ux_t
$$

Then the algorithm looks as follows:

<img src="DL_notes/image-20220207183223337.png" alt="image-20220207183223337" style="zoom:67%;" />

which essentially is when we have one layer.

- As with feedforward networks (NN), well use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks.
- more details on backpropagation through time is shown in later sections.

### RNN Architecture

Some common architectures used in RNN can be visualized as:

|                         One to Many                          |                         Many to One                          |                         Many to Many                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220203133619748](DL_notes/image-20220203133619748.png) | ![image-20220203212927731](DL_notes/image-20220203212927731.png) | ![image-20220203133652920](DL_notes/image-20220203133652920.png) |

where in the end you need to **specify size of output**. Neural networks are always trained with fixed length of input and output. 

- **one to many**: e.g. image to caption
- **many to one**: caption to image/stock price prediction/sentiment classification
- **many to many**: machine translation, video action classification
  - e.g. this can be done by "hardcoding" in the algorithm such that you ask the model to only output $y_t=f(Vh_t)$ if $t \in [T-2,T-1,T]$, for example.


There are many choices of architectures you can use:

<img src="DL_notes/image-20220203133936167.png" alt="image-20220203133936167" style="zoom:33%;" />

which would be useful for **audio to text**

### Loss Function

To complete our definition of the RNN architecture requires incorporating a loss function, with which we can **improve our model by gradient descending on the shared weigths**.

The simple idea is that **each output can be compared to the label**, so we are doing:

<img src="DL_notes/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" />

where:

- we have covered how states are computed, but the others:

  <img src="DL_notes/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" />

  so essentially $\hat{y} = \text{softmax}(Vh_t)$

Then, the **total loss is just the sum over the characters**:
$$
\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i) = \sum_{t=1}^{l_i}\mathcal{L}_{\text{character}}(\hat{y}_t^i, y_t^i)
$$
note that the $i$th input $x^i$ is a **sequence of characters**.

- the sequence length $l_i$ has **nothing** to do with dimension of the **$i$th input $x_t^i$ or $y_t^i$ at time $t$**, which are basically feature/output vectors for **each character** along the sequence of length $l_i$.

- then, since we could have $m$ sequences/sentences in the entire dataset, we would have:
  $$
  \mathcal{L}_{\text{total}}(\hat{y}_t, y_t) = \sum_{i=1}^{m}\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i)
  $$
  this means that together it will be a double sum.

### Deep RNN

We can **stack multiple hidden layers and connecting them** as follows:

<img src="DL_notes/image-20220203220235023.png" alt="image-20220203220235023" style="zoom: 50%;" />

where essentially:

- the output of the lowest layer is treated as the "current state/input" of the second last layer.
- the weights $U^l,W^l$ will be **shared within each layer $l$**, but there is still a **single $V$** for output

- therefore, the new state transition equation at layer $l$ becomes:
  $$
  h_t^l = g(W^lh_{t-1}^l + U^lh_{t}^{l-1})
  $$
  where $h_{t}^{l-1}$ is the state of previous layer at that time $t$, treated as an "input/current state".

Additionally, we can build **dependencies by**

<img src="DL_notes/image-20220203220609110.png" alt="image-20220203220609110" style="zoom:67%;" />

where then we need to specify a weight for connection  $o_{t-1} \to h_t$, which is certainly doable.

Last but not least, an important variant is the **bidirectional RNN**, which you shall see in the next section.

### Bidirectional RNN

Basically, the idea is that we not only want to remember **forward information**, but also **backward information** (context in both direction). Therefore, we consider each state $h_t$ being **duplicated into two**:

<img src="DL_notes/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 50%;" />

such that

- then you have two weights $W, \bar{W}$, where the former is used to flow between $h_{t-1} \to h_t$, the latter $\bar{h}_{t+1} \to \bar{h}_t$:
  $$
  \begin{align*}
  h_t &= g(Wh_{t-1} + Ux_t)\\
  \bar{h}_t &= g(\bar{W}\bar{h}_{t+1} + Ux_t)\
  \end{align*}
  $$

- then output basically depends on **both state information** by concatenating them
  $$
  o_t = V\begin{bmatrix}
  h_t\\
  \bar{h}_t
  \end{bmatrix}
  $$

Then we can stack those as well

<img src="DL_notes/image-20220203134545172.png" alt="image-20220203134545172" style="zoom: 80%;" />

### Backpropagation Through Time

Having defined the RNN architectures and loss function, our goal is to train the RNN by **finding derivatives and descending.** Essentially we will use **backpropagation** again as it is a deep network.

Then general form of the gradient of a sequence loss $\mathcal{L}_{ \text{sequence}}$ on some parameter $\theta$ is:
$$
\frac{d\mathcal{L}_{ \text{sequence}}(\hat{y} , y)}{d\theta} = \sum_{t=1}^{l_i}\frac{d\mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{d\theta} = \sum_{t=1}^{l_i}\sum_{t}\frac{\partial \mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{\partial h_t}\frac{\partial h_t}{\partial \theta}
$$
you will see that all derivatives from then on **will have dependence on $t$**, which is why we call it backpropagation through time.

Recall that in a simple RNN:

|                            Label                             |                           Diagram                            |                            Recall                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" /> | <img src="DL_notes/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" /> | <img src="DL_notes/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /> |

Then first computing the gradient for $V$ at time $t=3$
$$
\frac{\partial L_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial o_3}\frac{\partial o_3}{\partial V}
$$
notice that it ==does not depend on data from previous time==, since $o_t = Vh_t$ only depends on current time.

- therefore, updating $V$ at each iteration at time $t$ is simple

Now, if we consider updating $W$:
$$
\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W}
$$
but then we know that $h_3  = f(Wh_2 + Ux_3)$ which **depends on previous time**! Hence in this case we would have:
$$
\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \frac{\partial h_3}{\partial h_i} \frac{\partial h_i}{\partial W}
$$
but then, for instance:
$$
\frac{\partial h_3}{\partial h_1} = \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1}
$$
We can generalize the above even further to:
$$
\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \left( \prod_{j=i+1}^3 \frac{\partial h_3}{\partial h_{j-1}} \right) \frac{\partial h_i}{\partial W}
$$
if we take the activation function to be $\tanh$, then the part in parenthesis above is basically $\prod W^T \, \text{diag}(\tanh'(h_{t-1}))$, which means that as $t >> 3$, we would have backpropagation **rasing $W^T$ to a high power**.

- hence, RNN could suffer vanishing/explode gradients if the eigenvalues are less than one or greater than one, respectively

- graphically, where $E = L$ that we used above:

  <img src="DL_notes/image-20220203135108631.png" alt="image-20220203135108631" style="zoom: 50%;" />

- remember that we **only update once in the end for $W$**, because we only have **one shared $W$**! (same for $U,V$ if we are using a single layer RNN)

Finally, the algorithm is then summarized here:

<img src="DL_notes/image-20220203230211497.png" alt="image-20220203230211497" style="zoom:50%;" />

note that this is only the backpropagation part (second pass). The full algorithm of RNN would have a two-pass algorithm

1.  In the first pass, we perform forward inference, computing $h_t$ , $y_t$ , accumulating the loss at each step in time, **saving the value of the hidden layer** at each step for use at the next time step. 
2. In the second phase, we process the sequence in reverse, computing the required **gradients** as we go, computing and saving the error term for use in the hidden layer for each step **backward** in time

### RNN as Language Models

This is essentially an example of applying RNN in real life. This idea of how you **treat input/output** as probabilities is used in other models introduced next as well.

Now, consider the task of **predicting next word** again. Here we have:

- input as **text/sentences**
- output **probability** that each word $w_i \in V$ will be the next word

Then, we can have the following in each RNN layer:

![image-20220207185449002](DL_notes/image-20220207185449002.png)

where:

- $E$ is an **embedding matrix**, so $E x_t$ is embedding of current word $t$.

- the probability that a particular word $w_i$ in the vocabulary is the next word is represented by $y_t[i]$, the $i$th component of $y_t$$.
  $$
  y_t[i] = P(w_{w+1}=w_i| w_1, ..., w_t)
  $$
  where you can imagine $w_i \in V$ can be indexed easily

Then, while **training**, you would have a correct distribution $y^*_t$, which is essentially a **one-hot encoded vector** for the correct word $w^*_t$. Since this can be treated as a probability distribution, and our prediction $y_t$ is also a probability distribution:
$$
L_\text{Cross Entropy} = \sum_t L_{\text{CE}}(y_t, y^*_t) = \sum_t - \log y_t[w^*_{t+1}]
$$
where essentially $y_t[w^*_{t+1}]$ is the probability that our model predicts $w^*_{t+1}$ to be the next word correctly.

 Graphically, this is what happens when we are training:

<img src="DL_notes/image-20220207190334237.png" alt="image-20220207190334237" style="zoom: 80%;" />

where it is important to note that the loss is on the **probability for next word**.

## Gated Recurrent Unit

One solution for vanishing/exploding gradient would be to use **GRU** instead of an RNN architecture for transition.

In RNN, we had the following structure

|                          Structure                           |                        Encapsulation                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /> | <img src="DL_notes/image-20220203231239039.png" alt="image-20220203231239039" style="zoom:50%;" /> |

But now, what GRU does is essentially doing **more complicated thing for transition**

<img src="DL_notes/image-20220203135552047.png" alt="image-20220203135552047" style="zoom:33%;" />

where:

- there are many version of GRU circuits, many of which perform equally well on certain dataset
- the final best design was essentially discovered using a grid search over all possible gates. So there is kind of no theoretical reason why.

Schematically, GRU does the following:

|                        GRU Schematic                         |                          Equations                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220203232453595](DL_notes/image-20220203232453595.png) | ![image-20220203231843814](DL_notes/image-20220203231843814.png) |

where it emphasized that we need to be skilled at **translating between a graphical representation to equations**

- the shared weights are now $W_r, W_z, W$ and $U_r, U_z,U$ (and of course $V$ for output)
- the main changes are that we **added three more gates/components** before deciding what is $h_t$
- it is also important to notice the **plus operation** as the last step for state transition. This plus instead of multiply essentially solves the exploding gradient problem
- **inputs** of functions/state have arrows pointing in
- **weights** are labelled on the arrow
- **nonlinear functions** are not shown but applied **if an operator is not on the graph**
  - e.g. $h_t$ does not have a nonlinear function because it is specified we have a $+$ operation
- the rest of the architecture is the same as RNN

---

*An analogy of the input/state/gates*

Consider the example:

- state $h_{t-1}$ is the cloth we wear yesterday
- $x_t$ is the weather/input on day $t$/today
- $\bar{h}_t$ is the candidate clothes we *prepared/predicted* to wear
- $h_t$ is the actual clothes we wear on day $t$/today

Then, essentially those additional gates (the update and reset gates) determine ==to what extent== we ==take into account these factors==: 

- do we **ignore** the weather $x_t$ completely, 
- do we **forget** what we wore yesterday $h_{t-1}$
- and do we take into account our candidate clothes we prepared $\bar{h}_t$, and to **which extent.**

In short, the effect of those can be overviewed as below:

### Update Gate

The update gate basically does the following:
$$
z_t = \sigma (W_z h_{t-1} + U_z x_t)
$$
which is **between 0 and 1**, then is used in $h_t$ as:

<img src="DL_notes/image-20220203234102881.png" alt="image-20220203234102881" style="zoom:80%;" />

Examples of what $z_t$ does is shown below

|                 output is the new candidate                  |             output is the previous hidden state              |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220203233102030](DL_notes/image-20220203233102030.png) | ![image-20220203233355981](DL_notes/image-20220203233355981.png) |

### Reset Gate

The reset gate again is nonlinear:
$$
r_t = \sigma (W_r h_{t-1} + U_r x_t)
$$
This is used by the candidate activation:

<img src="DL_notes/image-20220203234223392.png" alt="image-20220203234223392" style="zoom: 80%;" />

The effects:

|        Candidate forgetting the previous hidden state        |                  Candidate does same as RNN                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220203233449807.png" alt="image-20220203233449807" style="zoom: 67%;" /> | <img src="DL_notes/image-20220203233500860.png" alt="image-20220203233500860" style="zoom:67%;" /> |

### Function

The last possible combination is:

- $z_t = r_t = 0$, then hidden state is only dependent ton current state as $h_t = \bar{h}_t = \phi(Ux_t)$
- $z_t =0, r_t = 1$, then we get back RNN because $h_t = \bar{h}_t = \phi(Wh_{t-1} + Ux_t)$

|  Output hidden state is only dependent on the current state  |                        Reduced to RNN                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220203234523035](DL_notes/image-20220203234523035.png) | ![image-20220203234531917](DL_notes/image-20220203234531917.png) |

## Long Short-Term Memory

Long short-term memory (LSTM) (Hochreiter & Schmidhuber 1997) was introduced two decades before the GRU (Cho et al. 2014). 

The LSTM is easy to train, and includes an **additional input and output compared with the RNN and GRU**. At each time step $t$:

- receives as input the current state $x_t$, the hidden state $h_{t-1}$, and **memory cell** $c_{t-1}$ of the previous time step
- outputs the hidden state $h_t$ and memory cell $c_t$

An encapsulation would look like this

<img src="DL_notes/image-20220204000418648.png" alt="image-20220204000418648" style="zoom:50%;" />

It is then combined such that we have

<img src="DL_notes/image-20220204000434163.png" alt="image-20220204000434163" style="zoom:50%;" />

- similarly, you can also combine to have a **bidirectional LSTM** by:

  |                      Bidirectional RNN                       |                      Bidirectional LSTM                      |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="DL_notes/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 33%;" /> | <img src="http://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f7bdb849dafe17c952bfd88b879e01f74cf59d78/4-Figure3-1.png" alt="Bidirectional LSTM (BiLSTM) Training Task - GM-RKB" style="zoom: 50%;" /> |

  where essentially **each LSTM cell at time $t$ is duplicated** (they also have a separate weights like in bidirectional RNN) into a forward direction and a backward direction. 

Each unit of LSTM look like:

|                        LSTM Schematic                        |                         Another View                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220204000548449.png" alt="image-20220204000548449" style="zoom: 50%;" /> | <img src="DL_notes/image-20220207190734060.png" alt="image-20220207190734060" style="zoom: 67%;" /> |

where the highlighted part is clear, same as RNN.

- so we have an additional five components:

  <img src="DL_notes/image-20220204000633720.png" alt="image-20220204000633720" style="zoom:67%;" />

- so now shared weights are $W_f, W_i, W, W_0$ and $U_f, U_i, U, U_0$

- outputs will be $c_t, h_t$, which will be inputs in the next LSTM unit.

Alike GRU, those three gate essentially regulates how much information can get through

### Forget Gate

The forget gate has the following equation

<img src="DL_notes/image-20220204000921911.png" alt="image-20220204000921911" style="zoom: 67%;" />

- or somtimes, we can simply this as $f_t = \sigma(W_f \cdot [h_{t-1},x_t]^T )$ for $W_f \equiv [W_f, U_f]$ being concatenated

Since it is sigmoid, we can investigate the extreme values

|              Memory Cell Ignore Previous Memory              |
| :----------------------------------------------------------: |
| <img src="DL_notes/image-20220204001008540.png" alt="image-20220204001008540" style="zoom:50%;" /> |

### Input Gate

The input gate has the function

<img src="DL_notes/image-20220204001140743.png" alt="image-20220204001140743" style="zoom:67%;" />

which again is **used by the memory cell** 

- controls **how much $\bar{c}_t$ will be included** in the new cell state $c_t$

For instance, since $\sigma \in [0,1]$

|         new candidate memory $\bar{c}_t$ is ignored          |
| :----------------------------------------------------------: |
| <img src="DL_notes/image-20220204001242584.png" alt="image-20220204001242584" style="zoom: 50%;" /> |

### Memory Cell

The memory cell has equation:

<img src="DL_notes/image-20220204001326157.png" alt="image-20220204001326157" style="zoom:67%;" />

which basically is updated very iteration to store some new memory, essentially **candidate memory**:

<img src="DL_notes/image-20220204001418530.png" alt="image-20220204001418530" style="zoom: 67%;" />

which then stores **information about previous state and input**

- then, when we are outputing $h_t$, it will **read from memory cell $c_t$**

### Output Gate

The output gate has equation:

<img src="DL_notes/image-20220204001608239.png" alt="image-20220204001608239" style="zoom:67%;" />

which is **essentially RNN**, highlighted in green:

<img src="DL_notes/image-20220207163227530.png" alt="image-20220207163227530" style="zoom:33%;" />

but notice that the next state $h_t$ also **included memory**:
$$
h_t = o_t \cdot \phi(c_t)
$$
which is a point-wise multiplication.

- function $\phi$ is a nonlinear function, such as $\tanh$
- so essentially $h_t$ depends on **output $o_t$ and cell memory $c_t$**, where $o_t$ is essentially the RNN cell

---

*For Example*

Recall that the output will essentially comes out from $h_t$, essentially:
$$
y_t = f(z_t) = f(Vh_t)
$$
where $f$ is an activation function if we are donig classification.

Then, some of the usages for LSTM would look like:

<img src="DL_notes/image-20220207164111464.png" alt="image-20220207164111464" style="zoom: 50%;" />

where your output at time $t$ essentialy comes out from $h_t$.

---

## GRU vs LSTM

**Similarities** between the two: 

- both units avoid repeated multiplications which cause vanishing or exploding gradients by a similarly positioned ==addition==

<img src="DL_notes/image-20220204001848344.png" alt="image-20220204001848344" style="zoom: 67%;" />

- update gate $z_t$ controls the amount of the new candidate to pass in the GRU; whereas the input gate controls the amount of the new candidate memory to pass in the LSTM

  |                             GRU                              |                             LSTM                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220204002239125](DL_notes/image-20220204002239125.png) | ![image-20220204002134935](DL_notes/image-20220204002134935.png) |

- Comparing the GRU reset gate controlling the candidate hidden state as highlighted in Figure 6.45 with the LSTM input gate controlling the candidate memory cell as highlighted in Figure 6.46 shows the **modulation of the candidate** in both units.

  |                             GRU                              |                             LSTM                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220204002408761](DL_notes/image-20220204002408761.png) | ![image-20220204002416921](DL_notes/image-20220204002416921.png) |

**Differences**

- as GRU has fewer gates, they have a **fewer numbe of parameters and trains faster** than LSTM

Lastly, an overview of NN, RNN and LSTM as "neuron":

![image-20220207191037059](DL_notes/image-20220207191037059.png)

## Sequence to Sequence

You can essentially use LSTM/RNN/GRU blocks as follows;

<img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" />

where:

- each component LSTM used here can be changed to GRU or RNN

- the **encoder** takes in the input sequence $(x_1, ..., x_s)$ and output a **context vector** $z=f(Vh_t)$

  - this is a **single vector**, which is treated as hidden state $h_0$ in the decoder

- the **decoder** also consists of LSTM/GRU that takes $h_0=z$ as the first hidden state, and generates output **sequence $(y_1,...,y_t)$**

  - therefore, the entire model is doing:
    $$
    (y_1,...,y_t) = \text{decoder}(\text{encoder}(x_1, ..., x_s))
    $$

- this is often used for **machine translation**, for instance

## Adding Attention

> Below introduces the idea of attention as essentially a **weighted sum over inputs**, and talks about **encoder-decoder attention**. For reference, checkout [Self-Attention](#Self-Attention) which is relevant but different.

For many applications, it helps to **add attention** to RNNs, so that we can **focus on certain part of the input sequence**

- Allows network to learn to ==attend to different parts of the input== at different time steps, shifting its attention to focus on different aspects during its processing.

- Used in image captioning to focus on *different parts of an image* when *generating different parts of the output sentence*.
- In MT, allows focusing attention on *different parts of the source sentence* when *generating different parts of the translation*.

For instance:

![image-20220207170115608](DL_notes/image-20220207170115608.png)

where we see that:

- the underlined text is our **query**
- the image is our **key** (the database of data we have)
- then we want to output the highlighted part

To see why this is useful, consider the task of doing **machine translation**, so we are using a Seq2Seq Model which consists of an encoder and a decoder as shown before:

<img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" />

Now, Seq2seq models **incorporating attention** can:

1. the **decoder** receives as input the **encoder encoder output sequence** $c_i=c_i(o_1, ...,o_t)$
   - where $o_t= V\begin{bmatrix}h_t\\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$
2. different parts of the output sequence **pay attention** to **different parts of the input sequence**

<img src="DL_notes/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 60%;" />

where essentially:

- for each input to **decoder** hidden state $\tilde{h}_t$, a context vector is used which is **attention** w.r.t step $t=1$ and of **all input sequences $x_1, ..., x_t$**

- specifically, in this **bidirectional RNN** as encoder, the context vector is computed as:
  $$
  c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T
  $$
  which is a **weighted sum** over **all hidden states in the encoder** (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:
  $$
  \alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j
  $$
  which basically represents the **amount of attention** output $o_i$ will give to input word $x_j$, for some score function. (checkout [Self-Attention](#Self-Attention)).

- This context vector, $c_i$, is generated anew with **each decoding step $i$** and takes all of the encoder hidden states into account in its derivation

- this is also called ==encoder-decoder attention==, which is different from [Self-Attention](#Self-Attention), and covered more in detail in [Attention](#Attention)

## SGNS Embeddings

Another common problem in language data is **how do we represent texts/words (tokens)**.

- one-hot encoding
  - problem: every two words have **the same distance**. i.e. losing relationship between them
  - problem: sparse vector.
- **feature embedding** for each word
  - we want to somehow learn word embedding from large **unsupervised** text corpus.

In this section we introduce one method for computing embeddings: **skip-gram SGNS with negative sampling**, sometimes called SGNS. 

- The skip-gram algorithm is one word2vec of two algorithms in a software package called word2vec

> *Intuition*
>
> The intuition of ==word2vec== is to train a classifier on a **binary** prediction task: given a word $w$, **how likely is it to show up near another word** $w_i$, e.g. apricot?
>
> - this can be done in **self-supervision**, which avoids the need for any sort of hand-labeled supervision signal
> - essentially training a logistic regression classifier
> - this is **static**, in that it learns one **fixed embedding for each word** in the embeddings vocabulary. 
>   - In the [Transformer](#Transformer)  chapter we will introduce methods for learning **dynamic contextual embeddings** like the popular family of BERT representations, in which the vector for each word is **different in different contexts**
>
> Then, ==while we are learning the likelihood==, we would have ==needed/learnt some representation $\vec{w}$ for a word $w$==, which will be our embedding!

Therefore, the model is simple:

1. Treat the target word $w$ and a **neighboring** context word as **positive** examples.
2. Randomly sample other words in the lexicon to get negative samples.
3. Use **logistic regression** to train a classifier to distinguish those two cases.
4. Use the **learned weights** $W$ as the **embeddings**, commonly represented as $E$

> **Note**
>
> This means that the embedding for word $w_i$ will be similar to $w_j$ if they are **physically close** to each other.
>
> - i.e. if phrases like "bitter sweet" will create problems in the embedding! (physically close but their meanings are different)

### SGNS Classifier

> **Goal of Classifier**:
>
> Given a tuple $w,c$ being target word $w$ paired with a candidate word $c$, what is the **probability** that $c$ is a **context word** (i.e. physically next to it)?
>
> To present such **probability**, we will use:
> $$
> P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}
> $$
> for $\vec{w},\vec{c}$ being the **embedding of word $w,c$**.
>
> - so on our way to learn such classifier, we would have learnt $\vec{w},\vec{c}$ which will be used for embedding

Consider we want to find out the **embedding of the word $\text{apricot}$**, and we have the following data:

![image-20220208214715503](DL_notes/image-20220208214715503.png)

where:

- let us take a window size of $2$, so that we view $[c_1, ..., c_4]$ above as **real context word** for $ \text{apricot}$.

- our goal is to have a logistic regression such that:
  $$
  P(+|w,c)
  $$
  is **high** if $c$ is a **real context word**, and that:
  $$
  P(-|w,c)  = 1-P(+|w,c)
  $$
  is **high** if $c$ is **not a context word**.

Then, the question is how do we model such probability? We assumed that **words next to each other** should have ==similar embeddings==. This means that:
$$
\text{Similarity}(w,c) \approx \vec{w} \cdot \vec{c}
$$
for $\vec{w},\vec{c}$ being the embeddings for the word $w,c$. Then, to map this to **probability that $c$ is a real context word for $w$ as**:
$$
P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}
$$
is high if high dot product = similar = next to each other. Then similarly, probability that $c$ is not a context word as:
$$
P(-|w,c) = \sigma(-\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(+\vec{w}\cdot \vec{c})}
$$
for $c$ being **negative samples** (not context words).

---

Now, this means that we can also assign "**similarity score**" between a target word and a **context window**:
$$
P(+|w, c_{1:L}) = \prod_{i=1}^L \sigma(\vec{c}_i \cdot \vec{w})
$$
where:

- we assumed **all contexts words are independent**
- we can also compute the log probability to make it a sum

---

Now, we can think of what are are learning graphically:

![image-20220208220831910](DL_notes/image-20220208220831910.png)

where essentially:

- Embedding matrix $E$ contains two matrices, $W$ for word embedding and $C$ for context embedding
  - i.e. for the $i$-th word (in the dictionary), its word embedding will be the $i$-th column of $W$, and similarly for context embedding 
  - i.e. every word will have **two embeddings**, one in $W$ and another in $C$. In reality people either only take $W$ or take $W+C$
  - if your vocabulary size is $|V|$, and you want an embedding of dimension $d$, then $W,C \in \mathbb{R}^{d \times |V|}$ so that you can fetch the embedding from a one-hot vector.

- we have **two embeddings** because a word $w$ could be treated as a target, but sometimes it might also be picked as a context $c$, in which we update the embedding separately.

### Learning the Embedding

Our target is to learn the matrix:

![image-20220208220831910](DL_notes/image-20220208220831910.png)

Let us begin with an example text, where our target word currently is $w=\text{apricot}$.

![image-20220208221658252](DL_notes/image-20220208221658252.png)

Then taking a window size of $2$, we also want to have **negative samples** (in fact, more negative samples than positive ones per target word so that we are called **SGNS**):

![image-20220208221831637](DL_notes/image-20220208221831637.png)

where here:

- each of the training sample $(w, c_{pos})$ comes with $k=2$ negative samples, for $k$ being tunable

- the negative samples are sampled **randomly by**:
  $$
  \text{Prob of sampling $w$}= P_\alpha(w) = \frac{ \text{Count}(w)^\alpha }{\sum_{w'} \text{Count}(w')^\alpha}
  $$
  where we usually take $\alpha = 0.75$ so that **rare words have a  better chance**

  e.g. if $P(a)=0.99,P(b)=0.01$, doing the power would give:

  <img src="DL_notes/image-20220208222147320.png" alt="image-20220208222147320" style="zoom:80%;" />

Then, **given a positive pair and the $k$ negative pairs**, our ==loss function to minimize would be==
$$
L_{CE} = -\log{\left[ P(+|w,c_{pos}) \cdot \prod_{i=1}^k P(- |w,c_{neg})\right]} = - \left[ \log \sigma(c_{pos} \cdot w) + \sum_{i=1}^k  \log \sigma(-c_{neg} \cdot w) \right]
$$
which we want to **minimize** by **updating $\vec{w} \in W$** for the target word and $\vec{c} \in C$ for the context word.

Therefore, we need to take the derivatives:

<img src="DL_notes/image-20220208222630846.png" alt="image-20220208222630846" style="zoom:80%;" />

Then the update equations

<img src="DL_notes/image-20220208222649078.png" alt="image-20220208222649078" style="zoom:80%;" />

So graphically, we are doing:

<img src="DL_notes/image-20220208222800901.png" alt="image-20220208222800901" style="zoom:80%;" />

notice that we update target word embedding in $W$ but context words in $C$.

Then that is it! You now have leant **two separate embeddings** for each word $i$:

- $\vec{w}_i = W[i]$ is the embedding **target embedding** for word $i$
- $\vec{c}_i = C[i]$ is the embedding **context embedding** for word $i$

(then it is common to either just add them together as $\vec{e}_i = \vec{w}_i+\vec{c}_i$ as the embedding for word $i$, or just take $\vec{e}_i = \vec{w}_i$.)

### Other Embeddings

Some problem with SGNS Embedding (Word2Vec) is:

- what if we want to know the embedding of an **unknown word** (i.e. unseen in the training corpus)?
- **sparsity**: in languages with rich morphology, where some of the *many forms for each noun and verb may only occur rarely*

There are many other kinds of word embeddings that could deal with those problems. Here we briefly cover two:

- **Fasttext**: Fasttext deals with these problems by using **subword** models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols `<` and `>` added to each word

  For example, with $n = 3$ the word where would be represented by the sequence `<where>` plus the character n-grams:
  $$
  \text{<wh, whe, her, ere, re>}
  $$
  then a skipgram embedding is learned for each constituent n-gram, and the word `where` is represented by the **sum of all of the embeddings of its constituent n-grams**. Therefore, ==Unknown words== can then be presented only by the sum of the constituent n-grams!

- **GloVe**: GloVe model is based on capturing **global corpus statistics**. GloVe is based on ratios of probabilities from the word-word cooccurrence matrix, combining the intuitions of count-based models like PPMI while also capturing the linear structures used by methods like word2vec.

- and many more

# Graph Neural Network

In this chapter we describe graph neural networks, applied to networks or general graphs sharing weights across neighborhoods as shown below:

<img src="DL_notes/image-20220211223146024.png" alt="image-20220211223146024" style="zoom: 50%;" />

Essentially graphs are everywhere, and we deal with data that can be represented as a graph by the following idea:

- Each **node** in a network may have an **associated feature vector** that represents its attributes
- The edge information would be **also encoded** in the **feature vector** of the node
  - essentially, a node will **aggregate information from neighbors** to achieve this

> **Summary**
>
> In the end, a GNN basically attempts to **encode much information about the graph** into **each node of the graph**, resulting in some complicated **embedded vector $h_i^l$ for each node $i$.** 
>
> Then, this information is used to do downstream tasks such as node classification.

Then, essentially we have a list of feature vectors representing the graph. With that, some **common tasks** include:

1. **Node prediction**: Predicting a property of a graph node.
2. **Link prediction**: Predicting a property of a graph edge. 
   - For example, in a social network we can predict whether two people will become friends
3. **Graph or sub-graph prediction**: Predicting a property of the entire graph or a sub-graph.
   - For example, given a graph representation of a protein we can predict its function as an enzyme or not

<img src="DL_notes/image-20220211222822911.png" alt="image-20220211222822911" style="zoom:50%;" />

## Definitions

A graph $G = (V, E)$ contains a set of $n$ vertices (or nodes) $V$ and set of $m$ edges $E$ between vertices. The edges of the graph can either be undirected or directed.

Two basic graph representations are an **adjacency matrix** and **adjacency list**.

> An **adjacency matrix** $A$ of dimensions $n \times n$ is defined such that:
> $$
> A_{i,j} = \begin{cases}
> 1, & \text{if there is an edge between vertex $i$ and $j$}\\
> 0,& \text{otherwise}
> \end{cases}
> $$
> where if edges have weights, then replace $1\to w$

For example:

|                       Adjacency Matrix                       |                            Graph                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220211230849534.png" alt="image-20220211230849534" style="zoom:50%;" /> | <img src="DL_notes/image-20220211230856499.png" alt="image-20220211230856499" style="zoom:50%;" /> |

> An **adjacency list** is a list of edges for each node.

**Though** adjacency matrix and list encode the same information:

- different **permutations of the node labels** result in different adjacency matrices. In contrast, an adjacency list of the edges in the graph is invariant to node permutations.
- **storing** an adjacency matrix takes $O(n^2)$ memory for $n$ being the number of nodes; whereas for adjaceny matrix it only takes $O(m)$ for $m$ is the number of edges in the graph. Since most graphs are sparse, this makes it even more appropriate to use adjacency list instead.

> The **degree** of a node $d_i$ represents the number of edges incident to that node (i.e. number of connections that it has to other nodes in the network)
>
> - average degree of a graph is the average degree over all its node:
>   $$
>   \frac{1}{n}\sum_{i=1}^n d_i
>   $$
>   which is $2m/n$ for an undirected graph and $m/n$ for a directed graph
>
> You can also represent the degree matrix $D$ being a diagonal matrix (so it can be used for calculating Laplacian):
> $$
> D_{i,i} = \text{degree}(v_i) = \sum_{j=1}^n A_{i,j}
> $$

For example, the degree matrix for the above graph:

<img src="DL_notes/image-20220211231752362.png" alt="image-20220211231752362" style="zoom:50%;" />

> for directed graphs the **indegree** of a node is the number of edges leading into that node and its **outdegree**, the number of edges leading away from it.

> The **graph Laplacian matrix** $L$ is the difference between the degree matrix and adjacency matrix $L = D  A$.
>
> - for functions, Laplacian measures the *divergence*:
>
>   <img src="DL_notes/image-20220211232324624.png" alt="image-20220211232324624" style="zoom:67%;" />
>
> - for more reference to see how it works: https://mbernste.github.io/posts/laplacian_matrix/

An example would be:

<img src="DL_notes/image-20220211232434295.png" alt="image-20220211232434295" style="zoom:50%;" />



notice that:

- The adjacency matrix and the degree matrix are symmetric and therefore the Laplacian matrix is symmetric

Some other related matrix would be:

- **graph symmetric normalized Laplacian**
  $$
  \hat{L}=D^{-1/2} L D^{-1/2}=I-D^{-1/2} A D^{-1/2}
  $$

- **random walk normalized Laplacian matrix**:
  $$
  L_r=D^{-1}L = I-D^{-1}A
  $$

> A Laplacian matrix $L$ of a graph with $n$ nodes ha**s $n$ eigenvectors with eigenvalues which are non-negative** since the Laplacian matrix $L$ has non-negative eigenvalues.
>
> The number of **zero eigenvalues** of the Laplacian matrix of a graph is the **number of its connected components**.

> **Sub-graph** of a graph is **a subset of edges** and **all their nodes** in the graph.

> A **walk** is a sequence of vertices and edges of a graph i.e. if we traverse a graph then we get a walk.
>
> - a walk can be open or closed (i.e. end same as start)
> - vertices and Edges can be **repeated** in a walk

An example of a walk would be:

<img src="DL_notes/image-20220211233110467.png" alt="image-20220211233110467" style="zoom: 50%;" />

here `1->2->3->4->2->1->3` is a walk

> **Trail** is an open walk in which **no edge is repeated**.
>
> - vertex can be **repeated**

For example:

<img src="https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-4.png" alt="img" style="zoom: 33%;" />

here Here `1->3->8->6->3->2` is trail 

> **Path** is a trail in which neither vertices nor edges are repeated

> The matrix $A^k$ from an adjacency matrix contains $A_{i,j}$ being the **number of walks of length $k$** in the graph between the node in row $i$ and the node in column $j$.

## Problem using Graphs

Polynomial

- **Minimum Spanning Tree (MST)**: For an undirected graph, produce an acyclic tree that is the subset of the graph that spans all of the Vertices (Spanning), and it needs to have a minimum sum in terms of the edges included (minimum)

  - Greedy algorithms with time complexity $O(|E|\log|V|)$: **Boruvka**, **Prim**, **Kruskal**
  - similar to Dijkstra, basically a graph without cycles

- **Single-Source Shortest Paths (SSP)**

  - For SSP with nonnegative weights: **Dijkstras** algorithm. Complexity $O(|V|\log|V| + |E|)$ using a heap

    - essentially the greedy step is that we set the node that is marked with **smallest tentative distance** as the current node/completed.

      ![Dijkstra Animation.gif](https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Dijkstra_Animation.gif/220px-Dijkstra_Animation.gif)

  - For general SSP: **Bellman-Ford** runs in $O(|V||E|)$


NP-hard

- **Traveling Salesman Problem (TSP)**
  - e.g. find shortest tour visiting each city once and returns to start.
- **Vehicle Routing Problem (VRP)**

> *Recall*
>
> **Dijkstra's Algorithm**:
>
> 1. initialization: marks everything as unvisited, and set each $D_v$ field to max. Then set the source $D_v$ to 0 and visited
> 2. vistit each reachable vertices and update the field
>    - update $D_v$ only when it is smaller
>    - but not update *visited*, yet
> 3. then we take the greedy step of marking the **unvisited vertex with current smallest $D_v$** to be visited
> 4. continues with that vertex
>
> **Prim's Algorithm**
> this is pretty much the same as Dijkstra's Algorithm, but since we are constructing a tree, the difference is
>
> - *known* means whether if we have **included that vertex into the tree**
> - $D_v$ means the current smallest distance we currently know to bring that vertex into the graph (not cumulative)
> - $P_v$ vertex that achieves the shortest distance in the $D_v$ field

## Node Embeddings

> Graph embeddings basically means finding "**latent vector representation**" of graphs which captures the **topology** (in very basic sense) of the graph. We can make this "vector representation" rich by also considering the vertex-vertex relationships, edge-information etc. 
>
> - the following assumes that ==each vertex/node has an associated feature vector==
> - so in a sense it is "node embedding"

There are roughly two levels of embeddings in the graph (of-course we can anytime define more levels by logically dividing the whole graph into subgraphs of various sizes):

- **Vertex Embeddings** - Here you find latent vector representation of *every vertex* in the given graph. You can then compare the different vertices by plotting these vectors in the space and interestingly "similar" vertices are plotted closer to each other than the ones which are dissimilar or less related. This is the same work that is done in "DeepWalk" by Perozzi.
- **Graph Embeddings** - Here you find the latent vector representation of the whole graph itself. For example, you have a group of chemical compounds for which you want to check which compounds are similar to each other, how many type of compounds are there in the group (clusters) etc. You can use these vectors and plot them in space and find all the above information. This is the work that is done in "Deep Graph Kernels" by Yanardag.

<img src="https://snap-stanford.github.io/cs224w-notes/assets/img/node_embeddings.png?style=centerme" alt="node embeddings" style="zoom: 20%;" />

For example, we may optimize for the similarity between nodes $i$ and $j$, such that their similarity $s(i, j)$ is maintained after the embedding $f(i)^T f(j)$.

### Shallow Embedding

Shallow encoding is the simplest embedding approach, it means it is just an embedding-lookup :
$$
f(v_i) =We_i
$$
for $e_i \in \mathbb{I}^{n}$ is essentially a **one-hot encoded** vector, and $W \in \mathbb{R}^{d \times n}$ if there are $n$ nodes.

- This results in a problem with shallow embeddings, which is that they do not share weights, i.e. does not scale with the number of nodes

### Node Similarity

One key idea of embedding is that **similarity of nodes** is preserved in the embedding space. To formally use that as objective for learning the embedding (e.g. shallow encoding), we need to define **similarity**

- essentially different similarity metric captures **different properties of a graph**, results in **different loss functions** hence **different embedding algorithm**

#### Adjacency-based Similarity

> **Similarity** between nodes $i$ and $j$ is the weight on the edge between them $s(i, j) = A_{i,j}$ where $A$ is the weighted adjacency matrix.
>
> - this is a bit "bad" because non-neighbors will have weight $0$ as it is adjacency matrix

Them, we can define **loss** to find the **embedding matrix $W$**:
$$
\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}||^2
$$
over all pairs of nodes in the graph

#### Multi-hop Similarity

> Instead of only considering immediate neighbor in $A$, we can consider $k$-hop neighbors by using $A^k$ to be the **adjacency matrix**. 
>
> - an improvement over adjacency based similarity

$$
\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}^k||^2
$$

#### Overlap Similarity

Another measure of similarity is the overlap between node neighborhoods as shown in Figure 7.9.

<img src="DL_notes/image-20220212002606862.png" alt="image-20220212002606862" style="zoom: 67%;" />

Suppose nodes $i$ and $j$ share common nodes. We can then minimize the loss function **measuring the overlap** between neighborhood:
$$
\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-S_{i,j}||^2
$$
where:

- $S_{i,j}$ could be Jaccard overlap or Adamic-Adar score

### Random Walk Embeddings

Essentially an alternative of shallow embedding.

> A **random** walk in a graph begins with a node $i \in V$ and repeatedly walks to one of its neighbors $v\in N(i)$ with probability $1
> /d(i)$ for $t$ steps until reaching and ending node $j$ on the graph.
>
> In essence:
>
> 1. given a graph and a starting point, we select a neighbor of it at random
> 2. move to this neighbor; then we select a neighbor of this point at random
> 3. move to it, etc. until $t$ steps are gone
>
> So $\text{similarity}(u,v)$ is defined as the probability that $u$ and $v$ **co-occur on a random walk** over a network. 

Formally:
$$
f(i)^Tf(j) \propto P(\text{$i$ and $j$ co-occur on the random walk}) = p(i|j)
$$
So basically we want to learn node embedding such that **nearby nodes are close together in the network**. Hence we want to maximize the likelihood of random walk co-occurrences, we compute loss function as:
$$
\mathcal{L} = \sum_{i \in V}\sum_{j \in N(i)} - \log p(j|f(i))
$$
for
$$
P(j|f(i)) = \frac{\exp(f(i)^T f(j))}{\sum_{j \in V} \exp(f(i)^Tf(j))}
$$
Then basically we want to find $W$ for $f(i)=We_i$ such that the loss can be minimized.

## Graph Embedding

We may also want to embed an entire graph $G$ or subgraph in some applications

<img src="https://snap-stanford.github.io/cs224w-notes/assets/img/graph_embedding.png?style=centerme" alt="GraphE" style="zoom:50%;" />

There are several ideas to accomplish graph embedding:

1. The simple idea (Duvenaud et al., 2016) is to run a standard graph embedding technique on the (sub)graph GG, then just sum (or average) the node embeddings in the (sub)graph GG.

   - i.e. taking the sum of the embeddings of the nodes in the sub-graph $\sum_{i \ni S} f(i)$

2. Introducing a virtual node to **represent** the (sub)graph and run a standard graph embedding technique

   <img src="DL_notes/image-20220212000807810.png" alt="image-20220212000807810" style="zoom:50%;" />

3. We can also use **anonymous walk embeddings**. In order to learn graph embeddings, we could enumerate all possible anonymous walks aiai of ll steps and record their counts and represent the graph as a probability distribution over these walks. To read more about anonymous walk embeddings, refer to *Ivanov et al., Anonymous Walk Embeddings (2018)*.

## Neighborhood Aggregation

Once embedding in done, we finally **start with GNN**.

We consider graph neural networks (GNNs) which ==take into account neighbors of each node== (i.e. their embeddings) 

- if you naively think of concatenating feature vector $f(i)$ into things such as adjacency matrix. Then this will be problematic as: what if you wanted to add a node to the graph? Then the graph size changed -> adj matrix size change -> new architecture.
- basically, the number of parameters is **linear in the size of the graph**, the network is dependent on the order of the nodes and does not accommodate dynamic graphs

> **Desired Properties**
>
> - Invariant to node ordering
> - Locality, operations depend on neighbors of a given node
> - Number of parameters independent of graph size
> - Model independent of graph structure
> - Able to transfer across graphs

The problem above can be **solved** by:

- aggregating information from neighboring nodes in a BFS manner

  <img src="DL_notes/image-20220212005141979.png" alt="image-20220212005141979" style="zoom:50%;" />

- aggregating information in a chain, in a DFS manner

  <img src="DL_notes/image-20220212005155734.png" alt="image-20220212005155734" style="zoom:50%;" />

Here we will discuss the first architecture, which can be visualized as:

- for each node in the graph in turn, 
- pick up the graph from that node as the root allowing all other nodes to dangle
- building a computation graph where that node is the root
- propagate and transform information from its neighbors, its neighbors neighbors etc, as shown below

<img src="DL_notes/image-20220212005458064.png" alt="image-20220212005458064" style="zoom: 50%;" />

where:

- the information to collect from each vertex would be their **node embedding**

- each grey box would contain **weights** for them to "transform"

Most graph neural networks are based on aggregating information into each node from it's neighboring nodes in a layer in the above manner:
$$
h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}
$$
so that essentially:

- $h_i^l$ is the feature representation of node $i$ **at layer $l$**
  - e.g $h_i^0$ is the 0 layer aggregation so $h_i^0 = f(i)$ is the raw embedding
  - the feature vector $h_i^{i-1}$ of the previous **layer embedding**
- essentially we are doing for each $l$:
  - aggregate embedding from **neighbors**
  - combining with your **previous embedding**

Next, we consider each node in turn, and generate a computation graph for each node where that node is the root. Finally, we will **share the aggregation parameters** across all nodes, for every layer of neighbors, as shown in Figure below:

<img src="DL_notes/image-20220212012049523.png" alt="image-20220212012049523" style="zoom: 80%;" />

where the grey boxes are the aggregation parameter/weights.

Formalizing with weights:
$$
h_{i}^l = \sigma\left(B^l h_{i}^{l-1}+W^l \sum_{j \in N(i)}\frac{h_j^{l-1}}{|N(i)|} \right)
$$
where:

- this is essentially a **recursive formula**, the base case is:
  $$
  h_{i}^1 = \sigma\left(B^1 h_{i}^{0}+W^1 \sum_{j \in N(i)}\frac{h_j^{0}}{|N(i)|} \right)=\sigma\left(B^1 x_{i}+W^1 \sum_{j \in N(i)}\frac{x_j}{|N(i)|} \right)
  $$
  for the $0$-th layer embedding is the same as the **raw node embedding $x_i=f(i)$**. Notice that ==embedding of the node itself== is included in this operation (along with its neighbors)!

- so technically each layer has two weights: **$B^l$ is a matrix of weights for self-embedding** and **$W^l$ for neighbor embedding**.

With this, we can now perform tasks such as **node classification**!

### Supervised Node Classification

For the task of node classification, given $m$ labeled nodes $i$ with labels $y_i$ we train a GNN by minimizing the objective:
$$
\mathcal{J}=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(y^i,\hat{y}^i)
$$
where the prediction $\hat{y}^i$ will be some neural network output based on the **layered embedding $h_i^l$** (at the last layer) we discussed before.

## GNN Architecture

Essentially each architecture varies by **how they perform aggregation**:
$$
h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}
$$

### Graph Convolution Network

A graph convolution network (GCN) (Kipf & Welling 2017) has a similar formulation using a single matrix for both the neighborhood and self-embeddings:\
$$
\begin{align*}
h_{i}^l
&= \sigma\left( \frac{1}{\hat{d}_i}W^l h_{i}^{l-1} + \sum_{j \in N(i)}\frac{\hat{A}_{i,j}}{\sqrt{\hat{d}_j \hat{d}_i}}W^lh_{j}^{l-1} \right)
\end{align*}
$$
where:

- where $\hat{A}= A+I$ is the adjacency matrix including **self loops**, $\hat{d}_i$ is the degree in the graph with **self loops**, and $\sigma$ a non-linear activation function

### Grated Graph Neural Networks

This is the second architecture mentioned in this section, which is similar to DFS: instead of sharing weights across neighborhoods (i.e. horizontally), **weights are shared across all the layers in each computation graph (vertically)**.

In gated graph neural networks (Li, Tarlow, Brockschmidt & Zemel 2016) nodes aggregate messages from neighbors using a neural network, and similar to RNNs parameter sharing is across layers:

<img src="DL_notes/image-20220212014256440.png" alt="image-20220212014256440" style="zoom: 67%;" />

# Transformers

While the addition of gates allows LSTMs/GRUs to handle more **distant information** than RNNs, they dont completely solve the underlying problem:

- passing information through an **extended series of recurrent connections** leads to information **loss**
  - i.e. "too distant" information are still lost

- Moreover, the inherently sequential nature of recurrent networks makes it **hard to do computation in parallel.**

> These considerations led to the transformers development of **transformers**  an approach to sequence processing that ==eliminates recurrent connections== and returns to architectures reminiscent of the fully connected networks
>
> - Transformers map sequences of input vectors $(x_1, ...,x_n)$ to sequences of output vectors $y_1,...,y_n$ of the **same length.**
>   - if our input is a sequence of tokens, then we can image each token represented as vector by $x_i=  Ew_i$ for an embedding matrix (e.g. see [SGNS Embeddings](#SGNS Embeddings))
> - Transformers are made up of stacks of transformer blocks, which are **multilayer** networks made by combining
>   - simple linear layers
>   - feedforward networks (i.e. NN)
>   - self-attention layers/multihead attention (key innovation)

## Self-Attention

> **Self-attention** allows a network to directly ==extract== and use information from ==arbitrarily large contexts== without the need to pass it through intermediate recurrent connections as in RNNs.
>
> - this means when processing each item in the input for $y_i$, the model **has access to all of the inputs up to and including** the one under consideration
> - moreover, the computation performed for each item is **independent of other computations**. This allows for ==parallel computation==!

> **Heuristics**:
>
> Given some data points you already know, e.g. $x_i, y_i$ being the **keys** and **values** 
>
> - then, you are **given a query** $x$, which you want to know the result $y$. The idea is to output:
>   $$
>   y = \sum_{i=1}^n a(x,x_i)y_i
>   $$
>   so essentially the output for a query $x$ will **be weighted average** for the $x_i$ (keys) that we already know. Since we can say that **$x_i$ near $x$ would be more important** to consider:
>   $$
>   \alpha (x,x_i) = \frac{k(x,x_i)}{\sum_{j}k(x,x_j)}
>   $$
>   for example $k$ kernel could be a **gaussian kernel**.
>
> Hence, attention is essentially $\alpha(x,x_i)$, a measure of **how relevant keys are to a query**

Graphically:

<img src="DL_notes/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" />

where notice that:

- inputs and outputs of the same length
- when processing each item in the input for $y_i$, the model **has access to all of the inputs up to and including** the one under consideration

Then, a **simple attention model** would be to consider $y_i$ being some **weighted version**:
$$
y_i = \sum_{j \le i} \alpha_{ij} x_j
$$
where $\alpha_{ij}$ aims to capture the **similarity/relevance** of input $x_j$ for the **current input $x_i$**:
$$
\alpha_{ij} = \text{softmax}( \text{score}(x_i, x_j) ) = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j \le i
$$
where

- the key aim is to have $\alpha_{ij}$ compare **an item of interest** (i.e. $x_i$) to a **collection of other items** (i.e. $x_j, \forall j \le i$) in a way that ==reveals their relevance== in the current context

- a simple example would be relevance = similarity:
  $$
  \text{score}(x_i, x_i) = x_i \cdot x_j
  $$

> **Intuition**
>
> Though the above is simplified version it represents the core of an attention-based approach:
>
> - a set of ==comparisons== to **relevant** items in some context
> - normalization of those scores to provide a probability distribution
> - the output of self-attention $y$ is a **weighted sum** of the inputs using the above distribution

Transformers allow us to create a more sophisticated way of representing **how words can contribute** to the representation of longer inputs. In essence, we will have **three input embeddings**:

- **query**: current focus of attention $q_i = W^Q x_i$
- **key**: the role of preceding input $k_j=W^Kx_j$, which will be compared against current focus $q_i$
- **value**: used to compute the output for the current focus $v_j = W^V x_j$

All the intermediate values are of dimension $d$, which means $W^Q,W^K, W^V$ **all will be $\mathbb{R}^{d\times d}$**

- this will be changed when we have a multi-headed attention, because technically we **only needed $W^Q, W^K$ to be in the same dimension**. (as we need dot products)

These **embedded inputs** are then used for:
$$
\begin{align*}
\text{score}(x_i, x_j) &= q_i \cdot k_j\\
\alpha_{ij} &= \text{softmax}( \text{score}(x_i, x_j) )\\
y_i &= \sum_{j \le i} \alpha_{ij} v_j
\end{align*}
$$
where:

- the second step is the same as in our simple model
- now $q_i \cdot k_j$ measures the **relevance** between a **query and key** (e.g. a SQL search *query*, and the data *keys* you have in your table)
- finally, the weighted sum of $v_j = W^V x_j$ consists of the focus $x_i$ itself outputs the result **value**

Graphically, for computing $y_3$ in our previous example:

<img src="DL_notes/image-20220207203739212.png" alt="image-20220207203739212" style="zoom: 50%;" />

note that:

- in reality, the dot product for score could be **very large**, so that having $ \text{softmax}$ later on rasing it to some power will cause overflow. Hence we usually do:
  $$
  \text{score}(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
  $$
  for $d_k$ being the **dimensionality of the query/key vector**.

Finally, we can convert everything to a **single matrix-matrix multiplication** as each $y_i$ is independent:
$$
Q = XW^Q; K = XW^K; V=XW^V
$$
where we are packing the input embeddings of the $N$ tokens/words of the input sequence into a single matrix $X \in \mathbb{R}^{N \times d}$.

- i.e. each **row vector** is the embedding of a token
- then $Q,K,V \in \mathbb{R}^{N\times d}$

Then the final output can be done in a **single shot of $Y \in \mathbb{R}^{N \times d}$** as:
$$
Y = \text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V
$$
where:

- this can be parallelized and is fast as it is just a matrix-matrix multiplication

- ==but for langue modelling in guess next word==, the part $QK^T$ compute the **full matrix**:

  ![image-20220207204740770](DL_notes/image-20220207204740770.png)

  but we obviously want $q_1 \cdot k_2$ to be not there (so that $ \text{Softmax}$ of this gives $0$). Hence we will need **upper-triangular values to be set to $-\infty$**. (otherwise we are "seeing the next word" when we want to predict the next word)

- this makes it clear that the computation is $O(N^2)$ for $N$ being the length of text you are inputting. Hence, generally you want to **avoid** putting in long texts such as Wikipedia pages/novels.

---

*For Example*:

Essentially what happens is that each output $y_t$ will take into **account the entire input sequence $x_i$**, but have **weights** on more relevant ones to the key (i.e. $x$) to spit out $y_t$ as **a weighted average of the values** (i.e. $y_i$).

- think of this as output $y$ being a weighted average of:
  $$
  y = \sum_i \alpha(x,x_i)\cdot y_i
  $$
  for $x$ being the query, $y$ being what we want, and $x_i,y_i$ are known inputs to be keys and values.

A concrete example would be. consider computing $y_{t=1}$ from three inputs of dimension $4$:

<img src="DL_notes/image-20220210164535973.png" alt="image-20220210164535973" style="zoom: 50%;" />

Then, essentially this is what happens:

1. Compute key, value, query representation of all input (from some embedding matrix $W^Q,W^K,W^V$)

   <img src="DL_notes/image-20220210164631592.png" alt="image-20220210164631592" style="zoom: 50%;" />

   so essentially query $q$ is the final question we are interested in

2. Calculate the **attention score** for $y_{t=1}$, essentially meaning how important each data key $k_1,...,k_3$ is relevant to the query

   <img src="DL_notes/image-20220210165117963.png" alt="image-20220210165117963" style="zoom: 50%;" />

   which basically is doing a SoftMax as mentioned before

3. Then, you use the attention score to **scale the values** (i.e. doing $\alpha(x,x_i)y_i$):

   <img src="DL_notes/image-20220210165421872.png" alt="image-20220210165421872" style="zoom: 50%;" />

4. Finally, you sum up the weighted values to spit out $y_{t=1}$:

   <img src="DL_notes/image-20220210165351646.png" alt="image-20220210165351646" style="zoom: 50%;" />

   (note that technically value would be of dimension $4$, so that the final output has the same dimension as input)

> **Take Away Message**
>
> Like RNN architectures that remembers information of a sequence, self-attention does it even **better** by **taking the entire input sequence into consideration** at each time for output.
>
> - this means parallelization of code
> - i.e. **each output** (e.g. token) will have attended to (with weights) the **entire input sequence**

## Transformer Blocks

The core of a transformer composes of **transformer blocks**, which essentially consists of:

- **self-attention** layer (e.g. a multihead attention layer = multiple self-attention layer)
- **normalization** layer
- **feedforward** layer

|                 Single Self-Attention Layer                  |                   Multihead Self-Attention                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220209004304005](DL_notes/image-20220209004304005.png) | <img src="DL_notes/image-20220209004414819.png" alt="image-20220209004414819" style="zoom:67%;" /> |

where here:

- we will mainly address the transformer block with single self-attention first.

From the left figure, we can summarize what we are doing as:
$$
\begin{align*}
z &= \text{LayerNorm}(x+ \text{SelfAttn}(x))\\
y &= \text{LayerNorm}(z+ \text{FFNN}(z))
\end{align*}
$$
where we are outputting $y$

- we have **Layer normalization** (or layer norm) because it can be used to **improve training performance** in deep neural networks by keeping the values of a hidden layer in a range that **facilitates gradient-based training**:
  $$
  \text{LayerNorm}(x) = \gamma \hat{x} + \beta
  $$
  for $\gamma, \beta$ are **learnable parameters** representing gain and offset, and $\hat{x}$ is the **normalized version of $x$**:
  $$
  \hat{x} = \frac{x-\mu}{\sigma}
  $$

- **residual connection**, as mentioned before, allows information from the activation going forward and the gradient going backwards to skip a layer **improves learning** and gives higher level layers **direct access to information from the past**

- **input and output dimensions** of these blocks are **matched**/the same so they can be stacked just as was the case for stacked RNNs.

(but what are the inputs $x$? You will soon see that the vector $x$ would come from $x+p$ which is the embedding of the input + positional embedding of the input)

> **Note**
>
> Now you may wonder what is the embedding used for tokens, i.e. $x$?
>
> - e.g. BERT uses Wordpiece embeddings for tokens. 
> - In fact, the full input embedding for a token is a **sum** of the **token embeddings**, the **segmentation embeddings**, and the **position embeddings**.

### Multihead Attention

Why are we not satisfied with single self-attention? A single word in a sentence can relate to each other in **many different ways** simultaneously!

- It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. (e.g. syntactic, semantic, and discourse relationships)
- Transformers address this issue with **multihead self-attention** layers.

Therefore, the idea is that we have sets of **self-attention layers**, called ==heads==, that **reside in a parallel fashion**. The aim is that we want **each self-attention layer/head** capture **different** aspects of the relationships that exists among inputs:

<img src="DL_notes/image-20220209010338126.png" alt="image-20220209010338126" style="zoom: 67%;" />

where essentially, in the end we need input and output **both** of **dimension $d$**:

- here we have four heads, $h=4$

- since we want to capture different relationships, **each head** consist of its **own set of key, query, value embedding matrix** $W_i^K,W_i^Q,W_i^V$ for head $i$

- then, what we do inside each head/self-attention is the same as what we have covered before

- remember that embeddings does not need to have the same dimension as input (which is $d$). Also recall that we **only needed key and query to be of the same dimension**, therefore, here we have:
  $$
  W_i^Q,W_i^K \in \mathbb{R}^{d \times d_k};\quad W_i^V \in \mathbb{R}^{d \times d_v}
  $$
  Then, if we pack them with inputs:
  $$
  Q_i = XW_i^Q \in \mathbb{R}^{N \times d_k}; \quad K_i = XW_i^K \in \mathbb{R}^{N \times d_k};\quad  V_i=XW_i^V\in \mathbb{R}^{N \times d_v}
  $$
  for **each** head $i$. Then the final output **for each head** is essentially **the same as mentioned before**:
  $$
  A_{i}(Q_i, K_i, V_i) = \text{SelfAttn}_i(Q_i,K_i,V_i) = \left[\text{Softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)\right]V_i
  $$
  which is of shape $N \times d_v$

- Remember that input of **$N$ tokens** of dimension $d$ has size $X \in \mathbb{R}^{N \times d}$. Now we have essentially $h \times N \times d_v$. Therefore, we perform:

  1. concatenating the outputs $A_i$ from each head
  2. using a linear projection $W^O \in \mathbb{R}^{hd_v \times d}$ for the concatenated outputs.

  Therefore we have:
  $$
  \text{MultiHeadAttn}(X) = [\text{head}_1 \oplus ... \oplus \text{head}_h] W^O\\
  \text{head}_i = \text{SelfAttn}_i(Q_i,K_i,V_i)
  $$
  so that the final output is of size $N \times d$, which is the same as input.

Then, since output size is the same as input size, we can stack them easily like:

<img src="DL_notes/image-20220209012338218.png" alt="image-20220209012338218" style="zoom: 67%;" />

where in this case the next layer is also a multihead attention layer.

- notice that the input is now $X+P$, which is input embedding + positional embedding, which we will cover next

### Positional Embedding

How does a transformer **model the position of each token** in the input sequence? With **RNNs**, information about the order of the inputs was ==built into== the structure of the model

- we want to learn the relative, or absolute, positions of the tokens in the input

**Solution**: modify input embedding by $X:= X+P$ for $P$ being ==positional embedding==.

For example, we can do the following being the **absolute positional embedding**:

<img src="DL_notes/image-20220209013143904.png" alt="image-20220209013143904" style="zoom:67%;" />

where:

- A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits
- here we are mapping each input embedding vector $x_i$ to a **scalar $p_i$**, which is absolute positional encoding as it is **relative to a global origin** of $0$.
  - problems of this include that numbers/encoding will be big for long sequences, which could cause exploding gradients
  - if we simply divide by sequence length, the problem is $4/5=8/10=12/15$ all equals $0.8$ but signifies different position.

It turns out that what we use is this
$$
P_{pos,2i} = \sin\left(\frac{pos}{10000^{2i/d}}\right),\quad P_{pos,2i+1} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$
where we essentially use a **combination of sine and cosine**, $\text{pos}$ is the position of word in the sentence

- (resource on explaining how we got there: https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)

## Encoder and Decoder

> **Encoder-decoder networks**, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences.
>
> - an **encoder** network that takes an input sequence and **creates a contextualized representation**, $c$
> - a **decoder** then takes that context $c$ which generates a **task specific output sequence**

To be more specific

- An **encoder** that accepts an input sequence, $x_1,...x_n$, and generates a corresponding sequence of contextualized representations, $h_1,...h_n$
- A **context vector**, $c$, which is a function of $h_1,...,h_n$, conveys the essence of the input to the decoder.
- A **decoder**, which accepts $c$ as input (first hidden state) and generates an arbitrary length sequence of hidden states $h_1,...,h_m$, from which a corresponding sequence of output states $y_1,...,y_m$, can be obtained

> The important thing of this idea is that LSTMs, convolutional networks, and Transformers **can all be employed as encoders or decoders**.

Therefore, you will soon see that there are three main types of transformers:

- encoder only
- decoder only
- encoder + decoder

---

*For Example*, as you might have seen before, consider the simple RNN encoder-decoder architecture for MT:

<img src="DL_notes/image-20220210183154677.png" alt="image-20220210183154677" style="zoom:67%;" />

where notice that:

- the **context vector** is simply the **last state of encoder** (you will see how useful Attention is soon)

- The **decoder** autoregressively generates a sequence of outputs (i.e. output of previous state becomes input of next state, like a regression), an element at a time, until an end-of-sequence marker is generated. 
- Each hidden state of decoder is conditioned on the previous hidden state and the output generated in the previous state

---

### Attention

This is also referred to as **encoder-decoder attention**

---

*Recall*: this is useful when we have a encoder and decoder of RNN/LSTM/GRU like the following:

<img src="DL_notes/image-20220209015006640.png" alt="image-20220209015006640" style="zoom:80%;" />

where:

- **without attention**, the input to decoder would **only be the last state** of encoder. So attention mechanism is a solution to the bottleneck problem, a way of allowing the **decoder** to get information from **all the hidden states of the encoder** (essentially by a weighted sum over all the states)
- then, the **weights $\alpha_{ij}$** will focus on (attend to) a particular part of the source text that is **relevant for the token the decoder is currently producing**.
  - this means that $c_i$ will be different for each token in decoding

---

> **Goal**
>
> We want to find a way that allows the **decoder** to get information from **all the hidden states of the encoder**, not just the last hidden state.
>
> - we can't use the entire tensor of encoder hidden state vectors directly as the context for the decoder, as the number of hidden states **varies** with the size of the input

Essentially, the idea of attention is similar to that of ==self-attention==, but the difference is that the **query** we care about is the **decoding state $h^d_t$**, whereas in self-attention both query and keys are the input.

- i.e. here we care about: **how relevant is each information from encoder $h^e_i$ to the current decoding step $h^d_t$?**

- therefore, the idea is the same: we are doing some kind of **weighted average of keys $h^e_i$**.

> *Recall*
>
> Given some query $x$, and we want the prediction $y$:
> $$
> y = \sum_{i=1}^n a(x,x_i)y_i
> $$
> **Attention** is essentially weights $\alpha(x,x_i)$, a measure of **how relevant keys $x_i$ are to a query $x$**

Therefore, the idea for **encoder-decoder attention** is to consider relevant between $h^e_j,\forall j$ and the current query $h_{i-1}^d$ (we want $h_i^d$, which is not computed yet)

1. the **decoder** receives as input the **encoder encoder output sequence** $c_i=c_i(h^e_1, ...,h^e_t)$
   - essentially everything the **encoder** has got
2. different parts of the output sequence **pay attention** to **different parts of the input sequence**
   - essentially, given that we want to compute $h^d_t$, how much weight should be give to each input/stuff we know $h^e_{j}, \forall j$

|                    Simple Encoder-Decoder                    |
| :----------------------------------------------------------: |
| <img src="DL_notes/image-20220210185934352.png" alt="image-20220210185934352" style="zoom: 67%;" /> |

where essentially:

- This context vector, $c_i$, is generated anew with **each decoding step $i$** and takes all of the encoder hidden states into account by a weight computed as:
  $$
  \alpha_{ij}  = \frac{\exp \left( \text{score}(h^d_{i-1},h^d_j ) \right)}{\sum_{k=1}^i \text{exp}( \text{score}(h^d_{i-1},h^d_j))}, \quad \forall j
  $$
  which obviously **depends on which output stage $i=t$ decoder is at**. Then, simply do:
  $$
  c_i = \sum_{j} \alpha_{ij}h_j^e
  $$
  being a weighted sum of all the encoder hidden states.

- then, obviously it is left for us to design a **score function** (e.g. dot product) that reflects the **relevance between $h_{i-1}^d,h^e_{j}$**, so that we could "pay more attention" to only specific $h_j^e$

Another more complicated example would be:

|              Encoder-Decoder with Bidirectional              |
| :----------------------------------------------------------: |
| <img src="DL_notes/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 50%;" /> |

where

- the context vector is a function $c_i=c_i(o_1, ...,o_t)$ instead, for $o_t= V\begin{bmatrix}h_t\\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$.

- specifically, in this **bidirectional RNN** as encoder, the context vector is computed as:
  $$
  c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T
  $$
  which is a **weighted sum** over **all hidden states in the encoder** (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:
  $$
  \alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j
  $$
  which basically represents the **amount of attention** output $o_i$ will give to input word $x_j$, for some score function.

## Word Embeddings

For more details, please refer to the **NLP notes** on the section "Vector Semantics".

The general idea is that:

- we want to replace 1-hot word representation with lower dimensional feature vector for each word
- we can do this by learning word embedding from **large unsupervised text corpus**
  - e.g. distribution models, logistic regression models (e.g. Word2Vec), etc.

The goal is to find **an embedding matrix $E$** that takes a one-hot encoded word and returns an embedding. The general approach **machine learning** takes is to do the following:

1. input: each word in a vocabulary being a one-hot encoded vector
2. initialize an embedding matrix $E$ randomly.
3. for each sentence/window of words, **mask one word out**
4. now, given the context words $w_c$, our job is to output **vector of probability** $y$ for the masked word, so that $y[i]$ corresponds to the probability that word $i$ in the dictionary is the masked word
   1. convert all words to $e_c = Ew_c$
   2. put it in a neural network, e.g. FFNN, to learn the weights and **update the embedding matrix $E$**
5. After minimization, output $E$

For instance, consider the following sentence in our training data:
$$
\text{Mary had a little lamb whose \textbf{fleece} was white as snow}
$$

We would like to mask the word *fleece* out. Then, we construct the following network:

<img src="DL_notes/image-20220210135056641.png" alt="image-20220210135056641" style="zoom:50%;" />

The idea is to **learn by predicting the masked word**, e.g. *fleece*

- what is the probability that the masked word (i.e. *fleece* is masked) given the current context words/sentences in the window
- $\theta$ will be the weights in the FFNN, which we don't care about in the end
- since we are just masking words in a sentence, we are creating a supervised training set from **unsupervised set**!

Once you trained your embedding matrix, you can even **fine tune that to your specific task/vocabulary**. For example:

<img src="DL_notes/image-20220210135530591.png" alt="image-20220210135530591" style="zoom: 50%;" />

where input will use an embedding matrix $E$ that we have trained before

- then use the pre-trained embedding and update that during gradient descent as well

  ```python
  tf.keras.layers.Embedding(
      input_dim=len(ENCODER.get_vocabulary())+2,
      output_dim=LSTM_param['units'],
      embeddings_initializer=keras.initializers.Constant(EMBEDDING_MATRIX), # pretrained embedding
      trainable=False # whether if to fine tune
  )
  ```

## Beam Search

The decoding algorithm we gave above for generating translations has a **problem** (as does the autoregressive generation we introduced in Chapter 9 for generating from a conditional language model:

1. at each time step in decoding, the output $y_t$ is chosen by computing a softmax over the set of possible outputs
2. then, **only** the vocabulary with the **highest probability** is picked
   - this is also called **greedy decoding**

Indeed, greedy search is not optimal, and **may not find the highest probability translation** as in the end we have a list of tokens.

For instance, consider the goal of **generating a sentence** from the decoder:

<img src="DL_notes/image-20220210200927842.png" alt="image-20220210200927842" style="zoom:67%;" />

where we see that:

- the **most probable sentence** should be "ok ok `</s>`" with probability $0.4 \times 0.7 \times 1$
- the **greedy** algorithm would have picked *yes* as the first word, which is suboptimal

Now, since this tree is growing as $t$ grows, this turns out to be an **exhaustive search** (dynamic programming will not work). This is obviously not good, so we instead consider the method called **beam search**:

1. instead of choosing the best token to generate at each timestep, we **keep $k$ possible tokens at each step**
2. At subsequent steps, **each** of the $k$ best hypotheses is extended incrementally by being passed to **distinct decoders**
   - i.e. if we have $k=2$ for the first level, then we will have **2 distinct decoders** for the next, as shown below
3. Each of these hypotheses is scored by $P(y_i | x,y_{<i})$, basically just multiplying the probability.
4. Then, we **prune the hypothesis** down to the best $k$, so there are **always only $k$ decoders**.

Graphically:

<img src="DL_notes/image-20220210201708295.png" alt="image-20220210201708295" style="zoom:67%;" />

where we chose $k=2$ as an example:

- at $t=2$, we have four hypothesis initially, but then we pruned down to $2$ again. Therefore, we always have only $k=2$ decoders at a time.

More details on how we compute the probability score:

<img src="DL_notes/image-20220210202257019.png" alt="image-20220210202257019" style="zoom:67%;" />

Then, we if use **log probability**, at each step, to compute the probability of a partial translation, we simply **add** the log probability of the prefix translation so far to the log probability of generating the next token:

<img src="DL_notes/image-20220210202356694.png" alt="image-20220210202356694" style="zoom: 67%;" />

where notice that:

- here we **terminated with 2 sentences of different length**. This maybe problematic as sentences with shorter length will have a higher probability

Therefore, we would consider some **normalization** against the length of sentences:

<img src="DL_notes/image-20220210202626578.png" alt="image-20220210202626578" style="zoom: 80%;" />

for a sentence of length $T$.

This ends up with the following algorithm:

<img src="DL_notes/image-20220210202738922.png" alt="image-20220210202738922" style="zoom:80%;" />

> **Note**
>
> What do we do with the resulting $k$ hypotheses? 
>
> - In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that. 
> - In other cases our downstream application might want to look at all $k$ hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores.

## Encoder-Decoder with Transformers

The encoder-decoder architecture can also be **implemented using transformers** (rather than RNN/LSTMs) as the component modules

- An **encoder** essentially consist of **stacks of transformer blocks**, typically $6$ layers
- A **decoder** as well consists of stacks of transformer blocks

For instance, it could look like the following:

<img src="DL_notes/image-20220210212353425.png" style="zoom:67%;" />

where we notice that:

- the only thing we don't know is **cross-attention**, which is also very **similar to the encoder-decoder attention**, which we have covered before.
- the **input of decoder** is the auto-regressive output of itself generated

---

**A more detailed visualization at decoding**:

Essentially for decode, we usually do it **auto-regressively** so that suppose our ==decoder== has `Attention` with `MAX_SEQ_LEN=32`. Then:

<img src="DL_notes/transformer_decoding_1.gif" style="zoom: 50%;" />

the input to decoder done **auto-regressively**:

- at $t=0$, there is no input/or we have `<pad><pad>...<s>` for filling the `32` sequence length and a positional embedding
- get cross attention from encoder output
- generate an output "`I`" and feed back as input at $t=1$. So we **get `<pad><pad>...<s>I` as the input of decoder**
- repeat until `</s>` is generated

> **Note**: Comparing with using a RNN based decoder, when we are generating output $o_{t+1}$, the *difference* is:
>
> - **transformer** based can **only condition/read in** $o_{t-31},...,o_t$ for a max sequence length of `32` for the attention layer
> - **RNN** based can use $h_t$ which ==encodes all previous information==. However, it has ==no attention/is sequential==.

---

To zoom in a bit, we see that

<img src="DL_notes/image-20220210212608845.png" alt="image-20220210212608845" style="zoom: 67%;" />

Now, we have:

- an **encoder** that takes the source language input words $X=x_1, ...., x_t$ and maps them to an output representation $H_{enc}=h_1,...,h_t$, as with the encoders discussed before

- a **decoder** then attends to the **encoder representation** and generates the target words one by one

  - here, at each timestep conditioning on the source sentence (weighted by attention) and the previously generated target language words (see previous figure, essentially output $h^d_{t-1}$ is piped in as input)

- **Cross-attention** has the same form as the **multi-headed self-attention** in a normal transformer block, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder. (i.e. the input/output is similar to the **encoder-decoder attention** discussed in [Attention](#Attention))
  
- **Self-attention** in **encoder** is allowed to look ahead at the entire source language text, i.e. weights will be distributed on the entire input sequence

- **Casual Self-attention** in **decoder** is what we have covered in the section [Self-Attention](#Self-Attention), so that we only have **previous label inputs available**:

  <img src="DL_notes/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" />

  (otherwise we will be cheating, as we are predicting the next word yet the next word is already available to us). This is sometimes also referred to as **left-to-right attention**, as it attends to input sequentially left to right.

Since the only thing "new" here is the cross attention, we focus on that. Essentially we need to consider what is $x,x_i,y_i$ analogue from the equation $\sum \alpha(x,x_i)\cdot y_i$ in this case.

- the **query**, i.e. $x$, will be sequence of **the output of previous decoder layer** $H^{dec[i-1]}$. We want to know the "answer" to this query

- the **key, value** will be the **final output of encoder** $H^{enc}=h_1 ,...,h_t$ (stuff we want to pay attention to)

Then, since it is similar to self-attention, we will have **weights** $W^Q,W^K,W^V$:
$$
Q=W^QH^{dec[i-1]};\quad K=W^KH^{enc};\quad V=W^VH^{enc}
$$
Then the rest is the same as the weights we calculated in self-attention:
$$
\text{CrossAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V
$$
The cross attention thus allows the decoder to attend to each of the source language words with weights that depend on current focus (query).

## Transformer Models

The big thing to distinguish between models would be:

- encoder/decoder or both?
- **what is the optimization objective**?
  - e.g. masked word to predict next, then likely a **decoder only transformer** (e.g. GPT)
  - e.g. masked word/fill in blank prediction and next sentence prediction, then likely a **encoder only transformer** (e.g. BERT)
- **what is the dataset** that it trained on?

### BERT

Lets begin by introducing the **bidirectional transformer** encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). 

> When applied to **sequence classification** and **labeling problems** causal models have obvious shortcomings since they are based on an incremental, **left-to-right processing** of their inputs.
>
> - Bidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown below
> - hence, it is not suitable for tasks such as predict the next word

|                      Causal Transformer                      |                  Bidirectional Transformer                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220210220737829](DL_notes/image-20220210220737829.png) | ![image-20220210220745062](DL_notes/image-20220210220745062.png) |

where essentially BERT is doing the one on the right.

- as you shall soon notice, BERT is a **encoder only transformer**

Essentially, inside BERT we have essentially stacks of transformer blocks for encoder/decoder:

<img src="DL_notes/image-20220210221236252.png" alt="image-20220210221236252" style="zoom:67%;" />

where:

- **bidirectional** encoders use self-attention to map sequences of **input** embeddings $x_1,...,x_n$ to sequences of **output** embeddings $y_1,...,y_n$ of the same length. Again, the aim is to produce **contextualized vectors** that takes in information from the entire input sequence

- the contextualization in **self-attention** is one in the same manner covered in section [Self-Attention](#Self-Attention), so the formulas are still:
  $$
  \text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V
  $$
  for input $X$, and **embedding matrix** $W^K,W^Q,W^V$ will be learned.
  $$
  Q = XW^Q; K = XW^K; V=XW^V
  $$
  However, the difference is that for **left-to-right** models, we had the matrix $QK^T$ being:

  |                        Left-To-Right                         |                        Bidirectional                         |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220210221709470](DL_notes/image-20220210221709470.png) | ![image-20220210221644852](DL_notes/image-20220210221644852.png) |

  so basically we removed the mask

- Beyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models

However, this is very useful because it is a multi-purpose model. best results in **many different tasks!**

- **Inputs** to the model are segmented using **subword tokenization** and are combined with **positional embeddings**. To be more precise, recall that for transformers, ==input dimension and output dimension are the same==:
  - The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings.
  - BERT uses ==Wordpiece embeddings== for tokens
- always remember, essentially what those model learn are **weights** $W$.

#### Training BERT

Since the entire context is available instead of trying to predict the next word, the model learns to perform a **fill-in-the-blank** task, technically called the **cloze task**.

- in fact, a lot of the models are trained in this format. Vision Transformer also does it by masking patches in the image and predicting the patch.

For instance, ==instead of== doing:
$$
\text{Please turn your homework \_\_\_\_}.
$$
For bidirectional transformers:
$$
\text{Please turn \_\_\_\_ homework in}.
$$
Therefore, the idea is that:

- during training the model is **deprived of one or more elements of an input sequence** and must generate/**output a probability distribution** over the vocabulary for **each of the missing items**
- then, it is essentially again a supervised training from self-supervised data, where loss would simply be **cross entropy.**

For example, an example architecture would look like:

<img src="DL_notes/image-20220210222426656.png" alt="image-20220210222426656" style="zoom:67%;" />

so we see that:

- a token part of a training sequence is used in one of three ways:

  - It is replaced with the unique vocabulary token `[MASK]`.
  - It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.
  - It is left unchanged.

  For BERT, 80% are masked, 10% replaced with randomly selected tokens, and 10% are unchanged

- training objective is to **predict the original inputs** for each of the masked tokens using a bidirectional encoder

- To produce a probability distribution over the vocabulary for each of the masked tokens, the **output** vector from the final transformer layer for **each of the masked tokens** is:
  $$
  y_i = \text{Softmax}(W_Vh_i)
  $$
  for some weights $W_V$ which will be learnt.

#### Transfer Learning through Fine-Tuning

The idea of pretrained model is simple. Consider a pretrained BERT (so that weights such as $W^K,W^V,W^Q$ are already learnt), and you want to use it to do tasks such as **sequence classification**

- e.g. sentiment analysis

The kind of question you need to think about is:

- transformer based encoders have **output shape same as input shape**. Therefore, essentially we can get embeddings for each **word** in the context. But here we would like to obtain an **embedding of the entire sequence** for classification. Where do we get that?
- Given an embedding of entire sequence, what should we do next?
- Can we **fine tune weights from pretrained models** as well?

In essence, this is what happens:

<img src="DL_notes/image-20220210224802368.png" alt="image-20220210224802368" style="zoom:67%;" />

where:

- We need an additional vector is added to the model to stand for the **entire sentence sequence**. This vector is sometimes called the **sentence embedding**. In BERT, the `[CLS]` token plays the role of this embedding. (i.e. the embedding of this token is what we want)
  - This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding

- onece we have this **sentence embedding**, we can pipe this into our own **neural network/logistic regression classifier**, which basically learns a weight $W_c$. Then, our output for **classification** essentially comes from:
  $$
  y = \text{Softmax}(W_C y_{cls})
  $$

- we can also **update the pretrained weights** during our fine-tuning as well: In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often **limited to updates over the final few layers** of the transformer.

---

*For Example*: Sequence Labeling

To demonstrate the multipurposeness of BERT, consider the task of **part-of-speech tagging** or **BIO-based named entity recognition**. Since we know that given a sequence of words, BERT produces a sequence of **embeddings**:

- the final output vector corresponding to **each input token embedding** is passed to a classifier (since each token will be tagged)
- then, we just need to **learn the weights in the classifier** $W_K$

So essentially:

<img src="DL_notes/image-20220210225529070.png" alt="image-20220210225529070" style="zoom:67%;" />

where 

- notice that the embedding for `CLS` is useless in this case

- our output tag for each token will come from:
  $$
  y_i = \text{Softmax}(W_K z_i)
  $$
  for output from BERT will be $z_i$, and then we can **greedily assign tags as**:
  $$
  t_i = \arg\max_k(y_i)
  $$
  for a tag consist of $k$ classes.

### GPT-2

The GPT-2 is built using **transformer decoder blocks**. BERT, on the other hand, uses **transformer encoder blocks**. 

We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, **outputs one token at a time**:

<img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif" alt="img" style="zoom:50%;" />

notice that:

- The way these models actually work is that after each token is produced, that **token is added to the sequence of inputs**. And that new sequence becomes the input to the model in its next step. This is an idea called **auto-regression**

Therefore, essentially the model contains:

<img src="DL_notes/image-20220210232055303.png" alt="image-20220210232055303" style="zoom:50%;" />

where:

- unlike BERT which uses bidirectional transformer as it wants to **encode contextualized information**, GPT cares more about **next word prediction**, hence it reverted to the **masked/left-to-right self-attention** during training:

  <img src="DL_notes/image-20220210232238810.png" alt="image-20220210232238810" style="zoom:50%;" />

- alike encoders of course, we need some **input to start with**

  <img src="DL_notes/image-20220210232603576.png" alt="image-20220210232603576" style="zoom:50%;" />

  it turns out that essentially you would also have input **embedded** before putting into your model.

> **Note**
>
> If you compare the architecture of encoder and decoder, they are kind of similar:
>
> |                         Encoder Only                         |                         Decoder Only                         |                       Encoder-Decoder                        |
> | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
> | <img src="DL_notes/image-20220210233157758.png" alt="image-20220210233157758" style="zoom: 67%;" /> | ![image-20220210233050700](DL_notes/image-20220210233050700.png) | <img src="DL_notes/image-20220210233259239.png" alt="image-20220210233259239" style="zoom:67%;" /> |
>
> where:
>
> - both encoder and decoder are transformer blocks, so **output shape same as input shape** (applications bascially only do something to the output of the last layer)
> - encoder and decoder used alone have a slightly different attention mechanism, with the latter being usually a **masked/left-to-right attention**
> - when you have encoder+decoder, you then need a **cross-attention** to get stuff across from encoder to decoder
> - the **output embedding** in the decoder is essentially the auto-regressive genreated output. i.e. at $t=0$, it is fed with `<s>` and suppose it generated `hello`. Then at $t=1$, fed with `<s>, hello`.

### Efficient Transformers

One problem now is that **attention computation** is quadratic in the length of sentences.

**Solution**: linear time transformer by approximations, transfer learning

<img src="DL_notes/image-20220210234034616.png" alt="image-20220210234034616" style="zoom:67%;" />

# Generative Adversarial Network

Generative adversarial networks (GANs) are an **unsupervised generative model** used for ==generating samples== that are similar to training examples. GANs have also been used for generating images from text, generating audio from images, and generating images from audio, etc.

- we are having a generator that **learns the structure of the data**

> **Summary**
>
> Essentially you have **two models**, a generator and a discriminator:
>
> - **generator** is trained to produce fake examples that fool the discriminator, hence its loss is based on the success of  discriminator
> - **discriminator** learns to distinguish between the generator's fake synthesized samples and real data
>
> Therefore, you will have essentially a **minimax optization problem** that looks like:
> $$
> \min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]
> $$
>  where essentially:
>
> - discriminator outputs a **probability** (that it is real) given some data $x$, and generator **generates** some data $G(z)$ from **noise** $z$ from some distribution (e.g. Gaussian)
> - the loss says discriminator $D(x)$ should be as high as possible, so that discriminator is doing well, and $D(G(z))$ should be as high as possible as well, so that generator is doing well
> - The generator and the discriminator are trained alternately, for $G(z)$ being descent first, and then $D(x)$.
>
> Then, essentially we update the two models with the two losses:
>
> - For **generator**, the the gradient step is:
>   $$
>   \nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}
>   $$
>   in some other algorithm, we could maximize $\log D(G(z))$ instead
>
> - For **discriminator**, we will have the same as maximization problem as before, so the gradient step is:
>   $$
>   \nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}
>   $$
>
> In practice
>
> - once we finished training, we will take the generator to deploy in our various appications.
>
> - relevant problems include **saturation problem**, **mode collapse** and **convergence problem**, which will be discussed in more detail later.

Note that:

- The idea of minimax is not new. In game theory, we think about *minimizing* the loss involved when the **opponent** selects the strategy that gives *max*imum loss
- from a biological point of view, this is essentially co-evolution
- The discriminator and generator form two dueling networks with opposite objectives. This **unsupervised** setting eliminates the need for labels, since the **label is whether the sample is real or not**.

On a bigger context, since we are trying to learn the given data distribution:

- we can learn it **explicity** 
  - VAE
  - Markvo Chain
- **implicitly**
  - GAN

## Minimax Optimization

Graphically, this is what the architecture looks like

<img src="DL_notes/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 67%;" />

the joint optimization problem for the two agents is:
$$
\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]
$$
where we are taking **expected value** over real data, but in **practice** we can only do empirical mean

- The term $D(x)$ is the discriminators estimated probability that **(real) data $x$ is real**
  - The goal of the discriminator is to classify correctly real and generated data.
- $G(z)$ is the output of the generator given random noise $z$.
  - the goal is to generate a signal from random noise $z \sim P(z)$ in a way that it will be difficult for the discriminator to distinguish
- $D(G(z))$ is the discriminators estimated probability that a **synthesized sample is real**.
  - the goal of geneartor is to make $D(G(z)) \to 1$


Since we care about the **network parameters** in the end:
$$
\min_\phi\max_\theta V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]
$$
then **how do we optimize this**?

- Since both the generator and discriminator will be represented by neural networks, problem is non-convex and non-concave
- hence we can only do **gradient descent type solution**
- The first term of $\log D(x)$ is independent of the generator and therefore the **generator only minimizes** the function $\log(1  D(G(z)))$.

However, there is a problem with this loss. In practice, it saturates for the generator, meaning that the generator quite frequently **stops training** if it doesnt catch up with the discriminator, so that:
$$
\nabla_\phi \log(1-D_\phi(G_\theta(z))) \to 0
$$
when the **discriminator becomes optimal** ($D$ is close to $D^*$). So gradient vanishes and **generator** cannot evolve!

### Mode Collapse

This issue is on the unpredictable side of things. It wasnt foreseen until someone noticed that the generator model could only generate one or **a small subset of different outcomes** or modes, instead of a range of outputs.

> If at each iteration the generator optimizes for the specific discriminator and the **discriminator is not able to correctly classify the synthesized samples** as fake, the generator will synthesize a small set of samples, not diverse samples, which is known as **mode collapse**

This may happen when:

1. In the process of training, the **generator is always trying to find the one output** that seems most plausible to the discriminator.
2. if the **next generation of discriminator gets stuck** in a local minimum and doesnt find its way out by getting its weights even more optimized, itd get easy for the next generator iteration to find the most plausible output for the current discriminator. 
3. This way, it will keep on repeating the same output and refrain from any further training.

When the discriminator using **Wasserstein loss** does not get stuck in local minima it learns to reject the generators repeated synthesized samples, encouraging the generator to synthesize new samples and diversify.

One solution is that:

- In order to void mode collapse, encouraging the generator to diversify the synthesized samples and not optimize for a constant discriminator, the **generator** loss function may be modified to include **multiple subsequent discriminators**

### Convergence Problem

The utopian **situation where both networks stabilize and produce a consistent result is hard to achieve in most cases.** 

- If the generator succeeds all the time, the discriminator has a 50% accuracy, similar to that of flipping a coin. This poses a threat to the convergence of the GAN as a whole.

If discriminator is just randomly guessing, then as the **discriminators feedback loses its meaning** over subsequent epochs by giving outputs with equal probability, the **generator may deteriorate its own quality** if it continues to train on these junk training signals.

## Divergence between Distributions

> **Key Question**:
>
> **Why** did we choose a loss function in the form of:
> $$
> \begin{align*}
> V(D_\phi,G_\theta)
> &=\mathbb{E}_{x \sim P_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim P_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]\\
> &=\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]
> \end{align*}
> $$
> specifically, why using $\log$ probability?
>
> The short **answer** is:
>
> - loss function here represents the **distance** between the **distribution** of the synthesized samples and the distribution of the real data
> - so essentially we want to **minimize the difference** between our approximate distribution $P_G$ from generator and the true distribution $P_{data}$
>
> In fact, if $D_\theta(x)$ can be any function, it can be shown that the above is **equivalent to minimizing the JS divergence** between **distribution $P_{data}$ and $P_{G}$**.
>
> - for more detail, checkout https://colinraffel.com/blog/gans-and-divergence-minimization.html

Essentially, our goal is to **minimize the distribution divergence** between true distribution $P_{data}$ and our generated one $P_G$. To do this, we need a metric to measure **"distance/difference"** between two distributions, and the simplest one would be the KL divergence.

### KL Divergence

KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data, i.e. how many **bits do we need to specify the data**. This idea is captured in Entropy:
$$
H = -\sum_{i=1}^N p(x_i) \cdot \log p(x_i)
$$
where:

- if $\log_2$, then we can interpret entropy as "the minimum number of bits it would take us to encode our information".
- just looking at extreme values, if $p(x_i)=0$ or $p(x_i)=1$, then there is nothing random and entropy is $0$. For $p(x_i)=0.5$, entropy is large.

Then, KL divergence is basically a measure of "**how many bits of information we expect to lose**" if we use $q(x_i)$ to approximate $p(x_i)$:
$$
D_{KL}(p||q) = \sum_{i=1}^N p(x_i)\cdot (\log p(x_i) - \log q(x_i))=\sum_{i=1}^N p(x_i)\cdot \log\frac{p(x_i)}{q(x_i)}
$$
Then:

- this is essentially the same as saying:
  $$
  D_{KL}(p||q)=\mathbb{E}[\log p(x) - \log q(x)]
  $$
  which reminds us of the **original GAN loss**.

- for a **continuous distribution**:
  $$
  D_{KL}(p||q)=\int p(x)\cdot \log\frac{p(x)}{q(x)}\,dx
  $$

- a **reversed KL divergence** is then:
  $$
  D_{KL}(q||p)=\int q(x)\cdot \log\frac{q(x)}{p(x)}\,dx
  $$
  for $q$ being our approximate distribution.

- therefore, this means KL divergence is **non-negative and asymmetric**



The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$

### Jensen-Shannon Divergence

The Jensen-Shannon (JS) divergence $D_{JS}$ is a symmetric smooth version of the KL divergence defined by:
$$
D_{JS}(p||q) = \frac{1}{2}D_{KL}(p||m)+\frac{1}{2}D_{KL}(q||m),\quad m=\frac{1}{2}(p+q)
$$
The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$. In fact, this formulation of divergence metric leads to the **formulation of loss**:
$$
\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]
$$

### Bregman divergence

The KL divergence and JS divergence are both **special cases of the Bregman divergence**. 

The Bregman divergence is defined by a convex function F and is a **measure of distance between two points $p$ and $q$** defined by:
$$
D_F(p,q)= F(p)-F(q)-\lang \nabla F(q), p-q \rang
$$
where then, many **divergence metrics** come from defining some **convex function $F$**:

- if $F(p)=p\log p$, then we recover the **generalized KL divergence**:
  $$
  \begin{align*}
  D_F(p,q)
  &= p\log p - q \log q - (\log q + 1)(p-q)\\
  &= p \log(\frac{p}{q}) + (q-p)
  \end{align*}
  $$

- if $F(p)=p\log p - (p+1)\log(p+1)$, we get **JS divergence**.

- if $F=(1-p)^2$, we get a **Pearson $\chi^2$ divergence**, which leads to ==loss in least square GAN==.

  The loss for discriminator being:
  $$
  \mathbb{E}_{x \sim P_{data}}[D_\phi(x)^2]-\mathbb{E}_{z \sim p_{z}(z)}[D_\phi(G_\theta(z))^2]
  $$
  and the loss for geneator:
  $$
  \mathbb{E}_{z \sim p_{z}(z)}[(D_\phi(G_\theta(z))-1)^2]
  $$
  providing a smoother loss

### Optimal Objective Value

Now, we come back to our discussion of the original loss function. Since the original loss is essentially **minimizing JS divergence** between $P_{data}$ and $P_{G}$, we obviously want the optimal solution to be $P_G=P_{data}$.

- in practice, this is **not computable** since it involves knowing the true distribution. Therefore, in practice we just do **gradient descent type updates**.

Here, we show that the above statement is true. For **discriminator**, we want:
$$
\max_\phi V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{\hat{x}\sim p_G}[\log (1-D(\hat{x}))]
$$
Hence:
$$
\mathcal{L}(x;\theta) = P_{data}\cdot \log D(x) + P_G \cdot \log(1-D(\hat{x}))
$$
taking derivative and setting it to zero, we obtain:
$$
D^*(x)=\frac{P_{data}}{P_{data}+P_G}
$$
Then plugging this back in to solve for **generator**:
$$
\min_G  V(G,D^*) = 2D_{JS}(P_{data}||P_G)-2\log 2
$$
Then if you plugin to the equation for JS divergence, you will realize that this is **minimized** if $P_{data}=P_G$.

## Gradient Descent Ascent

Since in reality we don't know the true distribution, we only have samples from it, we can only **update our guesses** in a gradient descent type of manner as the overall loss is non-convex and non-concave.

In our setting we use a stochastic variant of GDA with mini-batches, in which the ==descent== update for the **generator** neural network is:
$$
\nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}
$$
so that we have performed the **$\min_G$** step, and then our **discriminator** will have an ==ascent== update:
$$
\nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}
$$
note that:

- If $V$ were convex-concave then playing the game simultaneously or in a sequential order would not matter; however, in our case $V$ is non-convex non-concave and the **order matters**

- Unfortunately, GDA may converge to points that are not local minimax or fail to converge to a local minimax. A modification of GDA (Wang, Zhang & Ba 2020) which partially addresses this issue is:

  <img src="DL_notes/image-20220211184103651.png" alt="image-20220211184103651" style="zoom:67%;" />

  which is a modified gradient ascent rule for $f=V$. This converges and ==only converges to local minimax points==, driving the gradient quickly to zero and improving GAN convergence

### Optimistic Gradient Descent Ascent

When introduced, GANs were implemented using momentum. However, later on the implementations did not use momentum, and using a **negative momentum made the saturating GAN work**. An algorithm which solves the minimax optimization problem by using negative momentum is **optimistic gradient descent ascent (OGDA)** (Daskalakis et al. 2017).

- this is fonud in empirical experiences

The negative momentum update is:

<img src="DL_notes/image-20220211184326881.png" alt="image-20220211184326881" style="zoom: 67%;" />

where $f=V$ in our case.

- the one with $x$ is for gradient descent, the one with $y$ is for gradient ascent
- OGDA yields better empirical results than GDA, and can be interpreted as an approximation of the proximal point method

## GAN Training

When the generator training is successful the discriminator cannot distinguish between real data and fake samples synthesized
by the generator. So baically the architecture is as follows:

<img src="DL_notes/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 50%;" />

where both generator and discriminator are represented by neural networks and are both trained by backpropagation.

The algorithm for training is as follows, essentially we train them ==alternatingly==

<img src="DL_notes/image-20220211184827501.png" alt="image-20220211184827501" style="zoom:50%;" />

where the end goal is **we get good quality images from the generator** and discriminator wont be able to differentiate between real and fake images.

- Training the **discriminator** uses real data as positive examples and samples synthesized by the generator as negative examples
  - When the discriminator is trained, the ==generator is not trained== and its parameters are held fixed
  - discriminator loss serves as a signal to the generator for updating its parameters by backpropagation
- the **generator** learns to synthesize realistic samples by the feedback it receives from the discriminator
  - During generator training the ==discriminator parameters are held fixed==.

- we could have also reversed the training to train generator first, but the result will be difference as order matters.

## GAN Losses

As described, different Bregman divergences and loss functions have been explored with the goals of improving GAN training stability and diversity. Here we describe a few that is important.

### Wasserstein GAN

One problem with JS divergence is that, if the real data distribution and generator distribution do not overlap then the JS divergence is zero $D_{JS} = 0$

- when distributions have non-overlapping support
- (support of a function is a **closure** $S=\{x \in \mathbb{R}^n:f(x) \neq 0\}$)

Graphically:

<img src="https://i.stack.imgur.com/NbLrO.png" alt="enter image description here" style="zoom: 40%;" />

Fortunately, this issue has been **resolved** by using the Earth Movers Distance (EMD) or **Wasserstein-1 distance**:

<img src="DL_notes/image-20220211200143137.png" alt="image-20220211200143137" style="zoom: 67%;" />

where:

- where $\gamma$ denotes how much mass, or earth, must be moved from $x$ to $y$ in order to transform distribution $P$ into distribution $Q$,
- $\Pi (P,Q)$ denotes the set of all disjoint distributions with marginals $P$ and $Q$.

Graphically, we are doing:

<img src="DL_notes/image-20220215142125264.png" alt="image-20220215142125264" style="zoom:33%;" />

where $P_r$ is the real distribution, and the distance between $P_r,P_\theta$ is to measure how many "piles of dirt" dow we need to shovel from $P_r\to P_\theta$ so that the distributions are the same.

Computing $W(P,Q)$ is **intractable** since it requires considering all possible combinations of pairs of points between the two distributions, computing the mean distance of all pairs in each combination, and taking the minimum mean distance across all combinations. 

Fortunately, an alternative is to **solve a dual maximization problem** that is tractable, which results in the ==Wasserstein loss==:
$$
\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[ D(x)]-\mathbb{E}_{z \sim p_{z}(z)}[D(G(z))]
$$
if you use WGAN, which is Wasserstein loss:

- this loss outputs **a real value** that is **larger for real data** than **synthesized** samples
  - as compared to the original GAN uses the minimax loss in which the discriminator outputs a probability in $[0, 1]$ of a sample being real or synthesized
  - therefore, the WGAN discriminator is called a critic since it does not output values in $[0, 1]$ for performing classification
- There is no sigmoid in the final layer of the discriminator and the range is $[\infty,\infty]$.

Then, this means that the **generator** loss functions is:
$$
\min_G -\mathbb{E}_{z \sim p_{z}(z)}[D(G(z)]
$$
The **discriminator** loss function is the entire loss.

> **Note**
>
> Recall that mode collapse happens when a **discriminator hits a local minium**. Then, if it fails to reject the fake sample from generator, **geneartor would only generate a subset of the distribution**, which is mode collapse.
>
> However, Wasserstein loss ==prevents discriminator from hitting a local minimum==, hence allowing it to learn to escape.

### Loss Function Summaries

<img src="DL_notes/image-20220215140041605.png" alt="image-20220215140041605" style="zoom: 50%;" />

## GAN Architectures

Firs, an example of the choice of generator/discriminator

<img src="DL_notes/image-20220215140610509.png" alt="image-20220215140610509" style="zoom: 50%;" />

where:

- this would be an example of generator (left to right). Discriminator *could* be the same architecture but left to right
- therefore, the generator above is also called a **transposed CNN**

---

Below are some nowadays application of the GAN idea

- **Progressive GAN**: A coarse-to-fine approach for training allows generating images at increasing resolutions.

  - training the generator and discriminator using low-resolution images and incrementally add layers of higher-resolution images during training.

- **Deep Convolutional GAN**: A GAN that uses convolutional neural networks (CNNs) as the generator and discriminator.

  - using a CNN as the discriminator network and a deconvolution neural network as the generator

- **Semi-Supervised GAN**: Instead of having the discriminator be a binary classifier for real or fake samples, in a semi-supervised GAN (SGAN) the discriminator is a multi-class classifier

  - recall that all the GANs are normally unsupervised, as labels is just synthetic images or real ones
  - The discriminator outputs the likelihood of sample to be synthesized or real, and if the sample is *classified as real* then the discriminator outputs the *probability of the $k$ classes*, estimating to which class the sample belongs

- **Conditional GAN**: models the conditional probability distribution $P(x|y)$ by training the generator and discriminator on *labeled* data, with labels being $y$.

  - Replacing $D(x)$ with $D(x|y)$ and $G(z)$ with $G(z|y)$ for the loss, you get conditional GAN
    $$
    \min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z|y)))]
    $$

  - Providing labels allows us to ==synthesize samples in a specific class== or with a specific attribute, providing a level of control over synthesis

- **Pix2Pix:** Image-to-Image Translation

  - Given a training set of unfiltered and filtered image pairs $A : A'$ and a new unfiltered image $B$ the output is a filtered image $B'$ such that the analogy $A:A'::B : B'$ is **maintained**

  - An input image is mapped to a synthesized image with different properties. The loss function is a **combination of the conditional GAN loss** with an additional loss term which is a **pixel-wise loss** (i.e. sum of loss per pixel on the image) that encourages the generator to match the source image:

    <img src="DL_notes/image-20220211203343602.png" alt="image-20220211203343602" style="zoom:67%;" />

    with weight $\lambda$

- **Cycle Consistent GAN**: learns unpaired image-to image translation using GANs *without pixel-wise correspondence*

  - The training data are image sets $X \in A$ and $Y \in A'$ from two different domains $A$ and $A'$ **without pixel-wise correspondence** between the images in $X$ and $Y$.

  - CycleGAN consists of 

    - two generators $G(X) = \hat{Y}$ and $F(Y ) = \hat{X}$ 
    - two discriminators $D_Y$ and $D_X$. 

    The generator $F$ maps a real image $X$ to a synthesized sample $\hat{Y}$ and the discriminator $D_Y$ compares between them

    <img src="DL_notes/image-20220211204326281.png" alt="image-20220211204326281" style="zoom:50%;" />

  - CycleGAN maintains two approximate cycle consistencies:

    - The first cycle consistency  $F(G(X)) \approx X$ approximately maintains that mapping a real image $X$ to a synthesized image $\hat{Y}$ and back is similar to $X$
    - the second cycle consistency $G(F(Y )) \approx Y$ approximately maintains that mapping a real image $Y$ to a synthesized image $\hat{X}$ and back is similar to $Y$.

    Graphically, this is one of the consistency loop

    <img src="DL_notes/image-20220215143023657.png" alt="image-20220215143023657" style="zoom: 50%;" />

  - The overall loss function is defined by (Zhu et al. 2017):

    <img src="DL_notes/image-20220211204534646.png" alt="image-20220211204534646" style="zoom:67%;" />

    where the cycle consistency loss is defined by:
  
    <img src="DL_notes/image-20220211204602542.png" alt="image-20220211204602542" style="zoom: 67%;" />
  
    which is weighted by $\lambda$

## Evaluation and Application

After training the generator so that discriminator cannot do any better, there is still the chance that **generated samples are garbage** if discriminator is garbage. Therefore, we have some metrics to **evaluate how well generator's end product** is doing.

The Inception Score (IS) and Frechet Inception Distance (FID) measure the **quality of synthesized examples** using **pre-trained** neural network classifiers

- **Inception Score**: The Inception Score (IS) (Salimans et al. 2016) automatically evaluates the quality of images synthesized by the generator by using the **pre-trained Inception v3 model** (Szegedy et al. 2016) for classification.
  - A **higher** Inception Score is **better**, which corresponds to a larger KL divergence between the distributions.
- **Frechet Inception Distance**: The Frechet Inception Distance (FID) is also based on the Inception v3 modally. 
  - The FID uses the feature vectors of the last layer for real and synthesized images to generate multivariate Gaussians that model the real and synthesized distributions
  - A **lower** FID is **better**, which corresponds to similar real and synthesized distributions.

Finally, GAN architectures can be applied in numerous fields such as:

- Image Completion
- Super Resolution and Restoration
- Style Synthesis
- De-Raining
- Text-to-Image Synthesis
- Music Synthesis
- etc.

# Deep Variational Inference

This chapter begins with a review of variational inference (VI) as a fast approximation alternative to Markov Chain Monte Carlo (MCMC) methods, solving an optimization problem for **approximating the posterior**

- Amortized VI leads to the **variational autoencoder (VAE)** framework which is introduced using deep neural networks and graphical models and used for learning representations and generative modeling.

The setup is as follows. Consider we are given some **data $x$**, so that we can visualize this as a probability distribution $p(x)$:

<img src="DL_notes/image-20220215194410664.png" alt="image-20220215194410664" style="zoom:50%;" />

Now, suppose that in reality, this is the actual data:

<img src="DL_notes/image-20220215194455483.png" alt="image-20220215194455483" style="zoom:50%;" />

where essentially:

- $z \in [1,2,3]$ is a ==hidden variable== that signifies the three cluster within this data distribution
- Essentially, the real distribution $p(x)$ is actually coming from both $x$ and $z$. But only $x$ is observed, and we **do not know what $z$ is in advance**

---

More formally, the problem task is as follows.

- our target model is $p_\theta (x)$
- our data is $D = \{x_1, x_2, ...,x_N\}$

- we **want to model hidden variables as well**

Therefore, our **maximum likelihood fit becomes**:
$$
\arg\max_\theta \frac{1}{N}\sum_i \log p_\theta (x_i) \to \arg\max_\theta \frac{1}{N}\sum_i \log \left( \int p_\theta(x_i|z)p(z) dz\right)
$$
which is **problematic** we would have needed to compute $\int p_\theta(x_i|z)p(z) dz$ for every data.

Therefore, the idea is:

1. observe some data $x$, and take a guess **on the underlying $z$** (e.g. this pile of data belongs to this cluster)
2. construct "fake labels $z$" for your each of your data $x_i$
   - technically you would guess a $p(z|x_i)$ over it
3. do maximum likelihood on that $x_i$ and $z$ associated

Then, the kind of objective you want to do would be:
$$
\arg\max_\theta \frac{1}{N} \sum_i  \mathbb{E}_{z \sim p(z|x_i)}[\log p_\theta (x_i,z)]
$$
so you are **maximizing over the joint**, so that once done:

- obtain a model $\theta$, but also get information on $p(z|x_i)$.
- but we are taking an average/expected value over $p(z|x_i)$, ==how do we calculate this==?

## Markov Chain Monte Carlo

The idea is simple. Our task is to compute some **posterior $p(z|x_i)$** for some observation $x_i$. For this section, imagine that you are observing a **coin**, and you want to model $p(\theta |x)$ for $\theta$ being the probability of getting a head.

- essentially $\theta=z$ is some **hidden information from us**, but carries essential information on how data is generated
- the only observables are $x$

Using Bayes, we know that:
$$
\underbrace{p(\theta |x)}_{\text{posterior}} = \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}\,\,\frac{1}{p(x)}
$$

> *Intuition*
>
> We can take a **guess on the prior distribution $p(\theta)$**, and with observables available, we can **compute $p(x|\theta)$**
>
> - e.g. for a coin toss, guess that $p(\theta) = \text{Unif}[0,1]$, and $p(x|\theta)=p_\theta(x) \sim \text{Bern}(\theta)$
> - essentially, both quantities are now known
> - however we often **cannot compute $p(x)$**, since would involve computing some nasty integrals. Hence, we need some technique to estimate $p(\theta | x)$ when we only know $p(x|\theta)p(\theta)$

---

*For Example*: Coin Toss

The likelihood $p(x=4|\theta)$=probability of getting four heads with $\text{Bern}(10,\theta)$, mewing that our traying data has four heads in total. The priors are beta distributions which we guessed. Then, the posterior is the product of the two:

|                  Less Data (for likelihood)                  |                  More data (for likelihood)                  |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220217152847817](DL_notes/image-20220217152847817.png) | ![image-20220217152855335](DL_notes/image-20220217152855335.png) |

where the key observation here is that:

- if **prior is informative**, then we don't need very good likelihood data to obtain a reasonable estimate for posterior
- if **likelihood is informative**, i.e. we have lots of data, then we don't need to have a very informative prior

---

Now, lets get back to the task of finding out the **posterior $p(\theta|x)$**, or in general $p(z|x)$.

> *Intuition*:
>
> We do know that
> $$
> \underbrace{p(\theta |x)}_{\text{posterior}} \propto \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}
> $$
> So the idea is that:
>
> 1. pick some proposal distribution $q(\theta)$
>    - e.g. a normal distribution
> 2. sample a data point from the proposal distribution $\theta^*$
> 3. ==save this data point $\theta^*$ if $p(x|\theta^*)p(\theta^*)$ is very likely==, reject it otherwise
> 4. repeat step 2 to 3
>
> Then, you end up with lots of $\theta_i$, and it should **resemble the posterior distribution $p(\theta|x)$** due to the save/rejection step.

In more details, consider the following **MCMC Metropolis Hastings** algorithm

1. select some initial value $\theta_0$ to start with

2. for $i=1,...,m$ do:

   1. **Markov**: update the proposal distribution $q(\theta^*|\theta_{i-1})$, based on the previous sample

   2. **Monte Carlo**: pick a candidate $\theta^* \sim q(\theta^* | \theta_{i-1})$

   3. consider whether or not to accept the candidate by considering:
      $$
      \alpha = \underbrace{\frac{p(\theta^*|x)}{p(\theta_{t-1}|x)}}_{\text{can't compute}} = \underbrace{\frac{p(x|\theta^*)p(\theta^*)}{p(x|\theta_{i-1})p(\theta_{i-1})}}_{\text{unknown}}
      $$
      if $\alpha \ge 1$, **accept $\theta^*$** so that $\theta_i \leftarrow \theta^*$

      if $0 < \alpha < 1$, accept $\theta^*$ with **probability $\alpha$**, else **reject** it.

The resulting drawn distribution of $\theta^*$s will assemble the posterior distribution $p(\theta|x)$.

- notice that if our proposal $q$ is **very close to $p(\theta|x)$**, then **many samples will be accepted** and we converge fast
- otherwise, we may need a lot of time.

Graphically, if we picked 

- proposal $q(\theta^*\theta_{i-1}) = \mathcal{N}(\theta_{i-1},1)=\text{Beta}(1,1,\theta_{i-1})$,
- likelihood $\text{Bin}(10,\theta)$ and we **observed 4 heads** in our data

|                         5 Iterations                         |                       5000 Iterations                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220217190211945](DL_notes/image-20220217190211945.png) | ![image-20220217190327902](DL_notes/image-20220217190327902.png) |

However, the problem with this approach is that **sampling and adjusting takes time**, and for large number of sampes, it becomes very slow.

- this introduces us with the following sections on using ML models such as VAE to solve the **inference problem of $p(\theta|x)=p(z|x)$** in general.

## Variational Inference

We begin with:

- **observed data $x$**, continuous or discrete
  - e.g. $x$ may be an image of a face
- suppose that the process **generating the data** involved **hidden latent variables $z$**
  - e.g. $z$ a hidden vector describing latent variables such as pose, illumination, gender, or emotion
  - e.g. $z$ could represent cluster information

And you see that constructing a model to model both $x_i,z$ requires knowledge over:
$$
p(z|x_i)
$$
which is what this method is **attempting to solve**.

> **Summary**
>
> Our aim is to **estimate** $p(z|x_i)$: for example, answering the question what are the hidden latent variables (poses, gender, emotion) $z$ for a given observation (image) $x_i$.
>
> However, this involves computing $p(x_i)$ in the following expression
> $$
> p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}
> $$
> which is intractable as the denominator cannot be computed.
>
> Hence, the idea is to **approximate $p(z|x_i)$** using $q_{i}(z) \in Q$, and **minimize the distance** using a metric such as KL-divergence. Therefore, this becomes an **optimization problem**, for ==each $x_i :=x$:==
> $$
> q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz
> $$
> which can be simplied using Bayes to (since $p(z|x)$ we don't know either)
> $$
> \arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]
> $$
> where 
>
> - $z$ would be sampled from $z \sim q_\phi(z)$ as $q_\phi$ would be picked by us. Hence we can compute $p(x,z)=p(x|z)p(z)$ and $q_\phi(z)$
>- essentially $p(x,z)$ would involve a likelihood and a prior, which we know.
> - the first term is MAP and the second term encourages diffusion (entropy), or spreading of variational distribution.

First, let us *revise* Bayes theorem. For **each data point $x_i:=x$**
$$
p(z,x) = p(z|x)p(x)=p(x|z)p(z)
$$
where:

- $p(z,x)$ is the joint
- $p(z|x)$ is called the **posterior** (as you observed $x$)
- $p(x)$ is call the **evidence/marginal density**
- $p(z)$ is the **prior density** (before you observe $x$)
- $p(x|z)$ is the **likelihood** (given $z$, probability of seeing $x$)

> **Note**
>
> For using deep generative models, we may also want to say:
> $$
> p(x|z)=p(x|z_1)p(z_1|z_2)...p(z_{l-1}|z_l)p(z_l)
> $$
> for $l$ layers of hidden variables. This will be **discussed later**. Here we think of the simple case where we have only **one hidden variable**.

Our aim is to estimate $p(z|x_i)$:
$$
p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}
$$
which is intractable to compute because:

- for most models because the denominator is:
  $$
  p(x_i) = \int p(x_i|z)p(z)dz
  $$
  this is high-dimensional intractable integral which requires integrating over an exponential number of terms for $z$. 

- in the end, the posterior $p(z|x_i)$ is often intractable to compute analytically. For example, if $z$ is a vector of length $d$ (i.e. hidden state with $d$ "features"), **then $p(z|x_i)$ is a $d \times d$ matrix**, and the posterior is a function of the parameters of the model $p(z|x, \theta)$.

Therefore, the key idea is to ==estimate $p(z_i|x)$ by some variational distribution $q_\phi(z)\equiv q_i(z)$== from a family of distributions $Q$ and parameters $\phi$ such that $q_\phi \in Q$. Graphically:

<img src="DL_notes/image-20220215171509021.png" alt="image-20220215171509021" style="zoom:33%;" />

A typical choice for $Q$ is the **exponential family of distribution**:

> **Exponential Family Distribution**
>
> We say that a class of distributions is **in the exponential family** if it can be written in the form:
> $$
> p(y; \eta) = b(y) \exp(\eta \, T(y) - a(\eta))
> $$
> where:
>
> - $y$ means the labels/target in your dataset
> - $\eta$ is the **natural parameter** (also called the canonical parameter) of the **distribution**
> - $b(y)$ is the **base measure**
> - $T(y)$ is the **sufficient statistic** (see later examples, you often see $T(y)=y$)
> - $a(\eta)$ is the **log partition function**, which basically has $e^{-a(\eta)}$ playing the role of *normalization constant*
>
> so basically you can expression some distribution with the above form with any choice of $b(y), T(y), a(\eta)$, then that expression is in the exponential family.

And a choice of **closeness/distance** would be the (reverse) ==KL divergence==:
$$
KL(q(x) || p(x)) = \int q(x) \log \frac{q(x)}{p(x)}dx
$$
Therefore, our ==objective== is then (**recall that here $x_i := x$ represents a single data point**)
$$
q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz
$$
which is intractable since $p(z|x)$ is what we want to estimate. Hence we use bayes to simplify:
$$
\begin{align*}
\int q(z)\log \frac{q(z)}{p(z|x)}dz 
&= \int q(z)\log \frac{q(z)p(x)}{p(z,x)}dz \\
&= \int q(z)\log \frac{q(z)}{p(z,x)}+q(z) \log (p(x))dz \\
&= \log p(x) + \int q(z)\log \frac{q(z)}{p(z,x)}dz \\
&= \log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz
\end{align*}
$$
Now, notice that KL divergence is **non-negative**. Hence we know:
$$
\begin{align*}
\log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz 
& \ge  0\\
\log p(x) & \ge \int q(z)\log \frac{p(z,x)}{q(z)}dz \equiv \mathcal{L}
\end{align*}
$$
where the integral on the right denoted by $\mathcal{L}$ is known as the **evidence lower bound** (ELBO).

- therefore, minimizing KL divergence is the same as **maximizing ELBO**
- The ELBO is a **lower bound** on the log-likelihood of the data $x$ given the latent variable $z$. It is a lower bound because it is ==not possible to compute the exact log-likelihood== of the data $x$ given the latent variable $z$
- essentially $p(x,z)$ would involve a likelihood and a prior, which we know. See the example algorithm below to see how it works.

This finally can be written as:
$$
\arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]
$$
where we see that there is a tradeoff between these two terms.

- the first term places mass on the MAP estimate;
- whereas the second term encourages diffusion, or spreading the variational distribution.
- note that since we would have specified $q_\phi$, we could **sample $z \sim q_\phi$** to compute for $p(x,z)$

> **Note**
>
> Compared with this formulation, many other methods have short-comings:
>
> - mean-field variational inference (MFVI) **assumes a full factorization of variables** which is inaccurate
> - MCMC sampling methods (Brooks, Gelman, Jones & Meng 2011), such as the Metropolis Hastings algorithm, **may not be scalable to very large datasets** and may require **manually specifying a proposal distribution**

---

Recall that:

In the end we also want to recover our $\theta$ for model $p_\theta(x)$. Hence essentially our loss can be rephrased as (rephrasig $p(x,z)=p(x|z)p(z)$)
$$
\mathcal{L}_i(p,q_i ) =\mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_i(z)]
$$
Therefore, our algorithm would be, for each $x_i$:

1. calculate $\nabla_\theta \mathcal{L}_i(p,q_i ) $

   - sample $z \sim q_i(z)$

   - compute the gradient, which is only on the first term:
     $$
     \nabla_\theta \mathcal{L}_i(p,q_i) \approx \log p_\theta(x_i|z)
     $$
     this we **know because $x_i ,z$** are now "observed"

2. Update $\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}_i(p,q_i) $

3. update **$q_i$ to maximize $\mathcal{L}_i(p,q_i) $**

   - e.g. if you picked $q_i(z) = N(\mu_i, \sigma_i)$, then essentially $\phi = (\mu_i, \sigma_i)$
   - hence yuo would try to compute $\nabla_{\mu_i} \mathcal{L}_i$ and $\nabla_{\sigma_i} \mathcal{L}_i$ to optimize
   - but that will be **a lot of parameters**, if we have many data points.

Therefore, this introduces us to other alternatives such as VAE.

---

### Optimizing for $q_\phi$

In practice, there has been **many methods** in which one can use to update the estimate distribution $q_\phi$. Here we will discuss the approach using

- **Score function gradient** - generic and has **no requirement on $p_\theta(x)$**, but has ==large variance==;
- **Reparameterization gradient**. - less general purpose, **$p_\theta(x)$ needs to  be continuous**, much ==smaller variance==.

### Score Function Gradient

To find the best $p_\phi(z|x_i)$, we consider finding $\phi$:
$$
\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)-\log q_{\phi_i}(z)]= \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z)]
$$
let us denote $\log p(x_i,z)-\log q_{\phi_i}(z) \equiv f_\phi(z)$, then we have:
$$
\begin{align*}
\nabla_\phi \mathbb{E}_{z \sim q_\phi(z)}[f_\phi(z)]
&= \nabla_\phi \int f_\phi(z)q_\phi(z)dz \\
&= \phi \int (\nabla_\phi f_\phi(z))q_\phi(z)+ (\nabla_\phi q_\phi(z))f_\phi(z)dz \\
\end{align*}
$$
Now, it turns out that we can **compute** using the following ==trick==

<img src="DL_notes/image-20220215205915090.png" alt="image-20220215205915090" style="zoom: 50%;" />

where:

- the ==score function== is:
  $$
  \nabla_\phi \log q_\phi(z) = \frac{\nabla_\phi q_\phi(z)}{q_\phi(z)}
  $$

This therefore means that:
$$
\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z) \nabla _\phi \log q_\phi(z)]
$$
which we can then **estimate by sampling from the distribution $q_\phi(z)$ that we guessed**:
$$
\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k[\log p(x_i,z_i)-\log q_{\phi_i}(z_i) \nabla _\phi \log q_\phi(z_i)]
$$
which is called **Monte Carlo sampling**

### Reparameterization Gradient

The reparameterization trick utilizes the property that for **continuous** distribution $p_\theta(x)$, the following sampling processes are equivalent:
$$
\begin{align*} \hat{x} \sim p(x;\theta) \quad \equiv \quad \hat{x}=g(\hat{\epsilon},\theta) , \hat{\epsilon} \sim p(\epsilon) \end{align*}
$$
where:

-  instead of directly sampling from the posterior, we typically take random sample from a standard Normal distribution $\hat{\epsilon} \sim \mathcal{N}(0,1)$ and multiply it by the mean and variance 

- e.g. in our case, we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:
  $$
  z = \mu + \sigma \cdot \epsilon
  $$
  where $\epsilon \sim \mathcal{N}(0,1)$

Then, this **reparametrization can**:

1. express the gradient of the expectation
2. achieve a ==lower variance== than the score function estimator, and
3. differentiate through the latent variable $z$ to optimize by backpropagation

Then

<img src="DL_notes/image-20220215211858216.png" alt="image-20220215211858216" style="zoom: 50%;" />

This finally gives:
$$
\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k\{\nabla_\phi [\log p(x, g(\epsilon_i,\phi))-\log q_\phi(g(\epsilon_i,\phi))]\}
$$
being the update equation.

## Autoencoders

Instead of optimizing a separate parameter $\phi_i$ for **each example**, amortized variational inference (AVI) approximates the posterior across all examples together

The task of our model essentially involves finding **two sets of parameters $\theta, \phi$**. This makes it natural to consider an architecture using **encoder + decoder**, where essentially:

- **encoder** will perform $q_\phi(z|x)$ mapping. In other words, given an **input $x$**, **output a hidden state $z$**
- **decoder** will perform $p_\theta(\hat{x}|z)$, in an attempt to **reconstruct $x$** hence finding $p(x|z)$ with **input from hidden state $z$**

---

*Recap*: Autoencoder

Essentially an autoencoder is doing:
$$
F(x) = \text{decode}(\text{encoder}(x))
$$
where **making sure that** the dimension $z = \text{encoder}(x)$ is ==smaller than $x$==, hence we are "**extracting important features**" from $x$.

Therefore, the architecture for autoencoder looks like:

<img src="DL_notes/image-20220215220325253.png" alt="image-20220215220325253" style="zoom:67%;" />

where we can say that there are **two networks** (e.g. with two weights):
$$
\min_{W_e,W_d} \sum_{i=1}^m ||x_i - (W_d)^T f( (W_e)^T x_i )||^2
$$
for each data $x_i \in \mathbb{R}^d$

- we want $(W_e)^Tx_i$ to have a **smaller dimension than $d$** (otherwise reconstructing is trivial)
- if $f$ is an **identity matrix**, then this is equivalent of doing ==PCA==

---

This architecture can be used for:

- **denoising**

  <img src="DL_notes/image-20220217134248515.png" alt="image-20220217134248515" style="zoom:33%;" />

- **completion**

  <img src="DL_notes/image-20220217134306323.png" alt="image-20220217134306323" style="zoom:33%;" />

  for example:

  <img src="DL_notes/image-20220217134333967.png" alt="image-20220217134333967" style="zoom:33%;" />

### Variational Autoencoder

Then, for Variational Autoencoder, we are basically using autoencoder to learn $\theta, \phi$ by **maximizing ELBO**:
$$
\begin{align*}
\mathcal{L} 
&= \int q(z)\log \frac{p(z,x)}{q(z)}dz \\
&= \int q(z)\log p(x|z) dz - \int q(z) \log \frac{p(z)}{q(z)}dz\\
&= \mathbb{E}_{z \sim q(z)}[\log p(x|z)] - KL(q(z)||p(z))
\end{align*}
$$
Therefore, we can think of the following architecture to **solve this optimization problem**

|                           Network                            |                         Abstraction                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="https://miro.medium.com/max/1838/1*Q5dogodt3wzKKktE0v3dMQ@2x.png" style="zoom:67%;" /> | ![image-20220215220914001](DL_notes/image-20220215220914001.png) |

where:

- $\mathbb{E}_{z \sim q(z)}[\log p(x|z)] $ would correspond to **decoder**

  - given some **sampled $z \sim q(z)$**, this is the log-likelihood of the observed data $x$ (i.e. $x_i := x$).
  - Therefore, this measures how well the samples from $q(z)$ explain the data $x$, which can be seen as the **reconstruction error** to get $x$ back from an encoded latent variable $z$
- $ KL(q(z)||p(z))$ would correspond to the **encoder**, because we are trying to find out $q(z)$ that is close to $p(z)$

  - represents **encoding data from $x$ to latent variable $z$**
  - hence, if going well, this means that the explanation of the data ($z \sim q(z)$) does not deviate from the prior beliefs $p(z)$ and
    is called the **regularization term**


**In summary**:

- encoder neural network infers a hidden variable $z$ from an observation $x$. 
- decoder neural network which reconstructs an observation $\hat{x}$ from a hidden variable $z$.
- The encoder $q_\phi$ and decoder $p_\theta$ are trained end-to-end, optimizing for **both** the **encoder parameters $\phi$** and **decoder parameters $\theta$** by backpropagation

> Now there is a problem, because in the above algorithm, we would need to **sample $z$** in the encoder (not a differentiable operation). However, for backpropagation, we need each operation to be ==differentiable==.

Therefore, we need to **reparametrize** the sampling procedure ot the following:

<img src="DL_notes/image-20220217134853802.png" alt="image-20220217134853802" style="zoom:33%;" />

therefore, essentially:

- recall that we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:
  $$
  z = \mu + \sigma \cdot \epsilon
  $$
  where $\epsilon \sim \mathcal{N}(0,1)$

- we are **predicting the mean/variance**, instead of predicting $z$. Therefore, all we need to do is to **update mean/variance** of the distribution instead of update the "sampling procedure"
- the same goes on to decoder.

Then together, the model is really **learning**:

<img src="DL_notes/image-20220217135020612.png" alt="image-20220217135020612" style="zoom: 50%;" />

- which can be seen as a probabilistic model as it involves calculation from samplings using mean and variance

## Probabilistic Programming

Essentially you can compute **distributions, conditional distributions, etc** using a program.

- e.g. you can infer **conditional distribution $p(a+b+c|a+b=1)$** from only knowing the individual probabilities $p(a),p(b),p(c)$ 

<img src="DL_notes/image-20220217135414796.png" alt="image-20220217135414796" style="zoom:50%;" />

which is a powerful idea (e.g. used in MCMC and etc)

# Reinforcement Learning

Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning. 

- In **supervised** learning we are given input-output pairs
- in **unsupervised** learning we are given only input examples
- In **reinforcement** learning we learn from interaction with an environment to achieve a goal

> We have an **agent**, a learner, that makes decisions under uncertainty.
>
> In this setting there is an **environment** which is what the agent interacts with. The agents selects **actions** and the environment responds to those actions with a new **state** and **reward** . 
>
> The agent goal is to ==maximize reward over time== as shown below:
>
> <img src="DL_notes/image-20220217201444646.png" alt="image-20220217201444646" style="zoom:50%;" />

> **Resource**: https://web.stanford.edu/class/cs234/index.html

## Multi-Armed Bandit

Before considering reinforcement learning, we will consider the **stateless** (policy at time $t$ is identical) setting of a multi-armed bandit. 

Given $k$ slot machines:

- an **action** is to pull an arm of one of the machines
- at each **time step** $t$ the agent chooses an **action** at among the $k$ actions
- taking action a is pulling arm $i$ which gives a **reward $r(a)$** with probability $p_i$, which of course you don't know
  - i.e. Behind each machine there is a probability distribution, and by pulling an arm we get a *sample* from that distribution

Our goal is to **maximize the total expected return**. To do this, consider:

- each action has an expected or mean reward given that that action is selected; we can it **value** of that action

  - we denote the **true value of action $a$** as $q(a)$

  - the **estimated value of action $a$ at time $t$ as $Q_t(a)$**
    $$
    Q_t(a) =\text{Sample Mean}(a)
    $$
    which we can update per iteration/action taken

Then a simple idea is, at any time step, **pick the action** whose ==estimated value is greatest==
$$
a_t = \arg\max_{a}Q_t(a)
$$
where if we do this, we are doing a **greedy algorithm**

- If you select a greedy action, we say that you are **exploiting** your current knowledge of the values of the actions
- If instead you select one of the nongreedy actions, then we say you are **exploring** (could lead to better results in the long run)

---

*For Example*

Consider you are given

- two possible actions, picking red or blue (door, pill, etc).

At $t=0$, we can randomly pick an action. For instance we picked the red one, and receives a reward $0$:

| $t=0$                                                        |                            $t=1$                             |                            $t=2$                             |                            $t=3$                             |
| ------------------------------------------------------------ | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220217205400203](DL_notes/image-20220217205400203.png) | ![image-20220217205412649](DL_notes/image-20220217205412649.png) | ![image-20220217205426486](DL_notes/image-20220217205426486.png) | ![image-20220217205443361](DL_notes/image-20220217205443361.png) |

notice that

- at each time step we have **updated our estimate for value of an action $Q_t(a)$**
- we are endlessly choosing blue, but it could have been the case that the value we received for the red door of $0$ was simply bad luck, and that value was sampled from the tail of the distribution behind the red door
  - hence we need some **balance** between exploration and expoitation

### $\varepsilon$-greedy Approach

> If instead of taking a greedy action, we behave greedily most of the time, for example
>
> - with a small probability $\varepsilon$ we choose a **random** action 
> - with probability $1-\varepsilon$ we take the **greedy** action then we are acting $\varepsilon$-greedy

Then the algorithm is simply:

<img src="DL_notes/image-20220217210047233.png" alt="image-20220217210047233" style="zoom: 50%;" />

notice that:

- $r(a)-Q(a)$ is like an **error** of current estimate
- as an exercise, try to show that this is the same as calculating the runnning mean $Q_{t+1} = (1/k)\sum_i^k R_i$

### Upper Confidence

We can choose to be optimistic under uncertainty by **using both the mean and variance of the (estimated) reward**, taking the action using the upper confidence bound (UCB) criteria:
$$
a_t = \arg\max_a(\mu(r(a)) + \epsilon \sigma(r(a)))
$$
then in this case, you would also need to keep track of $\sigma(r(a))$ estimate.

## State Machines

The algorithms before are **stateless**. Now we consider adding a state, and the problem can be formalized as a state machine.

> The tuple $(S,X, f, Y, g, s_0)$ define the **state machine**.
>
> - $S$ is the set of possible states
> - $X$ is the set of possible inputs
> - $f:S\times X\to S$ transition function
> - $Y$ is the set of possible outputs
> - $g:S \to Y$ mapping from state to output
> - $s_0$ initial state

An example would be:

<img src="DL_notes/image-20220218170329457.png" alt="image-20220218170329457" style="zoom: 50%;" />

where notice that:

- $S = \{ \text{standing, moving} \}$, $X = \{ \text{slow, fast} \}$, 
- $f$ shown in orange and purple arrows, e.g. $s_1 = f(s_0 , \text{fast}) = \text{moving}$
- $s_0 = \text{standing}$ being the initial state
- $y=g(\text{standing}) = \text{standing}$ is the output in this case

Notice when we use a state machine, at each time step $t$ it is essentially resembling **RNN**:
$$
\begin{align*}
s_t &= f(s_{t-1},x_t)\\
y_t &= g(s_t)	
\end{align*}
$$
notice that

- this is the same as RNN if we use hidden state $h_{t-1}$ instead of $s_{t-1}$ here.
- everything is **deterministic**, i.e. given a state and an action, you know for certain what will be the next state

### Markov Processes

> In a **Markov model**, we assume that the probability of a state $s_{t+1}$ is dependent **only on the previous state $s_t$ and an action $a_t$**.
>
> Formally, we consider
>
> - $S$ a set of possible states
> - $A$ a set of possible actions
> - $T:S\times A \times S \to \mathbb{R}$ is the transition model with probabilities
> - $g$ is the mapping from state to output
>   - in the following examples they will just be identity operation

(notice that we haven't included reward $R$ in this model. Adding this information essentially makes the model to become a Markov Decision Process)

For instance:

<img src="DL_notes/image-20220218172310634.png" alt="image-20220218172310634" style="zoom:50%;" />

where you see

- $A = \{\text{slow}, \text{fast}\}$ denoted by orange and pick arc
- e.g. if the robot is $\text{fallen}$ and takes $\text{slow}$ action then with probability $p=3/5$ the robot will stay fallen, **but with $p=2/5$** the robot will stand up
  - notice that this means an action can potentially lead **any number of states**
  - this is no longer deterministic!

Notice that since we have **three states and two actions**, we have **two probability matrices** of $3\times 3$ in size:

<img src="DL_notes/image-20220218221548845.png" alt="image-20220218221548845" style="zoom:67%;" />

which corresponds to the figure above, and notice that:

- the $i$-th row denote transition from state $s_i$ to $s_j$. Therefore probability per row **adds up to $1$**

> Then, a **policy $\pi_t(a|s)=P(a_t=a|s_t=s)$** maps a **state $s$ to action $a$**, allowing agent to decide which action to take given the current state at time $t$.
>
> - Reinforcement learning methods specify how the agent changes its policy as a result of its experience

Graphically, this looks like:

<img src="DL_notes/image-20220222135047939.png" alt="image-20220222135047939" style="zoom: 33%;" />

This idea can be easily extended when we have observations $o_i \neq s_i$ at each state, and we make decisions **base on the observations $\pi(a_i | o_i)$**

<img src="DL_notes/image-20220222135329258.png" alt="image-20220222135329258" style="zoom: 50%;" />

### Markov Decision Processes

This is the more relevant introduction to RL.

> The tuple $(S,A,T,R,\gamma)$ define the **Markov Decision Process**.
>
> - $S$ is the set of possible states
> - $A$ is the set of possible actions
> - $T:S\times A \times S\to \mathbb{R}$ is the transition model with *probabilities*
>   - this is assumed to be known in advance in MDP. It will be unknown in RL.
> - $R: S \times A \to \mathbb{R}$ is reward function, given a state and action
>   - not $r(s,a,s')$ here because we need to "estimate" the reward before knowing what $s'$ is, which is probabalistic, so that we can define our policy based only on $s$ so that $\pi = \pi(a|s)$
> - $\gamma$ is the discount factor

The idea is simple:

1. At every time step $t$ the agent finds itself in state $ s \in S$ and selects an action $a \in A$.
2. the agent then moves to a new state $s \leftarrow s'$ in a **probabalistic manner** and receives a **reward**
   - the reward is a function of previous state and action $s,a$ (technically the expected value of $r(s,a,s')$)
3. repeat step 1

For instance, consider the following example, where we have a robot **collecting cans**. It can either search actively for a can (depletes battery), wait for someone to give a can, or recharge:

<img src="DL_notes/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" />



where we have provided:

- state $S$ being the larger nodes, actions $A$ being the smaller nodes

- transition $T(s,a,s')$ being the probability on the arrow, and $r(s,a,s')$ being the reward on the arrow.

  Reward function technically is:
  $$
  R(s,a) = \sum_{s'} r(s,a,s') p(s'|s,a)
  $$
  

They can also be summarized in the table:

<img src="DL_notes/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" />

notice that the above table essentially provides a **probability distribution for each possible state-action pair**
$$
p(s',r|s,a) \equiv P(S_{t+1}=s,R_{t+1}=r| S_t = s,A_t=a)
$$
since each transition essentially is a two tuple. Then, we can use this to compute useful quantities such as:

- **expected rewards for state-action** pair
  $$
  r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)
  $$
  or alternatively, if we have $r(s,a,s')$:
  $$
  r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{s'} r(s,a,s')p(s'|s,a)
  $$
  for instance, we can compute $r(s=\text{low}, a=\text{search})$ as:
  $$
  \begin{align*}
  r(s=\text{low}, a=\text{search})
  &= -3 \cdot \sum_{s'} p(s',r=-3|s,a) + r_{search}\cdot \sum_{s'}p(s',r=r_{search}|s,a)\\
  &= -3 \cdot (1-\beta) + r_{search}(\beta )
  \end{align*}
  $$
  which is essentially ==weighting the reward on the arrow with probability== (see graph above). We can compute this for **every state-action pair** and get a matrix of size $|S| \times |A|$:
  $$
  R(s,a) = \begin{bmatrix}
  r(s_0,a_0) & r(s_0,a_1)\\
  r(s_1,a_0) & r(s_1,a_1)\\
  r(s_2,a_0) & r(s_2,a_1)
  \end{bmatrix}
  $$
  then we can take the action that maximizes the reward from that state per **row** (greedy)
  
- **state transition probability**
  $$
  p(s'|s,a) = P(S_{t+1}=s| S_t = s,A_t=a) = \sum_r p(s',r|s,a)
  $$
  
- **expected rewards for state-action-next-state**:
  $$
  r(s,a,s') = \mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] = \frac{\sum_r r\, p(s',r|s,a)}{p(s'|s,a)}
  $$
  

> **Note**
>
> - The latter two quantities will be **the most important**, which are the ones drawn on the graph/in the table, and dynamics will be expressed almost exclusively using those two in further chapters.
> - In reality, the agent does not know $T(s,a,s')=P(s'|s,a)$ and $R(s,a)$. We wil lneed to learn them by **sampling** the environment.

## Value Functions

Almost all reinforcement learning algorithms involve estimating some kind of value functions:

- either estimating **functions of states** that estimate how good it is for the agent to be in a given state
- or **functions of state-action pairs** that estimate how good it is to perform a given action in a given state

> The notion of "how good" here is defined in terms of **future rewards** that can be expected. To be specific, we want to maximize the **cumulative reward it receives in the long run**, which is called the ==expected return $G_t$==

To formalize the above idea, we need to consider the **sequence of rewards** received ==at each step== after some time $t$:
$$
R_{t+1}, R_{t+2}, R_{t+3}, ...
$$
Then, we can define our ==goal being maximizing the expected return at time $t$==:
$$
G_t = f(R_{t+1},R_{t+2},...,R_{T})
$$
for $T$ being the final step. A simple example would be:
$$
G_t = R_{t+1} + R_{t+2} + .... + R_T
$$
and more commonly we can generalize this with **discounting**:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+.... = \sum_{k=0}^T \gamma^k R_{t+k+1}
$$
where $\gamma \in [0,1]$ is a discount rate:

- a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately.
- if $\gamma=0$, then the agent is **myopic** as it only consider maximizing immediate reward as $G_t = R_{t+1}$
- if $\gamma \to1$, then the objective takes future rewards into account more strongly: the agent becomes more **farsighted**. (if $\gamma =1$ the sum might explode if $T \to \infty$ and your sequence is not bounded.)

With this, we can define the following quantity that are crucial in making decisions.

> **State Value Function**
>
> The value of a state $s$ under a policy $\pi$, denoted $V_\pi(s)$, is the **expected return when starting in $s$** and following $\pi$ thereafter.
> $$
> V_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]
> $$
> and we call $T\equiv h$ is the horizon, which can be seen as the **number of time steps left**. Additionally, we define the value of the last state $t=T=h$ being
> $$
> V_\pi^{0}(s) = 0
> $$

Of course, in reality we can only estimate this expected value. This means that we need to consider the **most general case** for a ==stochastic policy $\pi(a|s)$ being a distribution==:
$$
\begin{align*}
V_\pi (s)
= \mathbb{E}_\pi [G_t | S_t =s]
&= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\\\
&= \mathbb{E}_\pi [R_{t+1}|s] + \gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&= \mathbb{E}_\pi [R_{t+1}|s] + \sum_a \pi(a|s) \sum_{s'}\sum_r p(s',r|s,a)\gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_{t+1}=s\right]\\
&= \sum _aR(s,a) \pi(a|s) + \gamma  \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)V_\pi(s')\\
\end{align*}
$$
is a essentially **recursive formula**, which we can **dynamically update**. Note that:

- $s'$ is essentially the **next state** from $s,a$ tuple we chose
- we have all the fuss/sums because the policy $\pi(a|s)$ is spitting out a **probability for taking each action $a$** when in a state $s$

- remember that:
  $$
  R(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)
  $$
  which 

  - do not forget that $R$ is basically the **expected reward after taken $s,a$ tuple**.
  - the $r$ is a random variable, and $p(s',r|s,a)$ would be the joint for all possible $s',r$ output.

  Therefore, the above essentially becomes the **Bellman equation**

> **Bellman Equation for $V_\pi$**
>
> For a **stochastic policy $\pi(a|s)$**, the formula above can be rewritten as:
> $$
> V_\pi (s)
> = \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]
> $$
> which can be interpreted as the **expected value over $a,s',r$** for $r+\gamma V_\pi(s')$, and we are **weighting it by $\pi(a|s)p(s',r|s,a)$**. Hence, graphically, we are essentially considering **all the possibilities from $s$**:
>
> ![image-20220222220640857](DL_notes/image-20220222220640857.png)
>
> and summing over **all of them**.
>
> Notice that this value function $V_\pi(s)$ is the **unique solution to its Bellman equation, given a $\pi(a|s)$**.
>
> - this **yields $|S|$ equations to be solved**, because it can be written as:
>   $$
>   V_\pi = r + TV_\pi
>   $$
>   for $V_\pi(s)$ is a **vector of values for each state $s$**, $T$ being the transition matrix, $r$ being the reward for each state.
>   
> - We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn $V_\pi(s)$.

If $\pi(s) \to a \in A$ being some **deterministic** policy, then we can say that:
$$
V_\pi(s) =  R(s,\pi(s)) + \gamma  \sum_{s'}p(s'|s,\pi(s))V_\pi(s')
$$
so that essentially:

- current reward + an expected value summing over all the possible next state $s'$ we could take
- again, a **recursive formula**, which we can dynamically update to solve for $V_\pi(s)$
- since this acts on $\pi(s)\to a \in A$ instead of $\pi(a|s)$ being a distribution, e.g. $\pi(s) = \arg\max_a \pi(a|s)$, we can use this formula to ==find a policy that maximizes the discounted return==.
-  this **yields $|S|$ equations to be solved**

---

Similarly, we can inductively compute $Q^h(s,a)$ which is the **action value function**.

> **Action Value Function**
>
> We denote $Q_\pi(s,a)$ as the **expected return** starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
> $$
> Q_\pi(s,a)=\mathbb{E}_\pi [G_t | S_t = s,A_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]
> $$
> Notice that obviously:
> $$
> V_\pi(s) = \sum_{a} \pi(a|s)Q_\pi(s,a)
> $$
> are related quantities.

Similar to the state value function, we can compute the $Q_\pi(s,a)$ using **dynamic programming** with the **Bellman's Equation** again:
$$
\begin{align*}
Q_\pi (s,a)
= \mathbb{E}_\pi [G_t | S_t =s, A_t=a]
&= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]
\end{align*}
$$
which following a similar derivation with the state value function, we can arrive at:

> **Bellman Equation for $Q_\pi$**
>
> For a **stochastic policy $\pi(a|s)$**, the formula above can be rewritten as:
> $$
> Q_\pi (s,a)
> = \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]
> $$
> which can be interpreted as the **expected value over $s',r$** whatever is in the bracket, meaning covering all possible $s',r$ as the next step if we did $s,a$. Then we are **weighting it by $p(s',r|s,a)$**. Hence, graphically, we are essentially considering **all the possibilities from $s,a$**:
>
> Graphically you are doing:
>
> <img src="DL_notes/image-20220222221819799.png" alt="image-20220222221819799" style="zoom: 80%;" />
>
> And **summing over all possibilities** weighted by their probability.

Again, this is not computable, so we usually consider estimating $V_\pi,Q_\pi$ from experience. For example:

- if an agent follows policy $\pi$ and **maintains an average**, for each state encountered, **of the actual returns that have followed that state**, then the average will converge to the state's value, $V_\pi(s)$.
- If **separate averages are kept for each action** taken **in a state**, then these averages will similarly converge to the action values, $Q_\pi(s,a)$.

---

*For Example*

Consider a setup where each cells of the grid correspond to the states of the environment. 

![image-20220222232047833](DL_notes/image-20220222232047833.png)

where:

- at each cell, **four actions are possible**: north, south, east, and west, and we assume the **actions are deterministic** in that $p(s \to s'|a)=1$.
- actions that would take the agent out of the grid leave its location unchanged, but also result in a **reward of $-1$**
- from state $A$, **all four actions yield a reward of $+10$** and take the agent to $A'$.
- from state $B$, **all actions yield a reward of $+5$** and take the agent to $B'$.
- other actions result in **a reward of $0$**

Now, to compute the **value function $V_\pi(s)$**, we need to specify a policy: Suppose the agent selects **all four actions with equal probability in all states**. Then, using $\gamma=0.9$, this policy gives:

![image-20220222232432754](DL_notes/image-20220222232432754.png)

which is the **value functions $V_\pi(s)$ for all state $s$ given this policy**.

- since the policy is stochastic, verify it using the Bellman's equation
- negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy
- $A$ is the best state to be in under this policy but its **expected return is less than 10** (which is its immediate reward), because from $A$ the agent is taken to $A'$, from which it is likely to run into the edge of the grid.
- State $B$, on the other hand, is **valued more than $5$** (which is its immediate reward), because from $B$ the agent is taken to $B'$, which has a positive value
  - because at $B'$ you have a possibility of bumping into $A$ or $B$

(At this point, you might want to improve the policy, i.e. to move towards $A$ more rather than randomly going in all direction. If you do that, then you will need to **recompute $V_\pi(s)$** because the above is only true for the stochastic policy)

### Optimal Value Functions

> **Goal**
>
> Solving a reinforcement learning task means, roughly, **finding a policy that achieves the most reward over the long run**. Then, with this definition, we can order policies by their **expected return**:
>
> A policy $\pi$ is defined to be better than or equal to a policy $\pi'$ if 
> $$
> \pi \ge \pi' \iff V_\pi(s) \ge V_{\pi'}(s),\quad \forall s
> $$
> which means its expected return is greater than or equal to that of $\pi'$ **for all states**. And there is ==always at least one policy that is better== than or equal to all other policies

Hence, we can define an **optimal policy $\pi^*$** that must satisfy the following:
$$
\pi^* \to \begin{cases}
V_{\pi^*}(s) = V^*(s) &= \max_\pi V_\pi(s)\\
Q_{\pi^*}(s,a) = Q^*(s,a) &= \max_\pi Q_\pi(s,a)
\end{cases}
$$
note that there may be more than one $\pi^*$, but then by definition they must share the same constraint above, i.e have the same state value function and action value function.

At this point, you might wonder **why do we need both $V_\pi(s),Q_\pi(s,a)$**? ==Technically, we only need one of them to find $\pi^*$==

- If you have the optimal value function, $V^*$, then the actions that appear **best after a one-step search** (i.e. the action that goes to the best valued next state) will be **optimal actions**

  Aa greedy policy is actually optimal in the long term sense **because $V^*$ already takes into account the reward consequences of all possible future behavior**.

- With $Q^*$, the agent does not even have to do a one-step-ahead search: for any state $s$, it can simply **pick any action that maximizes $Q^*(s|a)$** by:
  $$
  \pi^*(s) = \arg\max_a Q^*(s,a)
  $$
  again, a deterministic policy results.

  Hence, at the cost of representing a function of state{action pairs, instead of just of states, the optimal action-value function allows optimal actions to be selected without having to know anything about possible successor states and their values

Before we discuss **how to solve for the optimal solution**, consider the MDP case of the grid:

![image-20220222235445697](DL_notes/image-20220222235445697.png)

The **optimal solution of state value function** looks like

![image-20220222235513192](DL_notes/image-20220222235513192.png)

which, by greedily looking one step ahead, we have **found the optimal policy $\pi(s):S \to a \in A$**:

![image-20220222235610203](DL_notes/image-20220222235610203.png)

---

Now, consider **solving for $V^*$**. We know that:

1. The solution must satisfy Bellman's Equation
   $$
   V_\pi (s)
   = \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]
   $$

2. But because it is **optimal** (i.e. corresponds to the optimal policy), **value of a state** under an optimal policy must **equal the expected return for the best action from that state**.

Therefore, we get:
$$
\begin{align*}
V^* (s)
= \max_{a \in A} Q^*(s,a)
&= \max_a \mathbb{E}_{\pi^*}[G_t | S_t =s, A_t=a]\\
&= \max_a\mathbb{E}_{\pi^*} \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]\\
&= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s,A_t=a\right]\\
&= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}
$$
notice that the last two lines have **no reference to the optimal policy**. Those two lines are also called the ==optimality equation for $V^*$==.

Graphically:

|                Bellman's Optimality equation                 |                      Bellman's Equation                      |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220223005457522](DL_notes/image-20220223005457522.png) | ![image-20220223005536963](DL_notes/image-20220223005536963.png) |

(recall that once you have $V^*$, you can find $\pi^*$ easily.)

> **Note**
>
> To solve for the optimal state value function, the Bellman optimality equation is actually **a system of equations**, ==one such equation for each state==. So if there are $N$ states, then there are $N$ equations in $N$ unknowns (see example at the end)

The **Bellman's optimality equation for $Q^*$** also has to satisfy

1. Bellman's equation:
   $$
   Q_\pi (s,a)
   = \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]
   $$

2. Is **optimal**. Hence we consider $\gamma \max_{a \in A} Q^*(s',a')$ instead of $\gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')$


This gives the following **optimality equation**:
$$
\begin{align*}
Q^* (s,a)
&= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}
$$

and graphically:

|                Bellman's Optimality equation                 |               Bellman's Equation               |
| :----------------------------------------------------------: | :--------------------------------------------: |
| ![image-20220223010401381](DL_notes/image-20220223010401381.png) | ![ but ](DL_notes/image-20220223010502964.png) |

> **Note**
>
> The runtime of solving the optimal action value function will take $O(|A|\times|S|)$ for having $|A|$ possible actions.

---

*For Example*: Solving Optimal Solution for Robot Collection

Recall the setup being:

|                         Transitions                          |                          Tabluated                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" /> | <img src="DL_notes/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" /> |

Essentially we know $p(s',r|s,a)$, then since we have **two states**, let us encode the two states as $s_0=\text{high}=h,s_1=\text{low}=l$. Additinally:

- $s,w,re$ represents the **actions** search, wait, recharge.
- parameters $\gamma, \beta, \alpha$ are assumed to be known

Then, we have **two equations because we have two states**:

<img src="DL_notes/image-20220223011058649.png" alt="image-20220223011058649" style="zoom:80%;" />

And

<img src="DL_notes/image-20220223011119229.png" alt="image-20220223011119229" style="zoom:80%;" />

Once we solved it, say $V^*(h)=21$, $V^*(l)=10$, we have **solved the Bellman's Optimality** 

- there is exactly one pair of numbers, $V^*(h), V^*(l)$ that simultaneously satisfy these two nonlinear equations.

- essentially we can fill in the "cells" with values we found like in this example we discussed before

  <img src="DL_notes/image-20220223011350739.png" alt="image-20220223011350739" style="zoom:67%;" />

## Iterative DP Methods

If one can solve the Bellman's optimality equations, then an optimal policy can be easily found and there is no more work to do. However, in reality you need to face the following problem:

- even if all the information is known, computing for the solution takes **huge computation power**
- In most cases of practical interest there are **far more states** than could possibly be entries in a table/hence need huge memory, and **approximations** must be made
- in reality, certain aspects of the environment **may not be known**, such as transition probabilities

> In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be **approximated** in some way

Recall that for the Bellman's equation for **any policy**

$$
V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]
$$

$$
Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]
$$

and the **optimality equations**:
$$
\begin{align*}
V^* (s)
&= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}
$$

$$
\begin{align*}
Q^* (s,a)
&= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}
$$


> **Essentially** dynamic programming methods:
>
> - ==uses Bellman's equations as update rules== and improve policy by $\arg\max$ gives [Policy Iteration](#Policy Iteration)
>
> - ==uses the optimality constraint as update rules== for improving approximations of the desired value functions. This gives [Value Iteration](#Value Iteration)

### Policy Evaluation

First we consider how to compute the state-value function $V_\pi$ for an arbitrary policy $\pi$, using Bellman's equation.
$$
V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]
$$
Then we have $|S|$ simultaneous linear equations since we have $V_\pi(s):s \in S$. Then the idea is to consider a **sequence of approximate solutions $V_0,V_1,V_2,...$** such that they obey:
$$
V_{k+1} (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]
$$
Then, it can be shown that $\lim_{k \to \infty} \{V_k\} = V_\pi$, i.e. converges. Therefore our algorithm is simply:

<img src="DL_notes/image-20220224173242299.png" alt="image-20220224173242299" style="zoom:67%;" />

where the Bellman's equation essentially will iterate over all possible $s' \in S$ while using the current $V$.

- this algorithm is also called **iterative policy evaluation**

- To produce each successive approximation, $V_{k+1}$ from $V_k$, iterative policy evaluation applies the same operation to each state $s$ by looking at the old values of $s$ and **all possible one-step transitions**, which is called a ==full backup== since it looks at the entire tree:

  <img src="DL_notes/image-20220222220640857.png" alt="image-20220222220640857" style="zoom:50%;" />

In reality, Algorithm 11 will need **two arrays**, one to keep track of old $V_{k-1}(s)$ and the other to fill in the new $V_k(s)$:

<img src="DL_notes/image-20220224175028170.png" alt="image-20220224175028170" style="zoom:50%;" />

Another variant would be to use one array and **update the values in place**, that is, with each new backed-up value immediately overwriting the old one.  (it can be shown that this **also converges to $V_\pi$**)

- Then, depending on the order in which the states are backed up, sometimes new values are used instead of old ones on the right-hand side of Bellman's equation
- since it uses new data as soon as possible, it actually **converges faster**. Hence this will be used more often in reality.

---

*For Example*:

Consider the setup of the following environment, with each state/cell assigned a value for easier math definitions:

<img src="DL_notes/image-20220224175845046.png" alt="image-20220224175845046" style="zoom:67%;" />

where:

- non-terminal states are $S = \{1,2...,14\}$, and terminal states are the grey boxes
- We will assign a reward of $-1$ on all transitions until terminal state is reached. Therefore, the expected reward function becomes $r(s,a,s')=-1$ for all states/actions.
- the movement will be deterministic, so that $p(6|5,\text{Right})=1$, for example.

We consider evaluating a policy $\pi$ being equi-probable for all actions. Then, we can **iteratively compute $V_k$** using the above algorithm, and at each $V_k$, we can compute the **greedy policy w.r.t. $V_k$**.

<img src="DL_notes/image-20220224180250255.png" alt="image-20220224180250255" style="zoom:67%;" />

notice that the greedy policy for $V_3,V_{10},V_\infty$ is actually an **optimal policy in this setup** (though it is obtained from the state value function from random policy). In general, it is only ==guaranteed== that the **last policy**/bottom right would be an ==improvement over the given policy $\pi$==.

- this will hint at what we want to do in the next section

### Policy Improvement

Our reason for computing the value function for a policy is to help **find better policies**, just as what we have seen in the previous section. In general, it can be shown that 

> **Policy Improvement Theorem**
>
> Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, for all $s \in S$:
> $$
> Q_\pi(s, \pi'(s)) \ge V_\pi(s)
> $$
> Then it ==must be that==
> $$
> V_{\pi'}(s) \ge V(s),\quad \forall s
> $$
> Moreover, if there is a strict inequality in one of the state $Q_\pi(s, \pi'(s)) > V_\pi(s)$, then there must be at least a strict inequality for $V_{\pi'}(s) > V(s)$.

A simple simple example to see how the theorem works is to consider a changed policy, $\pi'$, that is identical to $\pi$ except that $\pi(s')=a \neq \pi(s)$ for **only one $s \in S$.** Then, if we know $Q_\pi(s,a) > V_\pi(s)$ for that $s$, it follows that $V_{\pi'}(s) > V_\pi(s)$.

Then, since it works for **one $s \in S$**, we can extend it to **all $s \in S$**: selecting at each state the action that appears best according to $Q_\pi(s,a)$ ==after you computed $V_\pi(s)$==:
$$
\begin{align*}
\pi^\prime (s)
= \arg\max_{a \in A} Q_\pi(s,a)
&= \arg\max_a\mathbb{E}_{\pi} \left[R_{t+1}+\gamma V_\pi(s_{t+1})|S_t=s,A_t=a\right]\\
&= \arg\max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\\
\end{align*}
$$
notice that:

- By construction, the greedy policy meets the conditions of the policy improvement theorem, so we know that it is as good as, or better than, the original policy: it **guarantees improvement** unless we are optimal already.
- it is essentially a function of $V_\pi(s)$, which depends on the original $\pi$ and a converged $V_\pi$. (this should provide enough hint how to design an algorithm to find out best $\pi^*$)

> **Extension to Stochastic Policy**
>
> Recall that for a stochastic policy $\pi(s)=\pi(a|s)$ spit out a probability distribution:
> $$
> Q_\pi(s,\pi^\prime(s)) = \sum_a \pi^\prime (a|s)Q_\pi(s,a)
> $$
> Then the idea is to basically, if there are several actions which the maximum can be achieved, we assign **each of those actions a portion of the probability** in the new greedy policy. A simple example of this would be the case we discussed before:
>
> <img src="DL_notes/image-20220224191414214.png" alt="image-20220224191414214" style="zoom:67%;" />
>
> where the LHS was the state function for equi-probable stochastic policy, whereas the RHS is the improved greedy policy, which is still stochastic.

### Policy Iteration

At this point, you know how to:

1. evaluate $V_\pi$ of a policy $\pi$
2. improve a policy $\pi \to \pi'$

Then we can basically repeat the above loop again and again to improve our policy. We can thus obtain a sequence of monotonically improving policies and value functions:
$$
\pi_0 \to V_{\pi_0}\to \pi_1 \to V_{\pi_1}\to .... \pi_* \to V_{\pi_*}
$$
Hence the algorithm is simply:

<img src="DL_notes/image-20220224191804784.png" alt="image-20220224191804784" style="zoom:67%;" />

which works because a **finite MDP has only a finite number of policies**, this process must converge to an optimal policy and optimal value function in a finite number of iterations. 

- While this looks like a painful computation heavy algorithm, it converges actually pretty fast, presumably because the value function changes little from one policy to the next are our policy is getting better

- but still, sometimes it takes a long time, in which case we can look at value iteration methods

### Value Iteration

The key idea is that **policy evaluation** step of policy iteration can be **truncated in several ways without losing the convergence** guarantees of policy iteration. The idea is to **combine policy improvement and evaluation**:
$$
V_{k+1} (s)
= \max_a \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]
$$
which if you recall, was the optimality constraint:
$$
\begin{align*}
V^* (s)
&= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}
$$

> And it can be shown that this sequence $\lim_{k \to \infty }\{V_k\} \to V^*$ **converges** directly to optimal state value function.

<img src="DL_notes/image-20220224193336422.png" alt="image-20220224193336422" style="zoom:67%;" />

where 

- Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement

- we can again use early stopping if it would run for a long time.

## Monte Carlo Methods

In this chapter we consider our first learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we ==do not assume complete knowledge of the environment==. Therefore, this big change means that MC methods requires only *experience*, i.e. ==sampling state/action/rewards== from an environment.

> Monte Carlo methods are ways of solving the reinforcement learning problem based on **averaging sample returns**. This places an assumption that it is defined for **episodic tasks**, instead of a continuous space.
>
> Specifically, features of this method include:
>
> - experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected
>
> - only on the completion of an episode are value estimates and policies changed

Again, the key difference here is that before we **computed value functions** from knowledge of the MDP, here we **learn value functions from sample returns** with the MDP.

### Monte Carlo Prediction

We begin by considering Monte Carlo methods for **learning the state-value function** for a given policy

> **Key Idea**
>
> The value of a state $s$ is the **expected return** - expected cumulative future discounted reward - **starting from that state**. So an obvious idea to estimate it is simply to **average the returns observed after visits to that state $s$**

Suppose we wish to estimate $V_\pi(s)$, the value of a **specific state $s$**, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state s in an episode is called a *visit* to $s$, so that $s$ may be visited many times in an episode.

- first-visit MC method estimates $V_\pi(s)$ as the ==average of the returns following first visits to $s$==
- every-visit MC method averages the returns following all visits to $s$

Then the algorithm for First visit MC prediction is:

|                         Abstraction                          |                           Details                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220224201733290.png" alt="image-20220224201733290" style="zoom:50%;" /> | <img src="DL_notes/image-20220224201144309.png" alt="image-20220224201144309" style="zoom:67%;" /> |

where notice that:

- the `if` statement in the highlight box basically checks if $S_t$ is the first time encountered in the sequence of states, action, reward pair.

The every-visit versoin looks like:

<img src="DL_notes/image-20220224201238212.png" alt="image-20220224201238212" style="zoom: 67%;" />

**Both first-visit MC and every-visit MC converge** to $V_\pi(s)$ as the number of visits (or first visits) to s goes to infinity. However, they do have certain different theoretic properties, which is not covered here. In reality, the first visit version is used more often.

Last but not least, we can also leverage DP to make the averaging easier as:
$$
\mu_t = \mu_{t-1} + \frac{1}{t}(x_t - \mu_{t-1})
$$
for each new data $x_t$ that would contribute to $\mu_t$.

---

*For Example:* Black Jack

This would be a nice exercise for you to think of how to convert a real life game into **state/action/reward** where we can use the above method to optimize.

Assuming you are clear of the rules, then

- Each game of blackjack is an episode. 
- Rewards of $+1,-1,0$ are given for winning, losing, and drawing, respectively. 
- All **rewards** within a game are zero, and we do not discount ($\gamma = 1$); therefore these terminal rewards are also the returns. 
- The player's **actions** are to hit or to stick. 
- The **states** depend on the player's cards and the dealer's showing card.

We further impose the assumption that the deck is infinite, so you don't need to remember what cards are dealt, and you are competing independently against the dealer. This then restricts our states and actions to:

- the player makes decisions on the basis of three variables: his current sum  $[12-21]$, the dealer's one showing card ($A-10$, and whether or not he holds a *usable* ace (if the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable)
- if you have sum below $12$, then you definitely call hit so that we don't need to compute.

This makes for a total of 200 states, and we can use MC method to compute best policy.

### Backup Diagram for MC Method

> The general idea of a backup diagram is to show at the top the **root node to be updated** and to show below all the transitions and leaf nodes **whose rewards and estimated values contribute to the update**

Graphically, MC methods when given an episode does:

|                       Iterative Method                       |                          MC Method                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220224205607347.png" alt="image-20220224205607347" style="zoom: 67%;" /> | <img src="DL_notes/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /> |

so the difference with those DP based method is that:

- DP diagram (LEFT) shows **all possible transitions**, the Monte Carlo diagram shows only those **sampled** **on the one episode**. 
- DP diagram includes only **one-step transitions**, the Monte Carlo diagram goes **all the way to the end of the episode**

> In particular, note that the computational expense of estimating the value $V_\pi(s)$ of a single state is **independent of the number of states**. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states.

### MC for Action Values

Again, if the model is not available (i.e. no external device can tell you $p(s',r|s,a)$ directly, or $r(s,a)$), you will need sampling to compute action value functions $Q_\pi$. Luckily, you soon realize the idea is the same as Prediction for state value function

> The policy evaluation problem for action values is to estimate $Q_\pi(s,a)$, the **expected return** when starting in state $s$, taking action $a$, and thereafter following policy $\pi$.
>
> Therefore, now we talk about **visits to a state-action pair** (state $s$ is visited and action $a$ is taken in it) rather than to a state in an episode. We estimates the value of a state-action pair as the **average of the returns** that have followed from:
>
> - the first time in each episode that the state was visited and the action was selected - First Time Method
> - or doing every time method

The only complication is that many state-action pairs **may never be visited**.

- If $\pi$ is a deterministic policy, then in following $\pi$ one will observe returns **only for one of the actions from each state**. 

  One way to fix this is by *specifying* that the episodes start in a state-action pair, and that *every pair has a nonzero probability* of being selected as the start.

- if policy $\pi$ is stochastic with a nonzero probability of selecting all actions in each state, then there is **no problem**. This is more reliable and used more often.

### Monte Carlo Control

We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate **optimal policies** using what we have built up so far (**methods for evaluating value functions**)

To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of **policy evaluation and policy improvement**, beginning with an arbitrary policy $\pi_0$ and ending with the optimal policy and optimal action-value function\
$$
\pi_0 \to q_{\pi_0} \to \pi_1 \to q_{\pi_1} \to ...\pi_* \to q_{\pi_*}
$$
which is like the DP iterative algorithm, but  now:

- Policy evaluation step: **Generating episodes** and using MC method to compute $q_{\pi_i}$ following that $\pi_i$

- Policy improvement: **Making the policy greedy** with respect to the current value function
  $$
  \pi_{k+1}(s) = \arg \max_a q_{k}(s,a)
  $$

Hence the full algorithm looks like:

<img src="DL_notes/image-20220224213351523.png" alt="image-20220224213351523" style="zoom:67%;" />

## Temporal-Difference Learning

TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.

- Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics
- Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).

Again this is a model-free approach.

> *Recall*: Model Free vs Model-Based
>
> **Model-based**: the agent, while learning, can ask the model for the *expected* next reward, or the full *distribution* of next states and next reward. i.e. it has *complete information for next state, next reward*.
>
> -  e.g. by computer code that understands the rules of a dice or board game
>
> **Model-free**: have nothing, purely sample from experience.
>
> - e.g. MC methods, TD learning, etc.

### TD Prediction

Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$:

- both methods update their estimate $V_\pi(s)$ for the nonterminal states $s_t$ occurring in that experience

However, the difference is that:

- Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V_\pi(s_t)$, i.e.  estimates $V_\pi(s)$ as the average of the returns following first visits to $s$. The every-visit update rule can be generalized to:
  $$
  V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]
  $$
  for $G_t$ is the actual return following time $t$, and $\alpha$ is a constant step-size parameter

- Whereas Monte Carlo methods must **wait until the end of the episode** to determine the increment to $V(s_t)$ (only then is $G_t$ known), TD methods need **wait only until the next time step**:
  $$
  V(s_t) \leftarrow V(s_t) + \alpha[R_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
  $$

Therefore, the big difference stems from the fact that the target value for MC update is $G_t$, but for TD update it is
$$
R_{t+1} + \gamma V(s_{t+1})
$$
The idea basically comes from the derivation that
$$
\begin{align*}
V_\pi (s)
&= \mathbb{E}_\pi [G_t | S_t =s]\\
&= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&= \mathbb{E}_\pi \left[ R_{t+1}+ \gamma V_\pi(s_{t+1})|S_t=s \right]\\
\end{align*}
$$
so that MC is using the first equality for estimating $V_\pi$, DP is using the last equality for estimating $V_\pi$.

- Monte Carlo target is an estimate because the expected value in the first line is not known
- The DP target is an estimate because $V_{\pi}(s_{t+1})$ is not known

Then TD is **combining DP + MC** by sampling form the environment like in the first equality, but uses DP for estimate $V_\pi(s)\approx V_\pi(s')$ and converge for solution. Hence the algorithm is

<img src="DL_notes/image-20220301170155096.png" alt="image-20220301170155096" style="zoom: 67%;" />

Graphically:

|                  Backoff Diagram for TD(0)                   |                    Backoff Diagram for MC                    |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220224223527026](DL_notes/image-20220224223527026.png) | <img src="DL_notes/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /> |

On a larger scale, you are basically only looking ahead a single step

<img src="DL_notes/image-20220224223446750.png" alt="image-20220224223446750" style="zoom: 50%;" />

---

*For Example*

Consider predicting the time for you to return to your home from an office.

<img src="DL_notes/image-20220224225822737.png" alt="image-20220224225822737" style="zoom:67%;" />

then in this example:

- **state** is given as "leaving office". "reach car", etc.
- **return** for each state is the actual time to go *from that state*
- **value** of each state $V_\pi(s)$ is the expected/actual time to go. Current **estimated value** for each state will there for be the "Predicted Time to Go"
- we pick $\gamma=1$

Then, if we do MC method, then essentially updates are proportional to the difference between $G_t - V(s_t)$, where $G_t$ is the actual time in the end:

![image-20220224230158369](DL_notes/image-20220224230158369.png)

essentially the error here would be the difference between "Actual Time to Go" and "Predicted Time to Go"

If we do TD method, then the update will be based on the **next immediate observed value** $R_{t+1}+\gamma V(s_{t+1})-V(s_t)$, hence it is ==Markov like== and looks like

![image-20220224230353766](DL_notes/image-20220224230353766.png)

where the length of the arrow is proportional to the error, proportional to the **temporal difference**.

### TD vs MC Methods

Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\alpha$, as long as $\alpha$ is chosen to be sufficiently small. The constant-$\alpha$ MC method also converges deterministically under the same conditions, but to a **different** answer.

---

*For Example*

Suppose you observe the following eight episodes:

<img src="DL_notes/image-20220301175321270.png" alt="image-20220301175321270" style="zoom:67%;" />

where we would like to estimate $V(A),V(B)$:

- the true value of $V(B)$ will be clearly $6/8$ because six out of the eight times in state $B$ the process terminated immediately with a return of $1$ and the other two $0$. This would also be the answer that both MC and TD will converge to.

- the value of $V(A)$ differs:
  - Observe that $100\%$ of the times the process was in state $A$ it traversed immediately to $B$ (with a reward of $0$); and since we have already decided that $B$ has value $V(B)=3/4$ , therefore $A$ must have value $3/4$ as well. This would be what TD converges to (in TD, the update depends on the difference between current reward and **next expected reward**).

    Graphically:

    <img src="DL_notes/image-20220301175551863.png" alt="image-20220301175551863" style="zoom:67%;" />

    which is basically **Markov**

  - The other reasonable answer is simply to observe that we have seen $A$ once and the return that followed it was $0$; we therefore estimate $V(A)$ as $0$. This is the answer that batch Monte Carlo methods give.

    (recall that in MC, the update depends on the difference between current reward and **final reward in the episode**.)

Consider optimal estimates in the sense that they **minimize the mean-squared error from the actual returns** in the training set, then

- MC would converge to the state value that gives minimum squared error on the training data.
- TD estimate would be exactly correct for the maximum-likelihood model of the **Markov process** (i.e. based on first
  modeling the Markov process, then computing the correct estimates given the model)

---

**Advantages of TD Method**

- an advantage over DP methods in that they do not require a model of the environment

- most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an **on-line**, fully incremental fashion.
- even as it is online, for any fixed policy $\pi$, the TD algorithm described above has been proved to **converge** to $V_\pi$,

### TD lambda

Notice that another view of the difference between TD and MC is essentially their backoff diagram:

<img src="DL_notes/image-20220301133529126.png" alt="image-20220301133529126" style="zoom: 67%;" />

where we notices that:

- TD looks at one step ahead in backoff diagram and updates **only depend on the next state**
- MC looks at a full height/depth in backoff diagram and updates depends on all the state until terminal

Then in TD $\lambda$, we essentially combine the two:

<img src="https://miro.medium.com/max/1838/1*L-LUOyW5W-0gBxx80GHdHQ.png" alt="Reinforcement Learning  TD() Introduction(1) | by Jeremy Zhang | Towards  Data Science" style="zoom:33%;" />

so that basically $TD(0)$ with $\lambda = 0$ is the same as the TD prediction we discussed in the previous section. 

## On/Off Policy Learning

On policy methods:

1. Estimate the value of a policy while using it for control (i.e. generating episodes)
2. Evaluate or improve the policy that is used to make decision (e.g. take greedy step)

Off policy methods

- Evaluate or improve a policy **different from that used to generate the data**
- Separate these two functions
- Behavior policy: policy used to generate behaviour
- Target policy: policy that is imitated and improved
- Follow behavior policy while improving target policy
- Reuse experience generated from old policie

### SARSA

SARSA is an **on-policy method** using TD methods for the evaluation of a **action value function**. In particular, this is done by:

1. for an on-policy method we must estimate $Q_\pi(s,a)$ for the current behavior policy $\pi$ and for all states $s$ and actions $a$.

Recall that an episode essentially looks like:

<img src="DL_notes/image-20220301202009996.png" alt="image-20220301202009996" style="zoom:67%;" />

where:

- In the previous section we considered transitions from state to state and **learned the values of states**. Now we consider transitions from state-action pair to state{action pair, and learn the **value of state-action pairs**

- therefore, as it is TD learning, we consider the update rule being
  $$
  Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) ]
  $$
  This update is done after every transition from a nonterminal state $s_t$.

  This rule uses every element of the quintuple of events, $(s,a,r',s',a')$, hence it is called SARSA

As in all on-policy methods, we continually **estimate $Q_\pi$ for the behavior policy $\pi$,** and at the same time **change $\pi$ toward greediness** with respect to $Q_\pi$.

Algorithm:

<img src="DL_notes/image-20220301201435273.png" alt="image-20220301201435273" style="zoom: 50%;" />

where 

- in Sutton and Bartol, the "improve policy" step is also called "policy derived from $Q$ from $\epsilon$-greedy"
- it can be shown that this will converge to some **optimal $Q_\pi$**, hence returning optimal policy
- note that this update of $Q_\pi$ is **updating  $Q_\pi$ for the $\pi$ it is currently following**. Hence it is called ==on policy==.

### Q-Learning

Recall that we know:
$$
Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]
$$
But we know that here we are model-less, hence we *don't know $p(s',r|s,a)$*. The idea is to essentially will learn the $Q$ function directly from experience, known as Q-learning:
$$
Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]
$$
comes from the **optimality constraint**, where the target is $R_{r+1} + \gamma \max_a Q(s_{t+1},a)$:
$$
\begin{align*}
Q^* (s,a)
&= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]
\end{align*}
$$

> **Therefore**, Q-learning **directly approximates $Q^*$** using MC + DP like approach: TD update rule.

Hence, the algorithm looks like:

<img src="DL_notes/image-20220301203045085.png" alt="image-20220301203045085" style="zoom: 80%;" />

where in this case

- the TD prediction task is to **evaluate $V_\pi(s)$** when given $\pi$, but here we can directly find $Q^*$ hence $\pi^*$!
- note that this update of $Q$ is **updating  $Q$ for the $\pi^*$** by directly estimating $Q^*$, but it **behaves by $\pi_b$**, which could be a policy with equal probability for every action. Hence it is **not** estimating $Q_\pi$ for the $\pi$ that generated the action like the one in SARSA. Hence it is called ==off policy==.
  - this way it is also continuously exploring


Since this is TD like rule, the backoff tree is also Markov like:

![image-20220224232119132](DL_notes/image-20220224232119132.png)

where updating the root node only depends on nodes one step away.

---

*For Example*: Cliff Walking

This example aims to compare the difference of the optimal policy learnt by SARSA and Q-learning. Consider the game of cliff walking, where you will have a start state $S$, and an end state $G$. Additionally:

- each state along any element will have reward of $-1$
- if you fall off the cliff (i.e. on the cliff cell), you have $-100$ and will be sent back to the start state
- the available actions are still left, right, up and down

Graphically:

![image-20220301205614330](DL_notes/image-20220301205614330.png)

Interestingly, assuming our action will be $\epsilon$-greedy (i.e. optimal for $1-\epsilon$ of the time)

- **SARSA** will converge to the **safe path** while **Q-learning** to the **optimal** path

  - so Q-learning results in its occasionally falling off the cliff because of the $\epsilon$-greedy action selection
  - SARSA takes the action selection into account and learns the longer but safer path through the upper part of the grid

  So in a sense Q-learning is **optimistic** about what happens when an action $a$ is taken, while SARSA is **realistic** about it.

- Of course, if $\epsilon$ were gradually reduced, then **both** methods would asymptotically converge to the optimal policy.

For those who are interested, the learning curve looks like:

<img src="DL_notes/image-20220301210416783.png" alt="image-20220301210416783" style="zoom:80%;" />

## Maximum Entropy RL

Before, our objective for those algorithms are to learn the **optimal policy by maximizing the expected return**.
$$
\pi^* = \arg\max_\pi \mathbb{E}[G_t]
$$
However, another interesting objective to consider is to **also maximize the entropy of actions**\pi^* = \arg\max_\pi \mathbb{E}[G_t]
$$
\pi^* = \arg\max_\pi \mathbb{E}[G_t] + H_\pi(a|s)
$$
where the entropy term is

$$
H_\pi(a|s) = \sum_t H_\pi(a_t|s_t) = \sum_t \mathbb{E}[-\log \pi(a_t|s_t)]
$$
Optimizing this objective will promote both high return and exploration. This then will lead to designing loss functions when doing NN based reinforcement learning.

## Summary of RL Models

In general, we have covered RL methods that are

- **Value-based**
  - Estimate value function $Q^*(s,a)$ or $V^*(s)$
  - then return policy by greedy
- **Policy-based**
  - Search directly for optimal policy $\pi$ Achieving maximum future reward
- **Model-based**
  - here it refers to you are either given the model/interaction environment
  - or you are learning the model itself
    - Build transition model of environment
    - Plan by lookahead using model (i.e. ask the model what the reward if action $a$ is taken, then choose best action)

Taxonomy:

<img src="DL_notes/image-20220301135343105.png" alt="image-20220301135343105" style="zoom: 50%;" />

---

*For Example*: World Model

> Our *world model* can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment

The architecture is basically as follows, where as the observation is the game

<img src="https://miro.medium.com/proxy/1*9WsfCbPtdoKzL5IPThLWRg.png" alt="World models  a reinforcement learning story | by SmartLab AI | Medium" style="zoom:50%;" />

then, notice that the controller $C$ output also to the RNN model, which **models the game** (in-game graphics)

- therefore, once trained, you can essentially play by just using RNN model, by feeding in actions to it and let it generate the output observaton/images
- since the policy it learns will then be **based on how $M$ models the game**, it is essentially learning the model.

# Deep Reinforcement Learning

*Recall that*

The previous chapter presents MDPs and reinforcement learning. A key difference between the two is that 

- when solving MDPs we know the transition function $T$ and reward function $R$
- in reinforcement learning we do not know the transition or reward functions. In reinforcement learning an agent samples the
  environment and the previous chapter ends with the Q-learning algorithm which learns $Q^*(s, a)$ from experience. 

But regardless which one we use, they are all **tabular methods** in nature, i.e. we need to he value function in some table. (e.g. for a state function with $|S|$ states, your table will be $|S|$ in size)

---

In many cases, storing the $Q$ values in a table may be infeasible when the state or action spaces are very large or when they are continuous. For example, the game of Go consists of $10^{170}$ states, or when the state or action spaces include continuous variables or complex sensations. A solution is to **approximate the value function or approximate the policy**.

> In Deep RL, we essentially take examples from a desired function (e.g., a value function) and attempts to generalize from them
> to construct an **approximation of the entire function** (e.g. in the entire continuous domain).
>
> - e.g. instead of predicting image classes using CNN, we may **predict** the values of a state or the **probabilities of actions $\pi(s|a)$ using a neural network**, and based on these probabilities we can take action

Then the idea for using NN as function approximation in RL can be shown below:

<img src="DL_notes/image-20220301135857210.png" alt="image-20220301135857210" style="zoom: 67%;" />

where essentially:

- Given a state $s$ as input, such as an image of pixels
- neural network outputs an approximated vector of probabilities for each action given the state, i.e. $\pi_\theta(a|s)$
- finally we can pick $a_t$ be the action by, e.g. taking the highest probability

To train the model in a supervised case, we have in general

|                    General Supervised ML                     |                        Supervised RL                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220301134342206.png" alt="image-20220301134342206" style="zoom:50%;" /> | <img src="DL_notes/image-20220301134356226.png" alt="image-20220301134356226" style="zoom:50%;" /> |

So that in a supervised case, we would need ground truth actions $a$ as the label to train the neural network to learn the stochastics policy $\pi_\theta(a|s)$ with weights $\theta$.

## Function Approximations

As mentioned before, real-world problems often consists of large or continuous state and action spaces, so tabular methods introduced in the previous section will not work for finding $V_\pi(s)$ or $Q_\pi(a|s)$. Hence, we resort to use **deep neural network** to find **approximations such as**

- $V_\theta(s) \approx V_\pi(s)$ for state value
- or $Q_\theta(s,a)\approx Q_\pi(s,a)$ for action value
- or $\pi_\theta(a|s)$ 

using only some finite number of weights usually much smaller than the total number of possible states.

However, the techniques learnt in previous sections aren't useless, recall that all the **back-off operations** we were doing for estimating some value function at state $s$:

- MC does $V(s_t) \to G_t$, shifting the estimated value towards $G_t$ (by some fraction), for $G_t$ being the **target**
- TD(0) does $V(s_t) \to R_{t+1} + \gamma V(s')$
- TD($\lambda$) does $V(s_t) \to G_t^\lambda$
- DP does $V(s_t) \to \mathbb{E}_\pi[R_{t+1} + \gamma V(s')|S_t=s]$ which is exact since model would be given.

Also, notice that the first three will backup the state $s_t$ only when encountered in some episode, whereas the last one will update every single $s$ as they are known.

> Another way to view this would be to think of the backoff shift as an **input-output pair**: the input of the function will be $s_t$, and you **want** the **output to be close to the RHS of the expression above**. Hence this could be used as a supervised dataset!

### State Value Function Approximation

As we are training a NN to approximate $V_\pi(s)$, e.g. w.r.t. ==some policy $\pi$==, we first need some definition of **loss function**. 

- Most supervised learning methods seek to minimize the **mean-squared error (MSE)**
- the label/**ground truth** will be $V_\pi(s)$ which is unknown, but can be any of the **approximate target** mentioned above, e.g. $R_{t+1} + \gamma \hat{V}_\theta(s')$.

Therefore, often our loss would be:
$$
J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (V_\pi(s) - V_\theta(s))^2 = \frac{1}{2}\mathbb{E}_s[(V_\pi(s) - V_\theta(s))^2] \iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]
$$
where:

- technically this will be a **weighted version** of MSE because $\sum_s \mu(s)=1$ gives relative importance to learn certain states. (because the number of parameters we learn will be much less than $|S|$, we cannot find the exact solution)
- $V_\theta(s)$ will be our approximate, and we want to learn $\theta$.
- in reality we use the sum term for loss, but for doing math using the $\mathbb{E}_s$ notation will be easier.
- so basically the **label is $V_\pi(s)$**, hint that we essentially have a ==supervised training==.

This general form is differentiable, simply:
$$
\nabla_\theta J(\theta) = - \mathbb{E}_s[(V_\pi(s)-V_\theta(s))\cdot \nabla_\theta V_\theta(s)]\\
\theta_{i+1} = \theta_i - \alpha \nabla_\theta J(\theta_i)
$$
for the gradient descent update. If we use **stochastic gradient descent of a single sample**, then we throw away the expectation/weighted average of the samples and do:
$$
\theta_{i+1} = \theta_i + \alpha (V_\pi(s_i) - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)
$$
where now the only **known is $V_\pi(s)$**. Hence, we consider using an **estimate such as the four targets mentioned before**

- MC learning with $V_\pi(s_i) \approx G_i$ being the return following state $s_i$:
  $$
  \theta_{i+1} = \theta_i + \alpha (G_i - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)
  $$

- TD(0) learning with
  $$
  \theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma V_\theta(s_{i+1})- V_\theta(s_i)) \nabla_\theta V_\theta(s_i)
  $$

> Not all approximation targets are **unbiased**. By unbiased, we need:
> $$
> \mathbb{E}[V_i] = V_\pi(s_i)
> $$
> where $V_i$ we inserted estimates such as $G_i$ in MC method.
>
> - for MC, this is **unbiased** because $\mathbb{E}[G_i] = V_\pi(s_i)$ by definition of value function. Therefore, this converges to a locally optimal approximation to $V_\pi(s_i)$.
> - but for methods such as TD($\lambda$), it can be shown that $\lambda < 1$ has a **biased estimate**, hence does not actually converge to a local optimum. (Nevertheless, such bootstrapping methods can be quite effective, and other performance guarantees are available for important special cases)

Essentially, though we do not know the ground truth $V_\pi(s)$, we can assume that every sample we got from reality **is the ground truth**, hence those update rules.

### Action-Value Function Approximation

Here the neural network inputs are the states $s$ and actions $a$ and the network parameterized by $\theta$ outputs a value $Q_\theta(s, a)$.

Then, in the same line, our objective would be be minimizing the MSE between the approximate action-value function $Q_\theta(s, a)$ and $Q_\pi(s,a)$. The idea is basically the same as above:
$$
J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q_\pi(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q_\pi(s,a) - Q_\theta(s,a))^2]
$$
Then computing the gradient and updating in SGD fashion:
$$
\theta_{i+1} = \theta_i + \alpha (Q_\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
$$
where since we don't know $Q_\pi(s,a)$, we use approximates such as:

- MC method:
  $$
  \theta_{i+1} = \theta_i + \alpha (G_i - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
  $$

- TD(0) learning
  $$
  \theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma Q_\theta(s_{i+1},a_{i+1}) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
  $$

> **Note**
>
> Both of the function approximation methods only approximates $Q_\pi(s,a)$ or $V_\pi(s)$ for some $\pi$. It **does not improve $\pi$**, hence it is purely doing the evaluation step.

## Model-Free Methods

Challenges of function approximation in the reinforcement learning setting include that

- the agents experience is **not independent and identically distributed** (IID)
- the agents policy affects the future data it will sample
- the environment may change, i.e. our **target is moving** (hence certain NN should not be used as they assume stationary target)
- may not converge (e.g. biased approximations)

And this section as well as the following are more models that help overcome those challenges. Here, we discuss model-free methods.

Model-free approaches may be divided into 

1. value-based or Q-leaning methods such as NFQ (Riedmiller 2005) and DQN (Mnih et al. 2015)
2. policy-based or policy optimization methods such as PPO (Schulman et al. 2017), and 
3. actor-critic methods such as DDPG (Lillicrap et al. 2016) which are a combination of both (i) and (ii).

### Experience Replay

In supervised learning the training examples may be sampled independently from an underlying distribution, but here data are corelated in time as we are taking actions.

> A solution to this problem, known as experience replay, is to use a replay buffer that stores a collection of previous states, actions, and rewards, specifically storing tuples of $(s, a, r, s')$
>
> Then each saved experience tuple may be **sampled** and used for updating the network weights. 

This means an experience tuple may be used multiple times, which is an efficient use of the data.

### Neural Fitted Q-Learning

Recall that in value approximation we are only approximating $Q_\pi(s,a)$ for some $\pi$. In order to **directly find $\pi^*$**, we can use the idea from **Q-learning** which directly finds $\pi^*$:
$$
Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]
$$
Therefore, the **analogy** here is to do, similarly:
$$
J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q^*(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q^*(s,a) - Q_\theta(s,a))^2]\iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]
$$
where the update rule is the same using SGD:
$$
\theta_{i+1} = \theta_i + \alpha (Q^*\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
$$
finally **approximating $Q^*$** using the update from Q-learning:
$$
\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q(s_{i+1},a) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
$$

> Q-learning diverges when using a neural network since there are correlations between the samples and the target is non-stationary. 
>
> Therefore, to remove the correlations between samples we may generate a data set from the agent's experience and use it as a **supervised training**

Then the algorithm becomes:

<img src="DL_notes/image-20220302001029747.png" alt="image-20220302001029747" style="zoom:67%;" />

where essentially:

- the pocliy we used to explore the world is just using $Q_\theta$, which is just a **forward pass of the NN**
- putting this in a ==supervised== fashion, labels become $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$.
- notice that states and rewards are generated by the environment and therefore the algorithm is **model free**

### Deep Q-Network

> Deep Q-networks (DQN) (Mnih et al. 2015) build upon fitted Q-learning by incorporating a replay buffer and a **second target neural network**.

In NFG we consider labels being $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$, but here we consider **another network with $\theta-$** such that:
$$
y_i = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a)
$$
with $\theta-$ being the **target network**. We uses this by making it appear static as compared to the ever changing $Q_\theta$ so that:
$$
\mathcal{L}(\theta_i) = \mathbb{E}_{(s,a,r,s')\sim D}[(y_i - Q_{\theta_i}(s_i,a_i))^2] = \mathbb{E}[(R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i))^2]
$$
then doing SGD the gradient update step is:
$$
\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) - Q_{\theta}(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)
$$
Then the algorithm is

![](DL_notes/image-20220302003105418.png)

where notice that:

- since we have a target $\hat{Q}$ that is updated much less frequently, this is helping the problem of chasing a non-stationary target as in NFQ. The label $y_i$ becomes dependent on $\hat{Q}_{\theta-}$ rather than $Q_\theta$
- notice that states and rewards are **generated from $Q_\theta$**, but policy learnt is **based on $Q_{\theta-}$**. Hence this is **off-policy** because the action value function learnt is towards a different policy that generated the data.

### Prioritized Replay

Before we are sampling from the replay buffer uniformly, prioritized experience replay (Schaul, Quan, Antonoglou & Silver 2016) samples **important transitions more frequently** which results in more efficient learning.

> We define more important transitions as the ones we **made the large DQN error**
> $$
> \text{Err}(s_i,a_i) = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i)
> $$

Then, we prioritize them by:
$$
\text{Priority}(s_i,a_i)=\frac{p_i^\alpha}{\sum_j p_j^\alpha}
$$
where $p_i$ would be proportional to DQN error and $\alpha$ is hyper-parameter controlling the amount of prioritization

- with $\alpha=0$ you have no prioritization and gets back uniform sampling

## Policy-based Methods

One problem with value based methods in the previous chapter such as NFQ and QDN cannot *easily deal with* **continuous actions**. Since we only know/approximated $Q^*(s,a)$, to convert that to action we need to do something like:

|                        Value Function                        |                        Policy/Action                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220304205646509](DL_notes/image-20220304205646509.png) | ![image-20220304205627824](DL_notes/image-20220304205627824.png) |

but imagining those cells being **infinitesimally small and infinitesimally many** so that the action space is continuous. This will be a headache to optimize over.

Then, the idea is why not **approximate policy itself as a continuous function**?

> **Policy-based methods** work well in continuous spaces for **learning stochastic policies**, the most common example would be having:
> $$
> a \sim \mathcal{N}(\mu_\theta(s), \sigma^2_\theta(s))
> $$
> where we learn $\theta$ from NN.

First, we present the general setup of obtaining the objective function and its gradients.

Consider starting with some policy $\pi_\theta$ (e.g. randomly initialized):

1. an agent then can interact wit the environment to generate (probabilistically) a **trajectory of state-action-reward episodes**:
   $$
   \tau = s_0,a_0,r_0,....,s_t,a_t,r_t
   $$
   and hence obtain a **return $g(\tau)$**:
   $$
   g(\tau) = \sum_t \gamma^t r_t
   $$

2. Then the **value of this policy** would be the the **expected/average return** over all trajectories (since the policy is probabilistic):
   $$
   \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right]
   $$

3. Therefore, naturally we say that the **best policy $\pi_\theta$ would have the highest value**:
   $$
   \max_\theta \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right] \equiv \max_\theta J(\pi_\theta)
   $$

Now, the we can basically do **gradient ascent** by considering:
$$
\theta = \theta + \alpha \nabla_\theta J(\pi_\theta)
$$

### Policy Gradient

The final step is to analytically find the gradient $\nabla_\theta J$. We will derive everything exactly and then replace the integral with the practical sum:
$$
J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \int g(\tau) p_\theta(\tau) d\tau \iff \sum_\tau p_\theta(\tau)g(\tau)
$$
but notice that since $\tau$ is a **trajectory generated by $\pi_\theta$**, we can further find:
$$
p_\theta(\tau) = \prod_t p(s_{t+1}|s_t,a_t)\pi_\theta(a_t|s_t)
$$
Now, consider taking the derivative:
$$
\nabla_\theta J = \int \nabla_\theta[g(\tau) p_\theta(\tau)]d\tau  =\int g(\tau)\nabla_\theta p_\theta(\tau) d\tau
$$
Now we *can* plugin the expression for $p_\theta(\tau)$, but we can **use a trick to remove the product** by considering:
$$
\nabla_\theta \log p_\theta(\tau) = \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)}
$$
hence:
$$
\nabla_\theta J =\int g(\tau)p_\theta(\tau)\nabla_\theta \log p_\theta(\tau) d\tau = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau) \nabla_\theta \log p_\theta(\tau)]
$$
Now we substitute in the $p_\theta(\tau)$:
$$
\nabla_\theta \log p_\theta(\tau) = \nabla_\theta \sum_t[\log [(s'|s,a) + \log \pi_\theta(a|s)]] =  \sum_t\nabla_\theta \log  \pi_\theta(a|s)
$$
so we get finally:
$$
\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]
$$
which we can then **estimate expected value using sums** in practice.

### REINFORCE Algorithm

> The REINFORCE algorithm estimates the policy gradient numerically by Monte- Carlo sampling: using **random samples** to **approximate the policy gradient**.

The idea is simple:

1. set $\nabla_\theta J=0$ for the start. We are treating each episode IID.
2. sample a trajectory $\tau$
3. compute $\nabla_\theta J = \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s)$ in this case
4. ascent and repeat

Then the algorithm is

<img src="DL_notes/image-20220304214257499.png" alt="image-20220304214257499" style="zoom:67%;" />

however, some problems with this algorithm is that:

- updating $\theta$ ==will change $\pi_\theta$==, which will be needed to ==generate $\tau$, the experiences==. Hence there **might be convergence problems**.
- additionally, the above also means this policy gradient have **high variance**

### Score Function Generality

Additionally, the general form of:
$$
g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \to g_t(\tau)\cdot \text{score}(\pi_\theta)
$$
and the $\text{score}$ expression appears in **many places when you do gradient**. Example include:

- Q actor-critic: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot V_t]$
- TD: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot \delta]$
- etc

### Policy Gradient Baseline

The estimate of a gradient over **many sampled experiences $\tau$** would be:
$$
\nabla_\theta J  \approx \hat{g} = \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)})
$$
but notice that if we consider a constant/variable $b$ that is **independent on $\theta$**:
$$
\frac{1}{n}\sum_{i=1}^n (g(\tau^{(i)}) - b) \nabla_\theta \log p_\theta(\tau^{(i)}) =  \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)}) - \frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)})
$$
but notice that for $\lim n\to \infty$
$$
 \frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)}) \to \sum_\tau b \nabla_\theta \log p_\theta  (\tau)\cdot p_\theta(\tau)
$$
and we can show thiat this is zero:

<img src="DL_notes/image-20220304215546064.png" alt="image-20220304215546064" style="zoom:67%;" />

so to measure performance we will usually **subtract of some baseline $b$** in the model, which is still an unbiased model.

## Actor-Critic Methods

On problem with policy-based method is that it updates $\theta$ whose $\tau$ sampled next depends on. This means that there could be **high variance** in the gradient which might cause **convergence problems**. The idea of this chapter is to ==solve== this using ==value function approximation to reduce this variance==.

> Actor-critic methods **combine policy-based methods** with **value-based methods** by using both the policy gradient and value function.

The basic step up will be a GAN like model:

- **actor**: a policy network $\pi_\theta$ with parameters $\theta$
- **critic**: a value network that contains $V_\phi(s)$ or $Q_\phi(s,a)$, or the advantage function $A_\phi(s,a)=g(\tau) - V_\phi(s)$
- **critic provides a loss function for the actor** and the gradients backpropagate from the critic to the actor

Since $g(\tau)$ changes every time we sampled. the idea is we **swap out $g(\tau)$** in the update rule:
$$
\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]
$$
for values such as:

- $g(\tau) \to Q_\phi(s,a)$, which leads to **Q-value actor-critic**
- $g(\tau) \to A_\phi(s,a) = g(\tau) - V_\phi(s)$ for advantage

for some **already fitted value functions**.

Therefore, this means the algorithm looks like

<img src="DL_notes/image-20220304220900366.png" alt="image-20220304220900366" style="zoom:67%;" />

where the:

- the yellow part updates the critic parameter, and the red updates actor

### Advantage Actor-Critic

Here we essentially consider swapping out $g(\tau)$ for $A_\phi(s,a)$ defined by:
$$
A_\phi(s,a) = \mathbb{E}_{r,s'}[r+ \gamma V_{\pi_\theta}(s')  - V_{\pi_\theta}(s)]
$$
which can be **estimated by TD error** $r+\gamma V_\phi(s')-V_\phi(s)$, hence resulting in the objective being
$$
\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \nabla_\theta \log  \pi_\theta(a|s) \cdot \gamma^{t-1}(r+\gamma V_\phi(s')-V_\phi(s)\right]
$$

### Natural Policy Gradient

In reinforcement learning the data set collected depends on the policy, which has the following **risk**:

- if updated parameters result in a poor policy
- will result in poor samples
- algorithm stuck at poor policy

> Therefore, when optimizing policy-based methods **choosing a step size**/learning rate for updating the policy parameters is (one of the) key.

In general, we had:
$$
\theta ' =\theta \gets \theta + \alpha \nabla_\theta J(\pi_\theta) = \theta + \Delta \theta
$$
one idea is to **constrain the size of this change of $J(\pi_\theta)$** by considering Taylor expansion and **restricting the latter term**
$$
J(\pi_{\theta'}) \approx J(\pi_\theta) + \nabla_\theta J(\pi_\theta)^T \Delta \theta
$$
with
$$
\max_{\theta'} \nabla_\theta J(\pi_\theta)^T \Delta \theta \quad \text{s.t.}\quad ||\Delta \theta||_2^2 \le \epsilon
$$
in our case $\Delta \theta = \alpha \nabla_\theta J(\pi_\theta)$. So this can be solved analytically with:\
$$
\Delta \theta = \sqrt{2 \epsilon} \frac{\nabla_\theta J(\pi_\theta)}{||\nabla_\theta J(\pi_\theta)||}
$$
**Alternatively**, we can directly **instead of constraining $\theta$, which constrains $\pi_\theta$**, we can constrain the **episode trajectory** itself. Since the trajectory is essentially a probability distribution, we can use KL divergence can constraint that:
$$
D_{KL}[p(\tau|\pi_\theta)\, |\, p(\tau |  \pi_{\theta'})] \le \epsilon
$$
Then this result in

<img src="DL_notes/image-20220304223202945.png" style="zoom: 67%;" />

which again can be solved analytically, and this called the **natural gradient**
$$
\Delta \theta =F^{-1}_\theta \nabla_\theta J(\pi_\theta)\sqrt{\frac{2\epsilon}{||\nabla_\theta J(\pi_\theta)^T F_\theta^{-1}\nabla_\theta J(\pi_\theta)||}}
$$
and $\nabla_\theta J(\pi_\theta)$ and $F_\theta$ may be approximated by sampling trajectories using conjugate gradient descent

> **Note** that since we are only dealing with $\Delta \theta$ for the first term in approximation, we are essentially using **first order methods**.

### Trust Region Policy Optimization (TRPO)

Essentially a method based on the natural gradient, but here we are:

- using second order method but **approximating the hessian** by using **conjugate gradient**
- alike NPG, constrain the surrogate loss by the **KL divergence** between the new and old policy

This results in the following algorithm

<img src="DL_notes/image-20220303135351156.png" alt="image-20220303135351156" style="zoom: 50%;" />

where surrogate loss is basically the loss that is caused by different policy trajectories.

<img src="DL_notes/image-20220303135530009.png" alt="image-20220303135530009" style="zoom:50%;" />

### Proximal Policy Optimization (PPO)

This method is based on TRPO however:

- is a first-order method that avoids computing the Hessian matrix 
- also avoids line search (at the end of previous algo) by **clipping** the surrogate objective.

This results in the constrained optimization or surrogate objective:

<img src="DL_notes/image-20220303135626037.png" alt="image-20220303135626037" style="zoom:50%;" />

<img src="DL_notes/image-20220304224444380.png" alt="image-20220304224444380" style="zoom:67%;" />

Then the algorithm looks like

<img src="DL_notes/image-20220304224501324.png" alt="image-20220304224501324" style="zoom:67%;" />

### Deep Deterministic Policy Gradient (DDPG)

DDPG may be used in continuous action spaces and combines **DQN** with **REINFORCE**. Essentially the idea is again, the actor critic loop:

- use DQN to estimate $Q_\phi(s,a)$ which will be a **critic**
- use REINFORCE to estimate $\pi_\theta(s)$ which will be an **actor**

The loss and gradients are defined by:

|                         Critic Loss                          |                          Actor Loss                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220304224806600.png" alt="image-20220304224806600" style="zoom:67%;" /> | <img src="DL_notes/image-20220304224852363.png" alt="image-20220304224852363" style="zoom:67%;" /> |

and

<img src="DL_notes/image-20220304224824803.png" alt="image-20220304224824803" style="zoom: 67%;" />

The algorithm being

<img src="DL_notes/image-20220304224921793.png" alt="image-20220304224921793" style="zoom:67%;" />

## Model-based Reinforcement Learning

Here, model-based reinforcement learning means we are **learning the environment model**, e.g. the transitions, to give us optimal policies. This can further be split into to kinds:

- given model/environment: AlphaZero
- learn model by sampling: World Model

### Monte-Carlo Tree Search

> A tree search starts at the root and explores nodes from there, **looking for one particular node** that satisfies the conditions mentioned in the problem.

Tree search has been used in cases such as board games, where we essentially want to do the best next move when we have a selection of possible moves $A$ and a selection of states $S$ to be in:

<img src="DL_notes/image-20220303140820378.png" alt="image-20220303140820378" style="zoom:50%;" />

However, solving this exactly **would need time complexity** of:
$$
O\left((|S||A|)^d\right)
$$
which is **exponential in time**. Hence, the idea for MC Tree Search is to **sample some branch only** to keep the exploration going.

> Monte-Carlo Tree Search (MCTS) runs simulations from a given state and therefore has time complexity of $O(nd)$ where $n$ is the number of simulations and $d$ the tree depth.

Then, once done, we **select actions based on Upper Confidence Bound**:
$$
Q(s,a) + c \sqrt{\frac{\log N(s)}{N(s,a)}}
$$
for $c$ being an exploration constant and

- $N(s,a)$ being the **number of action state pairs**
- $N(s) = \sum_a N(s,a)$ is the number of **state visits**

So we get the algorithm being:

<img src="DL_notes/image-20220304230357312.png" alt="image-20220304230357312" style="zoom:67%;" />

### Expert Iteration and AlphaZero

> Expert iteration, or AlphaZero, uses 
>
> - a neural network to output a policy approximation $\pi_\theta(a|s)$ and state value function $V_\phi(s)$ approximation for guiding MCTS.
> - use MCTS to output next best action
>
> Once dones, this is **merged** into a single network $f_\theta(s)$ that:
>
> - receives a state representation as input $s$ 
> - outputs a vector of probabilities **$p_\theta = P(a|s)$ over all valid actions** a and **state values $V_\theta (s)$ over states $s$.**
>
> And AlphaZero learns these action probabilities and estimated values from **games of self-play**

The parameters $\theta$ are updated by stochastic gradient descent on the following loss function:
$$
L(\theta) = - \pi \log  p + (V- e)^2 + \alpha ||\theta||^2
$$
where:

- the first term is a **cross entropy loss** between policy vector $p$ and search probabilities $\pi$ you have in reality
- the second term aims to **minimize the difference** between predicted performance $V$ and actual evaluation $e$
- the third is a **regularization term**

Then AlphaZero uses MCTS which is a stochastic search using upper confidence bound update rule of the action-value function
$$
U(s,a) = Q(s,a) + cP(a|s) \frac{\sqrt{N(s)}}{1+N(s,a)}
$$
where:

- $N(s,a)$ is the number of times action $a$ was taken from state $s$
- $P(a|s)=p_\theta(a)$ is the NN output of probability taking action $a$ from state $s$

Then at each step we take:

1. action $\arg\max_a U(s,a)$
2. add this new state to a tree
3. use MC Tree search on the new state and repeat

Graphically we are doing

<img src="DL_notes/image-20220303141302315.png" alt="image-20220303141302315" style="zoom: 67%;" />

### World Models

> A world model is a neural game simulator that uses a VAE and RNN to take action in an environment, which in turn **can be used to model the world/game itself**.

The overall structure has been covered before

<img src="DL_notes/image-20220303142142395.png" alt="image-20220303142142395" style="zoom:50%;" />

where we essentially learnt $M$.

- the VAE is trained on images from the environment learning a **low dimension latent representation** $z$ of state $s$.
- RNN is trained on the VAE latent vectors $z_t$ through time **predicting** $p(z_{t+1}|a_t, z_t, h_t)$.

- $z_t$ and RNN hidden vector $h_t$ are fed into a **neural network controller** which **outputs an action** that effects the environment

In addition, the power of this model is that:

- essentially the VAE addition could be used to generate and continue playing the game, because we can feed in from RNN and VAE can **"reconstruct" the image**
- some difference between GAN and this model is that here we are **learning/reconstructing the latent space $z$**, instead of the real data itself directly.

## Imitation Learning

Rather than learning from rewards, imitation learning **learns from example demonstrations** provided by an expert - ==behavior cloning==.

Therefore, we basically consider **learning $\pi_\theta$** that clones the **expert demonstration $(s,a) \sim D$**:
$$
\arg\max_\theta \sum_{(s,a) \in D} \log \pi_\theta(a|s)
$$

> However, learning a policy like by imitation has a natural pitfall: if it **encountered situations not well represented by the demonstrations** it may perform poorly, and once encountered may not recover from cascading errors.

Related models in this area include:

- **Dataset aggregation** (DAgger) (Ross, Gordon & Bagnell 2011) aims to *solve the problem of cascading errors by augmenting the data* with expert action labels of policy rollouts

- **Stochastic miximg iterative learning** (SMILe) (Ross & Bagnell 2010) trains a *new policy only on the augmented data* and then mixes the new policy with the previous policies

- **Generative adversarial imitation learning** (GAIL) (Ho & Ermon 2016) uses state and action examples $(s,a) \sim P_{real}$ from expert demonstrations as real samples for a discriminator in a GAN setting, and ask the generator to leanr $\pi_\theta(a|s)$:

  <img src="DL_notes/image-20220308131653510.png" alt="image-20220308131653510" style="zoom: 67%;" />

  so that basically discriminators $D_\phi$ wants to distinguish the actions from expert demonstrations $(s,a) \sim P_{real}$ and the generated pair $(s,\pi_\theta(s))$.

- **Inverse reinforcement learning** explicitly derives a *reward function from a set of expert demonstrations* and *uses that reward to learn an optimal policy*

## Maintaining Exploration

As we know, if a model has $0$ exploration then it might **not converge** to the optimal solution as it will be stuck with greedy choices. Hence we have introduced $\epsilon$-greedy  approach that does random action with probability $\epsilon$.

Here, we introduce **another way to promote exploration**, which is to add some **internal reward** (i.e. not explicitly given from the environment) each time when a **new state is explored**.

- this approach would also be useful for environments with **sparse rewards**

Related models include

- **Go-Explore**: provide bonus rewards for novel states, you can encourage agents to explore more of the state space, even if they dont receive any external reward from the environment.

  However,  one problem with these approaches is that they do a poor job at continuing to explore promising areas far away from the start state: **Detachment**

  <img src="https://www.alexirpan.com/public/go-explore/detachment.png" alt="Detachment diagram" style="zoom: 33%;" />

  And the proposed solution is to maintain a **memory of previously visited novel states**, so that:

  1. When learning, the agent first *randomly samples a previously visited state*, biased towards newer ones.
  2. It travels to that state, then *explores from that state*.

  Therefore, by chance we will **eventually resample a state near the boundary of visited states**, and from there it is easy to discover unvisited novel states

## Multiagent System

Here we consider essentially **other agents/players** in the system, which introduces the following changes:

- **Environments** that contain multiple agents
  - Agents act autonomously
  - Outcome depends on actions of all agents
  - Agents maximize their own reward
- **Observation**
  - Perfect information (i.e. your action decision can be made deterministically as you *already see/have everything you need*)
    - e.g. chess
  - Imperfect information (i.e. your action decision would be probabilistic because you *do not see/have opponent information yet*)
    - e.g. rock-paper-scissors
- **Competitive - cooperative**
  - Fully competitive zero-sum games: checkers, chess, Go
  - Fully cooperative

### Strategic Game

A matrix game (strategic game) can be described as follows

> Formally, we have the following setup:
>
> - $n$ agents, numbered from $i=1,...,n$
> - at each step, we have some **action profile $(a_1, ..., a_n)$** representing the action each agent $i$ took
> - utility function for each agent $(u_1,...,u_n)$ measures the "utility" of their action $(a_1, ...,a_n)$
> - an outcome of the game is the joint action of all agents
>
> We **assume** that **each agent maximizes its own utility over each outcome**

Additionally, we can represent the games they play as a **tree**, for example:

<img src="DL_notes/image-20220308201011752.png" alt="image-20220308201011752" style="zoom:50%;" />

in this example:

- **nodes** are states, edges are actions, leaves are utilities, e.g. $(u_1,u_2)$
- each node is **labeled with an agent** that can control it, so edge from a node is the action the agent at the node takes
- each path to a leaf is a **run**

---

*Perfect Information Game Example*

 in a **perfect information game**, we can explore the state by **not caring how you get to the state**

<img src="DL_notes/image-20220308133714063.png" alt="image-20220308133714063" style="zoom:50%;" />

therefore, the value of a state is based on the **subtree** where the current node is the root.

---

*Imperfect Information Game*

Here, we do not have everything you need to determine your action to win 100%. An example would be **rock-paper-scissor**, which we can characterize by:

|                            Matrix                            |                             Tree                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220308204135128.png" alt="image-20220308204135128" style="zoom:67%;" /> | <img src="DL_notes/image-20220308204119834.png" alt="image-20220308204119834" style="zoom:67%;" /> |

where the situation is that, if you are player B on the right figure, you do not know what A chose and hence all three states look the same to you

- if either player has some deterministic strategy, the player would be **exploited**. Hence the optimal solution, or its ==Nash Equilibrium==, is to **choose a random policy of $1/3$ for each action.** This probabilistic policy is often referred to as **mixed strategy**, whereas the deterministic one is called **pure strategy**.

- This can also be seen as a **zero-sum matrix game**. When we say that it is a zero-sum game, we mean that one wins the same amount as the other loses.

### Nash Equilibrium

The idea of Nash Equilibrium in a game is a collection of **strategies for all players** such that **no player can do better by changing its own strategy** given that **other players continue playing their NE strategies**

> Formally, the Nash Equilibrium set of strategy for all $n$ players is $(\pi_1^*, ...,\pi_n^*)$ such that
> $$
> V_i(\pi_1^*,...,\pi_i^*,...,\pi_n^*) \ge V_i(\pi_1^*,...,\pi_i,...,\pi_n^*) ,\quad i=1,...,n
> $$
> where $V_i(\cdot)$ is player $i$'s value function which is player $i$'s **expected reward/utility given all players' strategies**

It is important to know that:

- Clearly, the goal of converging to Nash equilibria is a good starting point for multi-agent learning algorithms. However, there commonly can be multiple Nash equilibria with different payouts, leading to greater difficulty in evaluating multiagent learning compared to single agent RL

- being in Nash Equilibrium therefore **does not equal to maximal gain/utility** attainable in a game, as it is **assuming** that all players want to maximize their **own benefit**

  An example would be the following matrix game:

  <img src="DL_notes/image-20220308205612393.png" alt="image-20220308205612393" style="zoom:67%;" />

  where the NE is for each player to *take*, as it maximizes their own benefit. However, the optimal solution is of course for both to *give*.

> In fact:
>
> - in **Perfect information game**, a **unique Nash equilibrium/optimal** is the value for player of current node in tree (as the value would be representative of all possible outcomes)
> - in **Imperfect information game**, there may be **multiple Nash equilibria**, no deterministic strategy may be optimal.

*For Example*: Simple Nash Equilibrium

<img src="DL_notes/image-20220308211944291.png" alt="image-20220308211944291" style="zoom:67%;" />

since the entire action space is shown, we see that:

- "Top + Left" is a **Nash Equilibrium** because either can benefit themselves more if the other player stayed there. A can switch to Bottom, but this would reduce his payoff from $5$ to $1$. Player B can switch from left to right, but this would reduce his payoff from $4$ to $3$. "Bottom + Right" is **also a Nash Equilibrium** for the same reason
- other cells are not because one player will be gained more by shifting away.

---

*For Example*: Mixed Nash Equilibrium Strategy

To produce mixed strategy, we **must** have at least one player going random policy, as otherwise the optimal solutoin will be determinstic as you can just exploit it.

Consider the example of *battle of sexes*, where the reward is shown in the table:

<img src="DL_notes/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" />

We now **suppose that** woman go to the Baseball game with ==probability $p$,== and the Man go to the Baseball game with probability $q$. Then, we can look at the **expected return** for each party after choosing an action:

<img src="DL_notes/image-20220308213426800.png" alt="image-20220308213426800"  />

hence, notice that:

- A mixed strategy in the battle of the sexes game requires both parties to randomize

- for **Man to randomize=indifference between going Baseball or Ballet**, we need
  $$
  1+2p = 2-2p \quad \to \quad p=1/4
  $$
  for woman

- for the **Woman to randomize**, the Woman must get equal payoffs from going to the Baseball game and going to the Ballet, which requires:
  $$
  2q = 3-2q \quad \to \quad q=3/4
  $$

- notice that the **above are independent decisions**, hence we can pick $q=3/4$ ==and== $p=1/4$ to achieve ==Nash Equilibrium== so that the probability of man/woman going to each sport being given by:

  <img src="DL_notes/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" />

  which is the result if man and woman following the NE.

Then, putting together the mixed strategy and the reward, we can compute the **expected payoff**:

|                            Reward                            |                        Random Policy                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="DL_notes/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" /> | <img src="DL_notes/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" /> |

where: 

- basically we do element-wise multiplication for the two matrix and will get
  $$
  \mathbb{E}[\text{man reward}] = \frac{9}{16} + \frac{9}{16} + \frac{6}{16} + 0 = \frac{3}{2}\\
  \mathbb{E}[\text{woman reward}] = \frac{6}{16} + \frac{9}{16} + \frac{9}{16} + 0 = \frac{3}{2}\\
  $$

- the above Nash equilibrium (following $p,q$) is indeed **smaller than if they had coordination** to go both baseball or both ballet.

> Here we have shown that **randomization** requires **equality of expected payoffs** (as otherwise it becomes deterministic), which would then result in Nash Equilibrium.

---

*For Example*:

Consider the game of guessing coin flip:

<img src="DL_notes/image-20220308211127704.png" alt="image-20220308211127704" style="zoom: 50%;" />

essentially, if $P1$'s coin is guessed correctly by $P2$ then $P1$ receives a penalty. We want to **consider the Nash Equilibrium **

|    Player 2 Guessing Head with $p=3/4$  and tail $p=1/4$     |                       Player 1                       |
| :----------------------------------------------------------: | :--------------------------------------------------: |
| ![image-20220308232956782](DL_notes/image-20220308232956782.png) | Player 1 can always achieve 0 by always playing sell |

where the key observation for imperfect game is that:

- for imperfect information game, you need to consider **the full tree** for possibility to determine your **optimal strategy**

### Counterfactual Regret Minimization

Key algorithm used in imperfect information games

- **Counterfactual**: If I had known
- **Regret**: how much better would I have done if I did something else instead?
- **Minimization**: what strategy minimizes my overall regret?

Then a Monte Carlo CFR does:

- Player 1 explores all options and player 2 samples actions
- Traverse entire tree
- Repeat switching roles: player 1 samples actions and player 2 explores all actions 

<img src="DL_notes/image-20220308141226579.png" alt="image-20220308141226579" style="zoom:50%;" />

where:

- we know that player 1 played green, which gained 50. But the other options are to get 0 or 100. Hence the regret for them is -50 and 50.
- use neural network to approximate the tree, which **guides the CFR search**

### Unification

One idea is that perfect information can be seen as a special case of imperfect information:

- realize that both basically includes a **Tree Search + DNN** to simulate the tree
  - perfect information does DNN + MC tree search
  - imperfect information does DNN + MC-CFR

- now a unified algorithm has come up which essentially can play both Chess, Go as well as games like Poker.

# Optional

Some random stuff added in lecture notes

## Adversarial Attacks

The idea is that we **add some noise** in an image, such that the **classification output** would be ==different==

- targeted
- untargeted

<img src="DL_notes/image-20220217135858473.png" alt="image-20220217135858473" style="zoom:33%;" />

The idea is therefore to do

<img src="DL_notes/image-20220217140014237.png" alt="image-20220217140014237" style="zoom:33%;" />

then

<img src="DL_notes/image-20220217135942981.png" alt="image-20220217135942981" style="zoom:33%;" />

which obviously requires you to know the network architecture in advance

## Problem Solving with DL

https://www.cs.columbia.edu/~idrori/drori2021math.pdf

> We demonstrate that a neural network pre-trained on text **and** finetuned on code solves Mathematics problems by program synthesis.
>
> - In this work we demonstrate that program synthesis is the key to solving math and STEM courses at scale, by turning questions into programming tasks

Some of the previous interesting work

- When **paired with graph neural networks (GNNs)** to predict arithmetic expression trees, Transformers pre-trained on text have been used to solve university level problems in Machine Learning (14) with up to 95% accuracy.

  however this previous work is limited to numeric answers and isolated to a specific course and does not readily scale to other courses

- it works because **humans solving problems by imaging them into computation trees**

---

Here, the idea is to:

![image-20220310205626517](DL_notes/image-20220310205626517.png)

1. turn question into programming task
   - add context (e.g. what course it is, e.g. use `numpy`)
     - assume that background knowledges from the available code repos on Github. e.g. if we are to solve a physics problem, it will assume physics knowledge from existing repos.
   - using codex to prompt codex (e.g. given a question, output a programming task)
2. automatically genreate program using a Transformer, OpenAI Codex, pre-trained on text and fine-tuned on code
3. execute program to obtain and evluate answers
4. automatically explain correct solution using codex

> The key takeaways is that:
>
> - transformers pretrained on text does **not directly work** on solving math problems
> - program synthesis using codex can assume "knowledge" of physics, math, etc. if there exists related code on Github

But exactly what context is needed?

![image-20220310210250349](DL_notes/image-20220310210250349.png)

In short we have mainly three kinds:

- **Topic Context**:  For example, without context, a question about networks may be about neural networks or communication networks, and therefore specifying the context is required.
- **Library Context**: using the Python libraries sympy and streamplot are added for solving the question and plotting a visualization
- **Definitions Context**: Often times, Codex doesnt have real-world grounding for what certain terms are defined as. As an illustrative example, Codex lacks knowledge of what a "Full House" means in poker. Explicitly defining these in terms that Codex understands can better guide its program synthesis.

Then, when transforming questions to Codex prompts, a key consideration is **how semantically close the original question is to a prompt that produces a correct solution**:

- To measure the distance between the original questions and the successful prompts, we use cosine similarity between Sentence-BERT
- we can also run **clusterings** from thos embeddings to see how courses are interrelated.

Finally, we can **create new questions** using Codex:

- We use Codex to generate new questions for each course. This is done by creating a numbered list of questions from the dataset. This list is cut off after a random number of questions, and the result is used to **prompt Codex to generate the next question**.

**More details on using Codex**

- We use OpenAIs davinci-codex engine for all of our generations. We fix all of Codexs hyperparameters to be the same for all experiments: top-$p$ which is the portion p of the token probability mass a language model samples from at each step is set to 1, sampling temperature is set to 0 (i.e. argmax), and response length is set to 200 tokens. 
- Both frequency and presence penalty are set to 0, and we do not halt on any stop sequences. Each prompt is structured as a Python documentation comment surrounded by triple quotations and line breaks. 

# Tutorials

Tutorials after classes

## Remote Jupyter

Here are some notes on how to setup a remote jupyter server and connect to it locally using VSCode.

Firstly, on the **remote** (e.g. `ssh` into it)

1. make sure you have already generated a **settings file** for you jupyter server. This should be located under `~/.jupyter/jupyter_notebook_config.py`. If not, you can generate it by:

   ```bash
   jupyter notebook --generate-config
   ```

2. Make sure that you have the following two lines in your config file

   ```config
   c.NotebookApp.allow_origin = '*' #allow all origins
   c.NotebookApp.ip = '0.0.0.0' # listen on all IPs
   ```

3. Start your jupyter server by doing (consider putting it into `tmux`):

   ```bash
   $ jupyter notebook --no-browser --port=9999
   # some logs omitted
   [I 13:00:56.602 NotebookApp] Jupyter Notebook 6.4.5 is running at:
   [I 13:00:56.602 NotebookApp] http://ip_of_your_server:9999/?token=xxx
   ```

   take a note of the line `http://ip_of_your_server:9999/?token=xxx`, which will be used later

Then, on your **local machine**, open up VSCode and:

1. connect to the remote jupyter server by putting in the url `http://ip_of_your_server:9999/?token=xxx` into the following popup

   ![image-20220223180806747](DL_notes/image-20220223180806747.png)

2. Reload the window, and if successful, you will be able to see a **remote kernel** at the following pop up

   ![image-20220223180957821](DL_notes/image-20220223180957821.png)

## PyTorch

In total there will be three parts of the tutorial.

1. Part I: introduction, compute derivatives, build a simple NN

What is PyTorch? Its a Python-based scientific computing package targeted at two sets of audiences:

- A replacement for NumPy to use the power of GPUs
- a deep learning research platform that provides maximum flexibility and speed

See the `tutorial/pytorch` for more details

### Part I

Basically data structures are very similar to the `np.array`:

```python
x = torch.zeros(5, 3, dtype=torch.long) # again same as numpy
print(x)
"""
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
"""
x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes
print(x)
"""
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
"""
# Generates random values from **Normal distribution**
x = torch.randn_like(x, dtype=torch.float)    # override dtype!
print(x)        # result has the same size as x
"""
tensor([[-0.0351,  1.3674,  2.3571],
        [ 0.1267,  1.2764, -1.0151],
        [ 1.8572, -3.1128,  0.5551],
        [-0.6236,  0.6248,  0.1378],
        [-0.8379, -0.2349, -1.0931]])
"""

print(x.size()) # instead of x.shape
```

note that:

- So, essentially, `new_ones` allows you to quickly create a ==new== `torch.Tensor` on the same device and data type ==from a *previously existing* tensor== (with ones), whereas `ones()` serves the purpose of creating a `torch.Tensor` ==from scratch== (filled with ones).

Some additional feature for arithmetic

```python
# normal way
p = torch.rand(5, 3)
q = torch.rand(5, 3)
print(p + q)

# method 2
result = torch.empty(5, 3)
torch.add(p, q, out=result) # as a paramter
print(result)

# method 3
q.add_(p) # in-place, noitce the UNDERSCORE
print(q)
```

Common Operations:

```python
# slicing is the same as numpy
print(result)
print(result[:, 1]) # all rows, 1st column only
"""
tensor([[1.2144, 0.3950, 0.9391],
        [1.0756, 1.0725, 1.1850],
        [1.2121, 1.2862, 0.6270],
        [0.9586, 0.6927, 0.9081],
        [0.8100, 0.7620, 1.6299]])
tensor([0.3950, 1.0725, 1.2862, 0.6927, 0.7620])
"""

x = torch.randn(4, 4)
y = x.view(16) # instead of reshape
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())
"""
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
"""

# permute!
x = torch.randn(3,220, 224)
y = x.permute(1,2,0) # 1th = 220, 2nd = 224, 0th = 3 
print(x.size(), y.size())
```

Conversion to list

```python
x = torch.randn(1)
print(x)
print(x.tolist()) # to a list
print(x.item()) # gets the number, works for tensors with single value

x = torch.randn(2, 2)
print(x)
print(x.tolist()) #works for tensors with multiple values
# print(x.item()) # does not work with multiple dimension
```

#### PyTorch and Numpy

PyTorch integrates very nice features with `numpy`. ==The Torch Tensor and NumPy array will share their underlying memory locations==, and changing one will change the other.

```python
a = torch.ones(5) # 1-d arrow
b = a.numpy() # converts to numpy

a.add_(1) # has to be inplace
print(a)
print(b)
""" both are changed
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
"""
```

this works because they will be pointing to the **same memory location** (probably share a field).

The reverse ==also works==

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a) # in-place
# below does not work because it returns a NEW ARRAY
# a = np.add(a,1) 
# a = a + 1
print(a)
print(b)
"""
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
"""
```

#### CUDA Tensors

This basically will make your code runs faster

```python
# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
# Devices available are 'cuda' and 'cpu'

x = torch.rand(5,3)
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to("cuda")``
    z = x + y
    print(z)
```

### Part 2

Too tired, look at the notebook.

Basically you have `pytorch` **automatically evaluated gradient** by:

```python
x = torch.ones(2, 2, requires_grad=True) # gradient evaluated AT this value
# x.add_(1) # once requires_grad, you cannot do in-place operation

y = x + 2 # only x will have `require_grad=True`
# y.retain_grad() # if you need the gradient of y as well
z = 3 * y * y
out = z.mean()


print(x)
print(y)
print(z)
print(out)
"""
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>)
tensor(27., grad_fn=<MeanBackward0>)
"""
```

Then, to evaluate gradient $\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1}$:

```python
#Let's backprop now
print(x.grad) # not yet computed
out.backward() # computed
print(x.grad)
"""
None

tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
"""
```

We have that $o = \frac{1}{4}\sum_i z_i= \frac{1}{4}\sum_i 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$. Therefore:
$$
\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)
$$
so evaluated at $x_i=1$:
$$
\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5
$$
which is what we had.

> **Note**
>
> Under the hood, the `grad` is done by keeping track of a computation graph behind all the operations.

Then, to temporarily disable the gradient:

```python
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
	print((x ** 2).requires_grad)
"""
True
True
False
"""
```

useful when the training is done.

### Part 3

Now we create a NN for MNIST dataset.

First import a bunch:

```python
import torch
import time
import matplotlib.pyplot as plt

# Import dataset related API
from torchvision import datasets
from torchvision.transforms import ToTensor

# Import common neural network API in pytorch
import torch.nn as nn
import torch.nn.functional as F

# Import optimizer related API
import torch.optim as optim

# Check device, using gpu 0 if gpu exist else using cpu
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
```

We will be mainly dealing with MNIST:

```python
# Getting data
training_data = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)

# how dset works
figure = plt.figure(figsize=(4, 4))
cols, rows = 2, 2
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(label)
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray") #.squeeze() to convert shape from (1, 28, 28) to (28, 28)
plt.show()
```

note that:

- the shape is `[28,28,1]` because `pytorch `set the first dimension to the **channel dimension.** We are using grey scale so it is $1$.

- squeeze can be quite dangerous if you do not know what you are doing:

  ```python
  >>> x = torch.zeros(2, 1, 2, 1, 2)
  >>> x.size()
  torch.Size([2, 1, 2, 1, 2])
  >>> y = torch.squeeze(x)
  >>> y.size()
  torch.Size([2, 2, 2])
  ```

  i.e. all dimension 1 will be removed if not specified

#### Preparing Data

**Preparing the data**

- into batches and shuffle

```python
# Important Library!!
from torch.utils.data import DataLoader

# Sectioning dataset into batches using dataloaders
train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
```

#### Create NN

```python
# Defining the neural network
class Network(nn.Module):
    def __init__(self):
        super().__init__()

        # define layers
        self.layers = self.get_fc_layers()

    def get_fc_layers(self):

        layers = nn.Sequential(
            nn.Linear(784, 128), # W^1, notice that 784=28*28
            nn.ReLU(),
            nn.Linear(128, 64), # W^2, input 128 dimension, output 64 dimension
            nn.ReLU(),
            nn.Linear(64, 10), # W^3, input 64 dimension, output 10
            nn.LogSoftmax(dim=1)
        )
        return layers


    # define forward function
    def forward(self, input): # in this case, it will be (64, 784) where 64 is the batch_size you had

        x = self.layers(input)
        return x

    # backward pass does not need to be specified because they are done automatically
        

net = Network() # Creating an instance of the Network class
net.to(device) # shifting to cuda if available*

# Below is how the network looks
"""
Network(
  (layers): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=10, bias=True)
    (5): LogSoftmax(dim=1)
  )
)
"""
# Defining our Loss Function
criterion = nn.CrossEntropyLoss()

# Defining optimizer
optimizer = optim.SGD(net.parameters(), lr=0.003, momentum=0.9) # e.g. ADAM
```

#### Training NN

```python
# Number of epochs
epochs = 15
for epoch in range(epochs):

    # Initialising statistics that we will be tracking across epochs
    start_time = time.time()
    total_correct = 0
    total = 0
    total_loss = 0

    for i, data in enumerate(train_dataloader, 0): # in batches
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data
        # loading onto cuda if available*
        if torch.cuda.is_available():
            inputs = inputs.to(device)
            labels = labels.to(device)

        # zero the parameter gradients: Clean the gradient caclulated in the previous iteration
        optimizer.zero_grad() #reset all graidents to zero for each step as they accumulate over backprop

        # forward + backward + optimize
        inputs = inputs.view(inputs.shape[0], -1) # reshapes [btach_size, 28, 28] to [batch_size, 784]

        # 1. forward propagation
        outputs = net.forward(inputs)
        loss = criterion(outputs, labels)

        # 2. backward propagation
        # Calculate gradient of matrix with requires_grad = True
        loss.backward() #computes dloss/dx for every parameter x which has requires_grad=True

        # 3. Apply the gradient calculate from last step to the matrix
        optimizer.step() # x += -lr * x.grad ie updates the weights of the parameters

        # housekeeping
        
        # Adding loss to total loss
        total_loss += loss.item()

        # Checking which output label has max probability
        _, predicted = torch.max(outputs, 1)
        total += labels.shape[0]

        # Tracking number of correct predictions
        total_correct += (predicted == labels).sum().item()

    # Calculating accuracy, epoch-time
    accuracy = 100* total_correct/total
    end_time = time.time() - start_time    

    # Printing out statistics
    print("Epoch no.",epoch+1 ,"|accuracy: ", round(accuracy, 3),"%", "|total_loss: ", total_loss, "| epoch_duration: ", round(end_time,2),"sec")
```

you basically have to specify the entire training steps.

#### Evaluating NN

```python
correct = 0.0
total = 0.0

# Since, we're now evaluating, we don't have to track gradients now!

with torch.no_grad():
  for i, data in enumerate(test_dataloader, 0):

    # everything here is similar to the train function, 
    # except we're not calculating loss and not preforming backprop
    inputs, labels = data
    if torch.cuda.is_available():
      inputs = inputs.to(device)
      labels = labels.to(device)


    inputs = inputs.view(inputs.shape[0], -1)
    # just forward propagatoin for computing Y_pred
    # no more backward updates
    outputs = net.forward(inputs)
    _, predicted = torch.max(outputs, 1)
    total += labels.shape[0]
    correct += (predicted == labels).sum().item()

print(f'Accuracy: {100 * correct/total}%')    
```

notice that we asked `pytorch` not to track gradients any more.

#### Create Own Dset

We can subclass `Dataset`. This gives us more control how the data is to be extracted from source (**before training**)

- We can also perform some additional transformatios before returning the data
- will work natively with `DataLoader`

```python
#CustomDataset is subclass of Dataset.
from torch.utils.data import Dataset
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self): # how many data points are there in your dset
        return len(self.labels)

    def __getitem__(self, idx): # get one (x_train, y_train) sample
        # we can do transformation on the given input before it is returned
        X = self.data[idx]
        y = self.labels[idx]
        return X, y
    
#Usage
train_dataset = CustomDataset(X_train, y_train)
train_loader = DataLoader(
    train_dataset,
    batch_size = 32,
    shuffle = True)
```

## Tensorflow

Implemented in C++ in the back, so highly optimized in performance.

The basic components are **tensors**, which is a multi-dimensional arrays with a ==uniform type==. Basicallya generalized matrix.

- we also talk about ranks in a tensor: a rank is the same idea as "dimension"

  - rank 1 tensor for vector

  - rank 2 for matrix

  - rank 3 for rgb image

- deep learning basically does **computations using tensors** that are very large. 

  - e.g. for NN, we can see the system as a graph and passing tensors across. Hence the name **TensorFlow**

### Basics

In general, you should specify:

- the size of the tensor
- the datatype

```python
tf.zeros([3, 4], dtype = tf.dtypes.float32)

x = tf.random.normal(
    [3,4,3],
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=42
)
print(x.shape)
print(x.dtype)
"""
(3, 4, 3)
<dtype: 'float32'>
"""
```

**Integrated with `numpy`**

- yet there is no "in-place operation" in tensorflow.

```python
ndarray = np.ones([3, 3])

tf_tensor = tf.multiply(ndarray, 42)
tf_tensor
"""
<tf.Tensor: shape=(3, 3), dtype=float64, numpy=
array([[42., 42., 42.],
       [42., 42., 42.],
       [42., 42., 42.]])>
"""

np.add(tf_tensor, 1)
"""
array([[43., 43., 43.],
       [43., 43., 43.],
       [43., 43., 43.]])
"""
```

**GPU Support**

- TensorFlow is able to detect it automatically

```python
if len(tf.config.list_physical_devices('GPU')) > 0:
    print ("GPU available and ready to use!")
else:
    print ("Unable to detect GPU")

        # to run on some specific GPU
if len(tf.config.list_physical_devices('GPU')) > 0:
    with tf.device("GPU"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
        x = tf.random.uniform([1000, 1000])
        # x.device specifies the device for operations on x 
        assert x.device.endswith("GPU:0")
        print ("On GPU")
    else:
        print('GPU unavailable')
```

#### Variables and Constants

Basically they are wrappers around Tensor object, but it is **important because**

- it allows ==in-place updates==
- they are automatically watched by `tf.GradientTape`, which does auto-differentiating
  - hence they are used for parameters such as **weights in NN**

Then, to create variables and constants

- variables are **tensors** that can be changed. They will be watched if you use `GradientTape`
- constants are **tensors** that *cannot be updated*. They will *not be* watched automatically.

```python
# for creating a Variable, you need to provide an initial value
my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
my_variable = tf.Variable(my_tensor) # so a wrapper

# Variables can be all kinds of types, just like tensors
bool_variable = tf.Variable([False, False, False, True])
complex_variable = tf.Variable([5 + 4j, 6 + 1j])
```

**In-place Updates**

```python
a = tf.Variable([2.0, 3.0])
# This will keep the same dtype, float32
a.assign([1, 2])
print("a:", a)

# Not allowed as it resizes the variable:
a.assign([1.0, 2.0, 3.0])
```

**Naming**

- helpful for you to know **what this tensor is for**, .e.g when reloading some model weights

- Variable names are preserved when saving and loading models.

  By default, variables in models will acquire unique variable names automatically, so you don't need to assign them yourself unless you want to.

```python
# Create a and b; they will have the same name but will be backed by
# different tensors.
a = tf.Variable(my_tensor, name="Tutorial")

# A new variable with the same name, but different value
# Note that the scalar add is broadcast
b = tf.Variable(my_tensor + 1, name="Tutorial")

# These are elementwise-unequal, despite having the same name
print(a == b)
```

#### Gradients

Basically the same as PyTorch, so that you need to do something "special" to enable it to **start memorizing the operations**

Consider the function we want to differentiate:

```python
def forward(x, y, z):
  return (x + y) * z
  
forward(x=1, y=2, z=-4) # -12
```

then, suppose we want to compute:
$$
\left. \nabla f  \right|_{1,2,-4}
$$
Then:

```python
# input needs to be tracked, hence using variables
params = [tf.Variable(1.0), tf.Variable(2.0), tf.Variable(-4.0)] 

with tf.GradientTape() as tape: # start tracking, i.e. a blank sheet of paper where each operatoin is recorded
    r = forward(*params)
  
grads = tape.gradient(r, params) # W.R.T. the params

print("Gradients:  drdx: %0.2f, drdy: %0.2f, drdz: %0.2f" % 
      (grads[0].numpy(), grads[1].numpy(), grads[2].numpy()))
"""
Gradients:  drdx: -4.00, drdy: -4.00, drdz: 3.00
"""
```

notice that:

- we used ` tape.gradient(r, params)` to differentiate w.r.t. all the `params` hence achieving the gradient. You can also compute $\partial f / \partial x$ by just passing in `tape.gradient(r, params[0])`
- you also **cannot call it multiple times**

Another example

```python
W = tf.Variable([[-0.5, 0.2],
              [-0.3, 0.8]])            # track the weight      
x = tf.Variable([0.2, 0.4])            # does not need if we do not diff w.r.t x
x = tf.expand_dims(x,1)
forward(W,x).numpy()
```

```python
params = [W, x]
with tf.GradientTape() as tape:
  result = forward(*params)
grads = tape.gradient(result, W) # w.r.t the weight only

print("Gradient with respect to w:  ", grads.numpy())
"""
Gradient with respect to w:   [[0.  0. ]
 [0.2 0.4]]
"""
```

- note that if you have a **constant Tensor** that you want to watch:

  ```python
  x = tf.constant(3.0)
  with tf.GradientTape() as g:
      g.watch(x) # NEEDED!
      y = x * x
  dy_dx = g.gradient(y, x) # Will compute to 6.0
  print(dy_dx)
  ```

### Training

The complete version.

**preparing data**

```python
from tensorflow.keras.layers import Dense, Flatten 
from tensorflow.keras import Model, Input
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split
import timeit


# Download a dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

print(x_train.shape)
print(x_test.shape)
"""
(60000, 28, 28)
(10000, 28, 28)
"""

# normalize the values to [0-1], hsuffle, and generates to batches
# returns an iterator
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train.astype('float32') / 255, y_train)).shuffle(1024).batch(32)

test_ds = tf.data.Dataset.from_tensor_slices(
    (x_test.astype('float32') / 255, y_test)).batch(32)

# A quick example of iterating over a dataset object
for image, label in train_ds.take(1):
    print("Batch Shape: ", image.shape)  #  (32, 28, 28) where 32 is the batch size
    plt.imshow(image[0])
    print(label[0])
```

**Training**

Now, in general there are two ways to establish and train a model

- using `Sequential` - easier to use
- using `subclassing` API - more flexible

Here we discuss the subclassing idea:

```python
class MyCustomModel(Model): # inherits from tf.keras.Model
  def __init__(self):
    super(MyCustomModel, self).__init__()
    # define your network architecture
    self.flatten = Flatten() # Layer 1: flatten
    self.dl_1 = Dense(128, activation='relu') # Layer 2: dense layer
    self.dl_2 = Dense(128, activation='relu') # Layer 3: dense layer
    self.d1_3 = Dense(10, activation='softmax') # Layer 4: multiclass classification

  def call(self, x):
    x = self.flatten(x) # basically the FORWARD PASS
    x = self.dl_1(x)
    x = self.dl_2(x)
    return self.d1_3(x)
```



```python
model = MyCustomModel()

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()  # loss function
# what optimization method we use. i.e. HOW you want to update the weights
optimizer = tf.keras.optimizers.SGD()

# For each epoch
for epoch in range(5):

    # For each batch of images and labels
    for images, labels in train_ds:

        # Open a GradientTape.
        with tf.GradientTape() as tape:

            # Forward pass
            predictions = model(images)

            # Calculate loss, done INSIDE the GradientTape since we want to do derivatives afterwards
            loss = loss_fn(labels, predictions)

        # Backprop to calculate gradients
        gradients = tape.gradient(loss, model.trainable_variables)

        # Gradient descent step, apply the `gradients` to `model.trainable_variables`
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    # Calculate loss on the test data
    test_loss = []
    for images, labels in test_ds:
        predictions = model(images)
        loss_on_batch = loss_fn(labels, predictions)
        test_loss.append(loss_on_batch)

    print("Epoch {}, Test loss: {}".format(epoch+1, tf.reduce_mean(test_loss)))
    
"""
Epoch 1, Test loss: 0.320291668176651
Epoch 2, Test loss: 0.26357364654541016
Epoch 3, Test loss: 0.2302776277065277
Epoch 4, Test loss: 0.20258881151676178
Epoch 5, Test loss: 0.17977921664714813
"""
```

### Note

TensorFlow 2.x

- uses **eager execution by default**: executes line by line, the same fashion as you wrote the code

TensorFlow 1.x

- builds a graph from your code, and execute according to the graph

- Graph execution has the obvious advantage that, since you can track the dependency of operations, you can **optimize the run by parallelization**
  - this means Graph based execution can be faster when you do model training
  - however, since the order of computation maybe different from your code, it would be ==hard to debug==

```python
# Model building
inputs = Input(shape=(28, 28)) 
x = Flatten()(inputs) 
x = Dense(256, "relu")(x)
x = Dense(256, "relu")(x) 
x = Dense(256, "relu")(x) 
outputs = Dense(10, "softmax")(x) 

input_data = tf.random.uniform([100, 28, 28])

# Eager Execution, by default
eager_model = Model(inputs=inputs, outputs=outputs)
print("Eager time:", timeit.timeit(lambda: eager_model(input_data), number=10000))

#Graph Execution 
graph_model = tf.function(eager_model) # Wrap the model with tf.function 
print("Graph time:", timeit.timeit(lambda: graph_model(input_data), number=10000))
"""
Eager time: 21.913783847000104
Graph time: 6.057469765000064
"""
```

## Keras

Keras has faster performance

- backed by TensorFlow, so `Keras `is a **high level API**
- easier to use than raw TensorFlow

Since `keras` is like a subclass of `tensorflow`, there is no "basics" per se.

### Sequential Model

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import Flatten, Dense # Dense layer is a normal NN layer
import numpy as np
import datetime
import matplotlib.pyplot as plt
```

note that we imported `keras` from `tensorflow`

We will use `Sequential`:

- architecture is basically like a list
- output of previous will be input of next

A Sequential model is **not** appropriate when:

- Your model has multiple inputs or multiple outputs
- Any of your layers has multiple inputs or multiple outputs
- You need to do layer sharing
- You want non-linear topology (e.g. a residual connection, a multi-branch model)

For dealing with these cases, Keras offers a Functional API as well. This handles non-linear topologies, shared layers and even multiple inputs or outputs.

- Read more here: https://keras.io/guides/functional_api/ 

```python
from keras.models import Sequential
keras.backend.clear_session()
```

> More information on `Keras `layers: https://keras.io/api/layers/ 
>
> Look into `Keras `models: https://keras.io/api/models/

Now let us **prepare the data**

```python
print(x_train.shape)
print(x_test.shape)
"""
(60000, 28, 28)
(10000, 28, 28)
"""
x_train = x_train.astype('float32') / 255. # "normalizing it"
x_test = x_test.astype('float32') / 255.
```

The four ==most important steps you will use==

```python
> model = Sequential() # define model

> model.add(Dense(2, input_shape=(2,))) # define architecture

> model.compile(...) # define optimizer

> model.fit(...) # fit
```

Let us build a 3 layer model:

```python
model= keras.Sequential()
model.add(Flatten())
model.add(Dense(128, activation='relu', input_shape= x_train.shape[1:])) # because input will be 28*28=724
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# add how we want to optimize the weights
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

# train
history = model.fit(x_train,
                    y_train,
                    batch_size=64, # each weigth update occurs per 64 data
                    epochs=5,
                    validation_data=(x_test, y_test))
```

where:

- Keras optimizers: https://keras.io/api/optimizers/
- Keras Losses: https://keras.io/api/losses/

the `model` and `history` variables are **important**.

- save/load/visualize the model

  ```python
  model.save('mnist_recognition.model') # save
   
  loaded_model= keras.models.load_model('mnist_recognition.model') # load
  
  predictions = loaded_model.predict(x_test) # predict
  ```

  visualize

  ```python
  keras.utils.plot_model(model, "mnist_model.png", show_shapes=True)
  ```

  

  <img src="DL_notes/image-20220127211241339.png" alt="image-20220127211241339" style="zoom:67%;" />

- plotting model performance overtime

  ```python
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  
  # Plot training & validation loss values
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  ```

  |                             Acc                              |                             Loss                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220127211406254](DL_notes/image-20220127211406254.png) | ![image-20220127211415578](DL_notes/image-20220127211415578.png) |

  where it is clear that test performance has stopped improving pretty much after the 1st epoch

### Callbacks

Reading: https://keras.io/api/callbacks/

A callback is an object that can **perform actions at various stages of training** (e.g. at the start or end of an epoch, before or after a single batch, etc).

You can use callbacks to:

- Write TensorBoard logs after every batch of training to monitor your metrics
- Periodically save your model to disk
- Do early stopping
- Get a view on internal states and statistics of a model during training
- ...and more

Common usages include:

```python
my_callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=2),
    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
]
model.fit(dataset, epochs=10, callbacks=my_callbacks)
```

#### TensorBoard

TensorBoard is a visualization tool provided with TensorFlow. This callback logs events for TensorBoard, including:

- Metrics summary plots

- Training graph visualization

> **Note**
>
> - The exclamation mark `!` is used for executing commands from the uderlying operating system; here is an example using WIndows `dir`:
>
>   ```py
>   !dir
>   # result:
>   Volume in drive C has no label.
>    Volume Serial Number is 52EA-B90C
>   
>    Directory of C:\Users\Root
>   
>   27/11/2018  13:08    <DIR>          .
>   27/11/2018  13:08    <DIR>          ..
>   23/08/2016  11:00             2,258 .adalcache
>   12/09/2016  18:06    <DIR>          .anaconda
>   ```
>
>   you can even do crazy things like
>
>   ```python
>   In [4]: contents = !ls
>   
>   In [5]: print(contents)
>   ['myproject.txt']
>   
>   In [6]: directory = !pwd
>   
>   In [7]: print(directory)
>   ['/Users/jakevdp/notebooks/tmp/myproject']
>   ```
>
> - the `%` mark is for **magic functions**
>
>   - basically a way to provide richer output than simple CLI, but also allows for customization.
>
>     ```python
>     %load_ext tensorboard # allows for magic functions in tensorboard
>     
>     # do something
>     
>     %tensorboard --logdir logs/fit
>     ```
>
>   - Besides `%cd`, other available shell-like magic functions are `%cat`, `%cp`, `%env`, `%ls`, `%man`, `%mkdir`, `%more`, `%mv`, `%pwd`, `%rm`, and `%rmdir`, any of which can be used without the `%` sign if `automagic` is on. This makes it so that you can almost treat the IPython prompt as if it's a normal shell:
>
>     ```python
>     In [16]: mkdir tmp
>                                                                                                                                                                                                                                                                                                                                                                                                                 
>     In [17]: ls
>     myproject.txt  tmp/
>                                                                                                                                                                                                                                                                                                                                                                                                                 
>     In [18]: cp myproject.txt tmp/
>                                                                                                                                                                                                                                                                                                                                                                                                                 
>     In [19]: ls tmp
>     myproject.txt
>                                                                                                                                                                                                                                                                                                                                                                                                                 
>     In [20]: rm -r tmp
>     ```

```python
%load_ext tensorboard
# Clear any logs from previous runs
!rm -rf ./logs/ 
```

Then we configure:

```python
log_dir="logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

model.fit(x=x_train, 
          y=y_train,
          batch_size=64,
          epochs=5, 
          validation_data=(x_test, y_test), 
          callbacks=[tensorboard_callback]) # specify callback
```

Finally:

```python
%tensorboard --logdir logs/fit
```

![image-20220127211824150](DL_notes/image-20220127211824150.png)

note that:

- if you are using VSCode, you need to switch your cell output format to `text/html` so this thing renders correctly

#### Early Stopping

```python
earlystopping_callback = keras.callbacks.EarlyStopping(
    monitor='val_loss', # which field in `history` you are looking at
    patience=2, # if stays the same for 2 epochs, stop
    verbose=0, mode='auto', baseline=None, restore_best_weights=True)
```

Then using this callback

```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

history=model.fit(x=x_train, 
            y=y_train,
            batch_size=64,
            epochs=5, 
            validation_data=(x_test, y_test), 
            callbacks=[tensorboard_callback, earlystopping_callback])
```

### AutoKeras

An even **higher level than `keras`**

- encapsulates even some basic preprocessing

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

import autokeras as ak
```

then the data:

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(x_train.shape)  
print(y_train.shape) 
```

finally, the model

```python
# Initialize the image classifier.
# didn't mention what architecture or optimizer to use!
clf = ak.ImageClassifier(overwrite=True, max_trials=1) # max #models to try

# Feed the image classifier with training data.
clf.fit(x_train, y_train, epochs=1)


# Predict with the best model.
predicted_y = clf.predict(x_test)
print(predicted_y)


# Evaluate the best model with testing data.
print(clf.evaluate(x_test, y_test))
```

**Validating**

- By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use `validation_split `to specify the percentage.

```python
clf.fit(
    x_train,
    y_train,
    # Split the training data and use the last 15% as validation data.
    validation_split=0.15,
    epochs=10,
)

# Predict with the best model.
predicted_y = clf.predict(x_test)
print(predicted_y)


# Evaluate the best model with testing data.
print(clf.evaluate(x_test, y_test))
```

## CNN with TF

For dense neural network it actually works on MNIST

- because the features that are encoded in the images are **easy representations**

Then if we check this on CIFAR10

- simple NN doesn't work anymore as the feature "extraction from previous layers" is not very representative

Remember to use transfer learning for using those models

- i.e. train on the last layer
- e.g. use VGG16 on tumor classification

## GNN with`dgl.ai`

`dgl.ai` (deep graph library), a framework that allows us to implement, experiment, and run graph machine learning techniques. The tutorial will explore how nodes and edges are represented as well as features of edges and nodes

## Pyro

A framework with pytorch as backend to do probablistic programming.

The idea is that you can do **probability computations** such as $P(x|z)$ by sampling $z$ from some prior distirbution of your choice.

## GAN

Basically read the following link https://www.tensorflow.org/tutorials/generative/dcgan, and the real exapmles below

```python
def discriminator(input_shape=(28,28,1)):
  '''INSERT YOUR CODE FOR THE MODEL DEFINITION BASED ON THE DESCRIPTION GIVEN ABOVE'''
  model = Sequential()
  model.add(InputLayer(input_shape=input_shape))
  model.add(Conv2D(64, 3, strides=(2,2)))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Dropout(0.4))
  model.add(Conv2D(64, 3, strides=(2,2)))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Dropout(0.4))
  model.add(Flatten())
  model.add(Dense(1, activation="sigmoid"))

  #Model compilation
  opt = Adam(lr=0.0002, beta_1=0.5)
  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
  return model

def generator(latent_dim):
	model = Sequential()
	n_nodes = 128 * 7 * 7
	model.add(Dense(n_nodes, input_dim=latent_dim))
	model.add(LeakyReLU(alpha=0.2))
	model.add(Reshape((7, 7, 128)))
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
	model.add(LeakyReLU(alpha=0.2))
	model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))
	return model

def generate_latent_points(latent_dim, n_samples):
	X = rand(latent_dim * n_samples)
	X = X.reshape((n_samples, latent_dim))
	return X

# use the generator to generate n fake examples with class labels
def generate_fake_samples(g_model, latent_dim, n_samples):
	x_input = generate_latent_points(latent_dim, n_samples)
	#Predicting outputs
	X = g_model.predict(x_input)
	y = zeros((n_samples, 1))
	return X, y

def generate_real_samples(dataset, n_samples):
  #Choosing random instances
	ix = randint(0, dataset.shape[0], n_samples)
	#Retrieving Images
	X = dataset[ix]
	#Generating class labels
	y = ones((n_samples, 1))
	return X, y

def define_gan(g_model, d_model):
	d_model.trainable = False
	model = Sequential()
	model.add(g_model)
	model.add(d_model)
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt)
	return model
```

Then training

```python
def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):
	bat_per_epo = int(dataset.shape[0] / n_batch)
	half_batch = int(n_batch / 2)
	num_steps = n_epochs * bat_per_epo
	pbar = tqdm(range(num_steps))
	for i in range(n_epochs):
		for j in range(bat_per_epo):
			X_real, y_real = generate_real_samples(dataset, half_batch)
			X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
			X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))
			d_loss, _ = d_model.train_on_batch(X, y) # first train discriminator
			X_gan = generate_latent_points(latent_dim, n_batch)
			y_gan = ones((n_batch, 1))
			g_loss = gan_model.train_on_batch(X_gan, y_gan) # fix discriminator and train generator
			
			# print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))
			pbar.update(1)
			pbar.set_description(f"[Epoch {i+1}/{n_epochs}] Step {j+1}/{bat_per_epo}: d_loss={d_loss:.3f} g_loss={g_loss:.3f}")

latent_dim = 100
d_model = discriminator()
g_model = generator(latent_dim)
gan_model = define_gan(g_model, d_model)
dataset = load_real_samples()
train(g_model, d_model, gan_model, dataset, latent_dim)
```

# Competitions

> **Object Detection in Haze Link**
>
> - http://cvpr2022.ug2challenge.org/track1.html

basically we need to do:

1. dehazing an image
2. object detection in those images
   - predict the bounding box for a vehicles

Some tipcs:

- data augmentation likely needed

---

Ideas for **dehazing**

- **Pix2Pix:** Image-to-Image Translation
  - Given a training set of unfiltered and filtered image pairs $A : A'$ and a new unfiltered image $B$ the output is a filtered image $B'$ such that the analogy $A:A'::B : B'$ is **maintained**
  - An input image is mapped to a synthesized image with different properties. The loss function is a combination of the conditional GAN loss with an additional loss term which is a pixel-wise loss that encourages the generator to match the source image
- https://arxiv.org/pdf/1912.07015.pdf
- https://link.springer.com/article/10.1007/s11042-021-11442-6

---

Ideas for **Object BB Drawing**

- ViT based

## DETR

Object detection (drawing bounding boxes).

Useful resources:

- https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook
- https://github.com/jasonyux/cvpr_2022 (checkout `models` folder)

**code**

```python
import os
import pathlib
import sys
root = pathlib.Path(__file__).parent.parent.resolve()
sys.path.insert(0, str(root))

import pytorch_lightning as pl
import torch
import json

from transformers import DetrFeatureExtractor
from torch.utils.data import DataLoader
from transformers import DetrConfig, DetrForObjectDetection
from tqdm.auto import tqdm
from utils.eval_util import *
from utils.utils import save_model_stats
from transformers.models.detr.modeling_detr import ACT2FN
from typing import Optional, Tuple


device = torch.device("cuda")

PRETRAIN = "facebook/detr-resnet-50"

class DetrAttention(torch.nn.Module):
	"""
	Multi-headed attention from 'Attention Is All You Need' paper.

	Here, we add position embeddings to the queries and keys (as explained in the DETR paper).
	"""
	def __init__(
		self,
		embed_dim: int,
		num_heads: int,
		dropout: float = 0.0,
		is_decoder: bool = False,
		bias: bool = True,
	):
		super().__init__()
		self.embed_dim = embed_dim
		self.num_heads = num_heads
		self.dropout = dropout
		self.head_dim = embed_dim // num_heads
		assert (
			self.head_dim * num_heads == self.embed_dim
		), f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads})."
		self.scaling = self.head_dim ** -0.5

		self.k_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)
		self.v_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)
		self.q_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)
		self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=bias)

	def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
		return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

	def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[torch.Tensor]):
		return tensor if position_embeddings is None else tensor + position_embeddings

	def forward(
		self,
		hidden_states: torch.Tensor,
		attention_mask: Optional[torch.Tensor] = None,
		position_embeddings: Optional[torch.Tensor] = None,
		key_value_states: Optional[torch.Tensor] = None,
		key_value_position_embeddings: Optional[torch.Tensor] = None,
		output_attentions: bool = False,
	) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
		"""Input shape: Batch x Time x Channel"""

		# if key_value_states are provided this layer is used as a cross-attention layer
		# for the decoder
		is_cross_attention = key_value_states is not None
		bsz, tgt_len, embed_dim = hidden_states.size()

		# add position embeddings to the hidden states before projecting to queries and keys
		if position_embeddings is not None:
			hidden_states_original = hidden_states
			hidden_states = self.with_pos_embed(hidden_states, position_embeddings)

		# add key-value position embeddings to the key value states
		if key_value_position_embeddings is not None:
			key_value_states_original = key_value_states
			key_value_states = self.with_pos_embed(key_value_states, key_value_position_embeddings)

		# get query proj
		query_states = self.q_proj(hidden_states) * self.scaling
		# get key, value proj
		if is_cross_attention:
			# cross_attentions
			key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
			value_states = self._shape(self.v_proj(key_value_states_original), -1, bsz)
		else:
			# self_attention
			key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
			value_states = self._shape(self.v_proj(hidden_states_original), -1, bsz)

		proj_shape = (bsz * self.num_heads, -1, self.head_dim)
		query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
		key_states = key_states.view(*proj_shape)
		value_states = value_states.view(*proj_shape)

		src_len = key_states.size(1)

		attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))

		if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
			raise ValueError(
				f"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}"
			)

		if attention_mask is not None:
			if attention_mask.size() != (bsz, 1, tgt_len, src_len):
				raise ValueError(
					f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
				)
			attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
			attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

		attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)

		if output_attentions:
			# this operation is a bit awkward, but it's required to
			# make sure that attn_weights keeps its gradient.
			# In order to do so, attn_weights have to reshaped
			# twice and have to be reused in the following
			attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
			attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
		else:
			attn_weights_reshaped = None

		attn_probs = torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)

		attn_output = torch.bmm(attn_probs, value_states)

		if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
			raise ValueError(
				f"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}"
			)

		attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
		attn_output = attn_output.transpose(1, 2)
		attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)

		attn_output = self.out_proj(attn_output)

		return attn_output, attn_weights_reshaped

class DetrEncoderLayer(torch.nn.Module):
	def __init__(self, config: DetrConfig):
		super().__init__()
		self.embed_dim = config.d_model
		self.self_attn = DetrAttention(
			embed_dim=self.embed_dim,
			num_heads=config.encoder_attention_heads,
			dropout=config.attention_dropout,
		)
		self.self_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim)
		self.dropout = config.dropout
		self.activation_fn = ACT2FN[config.activation_function]
		self.activation_dropout = config.activation_dropout
		self.fc1 = torch.nn.Linear(self.embed_dim, config.encoder_ffn_dim)
		self.fc2 = torch.nn.Linear(config.encoder_ffn_dim, self.embed_dim)
		self.final_layer_norm = torch.nn.LayerNorm(self.embed_dim)

	def forward(
		self,
		hidden_states: torch.Tensor,
		attention_mask: torch.Tensor,
		position_embeddings: torch.Tensor = None,
		output_attentions: bool = False,
	):
		"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*): position embeddings, to be added to hidden_states.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""
		residual = hidden_states
		hidden_states, attn_weights = self.self_attn(
			hidden_states=hidden_states,
			attention_mask=attention_mask,
			position_embeddings=position_embeddings,
			output_attentions=output_attentions,
		)

		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
		hidden_states = residual + hidden_states
		hidden_states = self.self_attn_layer_norm(hidden_states)

		residual = hidden_states
		hidden_states = self.activation_fn(self.fc1(hidden_states))
		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)

		hidden_states = self.fc2(hidden_states)
		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

		hidden_states = residual + hidden_states
		hidden_states = self.final_layer_norm(hidden_states)

		if self.training:
			if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():
				clamp_value = torch.finfo(hidden_states.dtype).max - 1000
				hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

		outputs = (hidden_states,)

		if output_attentions:
			outputs += (attn_weights,)

		return outputs


class DetrDecoderLayer(torch.nn.Module):
	def __init__(self, config: DetrConfig):
		super().__init__()
		self.embed_dim = config.d_model

		self.self_attn = DetrAttention(
			embed_dim=self.embed_dim,
			num_heads=config.decoder_attention_heads,
			dropout=config.attention_dropout,
			is_decoder=True,
		)
		self.dropout = config.dropout
		self.activation_fn = ACT2FN[config.activation_function]
		self.activation_dropout = config.activation_dropout

		self.self_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim)
		self.encoder_attn = DetrAttention(
			self.embed_dim,
			config.decoder_attention_heads,
			dropout=config.attention_dropout,
			is_decoder=True,
		)
		self.encoder_attn_layer_norm = torch.nn.LayerNorm(self.embed_dim)
		self.fc1 = torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
		self.fc2 = torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
		self.final_layer_norm = torch.nn.LayerNorm(self.embed_dim)

	def forward(
		self,
		hidden_states: torch.Tensor,
		attention_mask: Optional[torch.Tensor] = None,
		position_embeddings: Optional[torch.Tensor] = None,
		query_position_embeddings: Optional[torch.Tensor] = None,
		encoder_hidden_states: Optional[torch.Tensor] = None,
		encoder_attention_mask: Optional[torch.Tensor] = None,
		output_attentions: Optional[bool] = False,
	):
		"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the cross-attention layer.
			query_position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the self-attention layer.
			encoder_hidden_states (`torch.FloatTensor`):
				cross attention input to the layer of shape `(seq_len, batch, embed_dim)`
			encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""
		residual = hidden_states

		# Self Attention
		hidden_states, self_attn_weights = self.self_attn(
			hidden_states=hidden_states,
			position_embeddings=query_position_embeddings,
			attention_mask=attention_mask,
			output_attentions=output_attentions,
		)

		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
		hidden_states = residual + hidden_states
		hidden_states = self.self_attn_layer_norm(hidden_states)

		# Cross-Attention Block
		cross_attn_weights = None
		if encoder_hidden_states is not None:
			residual = hidden_states

			hidden_states, cross_attn_weights = self.encoder_attn(
				hidden_states=hidden_states,
				position_embeddings=query_position_embeddings,
				key_value_states=encoder_hidden_states,
				attention_mask=encoder_attention_mask,
				key_value_position_embeddings=position_embeddings,
				output_attentions=output_attentions,
			)

			hidden_states = torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
			hidden_states = residual + hidden_states
			hidden_states = self.encoder_attn_layer_norm(hidden_states)

		# Fully Connected
		residual = hidden_states
		hidden_states = self.activation_fn(self.fc1(hidden_states))
		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
		hidden_states = self.fc2(hidden_states)
		hidden_states = torch.nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
		hidden_states = residual + hidden_states
		hidden_states = self.final_layer_norm(hidden_states)

		outputs = (hidden_states,)

		if output_attentions:
			outputs += (self_attn_weights, cross_attn_weights)

		return outputs

class FullDetr(pl.LightningModule):
	def __init__(self, lr, lr_backbone, weight_decay):
		super().__init__()
		# replace COCO classification head with custom head
		pretrained_model = DetrForObjectDetection.from_pretrained(
			PRETRAIN, num_labels=1, ignore_mismatched_sizes=True,
			attention_dropout=0.1, dilation=True
		)

		# some customizations
		pretrained_model.model.query_position_embeddings = torch.nn.Embedding(10, 256)
		config = DetrConfig(
			dropout=0.1,
			activation_dropout=0.1
		)
		for i in range(len(pretrained_model.model.encoder.layers)): # there are 6 encoders
			pretrained_stat = pretrained_model.model.encoder.layers[i].state_dict()
			new_layer = DetrEncoderLayer(config)
			new_layer.load_state_dict(pretrained_stat)
			pretrained_model.model.encoder.layers[i] = new_layer
		for i in range(len(pretrained_model.model.decoder.layers)):
			pretrained_stat = pretrained_model.model.decoder.layers[i].state_dict()
			new_layer = DetrDecoderLayer(config)
			new_layer.load_state_dict(pretrained_stat)
			pretrained_model.model.decoder.layers[i] = new_layer
		
		self.model = pretrained_model

		self.feature_extractor = DetrFeatureExtractor.from_pretrained(PRETRAIN)
		self.model.to(device)

		# see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896
		self.lr = lr
		self.lr_backbone = lr_backbone
		self.weight_decay = weight_decay

	def forward(self, batch):
		img, target = batch['imgs'], batch['labels']
		encoding = self.feature_extractor(images=img, annotations=target, return_tensors="pt")
		pixel_values = encoding['pixel_values'].squeeze()
		labels = encoding['labels']
		labels = [{k: v.to(device) for k, v in t.items()} for t in labels]

		encoding = self.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt")
		enc_pixel_values = encoding['pixel_values'].to(device)
		pixel_mask = encoding['pixel_mask'].to(device)
		outputs = self.model(pixel_values=enc_pixel_values, pixel_mask=pixel_mask)
		return outputs, labels

	def common_step(self, batch, batch_idx):
		img, target = batch['imgs'], batch['labels']
		encoding = self.feature_extractor(images=img, annotations=target, return_tensors="pt")
		pixel_values = encoding['pixel_values'].squeeze()
		labels = encoding['labels']

		encoding = self.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt")
		enc_pixel_values = encoding['pixel_values'].to(device)
		pixel_mask = encoding['pixel_mask'].to(device)

		labels = [{k: v.to(device) for k, v in t.items()} for t in labels]
		
		outputs = self.model(pixel_values=enc_pixel_values,
							 pixel_mask=pixel_mask, labels=labels)
		
		loss = outputs.loss
		loss_dict = outputs.loss_dict

		return loss, loss_dict

	def training_step(self, batch, batch_idx=None):
		loss, loss_dict = self.common_step(batch, batch_idx)
		# logs metrics for each training_step,
		# and the average across the epoch
		self.log("training_loss", loss)
		for k, v in loss_dict.items():
			self.log("train_" + k, v.item())

		return loss

	def validation_step(self, batch, batch_idx):
		loss, loss_dict = self.common_step(batch, batch_idx)
		self.log("validation_loss", loss)
		for k, v in loss_dict.items():
			self.log("validation_" + k, v.item())

		return loss

	def configure_optimizers(self):
		param_dicts = [
			{"params": [p for n, p in self.named_parameters(
			) if "backbone" not in n and p.requires_grad]},
			{
				"params": [p for n, p in self.named_parameters() if "backbone" in n and p.requires_grad],
				"lr": self.lr_backbone,
			},
		]
		optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,
									  weight_decay=self.weight_decay)

		return optimizer

def collate_fn(batch):
	imgs = [item[0] for item in batch]
	labels = [item[1] for item in batch]
	batch = {}
	batch['imgs'] = imgs
	batch['labels'] = labels
	return batch


class FullCocoDetection(torchvision.datasets.CocoDetection):
	def __init__(self, img_folder):
		ann_file = None
		for file in os.listdir(img_folder):
			if file.endswith(".json"):
				ann_file = os.path.join(img_folder, file)
		super(FullCocoDetection, self).__init__(img_folder, ann_file)
		self.ann_file = ann_file
		self.ids_to_fn = None

	def __getitem__(self, idx):
		# read in PIL image and target in COCO format
		img, target = super(FullCocoDetection, self).__getitem__(idx)
		
		image_id = self.ids[idx]
		target = {'image_id': image_id, 'annotations': target}
		return img, target

	def id_to_filename(self, id):
		if self.ids_to_fn is None:
			self.ids_to_fn = {}
			data = json.load(open(self.ann_file, "r"))
			for x in data['images']:
				self.ids_to_fn[x['id']] = x['file_name']
		return  self.ids_to_fn[id]


if __name__ == "__main__":
	root = "datasets"
	model_name = "haze_full_detr_50"

	info = {
		"train_dset": str(os.path.join(root, 'full_train_haze_dset')),
		"test_dset": str(os.path.join(root, 'dry_test_dset')),
		"train_batch_size": 2,
		"test_batch_size": 2,
		"model_lr": 3e-5,
		"model_lr_backbone": 1e-5,
		"model_weight_decay": 1e-2,
		"num_epochs": 50
	}
	print(f"""
	Training with 
		{json.dumps(info, indent=2)}
	""")

	##### prepare data
	train_images = info['train_dset']
	test_images = info['test_dset']

	train_dataset = FullCocoDetection(img_folder=train_images)
	test_dataset = FullCocoDetection(img_folder=test_images)

	print("Number of training examples:", len(train_dataset))
	print("Number of validation examples:", len(test_dataset))

	train_dataloader = DataLoader(
		train_dataset, 
		collate_fn=collate_fn, 
		batch_size=info["train_batch_size"], 
		shuffle=True
	)
	test_dataloader = DataLoader(
		test_dataset, 
		collate_fn=collate_fn, 
		batch_size=info["test_batch_size"]
	)

	##### model
	model = FullDetr(
		lr=info["model_lr"], 
		lr_backbone=info["model_lr_backbone"], 
		weight_decay=info["model_weight_decay"]
	)
	model.to(device)

	##### train
	num_epochs = info["num_epochs"]
	num_training_steps = num_epochs * len(train_dataloader)

	progress_bar = tqdm(range(num_training_steps))

	optimizer = model.configure_optimizers()

	history = {
		"train_map":[],
		"val_map":[]
	}
	train_base_ds = get_coco_api_from_dataset(train_dataset)
	test_base_ds = get_coco_api_from_dataset(test_dataset)

	# accelerator = Accelerator()

	best_vmap = 0.
	for epoch in range(num_epochs):
		model.train()
		for batch in train_dataloader:
			loss = model.training_step(batch)
			loss.backward()
			# accelerator.backward(loss)

			optimizer.step()
			optimizer.zero_grad()
			progress_bar.update(1)
		
		model.eval()
		feature_extractor = model.feature_extractor
		tmap = calculate_full_map(feature_extractor, model, train_base_ds, train_dataloader)[0][1]
		vmap = calculate_full_map(feature_extractor, model, test_base_ds, test_dataloader)[0][1]
		history["train_map"].append(tmap)
		history["val_map"].append(vmap)
		print(f"train map={tmap}, val map={vmap}")

		if vmap > best_vmap:
			torch.save(model, f"models/{model_name}")
			best_vmap = vmap

	info['model_saved_checkpoint'] = best_vmap
	save_model_stats(model_name, history, info)
```

## Data Augmentations

Using `albumentations`

```python
import albumentations as A
import cv2
import numpy as np

transform = A.Compose([
		A.HorizontalFlip(p=0.5),
		A.VerticalFlip(p=0.5),
		A.Affine(scale=1.0, rotate=(-180, 180), p=0.9)
	], 
	bbox_params=A.BboxParams(format='coco', min_visibility=0.1)
)


sample_data = train_dataset[0] # numpy array of an image
bboxes = [] # bounding boxes
for label in sample_data[1]['annotations']:
	bboxes.append(label['bbox']+['car'])

transformed = transform(image=np.array(sample_data[0]), bboxes=bboxes)
transformed_image = transformed['image']
transformed_bboxes = transformed['bboxes']
```]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Logistics and Introduction]]></summary></entry><entry><title type="html">COMS6998 Dialog Systems</title><link href="/lectures/2022@columbia/COMS6998_Dialog_Systems.html/" rel="alternate" type="text/html" title="COMS6998 Dialog Systems" /><published>2022-10-19T00:00:00-04:00</published><updated>2022-10-19T00:00:00-04:00</updated><id>/lectures/2022@columbia/COMS6998_Dialog_Systems</id><content type="html" xml:base="/lectures/2022@columbia/COMS6998_Dialog_Systems.html/"><![CDATA[6998: Conversational AI

# Logistics and Related Topics

- paper sign up: https://docs.google.com/spreadsheets/d/1qUP7ngFG996foQN017L0gHDrorpZcPYAOcwgFFmeV_k/edit#gid=0
- instead of Piazza, we are using Slack (for a smaller group but more interactions)

Grading

| Grading                                                      |      |
| :----------------------------------------------------------- | ---- |
| Paper Presentation (35min, record and place ppt link in the google shee, this time also includes discussion) | 15%  |
| Reading Summary (weekly, 2 papers)                           | 15%  |
| Proposal                                                     | 10%  |
| Mid-Term Project Report                                      | 15%  |
| Final Report                                                 | 30%  |
| Class Attendance (ask questions in class)                    | 15%  |

The main conference for NLP would be ACL, and dialog has been a very popular field:

<img src="dialogs/image-20220907163617003.png" alt="image-20220907163617003" style="zoom:50%;" />

Task-oriented conversational agents can help complete task that are **more efficient, standardized, and cheaper** way.

## Dialog System Basics

Usually have two different goals and hence two metrics

<img src="dialogs/image-20220907164320144.png" alt="image-20220907164320144" style="zoom:33%;" />

- task-oriented chatbot: relative simple to evaluate
  - e.g. flight booking
- social chatbot: engage the user to stay in the conversation.

Usually, we consider dialog framework (==abstractions==) for task oriented chatbots:

|                            Module                            | Task                                                         | Example I/O                                                  | Solution/Architecture                                        |
| :----------------------------------------------------------: | ------------------------------------------------------------ | ------------------------------------------------------------ | :----------------------------------------------------------- |
| ![image-20220907164901763](dialogs/image-20220907164901763.png) | What is the user intent? For instance                        | **Problem**: user utterance $\to$ a distributed semantic representation<br/>**Sub-Tasks**: intent detection, slot filling<br/>e.g. I need something that is in east part of the town $\to$ `Inform: location=east` | classification+sequence labeling (BIO tags) or sequence generation (e.g. directly use T5 to generate the desired output) |
| ![image-20220907165352093](dialogs/image-20220907165352093.png) | Important information you need to remember over time to complete the task (i.e. **what is happening and what the user still needs**) (e.g. tracking NLU output over time).<br />This is actually very important because if you get this right, then you just need to do some API calls and done. | Input:<br />User: I am looking for a moderate price ranged Italian restaurant.<br/>Sys: De luca cucina and bar is a modern European restaurant in the center.<br/>User: I need something thats in the east part of town<br/>**Output**: `inform: price=moderate, location=east` | Sequence classification (e.g. given `price`, is it `low`, `moderate`, or `high`) or sequence generation |
| ![image-20220907165404636](dialogs/image-20220907165404636.png) | Dialog Policy Planning: plan what the system *should* say.<br />(e.g. use of offline RL) | **Problem**: dialog state $\to$ system action mean representation(intent + slot)/template<br/><br/>e.g. `inform: price=moderate, location=east` $\to$ `provide: restaurant_name, price, address` | supervised learning or reinforcement learning                |
| ![image-20220907165419278](dialogs/image-20220907165419278.png) | How to say it                                                | e.g. `provide: restaurant_name, price, address` $\to$ Curry prince is moderately priced and located at 452 newmarket road. | Seq-2-Seq generation                                         |

why has this been a popular framework?

- since this is a modular framework, it is easier to debug/find error and/or employ constraints
- however, it would become difficult to update the entire system since we need all components to be coherent

Of course, then you have this simple brute force approach

<img src="dialogs/image-20220907170317204.png" alt="image-20220907170317204" style="zoom:33%;" />

but then the problem is:

- hard to perform error analysis: did the model failed to understand? fail to plan? fail to generate?
- difficult to control the output as desired

> **Some challenges**:
>
> - Dialog history and/or context tracking is still sub-optimal
>
> - No data! No labelled data, and conversational data are mostly from company
> - Big domain shifts between different dialog domains
> - Difficult to evaluate how good your dialog system is (without human)

# Notes on Presentations

- explain table: what is measured/the metric
- anything wierd about table
- outline, why am I talking about this

# Dialog Datasets

Contains readings for different dialog datasets. Some common things you need to know is:

- **Wizard-of-Oz** (a style of collecting dialog data): connects two crowd workers playing the roles of the user and the system. The user is provided a goal to satisfy, and the system accesses a database of entities (basically to resolve the user's requests), which it queries as per the users preferences.
- **Machine-machine Interaction** (a style of collecting dialog data): the user and system roles are simulated to generate a complete
  conversation flow (e.g. generate what DA to do at each turn), which can then be converted to natural language using crowd workers.

## MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling

> **Aim**: Provide more data and ==across more than one domains even within a conversation==. Therefore, they collected a fully labelled (current *domain*, *dialog intent*, *slot*, and its *value*) human-human written conversations over multiple domains. This can then be used for belief tracking (i.e. what the system believe the user's intent is), dialog act and response generation.
>
> - cover between 1 and 5 domains per dialogue thus greatly varying in length and complexity. This broad range of domains allows to create scenarios where domains are naturally connected. 
> - For example, a tourist needs to *find a hotel*, to get the list of *attractions* and to *book a taxi* to travel between both places.

**Dataset Setup**:

- Each dialogue is annotated with a sequence of **dialogue states** (i.e. what the user is asking for, by tracking which intent/slot-value the user is trying to fulfill) and corresponding **system dialogue acts** (i.e. the DA of the system repones)

- A domain is defined by an ontology (i.e. set of concepts that are related), which is a collection of slots and values that the system has (know how to deal with):

  <img src="dialogs/image-20220911010133911.png" alt="image-20220911010133911" style="zoom: 67%;" />

  and in general, a dialogue act consists of the intent/act type (such as `request `or `inform`) and slot-value pairs. For example, the act `inform(domain=hotel,price=expensive)` has the intent `inform`, where the user is informing the system to constrain the search to expensive hotels.

**Dataset Collection Setup**:

1. sample domains to generate some dialog scenario

2. prompt a user with a **task template** (generated by machine but mapped to natural language using a template)

   <img src="dialogs/image-20220911010401705.png" alt="image-20220911010401705" style="zoom:67%;" />

   which is presented gradually to the user, and the user needs to ==fulfill those tasks by talking to the system==.

3. the system (wizard) is given some API/backend to query whatever he/she needs, and tries to answer the question. Note that the ==belief state can be tracked implicitly== here since we can just check what the system is querying the database to know what he/she thinks the user needs.

   - note that in the end, it is annotated again by the MT because there could be errors when performing the database queries (e.g. incomplete)

4. perform annotation on dialog act (e.g. `inform(domain=hotel,price=expensive)`) by using Amazon Mechanical Turk (by eliminating some poor workers). An example of what they are given is shown here

   <img src="dialogs/image-20220911011238227.png" alt="image-20220911011238227" style="zoom: 50%;" />

5. This results in the following data statistics:

   <img src="dialogs/image-20220911011549988.png" alt="image-20220911011549988" style="zoom: 67%;" />

   on the left we see that multi-domain dialogs tends to be longer, and on the right we see that system reponses tend to be longer. And finally, very self-explanatory:

   <img src="dialogs/image-20220911011454740.png" alt="image-20220911011454740" style="zoom: 67%;" />

**Dataset Benchmarks**: here they consider using this dataset to do three things:

- **dialog state tracking** (identify the correct slot-value pair): since other datasets only have a single domain, only the `restaurant` domain in this dataset is used:

  <img src="dialogs/image-20220911012028270.png" alt="image-20220911012028270" style="zoom:67%;" />

  Here, the model is a SOTA from another [paper](https://arxiv.org/abs/1807.06517) which basically considers learning slot-value pairs and domains separately during training, hence enabling model to **learn cross-domain slot-value pairs**:

  <img src="dialogs/image-20220911012133082.png" alt="image-20220911012133082" style="zoom:67%;" />

  the result is that overall accuracy is lower, hence this dataset is harder.

- **dialog-context-to-text generation** (more end-2-end than next one) given the oracle belief state (tracked when system is doing query). We generate the response by doing:

  <img src="dialogs/image-20220911012630020.png" alt="image-20220911012630020" style="zoom: 67%;" />

  which basically sets the hidden state $h$ for the decoder to be information about what user said (belief state), while attending to the user's actual utterances. Since this is generation, metrics such as BLEU for fluency would apply:

  <img src="dialogs/image-20220911012807592.png" alt="image-20220911012807592" style="zoom: 50%;" />

  where *Success* measures whether if the system has fulfilled the user's request, and *Inform* measure whether the system has provided an appropriate entity.

- **dialog-act-to-text generation**: since we have all the annotated dialog-act as well, we can test systems on their ability to generate utterances from meaning representations (e.g. dialog act to do next). Details skipped here.

## Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset

> **Aim**: In reality, there might be a dynamic set of intents and slots for a task, i.e. their possible values not known in advance. Therefore, they propose a **dataset for zero-shot settings** and also propose a ==schema-guided paradigm== to train models making **predictions over a dynamic set of intent/slots you can have in the input schema**.
>
> - essentially this dataset is collected by 1) generate outlines via M-M interaction, and then 2) convert to human language by AMT paraphrasing them

**Dataset Setup**:

<img src="dialogs/image-20220911014016544.png" alt="image-20220911014016544" style="zoom: 50%;" />

- a **schema** (see above) is a combination of intent and slots with some additional constraints (such as some intent requires at least certain slots to be there)
- there are also ==non-categorical== slots

**Dataset Collection**:

<img src="dialogs/image-20220911014417690.png" alt="image-20220911014417690" style="zoom: 67%;" />

1. first they use dialog simulators to interact with services to generate dialog outlines, basically a). This is achieved by first seeding the user agent with a scenario (among the 200 they identified), and then generate the dialog act to do for the next turn.
2. then the system agent generates in a similar fashion.
3. the rest is basically shown in the above flowchart.
4. the MT is asked to **exactly repeat the slot values** in their paraphrases. This has the advantage of easily string matching to find the ==slot span annotation automatically==. Also, it ==preserves all the annotation in a)==, hence there is no more need for human annotators.
   - perhasp a disadvantage: reduced noise in human conversation as every/most sentences would be goal oriented? Are the intent natural on a human-basis?

**Dataset Benchmark**: since one aim of this is to do ==zero-shot dialog state tracking==, they also made a model to do that

- their model is basically done by:
  1. obtain a **schema embedding** by converting the current information into an embedding (e.g. by BERT). which embeds all the intents, slots, and slot-values.
  2. obtain an **utterance embedding** of the user and the previous dialog history
  3. combine the above using the so called "projection" operation, which you can then use to do
     - **active intent** prediction: what is the current intent?
       - only a single intent per utterance?
     - **requested slots** by the users (a classification task)
     - **user goal** prediction: what is the slot-value currently (up until now) request by the user
       1. have a classifier to predict for each slot if things have changed
       2. if yes, predict what is the new value

- then, from the above we can consider evaluation tasks such as:

  - **Active Intent Accuracy**: how much active intent of the user I got right
  - **Request Slot F1**: macro-averaged F1 score for requested slots
  - **Average Goal Accuracy**: for each turn, the accuracy of the predicted value of each slot (fuzzy machine score used for continous values)
  - ==Joint Goal Accurarcy==: usually more useful/stricter than the above
  
  some important results are shown here
  
  <img src="dialogs/image-20220911020544880.png" alt="image-20220911020544880" style="zoom: 50%;" />
  
  where we notice that major factors affecting the performance across domains is:
  
  1. the presence of the service in the training data (seen services), so degrade for domains with more unseen services
  2. Among seen services, `RideSharing` domain also exhibits poor performance, since it possesses the largest number of the possible slot values across the dataset
  3. for categorical slots, with similar slot values (e.g. `Psychologist` and `Psychiatrist`), there is a very weak signal for the model to distinguish between the different classes, resulting in inferior performance

## Can You Put it All Together: Evaluating Conversational Agents Ability to Blend Skills

> **Aim**: Recent research has made solid strides towards gauging and improving performance of open domain conversational agents along specific skill. But a good open-domain conversational agent should be able to seamlessly **blend multiple skills** (knowledge, personal background, empathy) all into one cohesive conversational flow. Therefore, in this work they:
>
> - investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training
> - propose a new dataset, `BlendedSkillTalk`, which blends multiple skills into a single conversation (one skill per turn, but different across turns), to analyze how these capabilities would mesh together in a natural conversation

**Dataset Setup**

<img src="dialogs/image-20220911102517255.png" alt="image-20220911102517255" style="zoom: 67%;" />

- `BendedSkillTalk`, a small crowdsourced dataset of about 5k conversations in English where workers are instructed to try and be knowledgeable (`Wizard of Wikipedia`), empathetic (`EmpatheticDialogs`), or give personal details (`ConvAI2`) about their given persona, whenever appropriate.

  - ConvAI2 example:

    <img src="dialogs/image-20220914173659527.png" alt="image-20220914173659527" style="zoom: 50%;" />

    then the user is asked to converse on telling each other about their personalities

  - WoW example:

    <img src="dialogs/image-20220914173747579.png" alt="image-20220914173747579" style="zoom:33%;" />

    where the topic is used as an initial context

  - ED example:

    <img src="dialogs/image-20220914173900784.png" alt="image-20220914173900784" style="zoom:33%;" />

- labels are the skill per turn, namely which skill they used.

**Dataset Collection**

- To ensure MT workers not stick with one specific skill or being too generic, they prompt one user (**guided user**, denoted by `G`) with responses from models that have been trained towards a specific skill as inspiration (one each from each skill)
- Each starting conversation history is seeded randomly from the `ConvAI2`, `WoW`, `ED` dataset
  - if from `WoW`, a topic is also given. If from `ED`, the situation description is given
- In fact, for labelling we have 4 labels because `ED` utterances has "Speaker" and "Listener" taking different actions:
  - Knowledge, Empathy, Personal situations, Personal background

**Dataset Benchmarks**: since it is combining skills from three datasets essentially, we can have:

- a base architecture of poly-encoder which is a retrieval model: select from a set of candidates (the correct label combined with others are chosen from the training set). This is pretrained on the pushshift.io Reddit dataset

  - since it is asked to rank among candidates, metric such as `hit@k` applies. This metric works as follows. Consider two correct labels:

    ```bash
    Jack   born_in   Italy
    Jack   friend_with   Thomas
    ```

    and a bunch of synthetic negatives are generated, where the model is ranking them:

    ```
    s        p         o            score   rank
    Jack   born_in   Ireland        0.789      1
    Jack   born_in   Italy          0.753      2  *
    Jack   born_in   Germany        0.695      3
    Jack   born_in   China          0.456      4
    Jack   born_in   Thomas         0.234      5
    
    s        p         o            score   rank
    Jack   friend_with   Thomas     0.901      1  *
    Jack   friend_with   China      0.345      2
    Jack   friend_with   Italy      0.293      3
    Jack   friend_with   Ireland    0.201      4
    Jack   friend_with   Germany    0.156      5
    ```

    notice that if we are doing `hit@1`, then only $1/2$ times the model did correctly, but if we do `hit@3`, then the it is $2/2$.

- **Finetune on BST**: finetune the pretrained dataset directly on `BlendedSkillTalk ` Dataset

- **Multi-task Single-Skills**: finetune the model to multi-task on `ConvAI2`, `WoW`, and `ED`

  - however, note that there could be some stylistic difference between `ConvAI2` dialogs and `WoW` dialogs. For instance, the prior include a persona context, and the latter include a topic
  - therefore, to avoid model exploiting those, all samples are modified to ==always include a persona and a topic== (where there is already an alignment of `WoW` topics to `ConvAI2`)

- **Multi-task Single-Skills + BST**: after the multi-task training, finetune again on the BST dataset

- **Multi-task Two-Stage**: since many single-skilled models have already been trained, we can just use a top level BERT classifier to assign which model gets to score candidates.

- Then, along with just models trained on their own dataset, we can have 7 models to evaluate:

  <img src="dialogs/image-20220911104543720.png" alt="image-20220911104543720" style="zoom:80%;" />

  where we observe that:

  - BST shows balanced performance but failed to match the performance of the single-skill models on their original dset, as well as losing to MT Single-Skills.

  - MT Single-Skills does not do exactly well as single-skill model when evaluated on their own benchmark (for `ConvAI2` and `ED`). But perhaps this is unfair since those Single-Skill models only have to choose from candidates from their own domain. Hence, the author considers mixing candidate for them to also include samples from other dataset, which gives rise to the *Mixed-candidates evaluation*.

    Here, MT Single-Skills is doing better, suggesting that multi-task training results in ==increased resilience to having to deal with more varied distractor candidates==

- finally, there is of course the human evaluation:

  <img src="dialogs/image-20220911105544154.png" alt="image-20220911105544154" style="zoom: 67%;" />

# Dialog Understanding

Many models are basically performing the task of:

- **input** dialog history up to current turn
- **output**: "what does the user want from us" in terms of classifying **intent, slot, slot-value**
  - then by filling in the values, *e.g. book a plane from Paris to London*, the system can perform API based queries `source=London, dest=Paris`

## Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling

> **Aim**: we want to explore how the ==alignment information in slot filling== can be best utilized in the encoder-decoder models, and on the other hand, whether the alignment-based RNN slot filling models can be further improved with the attention mechanism that introduced from the encoder-decoder architecture . To this end, we propose an **attention-based neural network model for joint intent detection and slot filling**.
>
> The main idea is:
>
> - encode input sequence into a dense vector
> - then use this vector to decode/generate corresponding output sequence. Here LSTM is used so alignment is natural (see example below)

**Setup**

![image-20220916193314825](dialogs/image-20220916193314825.png)

- the input essentially is the sentence, and slot detection and slot-filling is **done simultaneously from the flat structure**
  - ==potential problem== cannot track implicit information, e.g. slot value not there, or hierarchical information (i.e. tagging v.s. parsing)
- let the input sentence be $x=(x_1,...,x_T)$, and output be $y=(y_1,...,y_T)$ and intent in addition.

**Architecture**

|                      no aligned inputs                       |                        aligned inputs                        |                 aligned inputs and attention                 |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220916193557680](dialogs/image-20220916193557680.png) | ![image-20220916193613690](dialogs/image-20220916193613690.png) | ![image-20220916193622327](dialogs/image-20220916193622327.png) |

- essentially you have an RNN for encoder, and an RNN for decoder with inputs using the hidden states from $x$ for alignment

  - **encoder**: bidirectional LSTM, so we get $[fh_1,...,fh_T]$ and $[bh_1,...,bh_T]$ being the forward/backward hidden states. Then $h_i = [fh_i, bh_i]$ is the total hidden state for each input $x_i$
  - **decoder**: unidirectional LSTM

- to model attention vector, we want to measure how each **hidden state $s_j$ in the decoder** relates to the **aligned word's hidden state $h_k$**:
  $$
  e_{i,k} = g(s_{i-1},h_k)
  $$
  then essentially $e$ is transformed into weights
  $$
  c_i = \sum_{j=1}^T \alpha_{i,j}h_j,\qquad \alpha_{i,j} = \mathrm{Softmax}(e_{i,j})
  $$
  essentially $c_i$ provides additional information to answer: which of the aligned word's hidden state $h_j$ relate to the current state I want to decode $s_{i-1}$. They also provided a visualization of this attention

  <img src="dialogs/image-20220916194611133.png" alt="image-20220916194611133" style="zoom:80%;" />

  where we are decoding the last tag, and we are attending to words such as `cleveland`.

  - they mentioned that the motivation to bring in an additional context $c$ is because **distant information just from $s_j$ tends to be diluted a lot and lost**.

- therefore, their ultimate model involves giving the decoder both the aligned hidden state $h_i$ and the attended context $c_i$ 

- ==TODO== two objectives and one network

- to further **utilize the decoded tags**, they can also do it **auto-regressively** by feeding the decoded tag into the forward direction of the RNN

  <img src="dialogs/image-20220916195139765.png" alt="image-20220916195139765" style="zoom:67%;" />

**Results**

- They are using ATIS-3 and DEC94 dataset, which has joint intent detection and slot filling task

- then essentially they were doing an ablation study of a) effect of alignment b) effect of attention:

  <img src="dialogs/image-20220916195457119.png" alt="image-20220916195457119" style="zoom:67%;" />

  so it is found that **alignment and attention does give improvement**, yet the latter gave only small improvement. They also investigate further and found that the attention was giving mostly uniform weights except for a few cases (hence the small boosts)

## Conversational Semantic Parsing for Dialog State Tracking

> **Aim**: By formulating DST (dialog state tracking) as a semantic parsing task over ==hierarchical representations==, we can incorporate semantic compositionality, cross domain knowledge sharing and co-reference.
>
> - essentially outputting a tree instead of slot-filling
> - additionally, they collected their own dataset `TreeDST `annotated with tree-structured dialog states and system acts
> - they combine essentially the idea of **semantic parsing (e.g. drawing tree from production rules)** to do DS tracking 
>   - in their approach, the tree drawing is done by predicting a node and its parent

**Setup**: 

<img src="dialogs/image-20220916202809150.png" alt="image-20220916202809150" style="zoom:67%;" />

- As in DST, the task is to ==track a users goal== as it accumulates over the course of a conversation. To capture a hierarchical representation of domain, verbs, and operators and slots:

  |                     Tree Representation                      |               Condensed, Dotted Representation               |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="dialogs/image-20220916202659271.png" alt="image-20220916202659271" style="zoom:50%;" /> | <img src="dialogs/image-20220916202702538.png" alt="image-20220916202702538" style="zoom:50%;" /> |

  this is done for both user and system, and this dotted form is also referred to as a **meaning representation**. Advantages of this include:

  - tracking ==nested intents== and representing compositions in a single graph
  - naturally supports ==cross domain slot sharing== and cross-turn co-reference through incorporating the shared slots or the references as sub-graphs in the representation

- At each turn we have

  - $x_t^u$ being user utterance, $y_t^u$ being user dialog-state
  - $x_t^s$ being system utterance, $y_t^s$ is system dialog act
  - both $y_t^s,y_t^u$ adopt the same structure semantic formalism

**Dataset**:

- collection is similar to the [SGD](#Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset) paper, where they:

  1. generate agendas/conversation flows by using machines
  2. convert them via templates to language
  3. ask MT to paraphrase to natural language

- specifically, the agenda generation for both user and system is done by:

  - a module **generating the initial user goal** $P(y_o^u)$. This can be done by sampling a ==production rule==

    <img src="dialogs/image-20220916204534378.png" alt="image-20220916204534378" style="zoom: 67%;" />

  - a module **generating system act** $P(y_t^s|y_t^u)$. This is done by looking at the user's act tree $y_t^u$, look up the production rules to figure out **how to finish the tree**, and take that as the system act

    <img src="dialogs/image-20220916205038811.png" alt="image-20220916205038811" style="zoom:67%;" />

  - a module for **user state update**/DS based on dialog history $P(y_t^u|y_{<t}^s,y_{<t}^u)$. There are two details in this task:

    - model "introduces a new goal, continues with the previous goal, or resumes an earlier unfinished goal". Therefore, a **stack is used** and updated. Therefore the top of the stack always represents the most recent unfished task $y_{t-1}^{top,u}$ and the corresponding system act $y_{t-1}^{top,s}$

    - the next dialog state $y_t^u$ is generated based on the **top elements of the stack as the dialog history**

      - ==precaustion:== implies an additional structure to dialog, hence unfair advantage as their model also stores a stack? (see their modelling choice of dialog history, which takes the top of the stack as well)

      since this is now generated from two trees (there are two at the top of the stack)

      <img src="dialogs/image-20220916205633058.png" alt="image-20220916205633058" style="zoom:67%;" />

      where the next user state will be combining the two

- finally, a quality control is also done by asking:

  - before the paraphrasing, filter out non-realistic interactions
  - after the paraphrasing, ask if the human-generated utterance preserves the meaning of the templated utterance

**Architecture**: 

- our task is to:

  - infer ==$y_t^u$== since $y_t^s$  is observed/what the system just did
  - to also track ==goal switching and resumption==, a **stack is used to store dialog states**

- to output tree structure, we essentially just need to decode:

  - a new node
  - the new node's parent in the existing tree

- additionally, they chose to ==maneuver their own features==:

  - **dialog history** is computed as a fixed-size history representation derived from the previous conversation flow $(Y_{<t})$, specifically the top of the stat $y_{t-1}^{top,u}$ along with:
    $$
    Y_{t-1}^u = \text{merge}(y_{t-1}^u, y_{t-1}^{top,u})
    $$

  - **encoding the features**: 

    <img src="dialogs/image-20220916210719872.png" alt="image-20220916210719872" style="zoom:80%;" />

    where essentially we have three features and hence three encoders:

    - encoder (bidirectional LSTM) for user utterance $x_t^u$
    - encoder (bidirectional LSTM) for system act $y_{t-1}^s$
    - encoder (bidirectional LSTM) for dialog state $Y_{t-1}^u$ described above

    where to encode the trees, they are **first linearized into strings by DFS**

  - **decoding**: they experimented with two versions:

    - ==just decode a flattened string==, as in Figure 1. The final aim is to simply ==compute the probability of next token== by accessing its probability of generation and copy

      Specifically, the decoder takes
      $$
      g_i = \mathrm{LSTM}(g_{i-1},y_{t,i-1}^u)
      $$
      which is basically **auto-regressive**, and the **attention** is used together with $g_i$ to compute probability of next token

      - attention: **attend current state to history**
        $$
        a_{i,j} = attn(g_i,H)
        $$
        where history would be either of the three from the three encoders. Then this is used for weights to produce:
        $$
        \bar{h}_i = \sum_{j=1}^n w_{i,j} h_i,\quad w_{i,j} = \mathrm{Softmax}(a_{i,j})
        $$

      - finally, the token distribution is computed by **concatenating $\bar{h}_i^x, \bar{h}_i^s, \bar{h}_i^u$** from the encoders and $g_i$ **to give $f_i$**

        <img src="dialogs/image-20220916211928284.png" alt="image-20220916211928284" style="zoom:67%;" />

      finally, since this is decoding a flat string, the loss is simply CCE of the correct $y_{y,i}^u$

    - ==decode a tree by generating nodes and select their parent relationships from the existing tree==: you basically take the hidden state/embedding of the previous node $n_{t,i-1}^u$ and its parent relation $r_{t,i-1}^u$ to be input features as well for decoding:
      $$
      g_i = \mathrm{LSTM}(g_{i-1},n_{t,i-1}^u , r_{t,i-1}^u)
      $$
      the we perform two predictions using two layers using $g_i$

      - predict next node probability using equation 4

      - select parent of the node by attending $g_i$ to **previously generated nodes** and choosing the most relevant

**Results**

- experiment on DST dataset, where for models not with hierarchical MR in mind, all training and testing here are flattened:

  <img src="dialogs/image-20220916212530129.png" alt="image-20220916212530129" style="zoom:67%;" />

  - ==TODO== doesn't this also mean that flattened based is naturally less accurate than tree based (TED-Flat v.s. TED-Vanilla) given that the **task is tree based**?

- evidence of compounding error if prediction is done auto-regressively:

  <img src="dialogs/image-20220916212637904.png" alt="image-20220916212637904" style="zoom:80%;" />

  where the oracle means substituting the gold previous state for encoding

# Task-Oriented Dialog

Essentially models that can complete task by providing/informing the right entities and complete the task successfully. A popular dataset that has been used for finetuning + testing is the **MultiWoZ** dataset, which contains an **automatic evaluation script** as well.

## GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog

In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning.

> Specifically, we introduce a ==dialog act prediction task for policy optimization== during pre-training and employ a consistency  regularization term to refine the learned representation with the help of unlabeled dialogs

**Background**:

- there are intrinsic differences between the distribution of human conversations and plain texts. Directly fine-tuning plain-text-trained PLMs on downstream dialog tasks **hinders the model** from effectively capturing conversational linguistic knowledge. Therefore, current attempts to tackle this issue try to **build Pre-trained Conversation Models (PCMs) by directly optimizing vanilla language model** objectives on dialog corpora
- Therefore, we hypothesize that explicitly incorporating the DA annotations into the pre-training process can also **facilitate learning better representations for policy optimization to improve the overall end-to-end performance**
- Although DAs are general tags to describe speakers communicative behaviors (Bunt 2009), current DA annotations in task-oriented dialog are still limited and **lack of unified taxonomy** because each dataset is small and scattered.

**Dataset**:

- To begin with, we build a unified DA taxonomy for TOD (task-oriented-dialog) and examine eight existing datasets to develop a new labeled dataset named `UniDA `with a total of 975K utterances. We also collect and process a large-scale unlabeled dialog corpus called `UnDial ` with 35M utterances

- We propose a more **comprehensive unified DA taxonomy** for task-oriented dialog, which consists of 20 frequently-used DAs

  <img src="dialogs/image-20220705152203066.png" alt="image-20220705152203066" style="zoom:67%;" />

**Model**:

- We choose ==UniLM== (Dong et al. 2019) as our ==backbone== model, which contains a bi-directional encoder for understanding and a unidirectional decoder for generation.

- We adopt a similar scheme of input representation in Bao et al. (2020), where the ==input embeddings consist of four elements: tokens, roles, turns, and positions==.

  - Role embeddings are like segmentation embeddings in BERT and are used to differentiate which role the current token belongs to, either user or system
  - Turn embeddings are assigned to each token according to its turn number.
  - Position embeddings are assigned to each token according to its relative position

- Four objectives are employed in our dialog pre-training process: response selection, response generation, ==DA prediction== and consistency regularization.

  ![image-20220705152829713](dialogs/image-20220705152829713.png)

  where the important components include:

  - **Response Selection**. For a context response pair $(c; r)$ from the corpus, the positive example (with label $l = 1$) is obtained by concatenating $c$ with its corresponding response $r$, and the negative example (with label $l = 0$) is constructed by concatenating $c$ with a response $r^-$ that is randomly selected from the corpus. Then simply for each pair:
    $$
    L_{RS} = -\log p(l=1|c,r)-\log p(l=0|c,r^-)
    $$
    where the predicted probability is fine-tuned by adding a linear head to the transformer and then a sigmoid on the `[CLS]` token:
    $$
    p(l=1|c,r) = \mathrm{sigmoid}(\phi_a(h_{cls}))
    $$
    note that $\phi_a$ is a fully-connected NN with output size of 1.

  - **Response Generation**. The response generation task aims to predict the dialog response r auto-regressively based on the dialog context $c$. Here we use the standard NLL loss per token in each generated sequence:
    $$
    L_{RG} = - \sum_{t=1}^T \log p(r_t|c,r_{<t})
    $$
    for $r_t$ is the $t$-th word in $r$, and $r_{<t}=\{r_1,...,r_{t-1}\}$. So basically this is the greedy prediction. 

  - **DA Prediction**: For a context response pair $(c; r)$ sampled from `UniDA`, the DA prediction task aims to predict the DA label $a$ of the response $r$ based ==merely on the context $c$==. However, since there are responses in `UniDA ` associated with multiple DA, we model it as a Bernoulli Distribution such that $a\equiv (a_1,a_2,...,a_N)$ for $N$ being the number of dialog acts, and $p(a|c)=\prod_i^N p(a_i|c)$. Therefore, taking the dialog context $c$ as input, we add a multi-dimensional binary classifiers on $h_{cls}$ to predict each act $a_i$:
    $$
    L_{DA}=-\sum_{i=1}^N \{y_i \log p(a_i|c)+(1-y_i)\log(1-p(a_i|c)\}
    $$
    and our prediction is a single $N$ dimensional vector
    $$
    p(a|c) = \mathrm{sigmoid}(\phi_b(h_{cls})) \in \mathbb{R}^N
    $$

  - **Consistency Regularization**: because there is no DA label for `UniDial`, we do some kind of self-supervision on inferring the DA labels based on a given dialog context $c$. Specifically, we use the same network $\phi_b$ to predict the dialog act twice after a dropout layer:
    $$
    q(a|c)=\mathrm{softmax}(\phi_b(h_{cls})) \in \mathbb{R}^N
    $$
    which basically predicts DA distribution of a given sequence. Then as we feed the sequence through the same dropout layer we have different hidden features, we can consider to match the two distributions:
    $$
    L_{KL} = \frac{1}{2}\left( D_{KL}(q_1||q_2)+  D_{KL}(q_2||q_1)\right)
    $$
    essentially making sure that the learnt features are useful for DA prediction.

- Hence, the pretraining objective is by mixing:

  - `UniDA` samples which includes DA annotation, hence
    $$
    L = L_{RS} + L_{RG} + L_{DA} + L_{KL}
    $$

  - `UniDial` samples which does not have DA annotation and some are very ==noisy==. Hence we consider a ==gating mechanism of weighting the $L_{KL}$== by looking at the entropy of the DA prediction $q(a|c)$:
    $$
    E = \sum_{i}^N q(a_i|c)\log(q(a_i|c))
    $$
    and we want to weight more on samples with low entropy:
    $$
    g=\min \left\{ \max\left\{0, \frac{E_{\max} - (E-\log E)}{E_\max} \right\},1 \right\}
    $$
    Therefore our loss for those data is:
    $$
    L = L_{RS}+L_{RG}+g L_{KL}
    $$

- Finally, once we trained our model with mixing `UniDA` and `UniDial`, it is a ==pretrained model and we can fine-tune it with our desired DA dataset== :
  $$
  L_{\text{fine-tune}} = L_{RS} + L_{RG} + \alpha L_{DA}
  $$
  for $\alpha=1$ if the dataset has annotated DA, and $\alpha=0$ otherwise. Notice that a response selection objective is still here even if we in the end only need generation capability. This is because we want to alleviate the model discrepancy between pretraining and finetuning.

## GODEL: Large-Scale Pre-Training for Goal-Directed Dialog

> **Aim**: GODEL leverages a **new phase of grounded pre-training** designed to better support adapting GODEL to a wide range of downstream dialog tasks that **require information external** to the current conversation (e.g., a database or document) (i.e. the environment $E$ in the model) to produce good responses. While GODEL out-performs previous models such as DialoGPT, they also introduce a novel evaluation methodology: the introduction of a notion of **utility** that assesses the usefulness of responses (**extrinsic** evaluation) in addition to their communicative features (**intrinsic** evaluation, e.g. BLEU). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.

**Setup**

- First, it is pre-trained in **three phases**, successively folding in data from web text, publicly available dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks (conditioned on information external to current conversation).

- we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this **extrinsic** dimension of functional **utility**, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.

  Therefore, a evaluation metric called ==Utility== is proposed, so that cross-dataset comparison can be made instead of the adhoc metrics for a dataset (e.g. Success-rate and Inform-rate).

  - currently the **utility** can only be human-evaluated

- after large scale pretraining, the model is ==tested== on **Multi-WOZ**, **CoQA**, **Wizard of Wikipedia**, and **Wizard of the Internet**

**Pretraining**

- GODEL is pre-trained in three phases: 

  1. Linguistic pre-training on **public web documents** to provide a basic capability for text generation. 
  2. Dialog pre-training on public **dialog data** to improve the models handling of general conversational behavior. 
  3. **Grounded dialog pre-training** to enable grounded response generation

  and since we can have grounded dialog, the input can have $S,E$ for $S$ being the dialog context history, and the additional information needed (e.g. price of a hotel) is the environment $E$

  <img src="dialogs/image-20220922214111334.png" alt="image-20220922214111334" style="zoom:67%;" />

  then, the loss for all pretraining task is the cross entropy for decoding each word:
  $$
  p(T|S,E) = \prod_{n=1}^N p(t_n | t_1, ..., t_{n-1}, S,E)
  $$
  so $T = \{t_1,...,t_n\}$ is the target sentence. Note that in tasks that does not require extra information, $E$ is left as empty.

- the **public dialog dataset** comes from the Reddit comment chains used for DialoGPT
- the **grounded dialog dataset** contains a collection of: DSTC7 Task 2 corps, MS MARCO, UnifiedQA, SGD

**Model**

- backbone based on T5,  T5-Large, and GPT-J is used
- the models are trained for at most 10 epochs, and we select the best versions on the validation set

**Experiments**

- we would like to test the **few-shot finetuning** ability of the mode, as well as full finetuning, because in general labeled task-oriented data is small in size

- dataset used for testing therefore include the ==untouched==: MultiWOZ, Wizard of Wikipedia, Wizard of Internet, and CoQA

  - specifically, for few-shot we consider tuning 50 dialogs for each task for finetuning
  - **automatic evaluation metrics are often setup already** for those datasets, especially for MultiWOZ

- results for few-shot finetuning and full finetuning

  |                     Few-shot Fine-tuning                     |                       Full Fine-tuning                       |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | <img src="dialogs/image-20220922215023283.png" alt="image-20220922215023283" style="zoom:50%;" /> | <img src="dialogs/image-20220922215110548.png" alt="image-20220922215110548" style="zoom:50%;" /> |


# Social Chatbot

If the aim is to **extend conversations** by engaging user in anyway, what can we do?

## Alquist 4.0 Towards Social Intelligence Using Generative Models and

> **Aim**: The system Alquist has a goal to conduct a **coherent and engaging conversation**, which essentially uses a ==hybrid of hand-designed responses== and ==generative models==
>
> - usually it is the hand designed responses tree that is functioning
> - when OOD is detected from the tree (using intent classifier), response generative model is used.
>   - note that **a lot of control is needed** for the generative model to merge nicely with the hand-written responses

**Setup**

- in order to entertain the conversational partner, one has to learn what entertains the partner first and then utilize the knowledge in the following conversation

  - *exploration* part, in which Alquist learns the preferences of the user, the main research and development emphasis was put on **Skimmer**, **User Profile building**, and **Entity and Knowledge Utilization**
  - *exploitation* part, in which Alquist utilizes the knowledge about the user, the main emphasis was put on the research and development of the **Dialogue Manager**, **Trivia Selection**, **Intent and Out-of-Domain classification**

- the basic flow looks like this:

  <img src="dialogs/image-20220930210844601.png" alt="image-20220930210844601" style="zoom:80%;" />

  and on a high level, we have on the left:

  1. First, the **Skimmer** analyses the user input for the mentioned pieces of information. The pieces of information are stored in the **User Profile**. 
  2. Based on the values stored in the user profile, the **Dialogue Management** selects the next dialogue to start, or selects and presents some trivia related to the actual topic of a conversation. 
     - The dialogue is directed according to the **Intent classification** of the user input. 
     - and if needed, knowledge base is queried for updated information
  3. And finally, if the **Out-of-domain classification** recognizes an unexpected user input whenever there is, the Neural Response Generator produces a coherent response based on the context of the conversation. Otherwise, it is the scripted dialog.

**Main Components**

- **Skimmer**: extract user information from sentences using regular expressions, and then stored the attribute-value into **User Profile**

- **User Profile**: essentially stores information about the user talking to Alexa:

  - long-term profile: global information the bot needs when the same user converses again later
  - short-term profile: stores information discussed during the current session/dialog

  so that short-term profile is reset at the beginning of each session

- **Entity and Knowledge Utilization**: include ==factual information== about the entity that the user is interested in. This means you need to do:

  - *entity recognition*: a sequence tagging task performed by Bi-LSTM
  - *entity linking and knowledge base*: utilize external public domain-specific dataset to obtain **information about the recognized entity**

- **Dialog Management**: there are ==several small scripted dialogs (of intents)==, which are of high quality and will be of focus of this system. Then, since there are several dialogs, a **dialog selector** is used when a small dialog finished and needs to find a continuation dialog:

  1. the **dialog selector** collects information about the previous context, such as user profile, topics discussed (from tags), and some additional constraints if the new dialog can start (prerequisites)
  2. if there is no trivia presented for the presently discussed topic, **choose a trivia**
  3. if there are some scripted dialogs that fulfill the constraints, consider them
  4. if there is none, use the **neural response generator**

- **Trivia Selection**: trivia scraped from reddit, and a model for outputting an embedding for **scoring cosine similarity** of a trivia and the current context is used

  - during scraping, vector embeddings are also stored with texts
  - during runtime, a candidate trivia list is retrieved using **full-text search**
  - context of $n=2$ most recent utterance-response pairs is encoded
  - cosine similarity computed and most relevant trivia is selected

  experimentally, the choice of model is determined empirically:

  <img src="dialogs/image-20220930212606488.png" alt="image-20220930212606488" style="zoom:67%;" />

- **Intent and OOD Classification**: since each user utterance is seen as intents, the scripted dialog cannot continue if an intent is OOD from the script:

  <img src="dialogs/image-20220930212722131.png" alt="image-20220930212722131" style="zoom:80%;" />

  where there are two intents in question:

  - global intent: used/can occur at anywhere
  - local intent: should be in the tree

  again, a combination of a) cosine similarity using a model b) filtering out intents if cosine similarity is not high enough c) if non is left, the intent is OOD.

  Which model to use is again chosen empirically, but notice that since such a task is not common in other dsets, they had to artificially create one using existing dset by leaving some intents out, and also hand-annotated a new one

  <img src="dialogs/image-20220930213028860.png" alt="image-20220930213028860" style="zoom:80%;" />

  notice that now performance speed also matters

- **Neural Response Generator**: used in two tasks: a) if **OOD**, use this model to generate b) generates a follow up question when a **trivia is selected** in the dialog manager. Notice that the aim of this is to ==compensate== the incomplete scripted dialog in some cases, but the major focus should be on the success/controllability of the scripted dialog:

  <img src="dialogs/image-20220930213317181.png" alt="image-20220930213317181" style="zoom:80%;" />

  where notice that the hand-designed dialog is restored at the end. To have this controllability, the model:

  - **either generates a question or a statement** (otherwise content generated is quite random)
  - so that statements followed by a question would be engaging
  - and statement itself can be used to bridge the gap between generated and hand-designed content

  To achieve this, essentially:

  - DialoGPT model is used, where the special token `QUESTION` and `STATEMENT` is appended/prompted along with the context
  - when DialoGPT generated a few candidate questions/statements, a DialoRPT is used to **rank the generated content**
  - tested on several datasets, by:
    - using NLTK tokenizer and CoreNLP to annotate each sentence as a statement or a question
    - then can convert many existing dialog dataset into such a format for generation

  results:

  <img src="dialogs/image-20220930213854750.png" alt="image-20220930213854750" style="zoom:80%;" />

  where for evaluation both automatic metric and human evaluation is used:

  - automatic metric is straightforward as the labels are known
  - human evaluation is done on if the generated content is **relevant and factually true**, i.e. it is `OK`
  - this is done similarly for checking the follow-up question generation after a trivia/fun facts, hence the two columns under `OK`

## Jason Wetson Guest Lecture

- Long term research goal

- ParlAI - collection of datasets, architectures, and integeration with Amazon MT

- find things that don't work, but make **your fix general**

- what is missing in current LM such as GPT-2/3:

  - knowledge and hallucination in GPT-3
  - hook on a retrieval system to incorporate knowledge and hence hallucinate less

- making an open domain dialog agent

  - wanted to have various skills such as Peronality, Emphathy, etc.
  - Hence collected trainning data on those skills
  - trained Blenderbot
    - Blenderbot 1: just stacking transformers
    - Blenderbot 2: lemon pick examples, and fix them. For exmaple, **very forgetful**, and a lot of **factual errors**
      - to solve factual errors, added internet search being part of the bot (generate an internet search query)
      - to solve the memory problem, had an addiitonal long term memory module to retrieve information
      - can even be used to recommend pizza places
    - Blenderbot 3: a bigger transformer, but collect more data to fix prior errors (e.g. still 3% hallucinations)
      - takes feedback live from people to learn during interactions, so **data from adversarial testers could be used**!
      - an big architecture, deciding if to do internet search, geneatre response, access memory, etc.

- how do we use those feedbacks from humans?

  - ==Director==: for each token, add a classification head on thumb-up or down. Therefore, when you generate, you can combine the two scores

    - https://arxiv.org/pdf/2206.07694.pdf

    - when you thumbs down, label entire sentence as negative. Will that smudge down good words, hence the how do you evaluate that classifier. This is what it does, but it still works.

  - however, in some cases that labelling can be more sensitive: e.g. if you have a gold correction sentence corresponding to a sentence, then you can align and *extract the part of the sentence* that is wrong.

**Still Problems on Chatbots**

- why is repetition ==neuron degenration==, liable to repeat

**Future of Chatbots**:

- how controllable is it to be applied in real life? Jason: probably not medical. But for entertainment, for gaming, recommendations pretty confident.
- Future of incoporating multimodal input into conversations? Jason: if text can be toxic, images can be even more toxic

# Knowledge Enriched Dialog Systems

Dialog system but aims to use/find external information to make responses more factual

## Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features

> **Aim**: Make a dialog system that is **controlled to stay faithful** to the evidence. They approach this by:
>
> - adding control tokens prepended to the input sequence
> - train dataset using additional information on the *objectiveness*, *lexical precision*, and *entailment* (it generated response follows from the given evidence)

**Setup**

- the general idea is to use a piece of evidence (provided beforehand) added to the conversation history as input

  <img src="dialogs/image-20221008141417379.png" alt="image-20221008141417379" style="zoom:80%;" />

- to train this type of model, dataset with only informative utterances are needed, but that kind does not exist. Since the only available ones (e.g. Wizard of Wikipedia) are generated by humans, they **mix chi-chat utterances/subjectivity** to **objective informative facts** in their responses

  - to deal with it, you either remove all subjective ones (e.g. has personal pronouns), but that leaves too little training data left

    <img src="dialogs/image-20221008141727577.png" alt="image-20221008141727577" style="zoom:80%;" />

  - or, as used in this paper, they score existing samples based on *objective voice*, *lexical precision* (sticking to fact), and *entailment* to provide additional ==signal during training and hence control during inference==

    - ablation study shows that those control does change performance
    - those control tokens are discussed next

- for training, **Wizard of Wikipedia** is used because there is a **gold-labelled evidence** provided in the dataset

**Model**

- tested on GPT2 and T5, hence overall pipeline looks like:

  <img src="dialogs/image-20221008142202615.png" alt="image-20221008142202615" style="zoom: 67%;" />

- they used two approaches in total to add control

  - added the **special control code** during training:
    - *objective voice*: estimated as a binary variable whether if first person pronoun is included
    - *lexical precision*: want most of the words in the response to be contained somewhere in the evidence (drawback is semantic similarity missing). This is then mapped to `<high-prec>`, `<med-prec>`, and `<low-prec>`
    - *entailment*: a binary variable to encourage response semantically entailed by the evidence. During training those are scored by a SOTA natural language inference model to estimate its entailment. Always `<entailed>` during inference
  - **resampling**: sample for $d$ times until a satisfactory response is found

- after training, performance is evaluated by

  - **automatic metric scoring** the responses on *objective voice*, *lexical precision*, *entailment*, and *BLEU* score compared to the original gold response

    <img src="dialogs/image-20221008143707179.png" alt="image-20221008143707179" style="zoom:67%;" />

    where ablation study is also here. However, since those metrics are all self-designed, human eval is needed.

  - **human evaluation:** subsample examples from generated response from the previous experiment and ask MT to score:

    |                           Table 4                            |                           Table 5                            |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |
    | ![image-20221008143835131](dialogs/image-20221008143835131.png) | ![image-20221008143850486](dialogs/image-20221008143850486.png) |

    where it seems that there is **better faithfulness (to evidence) and objectivity**

    - those scores are also found to be highly correlated to the automatic eval scores

    - notice that the gold-evidence is itself relevant to the conversation. So if I just copy-paste, or just summarize the evidence, won't that give me near perfect score on all three dimensions + probably also highly relevant and high fluency (since the evidence is a fluent text)? Trade off between abstractive and extractive summarization

## Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion

> **Aim**: An e2e model that includes capability of **Search-Engine, Knowledge Extraction, and Response Generation** all into one single model, while treating them as separate modular functionality. The final aim is again to be an open-domain knowledge grounded conversational agent.
>
> - achieved by using appending ==special tokens in the encoder== (or decoder) to indicate which module is being invoked
> - done study on treating each module as a separate model as well, but only found marginal improvement while model size becomes 3x big

**Setup**: the overall pipeline would look like:

1. ==search module==: input dialog context, **generate a relevant search query** for internet search engine (Bing)

2. ==knowledge module==: input the dialog context + returned documents (intersecting with Common Crawl and take top 5) and **generate their most relevant portion** to the context (i.e. extracting the useful portion)

   - for GPT based backbone, just append

   - for T5 based backbone, do a *fusion-in-decoder* which is essentially processing Question + Each Passage in parallel by an encoder, then concatenate the hidden encoder states, and feed into decoder (the model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-Decoder)

     <img src="dialogs/image-20221008145447935.png" alt="image-20221008145447935" style="zoom:80%;" />

     note that this style was also used a lot in Multi-Modal (visual-semantic grounding) work

3. ==response module==: input context and the extracted knowledge, and **generate a response**.

**Models**:

- in addition to GPT and T5, they additionally trained an encoder-decoder model from scratch, and called it **SeeKeR**
- specifically, it is pretrained on Reddit as well as LM tasks used in RoBERTa and CC100en

**Training**: two trainable tasks are proposed:

- Tasks for dialog

  - **search module task**: using Wizard of Internet which has relevant search queries as labels, train supervised
  - **knowledge module tasks**: need to extract knowledge from documents. Therefore, knowledge grounded dialog datasets with gold knowledge annotations are used, as well as QA datasets.
  - **response module tasks**: can reuse much dataset in the knowledge task by using context + gold knowledge response (and their special tokens) to generate the gold label response

- Tasks for LM: improve language ability for each component, hence they are based on Common Crawl and is large in size. Also, this will be *directly training for the prompt completion task*, which is essentially LM

  - **search module tasks**: predict document titles from document
  - **knowledge module task**: constructed a dataset where document contain the retrieved sentence in addition to the document, and the task is to get the retrieved sentence (i.e. extract only the portion that is relevant to the question)
  - **response module task**: input context plus the knowledge sentence and target is next sentence, using the same dset as above

  ![image-20221008150843877](dialogs/image-20221008150843877.png)

**Evaluation**

- compare against models such as BlenderBot 1,2, etc

- **Automatic Evaluation**: can be done using Knowledge F1 (overlap of the dialog response with annotated gold knowledge)

  - seems that only with gold knowledge response the model is working
  - ==TODO== knowledge F1 ignores the semantic, and a lot of discrepancy with the human evaluation?

- **Human Evaluation**: since this is e2e, it can converse with MT. Then for each turn in a conversation, they are asked to score several attributes from how knowledgeable it is to its engagingness.

  |                          Automatic                           |                            Human                             |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20221008151133688](dialogs/image-20221008151133688.png) | ![image-20221008151125898](dialogs/image-20221008151125898.png) |

- **Ablation Study**:

  - done on the pretraining objectives with those additional dialog datasets

  - testing if the additional LM task is helpful or not

    ![image-20221008151408879](dialogs/image-20221008151408879.png)

    ==TODO== it seems that with LM it is better, so are the modules compared the one with LM or the "standard SeeKeR"?

  - tested if separating each task to a different module would help. It does marginally but 3x the size

- **Prompt Completion**: in addition to conversing, it can perform prompt (part of a factual statement) and complete it with factual information

  - used topical prompts, ranges from Prime Minister of Haiti to the Rio Carnival
  - prompts look like `In recent developments we have learned the following about <TOPIC>` and then ask the model to continue
  - evaluation is done by human as well

- **Effect of Multi-Task Training**

  - Prompt Completion: a fully multi-tasked SeeKeR model performs very well, superior to all our GPT2-based SeeKeR models on every metric
  - Open-Domain Dialog: The model performs comparably, if not better, in all automated metrics on the task. In human evaluations, results suffer compared to the dialogue fine-tuned only model, with most metrics being lower

# Mixed Social and Task-Oriented Dialogue Systems

There is a goal, such as donation, but to be **effective social strategies are required** (e.g. to persuade, might need personal stories)

## INSPIRED: Toward Sociable Recommendation Dialog Systems

> **Aim**: Lack of dataset annotated with sociable strategies, but want to validate **whether sociable recommendation strategies are effective** for making a successful recommendation (which is task-oriented)
>
> - design an annotation scheme related to recommendation strategies based on social science theories
> - analyze and show that strategies such as sharing personal opinions or communicating with encouragement more frequently lead to successful recommendations

**Dataset**: the `Inspired` dataset

- since we are doing recommendation, they **curate a database with movies** and make sure they include movie trailers and metadata information
- then a dataset is constructed by MT to
  - as a **recommender**: gather preference information and make recommendation
  - as a **seeker**: looks of recommendation, and gets to watch the trailer at the end (used as an indicator of task success)
- additional collection details
  - first fill out personality traits
  - perform the conversation
  - perform a post-task survey of demographic questions
  - Seeker asked to rate the recommendation and get a chance to skip or watch the trailer
- **strategy annotation**
  - divide the recommendation strategy into two categories
    - **sociable strategies:** eight strategies related to recommendation task to build rapport with seeker
      - e.g. personal opinion, personal experience, similarity, encouragement, etc.
      - ==what they want to test==, if this is effective in task-oriented setting
    - **preference elicitation**: to know the seeker's taste directly
      - e.g. experience inquiry, opinion inquiry
    - (non-strategy: other utterances such as greeting)
  - first ask expert to label, then use those to evaluate MT's labels and get **consistent ones based on Kappa agreement**
- **recommendation success annotation**: success if defined if seekers finished watching a substantial portion (50%) of the recommended movie trailer and rate the trailer with a high score

**Results**

- found that ==social strategies does correlate to the probability of successful recommendation==

  <img src="dialogs/image-20221014201122513.png" alt="image-20221014201122513" style="zoom:67%;" />

- examine if the **quality of the movie matters more than recommendation**

  - ==TODO==: "*adding movie attributes* such as genre, recent release data have an impact on successful recommendation" how is this related to the question? I imagine something like measuring the correlation between quality of the movie v.s. successful recommendation and failed ones.
  - found that 96% of recommended movies are covered by the top five genres

**Modeling**: recommendation dialog system

- evaluate and show that using the strategies in this `Inspired ` dataset can create a **better recommendation system**

- **baseline** dialog model uses two separate pretrained language models to learn the recommender and the seeker **separately**

  ![image-20221014201740089](dialogs/image-20221014201740089.png)

  additionally, key terms such as movie names and actor names are delexicalized to terms such as  `[Movie_Title_0]` for later replacement

  ![image-20221014201838409](dialogs/image-20221014201838409.png)

- **strategy-incorporated model**: generate both the sentences but also strategies

  - The model first generates five candidate sentences. Then, it randomly selects a generated candidate that either contains encouragement strategy or has the greatest sentence length
  - so the model is *only* either doing encouragement or any other DA but has is the longest = i.e. **prioritize encouragement, then longest**

- use human evaluation to see which system is better (or can't tell which one is better) in terms of fluency, naturalness, persuasion ...

  ![image-20221014202221999](dialogs/image-20221014202221999.png)

  ==TODO== why, if you only prioritize encouragement, you can better fluency, consistency, naturalness? How much is this due to selecting long sentences?

## Effects of Persuasive Dialogs: Testing Bot Identities and Inquiry Strategies

> **Aim**: investigate how **identities** (if you know the bot is actually a bot) and **inquiry strategies** (ask personal related questions v.s. task related questions) influence the conversation's effectiveness. Specifically, it is measured by the performance on `Persuasion4Good` dataset.

**Hypothesis**

1. Hypothesis 1: Both identities (whether if you think it is chatbot or human) yield **equivalent persuasive and interpersonal outcomes**.
2. Hypothesis 2: **Personal inquires will yield greater persuasive** and interpersonal outcomes than non-personal inquiries.
3. Hypothesis 3: There is an interaction effect between **chatbot identity and persuasive inquiry type** on persuasive and interpersonal outcomes.
   - We speculate that if the chatbot tries to interact in a personal and human-like way (e.g., by asking personal questions), people may feel uncomfortable, which can subsequently degrade the interpersonal perceptions of the partner as well as their persuasiveness
   - e.g. if chatbot perceived as human + doing personal inquiry makes it more persuasive

**Setup**

- use `Persuasion4Good` dataset
- categorized the 10 persuasion strategies into two groups
  - **persuasive appeals:** do persuasion such as *emotional appeal*
  - **persuasive inquires:** ask questions to facilitate persuasion. This is further split into
    - Non-personal Inquiry refers to relevant questions without asking personal information. It include two sub-categories: 1) source-related inquiry that asks if the persuadee is aware of the organization, and 2) task-related inquiry that asks the persuadees opinion and experience related to the donation task.
    - Personal Inquiry asks about persuadees personal information relevant to donation for charity but not directly on the task, such as Do you have kids?

**Dialog System**

- use **agenda-based dialog system**, meaning that the flow/intent of what to say is more or less predefined

  <img src="dialogs/image-20221014203710682.png" alt="image-20221014203710682" style="zoom:80%;" />

- the overall model thus contains three main components

  - **NLU**: input user utterance and output user dialog act
    - a hard task and hence enhanced the input with sentiment score, context information output from CNN and pretrained character embedding, got around 62%
    - ==TODO== uses a regular expression and pre-defined rules to reach 84.1%. What happened here?
  - **Dialogue Manager**: outputs the next system dialog act, but follows the agenda shown above
    - the first green block is the control experiment: want to measure if having each of the four strategy would affect overall persuasion = ==if personal inquires yield more persuasiveness==
    - then, the system proceeds to persuasive appeal, where a **fixed strategy order**  is used
    - finally, ==when user dialog act is `agree-donation`== (predicted by NLU), enter `Agree Donation` stage and always present the three task in the order: ask donation amount, propose more donation, ...
    - in addition, a factual QA component is there in case if user asked fact related questions related to the charity
  - **Natural Language Generation**: there are three ways to generating: a) template base, b) retrieval-based from the training dataset and c) generation
    - ==template-based for Persuasive Inquiry:== we want to study the effects of different persuasive inquiries **instead of the impact of the surface-form**; therefore, the surface-forms of the persuasive inquiries should be a controlled variable that stays the same across experiments.
    - **retrieval-based persuasive appeal**: want to also be templated based but now you have a large context, hence that doesn't work anymore

**Evaluation**

- designed 2x4 cases
  - 2 cases of having bot being labelled as `Jessie` or `Jessie (bot)`
  - 4 cases of each of the persuasive inquiry strategy: personal + non-personal inquiry vs. personal inquiry vs. nonpersonal inquiry vs. no inquiries

- first found not much difference in which strategy used overall

  ![image-20221014204957608](dialogs/image-20221014204957608.png)

  but then they found that participants are **perceiving bots are humans or vice versa despite the given label**

- found that if bots are ==perceived as human==, then the above it matters

  ![image-20221014205113538](dialogs/image-20221014205113538.png)

  this shows that being a human is more persuasive hence:

  - ==nullifies Hypothesis 1==, which claims being perceived as human or machine does not matter
  - ==supports Hypothesis 2==, that using personal inquires help

- finally, we can check cases when the person believed in the given labelled identity

  ![image-20221014205642372](dialogs/image-20221014205642372.png)

  where this means that if bot's identity does not matter, then in each pair of blue and green bar it should be same height:

  - when participants talked to Jessie (bot) but perceiving it as a human, they were also more likely to donate than those in the same condition but perceiving it as a bot. In contrast, when participants talked to Jessie but suspected it was a bot, they were ==least likely to make a donation, which supported the UVM in Hypothesis 3==
  - also, result showed that Jessie (bot) would decrease the donation probability ($\beta$=0.52,  $p$<0.05). So the **bots identity matters in the persuasion outcome, which again disproves Hypothesis 1**]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[6998: Conversational AI]]></summary></entry><entry><title type="html">STFCS234 Reinforcement Learning</title><link href="/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/" rel="alternate" type="text/html" title="STFCS234 Reinforcement Learning" /><published>2022-09-30T00:00:00-04:00</published><updated>2022-09-30T00:00:00-04:00</updated><id>/lectures/2022@columbia/STFCS234_Reinforcement_Learning</id><content type="html" xml:base="/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/"><![CDATA[Stanford Reinforcement Learning: 

- course video: https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=1
- assignments: https://github.com/Huixxi/CS234-Reinforcement-Learning-Winter-2019
- slides: https://web.stanford.edu/class/cs234/CS234Win2019/schedule.html

# Introductions

Reinforcement Learning involves

- **Optimization:** find an optimal way to make decisions
- **Delayed consequences:** Decisions now can impact things much later
  - When planning: decisions involve reasoning about not just immediate benefit of a decision but also its longer term ramifications
  - When learning: temporal credit assignment is hard (what caused later high or low rewards?)
- **Exploration:** Learning about the world by making decisions and trying
  - censored data: Only get a reward (label) for decision made. i.e. only have one reality
  - in a sense that we need to collect our own training data
- **Generalization**
  - learn a policy which can do interpolation/extrapolation, i.e. handle cases when not met in training data

Examples:

|                          |                       Example/Comment                        |                    Strategies Involved                     |
| ------------------------ | :----------------------------------------------------------: | :--------------------------------------------------------: |
| Go as RL                 | <img src="stf_rl/image-20220516211946230.png" alt="image-20220516211946230" style="zoom:50%;" /> | Optimization<br />Delayed Consequences<br />Generalization |
| Supervised ML ad RL      |                                                              |              Optimization<br />Generalization              |
| Imitation Learning as RL | Learns from experience...of others; <br />Assumes input demos of good policies<br />Reduces RL to supervised learning | Optimization<br />Delayed Consequences<br />Generalization |

# Intro to Sequential Decision Making

The problem basically looks like

<img src="stf_rl/image-20220516213345977.png" alt="image-20220516213345977" style="zoom: 67%;" />

- **Goal**: Select actions to maximize total ==expected== future ==reward== 

- May require balancing immediate & long term rewards

Note that it is critical to pick a good reward function. Consider the following RL system:

- Agent: AI Teacher
- Action: pick a addition or subtraction problem for student to do
- World: some student doing the problem
- Observation: whether if student did it correctly or not
- Reward: +1 if correct, -1 if not

What would happen in this case? The agent could learn to **give easy problems**.

---

A more formal formulation of the task looks like

<img src="stf_rl/image-20220516214443813.png" alt="image-20220516214443813" style="zoom:67%;" />

where we have:

- at each time $t$
  - agent takes some **action** $a_t$
  - world updates given action $a_t$, emits **observation** $o_t$ and **reward** $r_t$
  - agent receives observation $o_t$ and reward $r_t$
- therefore, we essentially have a **history** $h_t=(a_1,o_1,r_1,...,a_t,o_t,r_t)$
- **State** is information assumed to determine what happens next
  - usually the agent uses (some function of) the history to determine what happens next, so $s_t = f(h_t)$
  - This is true state of the world is hidden from agent

## Markov Assumption and MDP

Why is Markov assumption so popular? It turns out that such an assumption can **always be satisfied** if we set state as history:
$$
p(s_{t+1}|s_t, a_t) = p(s_{t+1}|h_t,a_t)
$$
where RHS is like the true model, and LHS is what we are modelling.

However, in practice we often assume the **most recent observation** being sufficient to model the state
$$
p(s_{t+1}|s_t, a_t) = p(s_{t+1}|o_t,a_t)
$$
So that:

- If we consider most recent observation as your state, so $s_t = o_t$, then **the agent is modelling the world as MDP**

- If agent state is not the same as the world state (e.g. Use history $s_t = h_t$ , or beliefs of world state, or RNN), then it is a **Partially Observable MDP**, or POMDP
  - e.g. playing a poker game, where agent only sees its own card

Or other state representation (e.g. past 4 states). This choice affects **how big our state space is**, which has big implications for: 

- Computational complexity 
- Data required 
- Resulting performance

---

An example of this would be:

- a state include all history, e.g. our entire life trajectory = have only a single data sample
- a state include only most recent life experience = a lot of data samples

## Types of Sequential Decision Process

In reality, we may face different types of problems, each with different properties:

- **Bandits**: actions have no influence on next observations
  - e.g. whether if I clicked on the advertisement does not affect who the next customer (coming to the website) is
- **MDP and POMDP**: Actions influence future observations
- **Deterministic World**: given the same state and action, we will have the same/only one possible observation and reward
  - Common assumption in robotics and controls
- **Stochastic**:  given the same state and action, we can have multiple possible observation and reward
  - Common assumption for customers, patients, hard to model domains
  - can think of the case that we don't have good enough model for deterministic coin flipping, hence we model it as stochastic

## Example of MDP

Consider the case of having a Mars Rover exploring, which has seven possible states/locations to explore. We start at state $s_4$

<img src="stf_rl/image-20220516222302501.png" alt="image-20220516222302501" style="zoom:50%;" />

so we have

- **States**: Location of rover $(s_1, ...,s_7)$
- **Actions**: Left or Right 
- **Rewards**: $+1$ in state $s_1$,  $10$ in state $s_7$,  $0$ in all other states

We want to find a program that tells the rover what to do next. In general RL problems can be modeled in three ways

- **Model**: Representation of how the ==world== changes in response to agents action 

  - in this case we need to model transition
    $$
    p(s_{t+1}=s' | s_t = s,a_t=a)
    $$
    for any action state combination

  - and the reward
    $$
    r(s_t=s,a_t=a) \equiv \mathbb{E}[r_t|s_t =s,a_t=a]
    $$
    for any action state combination

- **Policy**: function mapping agents states to action (i.e. what to do next given current state)

  - hence we model, if deterministic
    $$
    \pi(s)=a
    $$
    spits out an action given a state

  - if stochastic
    $$
    \pi(a|s) = P(a_t=a|s_t=s)
    $$
    being a probability distribution given a state.

- **Value function**: ==Future rewards== from being in a state and/or action when following a particular policy

  - we consider the ==expected== discounted sum of future rewards **under/given a particular policy $\pi$**
    $$
    V_\pi(s_t=s) = \mathbb{E}_\pi [r_t + \gamma r_{t+1} + \gamma^2 r_{t+2}+...|s_t=s]
    $$
    for $\gamma \in [0,1)$ specifying how much we care about the future reward compared to current reward.

## Types of RL Agents

Therefore, from the above discussion, we essentially have two types of RL algorithms:

- **Model-based**
  - explicitly having a model of the world (e.g. models the transition function, reward)
  - may or may not have an explicit policy and/or value function (e.g. if needed compute from the model)
  - no longer needs interaction/additonal experience
- **Model-free**
  - explicitly have a value function and/or policy function
  - no model of the world

A more complete diagram would be

<img src="stf_rl/image-20220516224048782.png" alt="image-20220516224048782" style="zoom: 67%;" />

## Key Challenges in Making Decisions

- **Planning** (Agents internal computation)
  - Given model of how the world works 
    - i.e. you already know the transition and rewards
  - need an algorithm to compute how to act in order to maximize expected reward 
    - With no interaction with real environment 
- **Reinforcement learning** (we don't even have the model)
  - Agent doesnt know how world works 
  - Interacts with world to implicitly/explicitly learn how world works 
  - Agent improves policy (may involve planning)

so we see that RL deals with more problems: we also need to decide what action to do for a) getting the necessary information of the world, and b) achieve high future rewards

---

*For instance*

- Planning: Chess game
  - we already know all the possible moves, and rewards (who wins/losses)
  - we need an algorithm to tell us what to do next based on this model
    - doing a tree search, etc.
- Reinforcement Learning: Chess game with no rule book, i.e. don't know the rule of chess
  - first we need to directly learn by taking actions and see what happens
  - Try to find a good policy over time

## Important Components in RL

Agent **only experiences** what happens for the actions it tries. How should an RL agent balance its actions? 

- **Exploration**: trying new things that might enable the agent to make better decisions in the future 
- **Exploitation**: choosing actions that are expected to yield good reward given past experience 

Often there may be an exploration-exploitation tradeoff, as you might not the correct model of the world (since you only have finite experience). May have to sacrifice reward in order to explore & learn about potentially better policy

---

Another two very important component you often see in RL is:

- **Evaluation**: given some policy, we want to evaluate how good the policy is

- **Control**: find the good policy
  - e.g. do policy evaluation, and improve (iff the policy is stochastic)
  - so does more than evaluation

# MDPs and MRPs

Here we discuss how do we decide to take actions when **given a world model**. So here we will first discuss the problem of **planning**, instead of reinforcement learning (discussed later).

Specifically, we will cover

- Markov Decision Processes (MDP)
- Markov Reward Processes (MRP)
- Evaluation and Control in MDPs

Therefore, we basically consider

<img src="stf_rl/image-20220517132606977.png" alt="image-20220517132606977" style="zoom: 50%;" />

so mathematically we consider
$$
p(s_{t+1}|s_t, a_t) = p(s_{t+1}|o_t, a_t)
$$

## Markov Process/Chain

Before we discuss MDP, we first consider what is MP.

> **Markov Process**
>
> A Markov process has:
>
> - a set of finite states $s \in S$
>
> - a dynamics/transition model $P$ that specifies $p(s_{t+1}=s' | s_t=s)$. For a finite number of states, this can be modelled as
>   $$
>   P = \begin{bmatrix}
>   P(s_1|s_1) & P(s_2|s_1) & \dots & P(s_N|s_1)\\
>   P(s_1|s_2) & P(s_2|s_2) & \dots & P(s_N|s_2)\\
>   \vdots & \vdots & \ddots & \vdots\\
>   P(s_1|s_N) & P(s_2|s_N) & \dots & P(s_N|s_N)\\
>   \end{bmatrix}
>   $$
>
> notice that we have no action nor rewards: so we basically just observe this process being a **memoryless random process**

For instance, we can consider Mar's Rover having the following transition dynamics

|                         Markov World                         |                       Transition Model                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="stf_rl/image-20220517133727555.png" alt="image-20220517133727555" style="zoom: 50%;" /> | <img src="stf_rl/image-20220517134058349.png" alt="image-20220517134058349" style="zoom:50%;" /> |

where we see that $p(s_1|s_1)=0.6$, and $p(s_2|s_1)=0.4$, etc.

Using the transition model we can also mathematically compute the probability distribution of the next state given current state. For instance, if we are currently at $s_t=s_1$, then $p(s_{t+1}|s_t)$ is modelled by
$$
p(s_{t+1}|s_t=s_1) = [1, 0, 0,0,0,0,0]P = \begin{bmatrix}
0.6\\
0.4\\
0\\
\vdots\\
0
\end{bmatrix}
$$
Then, a sequence of states you would observe from the world would be sampled from the above distribution, so in the end you only see some deterministic observations/**episodes** such as

- $s_4 \to s_5 \to s_6 \to s_7 \to s_7, ...$
- $s_4 \to s_5 \to s_4 \to s_5 \to s_7, ...$
- $s_4 \to s_3 \to s_2 \to s_1 \to s_2, ...$
- etc.

## Markov Reward Process

Markov Reward Process is essentially Markov Chain + Reward

> **Markov Reward Process**: 
>
> Markov reward process involves
>
> - a set of finite states $s \in S$
> - a dynamics/transition model $P$ that specifies $p(s_{t+1}=s' | s_t=s)$. 
>   - for a finite number of states, this can be expressed as a matrix.
> - a **reward function** $R(s_t=s) \equiv \mathbb{E}[r_t | s_t=s]$, meaning the reward at a state is the expected reward of that state
>   - for a finite number of states, this can be expressed as a vector.
> - allow for a discount factor $\gamma \in [0,1]$
>   - mainly for mathematical convenient, which can avoid infinite returns and values/converges
>   - if episodes are finite, then $\gamma = 1$ works
>
> note that we still have no actions.

Once there is a reward, we can consider ideas such as returns and value functions

> **Horizon**: Number of time steps in each episode
>
> - Can be infinite
> - if not, it is called finite Markov reward process

> **(MRP) Return**: discounted sum of rewards from time step $t$ to Horizon
> $$
> G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...
> $$
> note that 
>
> - this is like a reward for a particular episode starting from time $t$
> - why geometric series? This is usually used for its nice mathematical properties.

> **(MRP) State Value Function $V(s)$**: expected return from starting in state $s$
> $$
> V(s) = \mathbb{E}[G_t|s_t=s] = \mathbb{E}[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... | s_t=s]
> $$

note that 

- if the process is deterministic, then $\mathbb{E}[G_t|s_t=s]=G_t$
- if the process is stochastic, then it is usually different.

---

*For instance*, consider the previous example of

|                         Markov World                         |                       Transition Model                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="stf_rl/image-20220517133727555.png" alt="image-20220517133727555" style="zoom: 50%;" /> | <img src="stf_rl/image-20220517134058349.png" alt="image-20220517134058349" style="zoom:50%;" /> |

but now we have:

- $R(s=s_1)=+1$, and $R(s=s_7)=+10$. Zero otherwise

Then consider sampling 2 episodes with 4-step length, using $\gamma=1/2$ we can compute the **sample return**

- $s_4,s_5,s_6,s_7$ having a return of $0+\frac{1}{2}\times 0+\frac{1}{4}\times 0+\frac{1}{8}\times 10=1.25$
- $s_4, s_4, s_5, s_4$ having a return of $0+\frac{1}{2}\times 0+\frac{1}{4}\times 0+\frac{1}{8}\times 0=0$

Finally, we can consider the state value function by averaging over all the possible trajectories for each state, and we would get
$$
V= [1.53, 0.37, 0.13, 0.22, 0.85, 3.59, 15.31]
$$
in this particular example. 

---

How do we ==compute== the state function in reality?

- Could estimate by **simulation** (i.e. generate a large number of episodes and take average)

  - notice that this method assumes no Markov process, as we are just sampling and averaging
  - we have theoretical bounds as well on how many episodes we need

- Or we can **utilize the Markov structure** and know that MRP value function satisfies
  $$
  V(s) = \underbrace{R(s)}_{\text{imemdiate reward}}+ \quad \underbrace{\gamma \sum_{s'\in S} P(s'|s)V(s')}_{\text{discounted sum of future rewards}}
  $$

Of course for computation we will use the latter case, which brings us to

> **Bellman's Equation for MRP**: for fintie state MRP, we can express $V(s)$ for each state using a matrix equation
> $$
> \begin{bmatrix}
> V(s_1)\\
> \vdots\\
> V(s_N)
> \end{bmatrix} = \begin{bmatrix}
> R(s_1)\\
> \vdots\\
> R(s_N)
> \end{bmatrix} + \gamma \begin{bmatrix}
> P(s_1|s_1) & P(s_2|s_1) & \dots & P(s_N|s_1)\\
> P(s_1|s_2) & P(s_2|s_2) & \dots & P(s_N|s_2)\\
> \vdots & \vdots & \ddots & \vdots\\
> P(s_1|s_N) & P(s_2|s_N) & \dots & P(s_N|s_N)\\
> \end{bmatrix}\begin{bmatrix}
> V(s_1)\\
> \vdots\\
> V(s_N)
> \end{bmatrix}
> $$
> or more compactly
> $$
> V = R + \gamma PV
> $$

Note that both $R, P$ is known. All we need is to ==solve for $V$==. This can be solved in two ways:

- directly with liner algebra
- iterative using DP

First, solving it with linear algebra
$$
\begin{align*}
V &= R + \gamma PV\\
V - \gamma PV &= R\\
V &= (I-\gamma P)^{-1}R
\end{align*}
$$
which requires solving matrix inverses, hence is $\sim O(N^3)$.

Another way to compute $V$ is by dynamic programming:

1. initialize $V_0(s)=0$ for all state $s$

2. For $k=1$ until convergence

   - for all $s \in S$

   $$
   V_k(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V_{k-1}(s')
   $$

As this is an iterative algorithm, the cost is $O(kN^2)$ for having $k$ iterations (so each iteration updates is only $O(N^2)$)

## Markov Decision Process

Finally we add in action as well, so essentially MDPs are Markov Reward Process + actions

> **Markov Decision Process**: MDP involves
>
> - a set of finite states $s \in S$
> - a finite set of actions $a \in A$
> - a dynamics/transition model $P$ ==for each action== that specifies $p(s_{t+1}=s' | s_t=s, a_t=a)$. 
> - a reward function $R(s_t=s,a_t=a) \equiv \mathbb{E}[r_t | s_t=s,a_t=a]$
> - allow for a discount factor $\gamma \in [0,1]$
>
> note that know we mostly deal with the "joint probability" of $s_t,a_t$ together.

For instance, for the Mars Rover MDP case

|                         World Model                          |                       Transition Model                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220517144202574](stf_rl/image-20220517144202574.png) | ![image-20220517144210423](stf_rl/image-20220517144210423.png) |

where in this case we are having deterministic actions, which nonetheless can be modelled as a stochastic transition with probability of one.

Once we have actions in our model, we also have the following

> **Policy**: specifies what action to take in each state
> $$
> \pi (a|s) = P(a_t=a|s_t=s)
> $$
> Basically a conditional distribution of actions on current state

Note that once you ==specified a policy==, you can ==convert MDP + policy to a Markov Reward Process==, so that
$$
R^\pi (s) = \sum_{a \in A} \pi(a|s)R(s,a)
$$
is again independent of action in MRP, and similarly
$$
P^\pi(s'|s) = \sum_{a\in A}\pi(a|s) P(s'|s,a)
$$
which implies we can use **same techniques to evaluate the value of a policy for a MDP** as we could to compute the value of a MRP

### MDP Policy Evaluation

For a **deterministic policy $\pi(s)$**, we can evaluate the value function $V^\pi(s)$ by, for example, the iterative algorithm mentioned before

1. initialize $V_0(s)=0$ for all state $s$

2. For $k=1$ until convergence

   - for all $s \in S$

   $$
   V^\pi_k(s) = r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')
   $$

   so essentially $V_k^\pi(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$, i.e. the value function if we are allowed to act for $k$ steps. Therefore, as $k$ increases we converge to infinite horizon.

> This is also called the **Bellman backup** for a particular (deterministic) policy.
>
> - note that of course we could have also computed $V^\pi$ analytically, or with simulation.

*For instance*, consider the setup of Mars Rover

<img src="stf_rl/image-20220517144202574.png" alt="image-20220517144202574" style="zoom:50%;" />

And in this case we have:

- Dynamics: $p(s_6 | s_6,a_1)=0.5, p(s_7 | s_6,a_1)=0.5, ...$
- Reward: for all actions, +1 in state $s_1$, and +10 in state $s_7$. Zero otherwise
- Policy: $\pi(s)=a_1$ for all states.
- $\gamma$ set to $0.5$

Let we initialize with $V^\pi_k=[1,0,0,0,0,0,0,10]$, we want to compute $V^\pi_{k+1}(s_6)$. From the iterative formula
$$
\begin{align*}
V^\pi_{k+1}(s_6) &= r(s_6, a_1) + \gamma [0.5 * V_k(s_6) + 0.5 * V_k(s_7)]\\
&= 0 + 0.5*[0.5 * 0 + 0.5 * 10]\\
&= 2.5
\end{align*}
$$

> Notice that we have **propagated** the reward information from $s_7$ to $s_6$ in this value function! If you do this for all states eventually such an information will be spread in all states.

### MDP Control

Ultimately we want our agent to **find** an optimal policy
$$
\pi^*(s) = \arg\max_\pi V^\pi(s)
$$
i.e. policy such that its value function is the maximum. Meaning that

> A policy $\pi$ is defined to be better than or equal to a policy $\pi'$ if 
> $$
> \pi \ge \pi' \iff V_\pi(s) \ge V_{\pi'}(s),\quad \forall s
> $$
> which means its expected return is greater than or equal to that of $\pi'$ **for all states**. And there is ==always at least one policy that is better== than or equal to all other policies as a policy is essentially a mapping.

Then, it turns out that

> **Theorem**:  MDP with infinite horizon:
>
> - there exists a *unique optimal value function*
> - the *optimal policy* for a MDP in an infinite horizon problem is
>   - *deterministic*
>   - *stationary*: does not depend on time step. (intuition would be that for infinite horizon, you have essentially infinite time/visits for each state, hence the optimal policy does not depend on time)
>   - *not necessarily unique*

Therefore, it suffices for us to focus on (improving) deterministic policies as our final optimal policy is also deterministic

---

*For example*: Consider the Mars Rover case again

<img src="stf_rl/image-20220517151127572.png" alt="image-20220517151127572" style="zoom: 50%;" />

 which have 7 states and 2 actions, $a_1, a_2$. How many deterministic policies are there?

- since a policy is essentially a mapping from states to actions, there are $2^7$ possible policies.

---

So how do we search for the ==best policy==?

- **enumeration**: compute for all $|A|^{|S|}$ possible deterministic policy and pick best one
- **policy iteration**: which is more efficient by doing policy evaluation + improvement

### MDP Policy Iteration

The goal is to improve a policy iteratively so we end up with an optimal policy:

1. set $i=0$
2. initialize $\pi_0(s)$ randomly for all $s$
3. while $i==0$ or $||\pi_i - \pi_{i-1}||_1 > 0$ being the L1-norm
   1. $V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$
   2. $\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)
   3. $i = i+1$

How do we improve a policy? First need to consider some new definition

> **State-Action Value Function**: essentially a value function exploring what happens if you take some action $a$ (e.g. different than some given policy) in each state
> $$
> Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')
> $$
> so essentially at each state $s$, we consider:
>
> - take action $a$
> - then follow policy $\pi$

Therefore, using this we can essentially improve a policy $\pi_i$ by:

1. compute the state-action value of a policy $\pi_i$

   - for each state $s\in S$ and $a \in A$

   $$
   Q^{\pi_i}(s,a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s')
   $$

2. compute the ==new policy $\pi_{i+1}$ by==
   $$
   \pi_{i+1}(s) = \arg\max_a Q^{\pi_i}(s,a),\quad \forall s\in S
   $$
   to prove that this is a better policy, we need to show that
   $$
   V^{\pi_{i+1}}(s) \ge V^{\pi_i}(s)
   $$
   which we will prove below. (inequality becomes strict if $V^{\pi_i}$ is suboptimal)

> Using this approach, we are **guaranteed** to arrive at the global optimum of best policy.

*Proof*: Monotonic Improvement in Policy.

We first know that:
$$
\max_a Q^{\pi_i}(s,a) \ge V^{\pi_i}(s)=r(s,\pi_i(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi_i(s))V^{\pi_i}(s')
$$
by definition of choosing a $\max_a$. Then, notice that
$$
\begin{align*}
V^{\pi_i}(s) 
&\le \max_a Q^{\pi_i}(s,a)\\
&= \max_a \{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \}\\
&= R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))V^{\pi_i}(s')\\
&\le R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))\left( \max_{a'} Q^{\pi_i}(s',a') \right)\\\\
&= R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))\left[ R(s',\pi_{i+1}(s')) + \gamma \sum_{s'' \in S} P(s''|s',\pi_{i+1}(s'))V^{\pi_i}(s'') \right]\\
&\le \dots\\
&= V^{\pi_{i+1}}(s)
\end{align*}
$$
where:

- notice that by definition of $Q^{\pi_{i}}$, we are **using $\pi_i$ for future steps** on the second and third equality

- the third equality comes from the fact that we know $\pi_{i+1}(s) = \arg\max_a Q^{\pi_i}(s,a)$ which spits out the action for maximizing $Q$

- the fifth equality is basically doing the same as doing everything from the first to the third equality
- hence essentially we are expanding and pushing $\pi_{i+1}$ to a future step, ==until we are using $\pi_{i+1}$ for all steps==

> Note that: this is an **monotonic improvement**
>
> - this mean that if the policy didn't change on one iteration, can it change in future iteration? No, because if taking the max cannot improve it, then it have reached global optimum.
> - Is there are maximum number of policy iteration? Yes, because there is only $|A|^{|S|}$ number of policies, and since improvement step is monotonic, each policy can only appear once (unless we reached optimal)

### MDP Value Iteration

Another approach to find an optimal policy is by **value iteration**.

- policy iteration: for each policy $\pi_i$, we get the value $V^{\pi_i}$ for the infinite horizon and improve it, until obtained best $V^{\pi^*}$
- value iteration: maintain the best $V(s)$ up to $k$ number of steps left in the episode, until $k \to \infty$ or converges

so we are computing a different thing here, but the final answer will be the same.

> *Recall* that value of a policy ==has to satisfy== the Bellman equation
> $$
> \begin{align*}
> V^\pi(s) 
> &= R^\pi(s) + \gamma \sum_{s' \in S} P^\pi(s'|s)V^\pi(s')\\
> &=R(s,\pi(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi(s))V^\pi(s')
> \end{align*}
> $$
> was the definition

Then we can consider a Bellman backup operator which **operates on a value function**:
$$
\mathbb{B}V(s) \equiv \max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V(s') \}
$$
so basically:

- $\mathbb{B}$ is like an operator, which will spit out a new value function
- it will improve the value if possible, as this is basically what policy iteration did

---

Then, with this, we define the algorithm for value iteration:

1. set $k=1$

2. Initialize $V_0(s)=0$ for all state $s$

3. loop until [finite horizon, convergence]

   - for each state $s$
     $$
     V_{k+1}(s) = \max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V_k(s') \}
     $$
     which can be views as just a Bellman backup operation on $V_k$
     $$
     V_{k+1} = \mathbb{B}V_k
     $$

4. then policy for acting $k+1$ steps (best policy) can be easily derived by the action that leads to the best state given current state
   $$
   \pi_{k+1}(s) = \arg\max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V_k(s') \}
   $$
   so basically considering best action if only act for $k=1$ step, the use this to compute $k=2$ steps, and etc.

How do we know that this converges? This is because Bellman's backup operator is a [contraction operator](#Contraction Operator) (if $\gamma \le 1$)

> Difference between Policy and Value iteration
>
> - **Value iteration**
>   - Compute optimal value as if horizon $= k $ steps
>     - Note this can be used to compute optimal policy if horizon $= k$, i.e. finite horizon
>   - Increment $k$
> - **Policy iteration**
>   - Compute infinite horizon value of a policy
>   - Use to select another (better) policy 
>   - Closely related to a very popular method in RL: policy gradient

#### Policy Iteration as Bellman Operations

Essentially policy iteration also derives from the Bellman's constraint, so we can express the policy iteration as Bellman operations as well.

First, we consider Bellman backup operator $\mathbb{B}^\pi$ for a **particular policy** which operates **on some value function** (which could have a different policy):
$$
\mathbb{B}^\pi  V(s) = R^\pi(s) + \gamma \sum_{s'\in S} P^\pi(s'|s)V(s')
$$
Therefore, this means that:

1. policy evaluation of $\pi_i$ is basically doing:
   $$
   V^{\pi_i} = B^{\pi_i}B^{\pi_i}...B^{\pi_i}V
   $$
   for some randomly initialized $V$

2. policy improvement
   $$
   \pi_{k+1}(s) = \arg\max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V^{\pi_k}(s') \}
   $$

#### Contraction Operator

> **Contraction Operator**: 
>
> Let $O$ be an operator, and $|x|$ denote any norm of $x$. If
> $$
> |OV-OV'| \le |V-V'|
> $$
> then $O$ is an contraction operator.

so basically

- distance between two value functions after applying Bellman's operator must be less than or equal to distance they had before.

- Given this property, it is straightforward to argue that distance between some value function $V$ to the optimal value function $V^*$ will decrease monotonically, hence convergence.

*Proof*

<img src="stf_rl/image-20220517171411902.png" alt="image-20220517171411902" style="zoom: 80%;" />

# Policy Evaluation with Unknown World

Before in the [MDPs and MRPs](#MDPs and MRPs) section we discussed the problem of planning: **if we know how the world works**, i.e. have some transition/reward function known, **how do we find out the best policy**.

However, what if we **do not have a world model** to begin with? This is what we will discuss in this section, including:

- Monte Carlo policy evaluation
- Temporal Difference (TD)

## Monte Carlo Policy Evaluation

First, recall that with a world model, our algorithm for **policy evaluation** is:

1. initialize $V_0(s)=0$ for all state $s$

2. For $k=1$ until convergence

   - for all $s \in S$

   $$
   V^\pi_k(s) = r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')
   $$

   so essentially $V_k^\pi(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$, i.e. the value function if we are allowed to act for $k$ steps. Therefore, as $k$ increases we converge to infinite horizon.

In other words, we are iteratively **estimating $V^\pi(s)$** by $V_k^\pi(s)$ as:
$$
V^\pi(s) = \mathbb{E}_\pi[G_t|s_t=s] \approx V_k^\pi(s) = \mathbb{E}_\pi[r_t + \gamma V_{k-1}|s_t=s]
$$
we can think update tule graphhically as:
$$
V^\pi(s) \leftarrow \mathbb{E}_\pi[r_t + \gamma V_{k-1}|s_t=s]
$$

| Tree of Possible Trajectories Following a Stochastic $\pi(a|s)$ |                Dynamic Programming Algorithm                 |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="stf_rl/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" /> | <img src="stf_rl/image-20220518174109832.png" alt="image-20220518174109832" style="zoom: 50%;" /> |

where notice that:

- essentially we are taking $V_{k-1}^\pi(s)$ for each state as the "true value functions", and then computing the highlighted expectation by
  $$
  V^\pi(s) \leftarrow r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')
  $$
  to update the value functions. Notice that for this to make sense we need to ==assume infinite horizon==, since we are treating $V^\pi(s)$ being stationary for each state, i.e. not a function of time step.

- therefore we are bootstrapping as we are taking the value function from previous iteration.

- notice that to compute this update we needed $P(s'|s,a)$ to **average over all possible futures** (and $R(s,a)$ as well)

---

Now, what happens if we ==do not know the world/model==? 

**In the framework of MC Policy Evaluation**, the idea is to notice that
$$
V^\pi(s) = \mathbb{E}_{T \sim \pi}[G_t | s_t=s]
$$
basically averaging the $G_t$ of each possible trajectory $T$ following $\pi$, i.e. **average over the branches of the tree**

<img src="stf_rl/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" />

therefore, if trajectories are all finite, **sample set of trajectories & average returns** would give us some approximation of the value function. Hence, properties related to this approach include:

- Does not require a known MDP dynamics/rewards, as we **just need sample trajectories as its reward**
- Does not assume state is Markov (e.g. no notion of next state, etc.)
- Can only be applied to **episodic MDPs**
  - Requires each episode to terminate

> **Aim**: estimate $V^\pi(s)$ given sampled episodes $(s_1,a_1,r_1,s_2,a_2,r_2,...)$ generated under policy $\pi$

Using this **idea of MC policy evaluation**, we have the following algorithms that can do this:

- first visit MC on Policy Evaluation
- every visit MC on Policy Evaluation
- incremental MC on Policy Evaluation

### First Visit MC On Policy Evaluation

Our aim is to estimate $V^\pi(s)$ by sampling trajectories and taking their mean (as being the MC method). In this case we consider the following algorithm:

1. Initialize $N(s)=0$, $G(s)=0, \forall s \in S$ 

2. loop

   1. sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},....,s_{i,T_i})$

   2. define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards
      $$
      G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}
      $$

   3. for each state $s$ visited in the episode $i$

      1. for the ==first time== $t$ that state $s$ is visited
         1. increment counter of total first visits for that state $N(s)=N(s)+1$
         2. increment total return $G(s)=G(s)+G_{i,t}$
         3. **Update estimate** $V^\pi(s)=G(s)/N(s)$

How does this algorithm work? How do we know that this way of estimating $V^\pi(s)$ being good (i.e. bias, variance, and consistent)?

> **Theorem**:
>
> - $V^\pi$ estimator in this case is an **unbiased** estimator of the true $\mathbb{E}_\pi[G_t|s_t=s]$
> - $V^\pi$ estimator in this case is also a **consistent** estimator so that as $N(s)\to \infty$, $V^\pi(s)\to \mathbb{E}_\pi[G_t|s_t=s]$

However, those this is a good news, it might not be efficient as we are throwing away many other visits to the same state, which brings us to [Every Visit MC On Policy Evaluation](#Every Visit MC On Policy Evaluation)

#### Bias, Variance and MSE Recap

Recall that a way to see if certain estimators work is to **compare against the ground truth**. Let $\hat{\theta}=f(x)$ be our estimator function (a function of the observed data $x$) for some true parameter $\theta$. 

- so basically $x \sim P(x|\theta)$ being generated from the true parameter
- we are constructing an estimator $\hat{\theta}=f(x)$ to estimate such a parameter. 
  - e.g. for $x \sim N(\mu, \sigma)$, we can estimate $\mu$ by $\hat{\mu} = \text{mean}(x)$ being our estimator

> **Bias**: The bias of an estimator $\hat{\theta}$ is defined by
> $$
> \text{Bias}_\theta(\hat{\theta}) \equiv  \mathbb{E}_{x|\theta}[\hat{\theta}] - \theta
> $$
> so that over many sampled datasets, how far is the average $\hat{\theta}$ from the true $\theta$

note that since we don't know what $\theta$ is in reality, we usually just bound it.

> **Variance**: the variance of an estimator $\hat{\theta}$ is defined by:
> $$
> \text{Var}(\hat{\theta}) \equiv  \mathbb{E}_{x|\theta}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])]
> $$
> which is basically the definition of variance itself and has nothing to do with $\theta$

In general different algorithms will have different trade-off between bias and variance.

> **MSE**: Mean squared error of an estimator $\hat{\theta}$ is
> $$
> \text{MSE}_\theta(\hat{\theta}) \equiv \text{Var}(\hat{\theta})+\text{Bias}_\theta(\hat{\theta})^2
> $$

### Every Visit MC On Policy Evaluation

Here, the change is small

1. Initialize $N(s)=0$, $G(s)=0, \forall s \in S$ 

2. loop

   1. sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},....,s_{i,T_i})$

   2. define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards
      $$
      G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}
      $$

   3. for each state $s$ visited in the episode $i$

      1. for the ==every time== $t$ that state $s$ is visited
         1. increment counter of total first visits for that state $N(s)=N(s)+1$
         2. increment total return $G(s)=G(s)+G_{i,t}$
         3. **Update estimate** $V^\pi(s)=G(s)/N(s)$

Although this is more data efficient as we performed more updates

> **Theorem**:
>
> - $V^\pi$ estimator in this case is an **biased** estimator of the true $\mathbb{E}_\pi[G_t|s_t=s]$
>   - the intuition here is that because each state in an episode is *not IID*, the $G_{i,t}$ is *not IID* either. Therefore, we will get a biased estimator in this case as we use all encounters of state $s$
> - $V^\pi$ estimator in this case is a **consistent** estimator so that as $N(s)\to \infty$, $V^\pi(s)\to \mathbb{E}_\pi[G_t|s_t=s]$
> - Empirically, $V^\pi$ has a lower variance

### Incremental MC On Policy Evaluation

In both previous algorithms we had to update the mean incrementally by 

1. first doing $N(s)=N(s)+1$ 

2. increment total return $G(s)=G(s)+G_{i,t}$

3. then update
   $$
   V^\pi(s)=\frac{G(s)}{N(s)}
   $$

4. 

We can also perform the **same update incrementally** by:

1. first doing $N(s)=N(s)+1$ 

2. update
   $$
   V^\pi(s)=V^\pi(s)\frac{N(s)-1}{N(s)} + \frac{G_{i,t}}{N(s)}=V^\pi(s)+\frac{G_{i,t}-V^\pi(s)}{N(s)}
   $$
   for basically ==adding a "correction term"== of $(G_{i,t}-V^\pi(s))/N(s)$

this idea of a correction term will be used later in TD algorithms, but in practice we can tweak this term so that, if we consider the following algorithm:

1. Initialize $N(s)=0$, $G(s)=0, \forall s \in S$ 

2. loop

   1. sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},....,s_{i,T_i})$

   2. define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards
      $$
      G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}
      $$

   3. for each state $s$ visited in the episode $i$

      1. for the ==every time== $t$ that state $s$ is visited

         1. increment counter of total first visits for that state $N(s)=N(s)+1$

         2. update
            $$
            V^\pi(s)=V^\pi(s)+\alpha\left( G_{i,t}-V^\pi(s) \right)
            $$

Then if we used:

- $\alpha = 1/N(s)$ then it is the same as every visit MC
- $\alpha > 1/N(s)$ then we are placing **emphasis on more recent ones/updates $ G_{i,t}-V^\pi(s)$**.
  - this could be helpful if domains are non-stationary. For example in robotics some parts can be breaking down over time, so we want to focus more on more recently learnt data

### MC On Policy Example

Consider a simple case of a Mars Rover again:

- reward being $R=[1,0,0,0,0,0,0,10]$
- initialize $V^\pi(s)=0$ for all $s$
- Our policy is $\pi(s)=a_1,\forall s$.
- Any action from state $s_1$ or $s_7$ gives termination
- take $\gamma = 1$

Suppose then we got a trajectory from this policy:

- $T = (s_3,a_1,0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text{{terminal}})$

**Question**: What is the first and every visit MC estimate of $V^\pi$?

- first visit of $V^\pi(s)=[1,1,1,0,0,0,0]$
- every visit is the same even though $s_2$ has two updates

> Notice that for all MC methods, we only was able to **perform updates until the termination of the episode**, since we need to know $G_{i,t}$.

### MC On Policy Evaluation Key Limitations

Graphically, what MC algorithms are doing is

| Tree of Possible Trajectories Following a Stochastic $\pi(a|s)$ |                    MC On Policy Algorithm                    |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="stf_rl/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" /> | <img src="stf_rl/image-20220518215716060.png" alt="image-20220518215716060" style="zoom:50%;" /> |

So notice that we are averaging across all trajectories, meaning that:

- **Generally high variance estimator**. Reducing variance can require a lot of data
- **Requires episodic settings** (since we can only update once episode terminated)

## Temporal Difference Learning

> "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning."  Sutton and Barto 2017

Some key attributes of this algorithm is:

- **Combination** of Monte Carlo & dynamic programming methods
  - therefore, it Bootstraps (dynamic programming, reusing $V_{k-1}^\pi(s)$) and samples (MC)
- **Model-free** (does not need to know the transition/reward functions)
- Can be used in episodic **or infinite-horizon** non-episodic settings
- Can **immediately updates** estimate of $V$ after each $(s,a,r,s')$ tuple, i.e. we do not need to wait until the end-of-episode like MC methods.

> **Aim**: estimate $V^\pi(s)$ given sampled episodes $(s_1,a_1,r_1,s_2,a_2,r_2,...)$ generated under policy $\pi$

this is the ==same== as MC on Policy Evaluation. But recall that if we are having a MDP model, the **Bellman Operator can be used to perform an update** of the $V(s)$:
$$
\mathbb{B}^\pi  V(s) = R^\pi(s) + \gamma \sum_{s'\in S} P^\pi(s'|s)V(s')
$$
which basically ==takes the current reward== and ==averages over the value of next state==. From this, we consider

> **Insight**: given some current estimate of $V^\pi$, we can update using
> $$
> V^\pi(s_t) =V^\pi(s_t)+\alpha( \underbrace{[r_t+\gamma V^\pi(s_{t+1})]}_{\text{TD target}}-V^\pi(s_t))
> $$
> for $r_t+\gamma V^\pi(s_{t+1})$ basically is our target (to approximate $r_t+\gamma \sum P^\pi(s'|s)V(s')$) to improve $V^\pi(s)$, as compared to the MC On-policy method which used $G_{i,t}$. 
>
> Notice that:
>
> - this target is basically also ==takes the current reward== and ==looks at the value of next state==. 
>
> - we are bootstrapping again, instead of waiting until end of episode, we use previous estimate $V^\pi(s)$ to compute our target $r_t+\gamma V^\pi(s_{t+1})$ and updates ==at each time step==. Therefore we also don't need episodic requirement.

> **TD Error**: we can also view the above update rule as basically correcting $V^\pi(s)$ by the error term:
> $$
> \delta_t \equiv [r_t+\gamma V^\pi(s_{t+1})]-V^\pi(s_t)
> $$

Under this framework of TD learning, we also have some variations:

- TD(0) Learning: only boostrap
- TD($\lambda$) Learning: MC update for $\lambda$ steps and then bootstrap

hence technically we have a continuum of algorithm between using bootstrap and using MC algorithm.

### TD(0) Learning

The simplest TD algorithm is TD(0):

1. input $\alpha$

2. initialize $V^\pi(s)=0$, $\forall s \in S$

3. loop

   1. sample tuple $(s_t,a_t,r_t,s_{t+1})$

   2. update the state:
      $$
      V^\pi(s_t) =V^\pi(s_t)+\alpha( \underbrace{[r_t+\gamma V^\pi(s_{t+1})]}_{\text{TD target}}-V^\pi(s_t))
      $$

4. 

which is a **combination** of MC sampling and bootstrapping:

- we needed sampling to get $s'$ for evaluating $V^\pi(s_{t+1})$
- we used the previous estimate of $V^\pi$ to calculate the values, which is boostrapping

### TD(0) Learning Example

Consider a simple case of a Mars Rover again:

- reward being $R=[1,0,0,0,0,0,0,10]$
- initialize $V^\pi(s)=0$ for all $s$
- Our policy is $\pi(s)=a_1,\forall s$.
- Any action from state $s_1$ or $s_7$ gives termination
- take $\gamma = 1$

Suppose then we got a trajectory from this policy:

- $T = (s_3,a_1,0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text{{terminal}})$

**Question**: What is the first and every visit MC estimate of $V^\pi$?

- first visit of $V^\pi(s)=[1,1,1,0,0,0,0]$
- every visit is the same even though $s_2$ has two updates

**Question**: What is the TD estimate of all states (init at 0, single pass) if we use $\alpha = 1$:

- $[1,0,0,0,0,0,0]$. Notice that here we have forgotten the previous history/**did not propagate** the information of reward at $s_1$ to other states in a single episode.
- to propagate this information to $s_2$, we need **another episode** which had the tuple $s_2 \to s_1$. To propagate to $s_3$, we need yet another episode that contains $s_3 \to s_2$, etc. So this ==propagation is slow==

### TD Learning Key Limitations

Graphically, temporal difference considers

|                    MC On Policy Algorithm                    |                  TD(0) On Policy Algorithm                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="stf_rl/image-20220518215716060.png" alt="image-20220518215716060" style="zoom:50%;" /> | <img src="stf_rl/image-20220518225606223.png" alt="image-20220518225606223" style="zoom:50%;" /> |

so we notice that TD(0):

- sits between MC and DP because we are still sampling stuff, but we are updating in a DP fashion

Although this algorithm allows us to update quickly:

- it is a **biased** estimator, since our update rules are using bootstrapping (i.e. $V^\pi(s)$ we used are estimates of the true $V^\pi(s)$, hence it will just be biased)

## Comparison Between DP,MC and TD

|                                                  |  DP  |  MC  |  TD  |
| ------------------------------------------------ | :--: | :--: | :--: |
| Usable when no models of current domain          |      | Yes  | Yes  |
| Handles continuing (non-episodic) domains        | Yes  |      | Yes  |
| Handles Non-Markovian domains                    |      | Yes  |      |
| Converges to true value in limit (assume Markov) | Yes  | Yes  | Yes  |
| Unbiased estimate of value                       | N/A  | Yes  |      |

note that

- for DP, it is the exact estimate.
- unbiased = for finite amount of data, on average it is $\theta$; consistent = for infinite amount of data it is $\theta$

> Note that here we are still living in the world of **tabular state space**, i.e. our state space is discrete. Once we move on to the case of having **continuous state space**, we will cover **function approximation** methods and a lot of them ==does not== guarantee convergence.

In addition:

- **MC: updates until end of episode**
  - Unbiased 
  - High variance 
  - Consistent (converges to true) even with function approximation
- **TD: updates per sample point $(s,a,r,s')$ immediately**
  - Some bias 
  - Lower variance 
  - TD(0) converges to true value with tabular representation
  - TD(0) does not always converge with function approximation

## Batch MC and TD

The aim is to use the data more efficiently, hence we might consider:

> **Batch (Offline) solution** for finite dataset:
>
> - Given set of $K$ episodes
> - Repeatedly sample an episode from $K$ (since we are offline, we can replay the episodes)
> - Apply MC or TD(0) to the sampled episode
>
> The question is, ==what will MC and TD(0) converge to==?

Consider the following example:

- Two states $A, B$ with $\gamma = 1$ and a small $\alpha < 1$
- Given 8 episodes of experience: 
  - $A, 0, B, 0 $
  - $B, 1$ (observed 6 times) 
  - $B, 0$

Graphically we have

<img src="stf_rl/image-20220518232255338.png" alt="image-20220518232255338" style="zoom: 67%;" />

**Question**: what is $V(A), V(B)$ under MC and TD(0) if we are sampling those data for infinite number of times until convergence?

- $V(B)=6/8$ for MC: because in general for 6 out of 8 episodes we get $B\to 1$ in the trajectory
- $V(B)=6/8$ for TD(0): because in general 6 out of 8 times we get $B \to \text{Terminate}$ we will have a reward of 1
- $V(A)=0$ for MC: in MC setting because updates are done per episode, and the only episode that contained state $A$ had a reward of zero
- $V(A)\neq 0$ for TD(0): in TD settings reward propagate back across episodes. Hence once $V(B)\neq 0$ and we sampled a tuple $A\to B$, we will obtain a non-zero value for $V(A)$

### Properties of Batch MD and TD

- Monte Carlo in batch setting 
  - converges to min MSE (mean squared error) Minimize loss with respect to observed returns
  - MC can be more data efficient than simple TD if markov assumption does not hold (i.e. we need to sample data in TD in order of the episode)

- TD(0) in batch setting
  - converges to DP policy V  for the MDP with the maximum likelihood model estimates
  - TD exploits Markov structure, if it holds then it is very data efficient

# Model Free Control

How does an agent **learn to act** when it does not know how the world works, and does not aim to construct a model (model-free).

- Previous section: ==Policy evaluation== with no knowledge of how the world works = how good is a specific policy?
- This section: ==Control (making decisions)== without a model of how the world works = how can we **learn a good policy**?

Many applications can be modeled as a MDP: 

- Backgammon, Go, Robot locomation, Helicopter flight, Robocup soccer, Autonomous driving, etc.

For many of these and other problems either: 

- MDP model is unknown but can be sampled 
- MDP model is known but it is computationally infeasible to use directly, except through sampling (e.g. climate simulation)

> **Optimization Goal**: identify a policy with high expected rewards (without model of the world). Certain features we will encounter like all RL algorithms:
>
> - **Delayed consequences:** May take many time steps to evaluate whether an earlier decision was good or not
> - **Exploration:** Necessary to try different actions to learn what actions can lead to high rewards

In this section, we will discuss two types of algorithms:

- **On-policy Learning**
  - as we had for policy evaluation
  - Direct experience 
  - Learn to estimate and evaluate a policy from experience obtained from following that policy
- **Off-policy Learning**
  - Learn to estimate and evaluate a policy using experience gathered from following a **different** policy
  - e.g. given history $s_1,a_1,s_1,a_1$ and $s_1,a_2,s_1,a_2$, be able to **extrapolate** what happens if you do $s_1,a_1,s_1,a_2$

## MC On-Policy Policy Iteration

Recall that **when we know the model**, we had the following algorithm for control:

1. set $i=0$

2. initialize $\pi_0(s)$ randomly for all $s$

3. while $i==0$ or $||\pi_i - \pi_{i-1}||_1 > 0$ being the L1-norm

   1. $V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$

   2. $\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)
      $$
      \pi_{i+1}(s) =  \arg\max_a \left\{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \right\} = \arg\max_a Q^{\pi_i}(s,a)
      $$

   which monotonically improves (strict inequality) the policy until optimal policy

> Now, we want to do the above two steps without access to the true dynamics and reward models. Notice that essentially policy improvement does $\arg \max_a Q^\pi$, so we want to find a way to ==estimate $Q^\pi$ directly without knowing world==.
>
> - previously we have only discussed how to do policy evaluation $V^\pi$ without the world, but ==not== how to improve it

Therefore, we consider the following **model-free policy iteration framework**:

1. initialize $\pi_0(s)$ randomly for all $s$

2. repeat

   1. policy evaluation: **compute $Q^{\pi_i}(s,a)$**

   2. policy improvement: update $\pi_{i+1}$ by:
      $$
      \pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)
      $$

But how do we estimate $Q^\pi$?

- MC for On Policy Q Evaluation (first visit and every visit)

### MC for On Policy Q Evaluation

The idea is basically the same as $V^\pi$ evaluation, where we consider:

1. Initialize $N(s,a)=0$, $G(s,a)=0, Q^\pi(s,a)=0,\forall s \in S, \forall a \in A$ 

   - in contrast to $V^\pi$ evaluation where we had $N(s)=0, G(s)=0$, etc.

2. loop

   1. sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},....,s_{i,T_i})$

   2. define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards
      $$
      G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}
      $$

   3. for each **state-action pair** $(s,a)$ visited in the episode $i$

      1. for the ==first time or every time== $t$ that the pair $(s,a)$ is visited
         1. increment counter of total first visits for that state $N(s,a)=N(s,a)+1$
         2. increment total return $G(s,a)=G(s,a)+G_{i,t}$
         3. **Update estimate** $Q^\pi(s,a)=G(s,a)/N(s,a)$

> Notice that a ==problem== with this algorithm: we can only evaluate $Q^\pi(s,a)$ for **state-action pairs that we have experienced**. And if $\pi$ is deterministic, we can't compute $Q^\pi(s,a)$ for any $a\neq \pi(s)$. This means that we cannot say anything about new actions. 
>
> The same problem goes with improvement, $\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)$ since we have initialized $Q^\pi(s,a)=0,\forall a \in A,\forall s \in S$.
>
> - one solution to deal with this is optimistic initialization so we initialize a high $Q^\pi(s,a)$ to promote ==exploration==

This means that our policy improvement will only look at actions we have taken, i.e. it ==will not explore new state-action pairs== so that 
$$
\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)
$$
will only end up choosing actions visited by $s,\pi(s)$.

> This means we may need to **modify the policy evaluation algorithm** to include non-deterministic state-action pairs for $Q^\pi(s,a)$

### Policy Evaluation with Exploration

Recall that the definition of $Q^\pi(s,a)$ was:
$$
Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')
$$
so to **find a good evaluation** of $Q^\pi(s,a)$, we need to:

- ==explore== every possible action $a$ from state $s$, to record $R(s,a)$
- after that, follow policy $\pi$ to get $\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')$

> **Insight**: since we need to explore every possible action, consider the simple idea to balance exploration and exploitation
> $$
> \pi(a|s) = \begin{cases}
> \pi(s),& p=1-\epsilon\\
> a \in A, & p = \epsilon / |A|
> \end{cases}
> $$
> which is called the ==epsilon-greedy policy== **w.r.t a deterministic policy $\pi(s)$.**

This also means we can also define $\epsilon$-greedy policy **w.r.t. a state-action value $Q(s,a)$**
$$
\pi(a|s) = \begin{cases}
\arg\max_a Q(s,a),& p=1-\epsilon\\
a \in A, & p = \epsilon / |A|
\end{cases}
$$
so that you will see we can use this to change the update rule we had, which is $\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)$.

---

*For Example*: Mars Rovers again, with 7 states:

- Now we specify rewards for both actions as we need to compute $Q(s,a)$
  $$
  r(\cdot, a_1) = [1,0,0,0,0,0,10]\\
  r(\cdot, a_2) = [0,0,0,0,0,0,5]
  $$

- assume current greedy policy $\pi(s)=a_1$
- take $\gamma =1, \epsilon = 0.5$

Then, **using $\epsilon$-greedy w.r.t to $\pi(s)$**, we got a sampled trajectory of
$$
(s_3,a_1,0,s_2,a_2,0,s_3,a_1,0,   s_2,a_2,0,   s_1,a_1,1, \text{terminal})
$$
**Question**: What is the first visit MC estimate of $Q$ of each $(s,a)$ pair?

- $Q^{\epsilon-\pi}(\cdot,a_1)=[1,0,1,0,0,0,0]$ since we are doing MC, we propagates the end-of-episode reward to all states
- $Q^{\epsilon-\pi}(\cdot, a_2)=[0,1,0,0,0,0,0]$ same reason as above
- notice that without $\epsilon$-greedy, we would have never got $(s_2,a_2,0)$, hence we would have $Q^{\epsilon-\pi}(\cdot, a_2)=[0,0,0,0,0,0,0]$

### $\epsilon$-greedy Policy Improvement

*Recall* that we previous thought of the following as the framework of Model free PI:

1. set $i=0$

2. initialize $\pi_0(s)$ randomly for all $s$

3. while $i==0$ or $||\pi_i - \pi_{i-1}||_1 > 0$ being the L1-norm

   1. $V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$

   2. $\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)
      $$
      \pi_{i+1}(s) =  \arg\max_a \left\{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \right\} = \arg\max_a Q^{\pi_i}(s,a)
      $$

   which **monotonically** improves (strict inequality) the policy until optimal policy

And we had the **problem** of $Q^\pi(s,a)$ only containing updates for very small set of $(s,a)$ pairs that we followed a deterministic $\pi(s)$. This means that policy improvement will ==not try new actions==.

> Now, the idea is to replace the update step **w.r.t the stochastic $\epsilon$-greedy policy $\pi_i$,** so that we consider from a $Q^{\pi_i}$ that:
> $$
> \pi_{i+1}(a|s) = \begin{cases}
> \arg\max_a Q^{\pi_{i}}(s,a),& p=1-\epsilon\\
> a \in A, & p = \epsilon / |A|
> \end{cases}
> $$
> being the ==new update rule==, which now can cover a wider range of $(s,a)$ pairs hence our updated $\pi_{i+1}$ will also contain ==new actions==.

But does this actually provide a monotonic improvement as well (as in the determinstic case)?

> **Theorem**: For any $\epsilon$-greedy policy $\pi_i$, the $\epsilon$-greedy policy $\pi_{i+1}$ w.r.t $Q^{\pi_i}$ is a **monotonic improvement**, such that $V^{\pi_{i+1}} \ge V^{\pi_i}$
>
> - with this theorem, we can have the [MC On Policy Improvement/Control](#MC On Policy Improvement/Control) algorithm which uses this to provide a better estimate of $Q^\pi(s,a)$ and improve $\pi$

*Proof*: We just need to show that the value for taking this new policy is higher:

<img src="stf_rl/image-20220519162000682.png" alt="image-20220519162000682" style="zoom: 80%;" />

where basically:

- on the fourth equality, we utilize the fact that $\sum_{a}\pi_i(a|s)=1$
- in the last equality, we basically cancelled out the $1-\epsilon$ again and obtained our result
- this of course **assumes that you know exactly $Q^{\pi_i}$ for a policy $\pi_i$**.
  - in case when you have an ==estimate== of $Q^{\pi_i}$, then there is ==no guarantee== of monotonic policy improvement. This will happen a lot in function approximation when we have a continuous state space.

This shows that by following $\pi_{i+1}$ the first step and then following $\pi_i$, we get a higher value. Then, we can show $V^{\pi_{i+1}} \ge V^{\pi_i}$ in the similar fashion as we proved the policy improvement by pushing out the $\pi_{i+1}$ policy to future terms.

---

Finally, we need guarantees on convergence

> **GLIE **(Greedy in the Limit of Infinite Exploration):
>
> - all state-action pairs are visitied an infintie number of times
>   $$
>   \lim_{i\to \infty} N_i(s,a) = \infty
>   $$
>   (satisfied as we have stochastic policy)
>
> - Behavior policy (the one we used to sample trajectory) **converges to greedy policy** (deterministic)
>   $$
>   \lim_{i \to \infty} \pi(a|s) \to \arg\max Q^\pi(s,a)
>   $$

A practice, instead of having $i \to \infty$, we can also reduce $\epsilon$ to have $\epsilon_i = 1/i$ to also satisfy GLIE (over time we have $0$ prob for exploration).

> **Theorem**: GLIE Monte-Carlo control converges to the optimal state-action value function
> $$
> Q(s,a) \to Q^*(s,a)
> $$
> from which we can find the optimal policy easily by taking $\arg\max_a Q^*(s,a)$

### MC On Policy Improvement/Control

Finally, putting everything together, we have the MC On-policy Policy Improvement algorithm as:

<img src="stf_rl/image-20220519164409454.png" alt="image-20220519164409454" style="zoom: 60%;" />

which basically combines:

- [MC for On Policy Q Evaluation](#MC for On Policy Q Evaluation)

- [$\epsilon$-greedy Policy Improvement](#$\epsilon$-greedy Policy Improvement)

- and $\epsilon$-Greedy$(Q)$ means that
  $$
  \pi_{k}(a|s) = \begin{cases}
  \arg\max_a Q(s,a),& p=1-\epsilon\\
  a \in A, & p = \epsilon / |A|
  \end{cases}
  $$

---

*For Example*: Mars Rovers again, with 7 states:

- Now we specify rewards for both actions as we need to compute $Q(s,a)$
  $$
  r(\cdot, a_1) = [1,0,0,0,0,0,10]\\
  r(\cdot, a_2) = [0,0,0,0,0,0,5]
  $$

- assume current greedy policy $\pi(s)=a_1$
- take $\gamma =1, \epsilon = 0.5$

Then, **using $\epsilon$-greedy w.r.t to $\pi(s)$**, we got a sampled trajectory of
$$
(s_3,a_1,0,s_2,a_2,0,s_3,a_1,0,   s_2,a_2,0,   s_1,a_1,1, \text{terminal})
$$
So we know the first visit MC estimate of $Q$ of each $(s,a)$ pair gives:

- $Q^{\epsilon-\pi}(\cdot,a_1)=[1,0,1,0,0,0,0]$ since we are doing MC, we propagates the end-of-episode reward to all states
- $Q^{\epsilon-\pi}(\cdot, a_2)=[0,1,0,0,0,0,0]$ same reason as above

now, given the policy evaluation:

**Question**: what is the greedy policy $\pi(s)$ w.r.t this $Q^{\epsilon - \pi}$?

- simply $\pi(s) = [a_1,a_2,a_1,\text{tie},\text{tie},\text{tie},\text{tie}]$

**Question**: therefore, what is the new improved policy $\pi_{k+1}=\epsilon\text{-greedy}(Q)$ if $k=3$?

- this means $\epsilon=1/3$

- then, from the above result, this means
  $$
  \pi_{k+1}(a|s) = \begin{cases}
  [a_1,a_2,a_1,\text{tie},\text{tie},\text{tie},\text{tie}], & p=2/3\\
  a \in A, & p = 1/6
  \end{cases}
  $$

## TD On-Policy Policy Iteration

*Recall that* the general framework for policy iteration in a model-free case

1. initialize $\pi_0(a|s)$ randomly for all $s$
2. repeat
   1. policy **evaluation**: compute $Q^{\pi_i}(s,a)$
   2. policy **improvement**: update $\pi_{i+1}$ using $Q^{\pi_i}$

> Recall that in the section [Policy Evaluation with Unknown World](#Policy Evaluation with Unknown World), we had **two ways of doing policy evaluation**, either doing MC or doing TD. We have come up with a way to compute $Q^\pi(s,a)$ using MC method, therefore, another variant is to use TD method.

Hence, the general algorithm for TD based On-Policy Policy Iteration looks like

1. initialize $\pi_0(a|s)$ randomly for all $s$

2. repeat

   1. policy **evaluation ==using TD==**: compute $Q^{\pi_i}(s,a)$

   2. policy **improvement (same as MC)**: update $\pi_{i+1}$ using $Q^{\pi_i}$ by
      $$
      \pi_{i+1} = \epsilon\text{-Greedy}(Q^{\pi_i})
      $$

### SARSA

Essentially using TD for policy evaluation:

<img src="stf_rl/image-20220519171606359.png" alt="image-20220519171606359" style="zoom:67%;" />

where:

- the key change is the update rule
  $$
  Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma Q(s_{t+1},a_{t+1})]}_{\text{TD target}}-Q(s_t,a_t))
  $$
  which can perform update per sample instead of waiting until end-of-episode (in MC case). 

- therefore, notice that the policy are updated much more frequently as well (as compared to the MC case)

- the $\alpha$ used is also often called the ==learning rate== in this context

> Note that we discarded the notation of $Q^\pi$ here and used $Q$, as we are frequently changing the policy when computing this running estimate to converge to $Q^*(s,a)$

### Convergence Properties of SARSA

> **Theorem**: SARSA for finite-state and finite-action MDPs **converges** to the optimal action-value, $Q(s,a)\to Q^*(s,a)$ ,under the following conditions:
>
> - the policy sequence $\pi_t(a|s)$ satisfies GLIE
>
> - the step size/learning rate $\alpha_t$ satisfy the Robbins-Munro sequence such that
>   $$
>   \sum_{t=1}^\infty \alpha_t =\infty\\
>   \sum_{t=1}^\infty \alpha_t^2 < \infty
>   $$
>   an example would be $\alpha_t = 1/t$

But as the above are conditions **sufficient** to guarantee convergence, in reality

- we could also **simply use $\alpha$ being some small constants** and empirically they often converge as well
- there are also some domains that is **hard to satisfy GLIE** (e.g. helicopter crashed, can no longer visit some other states). Some research has been working on how to deal with that, but for now we will not worry about it.

### Q-Learning

The only change will be in SARSA
$$
Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma Q(s_{t+1},a_{t+1})]}_{\text{TD target}}-Q(s_t,a_t))
$$
but **for Q-learning we consider** the update rule:
$$
Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma \max_{a'} Q(s_{t+1},a')]}_{\text{TD target}}-Q(s_t,a_t))
$$
so that

- instead of taking $a_{t+1}\sim \pi(a|s_{t+1})$, which is being ==realistic== (in SARSA), we are being ==optimistic== so that we update our $Q(s_t,a_t)$ by the **best value we could possibly get** using $ \max_{a'} Q(s_{t+1},a')$

- this means that in cases where we have lots of negative reward/risks in **early stages** (e.g. cliff walking), SARSA would produce better policy whereas Q-Learning would produce more risky policy

---

Hence the entire algorithm looks like

<img src="stf_rl/image-20220519175251084.png" alt="image-20220519175251084" style="zoom:67%;" />

where notice that:

- we no longer need the $a_{t+1}$ in the five tuple $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$ in SARSA, since we will be taking the next best action
- since we are being optimistic, does how Q is initialized matter? Asymptotically no, under mild conditions, but at the beginning, yes
- finally, like SARSA it is a policy based on TD, this means that the final reward will only **propagate slowly** to other states.

### Convergence Properties of Q-Learning

What conditions are sufficient to ensure that Q-learning with $\epsilon$-greedy exploration **converges to optimal $Q^*$** ?

- Visit all $(s, a)$ pairs infinitely often
- and the step-sizes t satisfy the Robbins-Munro sequence. 
- Note: the algorithm does not have to be greedy in the limit of infinite exploration (GLIE) to satisfy this (could keep $\epsilon$ large).

What conditions are sufficient to ensure that Q-learning with $\epsilon$-greedy exploration **converges to optimal $\pi^*$ ?** 

- The algorithm is GLIE (i.e. policy being more and more greedy over time), along with the above requirement to ensure the Q value estimates converge to the optimal Q.

### Maximization Bias

Because Q-Learning exploits the greedy action early on, we could have a **maximization bias** when estimating the value of a policy.

Consider the case of a single state MDP, so that $|S|=1$, with two actions. And suppose the two actions both have mean random rewards of zero: $\mathbb{E}[r|a=a_1]=\mathbb{E}[r|a=a_2]=0$

- then this means that $Q(s,a_1)=Q(s,a_2)=0=V(s)$ is the true estimate and is optimal.

In practice, we can only do finite samples taking action $a_1,a_2$. Let $\hat{Q}(s,a_1),\hat{Q}(s,a_2)$ be finite sample estimate of $Q$

- suppose we used an **unbiased estimate of $\hat{Q}(s,a)$** by taking the mean
  $$
  \hat{Q}(s,a_1) = \frac{1}{N(s,a_1)}\sum_{i=1}^{N(s,a_1)}r_i(s,a_1)
  $$

- Let $\hat{\pi}=\arg\max \hat{Q}(s,a)$ be the greedy policy w.r.t the estimated $\hat{Q}$

- then we notice that:
  $$
  \begin{align*}
  \hat{V}^{\hat{\pi}}
  &=\mathbb{E}[\max (\hat{Q}(s,a_1),\hat{Q}(s,a_2))]\\
  &\ge\max(\mathbb{E}[\hat{Q}(s,a_1)],\mathbb{E}[\hat{Q}(s,a_2)]\\
  &= \max[0,0]\\
  &= V^\pi
  \end{align*}
  $$
  meaning we have a **biased estimator** of $V^{\hat{\pi}}$ for the optimal policy.

To solve this, the idea is to instead split samples and use to create two independent unbiased estimates of $Q_1(s_1, a_i)$ and $Q_2(s_1, a_i),\forall a$.

- Use one estimate to select max action: $a^* = \arg\max_a Q_1(s_1,a)$ 
- Use other estimate to estimate value of $a^*:Q_2(s,a^*)$
- Yields unbiased estimate: $\mathbb{E}[Q_2(s,a^*)]=Q(s,a^*)$

This therefore gives birth to the double Q-Learning algorithm

### Double Q-Learning

So the only difference is now we have **two $Q$ estimates:**

<img src="stf_rl/image-20220519181511825.png" alt="image-20220519181511825" style="zoom:67%;" />

where in practice compared to Q-Learning:

- doubles the memory
- same computation requirements
- data requirements are subtle might reduce amount of exploration needed due to lower bias

Additionally

<img src="stf_rl/image-20220519181621394.png" alt="image-20220519181621394" style="zoom:67%;" />

# Value Function Approximation

In the previous section, we discussed how to do **control** (make decisions) **without a known model** of how the world works: 

- learning a good policy from experience (sampled episodes)
- update $Q$ estimate using a ==tabular representation==: ==finite== number of state-action pair

However, as you can imagine many real world problems have **enormous state and/or action space** so that we cannot really tabulate all possible values. So we need to somehow ==generalize== to those unknown state-actions.

> **Aim**: even if we encounter state-action pairs *not* met before, we want to make *good decisions* by past experience.

> **Value Function Approximation**: represent a (state-action/state) value function **with a parametrized function** instead of a table, so that even if we met an inexperienced state/state-action, we can get some values.

So we imagine

<img src="stf_rl/image-20220521111857200.png" alt="image-20220521111857200" style="zoom: 50%;" />

so that essentially we are parameterized the function with weights $W$ as in a deep neural network.

Using such an approach has the following benefits:

- Reduce memory needed to store $(P,R)/V/Q/\pi$
- Reduce computation needed to compute  $(P,R)/V/Q/\pi$
- Reduce experience/data needed to find a good  $(P,R)/V/Q/\pi$

but this will *usually* be a tradeoff:

- **representational capacity** (of the DNN to represent states) v.s. memory/computation/data needed

Then most importantly, we need to consider **what class of functions** do we consider for such an approximation? Many possible function approximators including 

- Linear combinations of features 
- Neural networks 
- Decision trees (useful for being highly interpretable)
- Nearest neighbors 
- Fourier/ wavelet bases

> But here we will focus on function approximators that are **differentiable**, so that we can easily optimize for (e.g. gradient descent). Two popular classes of differentiable function approximators include:
>
> - **linear feature representations** (this section)
> - **neural networks** (next section)
> 
> Also notice that since we are now doing gradient descent, our approximator will be local optimas instead of the global optimas.


## VFA for Policy Evaluation

> **Aim**: find the best approximate representation of $V^\pi$ using a parametrized function $\hat{V}$.

### VFA for Policy Evaluation with Oracle

In this case, we consider:

- given a policy $\pi$ to evaluate
- assume that there is an ==oracle== that returns the **true value for $V^\pi(s)$**

so we basically want our function approximation to look like the true $V^\pi(s)$ in our space. 

Then we can simply perform gradient descent type methods. For instance, 

- **loss function**
  $$
  L(w) = \frac{1}{2} \mathbb{E}_\pi[(V^\pi(s) - \hat{V}(s;w))^2]
  $$
  where $\mathbb{E}_\pi$ means expected value over the **distribution of states** under current policy $\pi$

- **then gradient update is**
  $$
  w:=w - \alpha \left( \nabla_w L(w) \right)
  $$
  and
  $$
  \nabla_w L(w) = \mathbb{E}_\pi[(V^\pi(s)- \hat{V}(s;w))\nabla_w \hat{V}(s)]
  $$

Then, for instance SGD considers using some batched/single sample to approximate the expected value:
$$
\nabla_w L(w) \approx (V^\pi(s)- \hat{V}(s;w))\nabla_w \hat{V}(s)
$$
for some sampled $s,V^\pi(s)$.

---

If we are considering a **Linear Value Function Approximation** with an Oracle, then simply we consider
$$
\hat{V}(s;w) = \vec{x}(s)^T \vec{w}
$$
for $\vec{x}(s)$ being a representation of our state. Then we acn also show the gradient to be:
$$
  \nabla_w L(w) = \mathbb{E}_\pi[(V^\pi(s)- \hat{V}(s;w)) \vec{x}(s)]
$$
since $\nabla_w \hat{V} = \vec{x}(s)$ in this case.
### Model Free VFA Policy Evaluation

Obviously most of the time we do not have an oracle, which is like knowing the model of the world already.

> **Aim**: do model-free value function approximation for prediction/evaluation/policy evaluation **without a model/oracle**
>
> - recall that this means given a fixed policy $\pi$
> - estimate its $V^\pi$ or $Q^\pi$

*Recall* that we have done model-free policy evaluation using tabular methods in:

- [Monte Carlo Policy Evaluation](#Monte Carlo Policy Evaluation): update estimate after each episode
- [Temporal Difference Learning](#Temporal Difference Learning): update estimate after each step

both of which essentially does:

- maintain a look up table to store current estimates $V^\pi$ or $Q^\pi$
- Updated these estimates after each episode (Monte Carlo methods) or after each step (TD methods)

---

> In VFA, we can basically ==change the update step== to be ==fitting the function approximation== (e.g. gradient descent)

This means that we need to have prepared:

- a feature vector to **represent a state $s$**
  $$
  x(s) = \begin{bmatrix}
  x_1(s)\\
  x_2(s)\\
  \dots\\
  x_n(s)
  \end{bmatrix}
  $$
  for instance, for robot navigation, it can be a 180-dimensional vector, with each cell representing the distance to the first detected obstacle. 
  However, notice that this representation also means that it is ==not markov==, as in different hallways (true states) you could have the same feature vector. But it could be still a ==good representation to condition our decision on==.
- choose a class of function approximators to approximate the value function
  - linear function
  - neural networks

#### MC Value Function Approximation

Recall that for MC methods, we used the following update rule for value function updates
$$
V^\pi(s)=V^\pi(s)+\alpha\left( G_{i,t}-V^\pi(s) \right)
$$
so that our ==target is $G_t$==, which is an **unbiased** but noisy estimate of the true value of $V^\pi(s_t)$.
> **Idea**: therere we treat $G_t$ being the "oracle" fo $V^\pi(s_t)$, from which we can get a loss and update our approximator.

This then means our gradient update (for SGD is):
$$
\begin{align*}
\nabla_w L(w) 
&=(V^\pi(s_t)- \hat{V}(s_t;w))\nabla_w \hat{V}(s_t)\\
&\approx (G_t- \hat{V}(s_t;w))\nabla_w \hat{V}(s_t)
\end{align*}
$$
If we are using a linear function:
$$
\nabla_w L(w) \approx (G_t- \vec{x}(s_t)^T \vec{w}) \vec{x}(s_t)
$$
and then just use $\vec{w} := \vec{w} - \alpha \nabla_w L(w)$ to do descent.

> This means that we essentially reduce MC VFA to doing ==supervised learning== on a set of (state,return) pairs: $(s_1, G_1), (s_2, G_2), ..., (s_T, G_t)$


Therefore our algorithm with MC updates looks like

<img src="stf_rl/image-20220521130409230.png" alt="image-20220521130409230" style="zoom: 50%;" />

where notice that:
- of course you can also have an every-visit version by changing line 5
- since we have a finite episode, we used $\gamma=1$ in line 6

---

*For instance*: consider a 7 state space with the transition looking like:

<img src="stf_rl/image-20220521144341630.png" alt="image-20220521144341630" style="zoom:50%;" />

so that:

- state $\vec{x}(s_1) = [2,0,0,0,0,0,1]$ so that $\vec{x}(s_1)^T \vec{w} = 2w_1+w_8$, etc.
- there are two states, $a_1$ being the solid lines and $a_2$ being the dashed lines.
- dashed transition means that here we have a uniform probability $1/6$ to be transitioned to any state $s_i$ for $i \in [1,6]$.
- we assume some small termination probability from state $s_7$, but not shown on the diagram
- all states have zero reward (so the optimal value function is $V^\pi = 0$)

Suppose we have sampled the following episode:
- $s_1, a_1, 0, s_7, a_1, 0, s_7, a_1, 0, \text{terminal}$
- we are using a **linear function**
- we have initialized all weights to one $\vec{w}_0 = [1,1,1,1,1,1,1]$
- $\alpha = 0.5$, take $\gamma = 1$

**Question**: what is the MC estimate of $V^\pi(s_1)$? $\vec{w}$ after one update?

- first we need $G_1$, which in this case is $0$
- our current estimate for $V^\pi(s_1)$ is $\vec{x}(s_1)^T \vec{w}_0 = 2+1=3$
- therefore, our update for one step is:
  $$
  \Delta w = -\alpha \cdot (0 - 3) \cdot \vec{x}(s_1) =-1.5\vec{x}(s_1)=[-3,0,0,0,0,0,-1.5]
  $$
- finally, our new weight is therefore $w + \Delta w$:
  $$
  \vec{w}_1 = [-2,1,1,1,1,1,-0.5]
  $$
  so essentially we are doing SGD per state in the episode.

But does such an update ==converge to the right thing==?

#### Convergence for MC Linear Value Function Approximation

Recall that if provided a policy, then MDP problem is reduced to a Markov Reward Process (by following that policy). Therefore, if we eventually sample many episodes, we get a **probability distribution** over states $d(s)$, such that:
- $d(s)$ is the stationary distribution over states following $\pi$
- then obviously $\sum_s d(s) = 1$

Since it is stationary, this means that the distribution after a single transition gives the same $d(s)$:
$$
d(s') = \sum_s \sum_a \pi(a|s)p(s'|s,a)d(s)
$$
must hold, if the markov process has ran long enough.

Using this distribution, we can consider the ==mean square error== of our estimators for a particular policy $\pi$:

$$
MSVE(\vec{w}) = \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2
$$

for a linear function, we use $\hat{V}^\pi(s;\vec{w}) = \vec{x}(s)^T \vec{w}$.

> **Theorem**: MC policy evaluation with VFA converges to the weights $\vec{w}_{MC}$ which has the ==minimum mean squared error== possible:
> $$
> MSVE(\vec{w}_{MC}) = \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2
> $$
> note that the error might not be zero, e.g. using a linear approximator has only a small capacity.

#### Batch MC Value Function Approximation

The SGD version basically performs an update per sample, which is suitable for online scenario. However, often we could get a set of episodes already sampled from using a policy $\pi$. Then we can perform a better weight update by considering:
$$
\arg\min_\vec{w} \sum_{i=1}^N (G(s_i)- \vec{x}(s_i)^T \vec{w})^2
$$
to approximate the expected value version of the MSVE. Then the optimal weights can be solved **directly** to be:
$$
\vec{w} = (X^TX)^{-1} X^T \vec{G}
$$
for:
- $\vec{G}$ is a vector of all $N$ returns
- $X$ is a matrix of the features of each of the $N$ states $\vec{x}(s_i)$
- but of course this would be memory intensive as we need to store all $N$ states and returns.
- finally, you can obviously have something in between SGD and this full batch.

#### TD Learning with Value Function Approximation

First recall that TD method considers bootstrapping and sampling to approximate $V^\pi$, so that the update rule is based on per sample:
$$
V^\pi(s) = V^\pi(s) + \alpha (r + \gamma V^\pi(s') - V^\pi(s))
$$
with the ==target== being $r+\gamma V^\pi(s')$, which is a biased estimate of the true $V^\pi$.

> **Idea**: use function to represent $V^\pi$, and use boostrapping + sampling in TD method, with the same update rule shown above.

(recall that we are still on-policy, we are evaluating the value of a given policy $\pi$ and all sampled data + estimation are on the same policy)

> Therefore, essentially we now consider TD larning being a supervised learning on the set of data pairs:
> $$
> (s_1, r_1 + \hat{V}^\pi(s_2;\vec{w})), (s_2, r_2 + \hat{V}^\pi(s_3;\vec{w})), ...
> $$
> then the MSE loss is simply:
> $$
> L(\vec{w}) = \mathbb{E}_\pi [ (r_j + \gamma \hat{V}^\pi(s_{j+1},\vec{w})) -\hat{V}(s_j; \vec{w})]
> $$
> Hence our gradient step with a SGD update (replacing mean with a single sample) is:
> $$
> \Delta w = \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \nabla_w \hat{V}^\pi(s;\vec{w})
> $$
> again, with target being $r+\gamma \hat{V}^\pi(s';\vec{w})$

In the case of a linear function, then we have:
$$
\begin{align*}
\Delta w 
&= \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \nabla_w \hat{V}^\pi(s;\vec{w})\\
&= \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \vec{x}(s)\\
&= \alpha \cdot (r + \vec{x}(s')^T \vec{w} - \vec{x}(s)^T \vec{w}) \cdot \vec{x}(s)\\
\end{align*}
$$
so we are boostrapping our target using our current estimate of $V^\pi(s)$.

Finally, the algorithm therefore looks like

<img src="stf_rl/image-20220521160015879.png" alt="image-20220521160015879" style="zoom:50%;" />

---

*For instance*: we can consider the same example with MC case to compare the difference:



<img src="stf_rl/image-20220521144341630.png" alt="image-20220521144341630" style="zoom:50%;" />

so that:

- state $\vec{x}(s_1) = [2,0,0,0,0,0,1]$ so that $\vec{x}(s_1)^T \vec{w} = 2w_1+w_8$, etc.
- there are two states, $a_1$ being the solid lines and $a_2$ being the dashed lines.
- dashed transition means that here we have a uniform probability $1/6$ to be transitioned to any state $s_i$ for $i \in [1,6]$.
- we assume some small termination probability from state $s_7$, but not shown on the diagram
- all states have zero reward (so the optimal value function is $V^\pi = 0$)

Suppose we have sampled the following episode:

- $s_1, a_1, 0, s_7, a_1, 0, s_7, a_1, 0, \text{terminal}$
- we are using a **linear function**
- we have initialized all weights to one $\vec{w}_0 = [1,1,1,1,1,1,1]$
- $\alpha = 0.5$, take $\gamma = 0.9$

**Question**: what is the TD estimate of $V^\pi(s_1)$? $\vec{w}$ after one update of $(s_1,a_1,0,s_7)$?

- using the update formula:
  $$
  \Delta \vec{w} = \alpha (0+ 0.9 * 3 - 3)\vec{x}(s_1) = -0.3\alpha \vec{x}(s_1)
  $$
  which is a much smaller weight update than the MC update.

#### Convergence for TD Linear Value Function Approximation

As mentioned in the MC case, we consider the MSVE for our estimators for a particular policy $\pi$:
$$
MSVE(\vec{w}) = \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2
$$

for a linear function, we use $\hat{V}^\pi(s;\vec{w}) = \vec{x}(s)^T \vec{w}$.

> **Theorem**: TD policy evaluation with VFA converges to the weights $\vec{w}_{TD}$ is ==within a constant factor== of minmum mean squared error possible:
> $$
> MSVE(\vec{w}_{TD}) \le \frac{1}{1-\gamma} \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2
> $$
> so it is slightly worse than MC method as it is biased, but it **updates/converges much faster**.

As mentioned before, this happens also because we are using 
- some feature representation for a state which might be a **subspace** of the true space of states
- in TD we are **bootstrapping**, which gives rise to bias and error

But this also means that if we have some one-hot encoded feature, **one for each state**, then:
$$
\min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2 = 0
$$
as you can have a unique value per state, even with a linear function. Therefore:
- if we used a MC method, then simply:
  $$
  MSVE(\vec{w}_{MC})= \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2 = 0
  $$
- if we used a TD method, then:
  $$
  MSVE(\vec{w}_{TD})\le 0
  $$
  is also optimal, and MC v.s. TD has no difference.

(of course this also means we have enough data)

> If our state representation is a **subspace** of the true state space, then using a TD might incur some error (due to bootstrapping error). But if the state representation is larger or equal to the true state space, then using a TD and MC has no difference.

## Control using VFA

Now for control, essentially we consider moving from policy evaluation (previous sections) to policy iteration. This is basically achieved by:

1. **estimate $Q^\pi(s,a)$** instead, using MC or TD technique 
2. perform **$\epsilon$-greedy policy improvement**

However, this can get unstable because we had the folllowing components:
- function approximation (has uncertainty)
- bootstrapping (has uncertainty)
- **off-policy** learning for policy improvement (has the biggest uncertainty)
  - we are changing the policy over-time, so no longer have a good estimate of the stationary distribution over states of a particular policy

But again, lets first go through the algorithm.

### Action-Value Function Approximation with an Oracle

> **Aim** approximate $Q^\pi(s,a)$ given a policy $\pi$, by $\hat{Q}^\pi(s,a)$

The idea is the same as what we see in [Model Free VFA Policy Evaluation](#Model Free VFA Policy Evaluation). Given an oracle $Q^\pi(s,a)$ which spits out the true value, our loss is simply:
$$
L(\vec{w}) = \frac{1}{2} \mathbb{E}_\pi [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;\vec{w}))^2]
$$

Then for stochastic gradient descent method, the gradient looks like:
$$
\begin{align*}
\nabla_w L(w) 
&= \mathbb{E}_w [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)] \\
&\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
\end{align*}
$$
for some sampled $s,a$ pair.

Finally, the features would then should include both a state and action pair:
$$
\vec{x}(s,a) = \begin{bmatrix}
  x_1(s,a)\\
  x_2(s,a)\\
  \vdots\\
  x_n(s,a)
\end{bmatrix}
$$

---

For a **Linear State Action Value Function Approximation**, we consider simply:
$$
\hat{Q}(s,a;\vec{w}) = \vec{x}^T(s,a)\vec{w}
$$

and the rest is trivial.

### Model Free VFA Control

Similar to how we did [Model Free VFA Policy Evaluation](#Model Free VFA Policy Evaluation), the idea is to approximate the oracle with ==some target==.
- in MC methods, the target is simply $G_t$, so we consider:
  $$
  \begin{align*}
  \nabla_w L(w) 
  &\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
  &\approx (G_t - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
  \end{align*}
  $$

- for SARSA, we used a TD target of $r + \gamma \hat{Q}^\pi(s',a';\vec{w})$ which leverages the current function approximation value to bootstrap:
  $$
  \begin{align*}
  \nabla_w L(w) 
  &\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
  &\approx (r + \gamma \hat{Q}^\pi(s',a';\vec{w})- \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
  \end{align*}
  $$
  and you see for the above update we need $s,a,r,s',a'$
- for Q-Learning, we use the TD target of $r + \gamma \max_{a'}\hat{Q}^\pi(s',a';\vec{w})$, which is optimistic and update gradient is:
  $$
  \begin{align*}
  \nabla_w L(w) 
  &\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
  &\approx (r + \gamma \max_{a'}\hat{Q}^\pi(s',a';\vec{w})- \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
  \end{align*}
  $$
  which we only needs $s,a,r,s'$

> For all of the above essentially $\hat{Q}(s,a)$ is a neural network, approximating a function of the feature vectors $\vec{x}(s,a)$

Graphically, the gradient descent algorithms are:
- performing a Bellman backup update and
- then projecting back to the space of our approximator

<img src="stf_rl/image-20220521203039620.png" alt="image-20220521203039620" style="zoom:67%;" />

so that we can imagine our control algorithm doing:

- the horizontal place is the space of our linear approximator
- first we perform one Bellman backup, and reaches $\mathbb{B}^\pi \vec{v}_w$, which is outside of our representation
- then, we project back to complete our gradient update $\Pi \mathbb{B}^\pi \vec{v}_w$

However, notice that:
- instead of iteratively performing Bellman's backup outside the space (shown in gray above), which would make us reach the true value function (e.g. dynamic programming), we are constantly projecting back
- since we are **projecting back at each step**, then the final fixed point would be the point of vector-zero PBE
- the best approximation $\Pi v_\pi$ in the value error (VE) sense by projecting the true value function, the best approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal difference error (TDE) senses are **all potentially different**

#### Convergence of VFA Control

Consider the following example. 

<img src="stf_rl/image-20220521201453082.png" alt="image-20220521201453082" style="zoom:67%;" />

Suppose we want to evaluate the policy of $\pi(\text{solid}|\cdot)=1$. Then as we are doing policy iteration, we consider $\epsilon$-greedy policy of:

- $\mu(\text{dashed})|\cdot)= 6/7$
- $\mu(\text{solid})|\cdot)= 1/7$
- $\gamma = 0.99$

as the behavioral policy used to sample data.

Suppose for simplicity that we then throw away all $(s,a,r,s')$ tuples whose $a\neq \pi(s)$ which we wanted to evaluate. It turns out that **TD-method will diverge**:

- the stationary distribution of states under the policy we want to approximate will contain a lot of $s_7$ states, so episodes look like $s_1,s_7,s_7,...$
- however, the data sampled using our behavior policy will contain episodes such as $s_1,s_7, \text{thrown away}, s_1, s_7, ...$

Therefore, the **distribution of states** we sampled will be very **different** from the distribution of state under $\pi(s)$.

Hence, in summary:

|            | Tabular  | Linear VFA              | Nonlinear VFA |
| ---------- | -------- | ----------------------- | ------------- |
| MC Control | Converge | Oscillates but Converge | No guarantee  |
| SARSA      | Converge | Oscillates but Converge | No guarantee  |
| Q-Learning | Converge | No guarantee            | No guarantee  |

but as VFA is very important, many research are done on this:
- Extensive work in better TD-style algorithms with value function approximation, some with convergence guarantees: see Chp 11 S&B
- Exciting recent work on batch RL that can converge with nonlinear VFA (Dai et al. ICML 2018): uses primal dual optimization

# Deep Reinforcement Learning

Before, we discussed value function approximation using NN and linear functions. Here, we will focus on **RL with function approximation using deep NN**:

- need to deal with very large state spaces (e.g. for self-driving, video games, etc.)
- an intuition for using DNN in RL is to have a DNN to represent the features and the last linear layer represent the linear function approximator for $Q(s,a)$
- why is DNN so popular/useful? Universal function approximator theorem.

(the following material assumes you know some basic DNN architectures, including CNNs, RNNs, etc.) Often the idea is to have:

- game images as state
- game actions as actions (e.g. left, right, up, down, etc.)
- game score as rewards
- model the $Q$ value function using a DNN


## Deep Q Learning

Recall that since Q-Learning was based on TD updates, we had a concern that those off-policy control algorithms can fail to converge:

- off-policy control + bootstrapping + function approximation, the deadly triad
- we have also seen some simple cases where the such an algorithm does diverge

However, in 2014, DeepMind tried again to combine DNN with RL and achieved extraordinary results in Atari: **in practice we can get reasonably good policies out**. This is essentially the DQN network:

![image-20220522155926513](stf_rl/image-20220522155926513.png)

where

- since we somehow need to represent velocities in game state, 4-in game frames are taken to represent a state
- the network models $Q(s,a)$ for a total of 18 possible joystick actions
- reward is the game score

---

We know that Q-learning with VFA can diverge. Two of the issues are:
- **correlations between samples**: consider an episode of $s,a,r,s',a',r'...$. we notice that $V(s)$ would be highly correlated with $V(s')$ in this case.
- **non-stationary targets due to bootstrapping**: the update target is current approximation, which changes as we perform an update!

DQN addresses those issues by using:
- **Experience replay**: help remove correlations
- **Fixed Q-targets**: help convergence

so that it is more likely to converge.

### DQNs: Experience Replay

> To help remove correlations, store dataset (called a ==replay buffer==) $D$ from prior experience. So that instead of having updates in the same order as the episode = high correlation, we sample from the buffer.

<img src="stf_rl/image-20220522161057736.png" alt="image-20220522161057736" style="zoom:80%;" />

note that this also means we need to decide **which experience to put in the buffer**. 
- Usually we can just place the most recent 1 million (for instance) samples in the buffer and sample from it.
- this means we also tend to set some fixed probability of how often we will want to sample new experiences

With this, the update step of the algorithm would look like:
1. sample an experience tuple $(s,a,r,s') \sim D$ from the replay buffer
2. compute the target value in a TD fashion as we are doing Q-learning:
   $$
   r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}) 
   $$
3. use SGD to update:
   $$
   \Delta w = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}) - \hat{Q}(s,a|\vec{w}))\nabla_w \hat{Q}(s,a|\vec{w})
   $$

now, as mentioned before another problem is that our update target is using the current approximation, which changes as we perform an update. We are chasing a **non-stationary target**.

### DQNs: Fixed Q-Target

> To help improve stability, fix the target weights used in the target calculation **for multiple updates**, instead of having it changed per update.

Let parameters $\vec{w}^-$ and $\vec{w}$ be the weights of the target and current network. Essentially the idea is 
1. sample an experience tuple $(s,a,r,s') \sim D$ from the replay buffer
2. compute the target value **using $\vec{w}^-$**
   $$
   r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}^-) 
   $$
3. use SGD to update:
   $$
   \Delta w = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}^-) - \hat{Q}(s,a|\vec{w}))\nabla_w \hat{Q}(s,a|\vec{w})
   $$
4. after a certain period, e.g. per 100 steps, update $\vec{w}^- := \vec{w}$.

### DQN Ablation Study

How well does the above improve performance?

<img src="stf_rl/image-20220522164035968.png" alt="image-20220522164035968" style="zoom: 50%;" />

so we see that:
- replay is **hugely** important

### DQN Summary

- DQN uses experience replay and fixed Q-targets 
- Store transition $s_t, a_t, r_{t+1}, s_{t+1}$ in replay memory $D$
- Sample random mini-batch of transitions $(s,ta,r,s')$ from $D$
- Compute Q-learning targets w.r.t. old, fixed parameters $w^$
- Update using $\epsilon$-greedy exploration, so as before need some decaying exploration
- Optimizes MSE loss between Q-network and Q-learning targets

After the success of DQNs, we then had many immediate improvements (many others!)
- **Double DQN** (Deep Reinforcement Learning with Double Q-Learning, Van Hasselt et al, AAAI 2016)
- **Prioritized Replay** (Prioritized Experience Replay, Schaul et al, ICLR 2016)
- **Dueling DQN** (best paper ICML 2016) (Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, ICML 2016)
- etc.

## Double DQN

*Recall* that we had Double Q-Learning because we had the **maximization bias challenge**: Max of the estimated state-action values can be a biased estimate of the max. To solve this issue, there comes the Double Q-learning algorithm which looks like:

<img src="stf_rl/image-20220522164647254.png" alt="image-20220522164647254" style="zoom:33%;" />

so that we basically maintained two Q-networks.

> Extend this idea to DQN using Deep network for the two $Q$ networks.

Result:

<img src="stf_rl/image-20220522165031684.png" alt="image-20220522165031684" style="zoom:80%;" />


## Prioritized Replay

The idea is that instead of sampling randomly from the replay buffer, maybe there are some better distributions to sample such that we can converge better.

Intuitively, consider the following example of the 7-state Mars Rovers again:

- reward is $R=[1,0,0,0,0,0,10]$
- any action from $s_1,s_7$ are terminal
- we initialized with the greedy policy of $\pi(s)=a_1,\forall s$
- we sampled a trajectory by having an $\epsilon$-greedy version of the policy $\pi$, and obtained
  $$
  (s_3, a_1, 0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text{terminal})
  $$
- the TD estimate of all states after an in-order update with $\alpha=1$ is: $[1,0,0,0,0,0,0]$ which we had computed before.
- the MC estimate with $\gamma =1$ gives $V = [1,0,0,0,0,0,0]$
  

Now, using experience buffer, we consider the four tuples:
$$
(s_3,a_1,0,s_2), (s_2,a_1,0,s_2), (s_2,a_1,0,s_1), (s_1,a_1,1,T)
$$

**Question** if we get to choose three replay backups to do, which should we pick?

- first pick the fourth tuple $(s_1,a_1,1,T)$ so that $V(s_1)=1+\gamma * 0 = 1$
- then pick the third tuple $(s_2,a_1,0,s_1)$, because we know that $V(s_1)=1$, so that $V(s_2)= 0 + \gamma * 1$ gets **propagated**
- finally pick the first tuple, $(s_3,a_1,0,s_2)$, because we know that $V(s_2)=\gamma$, and so we can further propagate back to $s_3$.
- for $\gamma =1$, this results in $V = [1,0,0,0,0,0,0]$. Notice that it would be ==the same as MC update==!

---

In theory, such an order is important. There has been research on this and we basically found that if we know the correct order for TD updates, we can require exponentially less updates as sample grows:

![image-20220522170804087](stf_rl/image-20220522170804087.png)

However, computing the exact ordering is intractable in practice.

> Therefore, the idea is to consider some ordering based on how big the error is. Consider a sample being $(s_i,a_i,r_i,s_{i+1})$, then we consider the ==priority== of such tuple $i$ being:
> $$
> p_i \equiv \left|  \underbrace{r + \gamma \max_{a'}Q(s_{t+1},a';\vec{w}^-)}_{\text{TD target}>  }- Q(s_i,a_i;\vec{w}) \right|
> $$
> then the probability of sampling such a tuple is basically proportional to the priority:
> $$
> P(i) \equiv \frac{p_i^\alpha}{\sum_k p_k^\alpha}
> $$
notice that if we set $\alpha=0$ we get the normal experience replay with equal probability.

## Dueling DQN: Advantage Function

The intuition here is that:

- Game score may be relevant to predicting $V(s)$
- But not necessarily in indicating **relative benefit of taking different actions** in a state

Therefore, we do not care about the value of a state, but about ==which actions at state $s$ has a better value==, which can be done by looking at the advantage function
$$
A^\pi(s,a) \equiv Q^\pi(s,a) - V^\pi(s) = Q^{\pi}(s,a) - Q^{\pi}(s,\pi(s)) 
$$

which is basically the advantage of taking action $a$ at state $s$ compared to taking $\pi(s)$.

Then the architecture looks like:

|                             DQN                              |                         Dueling DQN                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220522172421357](stf_rl/image-20220522172421357.png) | <img src="stf_rl/image-20220522172429762.png" alt="image-20220522172429762" style="zoom:67%;" /> |

where essentially 
- Dueling DQN keeps one estimate of $V(s)$ from the network and another for the advantage function
- then, it is recombined back to $Q(s,a) = A^{\pi}(s,a) - V^{\pi}(s,a)$ to perform the control algorithm.
- notice that we are having ==another set of features== to represent $A(s,a)$, which reiterates the point that we might need different features for approximating the advantage than the value function $V\left(s,a\right)$.

When compared to Double DQN with Prioritized Replay, this **already has a big improvement**:

![image-20220522173022329](stf_rl/image-20220522173022329.png)

also note that the performance for "Montezuma's Revenge" was still ==not improved==, i.e. those families of improvements mainly aimed at improving reward propagation, but not for games that require **a high exploration**.

### Identifiability of Advantage Function

However, realize that by the definition of advantage function, this means:
$$
\hat{Q}^{\pi}(s,a) = V^{\pi}(s) + A^{\pi}(s,a)
$$

but it is **not unique** as we can have the same $\hat{Q}$ for different $\hat{A}$ and $\hat{V}$. This would be problematic as then our labels for training $A,V$ would be non-unique.

Therefore, the idea is to impose some fixed points:

- option 1: Force $A(s,a)=0$  for $a$ is the greedy action
  $$
  \hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \underbrace{\left( \hat{A}(s,a;\vec{w}) - \max_{a'} \hat{A}(s,a';\vec{w}) \right)}_{\text{approximate } A(s,a)}
  $$
- option 2: Use mean to be zero as the baseline (more stable)
  $$
  \hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \underbrace{\left( \hat{A}(s,a;\vec{w}) - \frac{1}{\left|\mathcal{A}\right|} \sum\limits_{a'} \hat{A}(s,a';\vec{w}) \right)}_{\text{approximate } A(s,a)}
  $$

# Imitation Learning

Previously we have seen approaches using DNN for learning value functions, and by performing policy iteration, we can improve policy.

However, there exists hardness results that if learning in a generic MDP, can require large number of samples to learn a good policy.

So it is important to remember that:

> Reinforcement Learning: Learning policies guided by (often ==sparse==) rewards (e.g. win the game or not):
> - Good: as a simple, cheap form of supervision
> - Bad: requires high sample complexity for finding a good policy

Therefore, it most successful when:
- in simulation where data is cheap and parallelization is easy

==Not== when: 
- Execution of actions is slow
- Very expensive or not tolerable to fail (e.g. learning to fly a helicopter will require crashing many times)
- Want to be safe

> **Alternative idea**: instead of learning policies from scratch, we could use *structure and additional knowledge* (if there is) to help constrain and speed reinforcement learning:
> - imitation learning: we have a fix structure of policy to learn (this section)
> - policy search/gradient: we have a fixed class of policy to search from (next section)

One application for this is to help improve the performance on "Montezoma's Revenge" game:

<img src="stf_rl/image-20220523170757631.png" alt="image-20220523170757631" style="zoom:67%;" />

where it is:

- a game that you need to collect keys and explore many different rooms
- a very very long horizon game, and requires large exploration as well
- we see that DQN agents only reached the first two rooms (LHS), with exploration bonus (RHS), we can reach more rooms but still not finishing the game
- some of the early approaches to make this work is via imitation learning

## Learning from Demonstrations

One of the challenge in Montezuma's revenge is that long episodes means reward is very sparse. This means that you need to try many different things before getting some feedback of good or bad.

> Rewards that are **dense** in time closely guide the agent:
> - Manually design them: often brittle
> - Implicitly specify them through demonstrations: **imitation learning**!
> 
> Therefore, the idea is to have (optimal or pretty good) Expert provides a set of demonstration trajectories: sequences of states and actions

This is useful when it is easier for the expert to demonstrate the desired behavior rather than come up with a reward that would generate such behavior, or coding up the desired policy directly.

---

Hence in this section, we consider the following problem setup:
- given some transition model $P(s'|s,a)$
- no reward function $R$
- given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , ...)$  drawn from some teacher policy $\pi^{*}$ 

> **Aim**:
> - Behavior Cloning: learn the teacher's policy $\pi^*$ 
> - Inverse RL: find the reward function $R$ (e.g. to *understand* behavior of certain creatures)
> - Apprenticeship Learning via Inverse RL: find the reward function $R$ and then generate a good policy

## Behavior Cloning

The idea of this is simple. Since we need to learn the teacher's policy $\pi^{*}$, all we need to do is to ==learn the mapping from state to action==.

> Therefore, this can be formulated as a **supervised machine learning** problem:
> - Fix a policy class (e.g. neural network, decision tree, etc.)
> - Estimate a policy (i.e. state to action mapping) from training examples $(s_{0} , a_{0}) , (s_{1}, a_{1} ), ...$ 

However, remember that supervised learning ==assumes iid. $(s, a)$ pairs==. This means that 
- supervised learning assumes the **state distribution** you will get in any future (e.g. after taking some actions) is the **same** as the ones in the past (iid)
- however, in MDP processes, the state distribution would change after you take some actions. 

> **Problem: Compounding Errors**. Therefore errors training in a supervised fashion leads to high generalization error, so that once we went into an unseen state:
> - assumed same state distribution, hence gets errors
> - takes the wrong action and continue getting in unseen states
> 
> Therefore, we could often get a mismatch in training and test distribution as our data is limited:
> - train have $s_t \sim D_{\pi^*}$ sampled from the teacher policy
> - test will encounter $s_t \sim D_{\pi_{\theta}}$ sampled from the our estimated policy

Graphically, this means that as soon as you made a mistake, you will continuously deviate from the path:

<img src="stf_rl/image-20220523203357492.png" alt="image-20220523203357492" style="zoom: 50%;" />

theoretically, the generalization error in this case will be bounded by $\epsilon T^{2}$ instead of $\epsilon T$ bound in supervised learning.

### DAGGER: Dataset Aggregation

This is a simple idea to counter the compounding errors problem, so that during training we can **ask the expert** to provide the gold standard action:

<img src="stf_rl/image-20220523205338781.png" alt="image-20220523205338781" style="zoom:67%;" />

therefore, we get:

- in training we have $s_{t} \sim D_{\pi_\theta}$, which will be the same in test environment!
- note that we technically *don't need to know the teacher's policy $\pi^{*}$*, we just need to query something to get the gold standard action.

So in this technique we need to have an expert, kind of like ==human in the loop==. Therefore in many cases this could be very label intensive.

## Inverse Reinforcement Learning

*Recall* that our setup was:
- given some transition model $P(s'|s,a)$
  - there are also some extensions on how to do it without transition model as well
- no reward function $R$
- given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , ...)$  drawn from some teacher policy $\pi^{*}$ 

> Goal: infer the reward function $R$.

How does this work? Consider the following:
- With *no assumptions* on the optimality of the teachers policy, what can be inferred about R?
  - nothing, because if the teacher is doing random things, then you have no idea what the reward/goal is.
- Now *assume that the teachers policy is optimal*. What can be inferred about R?
  - given some optimal path, would the reward function be unique? No: consider the reward function $R=0$ for all states and actions. Then any policy/trajectory would be optimal hence there would be more than one unique answers.
  - in fact, a reward function of any constant will have this behavior. Therefore, we need to **impose additional constraints** to solve this problem

Some of the key papers are:
- Maximumum Entropy Inverse Reinforcement Learning (Ziebart et al. AAAI 2008)

## Apprenticeship Learning

*Recall* that the setup is"
- given some transition model $P(s'|s,a)$
- no reward function $R$
- given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , ...)$  drawn from some teacher policy $\pi^*$ 

> **Aim**: find a policy that is as good as the expert policy

(essentially this comes from trying to solve for the reward function, but realize that it is no longer needed.)

Like linear value function approximation, we can consider a model:
$$
R(s) = \vec{w}^{T} \vec{x}(s)
$$
for $\vec{x}(s)$  is a feature vector for states. 

Then, given a set of demonstrations from $\pi$, we want to identify $\vec{w}$:
$$
\begin{align*}
  V^{\pi} 
  &= \mathbb{E}_\pi\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R(s_t)\right]\\
  &= \vec{w}^{T}\mathbb{E}_\pi\left[\sum\limits_{t=0}^{\infty} \gamma^{t}\vec{x}(s_t)\right]\\
  &= \vec{w}^{T}\vec{\mu}(\pi)
\end{align*}
$$

for $\vec{\mu}(\pi) \equiv \mathbb{E}_\pi\left[\sum\limits_{t=0}^{\infty} \gamma^{t}\vec{x}(s_t)\right]$ can be interpreted as a distribution of states/features weighted by the discount factor.

But we know that an optimal policy will have by definition the largest value function:

$$
\begin{align*}
  V^{*}
  &\ge V^{\pi}\\
  \mathbb{E}_{\pi^{*}}\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R^{*}(s_t)\right]
  &\ge \mathbb{E}_{\pi}\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R^{*}(s_t)\right]\\
  \vec{w}^{*^{T}} \vec{\mu}(\pi^{*})
  &\ge \vec{w}^{*^{T}} \vec{\mu}(\pi)
\end{align*}
$$

> From this, the idea is that:
> - to find the optimal weights $w^{*}$, we need the expert policy to perform better than any other policy (under this reward function)
> - to perform close to the optimal value $\vec{w}^{*^{T}}$, we just need to match the features under the expert policy

Formally, if
$$
||\vec{\mu}(\pi) - \vec{\mu}(\pi^*)||_1 \le \epsilon
$$

then for ==all $\vec{w}$== with $||\vec{w}_\infty || \le 1$ :
$$
||\vec{w}^T\vec{\mu}(\pi) - \vec{w}^{T}\vec{\mu}(\pi^*)||_1 \le \epsilon
$$

> So that we can find the ==optimal policy== $\pi^{*}$ without needing to know the reward weight $\vec{w}$ by finding a policy $\pi$ such that:
> $$
> ||\vec{\mu}(\pi) - \vec{\mu}(\pi^*)||_1 \le \epsilon
> $$
> and for $\vec{\mu}(\pi)$, all we need to know is to compute the discounted sum of features $\vec{x}(s)$ following that policy $\pi$.

Therefore, this gives the Feature Expectation Matching:

<img src="stf_rl/image-20220523222433551.png" alt="image-20220523222433551" style="zoom: 67%;" />

where here we are picking the best policy and picking the corresponding reward function
- still doesn't deal with the problem that reward function is not unique
- if expert policy is suboptimal then the resulting policy is a mixture of somewhat arbitrary policies which have expert in the convex hull

Nowadays we tend to use DNN versions, and some extensions include:
- GAIL: Generative adversarial imitation learning (Ho and Ermon, NeurIPS 2016)

## Imitation Learning Summary

- Imitation learning can greatly reduce the amount of data need to learn a good policy
  - i.e. think of the expert demonstration as an additional "constraint"/structure 
- Challenges remain and one exciting area is combining inverse RL / Learning from demonstration and online reinforcement learning

# Policy Gradient

Similar to imitation learning, policy gradient/search methods imposes some **constraints** on the policy and hence makes learning more efficient. However, the changes are:

- recall that previously our Policy Improvement algorithm mainly involved:
  - policy evaluation: update our estimate of the value function ($V_{\theta} \approx V^{\pi}(s)$ or $Q_{\theta} (s,a) \approx Q^\pi (s,a)$  )
  - policy improvement: improve $\pi \gets \epsilon\mathrm{-Greedy}(Q_{\theta} (s,a))$

- however, in this section we will **directly parametrize the policy**, to consider the model:
  $$
  \pi_{\theta} (s,a) \equiv P_\theta[a|s]
  $$
  and the ==aim is to find a policy $\pi$ with the highest value function $V^{\pi}$==.

- of course we will do it in a model-free approach (i.e. without transition/reward function)

Graphically, we are at:

<img src="stf_rl/image-20220605205922887.png" alt="image-20220605205922887" style="zoom:67%;" />

so essentially:

|                                 | Learnt/Modelling Value Function | Learnt/Modelling Policy |
| ------------------------------- | :-------------------: | :-------------: |
| Value Based (previous sections) | Yes | Implicit (e.g. greedy) |
| Policy Based (here)             | No | Yes |
| Actor-Critic (next sections)    | Yes | Yes |

Why might we want to use policy search/gradient methods? Advantages:
- Better convergence properties (also depends on computation power)

- Effective in high-dimensional or **continuous action spaces**

- Can learn **stochastic** policies. 
  - Useful when: exists competitor and do no want to be exploited (e.g. rock-paper-scissor)
  
  - Useful when: the problem is not Markov, e.g. your grey states cannot be distinguished
  
    <img src="stf_rl/image-20220605211409890.png" alt="image-20220605211409890" style="zoom:67%;" />
  
    the problem that states cannot be distinguished is also called ==aliasing==. In this case using stochastic policy in grey states is a good idea/has a higher value.
  
  - Not very useful when: in a tabular MDP problem, optimal policy is deterministic

  - **monotonic improvement** of the policy (value-based methods, e.g. DQN, is noisy)

Disadvantages:
- Typically ==converge to a local rather than global optimum==
- **Evaluating** a policy (from which we compute gradient) is typically sample inefficient and high variance

## Policy Objective Functions

> **Goal**: given a policy $\pi_{\theta} (s,a)$  parameterized by $\theta$, we want to find $\theta$ that yields the **highest value** (not value function, but a single score)
> - in episodic setting (i.e. has terminal states), we can use the **start state** to compare policies:
>   $$
>   J_{1}(\theta) = V^{\pi_\theta}(s_1)
>   $$
>   for $s_1$ is the start state of the entire environment
> - in continuing/online environment where there is no terminal states, we can use the **average value** over all states:
>   $$
>   J_{avV}(\theta) = \sum_s d^{\pi_\theta} (s) V^{\pi_\theta}(s)
>   $$
>   where $d^{\pi_\theta}(s)$ is the **stationary distribution** of states for $\pi_\theta$. A similar version of this is to look at the **average reward per time-step**:
>   $$
>   J_{avR}(\theta) = \sum_s d^{\pi_\theta}(s) \sum_a  \pi_\theta(s,a) R(a,s)
>   $$

Now, we have a goal function from which we can ==optimize==. Therefore, it becomes an optimization problem and we can consider:

- gradient free optimization (useful when not differentiable, but not used often now)
  - Hill climbing
  - Simplex / amoeba / Nelder Mead
  - Genetic algorithms
  - Cross-Entropy method (CEM)
  - Covariance Matrix Adaptation (CMA)
- gradient based optimization (this section)
  - Gradient descent
  - Conjugate gradient
  - Quasi-newton

## Policy Gradient Methods

Consider the setup of:
- Define $V(\theta) \equiv V^{\pi_{\theta}}$  to make explicit the dependence of the value on the policy parameters
- assume episodic MDPs

Then policy search/gradient considers ==gradient ascent== w.r.t parameter $\theta$:
$$
\nabla \theta = +\alpha \nabla_{\theta} V(\theta)
$$

But the question is, ==what is $\nabla_{\theta} V(\theta)$== 
$$
\nabla_{\theta} V(\theta) = \begin{bmatrix}
  \frac{\partial V(\theta)}{\partial \theta_1} \\
  \frac{\partial V(\theta)}{\partial \theta_2} \\
  \vdots\\
  \frac{\partial V(\theta)}{\partial \theta_n}
\end{bmatrix}
$$

==when our parameter is on $\pi_\theta$?==

### Gradients by Finite Differences

The simplest approach to compute this by "trial and error":
1. for each dimension $k \in [1, n]$ for $\theta \in \mathbb{R}^{n}$:
   1. perturb the parameter by a small amount:
      $$
      V(\theta + \epsilon u_{k})
      $$
      for $u_k$ is a unit vector in the $k$-th dimension
   2. estimate the gradient w.r.t. $\theta_k$ by:
      $$
      \frac{\partial V(\theta)}{\partial \theta_k} \approx \frac{V(\theta + \epsilon u_{k}) - V(\theta)}{\epsilon}
      $$
2. repeat

While this method seems simple, it has been used practically and was effective (required only few trials).

---

*For instance: AIBO Policy Experiment*

The task was to train AIBO robots so that they can walk as fast as possible. Then there was research which had:

- open-loop policy: find a sequence of actions, irrespective of the state
- 12 parameters $\theta \in \mathbb{R}^{12}$ they wanted to search for:
  - The front locus (3 parameters: height, x-pos., y-pos.)
  - The rear locus (3 parameters)
  - Locus length
  - Locus skew multiplier in the x-y plane (for turning)
  - etc.

Then, essentially they needed to compute the gradient of the policy, which they can by:
- Ran on 3 AIBOs at once
- Evaluated 15 policies per iteration.
- Each policy evaluated 3 times (to reduce noise) and averaged
- Used $\eta = 2$ (learning rate for their finite difference approach)

And they were very successful in few iterations:

<img src="stf_rl/image-20220605224812515.png" alt="image-20220605224812515" style="zoom: 67%;" />

note that as we are converging to local optima, initialization for $\theta$  matters.

### Likelihood Ratio/Score Function Policy Gradient

Suppose we **can compute $\nabla_{\theta} \pi_\theta(s)$**. We want to find a way to compute $\nabla_\theta V(\theta)$ by analytical methods, and one of which is the likelihood ratio policy gradient.

Consider the following setup:

- denote a trajectory as $\tau = (s_{0}, a_{0} , r_{0} , ... , s_{T-1},a_{T-1},r_{T-1},s_{T},a_{T},r_{T})$. 
- denote the reward of the trajectory as $R(\tau) \equiv \sum\limits_{t=0}^{T} R(s_t, a_t)$
- then define the ==policy value== as:
  $$
  V(\theta) \equiv \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau) = \mathbb{E}_{\pi_{\theta}} \left[ \sum\limits_{t=0}^{T} R(s_{t} , a_{t} ) \right]
  $$
  for $P_{\pi_\theta}(\tau)$ being the probability over trajectories when executing policy $\pi_\theta$.

> In this new notation, our goal is to find parameters $\theta$ such that:
> $$
> \arg\max_\theta V(\theta) = \arg\max_\theta \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau) 
> $$
> notice that only $P_{\pi_\theta}(\tau)$ changes if you change $\theta$.

Therefore, we are interested in finding $\nabla_\theta V(\theta)$, which can be computed as:

<img src="stf_rl/image-20220605230842266.png" alt="image-20220605230842266" style="zoom: 67%;" />

which seems to have done nothing, but we will soon see that 

- since we are doing model free, ==this trick helps us to compute $\nabla_\theta \log P(\tau; \theta)$ without knowing the world==.
- to evaluate $\sum\limits_{\tau}$ and $R(\tau)$, we can just ==sample== a bunch of trajectories!

Therefore, first we approximate this by:

$$
\nabla_{\theta} V(\theta) \approx \hat{g} = \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)
$$

for which $R(\tau^{(i)})$ are known already, and we are weighting each trajectory equally.

> The above form of policy gradient can be interpreted as follows: consider the ==generic form of $R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)$ which can be made into:
> $$
> \hat{g}_i \equiv f(x_i) \nabla_\theta \log p(x_i;\theta)
> $$
> which means that:
> - $f(x)$ measure how good a sample $x$ is (e.g. $f(x_i) = R(\tau^{(i)})$ )
> - performing a gradient ascent step means we are **pushing up the log probability $p(x_i;\theta)$ of the sample, in proportion to how good it is $f(x)$**
> - note that this form is valid even if $f(x)$ is discontinous

Graphically:

<img src="stf_rl/image-20220605232345294.png" alt="image-20220605232345294" style="zoom: 50%;" />

Finally, we need to solve for the term $\nabla_\theta \log P(\tau^{(i)};\theta)$ to compute the gradient $\nabla_{\theta} V(\theta) \approx \hat{g}$: 

<img src="stf_rl/image-20220605232719619.png" alt="image-20220605232719619" style="zoom:67%;" />

notice that from this we ==only need to know $\nabla_{\theta} \log \pi_\theta$==! This term also gets a (relatively useless) name of ==score function==.

> **Score Function Policy Gradient:**
> 
> The goal is to find $\theta$ such that:
> $$
>  \arg\max_\theta V(\theta) = \arg\max_\theta \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau)
> $$
> which we can find local optimal using gradient ascent, by:
> $$
>  \begin{align*}
>   \nabla_{\theta} V(\theta) \approx \hat{g} 
    &= \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)\\
    &= \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
>  \end{align*}
> $$
> and the $m$ trajectories would be sampled under policy $\pi_\theta$. Notice that this is model free as we only need to evaluate the score function $\nabla_{\theta} \log \pi_\theta$.

Finally, it turns out that this can be extended to many other objective functions $J$ than the one shown above $J = V(\theta) = \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau)$.

> **Policy Gradient Theorem**: for any differentiable policy $\pi_{\theta} (s,a)$, for any policy objective function:
> - episodic $J = J_1$
> - average reward per time step $J = J_{avR}$
> - average value $J = (1 / (1 - \gamma)) J_{avV}$
> the policy gradient is:
> $$
>  \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ Q^{\pi_\theta}(s,a) * \nabla_{\theta} \log \pi_\theta (a|s)  ]
> $$
> which can be derived in the same manner as we have shown here (also shown in Sutton and Barto chapter 13)

## Reducing Variance in Policy Gradient

Previously, we were able to compute policy gradient without a world model by:

$$
\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
$$

for letting the objective function being $J = V(\theta)$. However, although this is an ==unbiased estimate of the gradient==, it is ==noisy (high variance) in practice==. Fixes include:

- temporal structure/bootstrapping methods (reduces variance)
- baseline
- alternatives to using MC return $R(\tau^{(i)})$ as targets, e.g. Actor-Critic methods for TD and bootstrapping

### Policy Gradient: Use Temporal Structure

It turns out that we can manipulate the same expression we had to achieve another gradient expression. Consider the previous conclusion:

$$
\nabla_{\theta} \mathbb{E}_\tau [R] = \mathbb{E}_\tau \left[ R \left( \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \right]
$$

for $R = \left( \sum\limits_{t=0}^{T-1} r_t \right)$. This means that:

$$
\nabla_{\theta} \mathbb{E}_\tau [r_{t'}] = \mathbb{E}_\tau \left[ r_{t'} \left( \sum\limits_{t=0}^{t'} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \right]
$$

Hence we can recover the original form by summing over $t$:

$$
\begin{align*}
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  &= \mathbb{E}_\tau \left[ \sum\limits_{t'=0}^{T-1} r_{t'}  \sum\limits_{t=0}^{t'} \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\
  &= \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum\limits_{t'=t}^{T-1} r_{t'} \right] \\
  &= \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) G_{t'} \right]
\end{align*}
$$

where
- the second last equality is due to the fact that, the $\nabla_\theta \log \pi_\theta(a_0|s_0)$ appears for all $r_{t'}$, and the $\nabla_\theta \log \pi_\theta(a_1|s_1)$ appears for all except $r_{t' = 0}$, etc.
- the last equality is due to the fact that $G_{t'}$ is the expected return since time $t'$

> Therefore, this yields a slight lower variance estimate of the gradient:
> $$
>   \nabla_{\theta} V(\theta) = \nabla_\theta \mathbb{E}_\tau [R] \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}
> $$

Notice that 
- as compared to the previous form $\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})$, we multiply each $\nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})$ by its own expected reward $G_t^{(i)}$ instead of the constant episodic reward $R(\tau^{(u)})$. This ==reduces the variance== of the gradient estimate.
- since this uses $G_t$, it is a MC estimate. 

### REINFORCE Algorithm

If we sample only one trajectory (doing SGD), then essentially the previous section says:

$$
\nabla_{\theta} V(\theta) \approx \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}
$$
for an sampled trajectory $i$. And this is basically what REINFORCE algorithm does:

<img src="stf_rl/image-20220606003925976.png" alt="image-20220606003925976" style="zoom:67%;" />

and of course if $\pi_{\theta} (a|s)$ is a neural network, then derivative can easily numerically evaluated as well (e.g. automatic differentiation in PyTorch/Tensorflow).

### Differentiable Policy Classes

Besides neural network, there are two classes of policy model that can be good to know:

- **Softmax Policy**: weight action using linear combination of feature $\phi(s,a)$ so that:
  $$
  \pi_\theta(a|s)  = \mathrm{Softmax}\left( \phi(s,a)^{T} \theta \right) 
  $$
  which outputs a probability per ==discrete action==. The score function for this type of model is:
  $$
  \nabla_{\theta} \log \pi_{\theta} (a|s) = \phi(s,a) - \mathbb{E}_{\pi_{\theta}} [\phi(s, \cdot )]
  $$
- **Gaussian Policy**: very useful for ==continuos action space==, which we can parametrize by:
  $$
  \pi_\theta(a|s) \sim N( \mu(s), \sigma^{2})
  $$
  for variance can be a fixed constant or parametrized as well. We model the mean by:
  $$
  \mu(s) = \phi(s)^{T} \theta
  $$
  being a linear combination again. Then the score function for this is:
  $$
   \nabla_{\theta} \log \pi_{\theta} (a|s) = \frac{(a- \mu(s))\phi(s)}{\sigma^{2}}
  $$

### Policy Gradient: Introduce Baseline

Another way to improve gradient estimates is to use a baseline $b(s_t)$. In the previous section we have derived:

$$
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum\limits_{t'=t}^{T-1} r_{t'} \right]
$$

It turns out that we can reduce variance by:

$$
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum\limits_{t'=t}^{T-1} r_{t'} - b(s_t) \right)\right]
$$

which is still an ==unbiased== estimate:

- a near optimal choice is to consider $b(s_t)$ as the expected return $b(s_{t}) \approx \mathbb{E}[r_{t} + r_{t+1} + ... + r_{T-1}]$. 
- in that case, this means that we are increasing logprob of action at proportionally to how much returns $\sum\limits_{t'=t}^{T-1}  r_{t'}$ are better than expected.

Why does this not introduce bias? We can prove this by showing:

$$
\mathbb{E}_\tau \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t) \right] = 0
$$

Proof:

<img src='stf_rl/image-20220606171712.png' style='zoom:60%;display:block;margin:auto'/>

note that this holds for any $b(s_t)$ that is a function of states, and:

- the second equality holds because $b(s_t)$ only depends on current and past states 
- the fourth equality holds because the probability of getting an action $a_t$ given $s_t$ is exactly specified by $\pi_{\theta} (a_{t} | s_t)$   

Finally the question is what choice of $b(s_t)$ could reduce variance? There will be a number of choices, and some of which we will discuss next.

#### Vanilla Policy Gradient Algorithm and Auto Differentiation

Using the baseline $b(s_t)$ derivation, the simplest algorithm would look like:

1. initialize policy parameter $\theta$, baseline $b$  (e.g. a NN, or a lookup table)
2. for iteration $i=1,2, ...$:
   1. collect a set of trajectory using current policy $\pi_\theta$:
   2. (MC target) at each time step $t=1 , ... , |\tau^{(i)}|$:
      1. compute average return $G_t^{(i)} = \sum_{t'=t}^{T}r_t^{(i)}$ 
      2. compute **advantage estimate**: $A_t^{(i)} = G_t^{(i)} - b(s_t)$
   3. refit the baseline by minimizing $\sum\limits_{i} \sum\limits_{t} \left\|b(s_t^{(i)}) - G_t^{(i)} \right\|^{2}$ 
      - or instead of going over all trajectories (off-policy), we can only use the trajectories of the most recent iteration as well (on-policy)
      - if only current iteration is used, then baseline is basically estimating $b(s_{t}) \approx V^{\pi_\theta}$ of the current policy
   4. (Policy Improvement) update the policy by policy gradient estimate by summing:
      $$
      \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t
      $$
      and plug this into SGD or ADAM, etc.

> Practically, many libraries have auto differentiation. Therefore, instead of calculating manually:
> $$
> \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t
> $$
> we can just define a ==surrogate loss function==:
> $$
> L(\theta) \equiv \sum_t \log \pi_\theta(a_t|s_t) \hat{A}_t
> $$
> And then let ==auto differentiation== compute the gradient:
> $$
> \hat{g} = \nabla_\theta L(\theta)
> $$

### Policy Gradient: Actor-Critic Methods

In the previous algorithm, we used MC targets which derives from the form:

$$
\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
$$

having $R(\tau^{(i)})$ as a target we know leads to high variance but is unbiased (MC updates). But we can then reduce variance by introducing bias using **bootstrapping** and **function approximation** (e.g. TD vs MC)

> **Actor-Critic Methods**: Estimate the value function $V$ or $Q$, called ==critic==. Therefore, it maintains an explicit representation of ==both the policy and the value function==, and update both. (e.g. A3C model)

Therefore, in the actor-critic case, we consider instead of:

$$
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum\limits_{t'=t}^{T-1} r_{t'} - b(s_t) \right)\right]
$$

Use:

$$
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( Q(s_t,a_t;w)- b(s_t) \right)\right]
$$

for $Q(s_t,a_t;w)$ being another function approximator using weights $w$. If we choose $b(s_t)$ to be the value function, then this becomes

$$
  \nabla_{\theta} V(\theta)  
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}^{\pi} (s_{t}, a_t) \right], \quad \hat{A}^{\pi} (s_{t}, a_{t}) \equiv Q(s_{t}, a_{t} ) - V(s_t)
$$

#### N-step Estimators

It turns out that how exactly we model the $Q$ function can also be varied. Recall that 

$$
\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
$$

The critic can select any blend between TD and MC estimates for the true-state action value function:

- $\hat{R}_t^{(1)} = r_{t} + \gamma V(s_{t+1})$ being "TD(0)", for which you bootstrap the value $V(s_{t+1})$ from your critic

- $\hat{R}_t^{(2)} = r_{t} + \gamma r_{t+1} + \gamma^{2} V(s_{t+2})$ being "TD(1)"
- ...
- $\hat{R}_t^{(\infty)} = r_{t} + \gamma r_{t+1} + \gamma^{2}r_{t+2} + ... = G_t$ being MC

From this , ths means hhat the ==critic can model $V$ and choose between==:

- $\hat{A}_t^{(1)} = r_{t} + \gamma V(s_{t+1}) - V(s_t)$ 
- $\hat{A}_t^{(2)} = r_{t} + \gamma r_{t+1} + \gamma^{2} V(s_{t+1}) - V(s_t)$ 
- ...
- $\hat{A}_t^{(\infty)} = r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + ... - V(s_{t}) = G_{t} - V(s_t)$ becomes the MC update used in the (Vanilla Policy Gradient Algorithm)[#Vanilla Policy Gradient Algorithm and Auto Differentiation].

to plug into the gradient equation:

$$
  \nabla_{\theta} V(\theta)  
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}^{\pi} (s_{t}, a_t) \right]
$$

and of course $\hat{A}_t^{(1)}$ would have **low variance** but high bias, and $\hat{A}_t^{(\infty)}$ would have high variance but low bias. Such models are also called ==Advantage Actor-Critic Methods== (A2C).

## Automatic Step Size Tuning

In supervised learning case, we can overshoot the gradient if our step size is too big. However, it is not too bad because at least the ==data is fixed==, i.e. you might recover the local minima later.

However, in Reinforcement Learning cases, **changing policy changes the data**, this means that if you overshoot and obtain bad policies, then you might have ==no data that can learn towards optimas==.

- previously we have been experimenting with different target values (MC vs TD target, etc)
- now, we care about how we use the gradient to update parameters, i.e. step size.

<img src='stf_rl/image-20220606215929.png' style='zoom:70%;display:block;margin:auto'/>

Therefore, what can we do to determine a good step size (and in order to **achieve monotonic improvement** we wanted before)?

-  **Line search** in direction of gradient
   -  Simple but expensive (perform evaluations along the line)
   -  Naive: ignores where the first order approximation is good or bad
-  Given old policy weights $\theta_i$, we want to somehow find $\theta_{i+1}$ such that:
   $$
   \arg\max_{\theta_{i+1}} V^{\pi_{\theta_{i+1}}}(s)
   $$
   so we ensure monotonic improvement and improvement is large. To do this we would need to do **off-policy evaluation**.

> Central Problem: We have trajectories genreated from our old policy $\tau \sim \pi_{\theta_i}$, and we want to know the **value of a new policy** $V^{\pi_{\theta_{i+1}}}$ (e.g. results from a gradient update). Essentially this is off-policy evaluation!

### Evaluating Off-Policy Value Function

How do we estimate $V^{\pi_{\theta_{i+1}}}$ given only old data $V^{\pi_{\theta_{i}}}$ and trajectories $\tau \sim \pi_{\theta_i}$? Consider the following equality:

$$
V(\tilde{\theta}) = V(\theta) + \mathbb{E}_{\pi_{\tilde{\theta}}} \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]
$$

for the advantage being $A_{\pi_\theta}(s,a)$ we can compute using old weights (i.e. $A_{\pi_\theta}(s,a)= Q^{\pi_\theta}(s,a) - Q^{\pi_\theta}(s,\pi_\theta (s))$ ) **but $(s,a) \sim \pi_{\tilde{\theta}}$ is from the new policy**.

This can then be expressed as:

$$
\begin{align*}
  V(\tilde{\theta}) 
  &= V(\theta) + \mathbb{E}_{\pi_{\tilde{\theta}}} \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]\\
  &= V(\theta) + \sum\limits_{s} \mu_{\tilde{\theta}}(s) \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]\\
  &= V(\theta) + \sum\limits_{s} \mu_{\tilde{\theta}}(s) \sum\limits_{a} \pi_{\tilde{\theta}}(a|s) A_{\pi_\theta} (s_{t}, a_t) \\
\end{align*}
$$

therefore, the only unknown is $\mu_{\tilde{\theta}}(s)$ which is a ==stationary distribution of states under new policy $\pi_{\tilde{\theta}}$== as we don't have new trajectories.

However, we do know $\mu_{\theta}(s)$ from the old policy $\pi_\theta$, and it turns out that the following **approximation** would provide a good lower bound estimate:

$$
L_{\pi_\theta} (\pi_{\tilde{\theta}}) \equiv V(\theta) + \sum\limits_{s} \mu_{\theta}(s) \sum\limits_{a} \pi_{\tilde{\theta}}(a|s) A_{\pi_\theta} (s_{t}, a_t)
$$

where we essentially substituted $\mu_{\tilde{\theta}}(s)$ for $\mu_{\theta}(s)$ . Notice that from this definition of objective function:

$$
L_{\pi_\theta} (\pi_\theta) = V(\theta)
$$

and of course we want $L_{\pi_\theta} (\pi_{\tilde{\theta}}) \approx V(\tilde{\theta})$, so there comes the theorem.

> **Conservative Policy Iteration**: if we take the new policy to be a mixture of old policy and a different policy:
> $$
> \pi_{new}(a|s) = (1-\alpha)\pi_{old}(a|s) + \alpha\pi_{old}'(a|s)
> $$
> then this guarantees a lower bound:
> $$
> V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{2 \epsilon \gamma}{(1-\gamma)^2} \alpha^2
> $$
> meaning the RHS is the lower bound of the true value we want $V^{\pi_{new}}$. And notice that all quantities on the RHS are computable/known!

The above need a mixture of policy for the new policy. Can we make it even better? It turns out that we can have a lower bound of ==any stochastic policy== by:

> **Lower Bound in General Stochastic Policy**: for any new stochastic polict $\pi_{new}$, the following holds:
> $$
> V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} (D_{TV}^{\max} (\pi_{old}, \pi_{new}))^2
> $$
> for $\epsilon = \max{s,a}\left| A_{\pi} (s,a) \right|$ and the distance is defined as $D_{TV}^{\max} (\pi_{1}, \pi_{2})=\max_s D_{TV}(\pi_{1}, \pi_{2})$ for:
> $$
> D_{TV}(\pi_{1}(\cdot |s), \pi_{2}(\cdot|s)) \equiv \max_a | \pi_{1}(a|s) - \pi_{2}(a|s) |
> $$
> essentially is a distance depending on the probability distribution.

But of course since finding max will be difficult to work with, often we use the fact that:

$$
D_{TV}(p,q)^{2} \le D_{KL}(p,q)
$$

being the KL divergence between two distribution, which is more computable. Hence this means we can use:

$$
V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{old}, \pi_{new})
$$

Finally, how do we use this result to make sure we are having monotonic policy improvement?

> **Guaranteed Improvement**: recall that the goal is to make sure that the new policy $\pi_{i+1}$ (e.g. after gradient descent updates) have a better value than the old policy $\pi_i$. To achieve this, if we consider defineing a ==metric==:
> $$
> M_i(\pi) \equiv  L_{\pi_{i}} (\pi) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{i}, \pi)
> $$
> Then, we realize that if we ==ensure improvement in this metric==:
> $$
> \begin{align*}
> M_i(\pi_{i+1}) - M_i(\pi_i) &> 0\\
> V^{\pi_{i+1}} - M_i(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i) &> 0 \\
> V^{\pi_{i+1}} - M_i(\pi_i) = V^{\pi_{i+1}} - V^{\pi_i} &> 0
> \end{align*}
> $$
> which achieved what we wanted, that $V^{\pi_{i+1}} - V^{\pi_i}$ is improved for all states (i.e. infinity norm), and where:
> - the second inequality due to the fact that we know by definition:
>   $$
>   V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{old}, \pi_{new})
>   $$
> - the last equality is because we know the KL divergence between the same distribution is zero:
>   $$
>   M_i(\pi_i) = L_{\pi_i}(\pi_i) - 0 = V^{\pi_i}
>   $$
>   as the advantage function is zero for the same policy.

This means that if the ==new policy has a higher $M_i$, then my new policy has to be improving==.

- however, for cases when you have a large state-action space, evaluating the quantity:
  $$
  \epsilon = \max_{s,a}\left| A_{\pi} (s,a) \right|
  $$
  would be difficult. So next we show some practical algorithms that "approximate" this.
- in general, algorithms that uses this type of objective will be called ==Minorization-Maximization (MM) algorithm==

## MM Objective and Trust Regions

In the first part of this section, we were considering policy gradient with the objective being the value function itself, so that we were considering:

$$
\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
$$

But from the previous section, we found a way to guarantee the improvement of the policy by looking at the lower bound of $V^{\pi_{i+1}}$. This means that we can also ==directly optimize over the lower bound==:

$$
M_{\theta_i}(\theta) \equiv L_{\theta_{i}} (\theta) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\theta_{i}, \theta)
$$

But to make it practical as it may be hard to compute $\epsilon$, we consider:

$$
J_{\theta_i}(\theta) \equiv L_{\theta_{i}} (\theta) - c \cdot D_{KL}^{\max} (\theta_{i}, \theta)
$$

for $c$ is a **penalty coefficient** that we choose as a hyperparameter. Note that 

- if we used $c = \frac{4 \epsilon \gamma}{(1-\gamma)^2}$ is computable in your case, then you need to use a **very small step size**.
- or we can approximately find a **step size** in this case that will likely make the monotonic improvement hold: **trust regions constraint**

### Trust Region Constraints

Notice that we can reformulate the above objective into a constraint optimization task, and from which we can **Use a trust region constraint on step sizes $\delta$ (Schulman, Levine, Abbeel, Jordan, & Moritz ICML 2015)**:

$$
\begin{align*}
  \max_{\theta} L_{\theta_{old}} (\theta &) \\
  \mathrm{subject\,to}\, D_{KL}^{s\sim \mu_{\theta_{old}}} (\theta_{old},\theta)\le &\delta
\end{align*}
$$

which uses the **average KL divergence** over states instead of max (max requires the KL is bounded at all states and yields an impractical number of constraints).

### Trust Region Policy Optimization

The goal is to compute the policy improvement based on:

$$
\begin{align*}
  \max_{\theta} L_{\theta_{old}} (\theta &) \\
  \mathrm{subject\,to}\, D_{KL}^{s\sim \mu_{\theta_{old}}} (\theta_{old},\theta)\le &\delta
\end{align*}
$$

for 
$$
L_{\theta_{old}} (\theta ) = V(\theta_{old}) + \sum\limits_{s} \mu_{\theta_{old}}(s) \sum\limits_{a} \pi_{\theta}(a|s) A_{\pi_{\theta_{old}}} (s, a)
$
we then make there further substitutions to make this more computable:

- instead of weigthing on the true stationary distribution:
  $$
   \sum\limits_{s} \mu_{\theta_{old}}\to \frac{1}{1-\gamma} \mathbb{E}_{s\sim \mu_{\theta_{old}}}
  $$
- use ==importance sampling== to estimate the desired sum, which enables the use of an **alternate sampling distribution $q$** (other than the new policy $\pi_\theta$):
  $$
  \sum\limits_{a} \pi_{\theta}(a|s) A_{\pi_{\theta_{old}}} (s, a)\to \mathbb{E}_{a\sim q}\left[ \frac{\pi_\theta(a|s_n)}{q(a|s_n)}  A_{\pi_{\theta_{old}}} (s_n, a)\right]
  $$
  for $s_n$ is a particular sampled state. A simple choice of $q$ is to take the old policy $q(a|s)=\pi_{old}$.
- finally:
  $$
  A_{\theta_{old}}\to Q_{\theta_{old}}
  $$
- Note that these 3 substitutions ==do not change the solution== to the above optimization problem, but make them more computable.

Therefore, this gives the following reformulation of the same objective:

> Trust Region Objective Function: we want to optimize:
> $$
> \begin{align*}
>    \max_{\theta} \mathbb{E}_{a\sim q}\left[ \frac{\pi_\theta(a|s_n)}{q(a|s_n)}  Q_{\pi_{\theta_{old}}} (s_n, a )\right] \\
>    \mathrm{subject\,to}\, \mathbb{E}_{s\sim \mu_{\theta_{old}}} \left[ D_{KL} (\pi_{\theta_{old}}(\cdot,s),\pi_{\theta}(\cdot,s)) \right] \le &\delta
> \end{align*}
> $$
> and often we take $q(a|s)=\pi_{old}$.

Which gives rises to the TRPO algorithm:

<img src='stf_rl/image-20220607164703.png' style='zoom:60%;'/>

which has the following benefits and has been popular:

- Policy gradient approach
- Uses surrogate optimization function
- Automatically constrains the weight update to a trusted region, to approximate where the first order approximation is valid
- Empirically consistently does well

# Monte Carlo Tree Search

Essentially it is Model-Based Reinforcement Learning, which aims to do:

- given experience, **learn the model** (i.e. transition and/or reward function)
- then use the model to plan and **construct a value function or policy**
  - since now you have that simulator model, you can also do model-free RL at this step

(whereas all the previous sections were directly learning the value function or policy from experience, which Model-Free RL)

Graphically, model-based RL is trying to answer the following questions:

<img src='stf_rl/image-20220608141633.png' style='zoom:50%;'/>

where the trick of this against the Model-Free approach is that, when finding the next action, we ==do not have to compute the full value function/policy== (as in Model-Free approaches). We only need to find the ==next single action==, which is exploited by the MCTS algorithm.

**Advantages**:
- Can efficiently learn model by supervised learning methods
  - e.g. for modeling transition function we need to spit out a distribution, and for reward, it is a regression problem.
- Can reason about model uncertainty
  - e.g.upper confidence bound methods for exploration/exploitation trade offs
- can be good at transfer learning if only the reward function is changed.
  - i.e. if you have the model learnt, then you can zero-shot transfer

**Disadvantages**:
- First learn a model, then construct a value function. This means there are potentially two sources of approximation error
  - e.g. like imitation learning, we could have compounding errors


## Model Learning

Recall that a model is a representation of an MDP $(S,A,P,R)$, which can be **parametrized by $\eta$**:

$$
M = (P_{\eta}, R_{\eta} ), \quad \begin{cases}
  s_{t+1}\sim P_\eta(S_{t+1}|S_t,A_t) \\
  r_{t+1} = R_\eta(S_t,A_t) =R_\eta(R_{t+1}|S_t,A_t)
\end{cases}
$$

assuming that the state space and action space are already known. Typically we also assume that state transitions and rewards are independent:

$$
\mathbb{P}(S_{t+1},R_{t+1}|S_t,A_t) = P_{\eta}(S_{t+1}|S_t,A_t)R_{\eta}(R_{t+1}|S_t,A_t)
$$

> **Goal**: estimate a model $M_\eta$ from experience $\{ S_1,A_1,R_1, ...,S_T \}$. This becomes a supervised problem to consider input of:
> $$
> \begin{align*}
>    S_1,A_1&\to R_1,S_2\\
>    S_2,A_2&\to R_2,S_3\\
>    &...
> \end{align*}
> $$
> since there are two outputs, we can consider:
> - learning $s,a\to r$ being a **regression** problem (output a scalar)
> - learning $s,a\to s'$ being a **density estimation** problem (e.g. a vector of size $\left|S\right|$ with each slot being a probability)
> 
> then the loss functions could be mean-square error and KL divergence, and etc.

Example Models:

- Table Lookup Model, e.g. if state and actions are discrete, we can just count them:
  $$
  \hat{P}(s'|s,a) = \frac{1}{N(s,a)} \sum\limits_{t=1}^{T}\mathbb{1} \{s'|S_t=s,A_t=a\}\\
  \hat{R}(r|s,a) = \frac{1}{N(s,a)} \sum\limits_{t=1}^{T}R(S_t=s,A_t=a)
  $$
- Linear Expectation Model
- Linear Gaussian Model
- Gaussian Process Model
- Deep Belief Network Mode
- Bayseian DNN
- etc.

---

*For instance*, consider using a table lookup model shown above and we have the following data:

| Data | Table Lookup Model |
| :--- | :---               |
| <img src='stf_rl/image-20220608151144.png' style='zoom:50%;'/>| <img src='stf_rl/image-20220608151159.png' style='zoom:50%;'/>|

where:

- for simplicity we assume there is only one action
- since we are storing averages and counts, essentially table look up decides:
  - $R(S=a)=0$ since all the time the immediate reward is zero
  - $R(S=b)=0.75$ on average

What would happen if we used a MC/TD estimate of the value $V(A),V(B)$?

- MC estimate considers full track, but since the track with $A$ is still zero:
  - $V(A)=0$  
  - $V(B)=0.75$
- TD(0) estimate with infinite sampling propagates information by $V(A)=r+V(B)$ since $A\to B$ happens 100\%. 
  - $V(B)=0.75$ still the same
  - $V(A)=0+0.75=0.75$ is different
- essentially the difference arises because TD assumes/factors in the MDP process, whereas the MC method does not.

However, if we used our simulated model and ==sampled from it==, you could see the following data

| Data | Table Lookup Model | Sampled Data |
| :---: | :---:               | :---:         |
| <img src='stf_rl/image-20220608151144.png' style='zoom:50%;'/>| <img src='stf_rl/image-20220608151159.png' style='zoom:50%;'/>| <img src='stf_rl/image-20220608152748.png' style='zoom:70%;'/>|


where notice that in this case, if you were to estimate a value function using MC method (e.g. for planning), you will get:

- $V(A)=1$ instead
- $V(B)=0.75$ remains the same

notice that value functions now becomes closer to our TD estimates.

## Planning with a Model

Once we have a learnt model, we can:

1. **sample experience from model** $(S,A,P_\eta,R_\eta)$
2. apply planning algorithms 
   - **model-free** RL to samples
      - e.g. MC control/policy iteration
      - e.g. SARSA, Q-Learning
   - **Tree search**!  

However, if the model $(S,A,P_\eta,R_\eta)$ is imperfect, then obviously the performance of your planning will be limited by your model.

- in those cases it is important to know *where it has gone wrong*, e.g. by reasoning about its uncertainty (see previous section), before switching back to use model-free RL

But for here, we will focus on having a reasonbly good model, and how we can use tree search algorithms to compute the next step without explictly modelling the value functions.

### Forward Search

The idea here is simple. Since we have a model that can spit out next state and rewards, we can build a tree by **trying out different actions**:

<img src='stf_rl/image-20220608154536.png' style='zoom:50%;'/>

But then the question is, what is the next action to take if we are currently at $s_t$? For that we need to construct a **value of the next actions**, and one way is to consider:

<img src='stf_rl/image-20220608154845.png' style='zoom:50%;'/>

namely:

- the value of the action is the **mean** of its future states
- the value of a state is the **max** of its actions

> Why might this be helpful? Notice that we don't need to solve whole MDP, just ==sub-MDP starting from the current state==! 

However, to compute this at every step would be expensive. Do we have other approaches?

### Simulation-Based Search

On the other hand, we can also utilize the **model-free approaches**, so that given a model $M=(S,A,P_\eta,R_\eta)$ we can:

1. given a policy $\pi$
2. sample experience ($K$ episodes) using the **model**:
    $$
    \{ S_t^{k},A_t^{k},R_{t}^{k}, ...,S_T^{k} \}_{k=1}^{K}\sim M_\eta
    $$
    do model-free control algorithsm treating those simulated experience to be real:
    - Monte-Carlo Control (e.g. policy iteration) $\to$ Monte-Carlo Search
    - SARSA $\to$ TD-Search

#### Simple Monte-Carlo Search

Since we are considering MC methods, then the idea is:

1. Given a model $M_\eta$ and a simulation policy $\pi$ 
2. for each action $a \in A$ 
   1. simulate $K$ episodes from current state $s_t$ by *considering all possible next actions, but then follows $\pi$*:
      $$
      \{s_t,a,R_{t+1}^{k},S^{k}_{t+1},\pi(S^{k}_{t+1}), ..., S_T^{k}\}_{k=1}^{K}\sim M_\eta,\pi
      $$
   2. from this, we can compute the $Q^{\pi}$ function for each possible next action:
      $$
      Q^{\pi}(s_t,a) = \frac{1}{K} \sum\limits_{k=1}^{K} G_t^{k}(a)
      $$
      for $G_t^{k}$ is the discounted reward of the $k$-th episode after following action $a$.
3.  Then the current best action is:
    $$
    a_{t} = \arg\max_{a} Q^{\pi}(s_t,a)
    $$
    which is like ==one-step of policy improvement==

But can we do better by finding an optimal policy instead of only improving by one-step?

#### Monte-Carlo Tree Search

*Recall* that we can compute the optimal policy by considering the ==expectimax tree== that we have shown before:

<img src='stf_rl/image-20220608154845.png' style='zoom:50%;'/>

however, the size of this tree scales as $O((|S||A|)^{H})$ for a height of $H$. Therefore, doing this at each step would be computationally intractable.

Instead, we consider a more computationally efficent way to approximate this full tree.

> **Heuristics**: instead of simulting all possibilities, we can Iteratively construct and update tree by performing **$K$ simulation episodes** starting from the root state (i.e. $K$ rollout trajectories).
> - note that for this to work we need to have a "good" policy while generating those trajectories, which we shall discuss next
> - in fact, one key difference here against the Smple MC Search is that the *policy generating each rollouts would be improving as well*
> 
> Then, after the search is finished, simply consider:
> $$
> a_t=\arg\max_a Q^\pi(s_t,a)
> $$
> note that as the policy $\pi$ genearing those trajectories improves, eventually this will **converge** to $\pi \to \pi^{*}$ and $Q^{\pi} \to Q^{*}$. 

So we will consider two main changes as compared to the simple MC search:

1. rolling out $K$ trajecories will involve *changing policies* as well 
2. To evaluate the value of a tree node $i$ at state action pair $(s, a)$, *average over all rewards* received (instead of max and average) from that node onwards across simulated episodes in which this tree node was reached

> The key unasnwered question is: How to select what action to take during a simulated episode? **Upper Confidence Tree Search**!
> - UCT: borrow idea from bandit literature and treat each node where can select actions as a multi-armed bandit (MAB) problem
> - i.e. want to **rollout more on actions that previously looked good**, and spend less time on actions that previously looked bad

Therefore, we consider maintaining an **upper confidence bound** over reward of each arm $s,a$ of an episode $i$:

$$
U(s,a,i) = \hat{Q}(s,a,i) + c\sqrt{\frac{\log(N(s))}{N(s,a)}}
$$

for $\hat{Q}(s,a,i)$ is just an average reward by counting:

$$
\hat{Q}(s,a,i)= \frac{1}{N(s,a,i)} \sum\limits_{k=1}^{K} \sum\limits_{u=t}^{T} \mathbb{1}(i\in \mathrm{episd.k})G_k(s,a,i)
$$

so essentially we are treating each state node as a separate MAB. Then, using this, while simulating episode $k$ at node $i$, our "policy" is to **select action/arm with highest upper bound** to expand (or evaluate) in the tree:

$$
a_{ik}=\arg\max U(s,a,i)
$$

This implies that the ==policy== used to simulate episodes with (and expand/update the tree) can ==change across each episode==. 

All in all, we will sample new episodes by:

- **Tree policy**: pick actions for tree nodes to maximize $U(S, A)$, the upper confidence bound. e.g. use it until all next action has at least one sampled trajectory
- **Roll out policy**: pick actions randomly, or another policy. This is used when we met a state with no data

For a more detailed example of this, see the next section.

## Case Study: GO

Go is 2500 years old and is considered as the Hardest classic board game:

<img src='stf_rl/image-20220608170059.png' style='zoom:60%;'/>

However, in this case:

- instead of a need to model the game, since all the rules are known, we **do not need $M_\eta$** in this case, but **only do MCTS**.
- since it is a two-player game, we consider a **minimax tree** (min value for opponent wining) instead of a expectimax tree

For instance, we can consider a reward function of:
$$
R_{T} = \begin{cases}
1, & \text{if black wins}\\
0, & \text{if white wins}
\end{cases}
$$

and our aim is to, say let black win. Then for $\pi=(\pi_B,\pi_W)$:

$$
V_\pi(s)=\mathbb{E}_\pi\left[R_T|S=s\right]=\mathbb{P}[\text{black wins}|S=s]\\
V^*(s)=\max_{\pi_B}\min_{\pi_W}V_\pi(s)
$$

is a minimax problem. Then, essentially we consider sampling:

|$t=1$ | $t=2$ | $t=3$ | $t=4$ | $t=5$ |
|:----:|:----:|:----:|:----:|:----:|
|<img src='stf_rl/image-20220608173657.png' style='zoom:50%;'/>|<img src='stf_rl/image-20220608172736.png' style='zoom:50%;'/>|<img src='stf_rl/image-20220608172750.png' style='zoom:50%;'/>|<img src='stf_rl/image-20220608172808.png' style='zoom:50%;'/>|<img src='stf_rl/image-20220608172827.png' style='zoom:50%;'/>|

where essentially for the first 5 sampled episodes:

- the choice of opponent $\pi_W$ could be the agent at previous iteration (i.e. doing self-play)
- the default policy could just be a random policy
- we are performing the **default/rollout policy** for states that we have not seen before
- perform **tree policy** with picking the upper confidence bound for states when we have seen all actions.


> **Importance of self-play**: essentially the idea is that in the beginning, both current agent and the opponent (e.g. previous checkpoint) would have a similar level. Then this makes the *reward signal being more dense* and the model learns faster.

And finally, for minimax tree, it looks like:

<img src='stf_rl/image-20220608174045.png' style='zoom:60%;'/>

where we are trying to minimize the probability of white winning and maximize the probability of black winning (as we are learning the policy for black).

**Advantages for MC Tree Search**:

- Highly **selective** best-first search
- Evaluates states dynamically (unlike e.g. DP)
- Uses **sampling** to break curse of dimensionality (model tree)
- Works for black-box models (only requires samples)
- Computationally efficient, anytime, parallelisable


# Safe Offline Reinforcement Learning

As we have known so far, RL algorithms updates/improves itself via interacting with the environment, which in some cases might not be feasible. (e.g. training conversation bot, but since its interaction is with people, it is often not possible).

<img src='stf_rl/image-20220821203131.png' style='zoom:50%;'/>


> Therefore, the aim of offline RL is to ==reuse previously collected datasets== (e.g. ImageNet) to create a data-driven RL framework.

Recall that some techniques we covered so far:

| Technique | Visual | Comments |
| :--:        |     :--: |   :--:    |
| on-policy RL|<img src='stf_rl/image-20220821203859.png' style='zoom:50%;'/> | continuously collects new data from environment
| on-policy RL|<img src='stf_rl/image-20220821204030.png' style='zoom:50%;'/>| collects new data after each policy update
| off-policy RL(sometimes called batch RL) | <img src='stf_rl/image-20220821204141.png' style='zoom:100%;'/> | we only have a buffer, no environment to interact with

Note that just as in off-policy RL, the policy we trained $\pi_\beta$ is different the policy that is used for data collection $\pi$.

---
In particular, consider solving the following problem:

<img src='stf_rl/image-20220821205626.png' style='zoom:80%;'/>

where essentially:
- we cannot interact with the student/let them do more experiments
- hence, we only have two sets of data, which you can think of as:
  $$
  \begin{cases}
    (s_{1} =\text{A}, a_{1} = \text{add}, a_{2} = \text{multiply}, r=95)\\
    (s_{1} =\text{B}, a_{1} = \text{multiply}, a_{2} = \text{add}, r=92)
  \end{cases}
  $$
- so the task is to learn a policy **only from a fixed set of data** but can **generalize**/reason about unseen cases

In addition to produing a new policy:

> ==Safe Offline/Batch RL== considers the following guarantee. Given past experience from some current policy $\pi$, we want to produce a new policy $\pi_\beta$ that
> - with the probability at least $1 - \delta$, will not be worse than $\pi_\beta$
> - guarantee should not contingent on tuning any hyperparamters
> 
> In general, we will be thinking of the **confidence** ($1-\delta$) we have on our predicted expected outcome using our policy $\pi_\beta$.

This is particularly useful in high stakes domain (e.g. medicine).

Finally, just to make the task clear:

> **Task**: given some dataset $D$, we want to learn the ==best possible policy $\pi_e$== from it. Notice that sometimes learning a very good policy might not even be possible if $D$ contains no good data (e.g. only visiting the first few rooms in Montezuma's Revenge). In general, it depends on the coverage and will be discussed in section [Importance Sampling for Policy Evaluation](#Importance Sampling for Policy Evaluation).

## Safe Offline RL Formulation

We will consider the following notation for consistency:

- Our Policy $\pi$: $\pi(a) = P(a_t = a|s_t=s)$
- Trajectory: $T=(s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, ..., s_{L}, a_{L}, r_{L})$
- Historical/Training Data: $D= \{ T_1, T_2, ..., T_n \}$
- Behavioral policy used to generated the data $D$: $\pi_b$
- Objective:
  $$
  V^{\pi} = \mathbb{E} \left[ \sum\limits_{t=1}^{L} \gamma^{t} R_{t} | \pi \right]
  $$

(note that in many cases we may not know what $\pi_b$ is, especially when it is generated by human. This will be discussed at the end of the section, **but for now we assume that it is known**.)

Then our aim is to have an algorithm $A$ learn a policy $A(D) \to \pi$ such that:
$$
P(V^{A(D)} \ge V^{\pi_{b}}) \ge 1 - \delta
$$

where the value of the behavioral policy $V^{\pi_{b}}$ can be estimated by MC estimation from the historical data, that $V^{\pi_b} (s) = (1/n)\sum\limits_{i=1}^{n} G_i(s)$ for $G_i(s)$ is the discounted return starting from state $s$ and ending at the end of the trajectory (and there are $n$ trajectories).

Or, more commonly, we could consider some external baselines $V_{\min}$ as well, that:
$$
P(V^{A(D)} \ge V_{\min}) \ge 1 - \delta
$$

Realize that to find out $V^{A(D)}$, we need to take our old data to **compute how good our new policy is**, which is intrinsically off-policy.

## Off Policy Policy Evaluation

> **Off Policy Policy Evaluation (OPE)**
> - for policy $\pi_e$ we want to evaluate (e.g. $A(D)\to \pi$), convert historical data $D$ into $n$ independent and unbiased estimates of $V^{\pi_e}$

Essentially:

<img src='stf_rl/image-20220821220503.png' style='zoom:50%;'/>

Therefore, our aim is:

> Estimate value of policy $\pi_1$, $V^{\pi_1}(s)$, but **given episodes $D= \{ T_1, T_2, ..., T_n \}$ generated under $\pi_2$**
> - recall that $V^{pi}(s) = \mathbb{E}_\pi [G_{t} |s_t=s]$
> - we want to learn an unbiased estimate of $V^{\pi_1}(s)$

### MC Off Policy Evaluation

Once mentioned unbiased, we should think of MC methods (as opposed to TD methods, which does sampling + bootstrapping). Additionally, this ==does not require the world to be Markov==, which in realiy is often not the case!

First, we realize that because the policies are different $\pi_{1} \neq \pi_{2} $, we have **two different distribution of states $\tau_{\pi_1}\neq  \tau_{\pi_2}$** (otherwise the policies have the same value)

| $\tau_{\pi_1}$  | $\tau_{\pi_2}$ |
| :--: | :--: |
| <img src='stf_rl/image-20220821231215.png' style='zoom:100%;'/> | <img src='stf_rl/image-20220821231318.png' style='zoom:100%;'/>

Suppose that $\pi_{1}, \tau_{\pi_1}$ is known/collected. The idea is: how do we change the distribution to that they look alike? ==Importance Sampling==!
- essentially reweight the samples from $\tau_{\pi_2}$ so that samples from $\tau_{\pi_1}$

### Importance Sampling for Policy Evaluation

> **Importance Sampling**: goal is to estimate the expected value of a function $f(x)$ under some probability distribution $q(x)$, without knowing the distribution $q(x)$.
>
> - we want to know:
>   $$
>   \mathbb{E}_{x \sim q}[f(x)]
>   $$
> - but we only have samples from $p(x)$, so that $x_1, x_2, ..., x_n$ sampled form $p(x)$
> - (just swap $x \to s$, and $f(x)\to G_(s)$ you should see why this could be helpful)

Under few assumptions, we can use those samples to obtain an unbiased estimate of $\mathbb{E}_{x \sim q}[f(x)]$
$$
\begin{align*}
  \mathbb{E}_{x\sim q}[f(x)] 
  &= \int_{x} q(x)f(x)dx\\
  &= \int_{x} \frac{p(x)}{p(x)}q(x) f(x) dx\\
  &= \int_{x} p(x) \left[ \frac{q(x)}{p(x)} f(x)\right] dx\\
  &\approx \frac{1}{n} \sum\limits_{i} \frac{q(x_i)}{p(x_i)}f(x_i),\quad x_i\sim p(x) 
\end{align*}
$$
so that essentially we sample data $x$ from $p(x)$, but ==reweigh== the probability so that it looks the same as if the came from $x\sim q(x)$

Therefore, back in the case of RL, let $T_j$ be the episode $j$ of states actions and rewards (that are collected/sampled by $p(x)$):
$$
T_{j} = (s_{j,1}, a_{j,1}, r_{j,1}, s_{j,2}, a_{j,2}, r_{j,2}, ..., s_{j,L}, a_{j,L}, r_{j,L})
$$

We need to first compute the ratio of $p(T_j|\pi_e)$ (the $q(x)$) over $p(T_j|\pi_b)$ (the $p(x)$):

$$
p(T_j|\pi_e) =p(s_{j,1}) \prod_{i=1}^{L}\underbrace{p(a_{j,i}|s_{j,i})}_{\mathrm{policy}} \underbrace{p(r_{j,i}|s_{j,i},a_{j,i})}_{\text{reward model}} \underbrace{p(s_{j,i+1}|s_{j,i},a_{j,i})}_{\text{transition model}}
$$

Then if we compute the ratio:

$$
\begin{align*}
  \frac{p(T_j|\pi_e)}{p(T_j|\pi_b)}
  &= \frac{p_{\pi_e}(s_{j,1})}{p_{\pi_b}(s_{j,1})} \prod_{i=1}^{L} \frac{p_{\pi_e}(a_{j,i}|s_{j,i})}{p_{\pi_b}(a_{j,i}|s_{j,i})}\\
  &= \prod_{i=1}^{L} \frac{p_{\pi_e}(a_{j,i}|s_{j,i})}{p_{\pi_b}(a_{j,i}|s_{j,i})}\\
  &= \prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})}
\end{align*}
$$

where 
- in the first equality, the over terms cancel since they do not depend on the policy chosen. 
- in the second equality, we are assuming the start state is the same for both policies

Notice that the interpretation here is simple. Consider comparing the value of the policy using its start state, then:
- we have a bunch of trajectories and computed expected rewards $(T_1,G_1),(T_2,G_2),...,(T_n,G_n)$ sampled from following $\pi_b$
- then the value of this policy $\pi_b$ can be estimated by $(1/n)\sum\limits G_i$
- but to estimate the value of $\pi_e$, we ==reweigh the probabilities by thinking how likely those trajectories are to be generated by $\pi_e$==. So that if it is more likely, i.e. $p(T_j|\pi_e)>p(T_j|\pi_b)$, we will increase the weight using the above ratio.

In general, $s_{j,1}$ can be any state as you can just rederive the above by considering $p(T_j|\pi_e,s=s_{j,1})$.

Finally, this gives us the estimate:

$$
\begin{align*}
  V^{\pi_e}(s)
  &\approx \frac{1}{n} \sum\limits_{j=1}^{n} \frac{p(T_j|\pi_e,s)}{p(T_j|\pi_b,s)} G(T_j)\\
  &= \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})} \right) G(T_j)
\end{align*}
$$

Finally, since future cannot affect past rewards, we typically consider rewards only up to the timestep $t$ to reduce variance in our estimate:

$$
\begin{align*}
  V^{\pi_e}(s)
  &\approx \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})} \right) \left( \sum_{t=1}^L \gamma^T R_t^{i} \right)
  &\equiv \frac{1}{n} \sum\limits_{j=1}^{n} w_i \left( \sum_{t=1}^L \gamma^T R_t^{i} \right)
\end{align*}
$$
note than since sometimes the importance weights $w_i$ can become very small, there is a Weighted Importance Sampling (WIS) algorithm to deal with this tha basically switch $1/n$ ratio to $1/\sum w_i$ (however, this increase bias while reduces variance)

Now, what are the assumptions used to make IS work? You might notice the term $\frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})} $ could have gone badly, and it is extactly the case

> **Importance Sampling Assumptions**: since we are reweighing samples from $\pi_b$, if we have distributions that are non-overlapping, then this will obviously not work.
> - in particular, if we have any single case that $\pi_b(a|s)=0$ but $\pi_e(a|s)>0$, then this will not work.
> - therefore, for this to work, we want to have a large ==coverage==: so that for $\forall a,s$ such that $\pi_e(a|s)>0$, you want $\pi_b(a|s)>0$.

Intuively, this means that if $\pi_e$ is not too far off from $\pi_b$, then the importance sampling would work reasonably.

### Adding Controlling Variates

Given some random variable $X$, we want to estimate $\mu = \mathbb{E}[X]$.

- we have our estimator being $\hat{\mu}=X$
- unbiased estimator: $\mathbb{E}\left[\hat{\mu}\right]=\mathbb{E}\left[X\right]=\mu$
- variance of this estimator: $\mathrm{Var}\left[\hat{\mu}\right]=\mathrm{Var}\left[X\right]$

Now, we want to show that in many cases we can use some tricks to reduce the variance while still having an unbiased estimator. Consider another random variable $Y$:

- let our new estimator be $\hat{\mu}=X-Y+\mathbb{E}[Y]$
- it is still unbiased because:
  $$
  \mathbb{E}[\hat{\mu}] = \mathbb{E}[X-Y + \mathbb{E}[Y]] = \mathbb{E}[X] - \mathbb{E}[Y] + \mathbb{E}[Y] = \mathbb{E}[X]
  $$
- it however can get a lower variance:
  $$
  \mathrm{Var}[\hat{\mu}] = \mathrm{Var}[X-Y+\mathbb{E}[Y]]=\mathrm{Var}[X-Y] = \mathrm{Var}[X] + \mathrm{Var}[Y] - 2 \mathrm{Cov}(X,Y)
  $$
  therefore, ==we just need $2\mathrm{Cov}(X,Y) > \mathrm{Var}[Y]$== to lower the variance in total while not introducing bias. Note that this might sound like some free lunch, but it is not because we are using $Y$ that has some information about $X$ since $2\mathrm{Cov}(X,Y) > \mathrm{Var}[Y]$.

---

Therefore, we can use this fact and use $X$ being the **importance sapmling estimator**, and $Y$ being a **control variate** build from an approxiamte model of the MDP (e.g. a Q value estimate). This gives a Doubly Robust Estimator (Jiang and Li, 2015)

> **Doubly Robust Estimator**: robust to (1) poor approximate model/control variate, and (2) error in estimates of $\pi_b$ (importnace sampling) because:
> - if the model is poor, the estimates $V^{\pi_b}$ is still unbiased
> - if importance sampling could not see some $\pi_b$ state/actions, but the model/contorl variate is good, then MSE will still be low.

And just to briefly show the equation:

$$
DR(\pi_{e} |D) = \frac{1}{n} \sum\limits_{i=1}^{n} \sum\limits_{t=0}^{L} \gamma^{t} \underbrace{w_i}_{\text{IS weights}} (R_t^{i} - \underbrace{\hat{q}^{\pi_e}(S_t^{i},A_t^{i})}_{\text{Y}})+\gamma^{t} \rho_{t-1}^{i}\underbrace{\hat{V}^{\pi_e}(S_t^{i})}_{\mathbb{E}[Y]}
$$

---

Finally, we can show the properties of those different algorithms empically by simulating a small grid world (e.g. 4x4 world), providing the algorithm with some data collected under polict $\pi_b$ and ask it to evaluate the value of some new policy $\pi_e$:

<img src='stf_rl/image-20220822195359.png' style='zoom:80%;'/>

where:
- the vertical axis is the MSE, which is $\hat{V}^{\pi_e}-V^{\pi_e}$ for $V^{\pi_e}$ is known beforehand since the world is simulated
- so we want to get low MSE with small amount of data

## High confidence Off-policy Policy Evaluation

Now, we have had some methods of how to obtain an OPE of $\pi_e$. The question is how confidence we are of our estimates?

> **High-confidence off-policy policy evaluation (HCOPE)**
> - Use a concentration inequality to convert the $n$ independent and unbiased estimates of $V^{\pi_e}$ into a $1-\delta$ confidence lower bound on $V^{\pi_e}$

You can often think of such as confidence as the confidence used for RL exploration: how confident are we in terms of the new actions we are taking?

Essentially:
<img src='stf_rl/image-20220821222311.png' style='zoom:50%;'/>

To be be able to know the confidence interval, we first need to revisit Hoeffding's inequality:

> **Hoeffding's Inequality**: let $X_1, X_2, ..., X_n$ be $n$ IID random variables such that $X_i\in[0.b]$. THen with the probability at least $1-\delta$:
> $$
> \mathbb{E}[X_i]\ge \frac{1}{n} \sum\limits_{i=1}^{n} X_i - b \sqrt{\frac{\ln(1/\delta)}{2n}}
> $$
> where $X_i=\frac{1}{n}\sum\limits_{i=1}^{n} (w_{i} \sum\limits_{t=1}^{L} \gamma^{T} R_{t}^{i})$ in our case

However, the problem with applying this directly is that we can get very high $b$ since the weights $w_i$ might become big for rare but successful events. 
- for instance, if we you have 10,000 trajectories/samples, we could get a 95% confidence lower bound of the policy's vlaue being $-5,8310,000$
- whereas the true value would be $0.19$

Graphically:

| IS weighted Return | Idea |
| :--:              | :--: |
|<img src='stf_rl/image-20220822204110.png' style='zoom:60%;'/>|<img src='stf_rl/image-20220822204215.png' style='zoom:70%;'/>|

where by cutting the large weights off, we will get a lower expected value but not have confidence interval exploding. Therefore, in practice, we can:

- take 20% of the data to optimize the cutoff
- use 80% of the data to compute lower bound

results for a certain policy on mountain climbing gives (true value $0.19$)

<img src='stf_rl/image-20220822204717.png' style="zoom: 80%;"/>

where the other columns are other concentration based methods.

## Safe Policy Improvement

> Safe policy improvement (SPI)
> - Use HCOPE method to create a safe batch reinforcement learning algorithm, by doing:
>   $$
>   \pi = \arg\max_{\pi_e} V^{\pi_e}
>   $$
>   with some confidence intervals

Essentially:

<img src='stf_rl/image-20220821222520.png' style='zoom:50%;'/>

for instance, if there is too little data, we should be able to say no that we can have some new policy/improved policy is safe.

Practically, some current approach involve

<img src='stf_rl/image-20220822205037.png' style='zoom:80%;'/>

Note that this is a algorithm that only **improves the policy for a single step**. Other future areas/goals involve improving the policy over multiple steps.

# General Offline RL Algorithms

> A relevant content will be UCB CS285 on YouTube. [link](https://www.youtube.com/watch?v=NV4oSWe1H9o&list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&index=71)

Besides those safety constraints, there are also many other offline RL algorithms that are worth discussion. Before going to any details, we want to make certain high level ideas clear. Given this dataset $D$:

- can offline RL perform **better than the best behavior in $D$**? Possibly yes, because of generalization: good behavior in one place may sufggst good behavior in another place
- can offline RL learn the **best policy just from $D$**? Often no, because we might have very few or no samples in $D$ where we see traces of the best policy, hence it is not even possible.

<img src='stf_rl/image-20220822210525.png' style='zoom:60%;'/>

But more generally, we consider:
- $D = \{(s_i, a_i, s_i^{\prime}, r_i\}$ the same as before
- $\pi_b$ is the behavior policy creating the data $D$, so that $s \sim \mu_{\pi_b}(s)$ and $a \sim \pi_b(a|s)$
- however, now we generally assume that ==$\pi_b$ is NOT known==
- we want to find a new policy $\pi_{\mathrm{new}}$ that can combine good decisions

And, as mentioned before, since $\pi_b \neq \pi_\mathrm{new}$ for $\pi_\mathrm{new}$ we want to learn, there is a problem of ==distribution shift==

> **Distribution Shift for learning Value Functions in Offline RL**: consider we are learning a value function $Q$. Idealy, if we want to learn $Q^{\pi_{b}}$, we consider the objective:
> $$
> \min_Q \mathbb{E}_{(s,a,s')\sim D} \left[  \left(\underbrace{r(s,a) + \mathbb{E}_{a'\sim \pi_b}[ Q(s',a')]}_{\mathrm{target}} - Q(s,a)\right)^2 \right]
> $$
> in practice, we can only do:
> - using sample mean instead of expcted value (as usual)
> - can only sample $a' \sim \pi_{e}$ since we do not have the behavioral policy, hence we are doing:
> $$
> \min_Q \mathbb{E}_{(s,a,s')\sim D} \left[  \left(\underbrace{r(s,a) + \mathbb{E}_{a'\sim \pi_\mathrm{new}}[ Q(s',a')]}_{\mathrm{target}} - Q(s,a)\right)^2 \right]
> $$
> which we only expect to work if $\pi_\mathrm{new}(a|s) \approx \pi_b(a|s)$.

However, in reality we usually consider having:
$$
\pi_{\mathrm{new}} = \arg \max_{\pi} \mathbb{E}_{a \sim \pi(a|s)} [Q(s,a)]
$$

so that the $\pi_\mathrm{new}$ learnt usually ends up fooling us by adverserially picking large values in the (inaccurate) Q function estiamte. Graphically:

| $\hat{Q}$ estimate of $\pi_\mathrm{new}$ | Actual Return of $\pi_\mathrm{new}$ |
| :--:                      | :--:                                |
| <img src='stf_rl/image-20220826192651.png' style='zoom:60%;'/> |<img src='stf_rl/image-20220826192630.png' style='zoom:60%;'/> |

Therefore, what can we do this such a problem?

## Policy Constraint Methods

A simple and a bit naive way is to consider a policy $\pi_\mathrm{new}$ such that:

$$
\pi_\mathrm{new}(a|s) = \arg\max_\pi \mathbb{E}_{a \sim \pi(a|s)} [Q(s,a)],\quad \mathrm{s.t.} D_{KL}(\pi||\pi_b) \le \epsilon
$$

and then continue with the Bellman update:

$$
Q(s,a) \gets r(s,a) + \mathbb{E}_{a' \sim \pi_\mathrm{new}} [Q(s',a')]
$$

while this seems to solve distribution shift, there are several issues:

1. usually we don't know what $\pi_b$ is (e.g. human collected data)
2. if you are too close to $\pi_b$, then you cannot improve your policy, even if your data $D$ already has a large support/coverage (e.g. $\pi_b$ is uniformly random)

But before we consider other methods, let us dive into more details about how/when this idea could work. Consider the case that you are at some state $s$ and given some rewards for some actions in your $D$:

| Data | Fitted Q Function | Constrained New Policy $\pi$ |
| :--: | :--: | :--:|
|<img src='stf_rl/image-20220826211857.png' style='zoom:100%;'/>|<img src='stf_rl/image-20220826211906.png' style='zoom:80%;'/>|<img src='stf_rl/image-20220826212034.png' style='zoom:80%;'/>|

where:

- the orange dots are the given $Q$ values
- the middle fitted $Q$ function would give bad policy exploiting the tails
- the last one seems better, but it is still giving a bit too high Q values to non-optimal actions

Therefore, given this intution, we would prefer to have a support policy (instead of $\pi_b$) to look like:

<img src='stf_rl/image-20220826212548.png' style='zoom:50%;'/>

so that KL-divergence would work the best. This means we consider the support constraint:

$$
\pi(a|s) \ge 0 \text{ only if } \pi_b(a|s) \ge \epsilon 
$$

which we can approximate using methods such as MMD as we don't have $\pi_b$ directly, but in practice they are very difficult to implement.

---

Finally, here we show some examples of how to ==explicitly include policy constraints== in practice:

1. consider the case of using Actor Critic methods, where the actor $\pi_\theta(a|s)$ has the objective:
   $$
   \theta \gets \arg\max_\theta \mathbb{E}_{s \sim D} \left[ \mathbb{E}_{a\sim \pi_\theta(a|s)} [Q(s,a)] \right]
   $$
   but for KL divergence constraint, we want:
   $$
   D_{KL}(\pi_\theta||\pi_b)  = \mathbb{E}_{\pi_\theta}[\log \pi_\theta(a|s) - \log \pi_b(a|s)] = \mathbb{E}_{\pi_\theta}[\log \pi_b(a|s)] - \mathcal{H}(\pi_\theta)
   $$
   for $\mathcal{H}(\pi_\theta)$ is the entropy. We can then easily convert constrained problems into unconstrained optimization using Legrange's Methods:
   $$
   \theta \gets \arg\max_\theta \mathbb{E}_{s \sim D} \left[ \mathbb{E}_{a\sim \pi_\theta(a|s)} [Q(s,a)+ \lambda\log \pi_b(a|s)] + \lambda \mathcal{H}(\pi_\theta(a|s)\right]
   $$
   for $\lambda > 0$ is the Legrange Multiplier. But again, this means you need to know $\pi_b$, which unfortunatly you cannot avoid and commonly is done by doing behavioral cloning on the dataset $D$.
2. Another simple way is to change the reward function:
   $$
   \bar{r}(s,a) = r(s,a) - D_{KL}(\pi||\pi_b)
   $$
   which can not only makes current behavior more consistent, but also accounts for future behavior/divergence. This form of approach is called ==Behavior Regularized Offline RL==

But again, generally best offline RL algorithms today do not use these methods.

## Implicit Policy Constraint Methods

Recall the for policy constraint, we are considering the problem:

$$
\pi_\mathrm{new}(a|s) = \arg\max_\pi \mathbb{E}_{a \sim \pi(a|s)} [Q(s,a)],\quad \mathrm{s.t.} D_{KL}(\pi||\pi_b) \le \epsilon
$$

and by using Legrange Duality and solving for the optimal policy we get:

$$
\pi^{\star}(a|s) = \frac{1}{Z(s)} \pi_b(a|s) \exp\left( \frac{1}{\lambda}A^{\pi}(a|s) \right)
$$

which intuitively means that:

- suboptimal actions are exponentially less likely
- even for very high advantage $A$, if the action is rare in behavior policy $\pi_b$ then this is bad
- $\lambda$ can be often treated as a hyperparameter or tuned

Since this is the best policy, then it means we can approximate using weighted maximum likelihood:

$$
\pi_\mathrm{new}(a|s) = \arg\max_\pi \mathbb{E}_{(s,a)\sim \pi_b(a|s)}  \left[ \log\pi(a|s)  \underbrace{\frac{1}{Z(s)}\exp\left(\frac{1}{\lambda}A^{\pi_{\mathrm{old} (s,a)}}\right)}_{\text{weights }w(s,a) } \right]
$$
where the advantage function is just given by critic. In a sense this is a **weighted behavior cloning**, so that it is imitating the good actions more than the bad actions.

To implement this, all you need to do is to change slightly the actor-critic training:

1. train critic as usual with the loss:
   $$
   L_C(\phi) = \mathbb{E}_{(s,a,s')\sim D} \left[ (Q_\phi(s,a) - ( r(s,a)+\gamma \mathbb{E}_{a'\sim \pi_\theta(a'|s')}[Q_\phi(s',a')] ))^{2}  \right]
   $$
2. train actor with our constrained loss:
   $$
   L_A(\theta) = - \mathbb{E}_{(s,a)\sim \pi_b(a|s)}  \left[ \log\pi_\theta(a|s)  \frac{1}{Z(s)}\exp\left(\frac{1}{\lambda}A^{\pi_{\mathrm{old} (s,a)}}\right) \right]
   $$
3. update the actor and critic loss via gradient descent alternatingly

More details see this paper [Nair, Dalal, Gupta, Levine. 2020](https://arxiv.org/pdf/2006.09359.pdf). Some more caustions of this method include the fact that you could still query OOD actions **during** training (constraints respected only at expectation), which could happen when computing the target value for updating critic or when estimating the advantage function.

### Implicit Q-Learning

Is there a way to avoid OOD actions while learning the Q function? This serves for the motivation of the work [Kostrikov, Nair, Levine. 2021](https://arxiv.org/pdf/2110.06169.pdf), with the idea being formulated as follows.

1. we consider instead of the following update rule
   $$
   Q(s,a) \gets r(s,a) + \mathbb{E}_{a' \sim \pi_{\mathrm{new}}}[Q(s',a')]
   $$
   we view it as 
   $$
   Q(s,a) \gets r(s,a) + \underbrace{V(s')}_{\text{just another neural network}}
   $$
2. next, we recall that a batch constrained TD updates considers:
   $$
   L(\theta) = \mathbb{E}_{(s,a,s')\sim D} \left[ ( r(s,a) +\gamma \max_{\pi_b(a'|s')>0}Q_{\hat{\theta}} - Q_\theta(s,a))^2 \right]
   $$
   thefore, to mimic this behavior, we have two choices:
   - using expectile loss $L_2^{\tau}$ directly on $Q$, so that we have:
      $$
      L(\theta) =  \mathbb{E}_{(s,a,s')\sim D} \left[ L_2^\tau( r(s,a) +\gamma \max_{\pi_b(a'|s')>0}Q_{\hat{\theta}} - Q_\theta(s,a)) \right]
      $$
      (explaination of expectile loss seen in the next paragraph) but this have a drawback, and that is since each update sample contains $(s,a,s')$, it includes the stochasticity of a sample getting high reward because of *lucky* transition to $s'$ instead of doing the correct action $a$. This we do not want to have in our learned critic/policy.
    - using expectile loss to first fit a value function $V$:
      $$
      L_V(\psi) = \mathbb{E}_{(s,a) \sim D} \left[ L_2^{\tau}(Q_{\hat{\theta}}(s,a) - V_\psi(s)) \right]
      $$
      which uses expectile because we want $V_\psi$ to learn the upper end in the distribution to approximate $\max Q_{\hat{\theta}}$, and then we simply use that and do:
      $$
      L_Q(\theta) = \mathbb{E}_{(s,a,s')\sim D} \left[ ( r(s,a) +\gamma V_\psi(s')- Q_\theta(s,a))^{2} \right]
      $$
      which notice that we do avoided the case of lucky transition while we are training the $V_\psi$ approximation function.
   
   note that one concern could be: "before we were worried about $Q(s,a)$ giving unlikely large values, but now we are fitting $V$ to get large values?" The answer is no, because the **previous problem is due to the fact that it comes from OOD actions**, but here $V$ is trained only in data and hence have no overestimation problem.

Finally, we graphically provide interpretation of why this expectile and the $V_\psi$ function would work. Consider the case that you have multiple trajectories, and for a continous/large action space, you might see:

<img src='stf_rl/image-20220827004946.png' style='zoom:60%;'/>

Suppose you are on the green circle state. Even though there is only one action, you can still consider the value of this state being a distribution because its neighbor states/trajectories has a different action. Therefore, this leads up to the following value function distribution from your data:

<img src='stf_rl/image-20220827005200.png' style='zoom:60%;'/>

where the probability of each value is induced by actions (e.g. neighbor/similar states) only. Then, if we go back to the objective of fitting a value function:

$$
V = \arg\min_{V} \frac{1}{N} \sum\limits_{i=1}^{N} L(V(s_i), Q(s_i,a_i))
$$

taking:

| MSE Loss | Expectile Loss, $x=V_\psi(s)-Q_{\hat{\theta}}(s,a)$ |
| :-------: | :-----------: |
| $(Q_{\hat{\theta}}(s,a) - V_\psi(s))^{2}$ | <img src='stf_rl/image-20220827010014.png' style='zoom:50%;'/> |
|<img src='stf_rl/image-20220827010118.png' style='zoom:60%;'/> | <img src='stf_rl/image-20220827010125.png' style='zoom:60%;'/>|

This means that when fitting the $V$:

| MSE Loss | Expectile Loss |
| :-------: | :-----------: |
| <img src='stf_rl/image-20220827010746.png' style='zoom:60%;'/> | <img src='stf_rl/image-20220827010347.png' style='zoom:65%;'/> |

Finally, when this $V$ learns from the good actions at each state, it can be effectively combined to our $Q$ network from which we can extract the policy, as indicated in the paper/algorithm overview above.

## Conservative Q-Learning Methods

Instead of trying to fix actors by asking them not to sample OOD actions, CQL methods try to repair critic directly. Specically, recall that our previous problem is:

| $\hat{Q}$ estimate of $\pi_\mathrm{new}$ | Actual Return of $\pi_\mathrm{new}$ |
| :--:                      | :--:                                |
| <img src='stf_rl/image-20220826192651.png' style='zoom:60%;'/> |<img src='stf_rl/image-20220826192630.png' style='zoom:60%;'/> |

SO essentially, if the idea is to push down large Q values in general (as they are likely to be wrong), being rather pessimistic:

| Fitted Q vs Real Q | Conservative Q-Learning |
| :--:                | :--:                    |
| <img src='stf_rl/image-20220827115742.png' style='zoom:50%;'/> |  <img src='stf_rl/image-20220827115829.png' style='zoom:50%;'/> |

a simple idea to achieve this is to consider the Q value objective:

$$
\hat{Q}^{\pi} = \arg\min_{Q} \,\,\max_\mu \underbrace{\alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]}_{\text{push down big Q-values}}
+ \underbrace{\mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]}_{\text{regular objective}}
$$

where notice that $\mu$ is picked so that $Q$ looks large, and then the final $Q$ is picked to minimize those. Hence in theory, it can be shown that if you pick a large enough $\alpha$, you can show that $\hat{Q}^\pi \le Q^{\pi}$ for the true $Q^\pi$.

But in reality, this generally pushes down all the Q values, which could be a little too pessimistic. Therefore, we could maybe do better by considering:

$$
\hat{Q}^{\pi} = \arg\min_{Q} \,\,\max_\mu \underbrace{\alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]}_{\text{always pushes down Q}}
- \underbrace{\alpha \mathbb{E}_{(s,a)\sim D}[Q(s,a)]}_{\text{push up (s,a) in data}}
+ \underbrace{\mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]}_{\text{regular objective}}
$$

where this is the CQL objective $L_{\mathrm{CQL}}$, such that:

- for $(s,a)$ samples in the dataset, the push up and down term will cancel the effect out and have little net effect
- for $(s,a)$ samples not in the dataset, the first term will push them down but the second will push up actions in dataset. This means that for the next time, $\mu$ would tend to select actions *similar to the dataset* since they have large values. 

Therefore, the intuition is that :
- if you meet OOD actions, the two terms will act to push them back to in-distribution actions.
- once they are in-distribution, the two terms more or less cancel out.

As a result:
- we no longer directly guarantee that $\hat{Q}^{\pi}(s,a) \le Q^\pi(s,a),\forall (s,a)$
- but we do guarantee that their expected value (i.e. value function) is bounded $\mathbb{E}_{\pi(a|s)}[\hat{Q}^{\pi}(s,a)] \le \mathbb{E}_{\pi(a|s)}[Q^{\pi}(s,a)], \forall s \in D$

Then, as shown in the paper [Kumar, Zhou, Tucker, Levine. 2020](https://arxiv.org/abs/2006.04779), implement RL algorithm with CQL:

1. update $\hat{Q}^{\pi}$ w.r.t $L_{\mathrm{CQL}}$ using the dataset
2. update your policy $\pi$:
   - if actions are discrete then simply:
      $$
      \pi(a|s) = \begin{cases}
        1, \text{if } a=\arg\max_a \hat{Q}(s,a)\\
        0 , \text{otherwise}
      \end{cases}
      $$
   - if actions are continous, then you usually have to fit another policy network $\theta$:
      $$
      \theta \gets \theta + \alpha \nabla_\theta \sum_i \mathbb{E}_{a \sim \pi_\theta(a|s)}[\hat{Q}(s_i,a)]
      $$
      which is basically like an actor-critic method.

Finally, to pick a $\mu$, we can first consider adding a max-entropy regularization term in our objective:

$$
\begin{align*}
  \hat{Q}^{\pi} 
  &= \arg\min_{Q} \,\,\max_\mu \alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]
- \alpha \mathbb{E}_{(s,a)\sim D}[Q(s,a)] - \overbrace{\mathcal{R}(\mu)}^{\text{regularizer}} \\
  &\quad + \mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]
\end{align*}
$$

then with the choice of taking $\mathcal{R}=\mathbb{E}_{s\sim D}[H(\mu(\cdot |s))]$, we will have an optimal choce with $\mu(a|s) \propto \exp(Q(s,a))$ and hence:
$$
\mathbb{E}_{a \sim \mu(a|s)} [Q(s,a)] = \log \sum\limits_{a} \exp(Q(s,a))
$$
so that:
- for discrete actions, we can just caulcate directly $\log \sum\limits_{a} \exp(Q(s,a))$
- for contionus actions, we can estimate $\mathbb{E}_{a \sim \mu(a|s)} [Q(s,a)]$ by using importance sampling

## Model-Based Offline RL

All the previous mentioned methods are model-free, i.e. we do not have a world model getting us the transitions/rewards. In the context of offline RL, model-based methods can be pretty useful as you can:
1. train a wolrd model using your data $D$
2. use that model to do planning directly or learn a policy from it

Usually, model-based RL methods work as follows:

<img src='stf_rl/image-20220827161230.png' style='zoom:50%;'/>

But since we cannot collect more data in offline RL, it means that:

| <img src='stf_rl/image-20220827161524.png' style='zoom:50%;'/> | <img src='stf_rl/image-20220827161544.png' style='zoom:50%;'/> |
| :---: | :---: |

so that essentially we are facing the same problem as those OOD actions in Q value estimates: now we are facing the threat that ==our policy could adversarially pick actions and transit to OOD states which the model could have some erroneously high values==.

Therefore, the general intuition for this is to apply some kind of penalty for policys that exploit too many OOD states/actions.

### Model-based Offline Policy Optimization

There are two papers on this topic of thought, which is:
- MOPO: Model-based Offline Policy Optimization (2020)
- MOReL: Model-based Offline Reinforcement Learning (2020)

where both of which **modifies the reward function** to trick the policy to stick more of less to data we have.

> **Uncertainty Penalty**: we would like to modify the reward function to penalize the policy for going into states that might be incorrect (due to inaccuracies in the learned model). Therefore, we in general consider:
> $$
> \tilde{r}(s,a) = r(s,a) + \lambda u(s,a)
> $$
> for $u$ is an *uncertain penalty* term.

While this sounds easy, it is usually hard to adjust the penalty term to make it work. Intuitvely, we would want:

| Before Penalty | After Penalty |
| :--: | :--: |
| <img src='stf_rl/image-20220827162535.png' style='zoom:50%;'/> | <img src='stf_rl/image-20220827162718.png' style='zoom:50%;'/> |

This means that this penalty term needs to be **at least as large as the error** the model makes at those erroneous states (hence to provide incentive for the policy to not exploit them). So how do we quantify this?

- one simple idea is to use **ensemble disagreement**: we train an ensemble of world models, and quantify the error by looking at the disagreement between the models.
- but of course there are other ways as well

Consider we are learning a policy $\hat{\pi}$, then in the MOPO paper it is proven that:

<img src='stf_rl/image-20220827163651.png' style='zoom:80%;'/>

where $M$ is the world model we trained, and $\nu$ measures the true return. This basically gave two important implications. As we can substitute the policy $\pi$ term:

1. Our learned policy can improve upon the behaviorial policy (in terms of the true return):
   $$
   \nu_M(\hat{\pi}) \ge \nu_M(\pi_b) - 2 \lambda \epsilon_u(\pi_b)
   $$
2. We can also quantify the gap between the learned policy and the true optimal policy in terms of model error:
   $$
   \nu_M(\hat{\pi}) \ge \nu_M(\pi^{\star}) - 2 \lambda \epsilon_u(\pi^{\star})
   $$
   and notice that if our model $M$ is very accurate, then $\epsilon_u$ is small and we are close to the optimal policy.

### Conservative Model-Based RL

> see the [COMBO paper](https://arxiv.org/pdf/2102.08363.pdf) for more details

Intuition: just like with CQL we can minimize the Q-value of OOD policy actions, we can minimize the **Q-value of model** for OOD state-action tuples.

$$
\begin{align*}
  \hat{Q}^{k+1}
  &\gets \arg\min_{Q} \beta ( \overbrace{\mathbb{E}_{(s,a) \sim \rho(s,a)}[Q(s,a)]}^{\text{push down}} - \overbrace{\mathbb{E}_{(s,a) \sim D}[Q(s,a)]}^{\text{push up}} )\\
  &+ \frac{1}{2} \mathbb{E}_{(s,a,s') \sim d_f}[ (Q(s,a) - \mathcal{B}^\pi \hat{Q}^{k}(s,a))^{2} ]
\end{align*}
$$

where $\rho(s,a)$ is from our model $M$, and $\mathcal{B}^\pi$ is a Bellman operator. This tries to make the Q-values from the models be worse, but Q-values from the dataset be better. Therefore, 

- if the model produces something that looks different from the dataset, then this objective can have $Q$ makes it look bad
- if the model produces something that is very close/indistinguishable from the dataset, then the first two terms cancel out

### Trajectory Transformer

> See [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/pdf/2106.02039.pdf)

The basic idea is:
1. consider trajectory as a whole, so we have a joint state-action model:
   $$
   p_b(\tau) = p_b(s_1, a_1, ..., s_{T}, a_T)
   $$
   which is the probabiliy under the behavioral policy.
2. then, we can use a transformer to model such a probability distribution.

Specifically, to deal with continuous states and actions, we discretize each dimension independently. Assuming N-dimensional states and M-dimensional actions, this turns $\tau$ into sequence of length $T(N +M +1)$:

$$
\tau = ( ... , s_t^1, s_t^{2}, ..., s_t^{N},a_t^{1},a_t^{2},...,a_t^{M},r_t, ...), \quad t=1,...,T
$$

so that subscripts on all tokens denote timestep and superscripts on states and actions denote dimension (i.e. $s_t^{i}$ is the $i$th dimension of the state at time $t$). While this choice may seem inefficient, it allows us to model the distribution over trajectories with more expressivity without simplifying assumptions such as Gaussian transition.

Then, we can model this as:

<img src='stf_rl/image-20220827173339.png' style='zoom:60%;'/>

where we can then **decode next state/actions given previous ones**, and note that both the model and search strategy are nearly identical to those common in natural language processing. 

Finally, to do planning, you need to keep in mind to take the action probabilities into account, so that you avoid OOD actions. But otherwise, you can:

- just use Beam search, but to promote non-myoptic chioces:
  1. given the current sequence (e.g. upto time stamp 3), sample the next $K$ tokens from the model (which means they are sampled from $p_b$)
  2. store the top $K$ tokens with highest cumulative reward $\sum_{t} r(s_t,a_t)$ instead of probability
  3. continue
- or you could even use MCTS

Why does/should this work?

- generating high-probability trajectories avoids OOD states and actions already, since we are sampling from $p_b$ when decoding.
- using a big model (e.g. transformers) tend to work well in offline mode

## Summaries and Directions on Offline RL

If you want to to **only train offline**:
- CQL: just one hyperparmeter and is well understood and widely tested
- Implicit Q-Learning: more flexible (offline+online), but has more hyperparameters

If you want to **train offline and finetune online**:

- not CQL because it tends to be too conservative
- Adavantage-weighted Actor-Critic (AWAC) is widely used and well tested
- Implicit Q-learning works well 

If you are confident that you **can train a good world model**:

- COMBO: similar properties from CQL but also benefits from being model-based
- Trajectory Transformer: very powerful, but is extremly computationally expensive to train and evaluate

<img src='stf_rl/image-20220827184217.png' style='zoom:60%;'/>

Finally, worthy mentions and challenages include that:
- as compared to supervised training where you can train/test offline, offline RL upto today still have to **test online** (costly and maybe even dangerous)
- statistically guarantees to help quantify **distributional shift** (currently pretty loose and incomplete)]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Stanford Reinforcement Learning:]]></summary></entry><entry><title type="html">APPH4901 AP Seminar</title><link href="/lectures/2022@columbia/APPH4901_AP_Seminar.html/" rel="alternate" type="text/html" title="APPH4901 AP Seminar" /><published>2022-09-07T00:00:00-04:00</published><updated>2022-09-07T00:00:00-04:00</updated><id>/lectures/2022@columbia/APPH4901_AP_Seminar</id><content type="html" xml:base="/lectures/2022@columbia/APPH4901_AP_Seminar.html/"><![CDATA[# Applied Physics Seminar

We will arrange speakers for each class, and it is recommended that you do some bio research/material readup.]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Applied Physics Seminar]]></summary></entry><entry><title type="html">COMS4705 NLP part2</title><link href="/lectures/2021@columbia/COMS4705_NLP_part2.html/" rel="alternate" type="text/html" title="COMS4705 NLP part2" /><published>2022-05-09T00:00:00-04:00</published><updated>2022-05-09T00:00:00-04:00</updated><id>/lectures/2021@columbia/COMS4705_NLP_part2</id><content type="html" xml:base="/lectures/2021@columbia/COMS4705_NLP_part2.html/"><![CDATA[Continuation from:

[NLP_part_1]: NLP.md	"NLP notes before midterm"

# Semantic Role Labeling

Here we consider solving the problem of, for each clause, determine the **semantic role played by each noun phrase** that is an argument to the verb. 

Well introduce 

- **semantic role labeling**, the task of assigning roles to spans in sentences, and
- **selectional restrictions**, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.

Examples would be:

<img src="NLP/image-20220302195745957.png" alt="image-20220302195745957" style="zoom:50%;" />

Then, this would be useful as it can act as a **shallow meaning representation** that can let us **make simple inferences** that aren't possible from the pure surface string of words:

- **Question Answering**, e.g. Who questions usually use Agents
- **Machine Translation Generation**

## Semantic Roles

A variety of semantic role labels have been proposed, common ones are:

- **Agent**: Actor of an action
- **Patient**: Entity affected by the action
- **Instrument**: Tool used in performing action.
- **Beneficiary**: Entity for whom action is performed
- **Source**: Origin of the affected entity
- **Destination**: Destination of the affected entity

Although there is no universally agreed-upon set of roles, the above list some thematic roles that have been used in various  computational papers

---

However, there are many **problems** with using those as-as. For instance, consider these possible realizations of the thematic arguments of the verb *break*:

<img src="NLP/image-20220302201246632.png" alt="image-20220302201246632" style="zoom: 80%;" />

These examples suggest that *break* has (at least) the possible arguments `AGENT`, `THEME`, and `INSTRUMENT`.

> The **set of thematic role arguments** taken by a verb is often called the **thematic grid**, q-grid, or **case frame**

An example would be for *break*:

<img src="NLP/image-20220302201350908.png" alt="image-20220302201350908" style="zoom:80%;" />

Additionally, researchers attempting to define role sets often find they need to **fragment** a role like `AGENT `or `THEME `into many specific roles. And it has proved difficult to **formally define** the thematic roles. This essentially leads to two class of models for solution

- define **generalized semantic roles** that abstract over the specific thematic roles.

  For example, `PROTO-AGENT` and `PROTO-PATIENT` are generalized roles that express **roughly** agent-like and roughly patient-like meanings, and those roles are not defined by some "necessary tabular conditions", but rather by **a set of heuristic features** that accompany more agent-like or more patient-like meanings.

  Then the **more an argument displays agent-like properties** (e.g. being volitionally involved in the event), the more likely it is to be classified as `PROTO-AGENT`.

- define **semantic roles that are specific** to a particular verb or a particular group of semantically related verbs or nouns

The first of them leads to the **PropBank** dataset, which uses both `proto`-roles and verb-specific semantic roles.

The second leads to **FrameNet** dataset, which uses semantic roles that are specific to a general semantic idea called a *frame*.

> **PropBank** is a verb-oriented resource, while **FrameNet** is centered on the more abstract notion of frames, which generalizes descriptions across similar verbs.
>
> Of course, both of which have sentences annotated with semantic roles.

## The Proposition Bank

Because of the difficulty of defining a universal set of thematic roles, the semantic roles in PropBank are defined with **respect to an individual verb sense**. Basically we have **verb sense-specific labels** (i.e. depending on the verb-sense, we will have different labels for arguments.)

In general, we will use labels names: `Arg0`, `Arg1`, `Arg2`, and so on:

- `Arg0 `represents the `PROTO-AGENT`
- `Arg1`, the `PROTO-PATIENT`
- `Arg2 `is often the benefactive, instrument, attribute, or end state
- `Arg3 `the start point, benefactive, instrument, or attribute, 
- `Arg4 `the end point

Examples include:

| ![image-20220302204216699](NLP/image-20220302204216699.png) | ![](NLP/image-20220302204206118.png) |
| ----------------------------------------------------------- | ------------------------------------ |

Additionally, PropBank also has a number of non-numbered arguments called `ArgMs`, (`ArgMTMP`, `ArgM-LOC`, etc.) which represent modification or adjunct meanings. As those are pretty much the same across verb senses, they are not listed with each frame file. However, they could be useful for training systems. Some examples of `ArgM`'s include:

<img src="NLP/image-20220302204509497.png" alt="image-20220302204509497" style="zoom:80%;" />

## FrameNet

> Whereas roles in the PropBank project are specific to an individual **verb**, roles in the FrameNet project are specific to a **frame**.

A **frame** is basically the holistic background knowledge that unites these words, such as:
$$
\text{reservation, flight, travel, buy, price, cost, fare}
$$
all defined with respect to a coherent chunk of common-sense **background information concerning air travel**.

> Therefore, FrameNet defines a set of **frame-specific semantic roles,** called **frame elements,** and includes a set of predicates that use these roles. (i.e. those roles of a word will be different across different frames)
> 
>- additionally, FrameNet also codes relationships between frames, allowing frames to inherit from each other, or representing relations between frames like causation

For example, the `change_position_on_a_scale` frame is defined as follows:

- This frame consists of words that indicate the change of an Items position on a scale (the `Attribute`) from a starting point (`Initial value`) to an end point (`Final value`).

And example sentences with their labels include:

<img src="NLP/image-20220302205208263.png" alt="image-20220302205208263" style="zoom: 80%;" />

so if a word/verb (predicate) is in this frame, the above would be the labels. Verbs/predicates used in this frame looks like:

<img src="NLP/image-20220302205628334.png" alt="image-20220302205628334" style="zoom:80%;" />

## Semantic Role Labeling Models

Now we get to the task of Semantic role labeling (sometimes shortened as SRL): automatically **finding the semantic roles of each argument of each predicate** in a sentence. This is often done by:

- supervised machine learning
- using FrameNet and PropBank to *specify predicates, define roles, and provide training and test sets*.

### Feature-based Algorithm for SRL

A simplified feature-based semantic role labeling algorithm basically do:

1. assign a parse (e.g. constituency parsing) of an input string
2. traverse the tree to find **all predicates**
3. for each **node that is a predicate**:
   1. do a classification on that node, any standard classification algorithms can be used.
   2. this can be done either by finding some **feature representation** of it, or using idea such as GNN

This results in a 1-of-$N$ classifier, i.e. where basically choosing one out of $N$ potential semantic roles (plus 1 for an extra `None` role for non-role constituents). The algorithm hence looks like:

<img src="NLP/image-20220302210527944.png" alt="image-20220302210527944" style="zoom:80%;" />

And the parse we did in step one could look like:

<img src="NLP/image-20220302210628528.png" alt="image-20220302210628528" style="zoom: 80%;" />

The general idea of those algorithms can be summarized into the following steps:

1. **Pruning**: Since only a small number of the constituents in a sentence are arguments of any given predicate, many systems use simple heuristics to prune unlikely constituents.
2. **Identification**: a binary classification of each node as an argument to be labeled or a `NONE`.
3. **Classification**: a 1-of-N classification of all the constituents that were labeled as arguments by the previous stage

However, since this **labels each argument of a predicate independently**, it is ingoing interactions between arguments, as we know the semantic roles are not independent.

Therefore, thus often **add a fourth step to deal with global consistency across the labels in a sentence**. This can be done by:

- local classifiers can return a list of possible labels associated with probabilities for each constituent, 
- a **second-pass** Viterbi decoding or re-ranking approach can be used to choose the best consensus label.

---

**Features** for Semantic Labelling Nodes

Common basic features templates mentioned above include

- Phrase type: The syntactic label of the candidate role, the **filler** (e.g. `NP`).
- Parse tree path: The path in the parse tree between the **predicate** and the candidate role filler.
- Position: Does candidate role filler precede or follow the predicate in the sentence?
- Voice: Is the predicate an active or passive verb?
- Head Word: What is the head word of the candidate role filler?

An example would be for the predicate *bit*:

<img src="NLP/image-20220302215527363.png" alt="image-20220302215527363" style="zoom: 50%;" />

---

Problems that this method will suffer:

- Due to **errors in syntactic parsing**, the parse tree is likely to be incorrect.
- Can have **many other useful features**.

### Neural Algorithm for SRL

> A simple neural approach to SRL is to treat it as a **sequence labeling task** like named-entity recognition, using the BIO approach.

Assume that we are given the predicate and the task is just **detecting and labeling spans**. This means that we will have:

- **Input**: sentence + the predicate, which will be separated by the `SEP` tag as shown in the example below
- **Output**: semantic labels + BIO tags, as each label is a constituent (can span over more than one word)
  - recall that BIO tags are: beginning, inside, outside.

An example architecture would be feeding it into a transformer:

<img src="NLP/image-20220302211635684.png" alt="image-20220302211635684" style="zoom:80%;" />

where of course those input word is mapped to pretrained embeddings.

Some problems that this method would encounter would be:

- Results may **violate constraints** like an action has at most one agent?

### Evaluation Metric for SRL

Since essentially we have **Identification** (should label or not) and **Classification** (which label), their performance can be evaluated separately:

- each **argument label** must be assigned to the **exact same parse constituent** as in the ground truth
- then, accuracy and recall can be used, for combined to look at F-score.

## Selectional Restrictions

We turn in this section to another way to represent facts about the **relationship between predicates and arguments**. 

> Frequently semantic role is indicated by a **particular syntactic position** . Examples include:
>
> - Agent: subject
> - Patient: direct object
> - Instrument: object of "with" `PP`
> - Beneficiary: object of "for" `PP`
> - Source: object of "from" `PP`
> - Destination: object of "to" `PP`

However, obviously this is **not always correct**.
$$
\text{The hammer hit the window.}
$$
then by the above logic *hammer* would be the `AGENT`, but it is actually an `INSTRUMENT` since it is not active.

Therefore, this means we also need to add **Selectional Restrictions**.

> A **selectional restriction** is a semantic type **constraint that a verb imposes** on the kind of concepts that are allowed to fill its argument roles.

For instance, consider the following two sentences:
$$
\text{I want to eat someplace nearby.}\\
\text{I want to eat Malaysian food.}
$$
where both involve the predicate *eat*:

- in the first case, we would want *someplace nearby* is an *adjunct* that gives the location of the eating event, **instead of direct object**
- in the second case, we would want *Malaysian food* to be *direct object*, **instead of adjunct**

Therefore, we see that:

- selectional restrictions are associated with senses, not entire lexemes
- yet another way is to instead specify **preference**, i.e. which one is preferred instead of which one is deferred, which we will see soon.

This can be very **useful** in that it can rule out **many ambiguities include**:

- **Syntactic Ambiguity**: John ate the spaghetti with *chopsticks*. would be wrong as `PATIENTS` of *eat* must be *edible*
- **Word Sense Disambiguation**: John *fired* the secretary. vs John *fired* the rifle.

But of course, it is difficult to acquire all of the selectional restrictions and taxonomic knowledge and applying them is also a problem.

---

In reality, taxanomic abstraction hierarchies or ontologies (e.g. hypernym links in **WordNet**) can be used to **determine if such constraints are met**.

- e.g. *John* is a Human which is a Mammal which is a Vertebrate which is an Animate

---

### Selectional Preferences

Early word sense disambiguation systems used this idea to rule out senses that violated the selectional restrictions of their governing predicates. However, soon it became clear that these **selectional restrictions were better represented as preferences** rather than strict constraints.

Basically there will be measurements being a probabilistic measure of the **strength of association between a predicate and a class** dominating the argument to the predicate. More details see the book.

# Advanced Semantics

This section aims to give a quick recap of **why we are using Pre-trained Language Models** today for so many tasks.

A quick recap. We used pre-trained **Embeddings** to do:

- e.g. skipgram word embeddings, compact dense representation for words
- captures **static semantic information** of a word

Yet in reality, tasks we need to consider usually includes doing things **after you got some embeddings**. This means having NN such as RNN/transformer for downstream tasks such as sentiment analysis. However, there we **also face problems if we want to train a model from scratch**: 

- need a collection of data + manual labeling
- often end up doing **transfer learning**: the training dataset is for move reviews, but for test we are working on Amazon product review

## Simple Transfer Learning

The idea with word embedding to have a *model learning some task agnostic data*, and then fine tune it to task specific data.

This has been commonly used for **fine-tuning word embeddings**, but you will see that we can also have a model **learn task agonistic objective** (e.g. a language model), and then **fine tune it for downstream task**:

<img src="NLP/image-20220309162325596.png" alt="image-20220309162325596" style="zoom:50%;" />

Then the **advantage of pre-trained model in this setting** would be:

- no need of large corpus, only a few for fine-tuning

- Can plug learned embeddings into the first layer

Additionally, for learning the task agnostic model, often:

- No labelled data required  just a large text corpus and do self-learning such as **masked word prediction** or **next word prediction**

### Problems with Pre-trained Word Embeddings

> Why can we **not** just use the pretrained embeddings? (but use pretrained language models)

One assumption/property we had so far for a word embedding include:

- **they are static**, hence does not care about the context, meaning the following would have the same
  - "She broke the windshield with a *bat*."
  - "He was driving like a *bat* out of hell."
- word embedding before learnt from **co-occurrence**, which **does not care about order!**

However, for an **entire network** trained only tasks such as language model related ones:

|                  Pretrained Word Embeddings                  |                  Pretrained Language Model                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP/image-20220310005529542.png" alt="image-20220310005529542" style="zoom:67%;" /> | <img src="NLP/image-20220310005553330.png" alt="image-20220310005553330" style="zoom:67%;" /> |

then you basically **remove the $\hat{y}$ layer and add a linear layer at the end** for transfer learning.

- e.g. include recurrent or Transformer layers to incorporate context
- Better parameter initializations, for make it fine-tunable

Now, we discuss **language model as a effective pretrain task** (e.g. next word prediction, masked word prediction, etc)

- It's self-supervised, so there's no need for labelled data. e.g. Large text collections are readily available on the web
- it's actually a difficult task, even for humans
  - A model would need to learn about syntax, semantics, and even some world knowledge in order to do well at this task
  - Luckily with a large number of parameters, large corpora, and lots of GPUs (or TPUs), we can do a pretty reasonable job

## Neural Language Model

*Recall*: N-gram Language Model

The N-gram language model (e.g. for next word prediction) has limitations:

- **Sparsity** (e.g. bigram matrix)
- Need to store ngrams (and counts)
  - Increases with n and corpus size
- Don't take into account word similarity

---

The solution to use **Neural Language Model**: predict the next word given the current context window

<img src="NLP/image-20220309164017685.png" alt="image-20220309164017685" style="zoom: 80%;" />

where the architecture is simply:

- input -> embedding layer to start with
- one or more intermediate layers, e.g. `Linear`, `RNN`, etc
- output $\vec{h}$ and attach a SoftMax layer
  - for fine-tuning the entire model, consider removing the softmax and attach another linear layer on top of $\vec{h}$ for your downstream task

This model **solved** several issues from N-gram models such as:

- we have no sparsity problem
- no need to store n-grams
- can capture word-similarity via embeddings

However, several **unsolved problems** include:

- Context window is limited (cannot just put in the entire document)
  - Still can't capture long range dependencies
- Increasing context window == increasing parameters

## LMs and Transfer Learning

Then the idea is that the LM task could be **useful for any downstream task**.

- intuition: predict the next word positive fromthe sentence: the movie is great; positive". Then we can use it as **sentiment analysis**

Therefore, you basically take the entire pre-trained model before the softmax layer as **initialization**, or as fine-tuning, or your downstream task. For instance:

- use an pretrained LSTM LM on a large corpus
- Use weights of embeddings and LSTM layers as initialization for the target task

Nowadays, besides LSTM LM, we have **many pretrained architectures** to choose from.

### Architecture Choices

In NLP we have **a lot of pretained language models**. For each, be careful to consider the following aspects:

- Model Architecture
- Pre-training Objective
- Pre-training Data
- Adaptation to downstream tasks

### Architecture Examples

> In reality, since the ==pretained model has a lot of parameters==, it is common to **freeze those weights** for the following models and **add a linear/NN layer** on top for your downstream task.

**GPT**

- Transformer decoder with 12 layers
  - 768 hidden units, 12 heads, 3072-dim feed forward layer
    (117M params)
- Pre-training objective: next word prediction
- Data:
  - BooksCorpus
  - 7K unpublished books covering a variety of genres (800M words)
  - Allows model to condition on long-range dependencies
- For GPT-2 and GPT-3
  - Larger models + more data = stronger LMs

then, the paper tested this model and applied to a variety of different downstream tasks.

---

**BERT**

- Pre-training objective: **masked word prediction** + **next sentence prediction**
  - LMs are unidirectional, but language understanding is bidirectional.
  - next sentence prediction: whether if the next sentence follows from the previous sentence

- 12 layer transformer encoder, 768 hidden units, 12 attention heads = 110M parameters
- Data
  - BooksCorpus (800M words)
  - English Wikipedia (2,500M words)
    - Text passages only
- Training time
  - 4 days on 4x4 or 8x8 TPU v2 slices
  - ~$7K to train BERT Large

Input architecture looks like

<img src="NLP/image-20220309170209027.png" alt="image-20220309170209027" style="zoom:50%;" />

where:

- segment embedding is to denote where we are putting the **separator**.

Then, using this model for **other downstream tasks** include

<img src="NLP/image-20220309170441560.png" alt="image-20220309170441560" style="zoom:50%;" />

Some importnat variants of BERT:

- RoBERTa
  - Next sentence prediction not necessary as objective
  - BERT + data + training steps
- SpanBERT
  - Mask out spans instead
    - e.g. which span of the reading/resource relates to the question that was asked
  - Significant improvements on span selection tasks (such as QA)

It turns out that this also can be tuned for **other languages**: https://github.com/google-research/bert/blob/master/multilingual.md

## Future of Pretrained Models

Currently the trend is 

1. to have an increasing size of model and data:
2. consider **zero/one/few-shot learning** instead of transfer learning

---

Model size

- BERT/GPT (2018) -> ~100M params
- GPT-2 (2019) -> 1.5B params
- T5 (2020) -> 11B param
- GPT-3 (2020) -> 175B params

Dataset sizes

- GPT (2018) -> 800M words
- BERT (2018) -> 3B words
- GPT-2 (2019) -> 40B words
- GPT-3 (2020) -> 500B words

---

Additionally, **different from the transfer learning paradigm**, recently we have been looking at **zero/one/few-shot learning**

|                          Zero-shot                          |                          One-shot                           |                           Few-hot                           |
| :---------------------------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------: |
| ![image-20220310010926634](NLP/image-20220310010926634.png) | ![image-20220310010905618](NLP/image-20220310010905618.png) | ![image-20220310010915069](NLP/image-20220310010915069.png) |

one key difference here is that ==we do NOT update weights==, we are only providing exmaples = providing a **context**.

# Machine Translation

> The task here is to **translate** one natural language into another. Some common usages include:
>
> - **pure translation**: Google translate
> - **computer-aided translation**: used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This is commonly used as part of **localization**: the task of *adapting* content or a product to a *particular language community*
> - **incremental translation**: translating speech on-the-fly before the entire sentence is complete
> - **Image-centric translation**: use OCR of the text on a phone camera image as input to an MT system to translate menus or street signs

The standard algorithm for MT:

- **statistical** methods (not used a lot today)

- the **encoder-decoder** network, also called the decoder sequence to sequence network, an architecture that can be implemented with RNNs or with Transformers. Why can we not just use an encoder/just an decoder?

  > Machine Translation needs a map from a sequence of input words or tokens to a sequence of tags that are **not merely direct mappings** from individual words, which is exactly what such an architecture is doing.

An example of MT task would be:
$$
\text{English}: \quad \text{He wrote a letter to a friend}\\
\text{Japanese}: \quad \underbrace{\text{tomodachi}}_{\text{friend}}\,\,\underbrace{\text{ni tegami-o}}_{\text{to letter}}\,\,\underbrace{\text{kaita}}_{\text{wrote}}
$$
which evinces some **key challenges** that makes the task difficult:

- **syntactical difference** amongst languages: in English, `verb` is in the middle while in Japanese, `verb` is at the end.
  - e.g. word ordering difference: `SVO` (e.g. English), `SOV` (e.g. Hindi), and `VSO` (e.g. Arabic) languages.
  - e.g. In some SVO languages (like English and Mandarin) `adjectives `tend to appear before `verbs`, while in others languages like Spanish and Modern Hebrew, `adjectives `appear after the `noun`
- **Pro-drop languages**: regularly omit subjects that must be inferred.
  - e.g. Chinese sometimes drop subjects

- **Morphological difference**
- **Lexical gaps**: there might not exist a one to one mapping for a word in foreign languages

> **Encoder-decoder** networks are very successful at handling these sorts of complicated cases of sequence mappings.

Indeed, the encoder-decoder algorithm is not just for MT; its the state of the art for many other tasks where complex mappings between two sequences are involved:

- summarization (where we map from a long text to its summary, like a title or an abstract)
- dialogue (where we map from what the user said to what our dialogue system should respond)
- semantic parsing (where we map from a string of words to a semantic representation like logic or SQL)
- and many others.

However, the current translation quality is **not perfect**:

- Existing MT systems can generate rough translations that at least convey the gist of a document
- High quality translations possible when specialized to narrow domains, e.g. weather forecasts.

## Language Divergence

This section discusses a bit more on the differences between languages that makes the task of MT difficult

> Languages differ in many ways, and an understanding of what causes translation such divergences will help us **build better MT** models. The study of these systematic cross-linguistic similarities and differences is called **linguistic typology**

### Word Order Typology

As we hinted it in our example above comparing English and Japanese, languages differ in the basic word order:

- Subject-Verb-Object order
- In some SVO languages (like English and Mandarin) `adjectives `tend to appear before `verbs`, while in others languages like Spanish and Modern Hebrew, `adjectives `appear after the `noun`

Visual example of word order difference hence complex mapping:

<img src="NLP/image-20220321190648445.png" alt="image-20220321190648445" style="zoom:67%;" />

### Lexical Divergences

Here we need to deal with problems such as:

- appropriate word/translation can vary depending on the **context**.
  - e.g. German uses two distinct words for what in English would be called a `wall`: `Wand `for walls inside a building, and `Mauer `for walls outside a building.
- Sometimes one language places more grammatical **constraints on word choice** than another.
- **lexical gap**: no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language.

### Morphological Typology

> Recall that a *morpheme* is the smallest unit of meaning that a word can be divided into.

Then, in many languages we have:

- difference in the **number of morphemes per word**
  - **isolating** languages like Vietnamese and Cantonese, one morpheme per word
  - **polysynthetic** languages like Siberian Yupik (`Eskimo`), in which a single word may have very many morphemes, *corresponding to a whole sentence* in English
- difference in the **degree a morpheme is separable**
  - **agglutinative** languages like Turkish, in which morphemes have relatively clean boundaries
  - **fusion** languages like Russian, in which a single affix may conflate multiple morphemes

This means that translating between languages with rich morphology requires dealing with structure below the word level, .e.g. use **subword models** such as BPE.

## Rule-Based MT Model

**Rules-based machine translation** (**RBMT**) is a machine translation approach based on hardcoded linguistic rules. The rules used here would include:

- lexical transfer
- lexical reordering
- etc.

### Direct Transfer

The task is to use rules to translate between, e.g. English to Spanish:

1. Use **morphological analysis**

   <img src="NLP/image-20220321194326179.png" alt="image-20220321194326179" style="zoom: 67%;" />

2. Use **lexical transfer rules** to find syntactic ==one to many mapping== of the translation of each word:

   <img src="NLP/image-20220321194444415.png" alt="image-20220321194444415" style="zoom: 67%;" />

   notice that here we did two things: do the translation per word (e.g. using a dictionary) + translated into basic grammar structure in Spanish

3. **Lexical Reordering**: fixing some more detailed word orders

   <img src="NLP/image-20220321194642643.png" alt="image-20220321194642643" style="zoom:67%;" />

4. **Morphological generation**: generate the morphology back from the first step

   <img src="NLP/image-20220321194835167.png" alt="image-20220321194835167" style="zoom:67%;" />

But of course even this rule-based approach has shortcomings in quality:

- lexical reordering does not adequately handle **more dramatic reordering** such as that required to translate from an SVO to an SOV language. This means we need **syntactic transfer rules** that map parse tree for one language into one for another.

  For example:

  <img src="NLP/image-20220321195042900.png" alt="image-20220321195042900" style="zoom:60%;" />

- some transfer requires **semantic information**. For example, in Chinese `PP` expressing *a goal* semantically should occur *before* `verb`, but in English, it occurs *after* the `verb`.

  Hence we need rules such as

  <img src="NLP/image-20220321195242729.png" alt="image-20220321195242729" style="zoom: 60%;" />

## Statistical MT

Of course rule-based approach have big problems

- difficult to come up with **good rules** between two languages

- it **does not scale** as it requires hand-written rules.

Instead of rule based direct transfer, consider a statistical model which at least scales

> SMT acquires knowledge needed for translation from a **parallel corpus** or bitext that contains the same set of documents in two languages.

### Noisy Channel Model

The idea is to consider, for example translating French to English:

> Source sentence (e.g. French) was generated from some ==noisy transformation== of the target sentence (e.g. English), as we have done in Spelling Correction.

Therefore, we consider finding the translation $\hat{E}$:
$$
\hat{E} = \arg\max P(E |F)
$$
for $F=f_1,f_2...,f_m$ being a sentence in French composing $m$ words, and $E = e_1,e_2,...,e_n$ being a sentence in English composed of $n$ words. Then using Bayesian rules:

$$
\hat{E} = \arg\max_E P(E|F)  = \arg\max_E \underbrace{P(F|E)}_{\text{likelihood of $E$}}\quad\underbrace{P(E)}_{\text{prior of $E$ occuring}}
$$
where $P(F|E)$ would then be computed by a **translation model** and $P(E)$ by a **language model**.

- e.g. $P(E)$ could come form a n-gram model. or a PCFG which captures syntactic structure as well, etc.

- to compute $P(F|E)$, we would ideally want to do:

  - find phrase alignments from a given $E$ to $F$ and translate each phrase
  - but this is hard to do, so in reality we consider **word alignment $A=a_1,...,a_k$** then translation

  $$
  P(F|E) = \sum_A P(F|E,A)P(A|E)
  $$

  more details on how this works is covered in the next section.

### Word Alignment for MT

Recall that our task is to compute $P(F|E)$, for $F$ being a random variable and $E$ of length $n$.

> To simplify the problem, typically assume the following:
>
> <img src="NLP/image-20220321202333366.png" alt="image-20220321202333366" style="zoom: 50%;" />
>
> where notice that:
>
> - given a $E$, each word in $E$ aligns to **one or more** words in $F$
>
> - each word in $F$ aligns to 1 word in $E$ (so it is a vector, as shown below)
>
> Then, an alignment in basically becomes a size $9$ vector which looks like:
> $$
> [1,2,3,3,3,0,4,6,5]^T
> $$
> which is for each word in $F$, the **index of the word in $E$ which generated it**. (then you can apply word-word level translation)

In general, such an alignment can be learnt from

- supervised word alignments, but human-aligned bitexts are rare and expensive to construct.

- so typically obtained using an **unsupervised EM-based approach** to compute a word alignment from unannotated parallel corpus.

### IBM Model 1

Now, ==assume that $P(F|E,A)$== is computable. The IBM model for SMT can generated a single $F$ from $E=e_1,...,e_n$ of length $n$ by:

1. choose length $k$, so that we would have $F=f_1,...,f_k$
2. choose an alignment $A=a_1,...,a_k$ which represents which English **word** it should comes from
3. For each position of word in $F$, generated a word $f_j$ **from** the aligned English version $e_{a_j}$

 Next, we can define how to compute $P(F|E)$ of $E$ having length $n$ by:

1. given some length distribution $P(K=k|E)$

2. assuming all alignments are qually likely, then there are $(n+1)^k$ possible alignments. Hence:
   $$
   P(A=a_1,...,a_k|E=e_1,...,e_n) = \frac{P(K=k|E)}{(n+1)^k},\quad \forall a_i
   $$
   i.e. same probability if given the same length. e.g. $P(A=0,1,2|E)=P(A=1,0,2|E)$

3. Given some translation probability per word from $e_y \to f_x$, let it be $t(f_x|e_y)$ we then have:
   $$
   P(F|E,A) = \prod_{j=1}^kt(f_j|e_{a_j})
   $$
   where the alignment would be given by previous step

4. Finally, we sum over all possible alignments:
   $$
   P(F|E) = \sum_A P(F|E,A)P(A|E) = \sum_A \frac{P(k|E)}{(n+1)^k} \prod_{j=1}^kt(f_j|e_{a_j})
   $$
   where the alignments $A$ would **vary both in** length $k$ and in the elements/indices within an alignment of same length.

Typically use an unsupervised EM-based approach to compute a word alignment from unannotated parallel corpus, e.g. you could have $A=0,1,2$, $A=1,0,2$, $A=1,0,0,2$, etc.

- notice that this is only a **forward algorithm**, so if we need to decode, this would be not very computational efficient.

---

Lastly, the decoding produce for finding the **best alignment** can be done by:
$$
\hat{A} = \arg\max_A \frac{P(k|E)}{(n+1)^k} \prod_{j=1}^kt(f_j|e_{a_j}) = \arg\max_A \prod_{j=1}^kt(f_j|e_{a_j})
$$
then, how do we maximize a product of terms? Since each term is independent, we can maximize it by **maximizing each term independently**. Hence:
$$
\hat{a}_j = \arg\max_{i} t(f_j,e_i),\quad \forall j
$$
which tells you which English word $f_j$ should align to.

- of course you can compute this once you know all the probabilities $P(k|E), t(f_y|e_x)$.

### HMM-Based Word Alignment

Obviously, one problem with IBM Model 1 is that it assumes all alignments are **equally likely** and ==does not take into account locality==, e.g. *next to each other words are likely to be next to each other in another language as well.*

> To solve this issue, **HMM models** can be used which models the **jump width** as hidden state, i.e. :
>
> 1. translate current word 
> 2. decide which next word to jump to for translation
> 3. repeat from step 1

First, an example would be

|                         Iteration 1                         |                         Iteration 3                         |                         Iteration 4                         |
| :---------------------------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------: |
| ![image-20220321224922541](NLP/image-20220321224922541.png) | ![image-20220321225048967](NLP/image-20220321225048967.png) | ![image-20220321225055378](NLP/image-20220321225055378.png) |

notice that:

- the jump could jump to the current word itself, as there could be one-to-many mapping

- the jump could jump both forward and backward discontinuously. E.g.

  <img src="NLP/image-20220321225219480.png" alt="image-20220321225219480" style="zoom: 25%;" />

Therefore, now we can define what this model really is

> - **Hidden states** are current English word $e_i$ being translated
> - **State transition** would be modelling the **jumps** for the next word, which is $a_{ij}=P(s_i \to s_j)$
> - **Observations** are the translated French word $f_j$
> - **Emission probability** is therefore $b_j(f_i)=P(f_i|e_j)$, which is basically probability of translation from $e_j \to f_i$

Then this means that:

- ==Observation likelihood $P(F|E)$== can be computed by the **forward algorithm** with HMM
- ==Decoding $\hat{E}=\arg\max_E P(E|F)$== can be computed by **Viterbi algorithm** with HMM

### Training Word Alignment Models

Both the IBM model 1 and HMM model require the following parameters

- $P(f_i|e_j)$ probability of individual word translation (shown in the example below)
- $P(K=k|E)$, length of target translation sentence

which can be obtained/trained on a **parallel corpus** to set the required parameters, including the specific ones such as:

- for HMM Model, we also need $P(e_i)$ and $P(e_i \to e_j)$ being transition probabilities

> **In general**
>
> - if we have a labelled/**supervised** (hand-aligned) training data, parameters can be estimated directly using frequency counts; e.g. **sentence alignment**. Which sentence in a corpus corresponds to which sentence is another corpus.
> - most often we have an **unsupervised** piece of parallel corpus. Then we need to estimate the probabilities using EM type algorithm.

A sketch of the algorithm looks like

<img src="NLP/image-20220321230852813.png" alt="image-20220321230852813" style="zoom: 50%;" />

For example, given two data in the training corpus:
$$
\text{green house} \iff \text{casa verde}\\
\text{the house} \iff \text{la casa}
$$

> Our aim is to be able to compute $P(s_i|e_j)$ for $s_i$ being a Spanish word, and perhaps vice versa.
>
> - notice that you should expect $\text{house} \iff \text{casa}$ to have a higher probability as this pair occurs more often
> - this is exactly what the EM algorithm tries to do

1. Step one: initialization. Here we can assume a uniform distribution such that each row/column sums to one

   <img src="NLP/image-20220321231613695.png" alt="image-20220321231613695" style="zoom: 33%;" />

   notice that each cell would represent $P(s_i|e_j)$ and vice versa.

2. **Expectation Step**: we impute the missing data, which is to consider all possible alignments/individual translations. Here we have four possible cases:
   $$
   \text{green} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{verde};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\\
   \text{green} \iff \text{verde}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/9}{2/9}=\frac{1}{2}
   $$
   and that
   $$
   \text{the} \iff \text{la}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\\
   \text{the} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{la};\quad p=\frac{1/9}{2/9}=\frac{1}{2}
   $$

3. **Maximization Step**: then we fill in the table using the previous probabilities

   <img src="NLP/image-20220321232324142.png" alt="image-20220321232324142" style="zoom:33%;" />

   And **normalizing to sum to one**:

   <img src="NLP/image-20220321232358667.png" alt="image-20220321232358667" style="zoom: 33%;" />

4. Repeat step 2-3 until convergence. Here just to be clear we show one more iteration of step 2. Possible alignments and their probability:

   <img src="NLP/image-20220321233011833.png" alt="image-20220321233011833" style="zoom: 67%;" />

   Hence
   $$
   \text{green} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{verde};\quad p=\frac{1/8}{3/8}=\frac{1}{3}\\
   \text{green} \iff \text{verde}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/4}{3/8}=\frac{2}{3}
   $$
   and
   $$
   \text{the} \iff \text{la}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/4}{3/8}=\frac{2}{3}\\
   \text{the} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{la};\quad p=\frac{1/8}{3/8}=\frac{1}{3}
   $$

Note that the above works by the assumption that many words will be **repeated**.

## Neural Machine Translation

As mentioned before, translation often involves a complex map between two sequences, hence usually we do

- **encoder-decoder** model (e.g. LSTM blocks)

  <img src="NLP/image-20220321234653968.png" alt="image-20220321234653968" style="zoom: 33%;" />

  e.g. transformer based encoder-decoder

- integrate an LSTM with language model using deep fusion.

  <img src="NLP/image-20220321234901398.png" alt="image-20220321234901398" style="zoom:67%;" />

  which basically is for decoder to **predict the next word** from a concatenation of the **hidden states of both** the translation and language LSTM models.

## Evaluating MT

Translations are evaluated along **two dimensions**:

1. **adequacy**: how well the translation captures the exact ==meaning== of the source sentence. Sometimes called **faithfulness** or fidelity.
2. **fluency**: how ==fluent== the translation is in the target language (is it grammatical, clear, readable, natural).

The most accurate metric is to have human to score the translations based on the above criterion, but that is inefficient and expensive. This in reality is done often in the following manner:

1. Collect one or more human **reference translations** of the source, i.e. gold standard
2. Compare MT output to these reference translations.
3. Score **result based on similarity** to the reference translations.

Some automatic scoring system implemented today include BLEU, NIST, TER, etc.

### BLEU

> **BLEU (Bilingual Evaluation Understudy)**: scores based on the following criteria:
>
> - What percentage of machine n-grams can be found in the reference translation?
> - Brevity Penalty: if translated sentence is too short, e.g. `the.`, it matches/precision $1.0$ but it is cheating!

To answer the first question, an example would be:

- finding shared unigrams:

  <img src="NLP/image-20220321235856265.png" alt="image-20220321235856265" style="zoom: 25%;" />

  where basically we count a match if the unigram appears in **at least one of the reference translation**

- finding shared bigrams

  <img src="NLP/image-20220322000121945.png" alt="image-20220322000121945" style="zoom:25%;" />

- until some fixed size $N$, typically $4$.

Finally, scoring the **first criteria** involves finding a **geometric mean**:
$$
\text{Precision}_{gm} = \sqrt[N]{\prod_{n=1}^NPr_n}
$$
However, this alone will **not work**, because you can have a **shorter sentence** which would give a higher score. Therefore, we also need a **brevity penalty**/the second criteria:

- ideally, we might compute $\text{Recall}$. which could solve the problem. But this is problematic since there are *multiple alternative gold-standard references*.

- therefore, we cook up with the following metric:

  Define effective reference length, $r$, as the **length of the reference sentence** with the **largest number of**
  **n-gram matches**. Then, if $c$ is the candidate sentence length:
  $$
  BP = \begin{cases}
  1,& \text{if }c > r\\
  e^{1-(r/c)},& \text{if }c \le r
  \end{cases}
  $$

So that the final score is:
$$
BLUE = \text{Precision}_{gm} \times BP
$$

## Challenges and Futures in MT

Certain challenges in MT include:

- OOV word in test set. We need smoothing or other techniques, such as subword.
- domain mismatch: e.g. corpus in movie but test in Amazon product review
- translating long context is hard, even with attention it is not completely solved
- low-resource language pairs
- NMT can pick up biases (e.g. gender bias)

Some future directions include:

- unsupervised MT attempts to learn language laignment form monolinguial data
- multilingual NMT, learn shared representation across all languages (which can solve low resource problem)
- Neural LSTM methods are currently the state-of-the-art.

# Sentiment Analysis and Classification

> In this chapter we introduce the algorithms such as Nave text Bayes algorithm and apply it to **text categorization**, the task of assigning a label or category to an entire text or document. In particular, we focus on categorizing text based on its sentiments, i.e. **sentiment analysis**.
>
> - e.g. positive or negative orientation that a writer expresses toward some object.
> - simplest version of sentiment analysis is a binary classification task
>
> Other commonly used names for sentiment analysis include: Opinion extraction; Opinion mining; Sentiment mining; Subjectivity analysis.

So now are are dealing with **classification task**, which is to be set in contrast to `Seq2Seq` task. Here we only need to output a **single** label/or a small fixed set for the **entire document**, e.g. a single written product review.

Other text categorization other than sentiment include:

- **Spam detection**: another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam
- **Authorship attribution**: whether if a given text is written by the person
- **Subject category classification**: which library category does a piece of text belong to? Science? Humanities? etc.

> The goal of classification is to take a single observation, i.e. a test sample, **extract** some useful features, and thereby **classify** the observation into one of a set of discrete classes.

And again recall that there are two broad class of classification algorithms:

- **Generative** classifiers like Nave Bayes
- **Discriminative** classifiers like logistic regression

---

*Real Life Example*: Twitter posts sentiment analysis

For posts from Twitter:

<img src="NLP/image-20220323202111044.png" alt="image-20220323202111044"  />

where we see "Happiness" surges around Election day and Thanksgiving. Other commonly used cases include:

- Movie: is this review positive or negative?

- Products: what do people think about the new iPhone?
- Politics: what do people think about this candidate or issue?
- Used as an input/feature to other task, such as predicting market trends from sentiment
- etc.

## Scherer Typology of Affective States

Some labels you can have for affective states human have:

- **Emotion**: brief organically synchronized  evaluation of a major event
  - angry, sad, joyful, fearful, ashamed, proud, elated
- **Mood**: diffuse non-caused low-intensity long-duration change in subjective feeling
  - cheerful, gloomy, irritable, listless, depressed, buoyant
  - more for *longer term predictions*
- **Interpersonal stances**: affective stance toward another person in a specific interaction
  - friendly, flirtatious, distant, cold, warm, supportive, contemptuous
  - used often for Social Media analysis, how users interact online between each other
- ==Attitudes==: enduring, affectively colored beliefs, dispositions towards objects or persons
  - liking, loving, hating, valuing, desiring
  - used a lot for product reviews and ==sentiment analysis==
- **Personality Traits**
  - nervous, anxious, reckless, morose, hostile, jealous
  - not very commonly used, but related to those personality test you take

Of course, which ones to use depends on the particular application.

## Sentiment Analysis and Attitudes

Often sentiment analysis is framed as the **detection of attitudes**. In particular, we want to find out

- **Holder** (source) of attitude

- **Target** (aspect) of attitude

- **Type** of attitude
  - From a set of types: Like, love, hate, value, desire, etc.
  - Or (more commonly) simple weighted **polarity**: positive, negative, neutral, together with *strength*
- **Text** containing this attitude: which sentence/document

In reality:

- **Simplest task:** Is the attitude of this text positive or negative? This will be our focus in this chapter as a ==classification task==.
- **More complex:** Rank the attitude of this text from 1 to 5, i.e. include *strength*
- **Advanced:** all the 4 subtasks.

## Sentiment Classification

Here we take on the **simplest task**: Is an IMDB movie review positive or negative?

For instance, we could have

|                           Positive                           |                           Negative                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| When Han solo goes light speed , the stars change to bright lines, going towards the viewer in lines that converge at an invisible point .<br/>Cool. | "snake eyes" is the most aggravating kind of movie : the kind that shows so much potential then becomes unbelievably disappointing |

In general, the steps we need to go through before and at classification time include:

1. **Tokenization**:

   - Deal with HTML and XML markup
   - Deal with Emoticons
   - Deal with Twitter mark-up (names, hash tags), e.g. `@xxx`

2. **Feature Extraction**

   - Some how handle *negation*, which could flip the meaning of a sentence:
     $$
     \text{I didnt like this movie.}\quad v.s.\quad \text{I did like this movie.}
     $$
     one idea is to convert the former to:
     $$
     \text{I didnt NOT\_like NOT\_this NOT\_movie}.
     $$
     basically adding NOT to every word between negative and the following punctuation.

   - Then perhaps we don't need all words? Only need the adjectives? (It turns out using All words work better)

3. **Classification** using different classifiers:
   $$
   \text{class} = \arg\max_{c_j \in C} P(C|W=w_1,...,w_n)
   $$

   - Naive Bayes
   - MaxEntropy
   - SVM

We will first introduce some **baseline algorithms** that works at least. 

### Baseline: Multinomial Nave Bayes

> **Multinomial Nave Bayes** is essentially a Naive Bayes classifier that is able to output a class among many classes (more than two). This is essentially done by:
>
> - (==assumption==: Naive Bayes) probabilities $P(w_i|c)$ are independent given the class $c$. Therefore it allows
>  $$
>   P(w_1,...,w_n|c_j) = \prod_i P(w_i|c_j)
>   $$
>   hence
>   $$
>   c = \arg\max_{c_j \in C}  P(c_j) P(w_1,...,w_n |c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)
>   $$

So for **Naive Bayes**, we consider:
$$
c = \arg\max_{c_j \in C} P(W|c_j)P(c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)
$$
meaning that we ==assume each word to be independently contributing to $c_j$==. Note that 

- this is a generative model because this equation can be understood as:

  1. generate a class $P(c_j)$
  2. generate the words from the class $P(w_i|c_j)$

- in many cases Naive Bayes is a linear classifier

  <img src="NLP/image-20220323222229514.png" alt="image-20220323222229514" style="zoom:80%;" />

  where you see the decision boundary for $P(c_1|x)=P(c_2|x)$ is a line.

To avoid numeric underflow, we would convert the estimate to log space:
$$
c_{NB} = \arg\max_{c_j \in C} \,\,\log P(c_j) + \sum_i \log P(w_i|c_j)
$$
But how do we ==learn== $P(c_j)$ and $P(w_i|c_j)$?

- learning the prior $P(c_j)$ is easy. Given $N_{doc}$ documents (e.g. posts), let $N_{c_j}$ be the number of document with sentiment class $c_j$. Then:
  $$
  \hat{P}(c_j) = \frac{N_{c_j}}{N_{doc}}
  $$

- learning the likelihood means counting how often is $w_i$ associated with class $c_j$. Hence we consider:
  $$
  \hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)}{\sum_{w\in V}\text{Count}(w_i, c_j)}
  $$
  where:

  - $\text{Count}(w_i, c_j)$ represent the number of times the word $w_i$ appears among all words in all documents of class $c_j$. 
  - In other words, we first concatenate all documents with class $c_j$ together, then count the number of occurrence of $w_i$.
  - since eventually we need this for all words, $V$ presents the **entire vocabulary** instead of vocabulary in class $c_j$.

Since we are using count, consider **smoothing** as well for unmet $w_i,c_j$ pair:
$$
\hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)+1}{\sum_{w\in V}\text{Count}(w_i, c_j)+1}=\frac{\text{Count}(w_i, c_j)+1}{\text{Count(total words in $c_j$)}+|V|}
$$
which is ==critically needed== as the likelihood term is a multiplication.

- for **OOV words**, we can use the technique introduced before by inducing `OOV` vocab in the training set. However, for Naive Bayes, it is more common to ==ignore those words completely== (as if you didn't see it)
- other processing step include **removing stop words** such as `a` and `the` for both train and test sets. Though the performance gain from this is not significant.

Therefore, the algorithm for learning is

<img src="NLP/image-20220323211934552.png" alt="image-20220323211934552" style="zoom:67%;" />

Then for testing, we simply perform:
$$
\hat{c} = \arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)
$$
Hence the algorithm for test is

<img src="NLP/image-20220323212120485.png" alt="image-20220323212120485" style="zoom:80%;" />

---

*Example*

Well use a sentiment analysis domain with the **two classes positive (+) and negative (-).** The train and test set is provided below

<img src="NLP/image-20220323223034920.png" alt="image-20220323223034920" style="zoom:67%;" />

What are the parameters $P(c_j)$ and $P(w_i|c_j)$ in this case?

- the prior $P(c_j)$ is simply counts:
  $$
  P(-) = \frac{3}{5},\quad P(+) = \frac{2}{5}
  $$

- then the smoothed likelihood essentially is
  $$
  \hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)+1}{\sum_{w\in V}\text{Count}(w_i, c_j)+1}=\frac{\text{Count}(w_i, c_j)+1}{\text{Count(total words in $c_j$)}+|V|}
  $$
  where $|V|=20$ and we see that $\text{Count(total words in $+$)}=14$ and $\text{Count(total words in $-$)}=9$. Hence some examples:

  <img src="NLP/image-20220323223909939.png" alt="image-20220323223909939" style="zoom:67%;" />

  and the rest is trivial.

For estimation, first we realize that the word *with* is OOV. Hence we ignored it as we are doing Naive Bayes. Then we are basically predicting $\text{"Prediction no fun"}$:

<img src="NLP/image-20220323224008083.png" alt="image-20220323224008083" style="zoom:67%;" />

hence the result is $-$ as it has a higher probability.

### Improvements From Baseline

While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance.

- does the $\text{Count}(w_i,c_j)$ really matters? Maybe all it matters is that fact that it **occurred** in $c_j$ document at least once!

- dealing with **negation** improves Naive Bayes accuracy as well:
  $$
  \text{I didnt like this movie.}\quad v.s.\quad \text{I did like this movie.}
  $$
  notice that the negation of *didn't* completely changed $P(\text{like}|c_j)$, for instance. The baseline that deals with this is mentioned before by converting them to
  $$
  \text{I didnt NOT\_like NOT\_this NOT\_movie}.
  $$
  where newly formed words such as $\text{NOT\_like}$ will be treated as a word.

- **insufficient labeled training data** to train accurate naive Bayes classifiers using all words in the training set. In those cases we will have to derive word features (e.g. labelled emotion carried in a word) using **sentiment lexicons**. Some popular ones online include:

  - General Inquirer, LIWC, The Opinion Lexicon, MPQA

  - example of annotated words from MPQA include:

    $$
    + : admirable, \,\,beautiful, \,\,confident, \,\,dazzling, \,\,ecstatic,\,\, favor,\,\, glee,\,\, great\\
    - : awful,\,\, bad,\,\, bias,\,\, catastrophe, \,\,cheat, \,\,deny,\,\, envious,\,\, foul,\,\, harsh, \,\,hate
    $$
    A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs.

Here we will go over details on how to implement the Binary NB variant.

> If we believe that word **occurrence** may matter more than word frequency: then we can do "Binary Naive Bayes", which clips all the word counts in **each document** at $1$. (i.e. it's like a Boolean switch for each word)
>
> - this results in **binary multimodal naive Bayes**, or binary NB
>
> - in practice this seems to be true, that performance is better.

In algorithm, basically you can **first reduce all duplicate words ==in each document== to one occurrence**, and then perform the same algorithm.

For example:

|                       Raw Naive Bayes                        |                     Boolean Naive Bayes                      |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP/image-20220323212718953.png" alt="image-20220323212718953" style="zoom:67%;" /> | <img src="NLP/image-20220323212725092.png" alt="image-20220323212725092" style="zoom:67%;" /> |

where the highlighted words are duplicates **within the document**. Hence the counts become:

<img src="NLP/image-20220323212755987.png" alt="image-20220323212755987" style="zoom:80%;" />

Then for testing, the we **do not remove words** as this is just for making the probability work better.

### Problems in Sentiment Classification

Some cases where Naive Bayes' assumption essentially **fails**:

- **Subtlety** in words: each word itself is neutral, but overall there is a sentiment:

  "*If you are reading this because it is your darling fragrance, please wear it at home exclusively, and tape the windows shut.*"

- **Thwarted Expectations and Ordering Effects:** where words such as *However* would suddenly change the sentiment

  "*This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance. **However, it cant hold up.***

In those cases just modelling $P(w_i|c_j)$ won't work. Neural Models that remember/see contexts would work better.

## Polarity in Sentiment Analysis

Here, the idea is that each **word/phrase** themselves could have polarity, meaning that they themselves **could be indicative of the sentiment of the sentence/document.**

> **Polarity** is float which lies in the range of $[-1,1]$ where $1$ means positive statement and $-1$ means a negative statement.

Some simple measurement of such is to consider:

- ==how likely each word== is to ==appear in each sentiment class== (discussed here)
- use some form of embedding, which could be learnt in a Word2Vec approach

For instance, consider the polarity of the word "*bad*":

<img src="NLP/image-20220328202939691.png" alt="image-20220328202939691" style="zoom:50%;" />

where

- each category $c$ would represent the word "*bad*" appearing in a 1-star, 2-star, 3-star review, etc
- notice that even in 10-star comments, we still see the word "*bad*" quite often. This could happen due to ==negations==.

Then, since we have the counts, we can compute the likelihood
$$
P(w|c) = \frac{\text{Count}(w,c)}{\sum_{w}\text{Count}(w,c)}
$$
where $c$ would represent 1-star, 2-star, ... Then we could **scale** it so it is **comparable between words**
$$
\frac{P(w|c)}{P(w)}
$$
for $P(w)$ would be a the probability of observing the word at all.

---

*Exmaple*: Polarity of each word in IMDB

Plotting the scaled likelihood for some common words in the 1-10 star IMDB reviews look like:

<img src="NLP/image-20220328203514745.png" alt="image-20220328203514745" style="zoom: 25%;" />

where:

- notice that "*good*" looks relatively flat across ratings (perhaps due to negations). This means "*good*" is ==not a good indicator== for positive sentiment/rather **neutral polarity**.
- words such as "*great*" and "*excellent*" would be a better "metric" for the positive rating of the movie/positive polarity.
- words such as "*somewhat*" and "*fairly*" are attenuators as they most happen in th middle of the rating; they dampen extremely positive/negative reviews
- the upshot is that even seemingly positive/negative words ***could* happen in all contexts**

### Logical Negation and Polarity

> Is **logical negation** (no, not) associated with negative sentiment? (we know there are problematic cases such as "I don't hate this movie" is a positive review with double negative.)

In general, there are **slightly more negation in negative sentiments**. For word such as "*no/not*", the plot looks like:

<img src="NLP/image-20220328163202281.png" alt="image-20220328163202281" style="zoom:50%;" />

where notice that:

- the scaled likelihood is slightly higher for low ratings for negation word such as "*no/not*"
- so even if we have double negatives such as "I don't hate this movie", this is still a useful feature

## Learning Sentiment Lexicons

Here we consider the case that you want to have some sort of word polarity for your domain task, **but there is no dataset with lexicons and ratings** so that you cannot really know which words are more positive/negative.

- e.g. maybe dealing with a academic domain, where polarity of words would be different from well-established datasets such as movie reviews

> Can we use a few hand-built patterns/lexicons to **bootstrap and build a larger lexicon list**?
>
> - more lexicons learnt means your model could be more robust to test set
> - useful for domain transfer

 The general idea/a simple rule:

- adjectives conjointed by "*and*" tend to have same/similar polarity. e.g. "*fair and legitimate*"
- adjectives conjointed by "*but*" tend to have different polarity. e.g. *fair but brutal*

Then, we can consider some kind of program that does:

<img src="NLP/image-20220328163454985.png" alt="image-20220328163454985" style="zoom: 50%;" />

where seeds are the **hand-built small lexicon list** you made. Now, the question become:

> Where do you find similar/different words? Use a database or search engine!

*Example*:

1. Start with a labelled seed set of words. For instance a **labelled seed set of 1336 adjectives** such that:

   - there are 675 **positive** ones: *adequate, central, clever, etc.*
   - there are 679 **negative** ones: *contagious, drunken, ignorant, etc*

2. Then we can find similar words from a database/a search engine by **conjoining with *"and"***:

   <img src="NLP/image-20220328163757433.png" alt="image-20220328163757433" style="zoom:33%;" />

3. However, words might appear in many pairs. For instance, we could have a "*fair and nice*", and but possibly *"fair and corrupt"* as well. 

   <img src="NLP/image-20220328164217042.png" alt="image-20220328164217042" style="zoom: 33%;" />

   Therefore, from the **seed set**, we can train a supervised classifier to assign a **polarity similarity score** to a given pair of word.

4. Then we can determine the cluster by:

   - for any two pair, if unknown polarity similarity, use the trained classifier
   - closer polarity words are grouped/clustered together

   Hence we would arrive at:

   <img src="NLP/image-20220328205610887.png" alt="image-20220328205610887" style="zoom:33%;" />

5. Output the clusters and words inside it

   <img src="NLP/image-20220328164411821.png" alt="image-20220328164411821" style="zoom:33%;" />

   note that of course we can get errors.

### Turney Algorithm

Notice the above is a semi-supervised approach to learn a lexicon list with polarity. Then with those polarity, we could potentially use to classify sentiment of documents containing those words. 

However, is there are way to do it in a **unsupervised** approach?

> **Unsupervised** classificaiton of reviews! Done by:
>
> 1. **Extract** a phrasal lexicon from reviews (following some pre-defined rules)
> 2. **Learn polarity** of each phrase (by co-occurance with words such as "*excellent*" and "*poor*" )
> 3. Rate a review by the **average** polarity of its phrases

For computation and simplicity, we only extract **two-word phrases** with **adjectives**. Then, the algorithm does

1. First we extract the phrases. We extract two-word phrases that satisfy the following POS tags:

   <img src="NLP/image-20220328164637033.png" alt="image-20220328164637033" style="zoom:33%;" />

   note that RB, RBR, RBS are the comparitive/superlative form of **adjectives** JJ.

2. Then we need to know the **polarity of phrase**. We hypothesize that:

   - Positive phrases co-occur more with *excellent*
   - Negative phrases co-occur more with *poor*
   - the choice of the two words should at least come from the graphs in section [Polarity in Sentiment Analysis](#Polarity in Sentiment Analysis)

   We can measure the **co-occurance** by PMI, which comes from a co-occurance matrix. Recall that it is a measure of **how often two events $x$ and $y$ occur together**, compared with what we would expect if they were **independent**:
   $$
   I(x,y) = \log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)
   $$
   Hence, the **pointwise mutual information** between a target word $w$ and a context word $c$ (for ==some window size such as 7==) is then defined as:
   $$
   \text{PMI}(w,c) = \log_2\left(\frac{P(w,c)}{P(w)P(c)}\right)
   $$
   for probabilities can be estimated with **word-context matrix**, we consider target words $w$ and context $c$. In this case, we don't really care about context but rather how often $w$ appears with "*excellent*" and "*poor*" as:
   $$
   \text{PMI}(w, \text{excellent}) = \log_2\left(\frac{P(w,\text{excellent})}{P(w)P(\text{excellent})}\right)
   $$
   and similarly for "*poor*". Then:

   - To get those **counts and the co-occurance matrix**, we will use the search engine:
     $$
     \hat{P}(w) = \frac{\text{hits}(w)}{N}, \quad N=\sum_w \text{hits}(w)
     $$
     Then $\hat{P}(w_1, w_2)$ can be esitmated by:
     $$
     \hat{P}(w_1, w_2) = \frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)}{\sum_{w_1, w_2}\text{hits}(w_1\,\, \text{Near}\,\, w_2)}=\frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)}{kN}
     $$
     for $k$ being the size of the window, i.e. being $k$ words apart. (we often drop $k$ in subsequent calculation as it is a constant)

   - Finally, the PMI is therefore
     $$
     \text{PMI}(w, \text{excellent}) = \log_2\left(\frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)/N}{\text{hits}(w)\text{hits}(w_2)/N^2}\right)
     $$

   Finally, we define the polarity of a phrase by doing the ==PMI difference between "*excellent*" or "*poor*"==:

   <img src="NLP/image-20220328165248858.png" alt="image-20220328165248858" style="zoom:33%;" />

3. Now for evaluating the **polarity of a single review**, we simply first compute the polarity for each extracted phrase in the review:

   <img src="NLP/image-20220328165526774.png" alt="image-20220328165526774" style="zoom:33%;" />

   notice that 

   - "*true service*" has a low score, i.e. co-occur more often with negative word such as "*poor*", hence indicative of bad reviews. The obvious one would be "*inconveniently located*"
   - then the **final score** for the review will be the **average**. Here it is $0.32$, which means this review is slighltly positive.

Results from this algorithm on the Epinions dataset:

- 170 (41%) negative
- 240 (59%) positive
- baseline (59%): guessing all positive
- Turney algorithm: 74%

Note that again, this is good given that it is fully unsupervised!

### Summary on Learning Lexicon

Both the algorithm covered above share the following pattern:

1. start with some seed set of words, e.g. "*good*", "*excellent*", "*poor*"
2. find **other words** that have similar polarity from some external dataset/search engine
   - using "*and*" and "*but*"
   - using co-occurance
3. add them to lexicon

## Other Sentiment Task

*Recall that*:

Often sentiment analysis is framed as the **detection of attitudes**. In particular, we want to find out

- **Holder** (source) of attitude

- **Target** (aspect) of attitude

- **Type** of attitude
  - From a set of types: Like, love, hate, value, desire, etc.
  - Or (more commonly) simple weighted **polarity**: positive, negative, neutral, together with *strength*
- **Text** containing this attitude: which sentence/document

In reality:

- **Simplest task:** Is the attitude of this text positive or negative? This will be our focus in this chapter as a ==classification task==.
- **More complex:** Rank the attitude of this text from 1 to 5, i.e. include *strength*
- **Advanced:** all the 4 subtasks.

---

> Here we discuss some approaches of identifying the **target/aspect** of a positive/negative review: 
>
> - target: "*The **food** was **great** but the **service** was **awful***".
> - aspect: automatically find out it is talking about "*food*" and "*service*"

### Finding Target/Aspect of a Sentiment

Some simple approaches for extracting **aspects** mentioned in some review:

- rule-based: simple
  1. find all frequenct phrases across reviews, e.g. "*fish tacos*"
  2. filter again by some rules such as "an spect should **occur after a sentiment word**". e.g. we need "*... great fish tacos*" to keep "*fish tacos*" unfiltered.
- ML based: some aspects might not be in the sentence, i.e. its **meaning is hidden**. e.g. "*... the place smells bad*" correspond to something like "*odor*".
  1. **hand-label** a small corpus of reviews sentences with aspect, e.g. whether it is about "*food, decor, service*", etc.
  2. train a classifier on it

Then, once you have some way to extract aspect from a sentence, consider the following pipeline:

<img src="NLP/image-20220328170839607.png" alt="image-20220328170839607" style="zoom: 50%;" />

hence the general idea would be:

- first do a **sentence** level sentiment classification
- from the positive/negative reviews, extract the aspects using the trained classifier
- hence obtain **aspects level classifcation**, essentially by combing the extracted aspect + whether if they are positive/negative from the sentence level

For Example, results would look like:

<img src="NLP/image-20220328214159748.png" alt="image-20220328214159748" style="zoom: 25%;" />

## Explaining Sentiment Classification

> Why is certain sentence clasified as positive/negative?

Still an active research topic, some current approaches:

- we just highlight the words associated with positive/negative score

- WT5, comes form T5. It is a text-to-text model, hene it can directly genrate a **text explaining** why it is a positive/negative post and classifying it

  <img src="NLP/image-20220328171519774.png" alt="image-20220328171519774" style="zoom:50%;" />

## Summary on Sentiment Classification

Generally modeled as **simple classification** or regression task

- predict a binary or ordinal label

Some common process/task/problems it involves:

- how to deal with **negation** is important

- Using all words (in **naive bayes**) works well for some tasks
  $$
  c = \arg\max_{c_j \in C} P(W|c_j)P(c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)
  $$

- Finding subsets of words may help in other tasks

  - Hand-built polarity lexicons

  - Use seeds and semi-supervised learning to induce lexicons

- A fully unsupervised approach such as Turney's Algorithm

# Statistical Significance Testing

> In building systems we often need to compare the performance of two systems. How can we know if the **new system we just built is better** than our old one? How **certain are we** if one model performend better than nother on some test set?

Let us have:

- two models we want to compare, model $A$ being the new one and $B$ being some baseline. 
- we are interested in testing on some metric $M$, such as $F_1$ score or accurarcy. 
- suppose we have some test set $x$ to evaluate on

Obviously we could measure:
$$
\delta(x) = M(A,x) - M(B-x) \equiv \text{Effect Size}
$$
for $M(A,x)$ is the performance of model $A$ on test set $x$ using metric $M$. Of course the **larger the effect size the better**, but another question we usually want to ask is:

> if $A$s superiority over $B$ is likely to hold again if we checked another test set $x'$

In the paradigm of statistical hypothesis testing, we test this by formalizing two hypotheses. Suppose you found $\delta(x)=0.2$, i.e. we have a $0.2$ higher e.g. accurarcy:
$$
H_0: \text{what you observed is just a random effect}\\
H_1: \text{it is not random. Our model is better}
$$
We want to show that $H_0$, the **null hypothesis**, has a ==low probability of happening==, so that what we observed is **not** random/by chance:
$$
\text{p-value}(x) \equiv P(\delta(X) \ge \delta(x) | H_0 \text{ is true})
$$
i.e. we try on some ==random== test set $X=x$ (with mean $0$) to get $\delta(x)$ again and again. If it is ==not by chance==, then $\text{p-value}(x)$ should be small.

- e.g. consider playing Poker, I claim that I am a better player, so the null hypothesis is that I win by luck.
- then, suppose $\delta(x)$ is I win 10 "random" games in a roll. We know that $P(\delta(X) \ge \delta(x) | \text{I win by luck})$ is small. Hence I can reject the null hypothesis and conclude that I am actually a better player, if $\delta(x)$ happened.

> Therefore, p-value gives the **probability of observing a test statistic as extreme as the one observed $\delta(x)$,** if the null hypothesis is true. 
>
> Hence, if the p-value is small, the observed test statistic is very **unlikely under the null hypothesis**

- the expected value of $\delta(X)$ over many test sets, if assuming $A$ is not better than $B$ is $\mathbb{E}_X \delta(X) = 0$ for $X$ comes from a distribution with zero mean
- how small should the p-value be? Often **threshold** such as $0.05,0.01$ is fine.

> How do we compute the probability needed?
>
> - In NLP we generally don't use parametric tests such as t-tests or ANOVAs as they make **assumptions on the distributions of the test statistic** (such as normality) that dont generally hold in our cases
>   So in NLP we usually ==use non-parametric tests based on sampling==.

Either:

- if we had lots of different **random test sets $x^\prime$ with mean $0$** we could just measure all the $\delta(x^\prime)$ for all the $x^\prime$. This gives a distribution, from which we can compute probability of at least $\delta(x)$ happening if it is a random effect
- use a bootstrap test by repeatedly drawing large numbers of smaller samples **with replacement**, under the assumption that the sample is representative of the population.

## The Paired Bootstrap Test

The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation.

> The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample.
>
> - in fact, the idea is to ==virtually create random test sets $X$== by ==sampling with replacement from a fixed test set $x$==.
> - since this means we have $X$ with mean of $\delta(x)$, we would have to change the formula a bit

Consider a tiny text classification example with a test set $x$ of $10$ documents. And suppose we have $M$ being accuracy and we have two models $A$, and $B$:

<img src="NLP/image-20220328231849869.png" alt="image-20220328231849869" style="zoom: 67%;" />

where a slash means the model got it wrong. For this test set $x$, the effect size is $\delta(x)=0.2$.

We need to create a large distribution of test sets $X$. We can do this by:

1. pick a large number $b$, e.g. $b=10^5$ being the number of tests $x^{(i)}$ we want to create
2. each test $x^{(i)}$ will have $n=10$ same as $x$. Hence we repeatedly **sample $n$ times from $x$ with replacement**.
3. do until we created $x^{(b)}$

<img src="NLP/image-20220328233452958.png" alt="image-20220328233452958" style="zoom:67%;" />

Now that we have the ==$b$ random test sets==, providing a sampling distribution, we can compute how likely $A$ made $\delta(x)$ by pure chance. We might naively consider:

$$
\text{p-value}(x) = \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)}) - \delta(x) \ge 0 \right)
$$

so that if ==p-value is low==, then $\delta(x)$ is ==not by pure chance==. However, this would be wrong as
$$
\text{p-value}(x) \equiv P(\delta(X) \ge \delta(x) | H_0 \text{ is true})
$$
**assumes** the $X$ would yield a mean of $\delta(X)=0$. Here we would yield a $\delta(X=x)=0.2$ since it all came from the test set $x$.

Therefore, the correct one would be
$$
\begin{align*}
\text{p-value}(x) 
&= \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)}) - \delta(x) \ge \delta(x) \right)\\
&= \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)})  \ge 2\delta(x) \right)
\end{align*}
$$

So that if we have $10^5$ tests and for $47$ of them it happened that $ \delta(x^{(i)})  \ge 2\delta(x)$, then it means p-value is $.0047$, i.e. it is very rare by chance. If we take a threshold of $0.01$ being significant, then we can say that this result $\delta(x)$ is statistically significant.

# Information Extraction and NER

structed textual document: tables

> Transform **unstructured** information in a corpus of documents or web pages **into a structured data**, e.g. populating a relational database, to enable further processing.

Consider receiving an email from job posting:

|                        Raw Data/Email                        |                        Extracted Data                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP/image-20220330162319299.png" alt="image-20220330162319299" style="zoom: 50%;" /> | <img src="NLP/image-20220330162434115.png" alt="image-20220330162434115" style="zoom: 50%;" /> |

The task would be to **extract** information from some **pre-defined attributes** we want to extract.

- i.e. givne the input, fill out the table (on the right) as much as we can
- notice that each extracted info is an **entity**

The general pipeline you want to consider is:

- **named entity recognition**
- **relation extraction**
- **template filling**

## Named Entity Recognition

> Specific type of information extraction in which the **goal is to extract formal names of particular types of entities** such as people, places, organizations, etc.

Usually this is **used as a preprocessing step** for some future tasks, such as the template filling task we had, or question answering. Notice that this is a task in between sequence level and token level task - it is a ==span-oriented application==.

Formally, given an input $x$ with $T$ tokens, $x_1,...,x_T$, a span is a continuous sequence of tokens with start $i$ and end $j$ such that $1 \le i \le j \le T$. Then in total  for all possible span length we could have:
$$
\frac{T(T-1)}{2}
$$
total possible spans. Most application literally **iterate through all possible spans**. Hence they often have some application-specific length limit $L$ such that $j-i < L$ is required/legal span. We refer tot the set of **legal span in $x$ as $S(x)$.**

An example would be

|                            Input                             |                        Output Desired                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP/image-20220330163241380.png" alt="image-20220330163241380" style="zoom: 50%;" /> | <img src="NLP/image-20220330163309719.png" alt="image-20220330163309719" style="zoom:50%;" /> |

where note that we only find NER for the three entities we care here. Hence entities such as "*Geneva Conventions*" SHOULD NOT be highlighted. Hence this task is generally not easy.

In general, today we have two approaches:

- train a BIO tagger. This will essentially be a **Seq-labelling model**
- train an end-to-end model using GPT-3. This will be a **Span-based Model**

### BIO-based NER

Some idea for **training BIO-based model** would be a ==Seq-labelling model==

|                        Training Data                         |                 BIO version of Training Data                 |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220331153159045.png" alt="image-20220331153159045" style="zoom:50%;" /> | <img src="NLP_part2/image-20220331153208512.png" alt="image-20220331153208512" style="zoom:50%;" /> |

Hence this is essentially a Seq-2-Seq decoding. A fuller example would be:

<img src="NLP_part2/image-20220331153630932.png" alt="image-20220331153630932" style="zoom: 50%;" />

==TODO: CRF==   on page 178-Chapter 8

### Span-based NER

Some idea for **training an end-to-end NN based model** would be a ==span-oriented application==.

1. generate **representations for all the spans** in the input. This often contains

   - representations of the span boundaries, $h_i,h_j$ being the embeddings
   - representation of the content in the span, $f(h_{i+1:j-1}) \to g_{ij}$
     - an example of such as function could be $f(h_{i+1:j-1}) = \bar{h}_{i+1:j-1}$ being the average
   - combine the two representation, $\text{SpanRep}_{ij} = [h_1;h_j;g_{ij}]$

   this can be done using contextualized input embeddings from the model

2. then, we have a span representation $g_{ij}$ for each span in $S(x)$. Then this is a **classification problem** where each span in an input is assigned a class label $y$:
   $$
   y_{ij} = \text{softmax}(\text{FFNN}(\text{SpanRep}_{ij})) \in \mathbb{R^{|Y|}}
   $$
   since most spans will not have a label, we add $y = \text{null} \cup y$ to the set of labels, and hence $|Y|=|y|+1$.
   
3. then for **decoding**, take $\arg\max y_{ij}$  to get the tag. 

> **Note** that some post-processing steps will need to be done to prevent overlapping classifications, as this scores all $T(T-1)/2$ spans. However, it does have a benefit as it naturally accommodate embedded named entities:
>
> - e.g. both "*United Airlines*" and "*United Airlines Holding*" would be evaluated
> - a BIO based tagging approach would have not looked at this.

A detailed **example** would be:

- instead of taking average, we consider using the embeddings as
  $$
  g_{ij} = \text{SelfAttn}(h_{i:j})
  $$
  so that the representation would be centered around the head of the phrase corresponding to the span. Then combining to get:
  $$
  \text{SpanRep}_{ij} = [h_1;h_j;g_{ij}]
  $$

- finally doing the same:
  $$
  y_{ij} = \text{softmax}(\text{FFNN}(\text{SpanRep}_{ij}))
  $$

Hence graphically

<img src="NLP_part2/image-20220331125718227.png" alt="image-20220331125718227" style="zoom:67%;" />

where:

- cross-entropy loss would be used

## Relation Extraction

Now we have detected named entities, our next step is to **discern relationships between those entities**. For instance:

<img src="NLP_part2/image-20220331154117733.png" alt="image-20220331154117733" style="zoom:50%;" />

where form the highlighted part, we know that:

- "*Tim Wagner*" is a spokesman for "*American Airlines*"
- "*United*" is a unit of "*UAL Corp*"
- etc.

All of those are simple ==binary relationships== which fall under some generic categorization such as:

- basic **3 relations**: employed-by; located-at; part-of

- a more **complicated 17 relations** used in ACE relation extraction

  <img src="NLP_part2/image-20220331154420120.png" alt="image-20220331154420120" style="zoom:50%;" />

Hence, a graphical representation of what are doing now is:

<img src="NLP_part2/image-20220331154717302.png" alt="image-20220331154717302" style="zoom:67%;" />

where:

- the first two steps can be done in a NER
- the **relation** can be seen as consisting of a **set of ordered tuples** over elements of a domain (e.g. named entities)

### Relation Extraction Algorithms

Now, we answer the question: how do we find those relations? In general, there are four main ways to do it

- handwritten patterns
- supervised machine learning
- semi-supervised (via bootstrapping or distant supervision)
- unsupervised

### Using Patterns to Extract Relations

Consider the example of:

<img src="NLP_part2/image-20220331160956115.png" alt="image-20220331160956115" style="zoom: 67%;" />

where we notice that "*Gelidium*" is a hyponym of "*red algae*", which can be identified using the following pattern
$$
NP_0 \text{ such as } NP_1 \{,NP_2,...,(\text{and,or})NP_i\}
$$
implies the relation
$$
\forall NP_i,i\ge 1, \text{hyponym}(NP_i, NP_0)
$$
which inspires pattern such as

| Pattern                                                      | Example                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="NLP_part2/image-20220331161313184.png" alt="image-20220331161313184" style="zoom:50%;" /> | <img src="NLP_part2/image-20220331161322250.png" alt="image-20220331161322250" style="zoom:50%;" /> |

but of course, with hand-crafted rules we have:

- advantage of high precision as they are tailored to specific domains
- low recall, missing a lot
- not scalable

### Relation Extraction via Supervised Learning

Consider now having the 

- **input** of: a fixed set of entities
- **label** of: a fixed set of pre-defined relations

Then approaches to make it a classification problem could be: for each possible pair, apply a **multiclass classification** for each class being the relation

<img src="NLP_part2/image-20220331161750368.png" alt="image-20220331161750368" style="zoom: 67%;" />

where the **feature**/embeddings of the named entity $e_1,e_2$ could be:

- word level embeddings of the named entity $e_i$
- head word embedding of the named entity
- encoding the named entity's type, e.g. if it is `ORG`, or `PER`
- number of entities in this sentence
- encoding some syntactic structure of the sentence
- etc

then the architecture of the classifier could be

<img src="NLP_part2/image-20220331162335505.png" alt="image-20220331162335505" style="zoom: 67%;" />

where notice that 

- essentially using a transformer, hence using context embeddings for the content

- we also included the entire sentence to give context for the two named entities in this case

of course, this can be optimized as we can skip some pairs for certain relation as it won't happen at at all

> But labeling a large training set is extremely expensive and supervised models are brittle: they **don't generalize well** to different text genres.

For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next.

- other problem include: what if the relation set is **not fixed**? Then of course supervised version would not work.

### Semi-supervised Relation Extraction

One idea is to **bootstrap** more relation-labeled entity pairs from some **known small sample labelled pair**. For instance, suppose you want to get a **relation being `airline/hub` pair**, and you already have
$$
\text{Ryanair has a hub at Charleroi} \iff \text{[ORG] has a hub at [LOC]} 
$$
being a known pair, you can:

1. finding other mentions of this relation in our corpus

   <img src="NLP_part2/image-20220331163230149.png" alt="image-20220331163230149" style="zoom: 67%;" />

2. use features such as context to extract **general patterns** such as the following

   <img src="NLP_part2/image-20220331163324364.png" alt="image-20220331163324364" style="zoom:67%;" />

   which signifies a "rule" for `airline/hub` pair

3. use these new patterns can then be used to search for additional tuples.

4. assign **confidence** values to new tuples, and add to the dataset if high confidence

   - this is to **avoid semantic drift**. In semantic drift, an erroneous pattern leads to the introduction of erroneous tuples, which, in turn, lead to the creation of problematic patterns and the meaning of the extracted relations drifts.

Hence, given some seed samples that have relation $R$, we can **populate this set** by

<img src="NLP_part2/image-20220331163657605.png" alt="image-20220331163657605" style="zoom: 67%;" />

Once done, apply supervised classification as mentioned above.

### Unsupervised Relation Extraction

> The goal of unsupervised relation extraction is to **extract relations from the web** when we have no labeled training data, and not even any list of relations.
>
> This task is often called **open information extraction** or Open IE.

For example, the ReVerb system (Fader et al., 2011) extracts a relation from a sentence s in 4 steps:
1. Run a part-of-speech tagger and entity chunker over $s$
2. For each verb in $s$, find the longest sequence of words $w$ that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.
3. For each phrase $w$, find the nearest noun phrase $x$ to the left which is not a relative pronoun, wh-word or existential *there*. Find the nearest noun phrase $y$ to the right.
4. Assign confidence $c$ to the relation $r = (x;w;y)$ using a confidence classifier and return it.

## Template Filling

> **Task**: fill the slots in the associated templates with **fillers** extracted from the text.
>
> - those fillers could come from text segments extracted directly from the text
> - consist of concepts that have been inferred from text elements through some additional processing

A template looks like

<img src="NLP_part2/image-20220331164508528.png" alt="image-20220331164508528" style="zoom:67%;" />

where we have four slots.

The next section describes a standard **sequence-labeling approach to filling slots**.

### Supervised Approach to Template Filling

We are given

- training documents with text spans **annotated** with predefined templates and their slot fillers. 
- note that there can be **multiple different templates**

Our goal is to 

- create one template for each event in the input
- fill in the slots with text spans.

Hence this means that we can split the task to train **two classifiers**

1. ==first system== decides whether the template is present in a particular sentence

   - called **template recognition** or **event recognition**

   Then the 

   - **input** could be a **sequence of words**, with the usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags, syntactic chunk tags, and named entity tags.
   - **output** is indicating any slot from a template being detected

   Then if detected, feed that sequence of words into the second system with that detected template

2. ==second system== has the job of role-filler extraction.

   - **for each role** in the template, e.g. `LEAD-AIRLINE`, `AMOUNT`, etc
   - train a **binary classifier** that on each possible span to decide if it  would fill in the blank

   However, this means that you would need to resolve conflicts as multiple non-identical text segments could be labeled for the same slot.

## IE Examples

below contains examples over some typical datasets you would deal with.

### MUC

> Message Understanding Conference (MUC) was an annual event/competition where results were presented.

But since this is found by DARPA, it cares more about military related. Focused on extracting information from news articles:

- Terrorist events

- Industrial joint ventures

- Company management changes

### Medline Corpus

Contains medical related data

|                            Input                            |                        NER: Proteins                        |
| :---------------------------------------------------------: | :---------------------------------------------------------: |
| ![image-20220330164520567](NLP/image-20220330164520567.png) | ![image-20220330164559910](NLP/image-20220330164559910.png) |

### Web Extraction

Another example would be **web extraction**

- because those dynamically webpages generally reads data from database, it is often a **semi-structured** data
- i.e., we want to view the webpage back as a **structured table/database**. An extractor for this case is often called a **wrapper**

An example would be the Amazon Book Descriptions pages:

|                      Input Information                      |                    Extracted Information                    |
| :---------------------------------------------------------: | :---------------------------------------------------------: |
| ![image-20220330164859648](NLP/image-20220330164859648.png) | ![image-20220330164910953](NLP/image-20220330164910953.png) |

where note that here:

- the **template could be learnt** as well

- since it is from HTML, we could image many rule-based approach works equally well

e.g. using **Enhanced REGEX** including

- `\b` for word boundary
- `\d` for a digit
- `{}` repetition operator, e.g. `A{1,5}` means you can have one-to-five `A`

Some examples include (in Perl):

- `/\b(\(\d{3}\)\s?)?\d{3}-\d{4}\b/` for US phone number

Hence for filling in each field:

- Price pattern: `\b\$\d+(\.\d{2})?\b`. However, we also need to to **identify proper context**.

  - Pre-filler pattern: `<b>List Price:</b> <span class=listprice>`
  - Post-filler pattern: `</span>`

  those could also be **induced**, e.g  by RAPIER

  <img src="NLP_part2/image-20220331171232523.png" alt="image-20220331171232523" style="zoom:50%;" />

- of course, for other non-html format, REGEX may not work as well hence you need to train classifiers

## Evaluating IE Accuracy

Consider we have a **fixed template** to begin with, which has $N$ slots. Then:

- our model extracted $C$ correct slot/value pairs in the template
- extracted a total number of $E$ pairs

Then we can apply 

- precision=$C/N$
- recall=$C/E$
- $F_1$ measure from the above

# Question Answering

> The goal of question answering is to build systems that **automatically answer questions** posed by humans in a natural language

Most question answering systems focus on a particular subset of these information needs: **factoid questions**, questions that can be answered with simple facts expressed in short texts:

- e.g. "*Where is the Louvre Museum located?*"

In general, the two common paradigms that solve this task are

- **Information-retrieval (IR) based QA**, sometimes called **open domain QA**, which relies on the vast amount of text on the web:
  1. Given a user question, perform pre-processing of the question including tokenization/normalization
  2. information retrieval is used to *find relevant passages*, i.e. find documents that contain an answer
     - using cosine similarity of some vector representation of the query and documents you have
  3. Then neural ***reading comprehension*** algorithms read these retrieved passages
  4. Draw the answer as a *span of text* from the passage (like SAT, GRE exams)
- **knowledge-based question answering**, which 
  1. builds a semantic representation of the query
  2. These meaning representations are then used to ***query databases of facts***

> **Bonus**: for large pretrained language models (i.e. on next word prediction), they can sometimes *directly answer the question*. This work because huge pretrained language models have already encoded a lot of factoids

## Information Retrieval

> **Information retrieval** or IR is the name of the field encompassing the retrieval of all manner of media based on user information needs. The resulting IR system is often called a **search engine**.

In particular, the IR we consider is called **ad hoc retrieval**, which:

- a user poses a **query** to a retrieval system
- we output an **ordered set of documents** from some collection

<img src="NLP_part2/image-20220404202909433.png" alt="image-20220404202909433" style="zoom: 50%;" />

where:

- A **document** refers to whatever unit of text the system indexes and retrieves (web pages, scientific papers, etc)
- A **term** refers to a word in a collection, but it may also include phrases
- the vector hence can be a TF-IDF vector or a bag of words. Then we can measure similarity for ranking.

### Term weighting and document scoring

Recall that if we use TF-IDF

> **TF-IDF** essentially consist of two components:
>
> - **term frequency** in log, essentially captures the frequency of words occurring in a document (i.e. the word-document matrix)
> - **inverse document frequency**, essentially giving a **higher weight** to words that occur **only in a few documents** (i.e. they would carry important discriminative meanings)

The **term frequency** in a document $d$ is computed by:
$$
\text{tf}(t,d) = \log_{10}(\text{count}(t,d) + 1)
$$
where:

- we added $1$ so that we won't do $\log 0$, which is negative infinity

The **document frequency**  of a term $t$ is the number of documents it occurs in.

- note that this is different from **collection frequency** of a term, which is the total number of times the word appears in any document of the whole collection

For example: consider in the collection of Shakespeares 37 plays the two words *Romeo* and *action*

<img src="NLP_part2/image-20220210001201947.png" alt="image-20220210001201947" style="zoom:50%;" />

Therefore, we want to **emphasize discriminative** words like Romeo via the **inverse document frequency** or IDF term weight:
$$
\text{idf}(t) = \log_{10}\left( \frac{N}{\text{df}(t) } \right)
$$
where:

- apparently $\text{df(t)}$ is the **number of documents** in which term $t$ occurs and $N$ is the total number of documents in the collection
- so essentially, the fewer documents in which a term occurs, the higher this weight.
- notice that we don't need $+1$ here because the minimum document frequency of a word in your corpus would be $1$

- again, because the number could be large, we use a $\log$.

Here are some IDF values for some words in the Shakespeare corpus

<img src="NLP_part2/image-20220210001906935.png" alt="image-20220210001906935" style="zoom: 50%;" />

Finally, the TF-IDF basically then does:
$$
\text{TF-IDF}(t,d) = \text{tf}(t,d) \times \text{idf}(t)
$$
An example would be:

<img src="NLP_part2/image-20220210002036715.png" alt="image-20220210002036715" style="zoom: 67%;" />

where notice that:

- essentially it is **still a term-document matrix**, but it is **weighted by IDF**.
- notice that because $\text{idf(good)}=0$, the row vector for *good* becomes all zero: this word appears in every document, the tf-idf weighting leads it to be ignored (as it is not very informative anymore)

### Document Scoring

Then, the above provide a way to **represent a document by a vector** (vertically). Then essentially:

- encode document $d$ to vector $\vec{d}$

- encode query $q$ to vector $\vec{q}$

- measure similarity as score:
  $$
  \text{score}(q,d) = \cos (\vec{q},\vec{d}) = \frac{\vec{q}\cdot \vec{d}}{|\vec{q}||\vec{d}|}
  $$

However, since **most queries are short and contain unique words**, we want to approximate this by:
$$
\begin{align*}
\text{score}(q,d) 
&= \frac{\vec{q}}{|\vec{q}|}\frac{\vec{d}}{|\vec{d}|}\\
&= \sum_{t \in q} \frac{\text{tf-idf}(t,q)}{\sqrt{\sum_{t'\in q}\text{tf-idf}(t',q)^2}} \cdot \frac{\text{tf-idf}(t,d)}{\sqrt{\sum_{t'\in d}\text{tf-idf}(t',d)^2}}\\
&\approx \sum_{t \in q}  \frac{\text{tf-idf}(t,d)}{\sqrt{\sum_{t'\in d}\text{tf-idf}(t',d)^2}}\\
&= \sum_{t \in q} \frac{\text{tf-idf}(t,d)}{|\vec{d}|}
\end{align*}
$$
where the third approximation comes from:

- since we assume sparse query and unique words, hence
  $$
  \text{TF-IDF}(t,q) = \text{tf}(t,q) \times \text{idf}(t)\approx \text{idf}(t)
  $$
  as term frequency is approximately one

- then the inverse document frequencies will be a constant for each document $d$, hence $|\vec{q}|$ will not affect ranking

---

*For Example*: Consider utilizing the above to rank the following query/document

<img src="NLP_part2/image-20220404204729668.png" alt="image-20220404204729668" style="zoom:50%;" />

then, we first compute the TF-IDF like a **term-document matrix**

<img src="NLP_part2/image-20220404204951658.png" alt="image-20220404204951658" style="zoom:50%;" />

where as an example, we see that document 2 which is "*Sweet sorrow*" has:

- term frequency of only those two words appearing once, then TF is computed by
  $$
  \text{tf}(t,d) = \log_{10}(\text{count}(t,d) + 1)=\log_{10}(1+1) = \log_{10}(2)
  $$
  for both word

- document frequency of "*sweet*" appears in **three of the collection of documents**, and "*sorrow*" only appeared here hence a count of one.

Finally, we use this forumula to compute similarity by **iterating through words in the query**
$$
\sum_{t \in q} \frac{\text{tf-idf}(t,d)}{|d|}
$$
which basically iterates over the word "*sweet*" and then "*love*", hence we get:

<img src="NLP_part2/image-20220404205323421.png" alt="image-20220404205323421" style="zoom: 50%;" />

> **Note** that in this formula, if ==document $d$ contains none of the words in query $q$, then it will have a score of zero==. This is why we needed the **inverted index** to quickly filter documents and only get the ones containing terms in $q$.

---

In the previous example, we have included **preprocessing steps** such as:

- remove punctuations and convert to lower case
- (deprecated) remove high-frequency words (e.g. **stop words**) from both the query and document before representing them

where the latter is no longer used as it is partly handled by the TF-IDF weighting already and it could make query such as "*to be or not to be*" to be reduced to "*not*", which makes IR hard.

### Inverted Index

As mentioned before, In order to compute scores, we need to efficiently **find documents that contain words in the query**.

> The basic search problem in IR is thus to find all documents $d \in C$ a collection that contain a term $q\in Q$ a query.

An inverted index basically maps each word to **indices of documents $d$** that contains the word. For instance, consider the same example as before

|                             Text                             |                        Example TF-IDF                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220404204729668.png" alt="image-20220404204729668" style="zoom: 67%;" /> | <img src="NLP_part2/image-20220404210338771.png" alt="image-20220404210338771" style="zoom: 50%;" /> |

The idea is to establish an **index of all words in our database**, and we get

<img src="NLP_part2/image-20220404210357269.png" alt="image-20220404210357269" style="zoom: 67%;" />

where:

- each word containing its **document frequency** in $\{\}$, which is the same as the ones computed in previous example
- a **pointer** to a postings list that contains document IDs and term counts in $[]$.
  - e.g. the word "*nurse*" appeared in document ID 1 and 4, each once.

Then, with this structure, we can:

- given a list of terms in query
- very efficiently get lists of all candidate documents **and** together with the TF and DF values, hence easily compute the scores.

### IR with Dense Vectors

The score and hence collection of document returned using TF-IDF heavily depended us to have **overlapping words in the document**.

> In other words, the user posing a query (or asking a question) needs to **guess exactly what words** the writer of the **answer** might have used to discuss the issue.
>
> - e.g. we want to search for "*tragic love story*" but Shakespeare writes instead about "*star-crossed lovers*".
>
> This is called the **vocabulary mismatch problem**

Therefore, the solution is to find a representation that can handle ==synonymy==: instead of (sparse) word-count vectors, ==using (dense) embeddings==, e.g. encoders like BERT.

In fact, in what is sometimes called a **bi-encoder** we use two separate encoder models $\text{BERT}_Q, \text{BERT}_D$:

- one to **encode the query** $q$
  $$
  h_q = \text{BERT}_Q(q)[\text{CLS}]
  $$
  where since we ==need a single vector==, we take the embedding of the $[\text{CLS}]$ token which encodes all information of the sequence

- one to **encode the document $d$**
  $$
  h_d = \text{BERT}_D(d)[\text{CLS}]
  $$

- use the dot product between these two vectors as the score
  $$
  \text{score}(d,q) = h_d \cdot h_q
  $$

Hence graphically

<img src="NLP_part2/image-20220404211226917.png" alt="image-20220404211226917" style="zoom:50%;" />

---

Some challenges in this area is:

- Among the many areas of active research are **how to do the fine-tuning** of the encoder modules on the IR task
- Efficiency is also an issue. For TF-IDF version we had inverted index which allowed us to quickly rank documents. But here, with **dense vectors we need a Nearest Neighbor search**, which is inefficient.
  - Modern systems therefore make use of approximate nearest neighbor vector search algorithms like ==Faiss==

## IR-Based Factoid QA

Finally, we come back to **how to perform QA task using IR**.

> The goal of IR-based QA (sometimes called open domain QA) is to answer a user's question by
>
> 1. finding a **related document** among the web or some other large collection of documents. (IR task)
> 2. finding **short text segments** from the selected related documents (what we are doing here)

Some examples of what we need to do include:

<img src="NLP_part2/image-20220404212126291.png" alt="image-20220404212126291" style="zoom:50%;" />

Therefore the dominant paradigm implementation basically includes two steps:

<img src="NLP_part2/image-20220404212600688.png" alt="image-20220404212600688" style="zoom:67%;" />

which is essentially a ==retrieve and read model==

1. use **retriever** to retrieve relevant passages from a text collection, usually using a search engines
   - this we have discussed before in IR, e.g. use Dense Vectors for embedding
2. a neural **reading comprehension** algorithm passes over each passage and ==finds spans that are likely to answer the question==
   - given the query $q$ and a sample passage $p$ that could contain the answer
   - return an answer $s$ or perhaps declare there is no answer
   - hence here it becomes a **close domain QA** as we only have a fixed set of selected documents to look at

For the above to work, we will need a few components in the pipeline (we have already discussed the retriever in IR):

- datasets for reading comprehension
- how to make a neural reader

### IR-based QA: Datasets

Our aim is to **train a reading comprehension systems**, so that given a passage and a question, and predicts a span in the passage as the answer. Therefore, we consider **comprehension datasets** containing tuples of *(passage, question, answer)*.

- **Stanford Question Answering Dataset (SQuAD)**, consists of passages from Wikipedia and associated questions whose answers are spans from the passage
- Squad 2.0 in addition adds some questions that are designed to be unanswerable

Examples look like:

<img src="NLP_part2/image-20220404213333825.png" alt="image-20220404213333825" style="zoom:50%;" />

However, those datasets are often constructed by having annotators first read the passage, then construct QAs. This could make the questions easier. Hence, we also attempted to make datasets from **questions that were not written with a passage in mind**.

- **TriviaQA dataset** resulting in 650K question-answer-evidence triples

- etc.

### IR-based QA: Reader

The retriever have now found a couple of relevant documents. Hence

> The **reader's** job is to take a passage and a question as input and **produce the answer**.
>
> - In the **extractive QA** we discuss here, the answer is a **span of text** in the passage
> - (skipped )more difficult task of **abstractive QA**, in which the system can write an answer which is not drawn exactly from the passage.

For example:

- given a question like *How tall is Mt. Everest?* and a passage that contains the clause *"Reaching 29,029 feet at its summit, a reader will output 29,029 feet."*
- output "*29,029 feet.*"

> Then this is a **span-labelling task** (e.g. [Span-based NER](#Span-based NER)), i.e. we **output a span** that we think constitutes the answer
>
> - given a question $q$ of $n$ tokens, $q_1,...,q_n$, and a passage $p$ of $m$ tokens that could contain the answer $p_1,...,p_m$
> - return $p(a|q,p)$ for each span $a \in S(p)$ a set of possible spans in $p$

Alike NER, we would need to score all possible spans, but we can make a **assumption/simplification** that
$$
P(a|q,p) = P_{start}(a_s|q,p)P_{end}(a_e|q,p)
$$
for basically $P_{start}(i|q,p)$ means token $p_i$ is the start of the span, and similarly for $P_{end}(j|q,p)$. How do we model such a probability? A standard baseline would be using BERT

<img src="NLP_part2/image-20220404215201459.png" alt="image-20220404215201459" style="zoom:67%;" />

where:

- since we are conditioned on $q,p$, we pass **both as input to the encoder**

- we add two new special vectors: a **span-start embedding $S$** and **a span-end embedding $E$**, which will be learned in fine-tuning so that
  $$
  P_{start}(i|q,p) = \text{Softmax}(S \cdot \vec{p}_i') = \frac{\exp(S \cdot \vec{p}_i')}{\sum_j \exp(S \cdot \vec{p}_j')}
  $$
  where the embedding vector $\vec{p}_i'$ of token $p_i$ would have already contained information from $q$. Similarly
  $$
  P_{end}(i|q,p) = \text{Softmax}(E \cdot \vec{p}_i') = \frac{\exp(E \cdot \vec{p}_i')}{\sum_j \exp(E \cdot \vec{p}_j')}
  $$

- then, the training objective would be  maximizing the probability of correct start and end positions, hence minimizing negative log likelihood:
  $$
  L = -\log P_{start}(i^*|q,p) - \log P_{end}(j^*|q,p)
  $$
  where $i^*, j^*$ are the gold labels.

- for inference, we output the **highest $\arg\max_{i,j} P_{start}(i|q,p)P_{end}(j|q,p)$ ** which is the same as finding the **highest score of**
  $$
  \arg\max_{i,j} P_{start}(i|q,p)P_{end}(j|q,p) = \arg\max_{i,j} S \cdot \vec{p}_i' + E \cdot \vec{p}_j'
  $$
  as products of the exponentials (from the Softmax) are basically comparing sums of exponents 

---

Other model prior to BERT include BiDAF, Bidirectional Attention Flow model:

![image-20220404225037681](NLP_part2/image-20220404225037681.png)

where the output is the same, $i,j$,  but here the major difference is that

- **attention**: query to context attention, i.e. $q\to d$, and context to query attention, i.e. $d \to q$

  |                             C2Q                              |                             Q2C                              |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |
  | ![image-20220404225356688](NLP_part2/image-20220404225356688.png) | ![image-20220404225408943](NLP_part2/image-20220404225408943.png) |

  so that:

  - Context-to-query attention: For each context word, choose the most relevant words from the query words.
  - Query-to-context attention: choose the context words that are most relevant to one of query words.

- **modelling**: uses Bidirectional LSTM for **encoding**, which is **sequential** and hence not efficient. So, its speed is suboptimal compared to Transformer based which suitable for **parallelization**.

## Knowledge-based QA

> **Knowledge-based question answering**: the idea of answering a natural language question by mapping it to a query over a **structured database**.

Two common paradigms are used for knowledge-based QA.

1. **graph-based QA**, models the knowledge base as a graph, i.e. entities as nodes and relations or propositions as edges between nodes.
2. **QA by semantic parsing**, using the semantic parsing methods

Both of the methods above would need some entity linking step. Hence this will be discussed first.

### Entity Linking

>  **Entity linking** is the task of associating a **mention in text** with the representation of some **real-world entity** in an ontology.
>
> - The most common ontology for factoid question-answering is Wikipedia, since Wikipedia is often the source of the text that answers the question

For example, 

- given the sentence "*Paris is the capital of France*"
- the idea is to determine that "*Paris*" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred to as "*Paris*".

In many practical cases, the set of entities will be the **set of Wikipedia articles**, where each article itself is an unique entity.

- skipped, but can be treated as a sequence labelling task and use NN based methods

### Knowledge-Based QA from RDF stores

Here we focus on the very simplest case of **graph-based QA**, in which the dataset is a set of factoids in the form of **RDF triples**

> RDF triple is a 3-tuple, a **predicate** with **two arguments**, expressing some simple relation or proposition. An example would look like
>
> <img src="NLP_part2/image-20220404221421943.png" alt="image-20220404221421943" style="zoom:67%;" />
>
> Then, we can use this by:
>
> - locate the entity asked in the query $Q$
> - check which relation is it asking, e.g. *birth-year*?
> - return the answer in this RDF triplet

Lets assume weve already done the stage of entity linking introduced in the prior section. Thus weve **mapped already** from a textual mention like *Ada Lovelace* to the canonical entity ID in the knowledge base. Then:

- determine which relation is being asked about. e.g.
  $$
  \text{When was ... born} \to \text{"birth-year"}
  $$
  so that the question becomes:
  $$
  \text{When was Ada Lovelace born?} \to \text{birth-year (Ada Lovelace, ?x)}
  $$
  For simple questions, where we **assume the question has only a single relation**, relation detection and linking can be done in a way resembling the neural entity linking models: computing **similarity (generally by dot product) between the encoding of the question text and an encoding for each possible relation**.

- then, once we located the relation like above, return the answer by fetching in the RDF database

### Knowledge-Based QA by Semantic Parsing

The second kind of knowledge-based QA uses a **semantic parser** to **map the question to a structured program** to produce an answer.

- basically into a query language like SQL or SPARQL

*Examples* of input question and output formatted query looks like:

<img src="NLP_part2/image-20220404222033378.png" alt="image-20220404222033378" style="zoom:50%;" />

where the logical form of the question is thus either in the form of a query or can easily be converted into one (predicate calculus can be converted to SQL, for example).

> In a **supervised** case, the task is then to take those pairs of training tuples (question and logical form pair) and produce a system that maps from new questions to their logical forms.

A common baseline algorithm is a simple sequence-to-sequence model, for example using BERT:

- using BERT to represent question tokens
- passing them to an encoder-decoder

<img src="NLP_part2/image-20220404222259514.png" alt="image-20220404222259514" style="zoom:67%;" />

---

*Recall*: Encoder-Decoder

Essentially for decode, we usually do it **auto-regressively** so that suppose our ==decoder== has `Attention` with `MAX_SEQ_LEN=32`. Then:

<img src="NLP_part2/transformer_decoding_1.gif" style="zoom: 50%;" />

the input to decoder done **auto-regressively**:

- at $t=0$, there is no input/or we have `<pad><pad>...<s>` for filling the 32 sequence length and a positional embedding
- get cross attention from encoder output
- generate an output "`I`" and feed back as input at $t=1$. So we **get `<pad><pad>...<s>I` as the input of decoder**
- repeat until `</s>` is generated

> **Note**: Comparing with using a RNN based decoder, when we are generating output $o_{t+1}$, the *difference* is:
>
> - **transformer** based can **only condition/read in** $o_{t-31},...,o_t$ for a max sequence length of `32` for the attention layer
> - **RNN** based can use $h_t$ which ==encodes all previous information==. However, it has ==no attention/is sequential==.

Essentially it is a Seq-2-Seq architecture, hence often done auto-regressively.

## Using Language Models

> An alternative approach to doing QA is to query a pretrained language model, forcing a model to answer a question **solely from information stored in its parameters**.

The popular model in this case is the **T5** language model, which is 

- an **encoder-decoder** architecture (mentioned above)
- pretrained to fill in masked **spans of task**

The general picture of what it does is

<img src="NLP_part2/image-20220404223704483.png" alt="image-20220404223704483" style="zoom:67%;" />

where as it is trained as a language model, we needed to **finetune** the T5 system to the question answering task, by giving it a question, and training it to output the answer text in the decoder.

## Evaluation of Factoid Answers

**Factoid question answering** is commonly evaluated using mean reciprocal rank, or **MRR**

> **MRR** is designed for systems that **return a short ranked list of answers** or passages for each test set question, so that:
>
> 1. each test set **question** is **scored** with the **reciprocal of the rank** of the first correct answer
> 2. then MRR is the **average of the scores** for each question in the test set

For example, if the system returned 

- five answers to a question (with its own ranking)
- but the first three are wrong (so the highest-ranked correct answer is ranked fourth)

Then the reciprocal rank for that question is $1/4$. (The score for questions that return no correct answer is $0$.)

Therefore, more formally, for a system returning ranked answers to each question in test set $Q$, then MRR is
$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$

---

On the other hand, **reading comprehension systems** on datasets like SQuAD are evaluated by:

- **Exact match**: The % of predicted answers that match the gold answer exactly
- **F1 score**: The average word/token overlap between predicted and gold answers.
  1. Treat the prediction and gold as a bag of tokens, 
  2. compute F1 for each question
  3. return the average F1 over all questions.

# Language Generation

> **Text generation** is a subfield of natural language processing (NLP). It leverages knowledge in computational linguistics and artificial intelligence to **automatically generate natural language texts**

Typically, you will see architectures generating a text be:

<img src="NLP_part2/image-20220406211349237.png" alt="image-20220406211349237" style="zoom: 67%;" />

where essentially:

- the language model's aim is to output a distribution for next word $w_{n+1}$, given the input $w_1,..,w_n$
- hence essentially it is outputting $P(w_{n+1}|w_{i},...,w_n)$, hence called the language model
- this is usually done in an **auto-regressive** way, i.e. generated output is fed back as input **until** we generated some end of sentence token, such as `</s>`.
- this architecture is related to many down stream tasks such as Machine Translation, Dialog Generation, etc.

## E2E Challenge

Here, we will discuss an example application, which is the **E2E challenge**:

- **domain**: restaurant recommendation
- **task**: we want to generate texts (one or more sentences) to recommend restaurants from an **input** **meaning representation** (MR)

An example of meaning representation include

|                          Example MR                          |                           Labelled                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220406162455714.png" alt="image-20220406162455714" style="zoom: 33%;" /> | <img src="NLP_part2/image-20220406162446553.png" alt="image-20220406162446553" style="zoom: 33%;" /> |

where essentially your output should fulfill the above requirement specified in the MR.

- **dialogue act**: in this competition, *request* or *inform* (here we only have two)

- **attributes/values**: unordered attributes of the generated text that you need to fulfill (all those attributes needs to be mentioned) this will be a fixed set, which is called ==domain ontology==:

  <img src="NLP_part2/image-20220406163143626.png" alt="image-20220406163143626" style="zoom:33%;" />

  so in this task, there will only be 8 types.

An example output that fulfills the about MR would be

<img src="NLP_part2/image-20220406163005861.png" alt="image-20220406163005861" style="zoom:50%;" />

### E2E Datasets

Finally, before we discuss how to construct a model, it is important to know **how training data is collected**:

- Crowd sourcing on **CrowdFlower** (similar to Amazon Mechanical Turk)
- essentially it asks you to generate the text given the MR (either in a pure text format or pictorial)

An example would be

|                          Pictorial                           |                           Text MR                            |                     Sample Training Text                     |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220406212209031](NLP_part2/image-20220406212209031.png) | ![image-20220406212215241](NLP_part2/image-20220406212215241.png) | ![image-20220406212221398](NLP_part2/image-20220406212221398.png) |

why did we also provided a pictorial representation to the CrowdFlower works? 

- The aim is to generate good training text from **human workers**.
- it turs out that they generate better texts if a picture is provided.

Then, you get **training dataset**:

- 6K MRs with average of 8.27 texts per MR

- 5 slots (attribute-key pairs) per MR

## Text Generation Modelling

Traditionally, for conditional text generation tasks, you will need to consider

- **Content selection** (Done for us as MR in the E2E task)
- **Aggregation**: which pieces of content go into which sentence? e.g. generate abstract syntax tree from selected content
- **Realization**: tree to real sentence

Which results in two paradigms:

- **Two step generation**: first find a sentence planning. Then, given a plan, realize it into natural language.
- **Joint one-step approach**: usually use Seq-2-Seq model, where input sequence would be a string of the requirements (e.g. MR)

For example:

|                           Input MR                           | Input MR into Tree                                           |                         Realization                          |
| :----------------------------------------------------------: | ------------------------------------------------------------ | :----------------------------------------------------------: |
| ![image-20220406164446805](NLP_part2/image-20220406164446805.png) | ![image-20220406164427972](NLP_part2/image-20220406164427972.png) | ![image-20220406164433839](NLP_part2/image-20220406164433839.png) |

### Generating using Seq-2-Seq

This is often the architecture used today, where we essentially have:

1. encode the input MR as as string

   - either make it a sequence of triples `(Dialogue Act, Slot, Value)`, so that you could get input like

     <img src="NLP_part2/image-20220406213200708.png" alt="image-20220406213200708" style="zoom: 50%;" />

   - or find a syntax tree and then use the string flattened version of the tree

     |                             Tree                             |                        String Version                        |
     | :----------------------------------------------------------: | :----------------------------------------------------------: |
     | <img src="NLP_part2/image-20220406213258926.png" alt="image-20220406213258926" style="zoom: 50%;" /> | <img src="NLP_part2/image-20220406213304043.png" alt="image-20220406213304043" style="zoom:50%;" /> |

2. **input** the above string

3. **output** generated text

Then, the Seq-2-Seq model basically does

<img src="NLP_part2/image-20220406164807069.png" alt="image-20220406164807069" style="zoom: 50%;" />

which is **basically a auto-regressive generation**.

- of course, you could also replace LSTM blocks with transformers

- the output at each state is essentially $P(y_{t}|y_1,...,y_{t-1},h_x)$ where $y_i$ are the generated ones and $h_x$ is the hidden state representation of the input sequence. Here we are only outputting the **single best word** at a time, hence
  $$
  o_t = \arg\max_y P(y_{t}|y_1,...,y_{t-1},h_x)
  $$
  but remember this is a ==greedy search==. In reality you can use **beam search** to allow for keeping more than one options.

### Problems with Seq-2-Seq

But, as we are using NN, there are some bigger problems. We **will not be able to enforce constraints** such as

- Missing to fulfill an attribute
- Added an attribute/value that did not exist in MR 
- Wrong value for an attribute (hallucination).

*For instance*: the red parts are the wrong values not mentioned in the MR

|                           Input MR                           |         Seq-2-Seq output with Beam Search of width 3         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220406214731888.png" alt="image-20220406214731888" style="zoom:50%;" /> | <img src="NLP_part2/image-20220406214756242.png" alt="image-20220406214756242" style="zoom:50%;" /> |

some intuition of why, instead of *Burger King*, we get those random place names:

- essentially the network learns some hidden representation of the words in the MR. So suppose we have the following training data

  | <img src="NLP_part2/image-20220406215139282.png" alt="image-20220406215139282" style="zoom: 50%;" /> | <img src="NLP_part2/image-20220406215144295.png" alt="image-20220406215144295" style="zoom: 50%;" /> |
  | :----------------------------------------------------------: | :----------------------------------------------------------: |

  then the model would have learnt a **similar vector representation** between "*Burger King*" and "*Yippee Noodle Bar*"

- then as what the models see are representations of "*Burger King*", it is likely that "*Yippee Noodle Bar*" could be outputted

How do we deal with those problems?

- **Missing/Added value**: use a **Re-ranker**, which is an additionally penalty using **hamming distance**

  <img src="NLP_part2/image-20220406165243051.png" alt="image-20220406165243051" style="zoom: 33%;" />

  where the sequence `110100` will be the output of an additional classifier (which we hope is doing the correct job)

  - it is called reranker as it adds the penalty and reranks the output
  - since we need to rank outputs, we would use a **beam search**
  - of course, this does not solve the problem entirely, but does help a bit

- **Wrong Value**: using data augmentation techniques - **faithful speaker**

Results

<img src="NLP_part2/image-20220406170039892.png" alt="image-20220406170039892" style="zoom:33%;" />

where we see:

- re-ranked + beam search does better than raw beam search
- raw string input does better than tree as it has more flexibilities.

> **However**, on many hard constraint tasks, seq2seq often fail to correctly express a meaning representation hence can be outperformed by hand-engineered solutions. But the key advantage is that it is much less costly in time and money.

### E2E Data Augmentation

Here, we discuss how to use data augmentation to solve the **wrong value problem**. We will discuss two approaches:

- **delexicalization**: simplest data augmentation without ML
- **faithful speaker**: data augmentation using ML. Done by noise injection and self-training.

#### Delexicalization

Basically we can

1. identify the lexical from the MR in the generated text
2. remove (some of) those lexicals, hence we obtained a **template** from MR -> text
3. generate new MR -> text using that template

For example:

|                           Train MR                           |                          Train Text                          |                   Delex Train Text Example                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220406220208189](NLP_part2/image-20220406220208189.png) | ![image-20220406220221854](NLP_part2/image-20220406220221854.png) | ![image-20220406220237532](NLP_part2/image-20220406220237532.png) |

#### Noise Injection and Self-Training

> Aim: use data augmentation to create **diverse MR/utterance pairs** not seen in the training distribution
>
> - the delexicalization technique would have created pairs of the same syntax/from same template

Basically we would need two constraints for this to work:

- text of a MR being diverse but is still related to the task (**Base Speaker**)
- the correct MR corresponding to the above text (**MR Parser**)

|                        Without Noise                         |                With Noise = Data Augmentation                |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220406170914463.png" alt="image-20220406170914463" style="zoom: 50%;" /> | <img src="NLP_part2/image-20220406224952166.png" alt="image-20220406224952166" style="zoom:50%;" /> |

Hence we have a **synthetic text and MR** pair, but the pair is ==still faithful==! The overall algorithm looks like

<img src="NLP_part2/image-20220406171201842.png" alt="image-20220406171201842" style="zoom: 50%;" />

where 

- to make sure the new pair is not too bad/unclean, we also added a filter.

- in the end you return $A$ which is the **augmented dataset.** 

> Train new seq2seq model on training + synthetic data: $D \cup A$. The new trained model is the called the **Faithful Speaker**

---

*For example:*

<img src="NLP_part2/image-20220406171433324.png" alt="image-20220406171433324" style="zoom:50%;" />

---

The advantage of this v.s. the **delexicalization** is that the generated text would have:

- variety in **words** used (which delexicalization also can do)
- variety in **sentence structure** (which delexicalization cannot)

### E2E Metrics and Baseline

Some common metrics used include

- **Precision**: want fulfilled tokens being correct

- **Recall**: want to have fulfilled most of the tokens.

- **Semantic Error Rate** (SER): 
  $$
  \frac{\text{\# missing}+\text{\# incorrect}+\text{\# added}}{\text{\# attributes}}
  $$
  basically the denominator is the sum of cases where the attributed was not fulfilled.

**Baselines** used in this task

- **SLUG**: ensemble of seq2seq models (e.g. transformer based, LSTM based, etc)
  - pooled overgeneration+reranking
- **DANGNT** (Nguyen and Tran, 2018): rule-based model
  - basically from the corpus, extract statistically 
  
- **TUDA** (Puzikov and Gurevych, 2018): template-based model

With their performance being

<img src="NLP_part2/image-20220406225848342.png" alt="image-20220406225848342" style="zoom:67%;" />

where we see **Faithful speakers** are performing better.

## Next Word Sampling

> **Next Word Sampling** means deciding **how to pick the next word** $w_t$ according to its *conditional probability distribution* $P(w_t|w_{1:t-1})$

Here we will see:

- **Greedy Search**: what we do most of the time, so that at each time $t$ of decoding, we pick the single most probable words. However, the problem is that this ==might not generate the most probable sequence==
- **Beam Search**: Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. But still, it does not solve the problem.
- **Top-k Sampling**: it is a *sampling* method (i.e. randomly choose) from the Top-$k$ most likely next word based on redistributing the probability mass only among those $k$ words
- **Temperature Sampling**: essentially rescaling the probability by temperature $T$, such that with $T$ is low only high energy (probability) would be "survive", and when $T$ is high, low energy states gets a greater chance than before to be sampled.

*Recap*:

**Beam Search**: recall that the basic idea under beam search is that it keeps the $k$ most probable at each time step

<img src="NLP_part2/image-20220406230644107.png" alt="image-20220406230644107" style="zoom:67%;" />

which can be done by:

- have $k$ decoders ready
- at each time step:
  - keep $k$ most probable choice
  - generate the next word from the $k$ most probable using the decoders
  - **prune down** to the $k$ most probable and repeat

### Top-k Sampling

**Top-k Sampling**: at each time step we get $P(w_t|w_{1:t-1})$ from our model:

- sample from the Top-$k$ most likely next word based on **redistributing the probability mass** only among those $k$ words

- this is used in GPT-2

This works because notice that at $t=2$, the top-k choices would have included most of the probability distribution already:

<img src="NLP_part2/image-20220406231742503.png" alt="image-20220406231742503" style="zoom:67%;" />

### Temperature Sampling

> **Temperature sampling** works by increasing the probability of the most likely words before sampling. The idea is borrowed from thermodynamics where essentially ==energy $\approx$ probability== so that:
>
> - *high temperature* means low energy (probability) states are more likely (than the were before) to be encountered
>
> - *lowering the temperature* allows you to focus on higher energy (probability) state

So that given a probability distribution $P(x)$:

- scaled probability by temperature $T$ is $z_i = \log P(x_i)/T$
- then get back probability using softmax $\exp(z_i)/\sum \exp(z_i)$

*For instance*, we can take $T=2$ being a low temperature:

|                            $P(x)$                            |                            $z_i$                             |                     Temperature Rescaled                     |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220407001537601](NLP_part2/image-20220407001537601.png) | ![image-20220407001426680](NLP_part2/image-20220407001426680.png) | ![image-20220407001432607](NLP_part2/image-20220407001432607.png) |

where we see that with low temperature, only high probability terms "survives". Result-wise:

- **high temperature** sample displays greater **linguistic variety**
- **low temperature** sample is more **grammatically correct**

# Text Summarization

The high level task of summarization would include:

- **input**: a database, an image, texts, software traces, etc
- **output**: a textual summary

> **Essentially** those types of generation models can **all be seen as next word generation**, but conditioned on different things:
>
> - **pure next word generation**:
>   $$
>   p(y_{i+1}|y_{i-L+1:i})
>   $$
>   for a window size of $L$ (depending on your machine's memory for NN models) and $y_{i-L+1:i}$ are the words generated so far. Note that this is basically ==language models==
>   $$
>   p(w_{i+1}|w_{i-L+1:i})
>   $$
>   which we have learnt in the beginning of NLP.
>
> - **next word generation conditioned**/based on some input sentence $\vec{x}=x_1,...,x_n$ as well:
>   $$
>   p(y_{i+1}|\vec{x},y_{i-L+1:i})
>   $$
>   which you will see is what ==summarization/conditional generation== is essentially about: modelling the above probability with some neural network.

Currently this field does not have the best performances. Why is this task hard?

- require **both interpretation and generation** of text
- handle **input** documents from **unrestricted domains** robustly (e.g. trained on one domain but genearlize to news, military, entertainment)

## Types of Summarization

In general, we can have the following types of summarization tasks:

- **informative** v.s. **indicative**: generated content can replacing the document v.s. just describes the document
- **extractive** v.s. **abstractive**: pick best $k$ words that most important v.s. generate $k$ words
- **single** v.s. **multi-document:** many coming form different domains
- **generic** v.s. **user-focused**: decides if we should use layman terms (e.g. commonly when in medical/legal domain, use professional terms)

Here, we will focus on mainly doing ==informative== summarization and compare:

- simple task of **extractive** summary
- using a NN model for **abstractive** summary

## Extractive Summarization

The formal definition of the task would be as follows.

- **given**: a fixed vocabulary $V$

- **input**: a sequence of $M$ words with $x_1,...,x_M$

- **output**: a sequence of $N < M$ words (e.g. known before generation) $x_{m_i} \in \{x_1,...,x_M\}$ such that
  $$
  \arg\max_{m \in \{1,...,M\}^N}s(\vec{x},\vec{x}_{m})
  $$
  is largest for $\vec{x}=x_1,...,x_M$ and $\vec{x}_m=x_{m_1},x_{m_2},...,x_{m_N}$, and $s$ is a **scoring function** yuo can generally customize. The most common one you will see for LM are:
  $$
  s(\vec{x},\vec{x}_m) = \log p(\vec{x}_m | \vec{x})
  $$

The common solution for this is to treat it as a **classification task**: 

- for input word $x_i$, should it be included in output list
- repeat, and output by **concatenating** those words

In fact, for humans summary writes, they often do cut and paste

- 81% of the sentences were constructed by cutting and pasting

## Abstractive Summarization

Not only pick $k$ words/phrase, but also **generate** words/phrase that summarizes the content.

- if we just concatenate sentences as above, they might not be coherent

Formally, such a task can be defined as

- **given**: a fixed vocabulary $V$

- **input**: a sequence of $M$ words with $x_1,...,x_M$

- **output**: a sequence of $N < M$ words (e.g. known before generation) $y_1,...,y_N$ such that
  $$
  \arg\max_{\vec{y}\in \mathcal{Y}}s(\vec{x},\vec{y})
  $$
  basically for $y_i \in V$, and $s$ is a **scoring function** you can generally customize. The most common one you will see for LM are:
  $$
  s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})= \sum_{i=0}^{N-1} \log p(y_{i+1}|\vec{x},y_{1:i})
  $$
  which is again(conditional) next word generation, and is often solved **greedily using auto-regressive decoder**.

You will soon seen in the [Attention Based Summarizer](#Attention Based Summarizer) how this probability $\log p(\vec{y} | \vec{x})$ can be ==modelled using a neural network==.

### Attention Based Summarizer

Here we discuss essentially a model proposed here: https://arxiv.org/abs/1509.00685, which is used for **headline generation**:

| Input:  | A detained iranian-american academic accused of acting out against national security has been released from a tehran prison yesterday after a hefty bail was posted, a top judiciary official said Tuesday. |
| ------- | ------------------------------------------------------------ |
| Output: | Detained iranian-american academic released from prison after hefty bail. |

Recall that we essentially need to model:
$$
s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})
$$
Since you will be using attention based model, the hidden representation for **output** will be applied over some attention layer. First, just like how we derived unigram/bigram/etc models:
$$
p_\theta(\vec{y}|\vec{x})=p_\theta(y_1,...,y_N|x_1,...,x_N) = \prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{1:i})
$$
being exactly true, but since for attention based model we need some **predefined sequence length** $C$ (as opposed to RNN type where you can encode a variable length by just looping it) such that:
$$
\prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{1:i}) \approx \prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{i-C+1:i})
$$
and hence for $s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})$ we get:
$$
\log p(\vec{y}|\vec{x}) \approx \sum_{i=0}^{N-1} \log p_\theta(y_{i+1}|\vec{x},y_{i-C+1:i})\equiv  \sum_{i=0}^{N-1} \log p_\theta(y_{i+1}|\vec{x},\vec{y}_C)
$$
being our model.

> Then for ==NN based model==, essentially the idea is to do:
> $$
> p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}(W \, \text{repr}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}
> $$
> so that $W \in \mathbb{R}^{|V| \times H}$ for $ \text{repr}(\vec{x},\vec{y}_C)$ being some vector of hidden dimension $H$.

For this paper, we consider:
$$
p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}(V\vec{h} + W\,\text{enc}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}
$$
for the first part being empirically useful to have as:

- $\tilde{y}_C = [Ey_{i-C+1},...,Ey_{i}]$ are the **embedded generated outputs**
- $h = \tanh(U \tilde{y}_C  )$ producing a single vector summarizing what we have so far

So basically we have the **general architecture of** splitting this to an encoder-decoder architecture:

- **encoder**: do the $\text{enc}(\vec{x},\vec{y}_C)$ part and ==get a vector of hidden dimension $H$==

- **decoder**: do the $\text{Softmax}(V\vec{h} + W\,\text{enc}(\vec{x},\vec{y}_C))$ part with ==auto-regressive generation==

<img src="NLP_part2/image-20220412010502499.png" alt="image-20220412010502499" style="zoom: 67%;" />

where:

- for attention based summarizer, the encoder part is essentially an **attention based encoder**
- however, of course you can customize what you do at that encoder

#### Baseline BoW Encoder

Consider the simple framework of doing:

1. Encode all the input but **ignoring generated context**:
   $$
   \tilde{x} = [Fx_{1},...,Fx_{M}]
   $$
   for $F \in \mathbb{R}^{H \times |V|}$

2. Then we consider a uniform distribution
   $$
   \vec{p} = [1/M,...,1/M]
   $$
   so that
   $$
   \text{enc}_1(\vec{x},\vec{y}_c) = \vec{p}^T \tilde{x}
   $$

so then the entire encoder network only needs $\theta_{enc} = \{F\}$

#### Convolutional Encoder

<img src="NLP_part2/image-20220412011723758.png" alt="image-20220412011723758" style="zoom:80%;" />

#### Attention Based Encoder

|                          Algorithm                           |                         Architecture                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220412011753872](NLP_part2/image-20220412011753872.png) | ![image-20220412011804443](NLP_part2/image-20220412011804443.png) |

which performs better than the others:

<img src="NLP_part2/image-20220411170238208.png" alt="image-20220411170238208" style="zoom: 67%;" />

## BART

Now, essentially for **text generation** we want some architecture that does:

- **encoder**: encode input $\vec{x}$
- **decoder**: uses $\text{enc}(\vec{x})$ and some representation of $\vec{y}_C$ to generate:

Hence we can use a classical architecture:

|                             Idea                             |                        BART Pretrain                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220412015746372.png" alt="image-20220412015746372" style="zoom:67%;" /> | <img src="NLP_part2/image-20220412020010963.png" alt="image-20220412020010963" style="zoom:67%;" /> |

which essentially does:
$$
p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}( \text{repr}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}
$$
where here $\text{repr}(\vec{x},\vec{y}_C)$ would come from weigths that does:

- embedding of $\vec{x}, \vec{y}_c$ using ==two networks== $f_\theta(\vec{x}),g_\phi(\vec{y})$
  - in contrast to generation with **GPT**, which only has a single network for encoding $f_\theta(\vec{x}), f_\theta(\vec{y})$

- doing multi-head attentions
- doing FFNN and others.

> But essentially we **comes down to having some good encoding network**. Therefore BART consider ==pre-train tasks== of corrupting documents and then optimizing a **reconstruction** loss:
>
> <img src="NLP_part2/image-20220412020232001.png" alt="image-20220412020232001" style="zoom:50%;" />
>
> and notice that In the extreme case, where all information about the source is lost, BART is equivalent to a language model.

Then, once trained, we **fine-tune on a range of tasks**

- **Sequence Classification**: the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier.
  - basically related to the `CLS  `token in BERT
- **(Conditional) Sequence Generation**: the encoder input is the input sequence (to be conditioned on), and the decoder generates outputs autoregressively

### Comparison Against GPT and BERT

What is special about using both encoder-decoder for the task of generation? Recall that:

|                             BERT                             |                             GPT                              |                             BART                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220412021011283.png" alt="image-20220412021011283" style="zoom:67%;" /> | <img src="NLP_part2/image-20220412021017237.png" alt="image-20220412021017237" style="zoom:67%;" /> | <img src="NLP_part2/image-20220412021023236.png" alt="image-20220412021023236" style="zoom:67%;" /> |

so that:

- **BERT**: Random tokens are replaced with masks, and the document is encoded bidirectionally. Missing tokens are predicted independently, so **BERT cannot easily be used for generation**
- **GPT**: Tokens are predicted auto-regressively, meaning GPT can be used for generation. However words can only **condition on leftward context**, so it cannot learn bidirectional interactions.
  - also it uses the same encoding network at all times
- **BART**: Inputs to the encoder need not be aligned with decoder outputs, ==allowing arbitrary noise transformations==, i.e. can generate "new words" for tasks such as summarization.
  - then the *likelihood of the original document* (right) is calculated with an autoregressive decoder
  - notice that the **order changed** as this is allowed for encoder-decoder type model! We are using ==two networks== $f_\theta(\vec{x}),g_\phi(\vec{y})$ for encoding the input and the output generated differently, as their interaction could be complex

# Chatbots & Dialogue Systems

This chapter introduces the **fundamental** algorithms of dialogue systems, or conversational agents. These programs communicate with users in natural language (text, speech, or both), and fall into two classes:

- **Task-oriented dialogue agents** use conversation with users to help complete tasks. Examples include Siri, Alexa, Google Now/Home, Cortana, etc., which can give directions, control appliances, find restaurants, or make calls.

  - for those products, we also needed Automatic Speech Recognition and Text to Speech with an architetcure

    <img src="NLP_part2/image-20220413161715955.png" alt="image-20220413161715955" style="zoom:33%;" />

    where basically dialogue manager decides what actions to take

- **chatbots** are systems designed for extended conversations, set up to mimic the unstructured conversations or chats characteristic of human-human interaction

  - mainly for entertainment
  - but also for practical purposes like making task-oriented agents more **natural**.

well discuss the :

- three major **chatbot** architectures: rule-based systems, information retrieval systems, and encoder-decoder generators.
- for **task-oriented agents**, we introduce the frame-based architecture (the GUS architecture) that underlies most task-based systems

## Properties of Human Conversation

Before we attempt to design a conversational agent to converse with humans, it is crucial to understand something about how humans converse with each other. For example:

<img src="NLP_part2/image-20220413234847649.png" alt="image-20220413234847649" style="zoom: 67%;" />

where `C `stands for client and `A `stands for Agent

- A dialogue is a sequence of **turns** (C1, A2, C3, and so on)

  - A system has to know when to *stop talking*; the client *interrupts* (in A16 and C17), so the system must know to stop talking

  - A system also has to know when to *start talking*. e.g. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond.

    This task is called endpointing or **endpoint detection**

- **Speech/Dialog Acts**: each utterance in a dialogue is a kind of action being performed by the speaker. Basically expresses an important component of the ==intention of the speaker== (or writer) in saying what they said

- **Grounding**: acknowledging that the hearer has understood the speaker (e.g. saying OK, as the agent does in A8 or A10.)

- **Sub-dialogues** and **Dialogue Structure**: QUESTIONS and ANSQWE, PROPOSALS and ACCEPTANCE/REJECTION, etc. These pairs, called **adjacency pairs** are composed of a first pair part and a second pair part

  - However, dialogue acts arent always followed immediately by their second pair **side sequence** part. The two parts can be separated by a side sequence
  - e.g. utterances C17 to A20 constitute a correction subdialogue; another common one would be **clarification questions**

- **Initiative**: whoever is taking the lead in the conversation is taking initiative. In normal human-human dialogue, however, its more common for initiative to shift back and forth between the participants

  - **Mixed initiative**, when initiative shifts between the two people, is very difficult for dialogue systems to achieve.
  - In contrast, the QA system would be a **user-initiative** system: user specifies a query, and the systems passively responds.
  - Its much easier to design dialogue systems to be *passive responders*.

- **Inference** and **Implicature**: consider

  <img src="NLP_part2/image-20220413235809097.png" alt="image-20220413235809097" style="zoom:67%;" />

  Notice that the client does not in fact explicitly answer the agents question. We need to inference the answer.

## Chatbots

> The simplest kinds of dialogue systems are **chatbots** systems that can carry on extended conversations with the goal of **mimicking the unstructured conversations** or characteristic of informal human-human interaction

A recent example would be Facebook's BlenderBot:

<img src="NLP_part2/image-20220414000102746.png" alt="image-20220414000102746" style="zoom:67%;" />

Like practically everything else in language processing, chatbot architectures fall into two classes: **rule-based systems** and **corpus-based systems**

- **Rule-based systems** include the early influential ELIZA and PARRY systems
- **Corpus-based systems** are more modern:
  1. mine large datasets of human-human conversations, 
  2. generate responses by either
     - using information retrieval to copy a human response from a previous conversation
     - using an encoder-decoder system to generate a response from a user utterance

### Corpus-based Chatbots

**Corpus-based chatbots**, instead of using hand-built rules, mine conversations of human-human conversations. These systems are enormously **data-intensive**, requiring hundreds of millions or even billions of words for training.

Most corpus based chatbots **produce their responses** to a users turn in context either by

- **retrieval** methods: using information retrieval to grab a response from some corpus that is appropriate given the dialogue context
- **generation** methods: using a language model or *encoder-decoder* to generate the response given the dialogue context

Essentially the two architectures look like:

|                       Retrieval Based                        |                       Generation Based                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220414001622581.png" alt="image-20220414001622581" style="zoom:80%;" /> | <img src="NLP_part2/image-20220414001637909.png" alt="image-20220414001637909" style="zoom:80%;" /> |

So that given a training corpus $C$ which contains many conversations, and given a user's query being $q$, we consider:

---

**Retrieval based**: return some appropriate turn $r$ as response by:
$$
r = \arg\max_{r \in C} \frac{q \cdot r}{|q||r|}
$$
where $q,r$ could be vectors computed by:

- TF-IDF encoding
- train an **encoder** for query and one for response (left figure in the above)

For instance, we could train two encoders by fine-tuning BERT so that we get $\text{BERT}_Q,\text{BERT}_R$:
$$
h_q = \text{BERT}_Q(q)[CLS]\\
h_r = \text{BERT}_R(r)[CLS]
$$
where recall that the `CLS` token is used to encode the entire sentence. Then:
$$
r = \arg\max_{r \in C} h_q \cdot h_r
$$

---

**Generation Based**: think of response production as an encoder-decoder task  ==transducing from the users prior turn to the systems turn==. So basically can treat it as a kind of translation.

<img src="NLP_part2/image-20220414002318125.png" alt="image-20220414002318125" style="zoom:50%;" />

Recall that for encoder-decoder models, it is auto-regressively generating but **conditioned on the entire query $q$**:
$$
\hat{r}_{t+1} = \arg\max_{w \ni V} P(w|q,r_1,...,r_{t})
$$
but sometimes instead of just user's query, we can encode the **entire conversation** to be conditioned on.

---

Other ideas for generation:

- fine-tune a large language model/**decoder only** on a conversational dataset and use the language model directly as a response generator.
- https://arxiv.org/pdf/1606.01541.pdf using Deep Reinforcement Learning
- https://aclanthology.org/D17-1230.pdf using Adversarial Learning

## GUS: Simple Frame-based Dialogue Systems

> We turn now to **task-based dialogue**, in which a dialogue system has the goal of helping a user solve some task like making an airplane reservation

First some terminologies:

- A **frame** is a kind of knowledge structure representing the kinds of intentions the system can extract from user 
- A frame consists of a collection of **slots**, each of which can take a set of possible **values**.
  - basically specifies what the system ==needs to know==
- A set of frames is sometimes called a **domain ontology**.

For example, in the travel domain:

<img src="NLP_part2/image-20220414003439552.png" alt="image-20220414003439552" style="zoom: 67%;" />

where:

- a lot might be of type *city*, *date*, *time*

- this would be an **example of a frame**

- technically, many `Types` in such a system is itself a frame. For instance

  <img src="NLP_part2/image-20220414003637003.png" alt="image-20220414003637003" style="zoom:67%;" />

### Control structure for frame-based dialogue

> The system's goal is to **fill the slots** in the frame with the fillers the user intends, and **then perform the relevant action** for the user (answering a question, or booking a flight).

To do this, the system need to do:

1. **asks questions** of the user (using pre-specified question templates associated with each slot of each frame)
   - If a users response fills multiple slots, the system fills all the relevant slots, and then continues asking questions to fill the remaining slots
   - there are also **condition-action rules** attached to slots. For example: a rule attached to the `DESTINATION `slot for the plane booking frame, once the user has specified the destination, might automatically enter that city as the default `StayLocation `for the related hotel booking frame.
2. Many domains **require multiple frames**. Then this control system needs to disambiguate **which slot** of which frame a given input is supposed to fill
3. Once the system has enough information it **performs the necessary action** (like querying a database of flights) and returns the result to the user

Exactly how do we do step 1 and 2? This is in the section covered below.

### Determining Domain, Intent, and Slot fillers in GUS

How do we implement the above control system? We need to essentially do three things:

- The first task is **domain classification**: is this user for example talking about airlines?
- **user intent determination**: what general task or goal is the user trying to accomplish?
- Finally, we need to do **slot filling**: extract the particular slots and fillers that the user intends the system to understand

> The **slot-filling** method used in the original GUS system, and still quite common in industrial applications, is to use **handwritten rules**

### Generation in GUS

Finally, once all information are collected and necessary actions are performed by the system, we **perform language generation** to respond to user.

- Frame-based systems tend to use **template-based generation**
- or **encoder-decoder network** with dependency on the dialogue state (i.e. the question user just asked)

## The Dialogue-State Architecture

> Modern research systems for task-based dialogue are based on a more sophisticated version of the frame-based architecture called the **dialogue-state** or or **belief-state** architecture.

Moderm architecture looks like the following:

<img src="NLP_part2/image-20220414011150489.png" alt="image-20220414011150489" style="zoom:67%;" />

where:

- for audio input/output, we had this extra two modules on the left
- the other four components are part of both spoken and textual dialogue systems
  - For example, like the GUS systems, the dialogue-state architecture has a component (**Natural Language Understanding Module**) for extracting slot fillers from the users utterance, but generally using machine learning rather than rules.
  - The **dialogue state tracker** maintains the current state of the dialogue (which include the users most recent dialogue act, plus the entire set of slot-filler constraints the user has expressed so far).
  - **dialogue policy** decides what the system should do or say next: e.g. wen to ask clarification question, when to make a suggestion, etc.
  - **natural language generation**: condition on the exact context to produce turns that seem much more natural

### Dialogue Acts

Different types of dialogue systems **require labeling different kinds of acts**, and so the tagsetdefining what a dialogue act is exactly tends to be designed for particular tasks. For example

|                         Dialogue Act                         |                    Labelled Conversation                     |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220414020147489.png" alt="image-20220414020147489"  /> | <img src="NLP_part2/image-20220414020239096.png" alt="image-20220414020239096" style="zoom:80%;" /> |

note that the above example also showed the content of each dialogue acts, which are the **slot fillers** being communicated

### Slot Filling

> Here, we discuss how the task of slot-filling, and the simpler tasks of domain and intent classification, are typically done. This is still part of the job of the **Natural Language Understanding Module.**

We consider the following situation:

- **input**: *"I want to fly to San Francisco on Monday afternoon please"*

- **output**: domain is AIRLINE, intent is SHOWFLIGHT, and the fillers being

  <img src="NLP_part2/image-20220414020743955.png" alt="image-20220414020743955" style="zoom:67%;" />

A simple architecture that does is is simply:

<img src="NLP_part2/image-20220414020838190.png" alt="image-20220414020838190" style="zoom:67%;" />

which is basically a **Sequence classification** model such that:

- we use BERT to embed each input
- then pass though FFNN with a softmax so that:
  - for each word, decide which tag it is
  - for the last tag `<EOS>`, use SoftMax to classify which intent and domain it is

Then, once the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., San Francisco), and these word strings can then be normalized to the correct form in the ontology

### Dialogue State Tracking

> The job of the dialogue-state tracker is to determine both the **current state of the frame** (the fillers of each slot), as well as the most **recent dialogue act**.

Since dialogue acts place some constraints on the slots and values, the tasks of **dialogue-act detection** and **slot-filling** are often performed jointly.

<img src="NLP_part2/image-20220414021745317.png" alt="image-20220414021745317" style="zoom:67%;" />

The simplest dialogue state tracker might just take the output of a slot-filling sequence-model. But more complicated cases include:

- detecting correction acts
- value being changed in the current sentence or should be carried over from previous sentences

Essentially it would be a **condensed information** that related to the entire conversation so far. This will be used in the next module to decide what policy to take.

### Dialogue Policy

> The goal of the **dialogue policy** is to decide **what action the system should take next**, that is, what dialogue act to generate.

Formally, at turn $i+1$ we want to predict **what action $a_{i+1}$ to take**, which would be based on the entire sequence of dialogue:
$$
\hat{a}_{i+1} = \arg\max_{a_{i+1} \in A}P(a_{i+1}|s_i)
$$
for $s_i$ the state being encoded with representation of:

- current state of Frame $i$
- previous user and system dialogue actions
- etc.

Learning a **best policy** would be done by ==reinforcement learning== system gets a **reward at the end of the dialogue**, and uses that reward to train a policy to take actions.

Additionally, this module might need to take on task of choosing some attributes (slots and values) which plans on what to say to the user, before feeding into the NLG module.

### Natural Language Generation Module

Once a dialogue act has been decided, we need to generate the text of the response to the user.

> The task of natural language generation (NLG) in the information-state content architecture is often modeled in two stages:
>
> - **content planning** (what to say)
> - **sentence sentence realization** (how to say it).

Here we assume that the previous module has chosen the dialogue act to generate, and chosen some attributes (slots and values) that the planner wants to say to the user.

Example input/output looks like

<img src="NLP_part2/image-20220414023112417.png" alt="image-20220414023112417" style="zoom: 50%;" />

where In the first example:

- the content planner has chosen the dialogue act `RECOMMEND `and some particular slots `(name, neighborhood, cuisine)` and their fillers
- The goal of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the figure (here we have two samples as we can sample the generation module again and get a different output)

However, the problem here is **training data**: it is hard ot get a good combination of every possible act/slots:

- Therefore it is common in sentence realization to increase the generality of the training examples by **delexicalization** (basically remove the formal nouns, so we get a template)
- or any other **data augmentation** tricks such as [Noise Injection and Self-Training](#Noise Injection and Self-Training)

## Persuasive System

This is a more advanced system, which is technically a task-oriented system but with the aim of **persuading people to do something**, So, to achieve that goal, certainly some social aspects of the conversation is needed.

<img src="NLP_part2/image-20220418162151114.png" alt="image-20220418162151114" style="zoom: 33%;" />



Some potential uses of this system include:

- exercise persuation
- suisicde counseling (train counseller by acting as a visitor)
- social distancing

Again, the aim is to **change people's belief and behavior** (might have ethnical issues). How do we do that using machine learning?

### Challenges in Persuasion

The basics of data-driven methods for persuasion includes the following three modules:

<img src="NLP_part2/image-20220418162626800.png" alt="image-20220418162626800" style="zoom: 50%;" />

where:

- like all tasks, it is important to collect some good data
- then, we want our chatbot to be able to **understand the semantics** of user's utterance (i.e. what they said)
- finally, it moves on to persuade by picking some **strategy** (and then generate text response)

---

**Data Challenges**

> To start a good project/model, first we need to **collect good data**.

Like many ML tasks, collecting good data is always important. But that is not easy because even chaning a single word in a sentence could have lead to an entire new utterance. Therefore, the set of **conversation path** with each node being a sentence is exponential

<img src="NLP_part2/image-20220418201834432.png" alt="image-20220418201834432" style="zoom:33%;" />

so ideally we want our training data to cover *all possible dialog trajectories*, but that is intractable. Hence some approaches include

- learning a subset of the tree and hope for the best
- try to merge different branches to make the tree smaller
- etc.

Additionally, as our aim is an **optimization** task, there could be different optimal path for a different user to be persuaded

<img src="NLP_part2/image-20220418202109304.png" alt="image-20220418202109304" style="zoom: 33%;" />

so that given some user information/background, the system needs to pick an optimal path down the tree **for that particular user**

---

**Semantic Challenges**

Many large models are able to make language generation fluent, but they often ==do not understand deep level semantics== of the language (e.g. Microsoft's Tay AI), which can lead to unsafe systems.

<img src="NLP_part2/image-20220418202331875.png" alt="image-20220418202331875" style="zoom: 33%;" />

one potential task on this direction would be: how do we build a system that can "filter out" those dangerous generated languages?

---

**Strategy Challenges**

In certain scenario, some persuasive strategy would be more effective than another.

<img src="NLP_part2/image-20220418202610174.png" alt="image-20220418202610174" style="zoom: 33%;" />

for instance, hearts and cakes could be more persuasive than pizzas in this scenario.

### Dialog Data Collection

How do we get dialog data?

- production dialog (i.e. companies with their customers) are not available due to privacy issues. 
  - a good source would be customer services, but again there are privacy issues
- role-play persuasion would be ineffective if there is **lack of incentives**. So persuasive quality might not be good.

Then people decided to **use monetary incentive** (i.e. using CrowdFlower/Mechanical Turks), with the persuasion task of donation:

<img src="NLP_part2/image-20220418203147938.png" alt="image-20220418203147938" style="zoom:50%;" />

- persuaders receive bonus when persuadees donate 

- persuadees donation sent to the charity

- also collect big-five personality of the persuadee (for personalization system)

Hence the overall work flow looks like:

<img src="NLP_part2/image-20220418203304657.png" alt="image-20220418203304657" style="zoom: 50%;" />

---

Some examples include: (Persuasion for Good dataset)

<img src="NLP_part2/image-20220418203511141.png" alt="image-20220418203511141" style="zoom: 33%;" />

where the person on the left will be the crowd-funded persuader.

#### Hierarchical Intent Annotation

> We also want to build a system that understand some **semantics** of the sentences/dialogs.
>
> - so not just fluency, but also semantics

The idea is to annotate each sentence an **intent/dialog act**, which can be hierarchical

<img src="NLP_part2/image-20220418163705297.png" alt="image-20220418163705297" style="zoom:50%;" />

where MIDAS were used for social chatbot annotation. 

An example of **annotated** piece of dialog would be

<img src="NLP_part2/image-20220418163827199.png" alt="image-20220418163827199" style="zoom: 50%;" />

### Classic Modular Dialog System

Recall that we had the framework for dialog system, usually it looks like

<img src="NLP_part2/image-20220418163940380.png" alt="image-20220418163940380" style="zoom: 50%;" />

where in particular:

| Module Name |                         Input/Output                         |
| :---------: | :----------------------------------------------------------: |
|     NLU     | <img src="NLP_part2/image-20220418204125487.png" alt="image-20220418204125487" style="zoom: 33%;" /> |
|     DST     | <img src="NLP_part2/image-20220418204220033.png" alt="image-20220418204220033" style="zoom: 33%;" /> |
|     DPL     | <img src="NLP_part2/image-20220418204457199.png" alt="image-20220418204457199" style="zoom: 33%;" /> |
|     NLG     | <img src="NLP_part2/image-20220418204555107.png" alt="image-20220418204555107" style="zoom: 33%;" /> |

- **NLU**: will be to understand the user's utterances (i.e. semantic intent of the sentence)
  - usually used for **intent detection**, then any Seq-classification model would work (as we have a limited intent)
  - usually it is used per sentence based, which is why we needed some kind of accumulator later which is the DST
- **DST**: input dialog history, output **slot and values**
  - the aim is to track dialog state: sequence of **information** that is **critical to remember**
  - e.g. for restaurant domain: how much food you want, location of restaurant
  - e.g. for donation persuasion: how much donation we got for far, etc.
  
- **DPL**: given the current state and user's input, **what policy/action should we take** now?
  - e.g. maybe we should provide some facts now
- **NLG**: given the action we want to take, generate the output languages

Now with a modular network, we can train it:

- each module separately
- jointly/end-to-end
- some other way

#### Modular Training Drawbacks

The problem with this kind of training is that:

- heavy expert involvement, to tune each module
- hard to tune individual modules, hence usualy need end-to-end trainng
- heavy manual annotation needed for training data
  - because each module needs different input/output


#### End-to-End Drawbacks

Avoiding the training/data annotation problem for modular training, we can train end-to-end:

- which has been done on social chatbots a lot, such as GPT-2

<img src="NLP_part2/image-20220418164610593.png" alt="image-20220418164610593" style="zoom:50%;" />

However, here we also have problems:

- difficult for error analysis: you **don't know which module went wrong**! (DST error? NLG error?)
- since they don't have semantic controls, we nede a large amount of data to train

#### MOSS: A combination

Here we combine the best of both types:

- use one encoder (shared), but four decoders (separated)

The aim is to build a system that is **end-to-end trainable** but also **trainable in a modular fashion** (without going out of sync):

|                         Architecture                         |                           Pipeline                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220418205320854.png" alt="image-20220418205320854" style="zoom:33%;" /> | <img src="NLP_part2/image-20220418205627564.png" alt="image-20220418205627564" style="zoom: 33%;" /> |

where:

- for end-to-end training, the shared encoder weights will only be updated once
- for modular training, shared encoder will **also be updated** so its "knowledge" can still be used in other modules (in sync)

- as you see in the pipeline, essentially each module uses output of the shared encoder but **conditions/uses** on the output of the previous module as well

> One key advantage of this architecture is that we can train it even if we only have **partial labels** (e.g. only have 30% fully labelled, or only 40% labelled with intent)

For instance:

Fine-tuning on donation persuasion: we only have 300 labeled but 700 unlabeled data. We can

1. first do *end-to-end training*: train all 1000 unlabeled on NLG, which does not need intent labeling

   <img src="NLP_part2/image-20220418210339653.png" alt="image-20220418210339653" style="zoom:33%;" />

2. then do *modular training*: we can tune all again with **300 labelled** to update the other three

   <img src="NLP_part2/image-20220418210356822.png" alt="image-20220418210356822" style="zoom:50%;" />

One take-away here is that it kind of depends how much data you have:

<img src="NLP_part2/image-20220418210805942.png" alt="image-20220418210805942" style="zoom: 33%;" />

where:

- if you have a lot of training dialogs then you might not even need to label them and it could work
- if you don't have much training dialogs, then having high quality labeled ones are needed

---

Other advantage of this kind of architecture is that:

- easy to perform error analysis (we can decode the output for each module for a given input)
- can handle complex task

### Human Evaluation on Persuasion

To analyze the performance of the model simply:

- compare against human persuaders: how many percent of persuadee did it succeed to persuade?
- build a baseline: take GPT-2 and just tune end-to-end on all 1000 data 

<img src="NLP_part2/image-20220418165357936.png" alt="image-20220418165357936" style="zoom:50%;" />

Why can human do so well? Where ==error analysis becomes important==!

#### Error Analysis: Unfaithful Outputs

Checking on some outputs, we see that sometimes chatbot lies when It has never seen an utterance.

- it **failed to understand the semantics**, and generated something related some the training data without understanding it

<img src="NLP_part2/image-20220418165535004.png" alt="image-20220418165535004" style="zoom:50%;" />

This can be dangerous in **deployment**. Therefore one approach is to have a ==safety net== that can **rank** which responses is better

1. use beam search, nucleus sampling, temperature sampling, to get $N$ candidates

   <img src="NLP_part2/image-20220418165726392.png" alt="image-20220418165726392" style="zoom:50%;" />

2. gather training data by having **humans** ranking it

   <img src="NLP_part2/image-20220418165846745.png" alt="image-20220418165846745" style="zoom:50%;" />

   given some sampled $N$ samples, with the intent and semantic slot, rank those responses by a human in the loop

3. once we have samples of human ranking, ==train a classifier== to rank it

   <img src="NLP_part2/image-20220418211910164.png" alt="image-20220418211910164" style="zoom: 33%;" />

Then, after this fix, we got a 10% boost from the MOSS architecture

<img src="NLP_part2/image-20220418170003541.png" alt="image-20220418170003541" style="zoom:50%;" />

but still a bit less than human performance.

#### Error Analysis: Persuasion Strategies

Consider the following example

|                         Failed Cases                         |                        Success Cases                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220418170147913.png" alt="image-20220418170147913" style="zoom: 50%;" /> | <img src="NLP_part2/image-20220418212225807.png" alt="image-20220418212225807" style="zoom: 33%;" /> |

where we see that there is a wide variety of actions to take than **credibility appeals** when persuading people.

- Therefore, we also need **persuasion strategies** (i.e. *how* to persuade), ==in addition== of the dialog act of `propose-donation` (by the DPL module, on *what* to do next)

<img src="NLP_part2/image-20220418170330288.png" alt="image-20220418170330288" style="zoom: 50%;" />

- so we added this module of **DSP: dialog strategy planning**, into the pipeline

  <img src="NLP_part2/image-20220418170541413.png" alt="image-20220418170541413" style="zoom:50%;" />

  note that of course this module needs past trajectory of user's intent/feedback as well as system's intent to decide what is the best strategy to pick

Hence essentially this module will be learning the **successful trajectories**

- e.g. a finite state transducer, or any RL type algorithm

<img src="NLP_part2/image-20220418170724440.png" alt="image-20220418170724440" style="zoom:50%;" />

---

Finally, once added, this can further improve can be made 

<img src="NLP_part2/image-20220418213013312.png" alt="image-20220418213013312" style="zoom: 33%;" />

#### Error Analysis: Personality Adaptation

<img src="NLP_part2/image-20220418213107004.png" alt="image-20220418213107004" style="zoom: 33%;" />

### Dialogs is More than NLP

For production use in reality, dialogs requires much more than just NLP:

<img src="NLP_part2/image-20220418171011546.png" alt="image-20220418171011546" style="zoom:50%;" />

where the Language understanding, Dialog Management, and Language Generation would be handled by NLP. But we also need:

- most of the time conversations are spoken: hence we have **multi-modal data** such as hand gestures and audio inputs
  - e.g. speeches have much noise. Can we still recover the intent with noises?
  - e.g. given some emotion of the speaker, what is the best strategy?
- need **common senses** to deal with out-of-distribution conversations (e.g. erratic inputs)
- other human factors used for judging the quality of conversations

## Ethnical Consideration of Chatbots

36% of the people thought that they are humans (but they are mechanical turkers, so it depends on the education level)

<img src="NLP_part2/image-20220418213919831.png" alt="image-20220418213919831" style="zoom: 33%;" />

which can be very dangerous

- e.g. scamming people for money.

- but you could also train a bot to depend against scammers as well

But anyway

<img src="NLP_part2/image-20220418214009877.png" alt="image-20220418214009877" style="zoom: 33%;" />

though it is rarely exercised.

# Meta Learning in NLP

Useful when you have **low resource** (training sets), but you still want to achieve some task. Commonly, the familar methods you might know include:

- taking a pre-trained model then **fine-tune** on your limited resource task (transfer learning)
- but here, we will introduce a **new method** to solve this kind of problem

## What is Meta-Learning

To understand what meta-learning is doing, we will see how we got **from the transfer learning to the meta-learning** task

- transfer learning
- multitask learning
- meta-learning

---

**Transfer Learning**

Assume we have learnt a network and is trained for english to french translation with enough data. 

But now we have a new task, which **does not have** enough data. But notice that we could had a simliar architecture

<img src="NLP_part2/image-20220420161918427.png" alt="image-20220420161918427" style="zoom: 40%;" />

so that:

- encoder layers **should work in both cases** because they are used to "understand english"
- then, all we need to do is to **fine-tune** the decoder layers, which is essentially transfer learning.

This works pretty well because in reality, because good models typiocally have large encoder layers but only few decoder layers.

---

**Multitask learning**

Now suppose we are given a training dataset that have:

- a lot of task (e.g. english to french, english to chinese, etc)
- ach task has only a few training data

|                             Data                             |                         Architecture                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220420162059537.png" alt="image-20220420162059537" style="zoom: 40%;" /> | <img src="NLP_part2/image-20220420162320284.png" alt="image-20220420162320284" style="zoom:40%;" /> |

the data for each language pair is too small, but it could be that **all of them combined is enough**. Then:

- you have the architecture on the right, which has a **shared encoder** which learns through all training data

- but what about the loss of that shared layer?

  - the loss will **most often be a linear combination of $L_i$ for task $T_i$**
    $$
    \mathcal{L}(\theta^{sh}, \theta^1, ...,\theta^T) =\sum_{i=1}^T \alpha_i \mathcal{L}_i(\theta^{sh}, \theta^i, D_i)
    $$
    so that each task essentially is $ \mathcal{L}_i(\theta^{sh}, \theta^i, D_i)$ for having a dataset $D_i$

---

**Meta-Learning:** two types of views of the task

- **Mechanistic View**:
  - normally we train by reading in the training dataset of samples, then inference on new sample
  - here, we train with a dataset where **each training sample is a dataset of a task $T_i$**. This dataset will be called a **meta-dataset**
  - of course, the aim of this is to **perform well on a new task $T$ with its dataset**
  
- **Probabilistic View**:
  - from the meta-dataset, we aim to extract a **good prior** that should be shared from those set of tasks
  - then, doing a few-shot learning on a new task will **use this prior** and then train to infer posterior parameters


In general, on common application of this is to make it ==perform well under few-shot conditions==.

|                         Typical View                         |                        Meta-Learning                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220420163130731](NLP_part2/image-20220420163130731.png) | ![image-20220420163134630](NLP_part2/image-20220420163134630.png) |

so essentially we have:

- at training: train support, train query sets
- at testing: test support, test query sets
- we want the model to **perform well in test support and test query**

Therefore, we want to ==simulate the testing environment== during our training procedure

- just like normal model training where our train/test environment are **both classification**, here our train/test environment should **both be few-shot learning**
- how do we do this? Essentially see [Model Agnostic Meta Learning](#Model Agnostic Meta Learning)

## Meta-Learning Algorithms

Broadly speaking, there are two types of meta-learning algorithms:

- **Optimization Based**: we still optimize over some loss of a model, but we introduce a **new training paradigm** that is suitable for donig few-shot learning at test time
  - i.e. at test time, we can *fine-tune* our model fast when only given a few-shot training samples
- **Metric Based:** learning a model that gives a **good representation/embedding** for each class $c_i$ (under few-shots), from which we can apply some metrics (e.g. distance) to measure, for a query sample $x_q$ which class $c_i$ is belongs to
  - i.e. at test time, we *directly use this model* (which learns some metrics) when only given a few-shot training samples

### Model Agnostic Meta Learning

Suppose we have some model $f_\theta$ we want to use, for example a CNN.

> The goal of few-shot meta-learning is to train a model $f_\theta$ that can quickly adapt to a new task using only a few datapoints and training update iterations to $\theta$.
>
> - in other words, we want some ==good initialization of $\theta_0$== (learnt from other tasks as a "warm start") such that in a new task it learns fast with only few samples

So how do we get such a handy $\theta_0$? In effect, the meta-learning problem **treats entire tasks as training examples**, i.e. we consider

- ==a distribution over tasks $p(T)$== that we want our model to adapt to
- for each task $T_i$ could have its own loss $\mathcal{L}_{T_i}$ and its training data distribution $q_i(x)$

Then, during **meta-training** (in a $K$-shot learning setting), the idea is, given a model $f_\theta$:

1. consider some random initialization of $\theta_0 \equiv \theta \equiv \phi$

2. while not done:

   1. sample a batch of tasks $T_{i}$s to train from $p(T_i)$:

      1. sample  a task $T_i$, e.g. compare "cat v.s. dog"
      2. draw $K$ samples from $q_i(x)$ for training, known as ==support set==
      3. compute its loss $L_{T_i}(f_\theta)$ using the $\theta$ initialization
      4. update/see how well this initialization works by doing a few descents:

      $$
      \theta_i' = \theta - \alpha \nabla_\theta L_{T_i}(f_\theta)
      $$

      for that specific $T_i$.

   2. update the **initialization $\theta\equiv \phi$** as how the **total loss over all tasks can be decreased if I have a better $\theta$** to begin with
      $$
      \theta \leftarrow \theta - \beta \nabla_\theta\sum_{T_i} L_{T_i}(f_{\theta_i'})
      $$
      where:

      - $f_{\theta_i'}$ is like the same model "fine-tuned" on task $T_i$.

      - $L_{T_i}(f_{\theta_i'})$ will now be evaluated on the **new samples** from $q_i(x)$, known as the ==query set== to test its generalization ability as well

      - since we are taking derivative w.r.t $\theta$, and we know (if only doing a single iteration)
        $$
        \theta_i' = \theta - \alpha \nabla_\theta L_{T_i}(f_\theta)
        $$
        essentially this term include **loss of losses**.

3. output the learnt $\theta$ being the ==better "initialization" parameters for your model $f_\theta$ which is ready for other tasks==

Hence the algorithm is

|                          Algorithm                           |                         Update Tree                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220331182301680](NLP_part2/image-20220331182301680.png) | <img src="NLP_part2/image-20220420163707807.png" alt="image-20220420163707807" style="zoom: 50%;" /> |

which we know is agonistic of what model $f$ we picked.

- so that essentially we update $\theta \to \theta'$, where the intermediate $\theta_i$ for each task will be only used for computing the loss for $\theta$ and updating it.
- since $\theta_i$ comes from computing task loss $\mathcal{L}_i$ and doing $\nabla_{\theta_i}\mathcal{L}_i$, and the final $\mathcal{L}(\theta)$ will depend on $\theta_i$, we essentially would need to compute **Hessian of $\nabla^2_{\theta_i} \mathcal{L}_i$** when we update $\theta \to \theta'$. This is computationally expensive, and hence we had work such as the Amazon's Reptile system that **approximates the hessian** and saves compute power.

- resource https://arxiv.org/pdf/1703.03400.pdf

### Metric-Based Network

The most notable example of this is the Siamese network, which has the generic task of classifing whether if the two images are the same.

<img src="NLP_part2/image-20220420164128809.png" alt="image-20220420164128809" style="zoom: 50%;" />

however, the difference with normal training is:

- the two input image could come from **any domain**, e.g. kid's drawings, signatures, facial photographs, etc.

- The idea of this network is to use a shared CNN backbone to get an embedding, then then compute probability by looking at the distance, so that the shared CNN backbone in the middle is generalizable.

> In general, metric-based network aims at training a network that can produce **good embeddings** for a sample in a new domain (haven't seen in training).
>
> - with few-shots, then $K$ samples for each class in the domain

Some other networks should should already know from the Deep Learning course notes:

- **Matching Network**

  <img src="NLP_part2/image-20220420215419650.png" alt="image-20220420215419650" style="zoom:50%;" />

  which basically creates an embedding for each input $x_i$, then for the new test sample $x_q$:
  $$
  \hat{y} = \sum_{i}a(x_i,x_q)y_i
  $$
  basically predicting by a weighted average.

- **Prototypical Network**
  
  <img src="NLP_part2/image-20220420215631978.png" alt="image-20220420215631978" style="zoom:50%;" />
  
  which basically produces a class prototype vector $c_k$, and then for the new sample $x$ classify the class by looking at the distance between its embedding $f_\phi(x)$ and the class vectors $c_k$

Basically they all tend to learn a metric/embedding to **produce a good latent space representation** of the class.

## Applications in NLP

How do we apply the idea of meta-learning in NLP applications?

### Meta Leanring for Dialog

Consider the task of

- building a dialog system in domain $A, B, C$
- training data of dialogs are in domain $A,E,D,F$

- preparing a model $M$ that would work not only in $A,B,C$, but perhaps new domains as well

Then essentially it becomes learning a model $M$ that **performs well** given a few-shot learning samples. Hence we consider utilizing MAML algorithm mentioned above:

<img src="NLP_part2/image-20220420165535700.png" alt="image-20220420165535700" style="zoom:50%;" />

where

- $M_k'$ are the temporary models with weights $\theta_k$ 
- The aim is to update $M$ with $\theta \to \theta'$, such that the final $\theta^*$ would work well under few-shot learning

The performance comparision:

<img src="NLP_part2/image-20220420165657359.png" alt="image-20220420165657359" style="zoom:50%;" />

where when the dialog system is trained with meta-learing is is better than just fine-tuning the model to another task

### Meta Learning for Machine Translatoin

As mentioned before, it is common to have a MT task that, given a few samples of translating from language $A$ to language $B$, we want it to work even if our training data only contained $C\to D, A\to C, D\to F$, etc.

Graphically, the comparision between Meta-learning and other methods mentioend above is:

<img src="NLP_part2/image-20220420165920052.png" alt="image-20220420165920052" style="zoom: 50%;" />

so that:

- for transfer learning, we want to start with $\theta$ that works best in one domain and then **move to others**

- for meta learning, you want to get $\theta$ such that it works ==best at adapation== (in all directions)

### Meta In Context Learning

Some research has found that

> (Large) Language models are **few-shot learners**
>
> - basically they seem to work well with few-shot even if the training procedure is just masked language/whatever that is used normally for a language model training

An example in practice is GPT 3. GPT 3 has huge parameters, and is trained on huge text data.

- hard to fit in normal school GPUs

- also hard to fine-tune as the model is huge

Therefore a new procedure is needed for **few-shot "training"**

- normally you would imagine we update $\theta$ per step with each new (few-shot) traing data
- but as that is intractable, we can just dump all the examples along with the query as input

<img src="NLP_part2/image-20220420170504178.png" alt="image-20220420170504178" style="zoom: 67%;" />

where:

- the grey ones are the few-shot examples, the black texts are the machines' output

- so that essentially it is few shot learning ==without updating weights==

How does that even work? As we have transformers in the model:

- the model not only attends to the input, it also attends to the context 
- therefore, we expect models to **use the attention mechanism** to understand the few-shot 

#### In Context Tuning

Recent results shown that, given that in-context learning works, we can also do the following (for smaller models such as GPT2):

- instead of doing one step update for the intermediate weights $\theta_i$ in MAML
- we put all the task $T_i$ as a single input with context and fine-tune $\theta \to \theta'$ directly

<img src="NLP_part2/image-20220420171303276.png" alt="image-20220420171303276" style="zoom:67%;" />

so that essentially:

- it is **fine-tuning** but the training set is a ==concatenation of support set and query set==
- since it is a concatenation, the problem becomes memory constraint.
- such an update method is also model agnostic

# Bias in NLP

Related to ethics and safety of NLP models. Consider cases where:

- Language models on Job description preferers/assigns better competency for male engineers over females
- Products such as Amazon Alexa outputting racist texts (just for example)

Therefore, in this section we will talk about some methods on

- how to **detect biases**
  - https://arxiv.org/abs/1608.07187
  - https://www.aclweb.org/anthology/N19-1063.pdf

- how do to **de-biasing**
  - https://www.aclweb.org/anthology/N19-1061.pdf


## Bias in Word Embeddings

> In machine learning, essentially bias refers to **prior information/the priors**. The are ==problematic when== such information is derived from aspects of human culture known to lead to harmful behavior
>
> - e.g. Word2Vec: "Man is to computer programmer as woman is to homemaker.

Just as there are tests to measure implicit bias in humans, there can be parallel test to measure bias in machine, and today we mostly do this by looking at the **learnt embeddings**. Here, we can consider word level embeddings.

> Word embedding representations, in addition to encoding semantic meaning, also encode **semantic analogy**.
>
> - using that, we can analyze whether if certain words are gender/race neutral, for example.

### Implicit Association Test and WEAT

A famous test by Harvard is useful to measure biases in Human: the Implicit Association Test

> The IAT measures the **strength of associations between concepts** (e.g., black people, gay people) and evaluations (e.g., good, bad) or stereotypes (e.g., athletic, clumsy). The main idea is that making a **response is easier when closely related** items share the same response key.

Some useful word pairs we will discuss are:

- Flowers, insects - pleasant, unpleasant
- European American Names, African American Names - pleasant, unpleasant
- Female names, male names - family, career
- etc.

This essentially inspires how we can perform a similar task for word embeddings

> **Word-Embedding Association Test (WEAT)** essentially computes such association of concepts with **cosine similarity** of the embeddings, and was able to replicate the stereotypes found in the IAT tests.

The details are formalized as follows. Consider:

- two sets of **target** words $X$ (e.g., programmer, engineer, scientist; and nurse, teacher, librarian) 
- two sets of **attribute** words $Y$ (e.g., man, male; and woman, female). 
- The **null hypothesis** is that there is no difference between the two sets of target words in terms of their relative similarity to the two sets of attribute words. 
- The **permutation test** measures the (un)likelihood of the null hypothesis by computing the probability that *a random permutation of the attribute words would produce the observed (or greater) difference* in sample means.

Therefore, the test statics of the **differential association of the two sets of target words** (so that it is highest when the two groups have a different association, e.g. $X \to A$ and $Y \to B$) computes:
$$
s(X,Y,A,B) = \sum_{x \in X}s(x,A,B) - \sum_{y \in Y}s(y,A,B)
$$
where
$$
s(w,A,B) = \text{mean}_{a \in A} \cos(\vec{w},\vec{a})-\text{mean}_{b \in B} \cos(\vec{w},\vec{b})
$$
essentially measure the association of words to attributes in $A$ v.s. attributes in $B$. So that this is large when the word $\vec{w}$ is **biased** towards a particular group of attributes.

From this we can also formulate the permutation test and the effective size (to reject the null hypothesis), for which we will skip here but can be found on https://www.science.org/doi/10.1126/science.aal4230

---

*Example of Biases Found in Word Embeddings (such as Word2Vec)*

Racial Bias: African American names v.s. European American names

|                            Target                            |                          Attributes                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220425204324985](NLP_part2/image-20220425204324985.png) | ![image-20220425204338074](NLP_part2/image-20220425204338074.png) |

Gender Bias: Females are more associated with family, men with careers

|                            Target                            |                          Attributes                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220425204532048](NLP_part2/image-20220425204532048.png) | ![image-20220425204541348](NLP_part2/image-20220425204541348.png) |

Gender Bias: Female terms more associated with arts, male terms with mathematics

|                            Target                            |                          Attributes                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220425204629557](NLP_part2/image-20220425204629557.png) | ![image-20220425204637821](NLP_part2/image-20220425204637821.png) |

---

Where does those bias come from? It is basically learnt from **current world statistics** (which is biased). For example

<img src="NLP_part2/image-20220425204745622.png" alt="image-20220425204745622" style="zoom:50%;" />

where we see that:

- it learns to output a high similarity score for occupations where there are more women
- but we want it to learn those occupations to be gender neutral even if this is the reality

### Effect of Bias on NLP

As we know that many laguage models are trained on a huge amount of text:

- easily pick up biases from old documents/books, for example
- different languages might have different associations/biases

>  Sapir-Whorf hypothesis: "real world" is to a large extent **unconsciously built up on the language habits** of the group

Therefore, together with biases thise produces problems such as:

- MT translation:
  - Chinese speeches do not distinguish between he/she
  - to translate to English, we need he/she
  - but we **should not** disambiguate those based on biases!
- Dialog
  - e.g. implicitly using a pronoun such as "he" when referring to boss?

Both of which not only relates to biases in word level embeddings, but also perhaps **bias in sentence level embeddings!**

## Bias in Sentence Embeddings

The idea is the same as WEAT, where we want to know if the same bias exist for sentence embeddings. The only difference is:

- we have sentences, hence added context
- the target and the attribute sets are sets of sentences

> **SEAT: Sentence Encoder Association Test**. This basically does the same procedure as WEAT where you can:
>
> - form a sentence from words by simple templates where the word has been inserted: "This is a `<word>`".
>   - this is often used to convert word attributes to sentence attributes.
> - or if you have sentences to tests as targets, use them directly

For example:

|                            Target                            |                          Attributes                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="NLP_part2/image-20220425210430527.png" alt="image-20220425210430527" style="zoom:67%;" /> | <img src="NLP_part2/image-20220425210435664.png" alt="image-20220425210435664" style="zoom:67%;" /> |

The upshot is that the same biases still exists.

### ABW and Double Bind

Two traditional biases that were studied are:

- **ABW**:  angry black woman stereotype, basically black women are portrayed as loud, angry and imposing
- **Double Bind**: women in professional settings treated with disadvantage v.s. man
  - If women succeed in a male dominated job, they are *perceived less likeable and more hostile* than similarly performing men
  - If success not clear, they are perceived *less competent and achievement oriented* than men

---

**ABW Study:**

- Target: Black and white women names
- Attributes: adjectives presented in Collins and their antonyms

**Double Bind Study** (likeable):

- Targets: male/female names. Kathy is an engineer with superior technical skills
- Attributes: likable and non-hostile terms: the engineer is nice

**Double Bind Study** (success):

- Targets: Kathy is an engineer
- Attributes: competent/achievement-oriented terms: The engineer is high performing.

Results:

<img src="NLP_part2/image-20220425211114721.png" alt="image-20220425211114721" style="zoom:67%;" />

Hence we see that:

- for word level, you can basically pass in the word itself as a sentence and take that as the mebdding

- ELMo, GPT, and BERT seem to have bias when used with sentence level  but not when they are doing on a word level
- however, there are discrepancies as many results does not seem to agree even with same p-values

---

The conclusion is that:

- SEAT can confirm that bias exists
- Cosine similarity may not be a suitable model of representational similarity in recent models (e.g., BERT)

## Debiasing

> Is it possible to **remove the bias** that is clearly present in real life statistics?

It is still under active research, but we do have some approaches

- **HARD-DEBIASED:** remove the bias after training by **post-processing**
-  **GN-GLOVE:** use a **loss function** during training that aims to **reduce bias**

However, the quick conclusion is that even those does remove surface level bias (the word embedding itself may not be biased), it does **not** reduce ==hidden biases== such as "doctor" and "boss" having a similar embedding v.s. "nurse" and "teacher"

### De-bias via Post Processing

Given a word $\vec{w}$, we can define its gender bias (for example) by its projection on the "gender direction":
$$
\vec{w} \cdot (\vec{he}-\vec{she})
$$
where $(\vec{he}-\vec{she})$ would be the gender direction. We can compute this in a more robust way by taking

1. taking 10 pairs of words that surely have gender differences
2. take the principle components of the 10 pairs

 Then, when we have this gender direction

1.  for all words that should not be inherently gendered
2. Zero the gender projection for each word

### De-bias via Loss Function

Instead of removing/post processing the embedding, consider having loss such that

> - words in different groups to differ in last coordinate (as we know they exist in our data)
> - neutral gender words to be orthogonal

Then essentially:

1. Train debiased word embeddings from scratch
2. Change the loss function for Glove as mentioned above
   - so essentially concentrate gender information in last coordinate
3. Once done, remove the last coordinate

### Hidden Bias via Clustering

> Essentialy the paper https://www.aclweb.org/anthology/N19-1061.pdf claims that the above does not remove the biases.
>
> They manage to hide the bias in their **neighborhood relationship**, even if their similarity to gender is removed.
>
> -  most word pairs maintain their previous similarity, despite their change in relation to the gender direction

Consider 

1. taking the most biased words in the vocabulary according to the original bias
   - 500 male biased, 500 female biased
2. Cluster into 2 clusters using k-means
   - compare the result before de-biasing and after de-biasing

Results:

<img src="NLP_part2/image-20220425165851071.png" alt="image-20220425165851071" style="zoom: 50%;" />

where we see that

- Bias still manifested by the word being close to socially-marked feminine words

- i.e. although they are hiding their bias w.r.t gender, but their **neighbor relationship** is still apparent

Therefore, this introduces a new mechanism for measuring bias: % male/female socially-biased words among the k nearest neighbors of the target word.

---

Another intuitive metric could be:

1. given 5000 most biased words according to original experiments
2. Train an SVM on 1000 random sample, predict gender for remaining 4000

So that if words embeddings still have biases, then models such as SVM will not be able to distinguish. However, in reality:

- Hard-debiased: 88.8% accuracy vs 98.25% accuracy with non-debiased version
- GN-Glove: 96.53% accuracy vs. 98.65% accuracy with non-debiased version

so, again those biases are hidden somewhere in the embeddings.

> Idea: using GAN to perhaps remove the bias.]]></content><author><name></name></author><category term="2021@Columbia" /><summary type="html"><![CDATA[Continuation from:]]></summary></entry><entry><title type="html">APPH3300 Applied Electromagnetism</title><link href="/lectures/2021@columbia/APPH3300_Applied_Electromagnetism.html/" rel="alternate" type="text/html" title="APPH3300 Applied Electromagnetism" /><published>2022-05-03T00:00:00-04:00</published><updated>2022-05-03T00:00:00-04:00</updated><id>/lectures/2021@columbia/APPH3300_Applied_Electromagnetism</id><content type="html" xml:base="/lectures/2021@columbia/APPH3300_Applied_Electromagnetism.html/"><![CDATA[Applied EM Equations

# Chapter 2

| Condition                                             | Equation                                                     | Comments                                                     |
| ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Discrete Charges $q_j \neq Q$                         | $\vec{F} = Q\sum_{j} \frac{1}{4\pi \epsilon_0}\frac{q_j}{|\vec{r}-\vec{r}_j^{\prime}|^2} \frac{\vec{r}-\vec{r}^\prime}{|\vec{r}-\vec{r}_j^{\prime}|} =QE(\vec{r})$ | Field comes from **other charges only!**                     |
| Continuous Charge distribution $\rho(\vec{r}^\prime)$ | $\vec{F} = Q\int \frac{1}{4\pi \epsilon_0}\frac{1}{|\vec{r}-\vec{r}^{\prime}|^2} \frac{\vec{r}-\vec{r}^\prime}{|\vec{r}-\vec{r}^{\prime}|}\rho dV' =QE(\vec{r})$ | Integrate over all space, so all $\rho(\vec{r}^\prime)$ is covered |
|                                                       | $\oint \vec{E}\cdot d\vec{a} = \frac{Q_{enc}}{\epsilon_0}$   | Useful when there is symmetry, so that $\oint \vec{E}\cdot d\vec{a} $ is easy to figure out |
|                                                       | $\vec{\nabla } \cdot \vec{E} = \rho / \epsilon_0$            | Proof from **divergence theorem**: $\oint \vec{E}\cdot d\vec{a} = \int (\vec{\nabla}\cdot \vec{E})dV$ |
|                                                       | $\vec{\nabla}\cdot\frac{\hat{r}}{r^2} = 4\pi \delta^3(r)$    | Proven from trying to recover the above: $\nabla \cdot \vec{E} = \frac{1}{4\pi \epsilon_0}\int \vec{\nabla}\cdot \frac{\hat{r}}{r^2}p(r')d^3r' = \rho(r')/\epsilon_0$ |
|                                                       | $\int_{a<0}^{b>0} \delta(x)dx = \int_{a}^b \frac{d}{dx}H(x)dx$ | Useful for **formally prove** integral results with delta function |
|                                                       | $\vec{\nabla}\times \vec{E}=\vec{0}$                         | Conservation of field/charge. Proven from showing that $\oint \vec{E}\cdot d\vec{l}=\int \vec{\nabla}\times \vec{E}\cdot d\vec{S}=0 $ for a single point charge, then superposition. |
|                                                       |                                                              | A common approach to **prove starting from a single point charge**, then just use superposition |
|                                                       | $V(\vec{r}) = - \int_O^r \vec{E}\cdot d\vec{l}$              | Useful when you **already know $\vec{ E}$**                  |
|                                                       |                                                              | Careful for regions when $\vec{E}$ is **not continuous**. Then you may need to split your integral. |
|                                                       | $\vec{E}=-\vec{\nabla}V$                                     | Useful for **finding** out $\vec{E}$, since $V$ is easier to compute |
|                                                       | $\nabla^2 V = \rho / \epsilon_0$                             | Poisson's equation. Useful to **solve when inside vacuum region** such that $\rho = 0$ globally in that region. |
|                                                       |                                                              | In side vacuum, $\nabla^2 V = 0$ is the Laplace Equation     |
|                                                       | $V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \int \frac{\rho(r')}{|\vec{r} - \vec{r}'|}d^3r'$ | Useful to figure out potential hence the **electric field** when **Gaussian surface** **cannot** be used |
|                                                       |                                                              | derived from finding out the potential of a point charge using $V(\vec{r}) = - \int_O^r \vec{E}\cdot d\vec{l}$, then superposition. |
| When evaluated **right above/below the surface**      | $\vec{E}_{above}-\vec{E}_{below} = \frac{\sigma}{\epsilon_0}\hat{n}$ | Useful to figure out **surface charge** when we know field   |
| True for **any surface charge**                       |                                                              | Field is discontinuous when we have **surface charge**       |
|                                                       |                                                              | Proven from showing $(E^\perp_{above}-E^\perp_{below})A=\frac{Q_{enc}}{\epsilon_0}$ from Gaussian box near the surface, and $E^\parallel_{above}-E^\parallel_{below}=0$ from $ \oint \vec{E}\cdot d\vec{l}=0$ |
|                                                       | $V(b)-V(a) = W/Q$                                            | Work done to move charge $Q$ from $a$ to $b$                 |
|                                                       |                                                              | proven from the definition $W = \int_a^b \vec{F}\cdot d\vec{l}=-Q\int_a^b \vec{E}\cdot d\vec{l}=Q[V(b)-V(a)]$ |
|                                                       | $W=\frac{1}{4\pi \epsilon_0}\sum_{i=1}^N\sum_{j > i}^N \frac{q_iq_j}{r_{ij}}$ | Work done to **assemble already-made charges**               |
|                                                       | $W=\frac{1}{2}\sum_{i=1}^Nq_i\left(\sum_{j\neq i}^N \frac{1}{4\pi \epsilon_0} \frac{q_j}{r_{ij}}\right) = \frac{1}{2}\sum_{i=1}^n q_i V(\vec{r}_i)$ | $V(\vec{r}_i)$ is due to **charges other than $q_i$**        |
|                                                       |                                                              | used when needed to compute energy of **discrete charges**   |
| True for any space                                    | $W=\frac{1}{2}\int V(\vec{r}')\rho(\vec{r}')d^3r'$           | Continuous version of the above                              |
|                                                       |                                                              | used for energy contained in the **entire distribution**, not exactly the same as discrete case |
| True for any space                                    | $W=\frac{\epsilon_0}{2}\left[\int |\vec{E}|^2d^3r' + \oint V\vec{E}\cdot d\vec{A}\right]$ | derived from above, using $\rho/\epsilon_0 = \vec{\nabla}\cdot \vec{E}$ and $\nabla{V}=-\vec{E}$ |
| Over **all space**                                    | $W=\frac{\epsilon}{2}\int |\vec{E}|^2d^3r'$                  | **Easier to compute**, often used.                           |
| Perfect Conductor                                     | $\vec{E}_{meat}=0$, $\rho_{meat}=0$                          | Charges free to move, will redistribute until $\vec{E}_{meat}=0$. Hence all charges are **on the surface** |
|                                                       | $V_{surf}=V_0$, $\vec{E}_{surf}={E}^\perp \hat{n}$           | $V(b)-V(a)=-\int_a^b \vec{E}\cdot d\vec{l}=0$ if on the surface, since $\vec{E}=0$ inside |
|                                                       |                                                              | $V_{meat}=V_0$ as well since there is no field               |
|                                                       |                                                              | Those conditions are often used as ==constraints== in a problem |
| Capacitance                                           | $C \equiv \frac{Q}{\Delta V}$                                | $Q=+Q$ of the **two conductors,** is the **charge stored**   |
|                                                       | $\Delta V =- \int_a^b \vec{E}\cdot d\vec{l}$ commonly used   | $\Delta V$ is the potential difference across the two conductor = **how much $\Delta V$** needed to store $Q$ |
|                                                       |                                                              | Should be purely **geometric**, in unit $\epsilon \cdot \text{Length}$ |
| Energy Stored in Conductor                            | $W=\frac{1}{2}\frac{Q^2}{C}=\frac{1}{2}CV^2$                 | Think of it being same as energy needed to **charge up conductor** = **work done** to move all the charges over |
|                                                       |                                                              | Proven because $dW = \Delta V(q)dq$ to move a charge $dq$ over, and then since $C=\frac{q}{\Delta V(q)}$ is geometric, perform $W=\int dW$ |
|                                                       | $W = \frac{\epsilon_0}{2}\int |\vec{E}|^2 d^3r'$ over all space | same result as above                                         |
| Current Density                                       | $\rho(r)\cdot v_e(r)\equiv J(r)$                             | density times velocity                                       |
|                                                       | $\vec{\nabla}\cdot \vec{J}  =0$                              | at equilibrium, as otherwise charge accumulates              |
|                                                       | $\Delta U(x) = eV(x)=\frac{1}{2}m_e v_e(x)^2$                | conservation of energy, and change in potential $\Delta U$ can be simply measured by $qV$ |

*Electric Field from Coulomb's Law:*

Using superposition we can show
$$
\vec{F} = Q\int \frac{1}{4\pi \epsilon_0}\frac{1}{|\vec{r}-\vec{r}^{\prime}|^2} \frac{\vec{r}-\vec{r}^\prime}{|\vec{r}-\vec{r}^{\prime}|}\rho dV' =QE(\vec{r})
$$
Then, if we have a **line charge**, and we are interested in $E(\vec{r})=E(z\hat{z})$ due to symmetry

<img src="em_equations/image-20220212154857740.png" alt="image-20220212154857740" style="zoom:50%;" />

Since we only need $z$, consider $\bar{r}=\vec{r}-\vec{r}^{\prime}$ in the above figure (red box)
$$
\begin{align*}
\vec{E}(\vec{r})
&= \frac{1}{4\pi \epsilon_0} \int \frac{1}{\bar{r}^2}\frac{\bar{r}}{\sqrt{\bar{r}^2}}\, \rho \, dV'\\
&=  \frac{1}{4\pi \epsilon_0} \int_{-L}^L \frac{1}{(z^2 + x'^2)}\frac{z\hat{z}-x'\hat{x}}{\sqrt{z^2 + x'^2}}\, \lambda \, dx'\\
\end{align*}
$$
essentially consider $\rho = \lambda \delta(y)\delta(z)$ since it is only on $x$-axis.

---

*Potential when You Know $\vec{E}$*

Consider the potential for a **shell charge**, which you already know the field:
$$
\vec{E} = \begin{cases}
\frac{Q}{4\pi \epsilon_0 r^2}, & r > R\\
0, & r < R
\end{cases}
$$
which can be proven from Gaussian surface. Then, we consider the harder potential, **for $r < R$**:
$$
\begin{align*}
V(r) 
&= -\int_O^r\vec{E}\cdot d\vec{l} \\
&= -\int_\infty^r \vec{E}\cdot d\vec{l}\\
&= -\int_\infty^R \vec{E}\cdot d\vec{l}-\int_R^r \vec{E}\cdot d\vec{l}
\end{align*}
$$
then you substitute in the **discontinuous field**.

---

*Conversion between Potential, Field, and Charge*

![image-20220212164103943](em_equations/image-20220212164103943.png)

where the highlighted ones are used most often.

---

*Induced Charges*

Consider a charge **neutral** conductor, with $+q$ inside a **cavity**:

![image-20220212175035891](em_equations/image-20220212175035891.png)

we want to show that (if the drawing above is a sphere)
$$
\vec{E}_{outside} = \frac{q}{4\pi \epsilon_0}\frac{1}{r^2}\hat{r}
$$
which can be argued by:

1. since $\vec{E}_{meat}=0$ for conductor, we draw the Gaussian surface around the cavity to show that:
   $$
   \oint \vec{E}\cdot d\vec{A} = 0 = \frac{Q_{enc}}{\epsilon_0}
   $$
   hence **negative charges distribute at surface of cavity**, such that $E_{meat}=0$ maintains

2. Then, since the negative charges effectively shielded off the $+q$, we can say that the remaining positive charge will have **distributed uniformly**. Hence outside the surface, we still have **symmetry**.

---

*Faraday Cage*:

If the **cavity contains no charge** inside a perfect conductor, then $\vec{E}_{\text{in cavity}}=0$

![image-20220212175551629](em_equations/image-20220212175551629.png)

This can be shown by drawing a **closed loop**, so that:

- $\oint \vec{E}\cdot d\vec{l}=0$ since it is closed
- the part $\int \vec{E}\cdot d\vec{l}=0$ that covers path **inside the conductor** because $\vec{E}_{meat}=0$
- hence, this means $\int \vec{E}\cdot d\vec{l}=0$ for **any path inside the cavity** as well.

# Chapter 3

Here we are concerned with finding out $V$ (hence $\vec{E}$) when we technically don't even know $\rho$ so that:
$$
V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \int \frac{\rho(r')}{|\vec{r} - \vec{r}'|}d^3r'
$$
cannot be used.

| Condition                                       | Equation                                                     | Comment                                                      |
| ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| In vacuum region so that $\rho=0$ everywhere    | $\nabla^2 V = 0$                                             | essentially **solving PDE** to get $V$                       |
|                                                 |                                                              | using separation of variables                                |
| Property for $V$ that solves $\nabla^2 V=0$     | $V(\vec{r})=\frac{1}{4\pi R^2}\oint_{B_R}VdA$                | Solution of $V$ value at $\vec{r}$ is the **average of surrounding** (mean value theorem) |
|                                                 |                                                              | (follows from the above) Extremum for $V$ only happens **at the boundary** |
| Earnshaw's Theorem                              | A charged particle cannot be held in stable equilibrium by electrostatic force | If stable equilibrium, $V(r_{eq}) > V(r_{surface})$ for a ball of radius $R$ around it. |
|                                                 |                                                              | But since $\nabla^2V=0$ holds, then from MV theorem this cannot be true. |
| Uniqueness Theorem                              | Solution to $\nabla^2V=0$ is unique                          | Proven by $V_3 = V_2 - V_1$, and showing that $V_3=0$ is a must if $V_1,V_2$ solves both PDE and boundary condition |
|                                                 | Solution to $\nabla^2V = - \rho / \epsilon_0$ is also unique | same idea as above                                           |
| Method of Images                                |                                                              | Figuring out $V$ is the same as solving $\nabla^2V=0$ with boundary condition. Use method of images to satisfy B.C. while making sure $\rho = 0$ in the interested region. |
|                                                 |                                                              | Works because of uniqueness theorem                          |
|                                                 |                                                              | Useful when you **don't** want to actually solve $\nabla^2V=0$ |
| 2D Cartesian Solution for Laplacian             | $V(x,y)=(Ae^{kx}+Be^{-kx})(C\sin(ky)+D\cos(ky))=X(x)Y(y)$    | General Solution, $k$ needs to be solved by looking at B.C.  |
|                                                 | $k=n \pi /a$, solution $V(x,y) = \sum_n V_n(x,y)$            | sometimes you can tell what $n$ should be, e.g. $n=5$. Then solution is simply one term of the sum |
| 2D Cylindrical Solution for Laplacian           | $\nabla^2V = \frac{1}{r}\frac{\partial }{\partial r}(r \frac{\partial V}{\partial r})+\frac{1}{r^2}\frac{\partial^2 V}{\partial \phi^2}=0$ |                                                              |
|                                                 | $V(r,\phi) = (Ar^m + Br^{-m})(C\sin(m\phi)+D\cos(m\phi))$    | General solution, $m$ needs to be solved by looking at B.C.  |
| 3D Spherical Solution without horizontal $\phi$ | $\nabla^2 V =  \frac{1}{r}\frac{\partial }{\partial r^2}(r^2 \frac{\partial V}{\partial r})+\frac{1}{r^2 \sin(\theta)}\frac{\partial^2 V}{\partial \theta^2}=0$ |                                                              |
|                                                 | $V(r,\theta, \phi)=(Ar^l + \frac{B}{r^{l+1}})P_l[\cos(\theta)]$ |                                                              |
| Legendre Polynomial                             | $P_0[\cos(\theta)]=1 \\ P_1[\cos(\theta)]=\cos \theta \\ P_2[\cos(\theta)]=(3\cos^2 \theta -1)/2$ | Good to remember                                             |
|                                                 | $\int_0^\pi P_i[\cos(\theta)]P_j[\cos(\theta)]\sin(\theta) \, d\theta = \begin{cases} 0, & i \neq j\\ \frac{2}{2i+1}, & i = j \end{cases}$ | Orthogonality, use for non-homogenous boundary condition (if needed) |
| Multipole Expansion                             | $V(\vec{r}) = \frac{1}{4 \pi \epsilon_0} \sum_{l=0}^\infty \frac{1}{r^{l+1}} \int (r')^l P_l[\cos(\theta)] \rho(r')d^3r'$ | Approximation useful when we are far away. Proof from $V(\vec{r}) = \frac{1}{4 \pi \epsilon_0}\int \frac{\rho(r')}{||\vec{r}-\vec{r}''||} d^3r'$ and using result below |
|                                                 | $\frac{1}{||\vec{r}-\vec{r}'||} = \frac{1}{r}\sum_l^\infty \left( \frac{r'}{r} \right)^l P_l[\cos(\theta)]$ | for $r = ||\vec{r}||, r'=||\vec{r}'||$, proven from binomial expansion of denominator taking out $R$. |
|                                                 | $V(\vec{r}) = \frac{1}{4 \pi \epsilon_0}\sum_{l=0}^\infty \frac{M_l}{r^{l+1}}P_l[\cos\theta]$, $M_l \equiv \int (r')^l \rho(r')d^3r'$ | assuming $P_l[\cos\theta]$ can be taken out from the integral |
|                                                 | $V_l = \frac{1}{4\pi \epsilon_0} \frac{M_l P_l[\cos\theta]}{r^{l+1}} $, so $V(\vec{r}) = \sum_{l=0}^\infty V_l$ | e.g. monopole potential take $l\to 0$, dipole potential take $l \to 1$, quadruple potential take $l\to 2$,... |
| Multipole Terms                                 | $M_l \equiv \int (r')^l \rho(r')d^3r'$                       | in continous case                                            |
|                                                 | $M_l \equiv \sum_i (r_i')^l q_i$                             | in discrete case (both assume $\theta$ being a constant, otherwise $P_l[\cos \theta]$ will be included) |
|                                                 | $V_l = \frac{1}{4\pi \epsilon_0} \frac{M_l P_l[\cos\theta]}{r^{l+1}} $ |                                                              |
| Dipole Moment                                   | $V_1(\vec{r}) = \frac{1}{4 \pi \epsilon_0} \frac{1}{r^{2}} \int r' \cos(\theta )\rho(r')d^3r'  = \frac{1}{4\pi \epsilon_0} \frac{1}{r^2} \int \hat{r}\cdot \vec{r}' \rho(r')d^3r'$ |                                                              |
|                                                 | $V_1(\vec{r}) = \frac{1}{4\pi\epsilon_0}\frac{1}{r^2}\hat{r}\cdot \vec{p}$, with $\vec{p} \equiv \int \vec{r}' \rho(r')d^3r' $ | Dipole Moment $\vec{p}$ without any assumption               |
|                                                 | $\vec{p} = \sum_i \vec{r}'_i q_i$                            | Dipole Moment in discrete case                               |

*Method of Imaging*

Consider finding $V$ in the following region where $z > 0$:

<img src="em_equations/image-20220212185301978.png" alt="image-20220212185301978" style="zoom:67%;" />

Notice that finding out $V$ in that region is the same as:

- solving $\nabla^2 V=0$
- B.C.: $V=0$ when $z=0$ as it is **grounded**
- B.C.: $V(r)\to 0$ if $r^2 >> d^2$, i.e. when far away
- $\vec{E}$ is **perpendicular to surface** at $z=0$
  - good check to make sure ==if method of imaging setup is correct==

Then, realize that this is the same as **solving for $V$ in Figure 3.11**.

- in the end we only take the solution for $z>0$

Since it is essentially two point charges, use **superposition of potential from point charges**:
$$
V(x,y,z) = \frac{1}{4\pi \epsilon_0} \left[ \frac{q}{\sqrt{x^2 + y^2 + (z-d)^2}} -\frac{q}{\sqrt{x^2 + y^2 + (z+d)^2}} \right]
$$

---

*Multipole Expansion Derivation Sketch*

This comes from the motivation to **approximate field** when we are **far from the charge density**. Technically the approximation
$$
V(\vec{r}) \approx \frac{1}{4\pi \epsilon_0} \frac{Q_{total}}{r^2}
$$
could work in some cases, in other case you might have $Q_{total}=0$ then this is **uninformative**, i.e. we need higher order terms. Hence we first use binomial expansion to show that:
$$
\frac{1}{||\vec{r}-\vec{r}'||} = \frac{1}{r}\sum_l^\infty \left( \frac{r'}{r} \right)^l P_l[\cos(\theta)]
$$
from doing
$$
\frac{1}{||\vec{r}-\vec{r}'||} = \frac{1}{\sqrt{r^2 + r^{'2}-2rr'\cos \theta}} = \frac{1}{r}\frac{1}{\sqrt{1+\epsilon}}
$$
then expanding the $1/\sqrt{1+\epsilon}$ to get Legendre polynomial, which ==assumes small $\epsilon$== hence being far away from charges.

---

*Multipole Expansion Example*

Consider a vertical line charge $\lambda (z')$, and we are interested in finding $V(\vec{r})$ far away from it.

<img src="em_equations/image-20220314110957682.png" alt="image-20220314110957682" style="zoom:80%;" />

Using the formula for expansion, we know:
$$
V(\vec{r}) = \frac{1}{4 \pi \epsilon_0} \sum_{l=0}^\infty \frac{1}{r^{l+1}} \int (r')^l P_l[\cos(\theta)] \rho(r')d^3r'
$$
then since $\rho(r') = \lambda(z')\delta(x')\delta(y')$, and since $\vec{r}' = z'\hat{z}$, meaning $r'=z'$, then we get:
$$
\begin{align*}
V(\vec{r})
&= \frac{1}{4 \pi \epsilon_0} \sum_{l=0}^\infty \frac{1}{r^{l+1}} \int (z')^l P_l[\cos \theta] \lambda(z')\delta(x')\delta(y')\,d^3r'\\
&= \frac{1}{4 \pi \epsilon_0} \sum_{l=0}^\infty \frac{P_l[\cos\theta]}{r^{l+1}} \iiint (z')^l \lambda (z')\delta(x')\delta(y')\,dx'dy'dz'\\
&= \frac{1}{4 \pi \epsilon_0} \sum_{l=0}^\infty \frac{P_l[\cos\theta]}{r^{l+1}} \int (z')^l \lambda (z')dz'
\end{align*}
$$
which is the furthest we can go, and the second equality comes from the fact that $\theta$ which is the **angle between $\vec{r}$ and $\vec{r}'$** is constant given a $\vec{r}$.

---

*Multipole Expansion Discrete case:*

If you are dealing with a **discrete** case, you can then either:

- express $\rho(\vec{r}')$ in terms of $\delta$ functions, hence point charges

- use the discrete formula for $M$. Since we assumed to be far away from the charges, usually $\theta_i$ between $\vec{r}$ and $\vec{r}_i'$ can be assumed constant hence:
  $$
  V(\vec{r}) = \frac{1}{4 \pi \epsilon_0}\sum_{l=0}^\infty \frac{M_l}{r^{l+1}}P_l[\cos\theta], \quad M_l \equiv \sum_i (r_i')^l q_i
  $$
  otherwise you will need:
  $$
  V(\vec{r}) = \frac{1}{4 \pi \epsilon_0}\sum_{l=0}^\infty \frac{M_l}{r^{l+1}}, \quad M_l \equiv \sum_i (r_i')^l \, P_l[\cos\theta_i] \, q_i
  $$
  

# Chapter 4

This chapter discuss **electric fields in matter (e.g. dielectric)**, which is what happens when an electric field $\vec{E}_{app}$ is applied on the matter.

| Condition                                                    | Equation                                                     | Comment                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Atomic Polarization                                          | $\vec{p} = \alpha \vec{E}_{applied}$                         | For molecules, $\alpha$ would be a function of "direction" if molecule is asymmetric (e.g. $H_2O$) |
| Polarization                                                 | $\vec{P}$=dipole moment per volume                           | describes polarization of material due to some externel field |
|                                                              | $\vec{p} = \vec{P}\,dV'$                                     | Want to find field **results** from polarization $\vec{P}$   |
|                                                              |                                                              | $\vec{P}(\vec{r}')=0$ if $\vec{r}'$ is vacuum, for example   |
| Potential due to Polarization $\vec{P}$                      | $V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{||\vec{r} - \vec{r}'||}dA'+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{||\vec{r} - \vec{r}'||}dV' $ | derived from $V(\vec{r})$ ==only== consisting of dipole terms, which is defined by $\vec{P}(\vec{r})$ |
|                                                              | $V(\vec{r})=V(\text{from $\sigma_b, \rho_b$})$               | i.e. you can **treat** the dielectric as having $\sigma_b$ on the surface and $\rho_b$ inside. |
|                                                              |                                                              | if $\rho_b = 0$, $V(\text{from $\sigma_b,\rho_b$})$ can be solved using **Laplacian $\nabla^2 V = 0$** with boundary of $\sigma_b$ |
|                                                              | $\sigma_b \equiv \vec{P}\cdot \hat{n}$                       | $\hat{n}$ points from the surface having dielectric to vaccum/non-dielectric place |
|                                                              | $\rho_b\equiv - \nabla \cdot \vec{P}$                        |                                                              |
| Interpretation of Bound Charge due to $\vec{P}$              | $\sigma_b \equiv \vec{P}\cdot \hat{n}$                       | $\sigma_b$ comes from lining up dipoles so that only ends matter |
|                                                              | $\rho_b\equiv - \nabla \cdot \vec{P}$                        | $\rho_b$ considers if dipole in between is not cancelled out, then the flux of polarization would relate to charge: $-\oint \vec{P}\cdot d\vec{A} =\int_V \rho_b \,d^3r'$ |
| Electric Displacement Field                                  | $\vec{\nabla} \cdot \vec{D}= \rho_{free}$, and $\vec{D} \equiv \epsilon_0 \vec{E}_{total} + \vec{P}$ | differential form                                            |
|                                                              | $\vec{\nabla}\cdot \vec{E}_{tot} = \frac{\rho_{free}+\rho_b}{\epsilon_0}$ | derivation, then using $\rho_b = -\vec{\nabla}\cdot \vec{P}$ |
|                                                              | $\oint \vec{D}\cdot d\vec{A} = Q_{free_{enc}}$               | integral form                                                |
|                                                              |                                                              | if have symmetry can use **Gaussian surfaces**               |
|                                                              |                                                              | used for proving boundary conditions for $\vec{D}$           |
|                                                              | $\vec{\nabla} \times \vec{D} = \vec{\nabla} \times \vec{P}$  |                                                              |
|                                                              | $\oint \vec{D}\cdot d\vec{r}=\oint \vec{P}\cdot d\vec{r}$    | Stokes' Theorem, derived from above                          |
|                                                              |                                                              | used for proving boundary conditions for $\vec{D}$           |
|                                                              |                                                              | the above is **true for any dielectric**                     |
| $\vec{D}$ and $\vec{E}$ relation                             | $\vec{\nabla}\cdot \vec{E}_{tot} = \rho_{tot} / \epsilon_0 \to \vec{E}_{tot} = \frac{1}{4\pi \epsilon_0} \int \frac{\rho_{tot}}{\symscr{r}} \hat{\symscr{r}}\,d^3r'$ | because $\vec{\nabla}\times \vec{E}_{tot} = 0$               |
|                                                              | $\vec{\nabla}\cdot \vec{D}= \rho_{free} \not\to \vec{D} = \frac{1}{4\pi } \int \frac{\rho_{free}}{\symscr{r}} \hat{\symscr{r}}\,d^3r'$ | because $\vec{\nabla} \times \vec{D} \neq 0$ often, hence $\vec{D}$ might not have a potential |
| Linear Dielectric                                            | $\vec{P} = \epsilon_0 \chi_e \vec{E}_{tot}$                  | $\chi_e$ is electric susceptibility, $\epsilon_0$ is permittivity of free space |
|                                                              |                                                              | if $\vec{E}_{applied}$ is given, we are stuck because it will produce $\vec{P}$, which produces $\vec{E}_{resp}$ hence affects $\vec{E}_{tot}$, which relates to $\vec{P}$... |
|                                                              | $\vec{D} = \epsilon_0(1+\chi_e)\vec{E}_{tot}\equiv \epsilon \vec{E}_{tot}$ | $\epsilon$ is permittivity of material                       |
|                                                              |                                                              | can use this to **break from the loop** if we can find $\vec{D}$ from Gaussian surface |
|                                                              | $\epsilon_r \equiv \epsilon / \epsilon_0 = 1+\chi_e$         | relative permittivity, $\chi_e=0$ in free space.             |
|                                                              |                                                              | also called **dielectric constant**                          |
|                                                              | $\vec{\nabla}\times \vec{D} = \vec{\nabla} \times (\epsilon \vec{E}_{tot}) \neq 0$ | because $\epsilon$ could vary in different position          |
| Space filled with homogenous Linear Dielectric $\chi_e$      | $\vec{\nabla} \times \vec{D} = 0$, $\vec{\nabla} \cdot \vec{D}= \rho_{free}$ | inside the homogenous linear dielectric, $\epsilon$ is constant |
|                                                              | $\vec{D} = \epsilon_0 \vec{E}_{free}$                        | $\vec{E}_{free}$ is field produced by free charges           |
|                                                              |                                                              | because now $\vec{\nabla}\cdot \vec{D}= \rho_{free} \to \vec{D} = \frac{1}{4\pi } \int \frac{\rho_{free}}{\symscr{r}} \hat{\symscr{r}}\,d^3r'$ holds |
| Happens when $\vec{\nabla} \times\vec{D} = \vec{\nabla} \times\vec{P}= 0$, commonly with a capacitor | $\vec{E}_{tot} = \frac{1}{\epsilon_r}\vec{E}_{free}\equiv \frac{1}{\epsilon_r}\vec{E}_{vac}$ | for capacitor: derived if $\vec{D}$ has the same shape as $ \vec{E}_{vac}$ , then use Gaussian to find $\vec{D}$ and use $\vec{D} = \epsilon \vec{E}_{tot}$ |
|                                                              |                                                              | in general, $\vec{\nabla} \times\vec{D} = 0$ alone would work. See the relevant section below |
|                                                              |                                                              | note that this is a special case on top of having a linear dielectric |
| Boundary Conditions for $\vec{D}$                            | $D_{abv}^\perp - D_{below}^\perp = \sigma_{free}$            | derived from drawing Gaussian box just above/below the surface |
|                                                              | $D_{abv}^\parallel - D_{below}^\parallel= P_{abv}^\parallel - P_{below}^\parallel$ | derived from drawing loop just above/below the surface       |
|                                                              |                                                              | True for any dielectric                                      |
|                                                              | $\vec{E}_{above}-\vec{E}_{below} = (\sigma_{total}/\epsilon_0) \hat{n}$ | (note that those always/still hold)                          |
|                                                              | $E_{abv}^\parallel - E_{below}^\parallel= 0$                 |                                                              |
| Dielectric inside Capacitor                                  | $C = \epsilon_r C_{vac}$                                     | $C_{vac}$ is capacitance without dielectric                  |
|                                                              |                                                              | holds whenever $\vec{E}_{tot} = \frac{1}{\epsilon_r}\vec{E}_{free}$ holds inside the capacitance. Hence $\Delta V = \Delta V_{vac}/\epsilon_r$ while $Q$ is the same. |
| Energy of setup with a Dielectric                            | $W=\frac{1}{2}\int_{all\,space}\vec{D}\cdot\vec{E}\,d^3r$    | holds if you have a linear dielectric so that $\vec{D}=\epsilon \vec{E}$ |
|                                                              |                                                              | derived from assuming bound charges fixed in place, we are only moving free charges into position |
|                                                              | $W = \frac{\epsilon_0}{2}\int |\vec{E}|^2 d^3r$              | still holds if we are assembling *all charges* independently, i.e. we also bring in bound charges |
|                                                              |                                                              | this will be smaller than $W=\frac{1}{2}\int_{all\,space}\vec{D}\cdot\vec{E}\,d^3r$ as it does not include potential/spring energy within dipoles |
| Force *on* Dielectric (due to capacitor)                     | $-F_{on\,diel}=\frac{dW}{dx}=-\frac{1}{2}V(x)^2\frac{dC(x)}{dx}$ | True either fixing $Q$ or fixing $V$, in which case $V(x)=V_0$ |
|                                                              |                                                              | sanity check: force should suck dielectric into capacitor    |
|                                                              | $dW = F_{me}dx=-F_{on\,diel}dx$, and $W=\frac{1}{2}\frac{Q^2}{C(x)}$ | derivation. For $C(x)$ and $V=V(x)$ if fixed total charge $Q$ |
|                                                              | $dW=-F_{on\, diel}dx + VdQ$, and $W=\frac{1}{2}C(x)V_0^2$    | derivation if fixing $V(x)=V_0$. Ended up the same formula for $W$ as above. |
|                                                              | $\vec{\nabla} W = -\vec{F}_{on\, diel}$                      | If in problem we have some 3D movement. Then $W$ is still one of the two above cases. |

*Atomic Polarization*

This is derived/==approximated== by considering the nucleus with charge $+q$ sits in **equilibrium** due to cancellation of external field $\vec{E}_{app}$ and $\vec{E}_e$ with is field of electron cloud (volume):

![image-20220314161623445](em_equations/image-20220314161623445.png)

Then since field inside *uniform sphere of charge* is:
$$
\vec{E}_e = -\frac{q}{4\pi \epsilon_0} \frac{r}{a^3} \hat{r},\quad  r<a
$$
hence if we obtained an equilibrium *at $r=d$*:
$$
\begin{align*}
\vec{E}_{app}(d) 
&=\frac{q}{4\pi \epsilon_0} \frac{d}{a^3} \hat{r}\\
qd \hat{r} \equiv \vec{p} &= (4\pi \epsilon_0 a^3)\vec{E}_{app}(d) 
\end{align*}
$$
hence the proportionality.

---

*Application of Bound Charges*

Consider a slab of dielectric with thickness $d$ carrying polarization $\vec{P} = k[1+(x/d)]\hat{x}$ :

![image-20220314163908686](em_equations/image-20220314163908686.png)

we do not care yet what external field is needed to produce this polarization. We want to know the electric field **produced by** this given polarization.

First, we can find the bound charges to be:
$$
\sigma_b = \vec{P}\cdot \hat{n} = \begin{cases}
2k, & x =d\\
-k, & x=0
\end{cases}\\
\rho_b = -\nabla \cdot \vec{P} = -\frac{k}{d},\quad 0<x<d
$$
Then, we can *treat this as real charges* to compute $V(\vec{r})$ due to the correctness of:
$$
V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{||\vec{r} - \vec{r}'||}dA'+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{||\vec{r} - \vec{r}'||}dV' 
$$
Now, since we know the charge distribution, we can exploit the symmetry and save doing the above extra integrals:

- drawing Gaussian surfaces inside/outside the slab gives:
  $$
  \vec{E}_< = \vec{E}_>\\
  (\vec{E}_{inside}-\vec{E}_{<})\cdot \hat{x} = \frac{k}{\epsilon_0}\left(1+\frac{x}{d}\right)
  $$

- but notice that the net charge is zero, hence:
  $$
  \vec{E}_< = \vec{E}_> = \vec{0}\\
  \vec{E}_{inside} = -\frac{k}{\epsilon_0}\left( 1+ \frac{x}{d} \right)
  $$

the result is consistent if you compute from $\vec{D}$ and use $\vec{D}=\epsilon \vec{E}_{tot}$.

---

*Homogenous Linear Dielectric*:

Find $\vec{E}_{tot}$ inside the dielectric with the following setup, $Q$ is free charge placed by us.

![image-20220314173419462](em_equations/image-20220314173419462.png)

First, a linear dieletric means $\vec{D} = \epsilon \vec{E}_{tot}$. Then, as $\epsilon$ is constant inside, we get $\vec{\nabla} \times\vec{D} = 0$. Then $\vec{\nabla}\cdot \vec{D}= \rho_{free}$ always hold, we **can use**:
$$
\vec{D} = \frac{1}{4\pi } \int \frac{\rho_{free}}{\symscr{r}} \hat{\symscr{r}}\,d^3r'
$$
meaning that:
$$
\vec{D} = \epsilon_0 \vec{E}_{free},\quad \vec{E}_{tot} = \frac{1}{\epsilon_r}\vec{E}_{free}
$$
since *inside the dielectric* we have $\vec{E}_{free} = q/(4\pi \epsilon_0 r^2) \hat{r}$, our final answer is:
$$
\vec{E}_{tot}^{inside} = \frac{1}{\epsilon_r} \left( \frac{q}{4\pi \epsilon_0 r^2} \right)\hat{r}
$$
which is intuitive because:

- dielectric getting polarized = induced dipole opposes $\vec{E}_{app}$, hence $\vec{E}_{tot}^{inside}$ becomes smaller
- if conductor, then $\epsilon_r \to \infty$ since $\chi_e \to \infty$. Then naturally $\vec{E}_{tot}^{inside}=0$ inside conductor

(can **verify this result by starting with drawing Gaussian** surface for finding $\vec{D}$)

---

*Treating $\sigma_b,\rho_b$ as real charges*:

Basically it works due to the proof that
$$
V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{||\vec{r} - \vec{r}'||}dA'+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{||\vec{r} - \vec{r}'||}dV' 
$$
Then, consider finding $V(r)$ for $r < a$ in the setup below:

![image-20220314173419462](em_equations/image-20220314173419462.png)

where $Q$ is a point charge sitting at origin. Then, we first can easily figure out $\vec{D}$ by:
$$
\int \vec{D}\cdot d\vec{S} = Q_{free_{enc}} = Q,\quad \text{true for all space}
$$
hence we get:
$$
\vec{D} = \frac{Q}{4 \pi r^2}\hat{r}\to \vec{E}_{tot} = \frac{Q}{4\pi \epsilon r^2}\hat{r}
$$
for $\epsilon$ ==actually changes in regions==, so that:

- $\epsilon=\epsilon_0$ in vacuum hence $r < a$ or $r > b$
- $\epsilon =\epsilon_r \epsilon_0$  inside dielectric

Though we can proceed easily to find $V(\vec{r})$ now, but we can show that we obtain the same result if we **treating $\sigma_b,\rho_b$ as real charges**:

- to find $\sigma_b, \rho_b$, first we need to find $\vec{P}$:
  $$
  \vec{P} = \epsilon_0 \chi_e \vec{E}_{tot} = \frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon r^2}\hat{r}
  $$

- then find $\sigma_b, \rho_b$:
  $$
  \sigma_b =\vec{P}\cdot \hat{n} = \begin{cases}
  -\frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon a^2} , & x =a\\
  \frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon b^2} , & x =b
  \end{cases}\\
  \rho_b = -\nabla \cdot \vec{P} =0,\quad a<x<b
  $$

Then we can consider $\vec{E}_{tot}$ from those charges while using symmetry:
$$
\begin{cases}
\vec{E}_{<} = \frac{Q}{4 \pi \epsilon_0 r^2} \hat{r}, & r < a\\
\vec{E}_{inside} = \frac{Q+Q_b(a)}{4 \pi \epsilon_0 r^2} \hat{r}= \frac{Q}{4 \pi \epsilon_0(1+\chi_e) r^2} \hat{r}, & a<r <b\\
\vec{E}_{>} = \frac{Q}{4 \pi \epsilon_0 r^2} \hat{r}, & r > b
\end{cases}
$$
using $\oint \vec{E}\cdot d\vec{A} = Q_{enc}/\epsilon_0$, where:

- $Q_b(a)$ is the bound charges at $r=a$, which is $4 \pi a^2 \sigma_b(a)$.
- this is consistent with $\vec{E}_{tot} = \frac{Q}{4\pi \epsilon r^2}\hat{r}$.

Finally, finding $V(\vec{r})$ for $r < a$ is just doing the integral:
$$
V(\vec{r}) = -\int_\infty^r \vec{E}\cdot d\vec{r} = -\int_\infty^b \vec{E}_<\cdot d\vec{r}-\int_b^a \vec{E}_{inside}\cdot d\vec{r}-\int_a^r \vec{E}_>\cdot d\vec{r}
$$
for $d\vec{r}=dr \hat{r}+ rd\theta \hat{\theta}$.

---

*Application of Force on Dielectric*

In most cases the force is only in **one direction**, hence using this suffices:
$$
-F_{on\,diel}=\frac{dW}{dx}=-\frac{1}{2}V(x)^2\frac{dC(x)}{dx}
$$
Consider the setup of

<img src="em_equations/image-20220327142501464.png" alt="image-20220327142501464" style="zoom: 50%;" />

where oil is the dielectric, with $\chi_e$ and $\rho_{oil}$. Here we assume the inner conductor is **fixed on $V=V_0$**, and the outer is grounded. We want to know the height of oil at equilibrium = at what $z$ we have $F_{on\,diel}$ cancels $F_{g}$

No matter fixing $V$ or fixing $Q$, we still have
$$
-F_{on\,diel}=\frac{dW}{dz}=-\frac{1}{2}V(z)^2\frac{dC(z)}{dz}=-\frac{1}{2}V_0^2\frac{dC(z)}{dz}
$$
Hence we just need to find out $C(z)$. Since $\Delta V=V_0$ is fixed, hence we just need to find out $Q(z)$ as we know:
$$
C(z) = \frac{Q(z)}{\Delta V}=\frac{Q(z)}{V_0}
$$
If we consider charges as line density $\lambda \cdot l$, then we know
$$
Q(z) = \lambda_{diel}z + \lambda_{vac}(l-z)
$$
Then $\lambda_{vac}$ can be easily calculated from $E_{vac}=V_0/(b-a)$ and using a Gaussian surface. Finding $\lambda_{diel}$ is similarly doing
$$
\int \vec{D}\cdot d\vec{S}=2\pi r l D=\lambda_{diel}l\quad \to \quad \vec{D} = \frac{\lambda_{diel}}{2\pi r}=\epsilon \vec{E}_{diel}
$$
Since we need $V_0$ to hold everywhere, we then can solve $\lambda_{diel}$:
$$
|\vec{E}_{diel}|\cdot (b-a)=|\vec{E}_{vac}|\cdot (b-a)\quad \to \quad \lambda_{diel}=\epsilon_r \lambda_{vac}
$$
which is a common result as we always had $\sigma_{diel}=\epsilon_r \sigma_{vac}$ whenever $\vec{D}$ and $\vec{E}$ has a similar shape + symmetry.

# Chapter 5

Electrostatics studies essentially the force on some test charge $Q$ due to some **static source charges** $\rho(\vec{r})$.

Magnetostatics studies the force on some **moving test charge** $Q, v$, due to mangetic field produced by **steady source current $J = \rho v$**. The important difference between magnetostatics and electrostatics is:

<img src="em_equations/image-20220423154819325.png" alt="image-20220423154819325" style="zoom:50%;" />

the wires are **still charge neural** hence no net electric field, but there is **magnetric field** due to moving charges and hence **Lorentz force** appears! Therefore, this chapter discusses 

- motion due to Lorentz force (e.g. cycltron motion)
- how to compute $\vec{B}$ in different scenarios (e.g. using $\vec{\nabla} \times \vec{A} = \vec{B}$)

| Condition                                          | Equation                                                     | Comment                                                      |
| -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Force due to $B$                                   | $\vec{F}_{mag}= Q(\vec{v}\times \vec{B})$                    | experienced by charge $Q$ moving with velocity $\vec{v}$     |
|                                                    | $\vec{F}_{total}=Q(\vec{E}+\vec{v}\times \vec{B})$           | if there is also an electric field                           |
|                                                    |                                                              |                                                              |
| Current                                            | $I = dq/dt$                                                  | Useful for deriving $I$ when given 3D shapes                 |
|                                                    | $I = \lambda v$                                              | for currents since $dq = \lambda dl$                         |
|                                                    |                                                              | for instance, $dq = \rho dV=\rho Adl$                        |
|                                                    |                                                              | assumes it is the direction of **positive charges** moving   |
| Force due to B *on* a current wire                 | $\vec{F}_{mag} = I \int d\vec{l}\times \vec{B}$              | For wire essentially it is a collection of moving charges with $dq= \lambda dl$ |
|                                                    | $d\vec{F}_{mag} = dq(\vec{v}\times \vec{B}) = \lambda dl(\vec{v}\times \vec{B})=(\vec{I}\times \vec{B})dl$ | derivation of the above                                      |
|                                                    |                                                              | basically still Lorentz force                                |
| Work done by $B$                                   | $dW_{mag} = \vec{F}_{mag}\cdot{d\vec{l}}=0$                  | Because $\vec{F}_{mag}= Q(\vec{v}\times \vec{B})$ but $d\vec{l}= \vec{v}dt$ |
|                                                    |                                                              | Therefore, it **only changes direction** of velocity         |
| Charge density                                     | $J = \rho v = dI/da_{\perp}$                                 | The parallel of $\rho$ in electrostatstics                   |
|                                                    |                                                              | Used for Biot-Savart Law                                     |
|                                                    | $I = \int \vec{J} \cdot d\vec{S}$                            | current in a wire with $\vec{J}$                             |
| Force due to $B$ on a current density              | $\vec{F}_{mag} = \int \vec{J}\times \vec{B}d\tau$            | essentially it is a collection of moving charges with $dq= \rho d\tau$ |
|                                                    | $d\vec{F}_{mag} = \rho d\tau (\vec{v}\times \vec{B}) =(\vec{J}\times \vec{B})d\tau$ |                                                              |
| Cycltron Drift Velocity                            | $\vec{v}_{drift} = \frac{\vec{E}\times \vec{B}}{B^2}$        | a moving charge then there is both $E$ and $B$ field, it drifts in addition to looping |
|                                                    |                                                              | better to find this during solving ODEs                      |
| Cycltron Rotation Frequency                        | $w_c = |qB/m|$                                               | found by solving the ODE                                     |
| Cycltron Radius                                    | $R = (mv) / (qB)$                                            | because $R = v/\omega_c$ for $v$ being the orbital speed     |
|                                                    |                                                              | works if we are only rotating                                |
| Biot-Savart Law Generic                            | $\vec{B}(\vec{r}) = \int \frac{\mu_0}{4\pi} \frac{\vec{J}(\vec{r}')\times(\vec{r}-\vec{r'})}{|\vec{r}-\vec{r'}|^3}\,d^3r'$ | True if $J$ is independent of time, i.e. steady              |
|                                                    | $\vec{E}(\vec{r}) = \int \frac{1}{4\pi \epsilon_0} \frac{\vec{\rho}(\vec{r}')\cdot(\vec{r}-\vec{r'})}{|\vec{r}-\vec{r'}|^3}\,d^3r'$ | parallel to the $\vec{E}$ field, basically $J \to \rho$      |
|                                                    | $\mu_0 = 1/(\epsilon_0 c^2)$                                 |                                                              |
| Biot-Savart if Steady current                      | $\vec{B}(\vec{r}) = \frac{\mu_0 I}{4\pi} \int \frac{d\vec{l}(\vec{r}')\times(\vec{r}-\vec{r}')}{|\vec{r}-\vec{r}'|^3}$ | basically we have here $\vec{J} = \vec{I}\delta^2(\vec{r}-\vec{r}')$ and that $\vec{I}dl = Id\vec{l}$ |
|                                                    |                                                              | works if constant current so $\vec{I}dl = Id\vec{l}$ holds   |
|                                                    | <img src="em_equations/image-20220423171201916.png" alt="image-20220423171201916" style="zoom: 67%;" /> | Integrates $dl'$ ==only== over where $\vec{r}' \neq 0$       |
| Magnetric Field of a Straight line wire            | $\vec{B} = \frac{\mu_0 I}{2\pi r} \hat{e}_\phi$              | Found either using Biot-Savart for current, or using the symmetry hence $\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc}$ |
|                                                    | <img src="em_equations/image-20220423171528461.png" alt="image-20220423171528461" style="zoom: 67%;" /> | Graphically it circulates the current                        |
| Divregence of B                                    | $\vec{\nabla}_{\vec{r}} \cdot \vec{B}(\vec{r})=0$            | for E field we had $\vec{\nabla}\cdot \vec{E}= \rho/\epsilon_0$ |
|                                                    | $\oint \vec{B}\cdot d\vec{S}=0$                              | for E field we had Gaussian surface $\oint \vec{E}\cdot d\vec{S}=Q_{enc}/\epsilon_0$! |
|                                                    |                                                              | derived using $\int \vec{\nabla}\cdot\vec{B} d\tau = \oint \vec{B}\cdot d\vec{S}$ |
|                                                    |                                                              | not very useful for B field calculation                      |
| Curl of B                                          | $\vec{\nabla}_{\vec{r}} \times \vec{B}(\vec{r})=\mu_0 \vec{J}(\vec{r})$ | for E field we had this is 0                                 |
|                                                    | $\oint \vec{B}\cdot d\vec{l}=\mu_0 I_{enc}$                  | very useful for B field calculation if we have symmetry in $J$ or $I$ setup! |
|                                                    |                                                              | called **Ampere Loops**                                      |
|                                                    |                                                              | derived using $\int \vec{\nabla}\times \vec{B}\cdot d\vec{S} = \oint \vec{B}\cdot d\vec{l}$ |
| Field of Solenoid with $n$ turns per unit legnth   | $\vec{B} = \mu_0 n I \hat{e}_z$                              | if inside the solenoid                                       |
|                                                    | $\vec{B}=0$                                                  | if outside                                                   |
|                                                    | <img src="em_equations/image-20220423174559253.png" alt="image-20220423174559253" style="zoom: 67%;" /> | Essentially solved by drawing Amphere loops                  |
| Magnetic Vector Potential                          | $\vec{\nabla}\times \vec{A} = \vec{B}$                       | so that $\vec{A}$ is easier to compute                       |
|                                                    |                                                              | from $\vec{E}= - \vec{\nabla}V$ for $V$ is easier to compute |
| Gauges for $\vec{A}$ due to above definition       | $\nabla(\nabla \cdot \vec{A})-\nabla^2\vec{A} = \mu_0 \vec{J}$ | if $\vec{\nabla}\times \vec{A} = \vec{B}$, and we know $\nabla \times \vec{B} = \mu_0 \vec{J}$ |
|                                                    | if $\nabla \cdot \vec{A} = 0$                                | Colomb's Gauge, used by this course                          |
|                                                    | $\vec{A}' = \vec{A}+ \nabla\lambda$ results in the same $B$ field | therefore we can always force $\nabla \cdot \vec{A} = 0$ by choosing $\lambda$ |
| Magnetic Vector Potential Definition               | $\vec{A} = \frac{\mu_0}{4\pi}\int \frac{\vec{J}(\vec{r}')}{|\vec{r}-\vec{r}'|}d^3r' $ | derived from the above with $\nabla^2 \vec{A}=-\mu_0 \vec{J}$, so that each compontent $A_x,A_y,A_z$ essentially is an analog version of $\nabla^2 V = - \rho / \epsilon_0$ and we know the solution $V$ from using $\rho$ |
|                                                    |                                                              | this means that $\vec{A}$ is usually in the **same direction** as current! |
|                                                    | Technically $\vec{A} = \frac{\mu_0}{4\pi}\int \frac{\vec{J}(\vec{r}')}{|\vec{r}-\vec{r}'|}d^3r' + \vec{A}(0)$ | for $\vec{A}(0)$ is a reference point                        |
|                                                    |                                                              | usually use $\vec{A}(\infty)=0$                              |
| Flux (open surface)                                | $\Phi \equiv \int \vec{B}\cdot d\vec{A} = \oint \vec{A}\cdot d\vec{l}$ | non-zero because this is an open surface                     |
|                                                    |                                                              | don't confuse against $\oint \vec{B}\cdot $                  |
|                                                    |                                                              | can be used to find $\vec{A}$ if we have symmetry and knows $\vec{B}$ |
| Multipole Expansion of $\vec{A}$                   | $\vec{A}(\vec{r}) = \frac{\mu_0}{4\pi}\sum_{n=0}^\infty \frac{1}{r^{n+1}}\int \vec{J}(\vec{r}')(r')^n P_n[\cos \theta^*]d^3r'$ | we can no longer take $P_n[\cos \theta^*]$ for $\theta^*$ is the angle between $\vec{r}, \vec{r'}$ over any shaped wire! |
|                                                    |                                                              | derivation is simply the same as electrostatic case, taking $\frac{1}{|\vec{r}-\vec{r}'|}=\frac{1}{r}\sum_{n}^\infty (\frac{r'}{r})^n P_n[\cos \theta^*]$ |
|                                                    | $\vec{A}(\vec{r})=\frac{\mu_0}{4\pi}\sum_{n=0}^\infty \frac{M_n}{r^{n+1}}$, for $M_n=\int \vec{J}(\vec{r}')(r')^n P_n[\cos \theta^*]d^3r'$ | $n$-th Moment term                                           |
|                                                    | $V(\vec{r}) = \frac{1}{4 \pi \epsilon_0}\sum_{l=0}^\infty \frac{M_l}{r^{l+1}}P_l[\cos\theta]$, $M_l \equiv \int (r')^l \rho(r')d^3r'$ | electrostatic case where $\theta^*=\theta=\text{const}$      |
|                                                    | <img src="em_equations/image-20220423233826716.png" alt="image-20220423233826716" style="zoom: 67%;" /> | Grahpically we we are doing                                  |
|                                                    |                                                              | Generic and true if far away                                 |
| Multipole Expansion of $\vec{A}$ with a wire       | $\vec{A}(\vec{r}) = \frac{\mu_0 I}{4\pi}\sum_{n=0}^\infty \frac{1}{r^{n+1}}\int (r')^n P_n[\cos \theta^*]d\vec{l}'$ | Using $\vec{J} = \vec{I}\delta^2(\vec{r}-\vec{r}')$ and then $\vec{I}dl = Id\vec{l}$ |
|                                                    | $\vec{A}(\vec{r})=\frac{\mu_0}{4\pi}\sum_{n=0}^\infty \frac{M_n}{r^{n+1}}$, for $M_n=\int I(r')^n P_n[\cos \theta^*]d\vec{l}'$ | $n$-th Moment term                                           |
|                                                    |                                                              | use this if we have a wire with current $I$                  |
| Monopole of $\vec{A}$ with a wire                  | $\vec{A}_{mono} = \frac{\mu_0 I}{4 \pi}\frac{1}{r}\oint d\vec{l}=0$ | using the expansion formula above and taking $n=0$           |
|                                                    |                                                              | always true if a **closed loop**                             |
| Dipole of $\vec{A}$ with a wire                    | $\vec{A}_{dip} = \frac{\mu_0 I}{4 \pi}\frac{1}{r}\oint r' \cos(\theta^*) d\vec{l}$ | using the above expansion with $n=1$                         |
|                                                    | $\vec{A}_{dip} = \frac{\mu_0 I}{4 \pi}\frac{(\oint I d\vec{A}')\times \hat{r}}{r}=\frac{\mu_0 I}{4 \pi}\frac{\vec{m}\times \hat{r}}{r}$ | proven from $\hat{r}\cdot \vec{r}' = r' \cos(\theta^*)$      |
|                                                    |                                                              | note that $\hat{r} = \vec{r}/|\vec{r}|^2$ is not a coordinate vector |
|                                                    | $\vec{m}=\int I d\vec{A}'$                                   | Dipole moment alternative version                            |
|                                                    |                                                              | A closed curve but open surface                              |
|                                                    |                                                              | Won't work if $I$ is over an entire surface instead of a wire |
| Dipole of $\vec{A}$ of a loop wire with radius $a$ | $\vec{A}_{dip}(\vec{r}) = \frac{\mu_0 Ia^2}{4r^2}\sin \theta \hat{e}_\phi$ | $\theta$ is a constant when given $\vec{r}$                  |
|                                                    |                                                              | either find with the generic formula for $\vec{A}_{dip}$, or use the short cut of $\vec{A}_{dip}=\frac{\mu_0 I}{4 \pi}\frac{\vec{m}\times \hat{r}}{r}$ for $\vec{m}$ is easy here |
|                                                    | $\vec{A} \approx \vec{A}_{dip}$                              | higher order terms drops very fast                           |
|                                                    | ![image-20220423235902806](em_equations/image-20220423235902806.png) | Graphical visualization                                      |
| $\vec{A}$ and $\vec{B}$ when given a dipole        | $\vec{A}(\vec{r}) = \frac{\mu_0}{4\pi} \frac{m_0 \sin\theta}{r^2}\hat{e}_\phi$ | if dipole is at origin $\vec{m} = m_0 \hat{e}_z$             |
|                                                    | $\vec{B}(\vec{r}) = \frac{\mu_0 m_0}{4\pi r^2}(2 \cos \theta \hat{e}_\rho + \sin \theta \hat{e}_\phi) $ | taking $\nabla \times \vec{A}$ with spherical coordinates    |
|                                                    | <img src="em_equations/image-20220424001736659.png" alt="image-20220424001736659" style="zoom:80%;" /> | Visualizatoin of a "Dipole"                                  |
| $\vec{A}_{dip}$ of rotating charges                | $\vec{A}_{dip} = \int d\vec{A}_{dip}^{loop}$                 | for we know that $d\vec{A}_{dip}^{loop}=\frac{\mu_0 a^2}{4r^2}\sin( \theta) dI\hat{e}_\phi$ for a loop current |
|                                                    |                                                              | basically suming over loops of wires. e.g. works by finding $dI = d\lambda \cdot v$ for $v = R\Omega$ |
|                                                    |                                                              | once we found $\vec{A}_{dip}$ recover $\vec{m}$ with $\vec{A}_{dip}(\vec{r}) = \frac{\mu_0}{4\pi} \frac{m_0 \sin\theta}{r^2}\hat{e}_\phi$ |
|                                                    | <img src="em_equations/image-20220424001530883.png" alt="image-20220424001530883" style="zoom: 67%;" /> | cannot use $\vec{m}=\int I d\vec{A}'$ directly as we obviously don't have a wire loop |

*Cycltron Motion*

In the generic case we have both $\vec{E}, \vec{B}$, we have both terms in the lorentz Force. 
$$
\vec{E} = E \hat{x}\\
\vec{B} = B \hat{z}
$$
Since we know $\vec{B}$ only **acts on $\vec{v}_{\perp}$** to the field itself, we decompose the EoM into velocity perpendicular to $B$ and parallel to $B$:
$$
m \frac{d\vec{v}_{\perp}}{dt} = q (E_x + \vec{v}_{\perp}\times \vec{B})\\
m \frac{d\vec{v}_{\parallel}}{dt} = 0
$$
we notice that $\vec{E} \perp \vec{B}$ in this setup, hence $\vec{v}_{\parallel}$ is unaffected. Then we consider solving in **cartesian coordiantes**:
$$
\frac{d\vec{v}_{\perp}}{dt} = \frac{q}{m} (E_x + \vec{v}_{\perp}\times \vec{B})\to 
\begin{cases}
\dot{v}_x = \frac{q}{m} (E_x + Bv_y)\\
\dot{v}_y = -\frac{q}{m} (Bv_x)
\end{cases}
$$
which is a coupled equation and **unsymmetric** we can solve by:

1. convert to symmetric equation with $v_y' = v_y + (E_x/B)$, and $v_x'=v_x$ to get:
   $$
   \dot{v}_x' = \frac{q}{m} ( Bv_y')\\
   \dot{v}_y' = -\frac{q}{m} (Bv_x')
   $$
   essentially **shifted a reference frame** as if there is no more electric field. Then obviously the motion is a ==circular loop== in this frame.

2. Solve and found that
   $$
   \ddot{v}_x' = -\frac{q^2B^2}{m^2} v_x'\\
   \ddot{v}_y' = -\frac{q^2B^2}{m^2} v_y'
   $$
   hence we get $w_c = |qB/m|$ being the rotating frequency, and solution for $v$s are obviously sines and cosines so we are looping.

Note that the drift velocity we fonud here is a specific case of:
$$
\vec{v}_{drift} = \frac{\vec{E}\times \vec{B}}{B^2}
$$
whenever we have both electric field and magnetic field for a moving charge.

(a good exercise here is to a) solve the motion with initial velocity zero, and b) solve the velocity at the bottom of the curve)

---

*Using Biot-Savart Law for Current*

Consider using:
$$
\vec{B}(\vec{r}) = \frac{\mu_0 I}{4\pi} \int \frac{d\vec{l}(\vec{r}')\times(\vec{r}-\vec{r}')}{|\vec{r}-\vec{r}'|^3}
$$
to compute the magnetic field at $\vec{r}=(x,0,0)$ WLOG for this

<img src="em_equations/image-20220423172057492.png" alt="image-20220423172057492" style="zoom: 67%;" />

Then:

1. this **only integrates over wire**, so $d\vec{l}' = dz'\, \hat{e}_z$ and $\vec{r}' = (0,0,z')$

2. then you basically get
   $$
   \vec{B}(\vec{r}) = \frac{\mu_0 I}{4\pi} \int \frac{dz'\hat{e}_z\times(x\hat{e}_x - z'\hat{e}_z)}{[x^2+ z'^2]^{3/2}}
   $$
   and yuo will be done

However, it would be wrong if you considered any of the following combination

- $d\vec{l}' = dz'\, \hat{e}_z$ and $\vec{r}' = (x',y',z')$
- $d\vec{l}' = \delta(x')\delta(y')dz'\, \hat{e}_z$ and  $\vec{r}' = (0,0,z')$, which technically works but is wierd

If you are unsure you can always go back to the generic expressoin with $J$, and then

- taking $\vec{J} = \vec{I}(z')\delta(x')\delta(y')$ in our case, then $\vec{I}dl = Id\vec{l}$
- since that is generic, use  $\vec{r}' = (x',y',z')$

(as an exercise, find $B(z)$ for a loop current with radius $R$)

---

*Field of a Solenoid*

Due to symmetry we know we can use Ampere's loops. 

![image-20220423174841393](em_equations/image-20220423174841393.png)

But first we need to show that **there is no $B_r, B_\phi$ component**. How do we argue this? Consider viewing the setup as

|                          $B_\phi=0$                          |                          $B_r = 0$                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220423230418492](em_equations/image-20220423230418492.png) | ![image-20220423230810409](em_equations/image-20220423230810409.png) |

where the arguments are simple:

- for $B_\phi$, such a loop would have $2\pi B_\phi = 0$ using Ampere's Law

- for $B_r$, we can suppose there is in the figure image with $B = B\hat{e}_r$

  - reverse the current, we would have $-B\hat{e}_r$
  - and then flip 180 degrees, still $- B\hat{e}_r$

  but the configuration is the same as the first image! Hence $B=0$

Therefore, $B=B_z\hat{e}_z$. So all we need to do is to draw loops that have $z$ component:

<img src="em_equations/image-20220423231247879.png" alt="image-20220423231247879" style="zoom:67%;" />

where 

- for the second loop we have $B_z(a)L - B_z(b)L = 0$. Hence $B_z(a)=B_z(b)$. But we konw the field is finite so that $B_z(\infty)=0$. Therefore $B_z(r)=0, r>R$
- then for the first lopo we get $B_z(r)L = \mu_0 (nL) I$, hence $B_z(r)=\mu_0 nL$ for $r < R$

---

*Magnetic Dipole of a Surface*

Consider computing $\vec{m}$ of a rotating disk with charge density $\sigma$ and rotation speed $\Omega$:

<img src="em_equations/image-20220424002154551.png" alt="image-20220424002154551" style="zoom:80%;" />

There are two ways to do it:

- consider finding $A$ then match to find $\vec{m}$ (do it as an exercise)
- consider $\vec{m} = \int d\vec{m}^{loop}$ 
  - we know for a loop $d\vec{m}^{loop} = (dI)A = \pi r^2 dI$
  - then we also know here that $dI = (d\lambda) v=(\sigma dr)(\Omega r)$
  - the integral becomes $\int_0^R \sigma \Omega \pi r^3 dr$

---

*B Field of a rotating shell of charge*

Essentially we integrate by summing over loop charges whose $\vec{A}_{dip}$ we already know

<img src="em_equations/image-20220424003406867.png" alt="image-20220424003406867" style="zoom:67%;" />

so we consider
$$
\vec{A}_{dip} = \int d\vec{A}_{dip}^{loop}
$$
for which we know in this case the loop wires are the red ones highlighted:
$$
d\vec{A}_{dip}^{loop}=\frac{\mu_0 a^2}{4r^2}\sin( \theta) dI\hat{e}_\phi
$$
and

-  $a = R \sin(\theta')$ in this case
- $dI = (d\lambda )v = (\sigma Rd\theta')(\Omega R\sin(\theta'))$
- then just put everything together and find $\vec{A}_{dip}$

Finally, we can use 
$$
\vec{A}_{dip}(\vec{r}) = \frac{\mu_0}{4\pi} \frac{m_0 \sin\theta}{r^2}\hat{e}_\phi
$$
for $\vec{m} = m_o \hat{e}_z$ to easily find $\vec{m}$ of this corresponding $\vec{A}_{dip}$

---

Summary of generic formulas

<img src="em_equations/image-20220424003649546.png" alt="image-20220424003649546" style="zoom:67%;" />

# Chapter 6

Magnetic fields in matter. Basically if we know a material is magnetized with $\vec{M}$ being the dipole per unit volume, what is the field $\vec{B}$ produced by such a magnetization.

- remember that all magnetic effects are due to moving charges
- in a small scale, it can be produced by electrons orbiting = electrons cloud spinning = spinning charged sphere; or it can be electrons self-spin. Both of which basicity create magnetic dipoles and hence $\vec{M}$

| Condition                                     | Equation                                                     | Comment                                                      |
| --------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Paramagnetism due to uniform $\vec{B}$        | $\vec{N} = \vec{m}\times \vec{B}$                            | dipole $\vec{m}$ not aligned with external field $\vec{B}$ will cause torque |
|                                               |                                                              | consider electron self-spin as a dipole in this example      |
| Diamagnetism due to uniform $\vec{B}$         | $\Delta \vec{m}=-\frac{1}{2}e\Delta v R\vec{e}_z = - \frac{e^2R^2}{4m_e}\vec{B}$ | if $\vec{B}=B\hat{e}_z$ is perpendicular to orbit, then it affects centripetal force, hence changing speed |
|                                               | $\vec{m}=I\int d\vec{a}=-\frac{1}{2}evR\hat{e}_z$            | without external field, $v$ solved from centripetal force being electrostatic attraction |
|                                               | $\frac{1}{4\pi \epsilon_0}\frac{e^2}{R^2}+ev'B = m_e\frac{v'^2}{R}$ | new speed $v'$ for same radius $R$ (due to quantum effect)   |
|                                               | $\Delta \vec{m}=-\frac{1}{2}e\Delta v R\vec{e}_z = - \frac{e^2R^2}{4m_e}\vec{B}$ | opposes the applied field $\vec{B}$                          |
|                                               |                                                              | Torque's effect of aligning the dipole is little in this case. |
| Magnetic Moment $\vec{M}(\vec{r})$            | $\vec{M}(\vec{r})\approx \sum \vec{m}$                       | think of $\vec{M}$ as specifying a collection of tiny dipoles $\vec{m}$ |
| $\vec{A}$ from $\vec{M}$                      | $\vec{A}=\frac{\mu_0}{4\pi} \sum_{i} \frac{\vec{m}\times \hat{r}}{|\vec{r}-\vec{r}_i'|}$ | discrete case                                                |
|                                               | $\vec{A}=\frac{\mu_0}{4\pi} \int \frac{\vec{m}\times \hat{r}}{|\vec{r}-\vec{r}'|}d^3r'$ | continuous case                                              |
|                                               |                                                              | note that $\hat{r}=\vec{r}/|\vec{r}|$                        |
|                                               | $\vec{A}_{dip} = \frac{\mu_0}{4\pi} \frac{\vec{m}\times \hat{r}}{|\vec{r}-\vec{r}'|}$ | derived because we know the field of a single dipole         |
|                                               |                                                              | Most generic form of finding $\vec{B}$ from given $\vec{M}$  |
| $\vec{A}$ from $\vec{M}$ using bound currents | $\vec{A}=\frac{\mu_0}{4\pi} \int_{all} \frac{\vec{J}_{b}}{|\vec{r}-\vec{r}'|}d^3r'+\frac{\mu_0}{4\pi}\oint \frac{\vec{K}_b}{|\vec{r} - \vec{r}'|}dA'$ | mathematically same as above                                 |
|                                               | $\vec{J}_b \equiv \nabla \times \vec{M},\quad \vec{K}_{b} \equiv \vec{M}\times \hat{n}$ | for $\hat{n}$ points from the magnetized material to vacuum  |
| Physical Interpretation of Bound Charges      | $\vec{K}_b = \vec{M}\times \hat{n}$                          | imagine uniform magnetized material = uniform tiny current loops |
|                                               |                                                              | Hence inner loops cancel, we only have net outer current     |
|                                               | $\vec{J}_b = \nabla \times \vec{M}$                          | If non-uniform magnetized material, then net $\vec{J}_b$ comes from difference in current loops |
| Auxiliary Field $\vec{H}$                     | $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$                     | more useful than $\vec{B}$ when we have magnetized material  |
|                                               |                                                              | $\vec{H}$ often in the **same direction** as $\vec{B}$ and $\vec{M}$ |
|                                               |                                                              | The magnetic parallel of $\vec{E}$ field. ($\vec{B}$ would be a parallel to $\vec{D}$ instead) |
|                                               | $\nabla \times \vec{B} = \mu_0 \vec{J}_{total} = \mu_0 (\vec{J}_b + \vec{J}_{free})$ | derivation from Ampere's Law, then use $\vec{J}_b = \nabla \times \vec{M}$ |
|                                               |                                                              | true in general                                              |
| Differential and Integral form of $\vec{H}$   | $\nabla \times \vec{H} = \vec{J}_{free}$                     | follows from the above                                       |
|                                               | $\oint \vec{H}\cdot d\vec{l} =\int \vec{J}_{free} \cdot d\vec{A}=I_{free_{enc}}$ | very useful when we have symmetry                            |
| Linear Material                               | $\vec{M} \equiv \chi_m \vec{H}$                              | $\vec{J}_{free} \to \vec{B}_{free}\to \vec{M}$ if material is magnetizable |
|                                               | $\vec{B}= \mu_0 (1+ \chi_m)\vec{H}=\mu \vec{H}$              | derived from $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$        |
|                                               | $\mu = \mu_r \mu_0 \equiv \mu_0(1 + \chi_m)$                 |                                                              |
| Boundary Conditions for $\vec{H}$             | $H^\perp_{above}-H^\perp_{below}=-(M^\perp_{above}-M^\perp_{below})$ | derived from Drawing a box on the surface and using $\oint \vec{H}\cdot d\vec{S}=-\int \vec{M}\cdot d\vec{S}$ (which comes from the definition of $\vec{H}$) |
|                                               | $H^\parallel_{above}-H^\parallel_{below}=K^{\perp}_{free}$   | from below                                                   |
|                                               | $\vec{H}^\parallel_{above}-\vec{H}^\parallel_{below}=\vec{K}_{free}\times \hat{n}$ | derived from Drawling a loop on the surface and using $\oint \vec{H}\cdot d\vec{l}=I_{free_{enc}}$. In this case we have two directions for $\vec{H}^\parallel$, hence the RHS. |
|                                               |                                                              | Useful for solving Laplacian with $\vec{H}=-\nabla W$        |
| Magnetic Scale Potential $W$                  | $\vec{H}=-\nabla W$                                          | if $\vec{J}_{free}=0$ so that $\nabla \times \vec{H}=\vec{0}$ |
|                                               | $\nabla^2 W = \nabla \cdot \vec{M} = 0$                      | if $\vec{\nabla}\cdot \vec{M}=0$, then it is Laplacian. True automatically if linear material. |
|                                               |                                                              | Combine with B.C.to solve Laplacian as in electrostatics     |
|                                               |                                                              | With very restrictive condition it holds                     |
| Field of Solenoid + Linear Material           | $\vec{B}_{in}=\mu_r \vec{B}_{vac_{in}}$                      | putting a linear material inside solenoid further increases the field produced |
|                                               |                                                              | derived easily from drawing Ampere loops and using $\oint \vec{H}\cdot d\vec{l}=I_{free_{enc}}$ |

*Calculate $\vec{B}$ from Bound Currents*

Suppose you are given a material with frozen in magnetization such that $\vec{M}=M\hat{e}_z$ inside the cylinder. The question is what is the magnetic field everywhere.

|                            Setup                             |                  Setup with Bound Currents                   |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image-20220503001559272](em_equations/image-20220503001559272.png) | ![image-20220503001603605](em_equations/image-20220503001603605.png) |

We know we can get $\vec{B}$ either:

- calculate $\vec{A}$ then do $\vec{B}=\nabla \times \vec{A}$. You wil see that we have many ways to find $\vec{A}$
  - if finite currents to integrate through, consider treating them as rotating charge = current loops $\vec{A}_{dip} = \int d\vec{A}_{dip}^{loop}$
  - or compute $\vec{A}$ from the bound charges using the generic formula above (a lot of work)
- calculate $\vec{B}$ directly by treating bound currents as currents (faster here)
- calculate using $\vec{H}$ if linear material (not applicable here)
  - if $\vec{J}_b=0$ and $\nabla \cdot \vec{M}=0$, then can use $\vec{H}=-\nabla W$ and solve laplacian

We first compute the bound currents and notice that:
$$
\begin{align*}
\vec{J}_b &= \nabla \times \vec{M} = \vec{0}\\
\vec{K}_b &= \vec{M}(\vec{r})|_{surf}\times \hat{n} = \begin{cases}
0, & \text{at top/bottom with }\hat{n} = \hat{e}_z\\
M\hat{e}_\phi & \text{at sides with }\hat{n} = \hat{e}_r
\end{cases}
\end{align*}
$$
With this we have the graph on the right, for currents going around the cylinder. In this case, we know that $\vec{B}=B_z\hat{e}_z$, which makes the computation straightforward.

Then we can easily compute $\vec{B}$ given this current by drawing ampere loop:
$$
\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc}
$$
Knowing that $\vec{B}_{outer}=\vec{0}$ from a solenoid, we have:
$$
B_z(r)L - 0 = \mu_0 L K_b = \mu_0LM
$$
Hence we obtain
$$
\vec{B}(\vec{r}) = \begin{cases}
\mu_0 M \hat{e}_z, & r < a\\
0, & r > a
\end{cases}
$$
which makes sense by right hand rule.

---

*Computing $\vec{A}$ given $\vec{M}$*

Here we consider some finite bound currents with spherical symmetry, so that we have a spherical **shell** with given magnetization of $\vec{M}=M\hat{e}_z$:

|                            Setup                             |                Parallel with Rotating Charges                |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="em_equations/image-20220503010500922.png" alt="image-20220503010500922" style="zoom:67%;" /> | <img src="em_equations/image-20220503010504464.png" alt="image-20220503010504464" style="zoom:67%;" /> |

First we compute the bound charges, and then decided what to do/which method to take:
$$
\begin{align*}
\vec{J}_b &= \nabla \times \vec{M} = \vec{0}\\
\vec{K}_b &= \vec{M}(\vec{r})|_{surf}\times \hat{n} = \vec{M}\times \hat{e}_\rho = M\sin \theta \hat{e}_\phi
\end{align*}
$$
So that essentially we have currents on the surface with $M\sin \theta$ dependence. This forbids us to use approaches with Ampere loops, but we realize that this is essentially integrating over many current loops = dipoles:
$$
\vec{A}_{dip} = \int d\vec{A}_{dip}^{loop}
$$
would be usable from previous chapter, which we basically know
$$
d\vec{A}_{dip}^{loop}=\frac{\mu_0 a^2}{4r^2}\sin( \theta) dI\hat{e}_\phi
$$
with $dI = (d\lambda )v = (\sigma Rd\theta')(\Omega R\sin(\theta'))$. Hence all we need to do is to convert **our $K$ to terms in $dI$**:
$$
\vec{K} = \sigma \vec{v} = \sigma(\Omega R \sin \theta) \hat{e}_\phi = \vec{K}_b = M \sin \theta \hat{e}_\phi
$$
Then we just basically can find $\vec{A}$, from which we can find $\vec{B}$.

---

*Calculate field using $\vec{H}$ from given $\vec{M}$*:

Consider given that $\vec{M} = kr^2 \hat{e}_{\phi}$ inside a cynlinder

|                            Setup                             |                         Ampere Loop                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="em_equations/image-20220503012829874.png" alt="image-20220503012829874" style="zoom:50%;" /> | <img src="em_equations/image-20220503012834480.png" alt="image-20220503012834480" style="zoom:50%;" /> |

Then we first know that the field $\vec{B},\vec{H}$ would therefore also be in $\hat{e}_{\phi}$ direction (do not confuse $\vec{M}$ with bound currents here). Hence drawing a Ampere loop inside:
$$
\oint \vec{H}\cdot d\vec{l} = 2\pi r H_\phi(r= I_{free_{enc}} = 0
$$
Hence we easily get $\vec{H}=0$ everywhere. Finally using $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$ we get back $\vec{B}$ field by:
$$
\vec{B} = \begin{cases}
\mu_0 kr^2 \hat{e}_\phi, & r < a\\
0, & r > a
\end{cases}
$$
since we know $\vec{M}$ as they are given.

- an exercise of this would be to compute using bound currents and arrive at the same solution here.

---

*When to use $\vec{H}$*

In general, we can do ampere loops with $\vec{B}$ if we are sure we know all contributions (e.g. from current and from $\vec{M}_{induced}$). But consider the case where you have a **magnetizable cylinder** with some external currents $\vec{J}_{free} = I/(\pi R^2)\hat{e}_z$ following through

<img src="em_equations/image-20220503013832964.png" alt="image-20220503013832964" style="zoom:67%;" />

The finding $\vec{B}$ naively using the loop above would give wrong result:
$$
2\pi B_\phi(r) = \mu_0 I \pi \left( \frac{r}{R}\right)^2
$$
but then yuo have ignored $\vec{M}_{induced}$ which could cause other components in $\vec{B}$. Therefore, the correct way is to consider $\vec{H}$:
$$
2\pi H_\phi(r) = I \pi \left( \frac{r}{R}\right)^2
$$
then using $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$ we get $\vec{B}$ (assuming $\vec{M}$ is linear)

---

*Parallels of $\vec{H}$ in Electrostatics*:

|                 | Electrostatics                                | Magnetostatics                      |
| --------------- | --------------------------------------------- | ----------------------------------- |
| Linear Material | $\vec{P}=\epsilon_0 \chi_e \vec{E}_{total}$   | $\vec{M} = \chi_m \vec{H}$          |
| Linear Material | $\vec{D} = \epsilon \vec{E}_{total}$          | $\vec{B} = \mu \vec{H}$             |
| Generic         | $\vec{D} = \epsilon\vec{E}_{total} + \vec{P}$ | $\vec{B} = \mu_0 (\vec{H}+\vec{M})$ |

so you see that the parallel of $\vec{E}$ is actually $\vec{H}$

---

*Magnetic Scalar Potential $W$*

With very restrictive condition a problem can be formulated as a Laplacian. Consider the setup where you have an **infinitely long** cylinder with $\vec{M}=M\hat{e}_x$. We want to find $\vec{B}$ everywhere.

<img src="em_equations/image-20220503131136729.png" alt="image-20220503131136729" style="zoom:80%;" />

First, it is always a good idea to figure out the bound currents:
$$
\begin{align*}
\vec{J}_b &= \nabla \times \vec{M} = \vec{0}\\
\vec{K}_b &= \vec{M}(\vec{r})|_{surf}\times \hat{n} = \vec{M}\times \hat{e}_r = M\sin \phi \hat{e}_z
\end{align*}
$$
where we basically converted to cylindrical coordinate $\vec{M}=M(\cos \phi \hat{e}_r-\sin \phi \hat{e}_\phi)$, and we ignored the top/bottom surface of of the cylinder since it is infinitely long.

Notice that we cannot use Ampere loops here due to $\phi$ dependence, hence we consider Laplacian by realizing that
$$
\vec{J}_{free} = \vec{0}\\
\nabla \cdot \vec{M} = \nabla \cdot (M\hat{e}_x)=0
$$
Therefore Laplacian holds for $\vec{H}=-\nabla W$:
$$
\nabla^2 W = 0
$$
We know the solution for cylindrical Laplacian, but we need boundary conditions:
$$
\begin{cases}
W_{out}=W_{in},& \text{continuity}\\
H^\perp_{above}-H^\perp_{below}=-(M^\perp_{above}-M^\perp_{below})=M\cos \phi, & \text{B.C.} \\
W_{out}(r \to \infty) = 0\\
|W_{in}(r=0)| \le \infty
\end{cases}
$$
And the solution takes the form of
$$
W(r,\phi)=(F_mr^m+G_mr^{-m})[L_m\cos(m\phi)+N_m \sin(m \phi)]
$$
then solve in the same manner as in electrostatics.]]></content><author><name></name></author><category term="2021@Columbia" /><summary type="html"><![CDATA[Applied EM Equations]]></summary></entry></feed>