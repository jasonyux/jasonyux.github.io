<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="/lectures/feed.xml" rel="self" type="application/atom+xml" /><link href="/lectures/" rel="alternate" type="text/html" /><updated>2023-05-11T22:09:58+00:00</updated><id>/lectures/feed.xml</id><title type="html">Lecture Notes</title><subtitle>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </subtitle><entry><title type="html">COMS3203 Discrete Math</title><link href="/lectures/2022@columbia/COMS3203_Discrete_Math.html/" rel="alternate" type="text/html" title="COMS3203 Discrete Math" /><published>2023-05-11T00:00:00+00:00</published><updated>2023-05-11T00:00:00+00:00</updated><id>/lectures/2022@columbia/COMS3203_Discrete_Math</id><content type="html" xml:base="/lectures/2022@columbia/COMS3203_Discrete_Math.html/"><![CDATA[<h1 id="discrete-math-logistics">Discrete Math Logistics</h1>

<ul>
  <li>Content on Courseworks, homework on Gradescope</li>
  <li><strong>Notes</strong> on Coursework provided by the Professor. Additional examples (not on the book) will be gone over during classes.</li>
  <li><strong>Recitations</strong> will be recorded, and content in the recitation needs to be studied</li>
  <li><strong>Homework</strong>: weekly, needs to be typed</li>
</ul>

<h1 id="logic">Logic</h1>

<p>Propositional logic, truth tables, Boolean algebra, inference.</p>

<h2 id="axioms-of-numbers">Axioms of Numbers</h2>

<blockquote>
  <p><strong>Natural Numbers</strong> contain all positive whole numbers (excluding zero)</p>

\[\mathbb{N} = \{ 1,2,3,... \}\]

</blockquote>

<blockquote>
  <p><strong>Even Numbers</strong> an integer $a$ if even IFF $a$ is divisible by 2; or exists an integer $b$ such that $a = 2b$</p>
</blockquote>

<blockquote>
  <p><strong>Odd Numbers</strong> an integer $a$ if is odd provided there exists a number $q \in \mathbb{Z}$ s.t. $a = 2q+1$</p>
</blockquote>

<blockquote>
  <p><strong>Prime Numbers</strong> let $p \in \mathbb{Z}, p&gt;1$. We call a number prime provided it is only divisible by $1$ and itself.</p>

  <ul>
    <li>e.g. 2, 3, 5…. By definition $1$ is therefore not prime.</li>
  </ul>
</blockquote>

<p>etc. Just to recall:</p>

<ul>
  <li>$\mathbb{Z}$ represents integer, $\mathbb{Q}$ represents rational numbers (fractions with integer num/denom), $\mathbb{R}$ represents real numbers</li>
  <li>$\mathbb{N}\subseteq Z \subseteq Q \subseteq R$</li>
</ul>

<h2 id="introduction-to-logic">Introduction to Logic</h2>

<blockquote>
  <p><strong>Proposition</strong>: a proposition is an objective, declarative statement that’s either true or false.</p>
</blockquote>

<p>for example, proposition includes:</p>

<ul>
  <li>$2+2=4$ (is true)</li>
  <li>$2 &gt; 5$ (is false)</li>
  <li>$x+2=5$ (used in first order logic as it depends on $x$=a predicate, but it is a proposition)</li>
</ul>

<p>not propositions:</p>

<ul>
  <li>Newborns are cute (subjective)</li>
  <li>Hello.</li>
</ul>

<blockquote>
  <p><strong>Atomic Proposition</strong>: simplest (elementary) form of a proposition. Some common notations used to denote atomic proposition include $p,q,r,t,s,z$ etc.</p>
</blockquote>

<blockquote>
  <p><strong>Compound Proposition</strong>: a proposition constructed from two or more atomic propositions using <em>connectives</em>, or adding a connective to an atomic proposition.</p>
</blockquote>

<blockquote>
  <p><strong>Major Connectives in Propositional Logic</strong>: if we have a proposition $p$</p>

  <ul>
    <li>
      <p><strong>negation</strong> (not): $\neg p$. For example, $p=$ it is snowing, $\neg p$ = it is not snowing. All possible negation actions include:</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230119110421501.png" alt="image-20230119110421501" style="zoom: 50%;" /></p>

      <p>note that the <strong>Truth Table</strong> above basically lays out all the possible “scenarios”</p>
    </li>
    <li>
      <p><strong>conjunction</strong> (and): $p \land q$.</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230119110900415.png" alt="image-20230119110900415" style="zoom:50%;" /></p>

      <p>now that there are 2 “variables”, we have $2^2$ possibilities</p>
    </li>
    <li>
      <p><strong>dis-junction</strong> (or): $p \lor q$.</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230119111803562.png" alt="image-20230119111803562" style="zoom:50%;" /></p>
    </li>
    <li>
      <p><strong>exclusive or</strong> (xor). $p \oplus q$. You expect only one of the options. interestingly this is more related to using “or” in natural language: e.g. “you can either have $p$ as side dish or $q$”</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230119112042366.png" alt="image-20230119112042366" style="zoom:50%;" /></p>
    </li>
    <li>
      <p><strong>implication</strong> (if, or implies): $p \to q$. For example, “if heavy snow ($p$), then Columbia is closed ($q$)”. So whenever $p$ is fulfilled, we expect $q$ to happen.</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230119112342403.png" alt="image-20230119112342403" style="zoom:50%;" /></p>

      <p>the first two rows are straight-forward. Consider “If I get an A in this course ($p$), then I will give you a dollar ($q$)”.</p>

      <ul>
        <li>If $p,q$ both happened, then I kept my promise. If $p$ happened but I didn’t give you a dollar ($\neg q$), then I broke the promise.</li>
        <li>If $p$ didn’t happen in the first place, then technically I <em>haven’t yet</em> break the promise. In this case we <strong>assume it is still true</strong>, i.e. the <mark>benefit of doubt</mark></li>
      </ul>
    </li>
    <li>
      <p><strong>if and only if</strong> (iff) $p \iff q$. Basically this means a conjunction to two implications $(p \to q)\land (q \to p)$</p>

      <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230124102509957.png" alt="image-20230124102509957" style="zoom: 43%;" /></p>

      <p>where the last one has $(p \to q)=T$ and $(q \to p)=T$ due to benefit of doubt. Hence $(p \iff q) = (T \land T) = T$. Notice that basically $p \iff q$ is true when $p,q$ are equivalently true or equivalently false.</p>
    </li>
  </ul>
</blockquote>

<p>note that</p>

<ul>
  <li>every connective mentioned above except for negation are <em>binary connectives</em>.</li>
  <li>the order of evaluation would go from
    <ol>
      <li>$()$ processed from inside to outside</li>
      <li>$\neg$ negation</li>
      <li>$\land$ and</li>
      <li>$\lor,\oplus$</li>
      <li>$\to$ implies</li>
      <li>$\iff$</li>
      <li>left to right</li>
    </ol>
  </li>
</ul>

<p>and in English</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230126104413095.png" alt="image-20230126104413095" style="zoom:50%;" /></p>

<p>where the more tricky ones are highlighted.</p>

<ul>
  <li>“$p$ only if $q$” means only if the consequence $q$ happens</li>
  <li>“$q$ is a necessary condition for $p$” means “$q$ is a necessary <em>conclusion</em> for $p$”</li>
  <li>a trick can be to consider which one maps correctly to “if $p$ then $q$”.</li>
</ul>

<hr />

<p><em>For Example</em>, consider $p \lor q \implies \neg r$.</p>

<p>First of all, how many rows will there be? Since there are three variables, $2^3=8$ rows.</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230124103755936.png" alt="image-20230124103755936" style="zoom:50%;" /></p>

<hr />

<p>Implication can be very tricky when combined with negation:</p>

<ul>
  <li>
    <p><strong>negation of implication</strong>: $\neg (p \to q)$ is $p \land \neg q$.</p>

    <ul>
      <li>for instance $\neg$(hit$\to$hurt) is (hit$\to\neg$hurt)</li>
      <li>is proven below (of course, they don’t come out of nowhere. See the example in the proof)</li>
    </ul>
  </li>
  <li>
    <p>if $p\to q$ is implication, then</p>

    <ul>
      <li><strong>converse</strong> $q \to p$</li>
      <li><strong>contrapositive</strong> $\neg q \to \neg p$</li>
      <li><strong>inverse</strong> $\neg p \to \neg q$</li>
    </ul>

    <p>why they are named like this can be seen more clearly from the truth table:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: left">p</th>
          <th style="text-align: left">q</th>
          <th style="text-align: center">$p \to q$</th>
          <th style="text-align: center">$q\to p$</th>
          <th style="text-align: center">$\neg q \to \neg p$</th>
          <th style="text-align: center">$\neg p \to \neg q$</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: left">T</td>
          <td style="text-align: left">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
        </tr>
        <tr>
          <td style="text-align: left">T</td>
          <td style="text-align: left">F</td>
          <td style="text-align: center">F</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">F</td>
          <td style="text-align: center">T</td>
        </tr>
        <tr>
          <td style="text-align: left">F</td>
          <td style="text-align: left">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">F</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">F</td>
        </tr>
        <tr>
          <td style="text-align: left">F</td>
          <td style="text-align: left">F</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
          <td style="text-align: center">T</td>
        </tr>
      </tbody>
    </table>

    <p>notice that $p \to q$ <mark>is the same as</mark> $\neg q \to \neg p$, and $q\to p$ <mark>is the same as</mark> $\neg p \to \neg q$.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Logical Equivalence</strong>: two propositions $p,q$ are called <em>logical equivalent</em> provided they have the <mark>same truth table</mark></p>

\[p \equiv q,\quad \mathrm{or}\quad p=q\]

  <p>we will mostly use $\equiv$. Note that this is <em>not a connective</em>, but rather a <em>relation</em>.</p>
</blockquote>

<p>Now you can show that $\neg (p \to q) \equiv p \land \neg q$</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">p</th>
      <th style="text-align: left">q</th>
      <th style="text-align: center">$p \to q$</th>
      <th style="text-align: center">$\neg (p \to q)$</th>
      <th style="text-align: center">$p\land \neg q$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">T</td>
      <td style="text-align: left">T</td>
      <td style="text-align: center">T</td>
      <td style="text-align: center">F</td>
      <td style="text-align: center">F</td>
    </tr>
    <tr>
      <td style="text-align: left">T</td>
      <td style="text-align: left">F</td>
      <td style="text-align: center">F</td>
      <td style="text-align: center">T</td>
      <td style="text-align: center">T</td>
    </tr>
    <tr>
      <td style="text-align: left">F</td>
      <td style="text-align: left">T</td>
      <td style="text-align: center">T</td>
      <td style="text-align: center">F</td>
      <td style="text-align: center">F</td>
    </tr>
    <tr>
      <td style="text-align: left">F</td>
      <td style="text-align: left">F</td>
      <td style="text-align: center">T</td>
      <td style="text-align: center">F</td>
      <td style="text-align: center">F</td>
    </tr>
  </tbody>
</table>

<p>And similarly, you can show that:</p>

\[p \to q \equiv \neg p \lor q \equiv \neg q \to \neg p\]

<p>So implication is just a <mark>disjunction</mark>! This is very important to know!</p>

<ul>
  <li>e.g. if I hit my thumb, it will hurt $\equiv$ I don’t hit my thumb, or else it will hurt</li>
</ul>

<blockquote>
  <p><strong>Tautology</strong>: a proposition is always true is called a <em>tautology</em>. For example, $p \lor \neg p$ is a tautology as no matter what value $p$ takes, <em>every</em> row in the truth table is <em>true</em>.</p>
</blockquote>

<p>A more interesting example would be</p>

<ul>
  <li>you may feel this is similar to $\iff$. But $\equiv$ is a relation <em>between</em> propositions, whereas $\iff$ is a <em>connective</em>.</li>
  <li>e.g. we know $p\to q \equiv \neg p \lor q$ as they have the same truth table. Then this also mean $(p\to q) \equiv (\neg p \lor q)$ is a <strong>tautology</strong></li>
</ul>

<blockquote>
  <p><strong>Fallacy/Contradiction</strong>: a proposition that is always false. For example, $p \land \neg p$.</p>
</blockquote>

<blockquote>
  <p><strong>Contingency</strong>:  a proposition that is not a tautology or fallacy, i.e. is a valid proposition. Usually we can just call this a (valid) “proposition”.</p>
</blockquote>

<hr />

<p><em>For Example:</em> Wason Puzzle</p>

<p>Consider a set of four cards. Each card has a letter on one side and one number on the other side.</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230124111846624.png" alt="image-20230124111846624" style="zoom:50%;" /></p>

<p>Consider the proposition: “If there is a vowel on one side, then there is an even number on the other side.”</p>

<p><strong>Question</strong>: What are the only cards that you <em>need to turn</em> to check if the proposition is True or False?</p>

<p><strong>Answer</strong>: you only need to turn $A$ and $3$. Let $p$=is vowel and $q$=other side is even. To falsify it, we need to basically get <strong>(vowel and not even)</strong> because this is the only case the <strong>implication $(p\to q)=F$</strong>. Therefore, the two relevant checks will be $p=T$ and $q=F$, corresponding to the card $A$ and card $3$.</p>

<p>More interestingly, notice that checking $3$ is the same as the $\neg q \to \neg p$ which is the <strong>contrapositive</strong>!</p>

<h2 id="laws-of-propositional-logic">Laws of Propositional Logic</h2>

<p>These basic laws (equivalences) are important as:</p>

<ul>
  <li>can be used to show that a proposition is equivalent than other ones, a tautology, of fallacy</li>
  <li>simplify a proposition</li>
</ul>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230126105134138.png" alt="image-20230126105134138" style="zoom:50%;" /></p>

<p>where the most important ones are highlighted. Most of them has something to do with simplifying $\land$ and $\lor$</p>

<ul>
  <li><strong>associativity</strong>: has to be used on the <em>same</em> connective (e.g. $(p\lor q)\land r$ has to be done in a distributive case)</li>
  <li><strong>distributivity</strong>: you can also show that $(q \lor r) \land p \equiv (q \land p) \lor (r \land p)$ by using the <em>commutativity law</em> and the first row in Distributive law</li>
  <li><strong>absorption</strong>: note that this is different from “Idempotence law”</li>
  <li><strong>DeMorgan’s</strong>: how to distribute <strong>negation</strong></li>
</ul>

<p><strong><em>Proof</em></strong> of $p \lor (p \land q) \equiv p$.</p>

<p>To get only $p$ on the RHS, we can consider $p \land T = p$ or $p \lor p = p$ as the last step. Here we use the first one:</p>

\[p \lor (p \land q) = (p \land T) \lor (p \land q) = p \land (T \lor q) = p \land T = p\]

<p><strong><em>Proof</em></strong> of $p \land (p \lor q) \equiv p$</p>

\[p \land (p \lor q) = (p \lor F) \land (p \lor q) = p \lor (F \land q) =  p \lor F = p\]

<p>which we already know is $p$ from the previous proof.</p>

<hr />

<p><em>For Example</em>: show that</p>

\[(p \lor q) \land (p \lor \neg q) \quad \text{is a tautology}\]

<p><strong><em>Proof</em></strong>: (first you start intuitively), realize that you are “or”ing $q$ and $\neg q$, hence:</p>

\[(p \lor q) \land (p \lor \neg q) = p \lor ( q \land \neg q) = p \lor (T) = T\]

<h2 id="logical-inference">Logical Inference</h2>

<p>Or logical deduction.</p>

<blockquote>
  <p><strong>Logical inference/argument</strong>: a sequence of propositions such that:</p>

  <ul>
    <li>all but the last propositions are called premises (i.e. precedes conclusion)</li>
    <li>the last proposition ($q$) is called conclusion</li>
  </ul>

\[\frac{p_1p_2,\dots p_n}{q}\]

  <p>an argument is called <mark>valid if the premises imply the conclusion</mark>. This means that:</p>

\[(p_1 \land p_2 \land \dots \land p_n) \to q \equiv T \quad \text{(is a tautology)}\]

</blockquote>

<p>But notice that implication has the benefit of doubt, so it <mark>only makes sure that if all $p_i$ is satisfied, $q$ will happen</mark>. If one of the $q_i$ is false, it does not affect anything.</p>

<hr />

<p><em>For Example</em>:</p>

\[\frac{p_1p_2}{p_1 \land p_2} \quad \text{is valid}\]

<p>because it is basically $p \to p$. In fact, if you have <strong>equivalence</strong>, such as</p>

\[\text{both}\quad \frac{\neg \neg p}{p},\frac{p}{\neg \neg p}\quad \text{are valid}\]

<p>you can also see that via a Truth Table</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230131105859364.png" alt="image-20230131105859364" style="zoom:50%;" /></p>

<hr />

<p><strong>Rules of Inference</strong></p>

<ul>
  <li>
    <p><strong>addition</strong>:</p>

\[\frac{p}{p \lor q}\]

    <p>you can prove this by showing that $p \to (p\lor q) \equiv T$</p>

\[\neg p \lor (p \lor q) = T \lor q = T\]

    <p>e.g. “it is raining” concludes “it is raining or xxxx” is obviously true.</p>
  </li>
  <li>
    <p><strong>conjunction</strong>:</p>

\[\frac{p \quad q}{p \land q}\]

    <p>basically means $(p \land q) \to (p \land q)$</p>
  </li>
  <li>
    <p><strong>simplification</strong>:</p>

\[\frac{p \land q}{p}\text{    and    } \frac{p \land q}{q}\]

    <p>because $p \land q \to p \equiv T$</p>
  </li>
  <li>
    <p><strong>disjunctive syllogism</strong>:</p>

\[\frac{p \lor q \quad \neg p}{q}\]

    <p>proof is left to you as an exercise, but intuitively if the premises is true, then it means $p$ is false hence $F \lor q$ is true concludes $q$ is true.</p>
  </li>
  <li>
    <p><strong>hypothetical syllogism</strong>:</p>

\[\frac{p \to q \quad q \to r}{p \to r}\]

    <p>which implies that implication is transitive.</p>
  </li>
  <li>
    <p><strong>resolution</strong></p>

\[\frac{p \lor q \quad \neg p \lor r}{q \lor r}\]

    <p>meaning that if $p\lor q$ and $\neg p \lor r$ is true, then it means either $q$ or $r$ has to be true. Hence the conclusion $q\lor r$ is true. You can again try to prove this formally $((p \lor q) \land (\neg p \lor r) \to q\lor r) \equiv T$</p>
  </li>
  <li>
    <p><strong>modus ponens</strong> (latin: mode that affirms)</p>

\[\frac{p \quad p \to q}{q}\]

    <p>is a <mark>very important law</mark>, as it means if $p \to q$ is true and $p$ happened, then $q$ must conclude.</p>
  </li>
  <li>
    <p><strong>modus tollens</strong> (latin: mode that denies)</p>

\[\frac{\neg q \quad p \to q}{\neg p}\]

    <p>very similar as above, as it means if $p \to q$ is true (e.g. raining $\to$ take umbrella) and $q$ did not happen (e.g. did not take umbrella), then $p$ did not happen (e.g. it is not raining).</p>
  </li>
  <li>
    <p><strong>transitivity</strong> this will be used a lot in proofs, so that basically if</p>

\[\frac{p}{q},\quad \frac{q}{r}\quad \text{are valid}\]

    <p>then</p>

\[\frac{p}{r}\quad \text{is valid}\]
  </li>
</ul>

<blockquote>
  <p>Inference is like implication except that it is (saying, since the premises are true, then this conclusion must be) <strong>always true</strong></p>
</blockquote>

<blockquote>
  <p>All <strong>laws of logic</strong> seen earlier can be written as <strong>rule of inference</strong>.</p>
</blockquote>

<p><em>For example</em>, you can transform $p \lor T \equiv T$ to inference as:</p>

\[\frac{p \lor T}{T} \text{    and  }  \frac{T}{p \lor T}\]

<p><em>Another example</em>: premise $p_1$=It’s not rainy but it’s nicer than yesterday. $p_2$=we will watch a movie only if it’s raining. $p_3$ we will go swimming if we do not watch a movie. $p_4$=if we go swimming, we will be home after sunset.</p>

<ol>
  <li>
    <p>Translate this into propositional logic (PL).</p>

    <p>let us use</p>

\[r:\text{rainy}\quad n:\text{nicer}\quad  w:\text{watch}\quad  h:\text{home}\quad  s:\text{swim}\quad\]

    <p>therefore this means</p>

    <ul>
      <li>$p_1 = \neg r \land n$</li>
      <li>$p_2 = w \to r$</li>
      <li>$p_3 = \neg w \to s$</li>
      <li>$p_4 = s \to h$</li>
    </ul>
  </li>
  <li>
    <p>Show that these premises lead to “we will be home after sunset” (premises are “true”)</p>

    <ol>
      <li>
        <p>first, we can simplify to get it is not raining:</p>

\[\frac{\neg r \land n}{\neg r}\]
      </li>
      <li>
        <p>then applying modus tollens</p>

\[\frac{w \to r \quad \neg r}{\neg w}\]
      </li>
      <li>
        <p>then applying modus ponens</p>

\[\frac{\neg w \to s \quad \neg w}{s}\]
      </li>
      <li>
        <p>finally</p>

\[\frac{s\quad s\to h}{h}\]
      </li>
    </ol>
  </li>
</ol>

<p><em>Another Example</em>: consider the whompus world where there are breezes $B$, pits $P$, and whompus $W$ such that $B_{11} \iff P_{12} \lor P_{21}$. And given that $\neg B_{11}$, show that $\neg P_{12}$</p>

<ol>
  <li>
    <p>we can break up the iff into $P_{12} \lor P_{21} \to B_{11}$ by simplification</p>
  </li>
  <li>
    <p>then apply modus ponens</p>

\[\frac{\neg B_{11}\quad P_{12} \lor P_{21} \to B_{11}}{\neg (P_{12} \land P_{21} )}\]
  </li>
  <li>
    <p>De Morgans law gives this means</p>

\[\neg (P_{12} \land P_{21}) = \neg P_{12} \land \neg P_{21}\]
  </li>
  <li>
    <p>finally, apply simplification</p>

\[\frac{\neg P_{12} \land \neg P_{21}}{\neg P_{12}}\]
  </li>
</ol>

<h2 id="first-order-logic">First Order Logic</h2>

<p>What if you want to say “there is no $W$ anywhere in the 5x5 cave”? Given current tools, this is not very convenient to do.</p>

<blockquote>
  <p><strong>First Order Logic</strong> (abbreviated as FOL), also known as <strong>predicate logic</strong>, combines quantifiers and predicates for a more powerful and compact formalism. So in a sense, PL is a <em>subset</em> of FOL.</p>
</blockquote>

<p>The idea is to:</p>

<ul>
  <li>instead of having just $p$, we consider $p(x)$, or $p(x,y)$, etc (i.e. predicates)</li>
  <li>and we add quantifiers such as $\forall, \exists$</li>
</ul>

<blockquote>
  <p><strong>Predicate</strong>: A predicate is a proposition whose truth value <mark>depends on one or more variables</mark>.</p>
</blockquote>

<blockquote>
  <p><strong>N-ary predicate</strong>: a predicate with $N$-variables</p>

\[\text{predicate}(\underbrace{x,y,z,...}_{\text{n variables}})\]

</blockquote>

<p>For example, this includes</p>

<ul>
  <li>(predicates) $x+2=5$; even$(x)$; $x+y &gt; 10$</li>
  <li>
    <p>i.e. they <em>can be true or false</em>, but it <em>depends</em> on the values of the variables</p>
  </li>
  <li>i.e. it is like a function that <mark>always evaluates to true or false</mark> given the “input values”</li>
</ul>

<blockquote>
  <p><strong>Quantifiers</strong></p>

  <ul>
    <li>universal quantifier: $\forall$ expresses such as “for all, for every, all of, for each, for any, any of, given any, for an arbitrary, etc.”</li>
    <li>existential quantifier: $\exists$ expresses such as “there exist, for some, for at least one, there is, there is at least one, etc.”</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Universal Quantifier</strong> additionally requires a <em>variable name</em> and a <em>domain</em>.</p>

\[\forall \underbrace{x}_{\text{variable}}  \underbrace{ \in A}_{\text{a domain}}\]

  <p>then this can be combined with a proposition</p>

\[\forall \underbrace{x}_{\text{variable}}  \underbrace{ \in A}_{\text{a domain}}\,\,(\text{predicate}...)\]

</blockquote>

<p>and then you can say something like “all aliens are green”, by considering $A$=the set of aliens:</p>

\[\forall a \in A\,\, (green(a))\]

<p>(which may or may not be true). Another example: all integers are even:</p>

\[\forall a \in \mathbb{Z}\,\, (x\,\text{mod }2 = 0)\]

<blockquote>
  <p>One powerful/additional feature of FOL is that your predicate can contain essentially <strong>functions</strong>, introducing many new operators such as “mod”, “equality”, etc.</p>
</blockquote>

<p>More examples: “All Aliens are green and nice. Domain: Alien denoted $A$”</p>

\[\forall a \in A \quad (\text{green}(a) \land \text{nice}(a))\]

<blockquote>
  <p><strong>Existential Quantifier</strong>: he existential quantifier, denoted with the symbol $\exists$, expresses the statements: there exist, for some, for at least one, there is, there is at least one, etc. The application of this is the same as the “Universal Quantifier”</p>
</blockquote>

<p>For example: There exists an integer that is both even and prime</p>

\[\exists x \in \mathbb{Z} \quad (\text{even}(x) \land \text{prime}(x))\]

<p><mark>but how do we prove it</mark>? In this example, we can by just realizing that $x=2$ satisfies. But in general, see <a href="#FOL Statements and Proofs">FOL Statements and Proofs</a></p>

<blockquote>
  <p><strong>Negation</strong> of FOL statements:</p>

  <ul>
    <li>$\neg (\forall x \quad p(x))$ means $\exists x \quad (\neg p(x))$</li>
    <li>$\neg (\exists x \quad p(x))$ means $\forall x \quad (\neg p(x))$</li>
  </ul>
</blockquote>

<p>For example:</p>

\[\neg(\forall x \exist y\quad p(x,y)) \equiv \exists x \forall y \quad\neg p(x,y)\]

<p>where we abbreviated $\forall x (\exist y\quad p(x,y))$ to just $\forall x \exist y\quad p(x,y)$</p>

<blockquote>
  <p><strong>Distributivity of FOL</strong>: in short, they <mark>do not distribute</mark></p>

  <ul>
    <li>
      <p>consider $\forall$:</p>

\[\forall x \quad P(x) \lor Q(x) \neq (\forall x \quad P(x)) \lor (\forall x \quad Q(x))\]

      <p>e.g. consider $P(x)$=even(x) and $Q(x)$=odd(x). Then LHS = $T$, but RHS = $F\lor F=F$</p>
    </li>
    <li>
      <p>consider $\exists$:</p>

\[\exists x \quad P(x) \land Q(x) \neq (\exists x \quad P(x)) \land (\exists x \quad Q(x))\]

      <p>e.g. consider $P(x)$=prime(x) and $Q(x)$=notprime(x). Then LHS=$F$ but RHS=$T\land T=T$</p>
    </li>
  </ul>
</blockquote>

<h3 id="domain-of-fol">Domain of FOL</h3>

<p>On the <strong>importance</strong> of the domain:</p>

<ul>
  <li>
    <p>Suppose you want to express: “All birds fly”</p>

\[\forall b \in B\quad \text{fly}(b)\]
  </li>
  <li>
    <p>“All birds <mark>except</mark> penguins fly”. Naively:</p>

\[\forall b \in B \quad (\text{fly}(b)\land \neg \text{penguin}(b))\]

    <p>but this also means we are stating two things:</p>

\[[\forall b \in B \quad \text{fly}(b)]\quad \land \quad [\forall b \in B \quad \neg \text{penguin}(b)]\]

    <p>i.e. whenever $b$ is a penguin, this will be false $\neq$ our original statement. The <mark>correct way</mark> to express it is</p>

\[\forall b \in B \quad ( \neg \text{penguin}(b) \to \text{fly}(b) )\]

    <p>i.e “if it is not a penguin, then it flies”, i.e. the <em>only false case is “not penguin, and can’t fly”,</em> and we are not claiming anything else than that, which is consistent with the statement.</p>

    <ul>
      <li>
        <p>(<em>optional</em>) technically you can also interpret this as “penguins don’t fly”. Then you might have</p>

\[\forall b \in B \quad ( \neg \text{penguin}(b) \iff \text{fly}(b) )\]
      </li>
      <li>
        <p>one way to think about this type of problem is to convert it into code</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">birds</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">penguin</span><span class="p">(</span><span class="n">a</span><span class="p">)):</span>
        <span class="n">fly</span>
    <span class="c1"># technically done
</span>    <span class="c1"># optionally:
</span>    <span class="k">if</span> <span class="n">penguin</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="ow">not</span> <span class="n">fly</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<p>Another example:</p>

<ul>
  <li>
    <p>“In the <strong>set of animals</strong>, all lions are fierce”</p>

\[\forall a \in A \quad \text{lion(a)} \to \text{fierce}(a)\]
  </li>
  <li>
    <p>“some lions are fierce”</p>

\[\exists a \in A \quad (\text{fierce}(x) \land \text{lion(a)})\]

    <p>is now correct.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong>: this means <em>usually</em></p>

  <ul>
    <li>$\forall$ goes with $\to$ implications (because $\forall$ is a strong quantifier)</li>
    <li>$\exists$ goes with $\land$ conjunctions</li>
  </ul>
</blockquote>

<p><em>For example</em>: Consider the domain of Humans, you want to express “All kids has pets”</p>

\[\forall h \in H\quad \text{kid}(h) \to \text{has pet}(h)\]

<p>then another way to do this is</p>

\[\forall h \in H ( \text{kid}(h) \to \exists \underbrace{ a \in A \quad \text{pet}(a) \land \text{is his pet}(a,k)}_{\text{exists a pet that the kid has}})\]

<p><em>For example</em>: “there exists a class that’s taken by every student”. Consider two domains, “event” (e.g. classes, workshops, conferences, sport events, etc) and “students”</p>

\[\exists e \in \text{event}\quad \text{class}(e) \land (\forall s \in \text{students} \quad \text{take}(s,e))\]

<h3 id="mixing-quantifiers">Mixing Quantifiers</h3>

<p>Consider the statement “All integers are even”, and</p>

\[\forall x \in \mathbb{Z}\quad ( \exists y\in \mathbb{Z}\quad x=2y )\]

<p>which is like a <mark>"nested loop"</mark> of, taking a loop over $x \in \mathbb{Z}$, and each is a even number.</p>

<p>Another example: what are the differences between:</p>

<ul>
  <li>$\forall x \in \mathbb{Z}\quad ( \forall y\in \mathbb{Z}\quad x+y=0 )$ is false as, given an $x$, I can easily find a $y$ such that it does not hold</li>
  <li>$\forall x \in \mathbb{Z}\quad ( \exists y\in \mathbb{Z}\quad x+y=0 )$ is true, by picking $y = -x$ for any given $x$</li>
  <li>$\exists x \in \mathbb{Z}\quad ( \forall y\in \mathbb{Z}\quad x+y=0 )$ is false, as this says “given a $x$”, all $y+x=0$ for any $y \in \mathbb{Z}$</li>
  <li>$\exists x \in \mathbb{Z}\quad ( \exists y\in \mathbb{Z}\quad x+y=0 )$ is true</li>
</ul>

<h3 id="fol-statements-and-prelude-to-proofs">FOL Statements and Prelude to Proofs</h3>

<p>Consider we want to prove:</p>

<ol>
  <li>$\forall x \in D\quad p(x):true$ means that $p(x)$ for <strong>all</strong> elements in $D$</li>
  <li>$\forall x \in D\quad p(x):false$ means that there is at least <strong>one</strong> element for which $p(x)$ is false</li>
  <li>$\exists x \in D \quad p(x): true$ means there is at least <strong>one</strong> element in $D$ for which $p(x)$ is true</li>
  <li>$\exists x \in D \quad p(x):false$: means that for <strong>all</strong> elements in the domain $p(x)$ is false</li>
</ol>

<p>This means that:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>$\forall x \quad p(x)$</th>
      <th>$\exists x \quad p(x)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prove</td>
      <td><strong>prove</strong> such as proof by contradiction, induction, etc.</td>
      <td>enough to find an <strong>example</strong></td>
    </tr>
    <tr>
      <td>Disprove</td>
      <td>enough to provide a <strong>counter example</strong></td>
      <td><strong>prove</strong> that $\forall x$ the predicate $p(x)$ is false, using …</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong> that sometimes when a domain is “clear”, instead of $\forall x \in A\quad p(x)$ you can just write $\forall x \quad p(x)$.</p>
</blockquote>

<p><em>For example</em>: consider the FOLs, for which we will prove</p>

<ol>
  <li>
    <p>“Any even integer is divisible by 2”. Let domain be $\mathbb{Z}$</p>

\[\forall x \in \mathbb{Z}\quad (\text{even}(x) \implies 2|x)\]

    <p>where $2\vert x$ means 2 divides x. Note that technically “if it is divisible by 2, it is an even integer” is also true, so technically $\iff$ here would be correct. But since we <em>are focused on proofs</em>, we might not want to do extra work, i.e. prove both sides if using $\iff$.</p>
  </li>
  <li>
    <p>“An integer is even iff it is divisible by 2”</p>

\[\forall x \in \mathbb{Z} \quad (\text{even}(x) \iff 2|a )\]
  </li>
  <li>
    <p>“The sum of two even integers is an even integer”</p>

\[\forall x,y \in \mathbb{Z} \quad (\text{even}(x) \land \text{even}(y)) \implies \text{even}(x+y)\]

    <p>but don’t you need to specify that $\text{even}(x,y)$ is also an integer? This is not necessary here by the axiom of closure property (adding two elements in $\mathbb{Z}$ results still in $\mathbb{Z}$).</p>
  </li>
  <li>
    <p>“Let $r\in \mathbb{R}$. If $r$ is irrational, then $\sqrt{r}$ is also irrational”. We are allowed to also use the set of rationals $\Q$</p>

\[\forall r \in \mathbb{R} \quad  (r \notin \Q \to \sqrt{r} \notin \Q)\]
  </li>
  <li>
    <p>“If $x$ is even, then $x^2+x$ is even.” Domain is integer.</p>

\[\forall x \in \Z \quad (\text{even}(x) \to \text{even}(x^2+x))\]
  </li>
  <li>
    <p>“There are no pair of integers that satisfy the expression $5x+25y = 1723$”</p>

\[\neg (\exist x,y \in \mathbb{Z}\quad 5x+25y=1723)\]

    <p>or alternatively you can say</p>

\[\forall x,y \in \mathbb{Z}\quad 5x+25y\neq 1723\]
  </li>
</ol>

<blockquote>
  <p><strong>Challenge question:</strong> there exists a unique with the quantifier we already have, to express</p>

\[\exists ! x \quad p(x)\]

  <p>where $\exists !$ is used now as a shortcut to express <mark>exists a unique</mark></p>

\[\exists x \quad p(x) \land (\forall y \quad p(y) \to (y = x))\]

  <p>this hints at, to prove a uniqueness theorem, you just need to show that: 1) there exist a solution, and 2) any other working solution is the same as that solution.</p>
</blockquote>

<p>Note that it would be <strong>incorrect</strong> in this case to say</p>

\[\forall x \quad p(x) \implies (\forall y \quad p(y) \to (y = x))\]

<p>because then, it could be there is <em>no such $x$ such that $p(x)$</em>, i.e. the “exist a solution” part we wanted to express is no longer there!</p>

<h1 id="proofs">Proofs</h1>

<p>First we need to define a few jargon</p>

<blockquote>
  <p><strong>Axiom</strong>: An axiom is a proposition that is assumed to be True. You don’t need to prove them.</p>
</blockquote>

<blockquote>
  <p><strong>Lemma</strong>: A preliminary mathematical proposition for which there is/<mark>needs a proof</mark>. Generally, lemmas precede a bigger result and lay the ground to a theorem.</p>

  <ul>
    <li>a baby version of “theorem”, but still requires a proof</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Theorem</strong>: A declarative mathematical statement for which there is/<mark>needs a proof</mark></p>

  <ul>
    <li>in greek this means “something needs to be proved”</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Corollary</strong>: A proposition that <em>follows from a theorem</em> in just a few steps.</p>

  <ul>
    <li>i.e. once the theorem is proven, these come very naturally/can be proven easily</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Conjecture</strong>: A strong mathematical result that is strongly believed to be true, but is <em>yet to be proven</em>.</p>
</blockquote>

<p>In general, you will be approaching them with:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230209112412432.png" alt="image-20230209112412432" style="zoom: 67%;" /></p>

<h2 id="proof-by-enumeration">Proof by Enumeration</h2>

<blockquote>
  <p>When the domain is small, we can verify the proposition <strong>for all values</strong>.</p>
</blockquote>

<p><em>For Example</em>: Proposition $\forall x \in \mathbb{Z}$ then $1 &lt; x &lt; 5 \implies (2\vert x) \lor (3\vert x)$</p>

<p><em>Proof</em>: we consider for each case $1&lt;x&lt;5$</p>

<ol>
  <li>when $x=2$, $2\vert x$ hence is true</li>
  <li>when $x=3$, $3\vert x$ is true hence true</li>
  <li>when $x=4$. $2\vert x$ is true hence true</li>
</ol>

<h2 id="direct-proof">Direct Proof</h2>

<p>Given a statement</p>

\[\forall x \in D\quad p(x) \implies q(x)\]

<p>Then in a direct proof you basically</p>

<blockquote>
  <p>Directly prove $p \implies q$ using definitions/axioms/logic. I.e. assuming $p$ is true, then $q$ is true.</p>
</blockquote>

<p><em>For Example</em>: $\forall a \in \mathbb{Z}$ and $\forall b \in \mathbb{Z}$ then</p>

\[\text{$a$ is even } \land \text{ $b$ is even } \implies (a+b) \text{ is even}\]

<p><em>Proof</em>: since we know $\text{$a$is even } \land \text{$b$is even }$</p>

<ol>
  <li>
    <p>this implies</p>

\[\exists k \in \mathbb{Z},\quad a=2k\\
\exists z \in \mathbb{Z},\quad b=2z\]
  </li>
  <li>
    <p>hence this means</p>

\[a+b = 2(k+z)\]
  </li>
  <li>
    <p>hence this means</p>

\[\exists t \in \mathbb{Z},\quad (a+b)=2t\]

    <p>e.g. by picking $t = (k+z)$</p>
  </li>
  <li>
    <p>therefore $(a+b)$ is even</p>
  </li>
</ol>

<blockquote>
  <p><em>Recall</em> that (we will use this quite often)</p>

  <ul>
    <li>$x$ is even $\iff$ $2\vert x$, or $\exists k \in \mathbb{Z},\  x = 2k$</li>
    <li>$x$ is odd $\iff$ $\neg 2\vert x$ or $\exists k \in \mathbb{Z},\  x = 2k+1$</li>
  </ul>
</blockquote>

<h2 id="proof-by-contradiction">Proof by Contradiction</h2>

<p>Consider a statement</p>

\[\forall x \in D \quad p(x) \implies q(x)\]

<blockquote>
  <p>Assume that the statement is not true, i.e.  $\neg ( p \implies q) = p \land \neg q$. Then show that this is impossible.</p>
</blockquote>

<p><em>For example</em>: prove $\forall a \in \mathbb{Z}$ and $\forall b \in \mathbb{Z}$ then</p>

\[\text{$a$ is even } \land \text{ $b$ is even } \implies (a+b) \text{ is even}\]

<p>by contradiction.</p>

<p><em>Proof</em>: assume that $p \land \neg q$</p>

<ol>
  <li>
    <p>this means we assume the following is true</p>

\[(\text{$a$ is even } \land \text{ $b$ is even })  \land \neg(a+b \text{ is even})\]

    <p><mark>notice this $\land$ induced by contradiction proof</mark>. This effectively allows us to use both LHS and RHS of the equation</p>
  </li>
  <li>
    <p>this means</p>

\[(\text{$a$ is even } \land \text{ $b$ is even })  \land (a+b \text{ is odd})\]
  </li>
  <li>
    <p>this implies</p>

\[\exists k \in \mathbb{Z},\quad a=2k\\
\exists z \in \mathbb{Z},\quad b=2z\]
  </li>
  <li>
    <p>but this means</p>

\[a+b = 2(k+z)\quad \text{is even}\]
  </li>
  <li>
    <p>yet we claimed $(a+b)$ is not even. Is a contradiction.</p>
  </li>
</ol>

<h2 id="proof-by-contrapositive">Proof by Contrapositive</h2>

<p>Given the statement</p>

\[\forall x \in D \quad p(x) \implies q(x)\]

<blockquote>
  <p>Prove by showing that $\forall x \quad \neg q(x) \implies \neg p(x)$, which <em>might be easier in times</em>.</p>

  <ul>
    <li>i.e. start with $\neg q(x)$, and conclude that $\neg p(x)$. e.g. by using a direct proof.</li>
    <li>this is useful usually when you realize the LHS of the original statement, i.e. $p(x)$ is <mark>hard to move forward</mark></li>
  </ul>
</blockquote>

<p><em>For Example</em>: prove that $\forall r \in \mathbb{R}$, if $r$ is irrational, then $\sqrt{r}$ is also irrational.</p>

<ul>
  <li>since this means $r \neq a/b$, which is hard to move forward. Hence we consider converting proof by contrapositive.</li>
</ul>

<p><em>Proof</em>: by contrapositive, then $\sqrt{r}$ is rational $\implies$ $r$ is rational. Using a direct proof</p>

<ol>
  <li>
    <p>since $\sqrt{r} \in \Q$, this means</p>

\[\exists a,b \in \mathbb{Z}, \quad \sqrt{r}=\frac{a}{b}\]
  </li>
  <li>
    <p>then this means</p>

\[r = \frac{a^2}{b^2}\]
  </li>
  <li>
    <p>this means</p>

\[\exists c,d \in \Z, \quad r = \frac{c}{d}\]

    <p>for example by letting $a^2=c$, $b^2=d$</p>
  </li>
  <li>
    <p>this means $r \in \Q$.</p>
  </li>
</ol>

<p><em>For Example</em>: let $a,b \in \Z$ prove that $a+b &gt; 100 \implies ((a &gt; 50) \lor (b &gt; 50))$</p>

<p><em>Proof</em>: We realize it is hard to move forward from $a+b &gt; 100$ as we have two numbers on LHS. Consider using the contrapositive</p>

\[((a \le 50) \land (b \le 50)) \implies a+b \le 100\]

<p>the rest is obvious by direct proof.</p>

<h2 id="proof-of-iff">Proof of IFF</h2>

<p>Consider the statement</p>

\[\forall x \in D\quad p(x) \iff q(x)\]

<p>Then essentially you need to</p>

<ul>
  <li>proof by any method that $p(x) \to q(x)$</li>
  <li>proof by any method that $q(x) \to p(x)$</li>
</ul>

<p><em>For Example</em>: prove that $\text{$x^2$is even} \iff \text{$x$is even}$. To prove this we need to prove both ways</p>

<p><em>Proof</em>. We can first prove the forward direction</p>

\[\text{$x$ is even}\implies\text{$x^2$ is even}\]

<ol>
  <li>since $x$ is even, then $x=2a$ for some $a \in \Z$</li>
  <li>therefore $x^2 = 4a^2$</li>
  <li>hence $x^2 = 2 (2a^2)$</li>
  <li>hence $x^2 = 2c$ for some $c \in \Z$</li>
  <li>so $x^2$ is even</li>
</ol>

<p>Now, we prove that</p>

\[\text{$x^2$ is even}\implies\text{$x$ is even}\]

<ol>
  <li>
    <p>its hard to do a direct proof, we can try instead contrapositive</p>

\[\text{$x$ is odd}\implies\text{$x^2$ is odd}\]
  </li>
  <li>
    <p>then this means $x = 2a+1$ for some $a \in \Z$</p>
  </li>
  <li>
    <p>hence $x^2 = 4a^2 + 2a + 1$</p>
  </li>
  <li>
    <p>hence $x^2 = 2(2a^2 + a) + 1$</p>
  </li>
  <li>
    <p>meaning $x^2 = 2c + 1$ for some $c \in \Z$</p>
  </li>
  <li>
    <p>so $x^2$ is odd</p>
  </li>
</ol>

<h2 id="proof-by-contradiction-1">Proof by Contradiction</h2>

<blockquote>
  <p>In a proof by contradiction, assume the opposite of what you are trying to prove. Then you make logical deductions until you arrive to a <strong>contradiction</strong>.</p>
</blockquote>

<p><em>For Example</em>: Prove by contradiction that there are no integer $x,y$ that satisfy $5x+25y = 1723$</p>

<p><em>Proof</em>: This means to prove that</p>

\[\forall x,y \in \Z \quad 5x + 25y \neq 1723\]

<p>By contradiction, we can first assume that this is false. Hence this means the below is true by assumption.</p>

\[\exists x,y \in \Z \quad 5x + 25y =1723\]

<ol>
  <li>
    <p>let this be true. Then this means</p>

\[5(x+5y) = 1723\]
  </li>
  <li>
    <p>hence</p>

\[x + 5y = \frac{1723}{5}\]
  </li>
  <li>
    <p>hence contradiction as LHS is integer, but RHS is not.</p>
  </li>
</ol>

<h2 id="proof-by-cases">Proof by Cases</h2>

<blockquote>
  <p>In proof by cases, try to break the proof into cases, if your starting point is too general. Make sure that your cases span <strong>all possibilities</strong></p>
</blockquote>

<p>So the general format would be to prove</p>

\[p_1 \lor \dots \lor p_n \implies q\]

<p>by proving for each $p_i \implies q$ by <em>proposition logic</em>.</p>

<hr />

<p><em>For Example</em>: Profe that for any integer $x$, $x^2 + x$ is even.</p>

<p><em>Proof</em> : this means that</p>

\[\forall x \in \Z\quad \text{$x^2+x$ is even} \equiv \forall x \in \Z\quad (x\in \Z \implies \text{$x^2+x$ is even} )\]

<p>this is straightforward using a direct proof, and essentially showing that $x(x+1)$ must include both an even number and an odd number.</p>

<p>To prove by cases, we can consider (inspired from the direct proof) that</p>

\[\forall x \in \Z\quad (\text{$x$ is even} \lor \text{$x$ is odd} \implies \text{$x^2+x$ is even} )\]

<ul>
  <li>case 1: $x$ is even, then $x = 2a$ for some $a \in \Z$
    <ul>
      <li>then $x^2 + x = 2(2a^2 + a)$ is even.</li>
    </ul>
  </li>
  <li>case 2: $x$ is odd, then $x=2a+1$
    <ul>
      <li>then $x^2 + x = 2(2a^2 + 3a + 1)$ is also even</li>
    </ul>
  </li>
  <li>therefore, $x^2 + x$ is even.</li>
</ul>

<h2 id="proof-by-counter-example">Proof by Counter Example</h2>

<blockquote>
  <p>Used to <strong>disprove</strong> some proposition, by finding an example such that the proposition is false.</p>
</blockquote>

<p>Usually you will disprove</p>

\[p \to q\]

<p>meaning to find an example such that $p$ is true but $q$ is false.</p>

<p>Some “tricks/hints” when you are stuck:</p>

<ol>
  <li>keep in mind of your domain</li>
  <li>try to create edge examples</li>
  <li>try to prove it (though you know it will not hold), so you can look for pattern when this will likely fail</li>
</ol>

<hr />

<p><em>For example</em>: disprove that $\forall a,b \in \Z\quad (a\vert b \land b\vert a)\implies a=b$</p>

<p><em>Proof</em>: counter example: $a=-b$, e.g. $a=2$ and $b=-2$. Then LHS is true, but RHS is false.</p>

<p>You can see how this edge case come about if you attempted a direct proof to “attempt to” show this is true</p>

<ol>
  <li>let $a\vert b \land b\vert a$ is true</li>
  <li>Then this means $a = xb$ and $b = ya$ for $x,y \in \Z$</li>
  <li>hence this means $a = xy \cdot a$ hence $xy = 1$</li>
  <li>but this means either $x = y = 1$ or $x=y=-1$.</li>
  <li>in the latter case, this implies $a=-b$, which means LHS is true but RHS is false</li>
</ol>

<blockquote>
  <p><strong>Note</strong> that this is <em>different</em> from proof by contradiction, from which it is a) used to <em>prove</em> instead of disprove; b) flipped the entire proposition</p>
</blockquote>

<h1 id="sets">Sets</h1>

<p>In general, we we refer to sets, we consider unordered sets</p>

<blockquote>
  <p>We will distinguish between two types of collections: <strong>unordered sets</strong> (in which, order does not matter) and <strong>ordered sets or lists</strong> (in which order matters).</p>
</blockquote>

<h2 id="unordered-sets">Unordered Sets</h2>

<blockquote>
  <p><strong>(Unordered) Set</strong>: is a collection of <mark>distinct</mark> object in which order does not matter.</p>
</blockquote>

<p>for example:</p>

\[p = \{ \text{eraser, pen, highlighter} \}\]

<p>or you can also have an infinite set</p>

\[\mathbb{Z} = \{ \dots, -2,-1,0,1,2,\dots \}\]

<p>for us, in general we assume that <mark>counting numbers start with $1$</mark></p>

\[\mathbb{N} = \{ 1,2,3,\dots \}\]

<p>though there are also some people considers $\mathbb{N}$ includes $0$. You can even have sets inside a set</p>

\[bp = \{ \text{notebook, laptop, } \{ \text{eraser, pen, highlighter} \} \}\]

<p>but if we the following is <strong>not a set</strong></p>

\[S = \{ 1,1,2,3 \}\]

<p>since you have duplicates. This becomes a <em>multiset</em>, which we will not cover in this class.</p>

<blockquote>
  <p><strong>Notation</strong>:</p>

  <ul>
    <li>
      <p>belongs to $\in$. Means is a <em>member/element</em> in a set is denoted as</p>

\[a \in \mathbb{Z}\]
    </li>
    <li>
      <p>empty set is denoted as either</p>

\[b = \{\},\quad\text{or}\quad b = \emptyset\]
    </li>
    <li>
      <p>does not belong to $\notin$:</p>

\[\text{fridge} \notin \text{backpack}\]

      <p>where backpack is a set.</p>
    </li>
  </ul>
</blockquote>

<p>some caution include that, let</p>

\[bp = \{ \text{notebook, laptop, } \{ \text{eraser, pen, highlighter} \} \}\]

<p>then technically</p>

<ul>
  <li>$\text{notebook} \in bp$</li>
  <li>$\text{pen} \notin bp$ because “pen” is technically <em>not a member</em> of the set, but ${ \text{eraser, pen, highlighter} } \in bp$</li>
</ul>

<blockquote>
  <p><strong>Subset</strong>: let $A,B$ be two sets. $A$ is a subset of $B$ provided that every element in $A$ is also an element in $B$. We denote that</p>

\[A \subseteq B\]

  <p>where this is “subset <em>or equal</em>”.</p>
</blockquote>

<p>Note that the difference between $\in$ and $\subseteq$ is that</p>

<ul>
  <li>$\in$ can be used <strong>with an element</strong> or set, since a set can be an element</li>
  <li>$\subseteq$ must only be used with a <strong>set</strong></li>
</ul>

<p>But note that we can <em>translate the definition of subset to FOL</em>:</p>

<p>Let $A,B$ be sets. Then</p>

\[A \subseteq B \iff (\forall a \in A\quad a \in B)\]

<p>(just as a practice) What if the domain is $\Z$, i.e. for both sets?</p>

\[A \subseteq B \iff (\forall a \in \Z \quad a \in A \implies a \in B)\]

<p><em>For Example</em>:</p>

<ul>
  <li>${4} \subseteq { 1,2,3,4 }$ is true</li>
  <li>${4} \nsubseteq { 1,2,3,{4} }$ since the element in LHS is $4$, whereas the element on RHS is ${4}$.</li>
  <li>${{4}} \subseteq { 1,2,3,{4} }$ is true</li>
  <li>${} \subseteq { 1,2,3,{4} }$ in fact, ${} \subseteq S$ for any set $S$.</li>
  <li>${} \notin { 1,2,3,{4} }$ as empty set is not a member of the RHS.</li>
</ul>

<blockquote>
  <p><strong>Equality between sets</strong>: two sets are equal $A = B$ provided</p>

\[A = B \iff A \subseteq B \land B \subseteq A\]

  <p>or we can write as FOL</p>

\[A = B \iff (\forall a \quad a \in A \iff a \in B)\]

</blockquote>

<p>note that the above strictly says “take any element $a$ in the universe, if $a \in A$ then $a \in B$ and vice versa”. Therefore the below is <strong>wrong</strong></p>

\[A = B \iff (\forall a \quad a \in A \land a \in B)\]

<p>this instead says “every element in the universe is both in $A$ and $B$”. But yuo can do something like (<strong>correct</strong>)</p>

\[A = B \iff (\forall a \in A \quad a \in B) \land (\forall a \in B \quad a \in A)\]

<p>which is basically saying $A \subseteq B \land B \subseteq A$.</p>

<blockquote>
  <p><strong>Proper Subset</strong>: being a subset <em>but not equal</em></p>

\[A \subset B \iff A \subseteq B \land A \neq B\]

</blockquote>

<p><em>For Example</em>: $\Z \subset \Q$ because</p>

\[\Z \subset \Q \iff \Z \subseteq \Q \land \Z \neq \Q\]

<p>and of course</p>

<ul>
  <li>${1} \subset {1,2}$</li>
  <li>${1} \subseteq {1,2}$</li>
  <li>but ${1,2} \subseteq {1,2}$ only</li>
</ul>

<blockquote>
  <p><strong>Universal Set</strong> denoted as $U$, which is the set of <strong>everything under consideration</strong>.</p>
</blockquote>

<p>(Barber’/Russell’s paradox: in general, whenever there is self-reference, there is typically a paradox)</p>

<blockquote>
  <p><strong>Power Set</strong>: the set of <em>all subsets</em> of a set $A$ is called a power set $P(A)$</p>
</blockquote>

<p>For example, let $S = {1,2}$, then</p>

\[P(S) = \{ \empty, \{1\}, \{2\}, \{1,2\} \}\]

<p>basically,  a set including $0$ elements on the set, $1$ element of the set, and then $2$ elements of the set.</p>

<p>This means <strong>empty set is an element of any powerset</strong>, so that</p>

\[P(\empty) = \{ \empty \}\]

<p>How many elements are there in a powerset? There are <mark>$2^k$ elements in the powerset $P(S)$</mark>, for $k$ is the number of elements in $S$.</p>

<ul>
  <li>
    <p>proof 1: For each element in $P(S)$, e.g. let $S={a,b,c}$, then any power set is essentially:</p>

    <table>
      <thead>
        <tr>
          <th> </th>
          <th>a</th>
          <th>b</th>
          <th>c</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\empty$</td>
          <td>F</td>
          <td>F</td>
          <td>F</td>
        </tr>
        <tr>
          <td>${a}$</td>
          <td>T</td>
          <td>F</td>
          <td>F</td>
        </tr>
        <tr>
          <td>${b}$</td>
          <td>F</td>
          <td>T</td>
          <td>F</td>
        </tr>
        <tr>
          <td>${c}$</td>
          <td>F</td>
          <td>F</td>
          <td>T</td>
        </tr>
        <tr>
          <td>${a,b}$</td>
          <td>T</td>
          <td>T</td>
          <td>F</td>
        </tr>
        <tr>
          <td>…</td>
          <td> </td>
          <td> </td>
          <td> </td>
        </tr>
      </tbody>
    </table>

    <p>so basically it is the number of combinations of the truth table</p>
  </li>
  <li>
    <p>proof 2: just use</p>

\[{0 \choose k} + {1 \choose k} + \cdots + {k \choose k} =2^k\]
  </li>
</ul>

<blockquote>
  <p><strong>Finite Set</strong>: a finite set is a set has $n$ elements for $n \in \Z^+$. Otherwise we call the set infinite.</p>
</blockquote>

<blockquote>
  <p><strong>Cardinality</strong>: let $A$ be a finite set, the number of elements in $A$ is called cardinality, denoted as $\vert A\vert$</p>
</blockquote>

<p>For example:</p>

<ul>
  <li>if $A$ has $n$ elements, then $\vert A\vert =n$.</li>
  <li>$S={1,2,3,…,20}$. Then $\vert S\vert =20$, and that $\vert P(S)\vert  = 2^{20}$.</li>
  <li>$\vert \Z\vert$ is then technically sloppy notation as $\Z$ is not a finite set, but we will use this later in functions.</li>
</ul>

<blockquote>
  <p>We have seen a lot of connections between FOL and sets. We will see how to build set using “<strong>set builders</strong>”, a compact way to define your set using FOL (and PL):</p>

\[S = \{ x | P(x) \}\]

  <p>where $P(x)$ is a FOL</p>
</blockquote>

<p><em>For Example</em>:</p>

<ul>
  <li>
    <p>instead of $S={1,2,3…,100}$, you can write</p>

\[S = \{ x | x \in \Z \land (1 \le x \le 100) \}\]

    <p>to avoid any ambiguity</p>
  </li>
  <li>
    <p>the set of even integers is then</p>

\[E = \{ x | x\in \Z \land 2|x \}\]

    <p>or more properly</p>

\[E = \{ x | x\in \Z \land (\exists a \in \Z \quad x=2a) \}\]
  </li>
  <li>
    <p>define the set of composite (non prime) number (excluding 1)</p>

\[S = \{ x | x\in \Z^+ \land \underbrace{(\exists a \in \Z^+ \quad (1&lt;a&lt;x) \land a|x)}_{\text{$a$ is non-prime}} \}\]

    <p>which basically says “every member in the set is (a positive integer AND is non-prime)”.</p>

    <p>note that it would be “incorrect” to write</p>

\[S = \{ x | x\in \Z^+ \quad {(\exists a \in \Z^+ \quad (1&lt;a&lt;x) \land a|x)}\}\]

    <p>because we are <em>not talking about $\forall x \in \Z^+$</em> or $\exists x \in \Z^+$, we are just letting $x\in \Z^+$ being a <strong>property of elements in this set.</strong></p>
  </li>
  <li>
    <p>define $S={1,4,9,16,…,100}$ by set builder:</p>

\[S=\{x|x\in Z\land (1\le x \le 100) \land (\exists a\in \Z \quad x=a^2 )\}\]

    <p>or you can</p>

\[S=\{x|x\in Z\land (1\le x \le 100) \land (\sqrt{x} \in \Z) \}\]

    <p>technically you can even omit $x\in \Z$ since by closure property this is implied by $\sqrt{x} \in \Z$.</p>
  </li>
</ul>

<blockquote>
  <p>Make sure that your set builder includes <strong>all the elements</strong> and <strong>only the elements intended</strong>, i.e. is correct and complete. This mean you should check</p>

\[\forall x \quad (x \in S \iff P(x))\]

  <p>so that forward is correctness, and backward is completeness (i.e. $\forall x \in U \quad P(x) \to x\in S$)</p>
</blockquote>

<h2 id="set-operations-and-properties">Set Operations and Properties</h2>

<blockquote>
  <p><strong>Set operations</strong> defined between 2 sets</p>

  <ul>
    <li>
      <p><strong>intersection</strong> of two sets:</p>

\[A \cap B = \{x | (x\in A) \land (x \in B) \}\]

      <p>basically conjunction in PL</p>
    </li>
    <li>
      <p><strong>union</strong> of two sets</p>

\[A \cup B = \{x | (x\in A) \lor (x \in B) \}\]
    </li>
    <li>
      <p><strong>difference</strong> between two sets</p>

\[A - B = \{x | (x\in A) \land (x \neq B) \}\]

      <p>in $A$ but no in $B$</p>
    </li>
    <li>
      <p><strong>complement</strong> (with respect to the universe)</p>

\[A^c = \bar{A} = \{x | x \notin A\}\]

      <p>where $x \in U$ part of the universe is assumed.</p>
    </li>
    <li>
      <p><strong>Cartesian product</strong> between two sets</p>

\[A\times B = \{ (a,b) | a\in A\land b \in B \}\]

      <p>for example $A={ c,d }$ and $B={e,f,g}$, then $A\times B={ (c,e),(c,f),(c,g),(d,e), (d,f),(d,g) }$. Therefore, this also means that the cardinality $\vert A \times B\vert  = \vert A\vert \cdot \vert B\vert$.</p>
    </li>
  </ul>
</blockquote>

<p>or even Venn Diagram</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230223110701172.png" alt="image-20230223110701172" style="zoom: 50%;" /></p>

<p>Then of course, you also have certain <strong>properties <em>of those operations</em></strong></p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230223111557244.png" alt="image-20230223111557244" style="zoom:50%;" /></p>

<p><em>For Example</em>:: prove that $\overline{A \cap B} = \bar{A} \cup \bar{B}$ using <strong>set builders</strong></p>

<p>since by definition of set operations</p>

\[\overline{A \cap B} = \{ x | x \notin (A \cap B) \}\]

<p>then we can rewrite as</p>

\[\overline{A \cap B} = \{ x | \neg (x \in (A \cap B)) \}\]

<p>but we can then define $A \cap B$ by</p>

\[\overline{A \cap B} = \{ x | \neg (x \in A \land x \in B) \}\]

<p>then by De Morgan</p>

\[\overline{A \cap B} = \{ x | x \notin A \lor x\notin B \}\]

<p>then finally just put it into our target form</p>

\[\overline{A \cap B} = \{ x | x \in \bar{A} \lor x \in \bar{B} \}\]

<p>hence</p>

\[\overline{A \cap B} = \{ x | x \in \bar{A} \cup \bar{B} \}\]

<p>But in general, there will be multiple ways to prove this.</p>

<h2 id="proofs-on-sets">Proofs on Sets</h2>

<p>We have covered a bit of how to prove set equality in previous section in an example. Here we discuss <em>more techniques</em> that you can use for set proofs.</p>

<ol>
  <li><strong>To prove that $A \subseteq B$</strong>
    <ul>
      <li>let $x \in A$</li>
      <li>…</li>
      <li>therefore $x \in B$</li>
    </ul>
  </li>
  <li><strong>To prove $A = B$ by showing that $A \subseteq B \land B \subseteq A$</strong>
    <ul>
      <li>show that $A \subseteq B$ by showing $x \in A$ … therefore $x \in B$</li>
      <li>show that $B \subseteq A$ by showing $x \in B$ … therefore $x \in A$</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>To prove equality, you can use <strong>any of the three methods</strong></p>

  <ol>
    <li>(element level) use <strong>set builder</strong>
      <ul>
        <li>e.g. see previous section on proving $\overline{A \cap B} = \bar{A} \cup \bar{B}$</li>
      </ul>
    </li>
    <li>(element level) use the <strong>subset</strong> technique above, e.g. to prove $F=E$
      <ul>
        <li>i.e. you can either prove $x\in F \implies x\in E$, and then again for $x \in E \implies x\in F$</li>
        <li>or you can shorten the above and <mark>show that $x\in F \iff x \in E$</mark> (e.g. see the last example in <a href="#Relations">Relations</a>)</li>
      </ul>
    </li>
    <li>(set level) using <strong>set operations</strong>, e.g. distributivity, De Morgan, etc.</li>
  </ol>
</blockquote>

<p><em>For Example</em>: let $F$ and $E$ be two sets.</p>

\[F = \{z|z \in \Z \land z = a+b \land a\text{ is odd} \land b \text{ is odd}\}\]

\[E = \{ x | x \in \Z \land 2 |x \}\]

<p>Prove that $F=E$.</p>

<ul>
  <li>
    <p>Prove that $F \subseteq E$</p>

    <ol>
      <li>
        <p>let $x \in F$.</p>
      </li>
      <li>
        <p>Then this means $x = a + b$ for some $a,b\in \Z$ where $a,b$ is odd</p>
      </li>
      <li>
        <p>therefore this means</p>

\[x = (2t+1)+(2w+1)\]
      </li>
      <li>
        <p>we can rewrite this as</p>

\[x = 2(t+w+1)\]
      </li>
      <li>
        <p>so $x$ is even, and $2\vert x$</p>
      </li>
      <li>
        <p>therefore $x \in E$</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Prove that $E \subseteq F$</p>

    <ol>
      <li>
        <p>let $x \in E$</p>
      </li>
      <li>
        <p>then this means $x=2t$ for some $t \in \Z$</p>
      </li>
      <li>
        <p>then we can rewrite this as</p>

\[x = (2t+1) - 1\]
      </li>
      <li>
        <p>therefore we can have $a = 2t+1$ and $b=-1$, meaning $a,b$ are odd</p>
      </li>
      <li>
        <p>therefore $x \in F$</p>
      </li>
    </ol>
  </li>
</ul>

<h2 id="size-of-the-union">Size of the Union</h2>

<blockquote>
  <p><strong>Size of the union</strong>: Let $A,B$ be two sets. Then</p>

\[|A\cup B| = |A| + |B| - |A \cap B|\]

  <p>But if $A,B$ are disjoint, then</p>

\[|A \cup B| = |A| + |B|\]

</blockquote>

<p>For example: What is $\vert A \cup B\cup C\vert$</p>

<p>Following the step above, we have</p>

\[\begin{align*}
&amp;\quad |A \cup B \cup C| \\
&amp;= |A \cup B| + |C| - |A \cup B \cap C| \\
&amp;= |A|+|B|-|A \cap B| + |C| - |(A \cup B) \cap C|
\end{align*}\]

<p>but notice that:</p>

\[(A \cup B) \cap C = (A\cap C) \cup (B \cap C)\]

<p>Hence</p>

\[|(A \cup B) \cap C| = |A\cap C| + |B \cap C| - |A \cap B \cap C|\]

<p>Hence we obtain</p>

\[|A \cup B \cup C| = |A|+|B|+|C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|\]

<blockquote>
  <p>Note that this including and then excluding back will also be touched on in counting, called the <strong>Principle of Inclusion Exclusion</strong>.</p>
</blockquote>

<p><em>Another in class example</em>: Let there be 16 people taking DM, 16 taking AP, and 11 taking OS. We are also given that</p>

<ul>
  <li>5 taking both DM and OS, and among them 3 taking AP as well</li>
  <li>8 people taking AP only</li>
  <li>5 people taking OS only</li>
</ul>

<p>We want to know how many want to take DM only.</p>

<p>Here, we solve it using <strong>Venn Diagram</strong></p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230304232008190.png" alt="image-20230304232008190" style="zoom: 33%;" /></p>

<h2 id="ordered-setslists">Ordered Sets/Lists</h2>

<p>Now, we turn to ordered sets, where the order of the elements matter.</p>

<blockquote>
  <p><strong>A list</strong> is an <mark>ordered</mark> set of element, and <mark>elements can now repeat</mark>. This is often denoted using parenthesis</p>

\[(e_1,e_2, \dots, e_n)\]

</blockquote>

<p>For example:</p>

<ul>
  <li>
    <p>$(9,1,1)$ is a valid ordered list</p>
  </li>
  <li>
    <p>we actually touched on this before</p>

\[A\times B = \{ \underbrace{(a,b)}_{\text{a list}} | a\in A\land b \in B \}\]

    <p>where order mattered, so that $A \times B \neq B \times A$.</p>
  </li>
  <li>
    <p>binary representation of numbers are also lists, so that all of the below are <em>different lists</em>: $101,110,011$.</p>
  </li>
</ul>

<h1 id="relations">Relations</h1>

<p>Remember that for Cartesian product, we had</p>

\[A\times B = \{ (a,b) | a\in A\land b \in B \}\]

<p>so that let $S={A,D }$ being Annie and Daniel, and $C={DM, OS}$ being Discret Math and OS. Then</p>

\[S \times C = \{ (A,DM),(A,OS),(D,DM),(D,OS) \}\]

<p>but this doesn’t mean anything. Say we want to use this to represent “taking classes”</p>

<p>So we are taking <strong>subset</strong> to represent a <strong>meaningful relationship</strong>. i.e.</p>

\[R_{\mathrm{take class}} = \{ (A,DM),(D,DM) \}\]

<p>hence also $R_{\mathrm{take class}} \subseteq (S \times C)$</p>

<blockquote>
  <p><strong>Relation</strong>: let $A,B$ be two sets. We say that $R$ is a relation from $A$ to $B$ provided</p>

\[R \subseteq A \times B\]

  <p>and that $R$ is a <strong>set of pairs</strong>.</p>
</blockquote>

<p><em>For Example</em>:</p>

<ul>
  <li>
    <p>Let $A = {1,2}$. Then</p>

\[A \times A = \{ (1,1),(1,2),(2,1),(2,2) \}\]

    <p>consider a relation</p>

\[R = \{ (1,1),(1,2),(2,2) \}\]

    <p>then this $R$ means $\le$. in other words:</p>

\[(x,y)\in R \iff x&lt;y\]
  </li>
  <li>
    <p>Let $A = {1,2}$. Let $R_{even}$ be defined <strong>from $A$ to $A$</strong></p>

\[(x,y) \in R_{even} \iff x+y \text{ is even}\]

    <p>then this means</p>

\[R_{even} = \{ (1,1),(2,2) \}\]

    <p>or using set builders, this means</p>

\[R_{even} = \{ (x,y) | x\in A \land y \in A \land 2|(x+y) \}\]
  </li>
</ul>

<blockquote>
  <p>In general, we <strong>notate the relation</strong> as two ways</p>

\[(x,y) \in R_{even} \iff x+y \text{ is even}\]

  <p>or</p>

\[x  R_{even} y \iff x+y \text{ is even}\]

</blockquote>

<p><em>For Example</em>: Let $R$ be a relation on $\Z$, i.e. from $\Z$ to $\Z$.</p>

\[x R y \iff x \text{ divides }y\]

<p>then what is $R$? We can first list a few examples</p>

\[R = \{ (2,4),(2,6),(1,2), \dots \}\]

<p>meaning</p>

\[R = \{ (x,y) | x\in \Z \land y \in \Z \land x|y \}\]

<blockquote>
  <p>Note that a relation can involve <mark>two or more sets</mark>, i.e. n-ary relations.</p>
</blockquote>

<p><em>For Example</em>: a simple one</p>

\[R = \{ (a,b,c)| a \in A \land b \in B \land c \in C \}\]

<p>is a legit relation.</p>

<blockquote>
  <p><strong>Inverse of a Relation</strong>: let $R$ be a relation from $A$ to $B$. We define the inverse of $R$ denoted as $R^{-1}$ as follows</p>

\[\forall x \in A,\forall y\in B\qquad (x,y)\in R \iff (y,x) \in R^{-1}\]

  <p>so that $R^{-1}$ is a <strong>relation from $B$ to $A$</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Proposition: inverse of inverse is itself</strong></p>

\[R = (R^1)^{-1}\]

</blockquote>

<p><em>Proof</em> (using the subset technique):</p>

<ul>
  <li>prove $R \subseteq (R^{-1})^{-1}$:
    <ol>
      <li>let $(x,y) \in R$.</li>
      <li>then by definition of inverse $(y,x) \in R^{-1}$</li>
      <li>therefore $(x,y) \in (R^{-1})^{-1}$</li>
    </ol>
  </li>
  <li>prove $(R^{-1})^{-1} \subseteq R$:
    <ol>
      <li>let $(x,y) \in (R^{-1})^{-1}$.</li>
      <li>then by definition of inverse $(y,x) \in R^{-1}$</li>
      <li>therefore $(x,y) \in R$</li>
    </ol>
  </li>
</ul>

<p>But we notice that the step is <mark>symmetrical/same</mark> in both ways. Hence this proof <mark>can be proven in much fewer lines</mark></p>

\[(x,y) \in R \iff (y,x) \in R^{-1} \iff (x,y) \in (R^{-1})^{-1}\]

<h2 id="representation-of-relation">Representation of Relation</h2>

<p>We can visualize a relation using:</p>

<ol>
  <li><strong>Directed Graphs</strong> (DiGraphs)</li>
  <li><strong>Matrix</strong> (a boolean matrix)</li>
</ol>

<blockquote>
  <p><strong>Digraph</strong>: a visual representation of a relation with a set of nodes (vertices) connected by <mark>directed</mark> edges (arcs)</p>
</blockquote>

<p>For example: this relation is a subset of cross product $R \subseteq {1,2,3}\times {1,2,3}$</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230302102725671.png" alt="image-20230302102725671" style="zoom: 40%;" /></p>

<p>notice that here <mark>each node is an element in the pair</mark>, and in general, the graph <mark>does not have to be connected</mark> (e.g. imagine $R={(1,1),(2,2)}$)</p>

<blockquote>
  <p><strong>Matrix</strong>: A relation $𝑅$ defined from set $𝐴$ to set $𝐵$ can be represented by a Boolean matrix in which the rows are elements of $𝐴$ and the columns are represented elements of $𝐵$ and each entry of the matrix at row $𝑖$ column $𝑗$ is 1 if there is a relation through $𝑅$ between the ith element in $𝐴$ and the jth element in $𝐵$, otherwise the entry is zero.</p>
</blockquote>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230302103442944.png" alt="image-20230302103442944" style="zoom:50%;" /></p>

<p>Note that since essentially relations are <em>sets</em>:</p>

<blockquote>
  <p><strong>All the properties on set operations apply to relations as well.</strong> For instance, intersection is commutative for relations, and we have $R_1 \cap R_2 = R_2 \cap R_1$</p>
</blockquote>

<h2 id="properties-of-relations">Properties of Relations</h2>

<p>Some relations <em>can have</em> <mark>special</mark> properties, which would come useful later</p>

<ul>
  <li>
    <p><strong>Reflexivity</strong>: let $R$ be a relation defined on a set $A$. The relation is <strong>reflexive</strong></p>

\[R\text{ is reflexive}\iff \forall x \in A\quad (x,x)\in R\]

    <p>or you can also denote</p>

\[R\text{ is reflexive}\iff \forall x \in A\quad xRx\]

    <p>note that</p>

    <ul>
      <li>
        <p>graphically, it means for $\forall x\in A$, you have a self-loop</p>
      </li>
      <li>
        <p>the following would be incorrect: $R^{-1} = R$. e.g. let $R={(1,2),(2,1)}$</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Not Reflexivity</strong>: then there exists a pair that is not in $R$:</p>

\[R\text{ is not reflexive}\iff \exists x \in A\quad (x,x)\notin R\]

    <p>or you can also denote</p>

\[R\text{ is not reflexive}\iff \exists x \in A\quad x\not R x\]
  </li>
  <li>
    <p><strong>Irreflexive</strong>: then basically <strong>no self-loops</strong>:</p>

\[R\text{ is irreflexive}\iff \forall x \in A\quad (x,x)\notin R\]

    <p>examples include “is parent of”, “&gt;”.</p>
  </li>
  <li>
    <p><strong>Symmetric</strong>: “all directed edges = undirected edges”</p>

\[R\text{ is symmetric}\iff \forall x,y \in A\quad (x,y)\in R \implies (y,x)\in R\]

    <p>e.g. $A={1,2,3}$, and a symmetric relation $R={(1,2),(2,1)}$</p>
  </li>
  <li>
    <p><strong>Not symmetric</strong>: negation of the above. There exist a pair that is not “reciprocal”  (including “self-reciprocity”)</p>

\[R\text{ is not symmetric}\iff \exists x,y \in A\quad (x,y)\in R \land (y,x)\notin R\]
  </li>
  <li>
    <p><strong>Antisymmetric</strong>: there is never reciprocity (except “self-reciprocity”)</p>

\[R\text{ is anti-symmetric}\iff \forall x,y \in A\quad xRy \land yRx \implies x=y\]

    <p>note that the this is <mark>different from the below</mark></p>

\[R\text{ is anti-symmetric}\iff \forall x,y \in A\quad (x,y)\in R \implies (y,x)\notin R\]

    <p>the upshot is that you can have a relation that is <mark>both symmetric and anti-symmetric</mark>, and also being both symmetric and not anti-sym.</p>
  </li>
  <li>
    <p><strong>Transitivity</strong>: very useful later</p>

\[R\text{ is transitive}\iff \forall x,y,z \in A\quad xRy \land yRz \implies xRz\]
  </li>
</ul>

<blockquote>
  <p>Note that when you have $\forall x\in A \quad \text{blablabla}$, be careful how <mark>$A$ is defined</mark></p>
</blockquote>

<p><em>For Example</em>: Consider the following relations defined by $A = {1,2,3}$</p>

<ul>
  <li>
    <p>$R_1= { (1,3),(3,3),(3,1),(2,2),(2,3),(1,1),(1,2) }$</p>

    <ul>
      <li>is reflexive: because $\forall x \in {1,2,3}\quad xRx$</li>
      <li>not symmetric: because $(2,3)\in R \land (3,2) \notin R$</li>
      <li>not anti-symmetric: because $1R3\land 3R1$ but $1\neq 3$</li>
      <li>not transitive: $2R3\land 3R1$ but $2 \not R1$ (better see this drawing a <strong>Digram</strong>)</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230302111414955.png" alt="image-20230302111414955" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>$R_2 = { (1,1),(2,2),(3,3) }$ is symmetric and anti-symmetric</p>

    <ul>
      <li>is reflexive</li>
      <li>is symmetric: because for each element <em>in the relation</em>, $xRy \to yRx$</li>
      <li>is anti-symmetric: yes. Just proof by enumeration.</li>
      <li>is transitive: yes. Just proof by enumeration.</li>
    </ul>
  </li>
  <li>
    <p>$R_3={ (1,1),(1,2),(2,3),(3,1),(1,3) }$</p>

    <ul>
      <li>not irreflexive: because $(1,1)$ is there</li>
      <li>is symmetric: no</li>
      <li>is anti-symmetric: no, because $(1,3)$ and $(3,1)$ exists</li>
      <li>is transitive: no, because $(2,3)$ and $(3,1)$ but no $(2,1)$</li>
    </ul>
  </li>
  <li>
    <p>$R_4 = { (1,2),(2,3),(1,3) }$</p>

    <ul>
      <li>is irreflexive: no self-loops</li>
      <li>is anti-symmetric: yes, because there is no reciprocity whatsoever</li>
      <li>is transitive: yes, proof by enumeration</li>
    </ul>
  </li>
</ul>

<p>Consider the following $R$ acting on $\Z$:</p>

<table>
  <thead>
    <tr>
      <th>$R$</th>
      <th>reflexive</th>
      <th>irreflexive</th>
      <th>symmetric</th>
      <th>anti-symmetric</th>
      <th>transitive</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$=$</td>
      <td>yes</td>
      <td>no</td>
      <td>yes</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>$\le$</td>
      <td>yes</td>
      <td>no</td>
      <td>no</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>$&lt;$</td>
      <td>no</td>
      <td>yes</td>
      <td>no</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>$R_=$ is symmetric because if $x=y \implies y=x$, and is transitive, since $x=y\land y=z \implies x=z$. Finally it is also anti-symmetric because $a=b\land b =a \implies a=b$ (different from transitive)</li>
  <li>$R_\le$ is  reflexive since $\forall x\in \Z\quad x\le x$. Hence this is also not irreflexive. It is also not symmetric since $1 \le 2$ but $2 \not \le 1$. By axioms $a \le b \land b \le a\implies a=b$ is anti-symmetric.</li>
  <li>$R_&lt;$ is anti-symmetric because of <em>benefit of doubt</em>: $a &lt; b \land b &lt; a\implies a=b$ is true because LHS is <em>always false</em>.</li>
</ul>

<blockquote>
  <p><strong>Equivalence Relation</strong>: Let $R$ be a relation on $A$. We call a relation an equivalence relation provided</p>

  <ol>
    <li>$R$ is reflexive</li>
    <li>$R$ is symmetric</li>
    <li>$R$ is transitive</li>
  </ol>
</blockquote>

<p>For example, let $R$ be a relation between sets “$R$: have same cardinality”. So that (<mark>notice now we are operating on sets)</mark> let $A,B$ be sets:</p>

\[A,B\subseteq S\quad ARB\iff |A| = |B|\]

<p>note that since we are operating on sets being subsets of $S$, this means our relations are $R\subseteq P(S)\times P(S)$ defined on power sets.</p>

<p>Then, we want to prove that $R$ is an equivalence relation:</p>

<ul>
  <li>is reflexive, since $\forall A \subseteq S\quad \vert A\vert =\vert A\vert$</li>
  <li>is symmetric, since $\forall A,B\subseteq S \quad \vert A\vert =\vert B\vert \implies \vert B\vert =\vert A\vert$</li>
  <li>is transitive, since $\forall A,B \subseteq S\quad \vert A\vert =\vert B\vert \land \vert B\vert =\vert C\vert \implies \vert A\vert =\vert C\vert$</li>
</ul>

<p>Why do we care about this property? Because</p>

<blockquote>
  <p><strong>Equivalence Classes</strong>: let $R$ be an equivalence relation on $A$. Then let $a\in A$. The equivalence class of $a$, denoted as $[a]$ is the set of <mark>all elements in $A$</mark>, related to $a$ <mark>through the relation $R$</mark>:</p>

\[[a] = \{ x | x\in A \land x Ra \}\]

  <p>notice that it is defined through a <strong><em>particular relation</em></strong>, and you will see <strong>$a$ itself will always belong to this set</strong></p>
</blockquote>

<blockquote>
  <p>Why do we care about equivalence sets? You will see that</p>

  <ul>
    <li>
      <p>objects in an equivalence sets share <em>similar properties</em></p>
    </li>
    <li>
      <p>can <em>partitions sets into equivalence classes</em> - cutting down the amount of cases necessary to prove something.</p>
    </li>
  </ul>
</blockquote>

<p>For example:</p>

<ul>
  <li>let $R$ being the cardinality of sets. Then $[\empty] = {\empty}$</li>
  <li>let $R$ be the relation “Have same parents as you” $[\text{you}] = {\text{you, your siblings}}$</li>
</ul>

<blockquote>
  <p><strong>Properties of Equivalence Classes</strong>: let $R$ be an equivalence class on $A$, then</p>

  <ol>
    <li>$a\in [a]$ is always there (because an equivalence class is reflexive)</li>
    <li>$\forall a \in A\quad [a]\neq \empty$ due to the above</li>
    <li>if I take the equivalence class of all elements, then $\bigcup_{a\in A}[a] = A$</li>
  </ol>
</blockquote>

<p>Proof for $\bigcup_{a\in A}[a] = A$. We notice that this is basically proving <strong>equality between sets</strong>. We can then do:</p>

<ul>
  <li>$\bigcup_{a\in A}[a] \subseteq A$:
    <ol>
      <li>let $x \in \bigcup_{a\in A}[a]$.</li>
      <li>then this means $x$ must belong to <em>some equivalence class</em> $\exists a\in A\quad x\in [a]$</li>
      <li>but we know $[a]\subseteq A$</li>
      <li>therefore $x \in A$</li>
    </ol>
  </li>
  <li>$A \subseteq \bigcup_{a\in A}[a]$
    <ol>
      <li>let $x\in A$</li>
      <li>then $x\in [x]$</li>
      <li>therefore $x \in \bigcup_{a\in A}[a]$.</li>
    </ol>
  </li>
</ul>

<p><em>For example</em>, consider $R$ be defined on a set of 2x2 board configuration $S$, so that:</p>

\[\text{board}_1\ R\ \text{board}_2 \iff \text{board 1 and board 2 can be obtained form each other by reflection or rotation}\]

<p>we can:</p>

<ol>
  <li>
    <p>show that this relation is an <strong>equivalence relation</strong> (i.e. is reflexive, symmetric, and transitive)</p>
  </li>
  <li>
    <p>we can find all its equivalence classes and realize they are <strong>partitions!</strong></p>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230307150928998.png" alt="image-20230307150928998" style="zoom: 33%;" /></p>
  </li>
</ol>

<h2 id="relation-of-congruence-modulo-n">Relation of congruence modulo n</h2>

<p>This is used a lot in cryptography, and number theory.</p>

<blockquote>
  <p><strong>Congruence modulo n</strong>: let $n$ be a positive integer. We say that two integers $x,y\in \Z$ are congruent modulo n:</p>

\[x \equiv y(\mathrm{mod}\ n)\]

  <p><mark>provided that $n$ divides the difference $n\vert (x-y)$</mark>. Note that In the context of relations, <strong>”$\equiv$” means congruent.</strong></p>
</blockquote>

<p><em>For example</em>,</p>

<ul>
  <li>$3\equiv 13 (\mathrm{mod}\ 10)$ reads as “$3$ is congruent to $13$ modulo $10$”</li>
  <li>$13\equiv 3 (\mathrm{mod}\ 10)$</li>
  <li>$3\not\equiv 22 (\mathrm{mod}\ 10)$</li>
</ul>

<blockquote>
  <p><strong>Theorem</strong>: let $n$ be a positive integer. The relation “congruent modulo $n$” is an <strong>equivalence relation</strong> on the set of integers</p>
</blockquote>

<p><em>Proof</em>:</p>

<ul>
  <li>
    <p>is reflexive. This means we want to show that</p>

\[\forall x \in \Z,\quad x\equiv x(\mathrm{mod}\ n)\]

    <p>by definition, this means $\iff$ $n\vert (x-x)$, which is true since $n\vert 0$.</p>
  </li>
  <li>
    <p>is symmetric. This means we want to show that</p>

\[\forall x,y \in \Z,\quad x\equiv y(\mathrm{mod}\ n)\implies y\equiv x(\mathrm{mod}\ n)\]

    <p>then</p>

    <ol>
      <li>let $x\equiv y(\mathrm{mod}\ x)$. This means $n\vert (x-y)$</li>
      <li>so $n\vert -(x-y)$</li>
      <li>hence $n\vert (y-x)$</li>
      <li>therefore $y\equiv x(\mathrm{mod}\ y)$</li>
    </ol>
  </li>
  <li>
    <p>is transitive. This means to prove that</p>

\[\forall x,y,z\in \Z,\quad x\equiv y(\mathrm{mod}\ n)\land y\equiv z(\mathrm{mod}\ n) \implies x\equiv z(\mathrm{mod}\ n)\]

    <p>then</p>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230307111950238.png" alt="image-20230307111950238" style="zoom: 50%;" /></p>
  </li>
</ul>

<blockquote>
  <p><strong>Partition</strong>: a partition of a set is a collection of non-empty pairwise-disjoint subset of $S$, so that the union gives $S$</p>
</blockquote>

<p><em>For example</em> ,consider the congruence modolo $2$ of $\Z$ is</p>

\[\forall a,b\in \Z,\quad a \equiv b(\mathrm{mod}\ 2)\]

<p>where basically $a \equiv b(\mathrm{mod}\ 2)$is $aRb$.</p>

<ul>
  <li>
    <p>then we have</p>

\[[0] = \{ x | x \in \Z \land 0 \equiv x \text{ (mod 2)} \} = \{ ..., -4,-2,0,2,4,... \}\]
  </li>
  <li>
    <p>continuing</p>

\[[1] = \{ x | x \in \Z \land 1 \equiv x \text{ (mod 2)} \} = \{...-3,-1,1,3,... \}\]
  </li>
  <li>
    <p>but eventually you will realize</p>

\[[2] = \{ x | x \in \Z \land 2 \equiv x \text{ (mod 2)} \} = \{ ..., -4,-2,0,2,4,... \}\]

    <p>which is same as $[0]$!</p>
  </li>
  <li>
    <p>and you can show that also $[3] = [1]$.</p>
  </li>
</ul>

<p>As a result , this creates a <strong>partition of $\Z$ into two partitions</strong>, even and odd numbers (if you consider equivalences for $\mod 4$, you get 4 partitions, etc.)</p>

<blockquote>
  <p>Proposition: let $n \in \Z ^+$ and $a,b \in \Z$. Then</p>

\[a \equiv b (\mathrm{mod}\ n) \iff a\ \mathrm{mod}\ n = b\ \mathrm{mod}\ n\]

  <p>which you can try to prove as an exercise.</p>

  <ul>
    <li>note that in general, proofs with relation are <strong>essentially set proofs</strong>. See HW6 Q6 or recitation 8 for example.</li>
  </ul>
</blockquote>

<h1 id="functions">Functions</h1>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230321104019461.png" alt="image-20230321104019461" style="zoom: 50%;" /></p>

<p>It can be seen also as a type of <strong>relation</strong> where you basically have ${(x,f(x))}$. For example, if $f(x)=x^2$, then on the set of integers:</p>

\[f=\{(0,0),(1,1), (-1,1), (2,4), (-2,4),...\}\]

<blockquote>
  <p><strong>Functions</strong>: let $A,B$ be two sets. A function $f$ from $A$ to $B$ denoted</p>

\[f:A \to B\]

  <p>so $f \subseteq A \times B$ where <mark>each</mark> element of $A$ appears <mark>exactly once</mark> as a first element of the ordered pairs of $f$. (i.e. <mark>vertical line test</mark>). So that</p>

\[\text{$f$ is a function from $A$ to $B$}\iff \forall a \in A\quad \exists! b \in B \quad b = f(a)\]

  <p>i.e. there is <mark>only one $b$ for each $f(a)$</mark>.</p>
</blockquote>

<p>Some terminology to be used here. Let $f:A \to B$. Then</p>

<ul>
  <li>$A$ is called the <strong>domain</strong></li>
  <li>$B$ is the <strong>co-domain</strong></li>
  <li>$b\in B$ is called the <strong>image</strong> of $a$</li>
  <li>$a\in A$ is called the <strong>pre-image</strong> of $b$</li>
</ul>

<blockquote>
  <p>More formally, the set of all second elements of the pairs of $f$ is called the <strong>image of $f$:</strong></p>

\[\mathrm{image}(f) = \{ b | b \in B \land \exists a \in A\land  f(a)=b \}\]

  <p>so that $\mathrm{image}(f) \subseteq B$</p>
</blockquote>

<p><em>For example</em>: let $A = {1,2,3,4}$ and $B={1,2,3,4,7,10,12}$.</p>

<ul>
  <li>let $f={(1,2),(2,3),(4,3),(3,1)}$. This is a <em>valid</em> function as <em>each input $a\in A$ has a unique output</em>.
    <ul>
      <li>the image of $f$ is then ${2,3,1}$.</li>
    </ul>
  </li>
  <li>let $f={(1,2),(2,3),(4,3)}$. This is a <em>not valid</em> function as $a=3$ has undefined output.</li>
</ul>

<hr />

<p>One way to visualize functions is <strong>bipartite graph</strong>. For example the function</p>

\[f=\{(1,2),(2,3),(4,3),(3,1)\}\]

<p>can be visualized as</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230321110509355.png" alt="image-20230321110509355" style="zoom: 33%;" /></p>

<p>so that the “vertical line test” in discrete function is that each $a\in A$ has <mark>only one edge going out</mark>.</p>

<p><em>For example</em>, let $f:{0,1}\times {0,1} \to \N \cup {0}$</p>

<ul>
  <li>
    <p>so that basically $A={(0,0),(0,1),(1,0),(1,1)}$, and $B={0,1,2,3,…}$</p>
  </li>
  <li>
    <p>and you are given:</p>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230321111129449.png" alt="image-20230321111129449" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>is this a valid function? Yes, <em>every</em> $a\in A$ has a <em>unique</em> (only one) output.</p>
  </li>
  <li>
    <p>and also $\mathrm{image}(f)={0,1,2} \subseteq \N \cup {0}$.</p>
  </li>
</ul>

<h2 id="properties-of-functions">Properties of Functions</h2>

<blockquote>
  <p><strong>Onto/surjective function</strong>: let $f:A \to B$ be a function. $f$ is onto IFF <strong>every element of $B$</strong> has a pre-image. You can translate this into FOL</p>

\[\forall b \in B \quad (\exists a \in A\quad  b=f(a))\]

  <p>as a result, $\mathrm{image}(f)=B$. For example, $A={(0,0),(0,1),(1,0),(1,1)}$, $B={0,1,2}$:</p>

  <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230321111129449.png" alt="image-20230321111129449" style="zoom: 33%;" /></p>

  <p>so every element in $B$ receives <mark>at *least* one 1 pre-image</mark>.</p>

  <ul>
    <li>for example, $f(x)=x^2$ for $x \in \Z$ is <em>not</em> onto since $5 \in \Z$ but there is no integer with $x^2 = 5$</li>
    <li>and observe that $\text{$f$is onto\ }\implies \vert A\vert  \ge \vert B\vert$</li>
    <li>is visually is also the <strong>pigeon hole principle</strong> ($A$ are the pigeons)</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>One-to-one/injection function</strong>: $f$ is one to one IFF no two elements of $A$ has the same image in $B$, i.e. every element in $B$ receives <mark>at *most* 1 pre-image</mark>. Note that this means one-to-one function <em>does not have to be onto functions</em>, i.e. you <em>DON’T</em> need $\mathrm{image}(f)=B$. Into FOL we have:</p>

\[\forall a_1,a_2 \in A\quad a_1 \neq a_2 \implies f(a_1) \neq f(a_2)\]

  <p>or you can re-write this as contrapositive</p>

\[\forall a_1,a_2 \in A\quad f(a_1) = f(a_2) \implies a_1 = a_2\]

  <p>which would be easier to use during proofs.</p>

  <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230323104803922.png" alt="image-20230323104803922" style="zoom: 33%;" /></p>

  <ul>
    <li>for example, $f(x)=x^2$ for $x \in \Z$ is <em>not</em> one-to-one since you can have $4=(+2)^2 = (-2)^2$.</li>
    <li>and observe that $\text{$f$is one-to-one\ }\implies \vert A\vert  \le \vert B\vert$</li>
  </ul>
</blockquote>

<p>This means:</p>

<ul>
  <li><strong>to prove a function is onto</strong>: take an arbitrary element $b \in B$, show it has a pre-image in $A$</li>
  <li><strong>to prove a function is one-to-one</strong>: take two arbitrary elements $a_1, a_2 \in A$, show that (e.g. direct proof) $f(a_1) = f(a_2) \implies a_1 = a_2$</li>
</ul>

<p><em>For Example</em>: let a function $f: \Q \to \Q$, and $f(x)=2x+1$.</p>

<ul>
  <li>prove that $f$ is onto.
    <ul>
      <li>let $b \in \Q$ be an arbitrary element</li>
      <li>then $b=2(x+1) \implies x = (b-1)/2$</li>
      <li>but since $b\in \Q$, therefore $x \in \Q$</li>
      <li>hence there is a pre-image $x\in \Q$ such that $f(x)=b$.</li>
      <li>so $f$ is onto</li>
    </ul>
  </li>
  <li>prove that $f$ is one-to-one</li>
  <li>let $a_1,a_2 \in \Q$ and that $f(a_1)=f(a_2)$</li>
  <li>then this $\implies 2a_1 + 1 = 2a_2 + 1$</li>
  <li>so $\implies a_1= a_2$</li>
  <li>therefore $f$ is one-to-one</li>
</ul>

<blockquote>
  <p><strong>Bijection</strong>: let $f:A \to B$ be a function that is <strong>both onto and one-to-one</strong>, then $f$ is a bijection, i.e. every element in $B$ receives <mark>exactly one pre-image</mark></p>

  <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230323105025411.png" alt="image-20230323105025411" style="zoom: 25%;" /></p>

  <ul>
    <li>and observe that $\text{$f$is a bijection\ }\implies \vert A\vert  = \vert B\vert$</li>
  </ul>
</blockquote>

<p><em>For Example</em>: consider</p>

<ul>
  <li>
    <p>let $f_1 : \Z^+<em>{odd} \to Z^+</em>{even}$ and $f_1(x)=x+1$ is a bijection as it is both onto (each $b$ has a pre-image) and one-to-one (every $b$ has at most one). This implies, intuitively, that $\vert Z^+<em>{odd}\vert  = \vert Z^+</em>{even}\vert$</p>
  </li>
  <li>
    <p>let $f_2 : \Z^+ \to Z^+<em>{even}$ and $f_1(x)=2x$ is also bijection. But this unintuitively implies $\vert Z^+\vert  = \vert Z^+</em>{even}\vert$! <mark>in general for infinite sets, "cardinality" doesn't really exist</mark>.</p>
  </li>
</ul>

<h2 id="composition-of-functions">Composition of Functions</h2>

<p>Consider two functions:</p>

<ul>
  <li>$f: A \to B$ and $g: C \to D$</li>
  <li>let $\mathrm{image}(f) \subseteq C$</li>
</ul>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230323110521659.png" alt="image-20230323110521659" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>Composition</strong>: let $f: A \to B$ and $g: C \to D$, and $\mathrm{image}(f) \subseteq C$. The composition of functions $f$ and $g$ is defined as</p>

\[g\circ f: A \to D,\quad g\circ f (x) = g(f(x))\]

  <p>note that we need $\mathrm{image}(f) \subseteq C$, not necessarily $B \subseteq C$.</p>
</blockquote>

<p><em>For example</em>: let $f:\Z \to \Z$ and $g : \Z \to \Z$</p>

<ul>
  <li>let $f(x) = x^2+1$ and $g(x)=2x-3$. THen $g\circ f(x) = g(x^2+1) = 2(x^2+1)-3=2x^2-1$</li>
</ul>

<blockquote>
  <p><strong>Composition of Functions preserve the property</strong> of $f,g$ if they share the same property. If $f,g$ have different properties, it is more complicated.</p>

  <ul>
    <li>e.g. $f$ is many-to-one and $g$ is one-to-one then $f\circ g$ is many-to-one</li>
  </ul>
</blockquote>

<h2 id="inverse-of-functions">Inverse of Functions</h2>

<blockquote>
  <p><strong>Identity Function</strong>: the identity function denoted $I_A$ is <strong>defined on $A$</strong> to have:</p>

\[I_A: A \to A,\quad I_A(a)=a\]

</blockquote>

<blockquote>
  <p><strong>Inverse Function</strong>: let $f: A\to B$. If there exists a function $g: B \to A$ such that</p>

\[g\circ f = I_A,\quad  f\circ g = I_B\]

  <p>then $g$ is called the inverse of $f$, called $g=f^{-1}$.</p>

  <ul>
    <li>basically you want $f \circ f^{-1} = I_B$. and $f^{-1}\circ f = I_A$.</li>
  </ul>
</blockquote>

<p><em>For Example</em></p>

<ul>
  <li>let $f(x)=3x-2$. What is $f^{-1}$ Essentially we are <strong>finding $x$</strong> from $y=f(x)$, therefore $f^{-1}(x)=(x+2)/3$</li>
</ul>

<p>Note that <strong>inverse $f^{-1}$ might not exist</strong>, i.e. not a function. E.g. $f(x)={(1,a),(2,a)}$, then $f^{-1}={(a,1),(a,2)}$!</p>

<blockquote>
  <p><strong>Theorem</strong>: let $f:A \to B$ be a function. Then $f^{-1}:B\to A$ exists provided $f$ is a <strong>bijection</strong>.</p>

  <ul>
    <li>need to be one-to-one, so that when reversed each input has only one image</li>
    <li>need to be on-to, so that when reversed each input has an image</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Theorem</strong>: let $f:A\to B$ be a bijection. Then $f^{-1}$ is <strong>unique</strong>.</p>
</blockquote>

<p><em>Proof</em>: By contradiction, suppose there are two inverse functions $f^{-1}_1$ and $f_2^{-1}$</p>

<ul>
  <li>then this means there is an element $f^{-1}_1(b) \neq f^{-1}_2(b)$</li>
  <li>but this means $f(f^{-1}_1(b)) \neq f(f^{-1}_2(b))$</li>
  <li>by definition, both $f_1^{-1},f_2^{-1}$ are inverse of $f$, so this cannot be right as we get $b \neq b$</li>
  <li>therefore, this is a contradiction and there can only be one unique inverse of $f$.</li>
</ul>

<h2 id="pigeon-hole-principle">Pigeon Hole Principle</h2>

<p>Given a function $f:A\to B$, recall that <strong>if $\vert A\vert  &gt; \vert B\vert$,</strong> then it <strong>must be some of the elements in $A$ going into the same image</strong></p>

<blockquote>
  <p><strong>PHP</strong>: if $p$ pigeons fly in $h$ holes, and $p &gt; h$, then at least one of the hole contains two or more pigeons.</p>
</blockquote>

<p><em>For example</em>:</p>

<ul>
  <li>
    <p>We know that there are 8M people in NYC. We know that each person can have a maximum number of 300,000 hair strands. Then by PHP this means there are people having the same number of fair strands.</p>
  </li>
  <li>
    <p>Let there be three pairs of socks, B, G, R. How many socks do I need to blindly pick so that I get at least one pair?</p>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230401010148088.png" alt="image-20230401010148088" style="zoom:15%;" /></p>
  </li>
  <li>
    <p>in a group of $n$ people, there will be always two people who <em>have the same number of friends</em>. Assume each person has at least one friend in this group. (Hint: the key is to realize the holes need to be “<em>the number of friends</em>”, and the maximum number of friends you can have is $n-1$)</p>
  </li>
  <li>
    <p>let $A = {1,2,3,4,5,6,7,8}$. If I <em>pick 5 random numbers</em>, will there be at least a pair of number that sums to $9$?</p>

    <p>the pigeons can be any <em>five numbers</em> $n_1,n_2,n_3,n_4,n_5$, the holes can be the pairs/the ways that sums to $9$.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Generalized PHP</strong>: Given $p$ pigoens and $h$ holes. If $p &gt; h$ then at least a hole contains:</p>

\[\lceil \frac{p}{h} \rceil \text{\ pigeons}\]

  <p>(the ceiling of $p/h$).</p>
</blockquote>

<p>This also covers the PHP, because let $p=n$ and $h=n-1$. THen by GPHP we have $\lceil n/(n-1) \rceil = 2$ meaning at least one hole contains 2 pigeons.</p>

<h2 id="infinity-and-countability">Infinity and Countability</h2>

<p>The aim is to discuss the size of a set who is infinite, but we saw in previous sections that if $f:A\to B$ is a bijection, then $\vert A\vert  = \vert B\vert$</p>

<blockquote>
  <p><strong>Countability</strong>: we say that set $S$ is countable $\iff$ there exists a bijection between $S$ and $\N$ (or some subset of $\N$).</p>

  <ul>
    <li>i.e. a set is countable, if you can <em>enumerate</em> all the elements without missing any</li>
  </ul>
</blockquote>

<p>For example:</p>

<ul>
  <li>
    <p>let $S={10,20,30,40,50}$. This is countable, since we can take the image being ${1,2,3,4,5}$</p>
  </li>
  <li>
    <p>let $S={2,4,6,8,…}$. This is countable, as you as enumerate them as is.</p>
  </li>
  <li>
    <p>let $S={\frac{a}{b}\vert a,b\in \Z \land b \neq 0}$. How do you enumerate the values of fractions? George Cantor showed this $\Q$ is countable using <strong>diagonalization</strong>.</p>

    <p>First, we show $\Q+$ is countable</p>

    <table>
      <thead>
        <tr>
          <th> </th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>…</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>1</td>
          <td>1/1</td>
          <td>1/2</td>
          <td>1/3</td>
          <td>…</td>
        </tr>
        <tr>
          <td>2</td>
          <td>2/1</td>
          <td>2/2</td>
          <td>2/3</td>
          <td>…</td>
        </tr>
        <tr>
          <td>3</td>
          <td>3/1</td>
          <td>3/2</td>
          <td>3/3</td>
          <td>…</td>
        </tr>
        <tr>
          <td>…</td>
          <td>…</td>
          <td>…</td>
          <td>…</td>
          <td>…</td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>so I cannot enumerate by row, as I will never come back from a row. same reason for column</li>
      <li>but I can go by a diagonal</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230330101348980.png" alt="image-20230330101348980" style="zoom:33%;" /></p>

    <p>therefore:</p>

\[\{ 1/1, \quad 2/1, \quad 1/2, \quad 3/1,\quad 2/2, \quad1/3, ... \}\]

    <p>so essentially countability can be seen as to <strong>find a strategy to enumerate it</strong> = can find a bijection from $\N$ by just corresponding each element here to a number in $\N$ (you do not need to know what that function is, but it can be enumerated).</p>

    <p>Finally to get $\Q$, we just need to insert each number’s negative in between.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Theorem</strong>: the set of real numbers $\R$ is <em>not</em> countable.</p>
</blockquote>

<blockquote>
  <p><strong>Corollary</strong>: the set of real numbers in $[0,1]$ is also <em>not countable</em>.</p>
</blockquote>

<p><em>Proof</em>: by contradiction using diagonalization.</p>

<ul>
  <li>assume I can list all the real numbers in $[0,1]$, so that ${r_1, r_2, r_3, …}$, e.g.
    <ul>
      <li>$r_1 = 0.4513201…$</li>
      <li>$r_2=0.3351238…$</li>
      <li>$r_3 = 0.1234223…$</li>
      <li>…</li>
    </ul>
  </li>
  <li>we can show that this is <em>missing at least one real number</em>, by
    <ul>
      <li>let $\text{decimal}<em>i(r</em>{new})$ be the $i$-th decimal number of $r_{new}$. E.g. $\text{decimal}<em>3(r</em>{1})=1$ above.</li>
      <li>I can invent a $r_{new}$ to be $\text{decimal}<em>i(r</em>{new}) \neq \text{decimal}<em>i(r</em>{i})$ for every $i$! e.g. $r_{new}=0.544…$</li>
    </ul>
  </li>
  <li>and the key is this $r_{new}\in R$! (therefore this won’t work with proving $\Q$ being uncountable, though we already know it is not)</li>
</ul>

<h1 id="advanced-proofs">Advanced Proofs</h1>

<p>Here we cover more techniques to prove things. We will cover</p>

<ul>
  <li>proof by mathematical induction (PMI)</li>
  <li>proof by smallest counter example (PSCI)</li>
  <li>proof by strong mathematical induction (PSMI)</li>
</ul>

<h2 id="proof-by-induction">Proof by Induction</h2>

<p>Consider proving propositions that look like</p>

\[\forall n\quad p(n),\quad n \in \Z^+\]

<p>The idea is to consider:</p>

<ol>
  <li>
    <p><a href="**prove** that the following is true">base case</a> show $p(a)$ is true</p>
  </li>
  <li>
    <p>if we can show $p(k) \implies p(k+1)$.</p>
  </li>
  <li>
    <p>then $\forall k \ge a \implies p(k)$. It is like a “chain reaction”.</p>

    <p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230330102531524.png" alt="image-20230330102531524" style="zoom: 25%;" /></p>
  </li>
  <li>
    <p>hence we have shown that $\forall n \in \Z^+ \quad n\ge a \implies p(n)$</p>
  </li>
</ol>

<blockquote>
  <p><strong>Proof Template for Induction</strong></p>

  <ol>
    <li>
      <p>write the proposition clearly</p>

\[\forall n \quad p(n)\]
    </li>
    <li>

\[p(a),\quad \text{a is the smallest case of n}\]
    </li>
    <li></li>
    <li>
      <p>[inductive step] <strong>prove</strong> that <strong>“all the next steps/domino” are true</strong> given $p(k)$</p>

\[p(k) \implies p(k+1)\]

      <p>here is when you can <em>technically</em> use all the different proof techniques (e.g. by contrapositive). But often we just do direct proofs.</p>
    </li>
    <li>therefore, $p(k)$ is true $\forall k \ge a$</li>
  </ol>
</blockquote>

<p><em>For Example</em>: prove that $\forall n \in \Z^+$</p>

\[p(n): 1+2+3+ \dots + n = \frac{n(n+1)}{2}\]

<p>Proof by induction: the proposition is already written in the “standard” format</p>

<ol>
  <li>
    <p>base case is $p(1)$.  This is true because</p>

\[p(1): 1 = \frac{1\cdot 2}{2}\]

    <p>is true</p>
  </li>
  <li>
    <p>inductive hypothesis that for $k \ge 1$, we assume $p(k)$ is true</p>

\[p(k):1+2+3+\cdots + k = \frac{k(k+1)}{2}\]
  </li>
  <li>
    <p>inductive step so $p(k+1):1+2+3+\cdots + k + k+1= \frac{k+2(k+2)}{2}$ and it requires you to <strong>prove that $p(k+1)$ is true by</strong></p>

\[p(k) \implies p(k+1)\]

    <p>we can consider a <strong>direct proof</strong></p>

    <ol>
      <li>
        <p>let $p(k)$. This means this is true</p>

\[1+2+3+\cdots + k = \frac{k(k+1)}{2}\]
      </li>
      <li>
        <p>then this means</p>

\[1+2+3+\cdots + k + k+1 = \frac{k(k+1)}{2} + k+1\]

        <p>this step can be interpreted in two ways (both valid)</p>

        <ul>
          <li>smudging the entire $p(k)$ to look like $p(k+1)$</li>
          <li>just want to see what does RHS look like by $1+2+3+\cdots + k + k+1 = p(k)+k+1$</li>
        </ul>
      </li>
      <li>
        <p>rewriting the RHS we get</p>

\[1+2+3+\cdots + k + k+1 = \frac{(k+2)(k+1)}{2}\]

        <p>which is exactly $p(k+1)$</p>
      </li>
      <li>
        <p>hence, we have shown that $p(k+1)$ is true assuming $p(k)$, i.e. $p(k)\implies p(k+1)$</p>
      </li>
    </ol>
  </li>
</ol>

<p><em>For example</em>: prove that $\forall n \in \Z, n \ge 1$</p>

<p>we want to show that</p>

\[1+3+5+\dots+(2n-1) = n^2\]

<p>Visually, this looks like</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230330105520061.png" alt="image-20230330105520061" style="zoom:33%;" /></p>

<p><em>Proof by induction</em>:</p>

<ol>
  <li>
    <p>base case is $p(1)$. This is easily true since $1=1^2$</p>
  </li>
  <li>
    <p>inductive hypothesis: assume $p(k)$ is true</p>

\[1+3+5+\dots+(2k-1) = k^2\]
  </li>
  <li>
    <p>inductive step: want to show that $p(k+1)$ is true given $p(k)$, so that considering direct proof</p>

    <ol>
      <li>
        <p>let $p(k)$ be true, hence</p>

\[1+3+5+\dots+(2k-1) = k^2\]
      </li>
      <li>
        <p>then moving to $p(k+1)$ we get</p>

\[1+3+5+\dots+(2k-1)+(2k+1) = k^2 + 2k+1\]
      </li>
      <li>
        <p>the RHS can be reformatted to be</p>

\[1+3+5+\dots+(2k-1)+(2k+1) = (k+1)^2\]
      </li>
      <li>
        <p>this is basically $p(k+1)$</p>
      </li>
    </ol>
  </li>
</ol>

<blockquote>
  <p><strong>Well-ordering principle</strong>: every set of non-negative integers <mark>has a starting element (i.e. a beginning)</mark></p>

  <ul>
    <li>The induction process is possible thanks to the principle of <strong>well-ordering principle</strong>.</li>
    <li>this does <mark>not necessarily have to be the smallest element</mark>. e.g. prove in the domain of only negative numbers you can start with the largest element, and prove $p(k)\implies p(k-1)$</li>
  </ul>
</blockquote>

<p><em>For example</em>:</p>

<ul>
  <li>is well-ordering: $\N={1,2,3,…}$, $S={4,5,6,…}$</li>
  <li><em>not</em> well-ordering: $\R$, $\Q$, or even $\R^+$.</li>
</ul>

<h2 id="proof-by-smallest-counter-example">Proof by Smallest Counter Example</h2>

<p>To compare this with what you</p>

<p>PMI: essentially proves the purple ones</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230330120500591.png" alt="image-20230330120500591" style="zoom: 7%;" /></p>

<p>PSCI: aims to prove the red one being false, hence there cannot be a CE:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230330120615053.png" alt="image-20230330120615053" style="zoom:9%;" /></p>

<blockquote>
  <p><strong>Proof Template for PSCI</strong></p>

  <ol>
    <li>
      <p>write in $\forall n \quad p(n)$</p>
    </li>
    <li>
      <p><strong>prove</strong> base case</p>
    </li>
    <li>
      <p><strong>assume</strong> that there exist exceptions/counter examples. <mark>Let $x$ be the smallest so $p(x)$ is False</mark>. Notice that this <strong>also means $p(x-1)$ is true</strong></p>
    </li>
    <li>
      <p><strong>prove</strong> that this is a <strong>contradiction, hence $p(x)$ cannot exist</strong>. i.e. this cannot hold.</p>

\[p(x)\text{ is False } \land\,\,  p(x-1)\text{ is True }\]
    </li>
    <li>
      <p>if there is no SCE, then there is <mark>no CE at all given the well-ordering principle</mark>. Hence the proposition is true.</p>
    </li>
  </ol>
</blockquote>

<p><em>Proof by SCE</em>: show that</p>

\[p(n): 1+2+3+ \dots + n = \frac{n(n+1)}{2}\]

<ol>
  <li>
    <p>base case $p(1)$ is true since $1=1\times 2/2$</p>
  </li>
  <li>
    <p>by contradiction, <strong>assume that $\exists x &gt; 1$ such that $p(x)$ is False, for $x$ is the smallest counter example</strong></p>
  </li>
  <li>
    <p>then we can show that this is a contradiction that this cannot happen. So that given</p>

\[p(x):1+2+3+ \dots + x \neq \frac{x(x+1)}{2}\]

    <p>but that</p>

\[p(x-1):1+2+3+ \dots + x-1 = \frac{x(x-1)}{2}\]

    <p>cannot hold.</p>
  </li>
  <li>
    <p>therefore, there is no SCE, meaning essentially</p>

\[\neg (\exist x &gt; 1 \quad p(x) \text{ is False})\]

    <p>so that for $n \in \Z^+$</p>

\[\forall n \quad p(n)\]
  </li>
</ol>

<p><em>Proof by SCE:</em> prove by SCE that</p>

\[\forall n \in \Z, \quad n&gt;0 \to 4|(5^n-1)\]

<ol>
  <li>
    <p>base case $n=1$ works easily</p>
  </li>
  <li>
    <p>by contradiction, assume $x$ the S.C.E such that $p(x)$ is false. i.e. $4$ does not divide $5^x-1$</p>

\[4 \nmid (5^x-1)\]
  </li>
  <li>
    <p>now we want to show that this is a contradiction: $p(x)$ is false and $p(x-1)$ is true cannot hold</p>

    <ol>
      <li>
        <p>since $p(x-1)$ is true, this means $4\vert (5^{x-1}-1)$</p>
      </li>
      <li>
        <p>this means $5^{x-1}-1=4\times q$, where $q \in \Z$. We want to move towards contradiction that $4 \nmid (5^x-1)$</p>
      </li>
      <li>
        <p>multiply by 5, we get</p>

\[5^x-4-1 = 4\times (5 \times 1)\]

        <p>hence</p>

\[5^x-1 = 4 \times( 5 \times q + 1)\]

        <p>meaning that $4 \vert  (5^x-1)$</p>
      </li>
    </ol>
  </li>
  <li>
    <p>therefore, there is no such $x$ such that $p(x)$ is false. Hence the proposition is true.</p>
  </li>
</ol>

<p><em>Proposition</em>: Every Integer is either odd or even.</p>

<p>Note that now we are considering $\forall n \in\Z$, meaning there is <em>technically</em> not a “well-ordering set”. But there can be a trick:</p>

<ol>
  <li>
    <p><strong>first prove by SCE</strong> that this holds when $\forall n \in \Z^+$</p>
  </li>
  <li>
    <p><strong>extend</strong> your above proof to the negative domain, since</p>

\[n \in \Z,n &lt; 0 \implies n = -m, m\in \Z^+\]

    <p>basically flipping the sign</p>
  </li>
</ol>

<h2 id="proof-by-strong-induction">Proof by Strong Induction</h2>

<p>The difference with proof by induction is:</p>

<ul>
  <li>could have many base cases</li>
  <li>making stronger assumptions that all cases from the base cases until $p(k)$ are true. Prove $p(k+1)$</li>
</ul>

<p>Graphically, it looks like:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th style="text-align: center">Visual</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>normal Induction</td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230404164715165.png" alt="image-20230404164715165" style="zoom:7%;" /></td>
    </tr>
    <tr>
      <td>strong induction</td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230404164747333.png" alt="image-20230404164747333" style="zoom:15%;" /></td>
    </tr>
  </tbody>
</table>

<p>and finally this also means base case for strong induction has to look like:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230404164821701.png" alt="image-20230404164821701" style="zoom:10%;" /></p>

<blockquote>
  <p>Therefore, this is useful when the result <strong>for $n = k+1$ depends on the result for some smaller value of $n$ (e.g. $n=k-4$), but it’s not the immediately previous value $k$.</strong></p>
</blockquote>

<blockquote>
  <p><strong>Proof Template for PSI</strong>: consider proving some $\forall n \quad p(n)$</p>

  <ol>
    <li>
      <p><strong>base case</strong>: prove the base cases (you will need to experiment to find out how many you need)</p>

\[p(a),p(a+1),p(a+2),...,p(a+z)\]
    </li>
    <li>
      <p><strong>inductive hypothesis</strong>: assume $p(k)$ hold for <mark>all numbers</mark> less than or equal to an arbitrary $k$, i.e. all the below are true</p>

\[p(a) \land p(a+1)\land \dots p(k)\]

      <p>(i.e. the purple window above)</p>
    </li>
    <li>
      <p><strong>inductive step</strong>: now it becomes</p>

\[p(a) \land p(a+1)\land \dots p(k) \implies p(k+1)\]

      <p>essentially to prove $p(k+1)$ you have a <strong>range of choices/assumptions to choose from</strong>. (see example below)</p>

      <ul>
        <li>The difference is actually only superficial compared to normal induction proofs, and the two proof techniques are equivalent.</li>
        <li>you can see this more in the example below</li>
      </ul>

      <p><mark>and you want to make sure $p(a+z+1)$ can be proven using this inductive step</mark> (where $p(a+z)$ you already show to be true in step 1)</p>

      <ul>
        <li>this usually depends on, say $p(k+1)$ required $p(k-3)$ to be true.</li>
        <li>so this means you need $p(k), p(k-1),p(k-2),p(k-3)$ to be true = <strong>need 4 base cases</strong> in step 1 for arbitrary $k$ to hold</li>
      </ul>
    </li>
    <li>
      <p><strong>conclusion</strong>: repeatedly apply the above proof to increase the purple window until all $\forall n$ are covered. (same as normal induction)</p>
    </li>
  </ol>
</blockquote>

<p><em>Example:</em> Stamping Problem. Prove that any amount of postage $\ge 2$ cent can be made with 2 cents or 3 cents stamp.</p>

<p>Let’s first attempt using simple induction.</p>

<ol>
  <li><strong>base case:</strong> I can make $p(2)$ with one stamp of 2 cent</li>
  <li><strong>inductive hypothesis:</strong> it is possible to make a postage worthy of $k$ cent using 2 or 3 cents stamp</li>
  <li><strong>inductive step:</strong> prove $p(k+1)$, show that you can make $k+1$ cent
    <ul>
      <li>case 1: $p(k)$ has only 2 cents stamp. Take a 2 away and replace by 3 cents.</li>
      <li>case 2: $p(k)$ has only 3 cents stamp. Take a 3 away and replace by two 2 cents.</li>
      <li>case 3: $p(k)$ has both 2 cents and 3 stamps. Take a 2 away and replace by 3, or take a 3 and replace by two 2 cents.</li>
    </ul>
  </li>
  <li>done.</li>
</ol>

<p>Let’s see how strong induction would prove this:</p>

<ol>
  <li>
    <p><strong>base case</strong>: we will see how many cases we need. First prove (skipped) that $p(2),p(3),p(4),p(5)$ are true</p>
  </li>
  <li>
    <p><strong>inductive hypothesis</strong>: by assumption then let the following until $k$ be true</p>

\[p(1) \land ... p(6) \land p(7)\dots p(k)\]
  </li>
  <li>
    <p>inductive step: to prove $p(k+1)$ all I need is to show</p>

\[p(1) \land ... p(6) \land p(7)\dots p(k) \implies p(k+1)\]

    <p>realize that</p>

\[p(k+1) = p(k-2+3)=p(k-2)+3\]

    <p>i.e. <mark>adding</mark> a 3 cent in the solution for $p(k-2)$. Since $p(k-2)$ is true by inductive hypothesis, we are done.</p>

    <ul>
      <li>This also means we need at least <mark>3 base cases</mark>, that $p(2),p(3),p(4)$ is already proven (it is)</li>
      <li>since we need the <em>left boundary of our purple window at $2 \le k-2$</em>, so $k\ge 4$. Therefore we needed base case up to $p(k=4)$</li>
    </ul>
  </li>
  <li>
    <p>therefore $p(k+1)$ is true.</p>
  </li>
</ol>

<blockquote>
  <p>Notice that we <mark>needed to directly prove (a minimum of) three base cases</mark>, since we needed to <mark>reach back three integers in our inductive step</mark>. It’s <strong>not always obvious how many base cases are needed until you work out the details</strong> of your inductive step.</p>
</blockquote>

<h1 id="number-theory">Number Theory</h1>

<p>Here we will discuss topics including</p>

<ul>
  <li>divisibility</li>
  <li>GCD/GCF (greatest common divisor/factor)</li>
  <li>fundemental theorem of arithmetic</li>
  <li>factoring</li>
  <li>modular arithmetic (clock arithmetic)</li>
  <li>introduction to cryptography</li>
</ul>

<h2 id="divisibility">Divisibility</h2>

<p>In the 1880, Peano’s axioms of natural numbers (converted into natural language):</p>

<ol>
  <li>
    <p>there is a starting point of numbers $x \in \N$, and it starts at 1</p>
  </li>
  <li>
    <p>let $s$ be a successor function. Every natural number has a successor:</p>

\[\forall n \in \N, \quad s(n) \in \N\]
  </li>
  <li>
    <p>and 1 is not a successor of any natural number, so that $1 \neq s(n)$</p>
  </li>
  <li>
    <p>let $s(n)$ is a one-to-one function., so that each number has a different successor.</p>
  </li>
  <li>
    <p>there is no other set of natural numbers.</p>
  </li>
</ol>

<p>These requirements enabled a “massive production of numbers”, and they have some interesting consequences</p>

<blockquote>
  <p><strong>Divisibility Theorem</strong>: let $a,b \in \Z$, and $b &gt; 0$, then there exist a <mark>unique</mark> pair $(q,r)$ such that</p>

\[a = bq +r,\quad 0 \le r &lt; b\]

  <p><mark>note that $r,b$ is positive.</mark></p>
</blockquote>

<p>for example:</p>

<ul>
  <li>$25 = 10\times 2 +5$, where $25=a$, $b=10$, etc.</li>
  <li>$-25 = 10 \times -3 +5$. Note that both $b$ and $r$ are positive.</li>
</ul>

<p>for example: prove that every integer is either odd or even</p>

<ol>
  <li>
    <p>by the divisibility theorem, there is a unique pair $(q,r)$, such that</p>

\[n = 2q + r,\quad 0 \le r &lt;2\]
  </li>
  <li>
    <p>i.e. if I divide by 2, the remainder can only be zero or one! then,</p>

    <ul>
      <li>if $r=0$, this means $n=2q$ hence $n$ is even</li>
      <li>if $r=1$, this means $n=2q+1$ hence $n$ is odd.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p><strong>Div and Mod Operators</strong>: let $a,b \in \Z, b &gt; 0$. We know that $a = bq +r,\quad 0 \le r &lt; b$ .Then</p>

\[\exists (q,r) \in \Z \times (\Z^+ \cup \{0\})\]

  <p>and then we define the operators</p>

  <ul>
    <li>$a \mathrm{\ div\ }b = q$</li>
    <li>$a \mathrm{\ mod\ }b = r$</li>
  </ul>
</blockquote>

<h3 id="least-common-multiple-and-greatest-common-divisor">Least Common Multiple and Greatest Common Divisor</h3>

<blockquote>
  <p><strong>Proposition</strong>: let $a,b \in \Z$,. Let let $d\in \Z^+$ means</p>

\[d|a \land d |b \implies d|(a+b) \land d|(a-b)\]

  <p>i.e. $d$ also <mark>divides any linear combination of $a,b$</mark>. This is very useful as it means, for example:</p>

\[gcd(a,b)≡gcd(−a,b)≡gcd(a,−b)≡gcd(−a,−b)≡gcd(∣a∣,∣b∣)\]

</blockquote>

<p>The above comes useful in later proofs. This proposition is itself relatively easy to prove, since:</p>

<ul>
  <li>to prove $d\vert (a+b)$ you can rewrite $a+b=dq_1 + dq_2=d(q_1+q_2)$ since $d\vert a \land d\vert b$</li>
  <li>to prove $d\vert (a-b)$ you can rewrite $a-b=dq_1 - dq_2=d(q_1-q_2)$ since $d\vert a \land d\vert b$</li>
</ul>

<blockquote>
  <p><strong>Greatest Common Factor/Divisor</strong>: let $a,b \in \Z$. We call $d$ the greatest common factor IFF</p>

  <ol>
    <li>$d \vert  a \land d\vert b$ (i.e. is a divisor)</li>
    <li>$\forall e \in \Z, \quad e\vert a \land e \vert b \implies e \le d$ (i.e. is the biggest)</li>
  </ol>
</blockquote>

<p><em>For example</em>: there are 28 roses, 14 daisies. What is the greatest number of identical bouquet of flowers can you make with no flowers left?</p>

<ul>
  <li>let there be $k$ bouquet of flowers. Realize this $k$ must be a divisor such that $k\vert 28$ and $k\vert 14$</li>
  <li>to have the most $k$, you basically need to find the greatest common divisor. In this case you can find by inspection, giving $14$.</li>
</ul>

<blockquote>
  <p><strong>Properties of GCD</strong></p>

  <ul>
    <li>$\mathrm{GCD}(a,0)=a$, since divisor for $0$ includes all numbers</li>
    <li>$\mathrm{GCD}(0,0)$ is undefined</li>
    <li>$\mathrm{GCD}(a,b)=\mathrm{GCD}(b,a)$</li>
    <li>$\mathrm{GCD}(a,b)=\mathrm{GCD}(\vert a\vert ,\vert b\vert )$</li>
    <li>$\mathrm{GCD}(a,b,c)=\mathrm{GCD}(a,\mathrm{GCD}(b,c))$.</li>
  </ul>
</blockquote>

<p><strong>But how do we find GCD</strong>?</p>

<p>Naive GCD: without loss of generalization, let $a \ge b$. Then</p>

<ol>
  <li>for each $d=1$ to $b$ (in case of co-prime, $\mathrm{GCD}(a,b)=1$ )
    <ol>
      <li>if $d\vert a\land d\vert b$ then $g=d$</li>
    </ol>
  </li>
  <li>return $g$</li>
</ol>

<p>Before we discuss a more efficient algorithm: <mark>observe: let $a,b\in \Z^+$. Realize that $\mathrm{GCD}(a,b)=\mathrm{GCD}(b,a \mod b)$.</mark></p>

<ul>
  <li>$\mathrm{GCD}(75, 63)=\mathrm{GCD}(63, 12)$
    <ul>
      <li>note that if you started with $\mathrm{GCD}(63, 75)\to\mathrm{GCD}(75, 63)$ since $63 \mod 75=63$ gets automatically swapped.</li>
    </ul>
  </li>
  <li>but you can continue: $\mathrm{GCD}(63, 12)=\mathrm{GCD}(12, 3)=\mathrm{GCD}(3, 0)$</li>
  <li>then you are done. GCD is 3!</li>
</ul>

<p>Graphically:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230406102803913.png" alt="image-20230406102803913" style="zoom: 33%;" /></p>

<p>Essentially at each iteration you managed to cut the “search space”!</p>

<blockquote>
  <p><strong>Euclid GCD Algorithm</strong>: let $a,b\in \Z^+$. Realize that $\mathrm{GCD}(a,b)=\mathrm{GCD}(b,a \mod b)$ as shown above (proof later). Then the algorithm becomes</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">recursive_gcd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="n">mod</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">b</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">recursive_gcd</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
</code></pre></div>  </div>

  <p>but you can of course write this into an iterative version</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">iterative_gcd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="n">mod</span> <span class="n">b</span>
    <span class="k">while</span> <span class="n">r</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">r</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="n">mod</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div>  </div>
</blockquote>

<p><em>Proof of Euclid’s divisibility</em>. The key is to utilize divisiblity that, consider $\mathrm{GCD}(a,b)$ then</p>

<ul>
  <li>$a=bq +r$, with $0 \le r &lt;b$ by divisibility</li>
  <li>hence $r=a-bq$ i.e. by definition, $r$ is basically $a \mod b$</li>
</ul>

<p>Therefore, the proof looks like</p>

<ol>
  <li>let $d$ be a divisor of $a$ and $b$. Then $r=a-bq$ means that
    <ol>
      <li>$d\vert a \implies a=d\cdot q_1$, and $d\vert b \implies b = d\cdot q_2$</li>
      <li>hence $r=dq_1 - qdq_2 = d(q_1 - qq_2)\implies d\vert r$</li>
      <li>so <strong>any divisor of $a,b$ is also a divisor of $r$</strong></li>
    </ol>
  </li>
  <li>let $c$ be a divisor of $b$ and $r$. Then $a=bq + r$ means that
    <ol>
      <li>similarly you can show that $c\vert a$</li>
      <li>so <strong>any divisor of $b,r$ is also a divisor of $a$</strong></li>
    </ol>
  </li>
  <li>form the above two conclusions, <mark>$a,b$ and $b,r$ have the same common divisors</mark>. (e.g. let $k$ be a common divsor for $a,b$ but not for $b,r$. Then this means $k \nmid r$, which cannot be due to our proof above!)</li>
  <li>Hence $\mathrm{GCD}(a,b)$ ad $\mathrm{GCD}(b,r)$ must be the same/share the same GCD!</li>
</ol>

<h2 id="prime-numbers-and-factorization">Prime Numbers and Factorization</h2>

<p>The idea is that</p>

\[\N = \{1,2,3,\dots\} = \{1\}\cup \text{primes}\cup \text{composites}\]

<p>and that interestingly:</p>

<ul>
  <li>primes are more “dense” when small</li>
  <li>there is an infinity amount of prime numbers</li>
</ul>

<blockquote>
  <p><strong>Fundemental theorem of arithmetic</strong>: let $n \in \Z \land n \ge2$. There is a <strong>unique</strong> factorization of $n$ into a product of primes. i.e. $n$ is either a prime of a product of primes (composites).</p>
</blockquote>

<p>To prove the above, you will need to show that</p>

<ol>
  <li>there <em>exists</em> a prime factoriazation for any $n \ge 2$</li>
  <li>this factorization is <em>unique</em> (this can get quite complicated to prove, see Gauss’s proof)</li>
</ol>

<p><em>Proof</em>: there exists a prime factorization. Proof by <strong>strong induction</strong></p>

<ol>
  <li>base case: let us use $p(2)=2,p(3)=3,p(4)=2\times 2$ have a prime factorization.</li>
  <li>inductive step: every number between $2$ and $k\ge 5$ can be factored into primes</li>
  <li>prove $k+1$: realize either that $k+1$ is a prime or a composite.
    <ul>
      <li>if $k+1$ is a prime, then $p(k+1)=k+1$, done.</li>
      <li>if $k+1$ is a composite
        <ul>
          <li>then $k+1 = a\times b$ for some factors $a,b&lt;k+1$. (technically this can be even strong that $a,b&lt; (k+1)//2$, but it is not necessary)</li>
          <li>but realize that we have assumed $p(a)$ and $p(b)$ are both having a prime factorization</li>
          <li>hence $k+1$ also has a prime factorization</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>therefore, $k+1$ also has a prime factorization.</li>
</ol>

<blockquote>
  <p><strong>Theorem</strong>: there is an infinite number of prime numbers.</p>
</blockquote>

<p><em>Kumer’s proof</em>: proof by <strong>contradiction</strong>. Suppose there is a <em>finite</em> number of primes:</p>

\[p_1 &lt; p_2 &lt; \dots &lt; p_r\]

<p>where $p_r &lt; \infty$ is finite. Then</p>

<ol>
  <li>$x=p_1\times p_2 \times \dots \times p_r$ is a composite included <strong>all possible primes</strong></li>
  <li>consider $x+1=p_1\times p_2 \times \dots \times p_r+1$
    <ul>
      <li>if $x+1$ is prime
        <ul>
          <li>we already get a <em>contradiction</em> $x+1 &gt; p_r$</li>
        </ul>
      </li>
      <li>if $x+1$ is a composite
        <ul>
          <li>then since $x$ is a composite including all primes, $x$ and $x+1$ <strong>must share at least a prime</strong>, let’s call this $p_i$</li>
          <li>then $p_i\vert x$ and $p_i \vert  (x+1)$, so that $x=p_iq_1$ and $x+1 = p_i q_2$</li>
          <li>$x+1-x=p_i q_2 -p_iq_1$ hence $1 = p_i(q_2-q_1)$</li>
          <li>but $p_i\vert 1$ cannot be true as the smallest prime number is $2$!</li>
          <li>this is a <em>contradiction</em></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Hence, by contradiction there is an infinite number of primes.</li>
</ol>

<blockquote>
  <p><strong>Sieve of Fratosthenes</strong>: what is a good way to find all primes until $n$? E.g. let us find all primes less than $n=20$</p>

  <ol>
    <li>write down every integer $2,3,\dots,20$</li>
    <li>for $i$ each of the above integer that is <em>not crossed out</em>
      <ol>
        <li>cross out all multiples of $i$</li>
        <li>stop at $\lfloor \sqrt{n} \rfloor$, because the biggest <em>possible</em> factor is $\lfloor \sqrt{n} \rfloor$</li>
      </ol>
    </li>
    <li>whatever not crossed out are the primes</li>
  </ol>
</blockquote>

<p>Graphically, if you pick $n=41$:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230406112112752.png" alt="image-20230406112112752" style="zoom:25%;" /></p>

<p>Finally, <strong>prime factorization</strong> can be used as a good way to find GCD by hand:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230406112659314.png" alt="image-20230406112659314" style="zoom:43%;" /></p>

<p>so basically all you need to do is to pick the minimum power. This would be useful when you need to find GCD by hand/both $a,b$ are relatively small. Otherwise using the Euclid algorithm should be much faster.</p>

<blockquote>
  <p>Note: what is the relationship between $a \mod b$ and $a-b$?</p>

  <ul>
    <li>since $a=qb + r$, this means $a \mod b = a - b -b -b \dots -b$ with $q$ number of $b$s</li>
    <li>this also means if $a/2 &lt; b$, then $q=1$, hence $a \mod b = a - b$</li>
  </ul>
</blockquote>

<h2 id="modular-arithmetic">Modular Arithmetic</h2>

<p>In many applications such as clock, month, cryptography in $\Z_n = {0,1,2,\dots, n-1}$, we need a “new” type of arithmetic to say something like $n+3 \to 2$  (e.g. 14pm is the same as 2pm)</p>

<ul>
  <li>
    <p>this should remind you of the congruence modulo $n$</p>
  </li>
  <li>
    <p>for example, clock would have $\Z_{12}={0,1,2,\dots, 11}$ and has 12 equivalence classes</p>
    <ul>
      <li>e.g. $\dots, -24,-12,0,12,24,\dots$ (given any number outside of the range, either plus 12 or minus12 until you get back into $\Z_{12}$)</li>
      <li>e.g. $\dots, -23,-11,1,13,25,\dots$</li>
    </ul>
  </li>
</ul>

<p>Then defining the “arithmetics”:</p>

<ul>
  <li>
    <p><strong>addition</strong> in $\Z_{n}$: let $a,b\in \Z_{n}$, then</p>

\[a \oplus b \equiv (a+b) \bmod {n}\]

    <p>e.g. let $n=10$, then $12\oplus 12 = 24 \bmod 10 = 4$.</p>
  </li>
  <li>
    <p><strong>multiplication</strong>: same thing, let $a,b\in \Z_{n}$,</p>

\[a \otimes b \equiv (a\times b ) \bmod n\]
  </li>
  <li>
    <p><strong>subtraction</strong>: now the problem is with $a,b\in \Z_{n}$, you can get negative numbers in the sense that:</p>

    <ul>
      <li>if $a - b &gt; 0$, then $a \ominus b = (a-b)\bmod n$</li>
      <li>if $a - b &lt; 0$, then $a \ominus b = (a-b)+n$ (i.e. add as many $n$ until you get back into $\Z_{n}$, in this case since $a,b\in \Z_n$, add once is enough)
        <ul>
          <li>e.g. let $n=10$, then $1-4 = -3 + 10 = 7$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>division</strong>: same setup, but recall that</p>

    <ul>
      <li>
        <p>in normal arithmetic we have $a / b = a \times b^{-1}$, where the reciprocal $b^{-1} = 1/ b$.</p>
      </li>
      <li>
        <p>here, we want to consider</p>

\[a \oslash b = a \otimes b^{-1}\]

        <p>and that by definition, the reciprocal <strong>has to satisfy $b^{-1}\otimes b = 1$</strong>. <mark>Any element in $\Z_n$ that has a reciprocal is called invertible in $\Z_n$)</mark>. Therefore, this division only <mark>exists when $b$ is invertible</mark>.</p>

        <ul>
          <li>for example, for $\Z_{10}$, only the following ${1,3,7,9}$ has inverse.</li>
          <li>you will see how this is related to coprimes.</li>
        </ul>
      </li>
    </ul>

    <p>How do we find the inverse in this new arithmetic?</p>

    <ul>
      <li>e.g. let $n=10$. Then $2 \oslash 7 = 2 \otimes 7^{-1} = 2 \otimes 3 = 6$, because $7^{-1}\times 3 = 21 \bmod 10 = 1$ is invertible!</li>
    </ul>
  </li>
</ul>

<p>Is there a systematic/mathematical way to find the inverse?</p>

<blockquote>
  <p><strong>Coprime</strong>: let $a,b\in Z^+$. We call $a,b$ being coprimes IFF $\gcd(a,b)=1$. We denote this with $a \perp b$.</p>

  <ul>
    <li>this is a generic definition for all $\Z^+$</li>
    <li>in the case of modular arithmetic, the only <strong>invertible</strong> elements in $\Z_n$ are the <mark>coprimes with $n$</mark></li>
  </ul>
</blockquote>

<p>Then, to find the inverse you can use:</p>

<blockquote>
  <p><strong>Extended GCD Algorithm</strong>: let $a,b,d \in \Z^+$. It turns out that if</p>

\[d|a \land d |b \land d= ax+by,\quad \text{for some $x,y\in \Z$}\]

  <p>then</p>

\[\gcd(a,b) = d\]

  <p>in other words, you can <mark>always write $\gcd(a,b)$ as $\gcd(a,b)=ax+by$!</mark></p>
</blockquote>

<p>How can this be true? Proof:</p>

<ul>
  <li>for any common divisor $d\vert a \land d \vert b$, so $d \le gcd(a,b)$ obviously</li>
  <li>but I also said $d = ax+by$. So any common divisor of $a,b$ must divide $d$. This means $gcd(a,b)$ divides $d$, hence $d \ge gcd(a,b)$
    <ul>
      <li>i.e. $d$ = any common divisor times something positive.</li>
    </ul>
  </li>
  <li>since $gcd(a,b) \le d$ and $\gcd(a,b) \ge d$, hence $gcd(a,b)=d$</li>
</ul>

<p><em>For example:</em> considering finding $\gcd(41,43)$, and show $\gcd(41,43)=41x+43y$</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230411233913776.png" alt="image-20230411233913776" style="zoom: 20%;" /></p>

<p>we want to show $\gcd(a,b)=ax+by$.</p>

<ul>
  <li>
    <p>since $\gcd(41,43)=1$ in the last step</p>
  </li>
  <li>
    <p>If I take the second and third step, we have $43= 41\times 1 + 2 \implies 2 = 43 - 41 \times 1$, and $41=2\times 20 + 1 \implies 1 = 41 - 2 \times 20$.</p>
  </li>
  <li>
    <p>hence I just substitute them in $1 = 41 - (43 - 41)\times 20 = 21 \times 41 - 43 \times 20$, hence</p>

\[\gcd(43,41) = 1 = 43 \times (-20) + 41 \times (20) = ax + by\]
  </li>
</ul>

<blockquote>
  <p><strong>Modulus Division</strong>: this is a consequence of extended GCD + all invertible elements in $\Z_n$ are coprimes. Therefore, take any invertible element in $\Z_n$ we have:</p>

\[\gcd(a,n)=1 = ax + ny\]

  <p>therefore, notice that in modular arithmetic $ny$ goes away, hence:</p>

\[a\otimes x = 1\implies x = a^{-1}\]

  <p>finally, you want to <strong>be careful that $x \in \Z_n$ by mapping it back</strong>.</p>
</blockquote>

<p>For example,</p>

<ul>
  <li>
    <p>what is $41^{-1}$ for $\Z_{43}$? We know that</p>

\[\gcd(43,41) = 1 = 43 \times (-20) + 41 \times (20)\]

    <p>therefore, $41^{-1}=20$.</p>
  </li>
  <li>
    <p>what is $3^{-1}$ for $\Z_{40}$? Given that</p>

\[\gcd(3, 40) = 1 =  3 \times (-13) + 40 \times (1)\]

    <p>therefore, $3^{-1}=13 + 40=27$</p>
  </li>
</ul>

<h2 id="public-key-cryptography">Public Key Cryptography</h2>

<p>Here we discuss the basics of RSA</p>

<ol>
  <li>
    <p>Bob will pick two large  prime numbers $p,q$, such that $n=p \times q$</p>
  </li>
  <li>
    <p>Bob chooses an $e$ that is coprime with $(p-1)$ and $(q-1)$ so that</p>

\[\gcd(e, (p-1)(q-1)) =1\]

    <p>and also knows $d=e^{-1}$ is invertible (and easy to find knowing $p,q$). This is the <strong>secret key.</strong></p>
  </li>
  <li>
    <p>the public key is invertible, $n,e$ is sent to Alice (and everybody can see the two). This is <strong>public key</strong>.</p>
  </li>
  <li>
    <p>Alice then prepares the encrypted message and encrypt it with $e$ to get $E(m)=m^e \bmod n$</p>
  </li>
  <li>
    <p>Bob then inverts this by, recall that $e\otimes d=1$, hence</p>

\[E^{-1}(E(m)) = (m^e)^d \bmod n = m\]

    <p>decodes it back.</p>
  </li>
</ol>

<blockquote>
  <p>So the key idea is you need $d$ to decode, but finding $d=e^{-1}$ will take years to compute if you don’t know it (or $p,q$) in advance.</p>
</blockquote>

<h1 id="counting">Counting</h1>

<p>Here we will talk about</p>

<ol>
  <li>counting lists: arrangements, permutations, anagrams</li>
  <li>counting sets: combination, pascal triangle, binomial coefficients</li>
</ol>

<h2 id="counting-lists">Counting Lists</h2>

<blockquote>
  <p><strong>Multiplication Theorem</strong>: the number of lists of length $k$ where elements can be chosen from a set of $n$ possible elements is</p>

\[\begin{cases}
n^k, &amp; \text{if repetitions are allowed}\\
(n)_k \equiv n!/(n-k)!, &amp; \text{if repetitions not allowed}
\end{cases}\]

  <p>where $(n)_k$ is called the <em>fallen factorial</em> because it does not go all the way down to $1$. This is basically <mark>permutations</mark>.</p>
</blockquote>

<p>Essentially we are constructing <em>lists</em> = order matters = permutations.</p>

<p>For example:</p>

<ul>
  <li>making phone numbers (list of length $10$) from all numbers $0\sim 9$ <em>with repetition</em> is $10^{10}$.</li>
  <li>making phone numbers (list of length $4$) from all numbers <em>without repetition</em> is $(10)_4 = 10! / 6!$</li>
  <li>if you have $5$ shirts, $3$ pants, and $4$ pairs of shoes, how many outfits can you make? $5\times 3\times 4$.</li>
</ul>

<blockquote>
  <p><strong>Anagrams</strong>: a permutation of the letters in a word, without repetition.</p>
</blockquote>

<p>for example:</p>

<ul>
  <li>how may anagrams can you create from “math”? $4!=24$ ways</li>
  <li>what about from “momo”? We need to be careful of duplicates created by repeating “m” ($2!$ ways) and “o” ($2$! ways). Therefore $4!/(2!\times 2!)$</li>
</ul>

<p>We can write the above process in a general formula:</p>

\[\frac{n!}{n_1!\times n_2!\times \dots \times n_k!} = \frac{n!}{\Pi_{i=1}^k n_i!}\]

<p>where there are $k$ unique letters, and each of those letters repeated $n_k$ times.</p>

<h2 id="counting-sets">Counting Sets</h2>

<p>Now, we want <mark>combinations</mark> of $k$ elements taken from a set of $n$ possible element. Since we are counting sets = <strong>order does not matter</strong></p>

<blockquote>
  <p><strong>Combination</strong>: the set of $k$ elements <mark>sets</mark> of $n$ elements of an $n$-element set such that $0 &lt; k \le n$ is called a combination</p>
</blockquote>

<blockquote>
  <p><strong>Binomial Coefficient</strong>: let $n,k&gt;0$. A binomial coefficient denoted as</p>

\[{n \choose k} = \frac{n!}{(n-k)!k!} = \frac{(n)_k}{k!}\]

  <p>is the number of combinations of choosing (sets of $k$ elements) from $n$ elements. $k!$ represents, given a permutation $[a,b,c]$, the number of ways you can <em>double count</em> if you are counting combinations.</p>
</blockquote>

<p>Why is it called binomial coefficients? Consider $(x+y)^k$. Then essentially you get</p>

\[(x+y)^k = \underbrace{xx...xxx}_{\text{length $k$}} + xx...xxy + xx..xyy  + \dots + yy...yyy\]

<p>therefore, to <em>collect</em> terms into $x^iy^{k-i}$, it becomes a combination problem $k\choose i$. Therefore you also get</p>

<blockquote>
  <p><strong>Binomial Theorem</strong>: let $n\in \N$, then</p>

\[(x+y)^n = \sum_{k=0}^n {n \choose k} x^{n-k}y^k\]

</blockquote>

<p>Pascal’s triangle: <em>visually</em> you can decompose a combination $n \choose k$ into two terms:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423025442744.png" alt="image-20230423025442744" style="zoom:50%;" /></p>

<p>For example, we have on the 3rd row ${3 \choose 0}=1$ and ${3 \choose 1}=3$, etc, but realize that:</p>

\[{3 \choose 1}= {2 \choose 0}+ {2 \choose 1}\]

<blockquote>
  <p><strong>Pascal’s Identity</strong>: for $n,k\in \Z$, and $0 &lt; k &lt; n$ is positive</p>

\[{n \choose k} = {n-1 \choose k-1} + {n-1 \choose k}\]

  <p>Intuitively, this works because:</p>

  <ul>
    <li>first ${n \choose k}$ represents finding <strong>sets of size $k$</strong> out of $n$ distinct elements</li>
    <li>let there be a “special element $sp$” amongst the $n$ element. The ${n \choose k}$ means “sets of size $k$ that includes $sp$” + “sets of size $k$ that exludes $sp$”. Hence we get:
      <ul>
        <li>“sets of size $k$ that (already) includes $sp$”  = choosing $k-1$ elements out of $n-1$</li>
        <li>“sets of size $k$ that exludes $sp$” = choosing $k$ elements out of $n-1$</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>From the above you also see the symmetry of combinations</p>

<blockquote>
  <p><strong>Rabbit property</strong>: the following are equivalent</p>

\[{n \choose k} = {n \choose n-k}\]

  <p>this makes sense because, say you are choosing a team of $k$ members out of $n$ people. Then for each team of $k$ you choose, you <strong>also implicitly chose a team of $n-k$</strong>. Since there is a one-to-one mapping of the teams you choose/left out = equality.</p>
</blockquote>

<h2 id="inclusion-exclusion-principle">Inclusion Exclusion Principle</h2>

<p>This is essentially used to count <strong>size of union sets</strong> with arbitrarily large number of sets.</p>

<p>For example, let there be 4 sets,</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423032103704.png" alt="image-20230423032103704" style="zoom:23%;" /></p>

<p>and you want to figure out the <strong>union size</strong> $\vert G \cup P \cup V \cup F\vert$?</p>

<p>The finding is that (without proof), let $A_1, …,A_n$ be finite sets, then</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423032434999.png" alt="image-20230423032434999" style="zoom: 33%;" /></p>

<p>so the sign is alternating. This leads to a general formula</p>

<blockquote>
  <p><strong>Principle of Inclusion/Exclusion</strong> (PIE): there is an alternating sign as the number of sets $\cap$ increases. Hence</p>

\[\left| \bigcup_{i=1}^n A_i \right| = \underbrace{\sum_{i=1}^n |A_i|}_{\text{n choose 1 terms}} - \underbrace{\sum_{1\le i &lt;j\le n} |A_i \cap A_j|}_{\text{n choose 2 terms}} + ...+\underbrace{(-1)^{n-1}|A_1 \cap ... A_n|}_{\text{n choose n terms}}\]

  <p>basically a formulaic way if there are large number of sets so that <em>Venn diagram is not drawable</em>.</p>
</blockquote>

<p><em>For example:</em> At the music academy, there are 43 students taking piano $A_P$, 57 students taking violin $A_V$, 29 students taking guitar $A_G$, and 18 taking flute $A_F$. There are 10 students in any two of these courses, 5 students in any three of them and 2 taking all courses. How many students are taking at least one course at the music academy?</p>

<p>This is equivalent of answering $\vert A_P\cup A_V\cup A_G \cup A_F\vert$, hence we get from PIE</p>

<ul>
  <li>
    <p>add $\vert A_P\vert  + \vert A_V\vert + \vert A_G\vert + \vert A_F\vert$ already known</p>
  </li>
  <li>minus $(\vert A_P\cap A_V\vert  + \vert A_P \cap A_G\vert  + …)$. Since “there are 10 students in any two of these courses” and there are 4 choose 2 ways to choose the two groups we get $10 \times {4 \choose 2}$</li>
  <li>add $(\vert A_P \cap A_V \cap A_G\vert + …)$ etc.</li>
</ul>

<p>So we have</p>

\[|A_P\cup A_V\cup A_G \cup A_F| = \underbrace{43+57+29+18}_{\text{1 term}} - \underbrace{10 \times {4 \choose 2}}_{\text{2 terms}} + \underbrace{5 \times {4 \choose 3}}_{\text{3 terms}} - \underbrace{2}_{\text{4 terms}} = 105\]

<h1 id="graph-theory">Graph Theory</h1>

<blockquote>
  <p>We will focus on simple graphs, that are <strong>undirected</strong>, has <strong>no loops</strong> (but can have cycle), and has <strong>no parallel edges</strong> (cycle of two nodes). So</p>

  <ul>
    <li><mark>at most one</mark> edge can exist for a pair of nodes</li>
    <li>should look simple as well</li>
  </ul>
</blockquote>

<p>Some definitions:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423035411986.png" alt="image-20230423035411986" style="zoom: 25%;" /></p>

<blockquote>
  <p><strong>Graph</strong>: A graph consists of a set of vertices and edges:</p>

\[G=(V,E)\]

  <p>in the example above, this graph can be denoted as</p>

\[G=(\{u,v,w,x,y,z\},\{e_1,...,e_8\})\]

</blockquote>

<blockquote>
  <p><strong>Edge</strong>: An edge in a graph joints two vertices (e.g. $u,v$). This edge can be denoted either as $uv$, ${u,v}$, or you can name it $e_1$.</p>
</blockquote>

<p>More definitions</p>

<ul>
  <li>
    <p><strong>adjacent</strong>: two <em>nodes</em> $u$ and $v$ are adjacent if there is an edge directly connecting them. Denoted as $u \sim v$.</p>
  </li>
  <li>
    <p><strong>neighborhood</strong>: the <em>set</em> of nodes that has an edge directly connecting it. Denoted as</p>

\[\mathcal{N}(x)=\{ v \in V | v \sim x \}\]
  </li>
  <li>
    <p><strong>degree</strong>: The number of neighbors of a vertex is called its degree, denoted</p>

\[d(x) = |\mathcal{N}(x)|\]

    <p>for example, $d(w)=2$ in the above figure as there are two connections. Furthermore, we denote</p>

    <ul>
      <li><strong>max degree of a graph</strong> as $\Delta(G)$. In the above figure $\Delta(G)=5$</li>
      <li><strong>min degree of a graph</strong> as $\delta(G)$. In the above figure $\delta(G)=1$</li>
    </ul>
  </li>
</ul>

<p>Observe that in the above example</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423041224774.png" alt="image-20230423041224774" style="zoom: 33%;" /></p>

<p>So why twice? The proof is quite intuitive</p>

<ul>
  <li>adding each edge adds a total degree of 2 to the graph</li>
  <li>if there are $\vert E\vert$ edges, then total degree is $2\vert E\vert$.</li>
</ul>

<blockquote>
  <p><strong>Handshaking</strong>: The sum of the degrees of each vertex in a graph $G=(V,E)$ is equal to twice the number of edges.</p>

\[\sum_{u\in V}d(v) = 2|E|\]

  <p>(this holds for non-simple graphs as well, if you treat a loop as an edge)</p>
</blockquote>

<p>This also means that <mark>a graph's total degree must be even</mark>.</p>

<h2 id="isomorphism">Isomorphism</h2>

<p>Now, we move to studying isomorphism between graphs, which arises when two graphs have the “<strong>same form.</strong>”</p>

<blockquote>
  <p><strong>Isomorphism</strong>: Two graphs $G=(V_G, E_G)$ and $H=(V_H, E_H)$ are isomorphic (“same-form”) if and only if there exists a bijective function (i.e. <strong>mapping of vertices</strong>) representing an isomorphism of $G$ to $H$:</p>

\[f:V_G \to V_H\]

  <p>such that</p>

\[\forall u,v \in V_G, \quad u \sim v \text{ in $G$} \iff f(u) \sim f(v) \text{ in $H$}\]

  <p>i.e. it requires <mark>adjacency between vertices is the same</mark></p>
</blockquote>

<p>Below would be an example</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423115306098.png" alt="image-20230423115306098" style="zoom:33%;" /></p>

<p>But below is not</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423115405904.png" alt="image-20230423115405904" style="zoom:33%;" /></p>

<p>because realize that the left graph has an edge connecting a vertex with (degree = 1) with (degree = 3), but the right graph only has (degree = 2) with (degree = 3).</p>

<p>In general:</p>

<ul>
  <li><strong>to prove isomorphism</strong>: find a bijection and show that the adjacency property (edge to endpoints) is preserved
    <ul>
      <li>this is generally considered a hard problem, as there is <em>no efficient algorithm</em> to find the bijection.</li>
    </ul>
  </li>
  <li><strong>to prove non-isomorphism</strong>: if adjacency are preserved, then there <strong>will be some invariant properties</strong> such as degrees. Therefore, if some properties are violated then they cannot be isomorphic</li>
</ul>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423115939111.png" alt="image-20230423115939111" style="zoom:33%;" /></p>

<h2 id="types-of-graphs">Types of Graphs</h2>

<p>some common types of graphs, which can have some useful properties.</p>

<blockquote>
  <p><strong>Null graph</strong>: A graph that does not have edges. So you have $G=(V, {})$. This is also often denoted as</p>

\[L_n = \text{null graph with n nodes}\]

  <p>e.g $L_1$ is one vertex zero edge, $L_2$ is two vertex zero edge, …</p>
</blockquote>

<blockquote>
  <p><strong>Complete graph</strong>: A graph with <strong>every possible pair of vertices</strong> has an edge. We use $K_n$ to denote a complete graph with $n$ vertices.</p>
</blockquote>

<p>for example:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423121353025.png" alt="image-20230423121353025" style="zoom:37%;" /></p>

<p>notice that</p>

<ul>
  <li>$K_1 = L_1$!</li>
  <li>for <mark>every vertex $v$,  it has $\deg(v)=n-1$!</mark></li>
</ul>

<blockquote>
  <p><strong>Path graph</strong>: A path graph has vertices ${v_1,…,v_n}$ and edges ${e_1,…,e_{n-1}}$  such that <strong>an edge $e_k$ joins ${v_k, v_{k+1}}$</strong>. We use $P_n$ to denote path graphs with $n$ vertices.</p>
</blockquote>

<p>For example</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423121642105.png" alt="image-20230423121642105" style="zoom: 33%;" /></p>

<p>notice that $\vert V\vert -1=\vert E\vert$.</p>

<blockquote>
  <p><strong>Cycle Graph</strong>: A path graph has vertices ${v_1,…,v_n}$ and edges ${e_1,…,e_{n}}$  such that an edge $e_k$ joins ${v_k, v_{k+1}}$ <strong>where $k+1$ is “$\bmod n$”</strong>. We use $C_n$ to denote path graphs with $n$ vertices.</p>
</blockquote>

<p>For example:</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423122142344.png" alt="image-20230423122142344" style="zoom:33%;" /></p>

<p>where notice that:</p>

<ul>
  <li><mark>all vertex has exactly degree 2</mark></li>
  <li>and $\vert V\vert =\vert E\vert$.</li>
</ul>

<blockquote>
  <p><strong>Regular Graph</strong>: A graph is regular provided <strong>every vertex has the same degree</strong></p>

  <ul>
    <li>so a $C_n$ is a regular graph of degree 2</li>
    <li>a $K_n$ complete graph is also regular of degree $n-1$</li>
    <li>a $L_n$ is also a regular of degree 0.</li>
  </ul>
</blockquote>

<h2 id="properties-of-graphs">Properties of Graphs</h2>

<p>Now we can talk about certain properties. First a few more definitions.</p>

<blockquote>
  <p><strong>Walk</strong>: A walk in $G$ is just <mark>any</mark> sequence of vertices where each vertex is adjacent to the next vertex (i.e. you can physically walk on the graph given this sequence).</p>
</blockquote>

<p>For example, given this graph</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230423123320755.png" alt="image-20230423123320755" style="zoom:43%;" /></p>

<p>where:</p>

<ul>
  <li>$(u,v,w,v,x)$ is a walk of length 4.</li>
  <li>$(v)$ is a walk of length 0.</li>
  <li>$(u,u,x,v,w)$ is NOT a walk because $u$ does not have a connection to $u$</li>
</ul>

<blockquote>
  <p><strong>Path</strong>: A path in a graph is a <strong>walk</strong> in which <mark>no vertex</mark> is repeated. We denote a path of length $n-1$ with $n$ vertices as $P_n$.</p>

  <p>Additionally, we denote <strong>$(u,v)$-path</strong> is a path that starts at vertex $u$ and ends at vertex $v$.</p>
</blockquote>

<p>Then we can define</p>

<blockquote>
  <p><strong>Connected Graph</strong>: A graph $G=(V,E)$ is called a connected graph provided <mark>each pair of vertices is connected by a path</mark> (i.e. you can reach any vertex from some random vertex). Formally:</p>

\[\forall u, v \in V,\quad \exists (u,v)\text{-path}\]

</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">connected</th>
      <th style="text-align: center">disconnected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://www.tutorialspoint.com/discrete_mathematics/images/connected_graph.jpg" alt="Connected vs Disconnected Graphs" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="https://www.tutorialspoint.com/discrete_mathematics/images/unconnected_graph.jpg" alt="Connected vs Disconnected Graphs" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<h2 id="trees">Trees</h2>

<blockquote>
  <p><strong>Tree</strong>: Let $T=G=(V,E)$. $G$ is a tree iff it is connected and <strong>has no cycle (acyclic)</strong>. A cycle can be seen as a path in which the first and last variables are the same. A graph that is acyclic contains <mark>no such paths</mark>.</p>
</blockquote>

<p><img src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20171027092037172-0306:9781316105450:08931fig2_1.png?pub-status=live" alt="Shapes of Graphs: Trees to Triangles (Chapter 2) - Applying Graph Theory in  Ecological Research" style="zoom:33%;" /></p>

<p>some properties of trees include:</p>

<ul>
  <li>there is <mark>exactly one path</mark> to reach from one vertex to another (since there is no cycle)</li>
  <li>for a tree <mark>$\vert V\vert  = \vert E\vert +1$</mark> (which can be proven by induction)</li>
  <li>A graph is a tree if and only if it a <mark>minimal connected</mark> (if you take out one edge, it becomes disconnected)</li>
</ul>

<blockquote>
  <p><strong>Leaf</strong>: let $G=(V,E)$, then a vertex of <strong>degree 1</strong> is called a leaf in a tree. And if you take out any of the leaf nodes, you <strong>still have a tree</strong></p>
</blockquote>

<blockquote>
  <p><strong>Theorem</strong>: let $G=(V,E)$. $G$ is tree provided that</p>

\[\forall u,v \in V \quad \exists!\text{u-v path}\]

</blockquote>

<blockquote>
  <p><strong>Theorem</strong>: for a tree $\vert V\vert =\vert E\vert +1$</p>
</blockquote>

<p><em>Proof</em>: by induction.</p>

<ul>
  <li>
    <p>base case: $\vert V\vert =1$ with $\vert E\vert =0$ holds with tree of 1 vertex</p>
  </li>
  <li>
    <p>inductive hypothesis: suppose $T_k$ is a tree with $k$ vertices. Suppose the proposition holds for $T_k$.</p>

\[|E_{T_k}| = |V_{T_k}| - 1\]

    <p>i.e. it has <strong>$k$ vertices with $k-1$ edges</strong>.</p>
  </li>
  <li>
    <p>inductive step: it works for a tree $T_{k+1}$, because $T_{k+1}$ must be $T_k$ <strong>plus a leaf node</strong>. So suppose we derach one leaf of $T_{k+1}$, we get a tree $T_k$. Therefore it <strong>means $T_{k+1}$ has one extra edge than $T_k$</strong> (and one leaf node).</p>

\[|E_{T_{k+1}}|=k=(k+1)-1\]

    <p>holds</p>
  </li>
</ul>

<blockquote>
  <p><strong>Hamiltonian Path</strong>: is a <mark>path</mark> containing <mark>all vertices</mark> (recall that a path walk <mark>without repeating vertices</mark>).</p>
</blockquote>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230425123039505.png" alt="image-20230425123039505" style="zoom:13%;" /></p>

<ul>
  <li>
    <p>How many walks are there between $a,b$? There is an <strong>infinity of walks</strong> since there are cycles</p>
  </li>
  <li>
    <p>How many paths are there between $a,b$? There are $4\times 2= 8$. How?</p>
    <ul>
      <li>it is equivalent to #ways to reach $a_7$ from $a$ times #ways to reach from $a_7$ to $b$.</li>
    </ul>
  </li>
</ul>

<p>There is no Hamiltonian path between $a$ and $b$. (complicated, in this case by enumeration)</p>

<p><em>For Example</em>: let $K_n$ be a complete graph with $n\ge 2$. How many Hamiltonian paths are there <strong>in total</strong>, between any pair of vertices?</p>

<ul>
  <li>notice that a complete graph is one for <em>every pair of vertex there is an edge</em>.
    <ul>
      <li>for $K_2$, since you can reach any from any other, there are two boxes = $2!$ ways (<code class="language-plaintext highlighter-rouge">ab</code> and <code class="language-plaintext highlighter-rouge">ba</code>)</li>
      <li>for $K_3$, you have $3!$ ways (<code class="language-plaintext highlighter-rouge">abc</code>, <code class="language-plaintext highlighter-rouge">acb</code>, <code class="language-plaintext highlighter-rouge">bac</code>, <code class="language-plaintext highlighter-rouge">bca</code>, <code class="language-plaintext highlighter-rouge">cab</code>, <code class="language-plaintext highlighter-rouge">cba</code>)</li>
      <li>for $K_4$, you have $4!$ ways</li>
    </ul>
  </li>
  <li><strong>so in total $n!$ Hamiltonian path for $K_n$</strong></li>
</ul>

<h2 id="eulerian-graphs">Eulerian Graphs</h2>

<blockquote>
  <p><strong>Eulerian trail</strong>: a <mark>walk</mark> in $G=(V,E)$ is called an eulerian trail provided it traverses every <mark>edge exactly once</mark>.</p>
</blockquote>

<blockquote>
  <p><strong>Eulerian tour</strong>: if an Eulerian trail start and end at the same vertex, it is called an Eulerian tour.</p>
</blockquote>

<blockquote>
  <p><strong>Eulerian graph</strong>: A graph with an Eulerian tour is called an Eulerian graph.</p>
</blockquote>

<p><em>For example</em>: consider the following graphs</p>

<p><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230425105112462.png" alt="image-20230425105112462" style="zoom:33%;" /></p>

<ul>
  <li>for graph $H$, we can
    <ul>
      <li>have an Eulerian trail from<code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code>: $x-v-u-w-v-y-w-x-y$.</li>
      <li>there is <em>no Eulerian tour</em> in $H$.</li>
    </ul>
  </li>
  <li>for graph $G$, we can
    <ul>
      <li>no Eulerian tour nor Eulerian trail in $G$</li>
    </ul>
  </li>
</ul>

<p>How do we justify? Realize if you have even one <strong>vertex with odd degrees</strong>, then there cannot be an Eulerian tour.</p>

<ul>
  <li>because to go out and get back, you in general must do <code class="language-plaintext highlighter-rouge">out</code> and <code class="language-plaintext highlighter-rouge">in</code>. So the degree <mark>has to be even</mark>.</li>
</ul>

<blockquote>
  <p>Theorem: let $G$ be a connected graph. If <strong>every vertex $v\in V(G)$ has even degree</strong>, then there is an Eulerian <mark>tour</mark> that begins and ends at (every) $v$</p>
</blockquote>

<blockquote>
  <p>Theorem: let $G$ be a connected graph with <strong>exactly two vertices of odd degree $u,v$.</strong> Then $G$ has a Eulerian <mark>trail</mark> that begins at $u$ and ends at $v$.</p>
</blockquote>

<p><em>For Example</em>: how many non-isomorphic graph with $n$ vertices are there? Given $n=4$,</p>

<ul>
  <li>
    <p>there are ${4 \choose 2}=6$ possible edges to place into a graph of 4 nodes.</p>
  </li>
  <li>
    <p>for every graph, you can choose to <strong>have the edge or not</strong>. Hence <mark>$2^6$ possible graphs</mark> (including isomorphic)</p>

\[\square \square \square \square \square \square\]
  </li>
  <li>
    <p>for non-isomorphic graph, you will need to draw all the equivalence groups/isomorphism</p>
  </li>
  <li>
    <p>there you will find there are 11 of them/non-isomorphic graph</p>
  </li>
</ul>

<blockquote>
  <p>There is an formula for this, but in the course we pretend we don’t know, and we have to count.</p>
</blockquote>

<h2 id="planar-graphs">Planar Graphs</h2>

<p>Is it possible to feed every house with utility without a cable (edge) crossing other edges?</p>

<blockquote>
  <p><strong>Planar Graph</strong>: A graph is a planar graph if it can be drawn in a <strong>two-dimensional space</strong> with <strong>no crossing edges</strong>. A planar graph divides the plane into areas called <strong>faces</strong>.</p>
</blockquote>

<p><em>For Example</em>: square graph being planar v.s. not planar.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">non-planar</th>
      <th style="text-align: center">planar</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230425123212482.png" alt="image-20230425123212482" style="zoom: 10%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-COMS3203_Discrete_Math/image-20230425123422760.png" alt="image-20230425123422760" style="zoom:10%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the planar graph no the right has “non-overlapping” faces.</p>

<blockquote>
  <p>Theorem: For <strong>every connected planar graph</strong>, we have:</p>

\[v+f = e+2\]

  <p>where $v$ is the number of vertices, $e$ the number of edges, and $f$ the number of faces.</p>
</blockquote>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Discrete Math Logistics]]></summary></entry><entry><title type="html">PHYS3008 EM n Optics</title><link href="/lectures/2022@columbia/PHYS3008_EM_n_Optics.html/" rel="alternate" type="text/html" title="PHYS3008 EM n Optics" /><published>2023-05-11T00:00:00+00:00</published><updated>2023-05-11T00:00:00+00:00</updated><id>/lectures/2022@columbia/PHYS3008_EM_n_Optics</id><content type="html" xml:base="/lectures/2022@columbia/PHYS3008_EM_n_Optics.html/"><![CDATA[<h1 id="recap">Recap</h1>

<h2 id="electrostatics">Electrostatics</h2>

<p>charges stationary $\rho(\vec{r})$ independent of time</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Basic facts about electric field</td>
      <td>$\oint \vec{E}\cdot d\vec{a} = \frac{Q_{enc}}{\epsilon_0}$</td>
      <td>Useful when there is symmetry, so that $\oint \vec{E}\cdot d\vec{a}$ is easy to figure out</td>
    </tr>
    <tr>
      <td> </td>
      <td>$V(\vec{r}) = - \int_O^r \vec{E}\cdot d\vec{l}$</td>
      <td>Useful when you <strong>already know $\vec{ E}$</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Careful for regions when $\vec{E}$ is <strong>not continuous</strong>. Then you may need to split your integral.</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{\nabla}\times \vec{E}=\vec{0}$</td>
      <td>Conservation of field/charge. Proven from showing that $\oint \vec{E}\cdot d\vec{l}=\int \vec{\nabla}\times \vec{E}\cdot d\vec{S}=0$ for a single point charge, then superposition.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>A common approach to <strong>prove starting from a single point charge</strong>, then just use superposition</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{E}\cdot d \vec{l} = 0$</td>
      <td>from the above.</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{E}=-\vec{\nabla}V$</td>
      <td>Useful for <strong>finding</strong> out $\vec{E}$, since $V$ is easier to compute</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\nabla^2 V = \rho / \epsilon_0$</td>
      <td>Poisson’s equation. Useful to <strong>solve when inside vacuum region</strong> such that $\rho = 0$ globally in that region.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>In side vacuum, $\nabla^2 V = 0$ is the Laplace Equation</td>
    </tr>
    <tr>
      <td>Over <strong>all space</strong></td>
      <td>$W=\frac{\epsilon}{2}\int \vert \vec{E}\vert ^2d^3r’$</td>
      <td><strong>Easier to compute</strong>, often used.</td>
    </tr>
    <tr>
      <td>Perfect Conductor</td>
      <td>$\vec{E}<em>{meat}=0$, $\rho</em>{meat}=0$</td>
      <td>Charges free to move, will redistribute until $\vec{E}_{meat}=0$. Hence all charges are <strong>on the surface</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$V_{surf}=V_0$, $\vec{E}_{surf}={E}^\perp \hat{n}$</td>
      <td>$V(b)-V(a)=-\int_a^b \vec{E}\cdot d\vec{l}=0$ if on the surface, since $\vec{E}=0$ inside</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$V_{meat}=V_0$ as well since there is no field</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Those conditions are often used as <mark>constraints</mark> in a problem</td>
    </tr>
    <tr>
      <td>Capacitance</td>
      <td>$C \equiv \frac{Q}{\Delta V}$</td>
      <td>$Q=+Q$ of the <strong>two conductors,</strong> is the <strong>charge stored</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\Delta V =- \int_a^b \vec{E}\cdot d\vec{l}$ commonly used</td>
      <td>$\Delta V$ is the potential difference across the two conductor = <strong>how much $\Delta V$</strong> needed to store $Q$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Should be purely <strong>geometric</strong>, in unit $\epsilon \cdot \text{Length}$</td>
    </tr>
    <tr>
      <td>Energy Stored in Conductor</td>
      <td>$W=\frac{1}{2}\frac{Q^2}{C}=\frac{1}{2}CV^2$</td>
      <td>Think of it being same as energy needed to <strong>charge up conductor</strong> = <strong>work done</strong> to move all the charges over</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Proven because $dW = \Delta V(q)dq$ to move a charge $dq$ over, and then since $C=\frac{q}{\Delta V(q)}$ is geometric, perform $W=\int dW$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Summary</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230125154805806.png" alt="image-20230125154805806" /></td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="magnetostatics">Magnetostatics</h2>

<p>when you have a <mark>steady current</mark>, but as it is current, changes are moving $\implies$ magnetic field</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Biot-Savart Law Generic</td>
      <td>$\vec{B}(\vec{r}) = \int \frac{\mu_0}{4\pi} \frac{\vec{J}(\vec{r}’)\times(\vec{r}-\vec{r’})}{\vert \vec{r}-\vec{r’}\vert ^3}\,d^3r’$</td>
      <td>True if $J$ is independent of time, i.e. steady</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{E}(\vec{r}) = \int \frac{1}{4\pi \epsilon_0} \frac{\vec{\rho}(\vec{r}’)\cdot(\vec{r}-\vec{r’})}{\vert \vec{r}-\vec{r’}\vert ^3}\,d^3r’$</td>
      <td>parallel to the $\vec{E}$ field, basically $J \to \rho$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mu_0 = 1/(\epsilon_0 c^2)$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Biot-Savart if Steady current</td>
      <td>$\vec{B}(\vec{r}) = \frac{\mu_0 I}{4\pi} \int \frac{d\vec{l}(\vec{r}’)\times(\vec{r}-\vec{r}’)}{\vert \vec{r}-\vec{r}’\vert ^3}$</td>
      <td>basically we have here $\vec{J} = \vec{I}\delta^2(\vec{r}-\vec{r}’)$ and that $\vec{I}dl = Id\vec{l}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>works if constant current so $\vec{I}dl = Id\vec{l}$ holds</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230125155110733.png" alt="image-20230125155110733" style="zoom: 50%;" /></td>
      <td>Integrates $dl’$ <mark>only</mark> over where $\vec{r}’ \neq 0$</td>
    </tr>
    <tr>
      <td>Magnetric Field of a Straight line wire</td>
      <td>$\vec{B} = \frac{\mu_0 I}{2\pi r} \hat{e}_\phi$</td>
      <td>Found either using Biot-Savart for current, or using the symmetry hence $\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230125155135485.png" alt="image-20230125155135485" style="zoom:50%;" /></td>
      <td>Graphically it circulates the current</td>
    </tr>
    <tr>
      <td>Field of a Toroid</td>
      <td>$B = \frac{\mu N I}{2 \pi s}$</td>
      <td>and $0$ outside. $s$ is basically the radial distance</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230126222238405.png" alt="image-20230126222238405" style="zoom:20%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Divregence of B</td>
      <td>$\vec{\nabla}_{\vec{r}} \cdot \vec{B}(\vec{r})=0$</td>
      <td>for E field we had $\vec{\nabla}\cdot \vec{E}= \rho/\epsilon_0$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{B}\cdot d\vec{S}=0$</td>
      <td>for E field we had Gaussian surface $\oint \vec{E}\cdot d\vec{S}=Q_{enc}/\epsilon_0$!</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived using $\int \vec{\nabla}\cdot\vec{B} d\tau = \oint \vec{B}\cdot d\vec{S}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>not very useful for B field calculation</td>
    </tr>
    <tr>
      <td>Curl of B</td>
      <td>$\vec{\nabla}_{\vec{r}} \times \vec{B}(\vec{r})=\mu_0 \vec{J}(\vec{r})$</td>
      <td>for E field we had this is 0</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{B}\cdot d\vec{l}=\mu_0 I_{enc}$</td>
      <td>very useful for B field calculation if we have symmetry in $J$ or $I$ setup!</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>called <strong>Ampere Loops</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived using $\int \vec{\nabla}\times \vec{B}\cdot d\vec{S} = \oint \vec{B}\cdot d\vec{l}$</td>
    </tr>
    <tr>
      <td>Field of Solenoid with $n$ turns per unit legnth</td>
      <td>$\vec{B} = \mu_0 n I \hat{e}_z$</td>
      <td>if inside the solenoid</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{B}=0$</td>
      <td>if outside</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230125155158303.png" alt="image-20230125155158303" style="zoom: 67%;" /></td>
      <td>Essentially solved by drawing Amphere loops</td>
    </tr>
    <tr>
      <td>Magnetic Vector Potential</td>
      <td>$\vec{\nabla}\times \vec{A} = \vec{B}$</td>
      <td>so that $\vec{A}$ is easier to compute</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>from $\vec{E}= - \vec{\nabla}V$ for $V$ is easier to compute</td>
    </tr>
    <tr>
      <td>Gauges for $\vec{A}$ due to above definition</td>
      <td>$\nabla(\nabla \cdot \vec{A})-\nabla^2\vec{A} = \mu_0 \vec{J}$</td>
      <td>if $\vec{\nabla}\times \vec{A} = \vec{B}$, and we know $\nabla \times \vec{B} = \mu_0 \vec{J}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>if $\nabla \cdot \vec{A} = 0$</td>
      <td>Colomb’s Gauge, used by this course</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}’ = \vec{A}+ \nabla\lambda$ results in the same $B$ field</td>
      <td>therefore we can always force $\nabla \cdot \vec{A} = 0$ by choosing $\lambda$</td>
    </tr>
    <tr>
      <td>Magnetic Vector Potential Definition</td>
      <td>$\vec{A} = \frac{\mu_0}{4\pi}\int \frac{\vec{J}(\vec{r}’)}{\vert \vec{r}-\vec{r}’\vert }d^3r’$</td>
      <td>derived from the above with $\nabla^2 \vec{A}=-\mu_0 \vec{J}$, so that each compontent $A_x,A_y,A_z$ essentially is an analog version of $\nabla^2 V = - \rho / \epsilon_0$ and we know the solution $V$ from using $\rho$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>this means that $\vec{A}$ is usually in the <strong>same direction</strong> as current!</td>
    </tr>
    <tr>
      <td> </td>
      <td>Technically $\vec{A} = \frac{\mu_0}{4\pi}\int \frac{\vec{J}(\vec{r}’)}{\vert \vec{r}-\vec{r}’\vert }d^3r’ + \vec{A}(0)$</td>
      <td>for $\vec{A}(0)$ is a reference point</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>usually use $\vec{A}(\infty)=0$</td>
    </tr>
  </tbody>
</table>

<h2 id="polarization-and-magnetization">Polarization and Magnetization</h2>

<blockquote>
  <p>Basically consider $E$ and $B$ Field inside Matter themselves (e.g. after providing an external $\vec{E}<em>{ext}$ or $\vec{B}</em>{ext}$)</p>
</blockquote>

<p>first, we show $E$ fields in matter, i.e. polarization $\vec{P}$, and we are <mark>only interested in what field this material will produce</mark>, and do not discuss how we get there.</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Electric Dipole</td>
      <td> </td>
      <td>when a piece of diaelectric material is placed under $\vec{E}_{ext}$, those little dipoles align themselves and form $\vec{P}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130144401254.png" alt="image-20230130144401254" style="zoom:40%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Polarization</td>
      <td>$\vec{P}\equiv$dipole moment per unit volume</td>
      <td>when a piece of dielectric material is placed under $\vec{E}_{ext}$, those little dipoles align themselves and form $\vec{P}$</td>
    </tr>
    <tr>
      <td>Potential due to Polarization $\vec{P}$</td>
      <td>$V(\vec{r}) = \frac{1}{4\pi \epsilon}\frac{\vec{p}]\cdot\hat{r}}{r^2} + O(\frac{1}{r^3})$</td>
      <td>multi-pole expansion of electric potential</td>
    </tr>
    <tr>
      <td> </td>
      <td>$V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{\vert \vert \vec{r} - \vec{r}’\vert \vert }dA’+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{\vert \vert \vec{r} - \vec{r}’\vert \vert }dV’$</td>
      <td>derived from $V(\vec{r})$ above but <mark>only</mark> consisting of dipole terms</td>
    </tr>
    <tr>
      <td> </td>
      <td>$V(\vec{r})=V(\text{from$\sigma_b, \rho_b$})$</td>
      <td>i.e. you can <strong>treat</strong> the dielectric as having $\sigma_b$ on the surface and $\rho_b$ inside.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>if $\rho_b = 0$, $V(\text{from$\sigma_b,\rho_b$})$ can be solved using <strong>Laplacian $\nabla^2 V = 0$</strong> with boundary of $\sigma_b$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\sigma_b \equiv \vec{P}\cdot \hat{n}$</td>
      <td>$\hat{n}$ points from the surface having dielectric to vaccum/non-dielectric place</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\rho_b\equiv - \nabla \cdot \vec{P}$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Interpretation of Bound Charge due to $\vec{P}$</td>
      <td>$\sigma_b \equiv \vec{P}\cdot \hat{n}$</td>
      <td>$\sigma_b$ comes from lining up dipoles so that only ends matter</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130205559197.png" alt="image-20230130205559197" style="zoom: 33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\rho_b\equiv - \nabla \cdot \vec{P}$</td>
      <td>$\rho_b$ considers if dipole in between is not cancelled out, then the flux of polarization would relate to charge: $-\oint \vec{P}\cdot d\vec{A} =\int_V \rho_b \,d^3r’$</td>
    </tr>
    <tr>
      <td>Displacement Field</td>
      <td>$\nabla \cdot \vec{E}_{total} = (\rho_f+\rho_b) / \epsilon_0$</td>
      <td>since now the polarization also produces field, we want to know a “general” equation for $\vec{E}$ field</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\nabla \cdot (\epsilon \vec{E}_{total}+\vec{P}) = \rho_f$</td>
      <td>use $\rho_b\equiv - \nabla \cdot \vec{P}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{D}\equiv \epsilon \vec{E}_{total}+\vec{P}$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Gauss Law for Electric Displacement Field</td>
      <td>$\nabla \cdot \vec{D} = \rho_f$</td>
      <td>derived from above</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{D}\cdot d\vec{A} = Q_{free_{enc}}$</td>
      <td>integral form</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>if have symmetry can use <strong>Gaussian surfaces</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>used for proving boundary conditions for $\vec{D}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{\nabla} \times \vec{D} = \vec{\nabla} \times \vec{P}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{D}\cdot d\vec{r}=\oint \vec{P}\cdot d\vec{r}$</td>
      <td>Stokes’ Theorem, derived from above</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>used for proving boundary conditions for $\vec{D}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>the above is <strong>true for any dielectric</strong></td>
    </tr>
    <tr>
      <td>$\vec{D}$ and $\vec{E}$ relation</td>
      <td>$\vec{\nabla}\cdot \vec{E}<em>{tot} = \rho</em>{tot} / \epsilon_0 \to \vec{E}<em>{tot} = \frac{1}{4\pi \epsilon_0} \int \frac{\rho</em>{tot}}{\symscr{r}} \hat{\symscr{r}}\,d^3r’$</td>
      <td>because $\vec{\nabla}\times \vec{E}_{tot} = 0$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{\nabla}\cdot \vec{D}= \rho_{free} \not\to \vec{D} = \frac{1}{4\pi } \int \frac{\rho_{free}}{\symscr{r}} \hat{\symscr{r}}\,d^3r’$</td>
      <td>because $\vec{\nabla} \times \vec{D} \neq 0$ often, hence $\vec{D}$ might not have a potential</td>
    </tr>
    <tr>
      <td>Linear Dielectric</td>
      <td>$\vec{P} = \epsilon_0 \chi_e \vec{E}_{tot}$</td>
      <td>$\chi_e$ is electric susceptibility, $\epsilon_0$ is permittivity of free space</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>if $\vec{E}<em>{applied}$ is given, we are stuck because it will produce $\vec{P}$, which produces $\vec{E}</em>{resp}$ hence affects $\vec{E}_{tot}$, which relates to $\vec{P}$…</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{D} = \epsilon_0(1+\chi_e)\vec{E}<em>{tot}\equiv \epsilon \vec{E}</em>{tot}$</td>
      <td>$\epsilon$ is permittivity of material</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>can use this to <strong>break from the loop</strong> if we can find $\vec{D}$ from Gaussian surface</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\epsilon_r \equiv \epsilon / \epsilon_0 = 1+\chi_e$</td>
      <td>relative permittivity, $\chi_e=0$ in free space.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>also called <strong>dielectric constant</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{\nabla}\times \vec{D} = \vec{\nabla} \times (\epsilon \vec{E}_{tot}) \neq 0$</td>
      <td>because $\epsilon$ could vary in different position</td>
    </tr>
    <tr>
      <td>Space filled with homogenous Linear Dielectric $\chi_e$</td>
      <td>$\vec{\nabla} \times \vec{D} = 0$, $\vec{\nabla} \cdot \vec{D}= \rho_{free}$</td>
      <td>inside the homogenous linear dielectric, $\epsilon$ is constant</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{D} = \epsilon_0 \vec{E}_{free}$</td>
      <td>$\vec{E}_{free}$ is field produced by free charges</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>because now $\vec{\nabla}\cdot \vec{D}= \rho_{free} \to \vec{D} = \frac{1}{4\pi } \int \frac{\rho_{free}}{\symscr{r}} \hat{\symscr{r}}\,d^3r’$ holds</td>
    </tr>
  </tbody>
</table>

<p><em>Application of Bound Charges</em></p>

<p>Consider a slab of dielectric with thickness $d$ carrying polarization $\vec{P} = k[1+(x/d)]\hat{x}$ :</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20220314163908686.png" alt="image-20220314163908686" /></p>

<p>we do not care yet what external field is needed to produce this polarization. We want to know the electric field <strong>produced by</strong> this given polarization.</p>

<p>First, we can find the bound charges to be:</p>

\[\sigma_b = \vec{P}\cdot \hat{n} = \begin{cases}
2k, &amp; x =d\\
-k, &amp; x=0
\end{cases}\\
\rho_b = -\nabla \cdot \vec{P} = -\frac{k}{d},\quad 0&lt;x&lt;d\]

<p>Then, we can <em>treat this as real charges</em> to compute $V(\vec{r})$ due to the correctness of:</p>

\[V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{||\vec{r} - \vec{r}'||}dA'+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{||\vec{r} - \vec{r}'||}dV'\]

<p>Now, since we know the charge distribution, we can exploit the symmetry and save doing the above extra integrals:</p>

<ul>
  <li>
    <p>drawing Gaussian surfaces inside/outside the slab gives:</p>

\[\vec{E}_&lt; = \vec{E}_&gt;\\
(\vec{E}_{inside}-\vec{E}_{&lt;})\cdot \hat{x} = \frac{k}{\epsilon_0}\left(1+\frac{x}{d}\right)\]
  </li>
  <li>
    <p>but notice that the net charge is zero, hence:</p>

\[\vec{E}_&lt; = \vec{E}_&gt; = \vec{0}\\
\vec{E}_{inside} = -\frac{k}{\epsilon_0}\left( 1+ \frac{x}{d} \right)\]
  </li>
</ul>

<p>the result is consistent if you compute from $\vec{D}$ and use $\vec{D}=\epsilon \vec{E}_{tot}$.</p>

<hr />

<p><em>Treating $\sigma_b,\rho_b$ as real charges</em>:</p>

<p>Basically it works due to the proof that</p>

\[V(\vec{r}) = \frac{1}{4\pi \epsilon_0} \oint_\Omega \frac{\sigma_b}{||\vec{r} - \vec{r}'||}dA'+ \frac{1}{4\pi \epsilon_0} \int_V \frac{\rho_b}{||\vec{r} - \vec{r}'||}dV'\]

<p>Then, consider finding $V(r)$ for $r &lt; a$ in the setup below:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20220314173419462.png" alt="image-20220314173419462" /></p>

<p>where $Q$ is a point charge sitting at origin. Then, we first can easily figure out $\vec{D}$ by:</p>

\[\int \vec{D}\cdot d\vec{S} = Q_{free_{enc}} = Q,\quad \text{true for all space}\]

<p>hence we get:</p>

\[\vec{D} = \frac{Q}{4 \pi r^2}\hat{r}\to \vec{E}_{tot} = \frac{Q}{4\pi \epsilon r^2}\hat{r}\]

<p>for $\epsilon$ <mark>actually changes in regions</mark>, so that:</p>

<ul>
  <li>$\epsilon=\epsilon_0$ in vacuum hence $r &lt; a$ or $r &gt; b$</li>
  <li>$\epsilon =\epsilon_r \epsilon_0$  inside dielectric</li>
</ul>

<p>Though we can proceed easily to find $V(\vec{r})$ now, but we can show that we obtain the same result if we <strong>treating $\sigma_b,\rho_b$ as real charges</strong>:</p>

<ul>
  <li>
    <p>to find $\sigma_b, \rho_b$, first we need to find $\vec{P}$:</p>

\[\vec{P} = \epsilon_0 \chi_e \vec{E}_{tot} = \frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon r^2}\hat{r}\]
  </li>
  <li>
    <p>then find $\sigma_b, \rho_b$:</p>

\[\sigma_b =\vec{P}\cdot \hat{n} = \begin{cases}
-\frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon a^2} , &amp; x =a\\
\frac{\epsilon_0 \chi_e  Q}{4\pi \epsilon b^2} , &amp; x =b
\end{cases}\\
\rho_b = -\nabla \cdot \vec{P} =0,\quad a&lt;x&lt;b\]
  </li>
</ul>

<p>Then we can consider $\vec{E}_{tot}$ from those charges while using symmetry:</p>

\[\begin{cases}
\vec{E}_{&lt;} = \frac{Q}{4 \pi \epsilon_0 r^2} \hat{r}, &amp; r &lt; a\\
\vec{E}_{inside} = \frac{Q+Q_b(a)}{4 \pi \epsilon_0 r^2} \hat{r}= \frac{Q}{4 \pi \epsilon_0(1+\chi_e) r^2} \hat{r}, &amp; a&lt;r &lt;b\\
\vec{E}_{&gt;} = \frac{Q}{4 \pi \epsilon_0 r^2} \hat{r}, &amp; r &gt; b
\end{cases}\]

<p>using $\oint \vec{E}\cdot d\vec{A} = Q_{enc}/\epsilon_0$, where:</p>

<ul>
  <li>$Q_b(a)$ is the bound charges at $r=a$, which is $4 \pi a^2 \sigma_b(a)$.</li>
  <li>this is consistent with $\vec{E}_{tot} = \frac{Q}{4\pi \epsilon r^2}\hat{r}$.</li>
</ul>

<p>Finally, finding $V(\vec{r})$ for $r &lt; a$ is just doing the integral:</p>

\[V(\vec{r}) = -\int_\infty^r \vec{E}\cdot d\vec{r} = -\int_\infty^b \vec{E}_&lt;\cdot d\vec{r}-\int_b^a \vec{E}_{inside}\cdot d\vec{r}-\int_a^r \vec{E}_&gt;\cdot d\vec{r}\]

<p>for $d\vec{r}=dr \hat{r}+ rd\theta \hat{\theta}$.</p>

<hr />

<p><strong>Magnetization</strong> in matter is <em>essentially the same</em> as polarization, but here due to <em>tiny current loops</em>:</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Magnetic Dipoles</td>
      <td>$\vec{m} = I \int d\vec{A}$</td>
      <td>magnetic dipole of a small current loop</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130142357629.png" alt="image-20230130142357629" style="zoom:33%;" /></td>
      <td>derived from multi-pole expansion of $\vec{A}$, and since the mono-pole term is zero, the first order term is dipole.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>if you look into any material, electron orbit themselves/spinning = constitute <strong>tiny current loops = dipole</strong>. However, most material has “no magnetization” because those dipoles are randomly oriented</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>But when an external field $\vec{B}_{\mathrm{ext}}$ is supplied, you get some net orientations = net magnetization $\vec{M}$</td>
    </tr>
    <tr>
      <td>Magnetic Moment $\vec{M}(\vec{r})$</td>
      <td>$\vec{M}(\vec{r})\approx \sum \vec{m}$</td>
      <td>think of $\vec{M}$ as specifying a collection of tiny dipoles $\vec{m}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{M}\equiv$ dipole moment per unit volume</td>
      <td>for continuous case</td>
    </tr>
    <tr>
      <td>$\vec{A}$ from $\vec{M}$</td>
      <td>$\vec{A}=\frac{\mu_0}{4\pi} \sum_{i} \frac{\vec{m}\times \hat{r}}{\vert \vec{r}-\vec{r}_i’\vert }$</td>
      <td>discrete case</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}=\frac{\mu_0}{4\pi} \int \frac{\vec{M}\times \hat{r}}{\vert \vec{r}-\vec{r}’\vert }d^3r’$</td>
      <td>continuous case</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>note that $\hat{r}=\vec{r}/\vert \vec{r}\vert$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}_{dip} = \frac{\mu_0}{4\pi} \frac{\vec{m}\times \hat{r}}{\vert \vec{r}-\vec{r}’\vert }$</td>
      <td>derived because we know the field of a single dipole</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Most generic form of finding $\vec{B}$ from given $\vec{M}$</td>
    </tr>
    <tr>
      <td>$\vec{A}$ from $\vec{M}$ using bound currents</td>
      <td>$\vec{A}=\frac{\mu_0}{4\pi} \int_{all} \frac{\vec{J}_{b}}{\vert \vec{r}-\vec{r}’\vert }d^3r’+\frac{\mu_0}{4\pi}\oint \frac{\vec{K}_b}{\vert \vec{r} - \vec{r}’\vert }dA’$</td>
      <td>mathematically same as above, derived from the continuous case</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{J}<em>b \equiv \nabla \times \vec{M},\quad \vec{K}</em>{b} \equiv \vec{M}\times \hat{n}$</td>
      <td>for $\hat{n}$ points from the magnetized material to vacuum</td>
    </tr>
    <tr>
      <td>Physical Interpretation of Bound Charges</td>
      <td>$\vec{K}_b = \vec{M}\times \hat{n}$</td>
      <td>surface current = current on the surface of material = treat as normal current and compute $\vec{B}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130143248855.png" alt="image-20230130143248855" style="zoom: 33%;" /></td>
      <td>imagine uniform magnetized material = uniform tiny current loops</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>Hence inner loops cancel, we only have net outer current</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{J}_b = \nabla \times \vec{M}$</td>
      <td>If non-uniform magnetized material, then net $\vec{J}_b$ comes from difference in current loops</td>
    </tr>
    <tr>
      <td>Auxiliary Field $\vec{H}$</td>
      <td> </td>
      <td>same motivation as $\vec{D}$, now in magnetism we have both contributions from $J_f$ and $J_b$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$</td>
      <td>more useful than $\vec{B}$ when we have magnetized material</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\vec{H}$ often in the <strong>same direction</strong> as $\vec{B}$ and $\vec{M}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>The magnetic parallel of $\vec{E}$ field. ($\vec{B}$ would be a parallel to $\vec{D}$ instead)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\nabla \times \vec{B} = \mu_0 \vec{J}<em>{total} = \mu_0 (\vec{J}_b + \vec{J}</em>{free})$</td>
      <td>derivation from Ampere’s Law, then use $\vec{J}_b = \nabla \times \vec{M}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>true in general</td>
    </tr>
    <tr>
      <td>Differential and Integral form of $\vec{H}$</td>
      <td>$\nabla \times \vec{H} = \vec{J}_{free}$</td>
      <td>follows from the above</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\oint \vec{H}\cdot d\vec{l} =\int \vec{J}<em>{free} \cdot d\vec{A}=I</em>{free_{enc}}$</td>
      <td>very useful when we have symmetry</td>
    </tr>
    <tr>
      <td>Linear Material</td>
      <td>$\vec{M} \equiv \chi_m \vec{H}$</td>
      <td>$\vec{J}<em>{free} \to \vec{B}</em>{free}\to \vec{M}$ if material is magnetizable</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{B}= \mu_0 (1+ \chi_m)\vec{H}=\mu \vec{H}$</td>
      <td>derived from $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mu = \mu_r \mu_0 \equiv \mu_0(1 + \chi_m)$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><em>Calculate $\vec{B}$ from Bound Currents</em></p>

<p>Suppose you are given a material with frozen in magnetization such that $\vec{M}=M\hat{e}_z$ inside the cylinder. The question is what is the magnetic field everywhere.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Setup</th>
      <th style="text-align: center">Setup with Bound Currents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130210609692.png" alt="image-20230130210609692" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130210622916.png" alt="image-20230130210622916" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>We know we can get $\vec{B}$ either:</p>

<ul>
  <li>calculate $\vec{A}$ then do $\vec{B}=\nabla \times \vec{A}$. You wil see that we have many ways to find $\vec{A}$
    <ul>
      <li>if finite currents to integrate through, consider treating them as rotating charge = current loops $\vec{A}<em>{dip} = \int d\vec{A}</em>{dip}^{loop}$</li>
      <li>or compute $\vec{A}$ from the bound charges using the generic formula above (a lot of work)</li>
    </ul>
  </li>
  <li>calculate $\vec{B}$ directly by treating bound currents as currents (faster here)</li>
  <li>calculate using $\vec{H}$, and then convert back with $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$ (works, left to you as a mental exercise)
    <ul>
      <li>furthermore, if $\vec{J}_b=0$ and $\nabla \cdot \vec{M}=0$, then can use $\vec{H}=-\nabla W$ and solve laplacian</li>
    </ul>
  </li>
</ul>

<p>We first compute the bound currents and notice that:</p>

\[\begin{align*}
\vec{J}_b &amp;= \nabla \times \vec{M} = \vec{0}\\
\vec{K}_b &amp;= \vec{M}(\vec{r})|_{surf}\times \hat{n} = \begin{cases}
0, &amp; \text{at top/bottom with }\hat{n} = \hat{e}_z\\
M\hat{e}_\phi &amp; \text{at sides with }\hat{n} = \hat{e}_r
\end{cases}
\end{align*}\]

<p>With this we have the graph on the right, for currents going around the cylinder. In this case, we know that $\vec{B}=B_z\hat{e}_z$, which makes the computation straightforward.</p>

<p>Then we can easily compute $\vec{B}$ given this current by drawing ampere loop:</p>

\[\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc}\]

<p>Knowing that $\vec{B}_{outer}=\vec{0}$ from a solenoid, we have:</p>

\[B_z(r)L - 0 = \mu_0 L K_b = \mu_0LM\]

<p>Hence we obtain</p>

\[\vec{B}(\vec{r}) = \begin{cases}
\mu_0 M \hat{e}_z, &amp; r &lt; a\\
0, &amp; r &gt; a
\end{cases}\]

<p>which makes sense by right hand rule.</p>

<hr />

<p><em>Calculate field using $\vec{H}$ from given $\vec{M}$</em>:</p>

<p>Consider given that $\vec{M} = kr^2 \hat{e}_{\phi}$ inside a cylinder</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Setup</th>
      <th style="text-align: center">Ampere Loop</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130211413434.png" alt="image-20230130211413434" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230130211426797.png" alt="image-20230130211426797" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Then we first know that the field $\vec{B},\vec{H}$ would therefore also be in $\hat{e}_{\phi}$ direction (do not confuse $\vec{M}$ with bound currents here). Hence drawing a Ampere loop inside:</p>

\[\oint \vec{H}\cdot d\vec{l} = 2\pi r H_\phi(r)= I_{free_{enc}} = 0\]

<p>Hence we easily get $\vec{H}=0$ everywhere. Finally using $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$ we get back $\vec{B}$ field by:</p>

\[\vec{B} = \begin{cases}
\mu_0 kr^2 \hat{e}_\phi, &amp; r &lt; a\\
0, &amp; r &gt; a
\end{cases}\]

<p>since we know $\vec{M}$ as they are given.</p>

<ul>
  <li>an exercise of this would be to compute using bound currents and arrive at the same solution here.</li>
</ul>

<hr />

<p><em>When to use $\vec{H}$</em></p>

<p>In general, we can do ampere loops with $\vec{B}$ if we are sure we know all contributions (e.g. from current and from $\vec{M}<em>{induced}$). But consider the case where you have a <strong>magnetizable cylinder</strong> with some external currents $\vec{J}</em>{free} = I/(\pi R^2)\hat{e}_z$ following through</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20220503013832964.png" alt="image-20220503013832964" style="zoom:67%;" /></p>

<p>The finding $\vec{B}$ naively using the loop above would give wrong result:</p>

\[2\pi B_\phi(r) = \mu_0 I \pi \left( \frac{r}{R}\right)^2\]

<p>but then you have ignored $\vec{M}_{induced}$ which could cause other components in $\vec{B}$. Therefore, the correct way is to consider $\vec{H}$:</p>

\[2\pi H_\phi(r) = I \pi \left( \frac{r}{R}\right)^2\]

<p>then using $\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$ we get $\vec{B}$ (assuming $\vec{M}$ is linear)</p>

<h1 id="chapter-7-electrodynamics">Chapter 7 Electrodynamics</h1>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ohm’s Law</td>
      <td>$\vec{J}=\sigma \vec{E} = nq\vec{v}$</td>
      <td>when you here you have a conductor, this applies</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{J}=\sigma \vec{f}=\sigma(\vec{E}+\vec{v}\times \vec{B})$</td>
      <td>how the above comes about = charges move given the force $\gets$ $\vec{B}$ is generally ignored</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>consistent with perfect conductor $\vec{E}_{meat}=0$, which refers to steady state position</td>
    </tr>
    <tr>
      <td>Mean free path $\lambda$</td>
      <td>$\bar{v} = \frac{1}{2}at = {a \lambda}/{2v_{thermo}}$</td>
      <td>why is current $\propto \vec{E}$ consistent with $\vec{F}=m\vec{a}\propto \vec{E}$? Because electrons zig-zags in material</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>hence what mattered is their average velocity $\propto \vec{E}$</td>
    </tr>
    <tr>
      <td>Resistance (easy)</td>
      <td>$R=\frac{l}{A\sigma}=\rho\frac{l}{A}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301002643207.png" alt="image-20230301002643207" style="zoom:33%;" /></td>
      <td>derived from $\vec{J}=I/A=\sigma \vec{E}$ in wire, then since $\vec{E}l=V$, you get $I(A/l)=\sigma V$</td>
    </tr>
    <tr>
      <td>EMF</td>
      <td>$\varepsilon = \oint {\vec{F}_{source}}/{q}\cdot dl$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301003229532.png" alt="image-20230301003229532" style="zoom:33%;" /></td>
      <td>fundamentally comes from why current is uniform: there are two forces acting on the electrons: $F_{total}=F_{ext}+ qE$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>then if we consider the <em>net effect over the entire loop</em> $\oint \vec{E}\cdot d\vec{l}=0$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>an example of $F_{ext}$ could be battery providing chemical power/”force”</td>
    </tr>
    <tr>
      <td>Motional EMF</td>
      <td>$\varepsilon = -d\Phi / dt$</td>
      <td>insight from below. Same physics as Lorentz force</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\varepsilon = \oint (qvB/q) \ dl=vBl$</td>
      <td>because magnetic force $q\vec{v}\times \vec{B}$ contributes to $F_{total}$, but integral over a loop is non-zero</td>
    </tr>
    <tr>
      <td>Induced Electric Field/Faraday’s Law</td>
      <td>$\nabla \times \vec{E} = -\partial \vec{B} / dt$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301004446864.png" alt="image-20230301004446864" /></td>
      <td>all three methods create moving charges = current, but only the first one is due to magnetic force (Lorentz)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>In the second and third, charges are static but we know they still moved $\gets$ there is an <em>induced electric field</em></td>
    </tr>
    <tr>
      <td> </td>
      <td>$-d\Phi /dt = \oint \vec{E}\cdot d\vec{l}=\varepsilon$ (by definition)</td>
      <td>changing magnetic field <em>induce electric field $\vec{E}$</em></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>from this $\varepsilon$ you can find the induced current</td>
    </tr>
    <tr>
      <td>Lenz’ Law</td>
      <td>nature abhors change in flux</td>
      <td>used to determine the direction of induced current from Faraday’s law</td>
    </tr>
    <tr>
      <td>Mutual Inductance</td>
      <td>$\Phi_1 = M_{12}I_2$</td>
      <td>$M_{12}$ would be the coefficient for the $\vec{B}_{21}$ field produced (from 2 on 1) as a function of $I_2$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$M_{12}=M_{21}$</td>
      <td>purely geometric, <mark>independent of current</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301010355036.png" alt="image-20230301010355036" style="zoom:33%;" /></td>
      <td>an example would be realizing $\Phi_1$ for the inner loop is just some geometric factor $\times I_2$. (recall that $B=\mu_0 n I$ for solenoid)</td>
    </tr>
    <tr>
      <td>Self Inductance</td>
      <td>$\Phi = LI$</td>
      <td>because its own current creates magnetic field</td>
    </tr>
    <tr>
      <td>Inductance</td>
      <td>$\Phi_1 = L_1 I_1 +M_{12}I_2$</td>
      <td>but in general we assume $I_2 » I_1$</td>
    </tr>
    <tr>
      <td>Back EMF</td>
      <td>$\varepsilon_{back} = -L dI/dt$</td>
      <td>derived from $\Phi=LI$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>therefore, $L$ is like <mark>inertia mass</mark>: the higher it is, harder for current to move/start moving</td>
    </tr>
    <tr>
      <td>Energy in circuit with $L$</td>
      <td>$U=(1/2)L I^2$</td>
      <td>derived from $W_q = -\varepsilon_{back}q$ which is the work done needed to push charges one loop when I tuned on the circuit</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>then take $/dt$ on both sides, and integrate get $\int dU = L \int IdI$</td>
    </tr>
    <tr>
      <td>Energy stored in magnetic field</td>
      <td>$U_B = (1/2\mu) \int_{\text{all space}} \vert B\vert ^2 dV$</td>
      <td>derived from the above, and using $LI = \Phi =  \int \vec{B}\cdot d\vec{A}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>consistent with the above. Hence can also be used to find $L$.</td>
    </tr>
    <tr>
      <td>Maxwell’s Equation</td>
      <td><mark>replacing the below</mark> to have $\nabla \times \vec{B}=\mu_0 \vec{J} +\mu_0 \epsilon_0 \frac{\partial \vec{E}}{\partial t}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301013726090.png" alt="image-20230301013726090" /></td>
      <td>realizing this form before is inconsistent if you test $\nabla \cdot (\nabla \times \vec{B})$ in case of <em>electrodynamics</em>, i.e. $d\rho /dt \neq 0$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>now, this means changing $\vec{B}$ will change $\vec{E}$, and <strong>also vice versa</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>essentially, these equations tells you how <mark>charges/currents produce fields</mark>.</td>
    </tr>
    <tr>
      <td>Change in polarization $\vec{P}$ cause new current</td>
      <td>$\frac{\partial \vec{P}}{dt}=\vec{J}_p$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301112055421.png" alt="image-20230301112055421" style="zoom: 50%;" /></td>
      <td>if $\vec{P}$ increased, then it means we also have changed $\sigma_b$: $dI = \frac{\partial \sigma_b}{dt}dA$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>since $\sigma_b = \vec{P}\cdot \hat{n}$, we get $dI/da = \partial \vec{P}/dt$</td>
    </tr>
    <tr>
      <td>Current and Charge in material</td>
      <td>$\rho_{total} = \rho_{free} + \rho_b = \rho_{free} - \nabla\cdot\vec{P}$</td>
      <td>since we can only control free charge, we want to express field <em>only as a function of free charge</em></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{J}<em>{total} = \vec{J}</em>{free} + \vec{J}<em>b + \vec{J}_p = \vec{J}</em>{free}+\nabla\times \vec{M} + \frac{\partial \vec{P}}{\partial t}$</td>
      <td>then we just plug those in Maxwell’s equation, which holds for $\vec{E}<em>{total}$ and $\vec{B}</em>{total}$</td>
    </tr>
    <tr>
      <td>Maxwell’s Equation in <strong>Material</strong></td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301112719153.png" alt="image-20230301112719153" /></td>
      <td>why is there a difference? Inside material we need to also consider effect from polarization $\vec{P}$ and magnetization $\vec{M}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\vec{D}=\epsilon_0\vec{E}+\vec{P}$<br />$\vec{H}=\frac{1}{\mu_0}\vec{B}-\vec{M}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>compared to the one in vacuum, there is an <em>extra</em> term $\partial D / \partial t\equiv \vec{J}_D$ named to be displacement current</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>e.g. inside a <strong>conducting material</strong>, you will have $\vec{J}_{f}=\sigma\vec{E}$ free charges moving and another $\vec{J}_D=\partial \vec{D}/dt$ if there is a change in field.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>summarizes the fields to <em>include contribution from polarization and magnetization</em>, but as a function of <em>free charges and current</em></td>
    </tr>
    <tr>
      <td>linear material</td>
      <td>$\vec{D}=\epsilon \vec{E}$; $\vec{H} = \frac{1}{\mu}\vec{B}$</td>
      <td>often used to simplify Maxwell’s equations in material to get only E and B</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>if you insert this into Maxwell’s equation in material and use $\mu \to \mu_0, \epsilon\to \epsilon_0$, you <mark>get back the equation in vaccum</mark>!</td>
    </tr>
    <tr>
      <td>Integral form of Maxwell’s Equations</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304000201785.png" alt="image-20230304000201785" /></td>
      <td>can be used to calculate fields</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>also for later chapters, essentially determines the <strong>boundary conditions</strong> when solving equations</td>
    </tr>
    <tr>
      <td>Boundary Conditions</td>
      <td>$D^\perp_1 - D^\perp_2 = \sigma_{free}$<br />$B^\perp_1 - B^\perp_2 = 0$</td>
      <td>derived using the geometry below</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304000717046.png" alt="image-20230304000717046" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{E}^\parallel_1 - \vec{E}^\parallel_2 = 0$<br />$\vec{H}^\parallel_1 - \vec{H}^\parallel_2 = \vec{K}_{free}\times \hat{n}$</td>
      <td>derived using the geometry below.</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304000742426.png" alt="image-20230304000742426" style="zoom: 33%;" /></td>
      <td>$\vec{K}<em>{free}$ comes from the possibility $I</em>{f_{enc}}\neq0$, and $\times \hat{n}$ is due to the direction requirement that $I_{f_{enc}} = \vec{K}_f \cdot (\hat{n}\times \vec{l})=(\vec{K}_f \times \hat{n})\cdot \vec{l}$</td>
    </tr>
    <tr>
      <td>Boundary Conditions for Linear Media</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304001455829.png" alt="image-20230304001455829" /></td>
      <td>simply when $\vec{D}=\epsilon \vec{E}$; $\vec{H} = \frac{1}{\mu}\vec{B}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>used quite often as we almost always have a linear media and this form is <strong>only in $\vec{E}$ and $\vec{B}$</strong></td>
    </tr>
  </tbody>
</table>

<p><em>Connection between electrostatics, magnetostatics, induced field:</em></p>

<table>
  <thead>
    <tr>
      <th>Electrostatics</th>
      <th>Magnetostatics</th>
      <th>Faraday’s Induced E field</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\nabla \cdot \vec{E}=0$</td>
      <td>$\nabla \cdot \vec{B}=0$</td>
      <td>$\nabla \cdot \vec{E}=0$</td>
    </tr>
    <tr>
      <td>$\nabla \times \vec{E}=0$</td>
      <td>$\nabla \times \vec{B}=\mu_0 \vec{J}$</td>
      <td>$\nabla \times \vec{E} = -\partial \vec{B} / dt$</td>
    </tr>
    <tr>
      <td>$\oint \vec{E}\cdot d\vec{l}=0$</td>
      <td>$\oint \vec{B}\cdot d\vec{l}=\mu_{0} I_{enc}$</td>
      <td>$\oint \vec{E}\cdot d\vec{l}=-d\Phi / dt$</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>Transformer</em> and using Inductance.</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301011348391.png" alt="image-20230301011348391" style="zoom:33%;" /></p>

<p>Consider the task is to find out (a) show that $V_1/V_2 = N_1/N_2$, (b) that $M^2=L_1L_2$, and (c) the differential equation to solve for $I_1(t),I_2(t)$</p>

<ul>
  <li>
    <p>let the magnetic flux for a single turn be $\Phi_0 = \vert \vec{B}\vert A$. Then we get</p>

\[\varepsilon_1 = N_1 \frac{d\Phi_0}{dt},\quad \varepsilon_2 = N_2 \frac{d\Phi_0}{dt}\]

    <p>then just divide</p>
  </li>
  <li>
    <p>We know that also</p>

\[\Phi_1 = L_1I_1 + M_{12}I_2 = N_1 \Phi_0\\
\Phi_2 = L_2I_2 + M_{21}I_1 = N_2 \Phi_0\]

    <p>then eliminating $\Phi_0$, and find</p>

\[\frac{L_1}{N_1}I_1 + \frac{M_{12}}{N_1}I_2 = \frac{L_2}{N_2}I_1 + \frac{M_{21}}{N_2}I_1\]

    <p>but <mark>as $M$ is purely geometric</mark>, this should hold even if $I_1=0$ and when $I_2=0$. From which you can show that $M_{12}M_{21}=L_1L_2$</p>
  </li>
  <li>
    <p>Let there be an external power giving $V_1\cos(\omega t)$ at $\varepsilon_1$. Then for this to hold, we must have:</p>

\[\varepsilon_1 = -\frac{d\Phi_1}{dt} = -(L_1\dot{I_1} + M_{12}\dot{I}_2) = -V_1 \cos(\omega t)\]

    <p>so basically ODE with $dI/dt$ but coupled</p>
  </li>
</ul>

<hr />

<p><em>How do you find inductance $L$</em>?</p>

<p>One trick is to realize that</p>

\[\Phi = LI = \int \vec{B}\cdot d\vec{A}\]

<p>then you just need to massage the RHS to have $(\text{something})\cdot I$.</p>

<p>Another trick is to use energy. Since energy stored of the circuit = energy of the fields, e.g. for a solenoid</p>

\[U = \frac{1}{2\mu_0}\int |B|^2 dV = \frac{1}{2}LI^2\]

<p>and there are no electric fields as wires are charge neutral.</p>

<hr />

<p><em>Using Maxwell’s Fix on Ampere’s Law</em></p>

<p>Consider finding $\int BdA$ given this Amperian loop.</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301110149680.png" alt="image-20230301110149680" style="zoom: 50%;" /></p>

<p>From $\nabla \times \vec{B}=\mu_0 \vec{J} +\mu_0 \epsilon_0 \frac{\partial \vec{E}}{\partial t}$, this means</p>

<ol>
  <li>
    <p>using the circular disk are:</p>

\[\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc} = \mu_0 I\]
  </li>
  <li>
    <p>if we use the version $\nabla \times \vec{B}=\mu_0 \vec{J}$, then we will get $0$. But:</p>

\[\oint \vec{B}\cdot d\vec{l} = \mu_0 I_{enc} + \mu_0 \epsilon_0 \int_S \frac{\partial \vec{E}}{\partial t}\cdot d\vec{A}\]

    <p>since there is no $I_{enc}=0$, but the electric field within the gap is changing as $\vec{E}(t)=\sigma(t)/\epsilon_0$:</p>

\[\oint \vec{B}\cdot d\vec{l} = \mu_0 \epsilon_0 \int_S \frac{\partial }{\partial t}\frac{Q(t)}{\epsilon_0 A}\cdot d\vec{A}\]

    <p>since $dQ/dt=I$, we recover the same expression as 1.</p>
  </li>
</ol>

<h1 id="chapter-8-conservation-laws">Chapter 8 Conservation Laws</h1>

<p>Basically <strong>how we get energy and momentum from fields themselves</strong> = found that in order for conservation of energy/momentum to work, <em>we need to attach energy and momentum to fields</em>. How do we know things <em>are conserved</em>?</p>

<ul>
  <li>if energy $U$ is conserved, then $dU/dt=0$</li>
  <li>if momentum $\vec{p}$ is conserved, then $d\vec{p}/dt = 0$</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Work Energy Theorem in Electrodynamics</td>
      <td>$\frac{dW}{dt}=-\frac{\partial}{\partial t}\int_{V}\underbrace{\left[ \frac{1}{2}\epsilon_0 E^2 + \frac{1}{2\mu_0}B^2\right]}<em>{u</em>{EM}}dV - \int_{\partial V}\underbrace{\frac{1}{\mu_0}(\vec{E}\times \vec{B})}_{\vec{S}}\cdot d\vec{a}$</td>
      <td>$dW$ here represent work done <strong>by EM field</strong> on <strong>charges</strong> in a volume $V$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>fundamentally, think about how to make charges move = $F=q(\vec{E}+\vec{v}\times \vec{B})$, when find $dW/dt$ in terms of fields.</td>
    </tr>
    <tr>
      <td> </td>
      <td>$u_{EM} = \frac{1}{2}(\epsilon_0 E^2 + \frac{1}{\mu_0}B^2)$</td>
      <td>$u_{EM}$ represent energy stored in the field in $V$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\vec{S}$ represent energy per unit area per time = energy flux going <strong>out</strong> of $V$ since $d\vec{a}$ points out</td>
    </tr>
    <tr>
      <td> </td>
      <td>$dW = \vec{F}\cdot d\vec{r} =q\vec{E}\cdot{\vec{v}}dt$, then $dW/dt = \int \vec{J}\cdot \vec{E}dV$ for a density of current</td>
      <td>derivation</td>
    </tr>
    <tr>
      <td>Poynting Vector</td>
      <td>$\vec{S}=\frac{1}{\mu_0}\vec{E}\times \vec{B}$</td>
      <td>energy flux, and also the <strong>direction of energy flowing</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\frac{\partial }{\partial t}\int_V u dV = - \int_{\partial V}{\frac{1}{\mu_0}(\vec{E}\times \vec{B})}\cdot d\vec{a}$</td>
      <td>why? Consider energy is conserved, $dW/dt=0$. Then if LHS decreases, energy is flowing <strong>out</strong> and $\vec{S}$ is positive if $\vec{S}$ also points out</td>
    </tr>
    <tr>
      <td>Need for Maxwell’s Stress Tensor</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304014504261.png" alt="image-20230304014504261" style="zoom:33%;" /></td>
      <td>Consider the instantaneous fields produced causing those forces. Note that net force $\neq 0$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so momentum is <em>naively not conserved</em> (need to take the fields into account)</td>
    </tr>
    <tr>
      <td>Property of 2D Tensor</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304020412812.png" alt="image-20230304020412812" style="zoom:33%;" /></td>
      <td>one way to visualize $\overleftrightarrow{T}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{v}=\begin{bmatrix} v^1\v^2\v^3 \end{bmatrix}=v^i$</td>
      <td>upper index is like a vector component (e.g. $\vec{r} = \begin{bmatrix} x\y\z \end{bmatrix}$)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>in our use, upper index often represents <strong>coordinate</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$w_i = [w_1, w_2, w_3]$</td>
      <td>lower index usually row vector, or for convenience</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>in our use, lower index often represents <strong>derivative of a coordinate</strong></td>
    </tr>
    <tr>
      <td>Einstein’s Summation Notation</td>
      <td>sum over repeated index</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\left( \vec{a} \cdot \overleftrightarrow{T} \right)<em>j = \sum</em>{i}a_i T_{ij}$ = a vector</td>
      <td>basically we want the $j$-th vector after this dot product.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>in the above visualization, $\vec{a}$ dots into $\overleftrightarrow{T}$ vertically</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\nabla \cdot \vec{T}^x = \partial_j T^{xj}$</td>
      <td>an example, so $\vec{T}^x$ is the horizontal vector</td>
    </tr>
    <tr>
      <td>Maxwell’s Stress Tensor</td>
      <td>$T^{ij}=\epsilon_0(E^iE^j - \frac{1}{2}\delta_{ij}E^2)+\frac{1}{\mu_0}(B^iB^j - \frac{1}{2}\delta_{ij}B^2)$</td>
      <td>so $T$ is a 2D tensor (matrix)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{F}_{\text{EM on charges}} = \oint \overleftrightarrow{T}\cdot d\vec{a} - \mu_0 \epsilon_0 \frac{d}{dt}\int_V \vec{S}dV$</td>
      <td>explains why we need the stress tensor</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>in my practice, even though this is used more often then the differential form, it is <mark>better to start your thinking with the differential form</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304015205849.png" alt="image-20230304015205849" style="zoom:33%;" /></td>
      <td>derived from considering <strong>force acting on charges</strong> due to EM fields, so $\vec{F}=q(\vec{E}+ \vec{v}\times \vec{B})=\int_V (\rho \vec{E}+\vec{J}\times \vec{B})dV$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$T_{ij}$ represents force acting in the i-th direction on surface oriented in $j$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{F}_{EM}=\oint \overleftrightarrow{T}\cdot d\vec{a}$</td>
      <td>why? if there is no change in fields $d\vec{S}/dt =0$. Then we have this</td>
    </tr>
    <tr>
      <td>Momentum of Fields (and conservation)</td>
      <td>$\frac{d}{dt}\vec{P}<em>{\text{mechanical}} = - \frac{d}{dt}\int_V \underbrace{\epsilon_0\mu_0\vec{S}}</em>{\vec{g}<em>{EM}}dV + \oint_S \underbrace{\overleftrightarrow{T}}</em>{\text{momentum flux}}\cdot d\vec{a}$</td>
      <td>re-written the same force formula from Maxwell’s Stress Tensor</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\frac{d\vec{g}<em>{mech}}{\partial t}=\vec{f}</em>{mech} = \nabla \cdot \overleftrightarrow{T} - \frac{\partial \vec{g}_{EM}}{\partial t}$</td>
      <td>differential form of the above</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\frac{d\vec{g}<em>{mech}^i}{\partial t}=\vec{f}</em>{mech}^i = \partial_jT^{ij} - \frac{\partial \vec{g}_{EM}^i}{\partial t}$</td>
      <td>equation for each component (e.g $i=x,y,z$)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>the integral form is used more in practice, but this may be easier to remember and better to start with</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\frac{d}{dt}\int_V {\epsilon_0\mu_0\vec{S}}dV = \oint_S {\overleftrightarrow{T}}\cdot d\vec{a}$</td>
      <td>how do you interpret this? When there is no change in momentum of charges:</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{g}_{EM} = \mu_0 \epsilon_0 \vec{S}$</td>
      <td>therefore $\overleftrightarrow{T}$ represents momentum flux <strong>passing through</strong> an area; then naturally $\epsilon_0\mu_0\vec{S}$ is the <strong>momentum density inside</strong> volume $V$</td>
    </tr>
    <tr>
      <td>Angular momentum of Fields</td>
      <td>$\vec{l}<em>{EM} = \vec{r}\times \vec{g}</em>{EM}$</td>
      <td>since it has momentum, the the laws apply</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{L}_{EM} = \int_V \vec{l}dV$</td>
      <td>integral form, angular momentum in a volume</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>questions on this usually involve some charged body <em>starts spinning</em> after there is a change in E or B field (e.g. switched off). This can be explained using the $\vec{l}_{EM}$ in fields</td>
    </tr>
  </tbody>
</table>

<p><em>Example using Poynting Vector</em></p>

<p>Consider a current flowing down a wire. What is the power in this wire using Poynting vector? Hint: $P=$energy per unit time delivered</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304013033237.png" alt="image-20230304013033237" style="zoom:33%;" /></p>

<p>Well, if we know the $\vec{E}$ and $\vec{B}$ field <em>just on the boundary</em>, then</p>

\[\int \vec{S}\cdot d\vec{a} = \text{energy flowing out of }\mathcal{V}\]

<p>since since we know $\vec{E}=V/L$, $\vec{B}<em>{surf} = \mu_oI / 2\pi a \,\, \hat{r}</em>\phi$, we get:</p>

\[\vec{S}_{surf} = \frac{VI}{L 2\pi a}\ \text{inwards}\]

<p>therefore, energy flows inwards, with power:</p>

\[P = \int Sda = \frac{VI}{L2\pi a}2\pi a L = VI\]

<hr />

<p><em>A summary of work-energy and momentum equations</em></p>

<p>Perhaps the differential form is easier to remember:</p>

\[\begin{align*}
\text{work-energy conservation}\qquad &amp;\frac{dw_{mech}}{dt} = -\nabla \cdot \vec{S} - \frac{\partial u_{EM}}{dt}\\
\text{momentum conservation}\qquad &amp; \frac{dg_{mech}^x}{\partial t} = \nabla \cdot \overleftrightarrow{T}^x - \frac{\partial g_{em}^x}{dt}\\
&amp; \frac{dg_{mech}^y}{\partial t} = \nabla \cdot \overleftrightarrow{T}^y - \frac{\partial g_{em}^y}{dt}\\
&amp; \frac{dg_{mech}^z}{\partial t} = \nabla \cdot \overleftrightarrow{T}^z - \frac{\partial g_{em}^z}{dt}\\
\text{charge conservation}\qquad &amp;0 = \nabla \cdot \vec{J} + \frac{\partial \rho}{\partial t}
\end{align*}\]

<p>where note that</p>

<ul>
  <li>$w_{mech}$ is the work density, i.e. $W_{mech}=\int w_{mech}dV$</li>
  <li>each equation above <strong>returns a scalar</strong></li>
  <li>$\overleftrightarrow{T}^x$ would represent the first <strong>row</strong> of the stress tensor, so that $\nabla \cdot \overleftrightarrow{T}^x = \partial_i T^{xi}$</li>
</ul>

<blockquote>
  <p>The take away message here is that <strong>field itself has momentum and energy $u_{EM}$ and $g_{EM}$</strong></p>
</blockquote>

<hr />

<p><em>Example of using Stress Tensor</em>: Calculate the <strong>force</strong> acting on the upper hemisphere of a uniformly charged sphere:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304151816115.png" alt="image-20230304151816115" style="zoom:50%;" /></p>

<p>Well, since this is about force, we can consider</p>

\[\vec{F}_{\text{EM on charges}} = \oint \overleftrightarrow{T}\cdot d\vec{a} - \mu_0 \epsilon_0 \frac{d}{dt}\int_V \vec{S}dV\]

<p>then this means the closed surface is bowl + disk. First since there is no time dependence, and since we know the net force <em>will be in $\hat{z}$</em>:</p>

\[\vec{F}_{\text{EM on charges}} = \oint \overleftrightarrow{T}\cdot d\vec{a} \quad\to\quad  {F}_{z} = \oint \overleftrightarrow{T}^z\cdot d\vec{a} = \oint T^{zi} da_i\]

<ul>
  <li>
    <p>force on bowl: the area vector is in spherical coordinate</p>

\[d\vec{a} = R^2 \sin(\theta)d\theta d\phi\ \hat{e}_r\]

    <p>and using the formula for $T^{ij}$:</p>

\[T^{ij}=\epsilon_0(E^iE^j - \frac{1}{2}\delta_{ij}E^2)+\frac{1}{\mu_0}(B^iB^j - \frac{1}{2}\delta_{ij}B^2)\]

    <p>since $B=0$, and you know that $\vec{E}$ on the surface is, again in spherical coordinate:</p>

\[\vec{E} = \frac{1}{4\pi \epsilon_0}\frac{Q}{R^2}\ \hat{e}_r,\quad \hat{e}_r=\sin\theta \cos\phi\hat{e}_x + \sin\theta \sin\phi\hat{e}_y + \cos\theta\hat{e}_z\]

    <p>then compute:</p>

\[T^{zi}da_i = T^{zx}da_x + T^{zy}da_y + T^{zz}da_z\]

    <p>and do the integral</p>
  </li>
  <li>
    <p>repeat for the disk, using the fact that field inside should go, in polar coordinate:</p>

\[\vec{E}_{disk}=\frac{1}{4\pi \epsilon_0}\frac{Qr}{R^3}\hat{r}\]

    <p>and</p>

\[d\vec{a} = -r drd\phi \hat{z}\]
  </li>
</ul>

<p>but you will see a trick if you <strong>start thinking with</strong> the local form</p>

\[\vec{f}_{mech} = \nabla \cdot \overleftrightarrow{T} - \frac{\partial \vec{g}_{EM}}{\partial t}\]

<p>so that, since <strong>outside the sphere there is no charge</strong>, $\vec{f}_{mech}=0$. Hence we can consider <strong>integrating over a different surface</strong></p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304153829185.png" alt="image-20230304153829185" style="zoom:33%;" /></p>

<p>where if we stretch this to be very big, $E\to 0$ at the bowl. Hence we <strong>only have the horizontal disk component to integrate</strong></p>

<ul>
  <li>
    <p>force on the outer disk (not touching the charge): in the radial direction</p>

\[\vec{E}_{outer} =\frac{1}{4\pi \epsilon_0}\frac{Q}{r^2}\hat{r}\]

    <p>with area for $F_z$</p>

\[d\vec{a} = -r drd\phi \hat{z}\]

    <p>this is much easier then to compute and integrate</p>

\[T^{zi}da_i = T^{zx}da_x + T^{zy}da_y + T^{zz}da_z = T^{zz}da_z\]
  </li>
  <li>
    <p>repeat for the inner disk, which is the same as our previous method</p>
  </li>
</ul>

<hr />

<p><em>Example of Conservation of Angular Momentum</em> (see example 8.4 on textbook) consider a very long solenoid with $n$ turns per length, and inside/outside of it are two cylindrical shell of length $l$ with radius $a,b$, charged with $Q,-Q$ respectively. When $I$ in the solenoid is <strong>switched off</strong> $I \to 0$, the two cylinders will start to <strong>spin</strong>. Physically why did this happen? Is angular momentum conserved?</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304155019677.png" alt="image-20230304155019677" style="zoom:33%;" /></p>

<p>The key mechanism is that there is a <em>change</em> in $B$ field as a result of switching off. Hence:</p>

<ul>
  <li>
    <p><strong>mechanism</strong>: there is a change in $B$, hence we get an induced electric field which would act on the charges</p>

\[\nabla \times \vec{E} = -\frac{\partial \vec{B}}{\partial t} \quad \to \quad \oint \vec{E}\cdot d\vec{l} = -\frac{\partial }{\partial t}\int \vec{B}\cdot  d\vec{A}\]

    <p>and since the current is changing, you will get something like:</p>

\[\vec{E}_{s&gt;R} = - \frac{1}{2}\mu_0n\frac{dI}{dt} \frac{R^2}{s}\ \hat{e}_\phi\\
\vec{E}_{s&lt;R} = - \frac{1}{2}\mu_0n\frac{dI}{dt} s\ \hat{e}_\phi\]

    <p>notice that this field is in the $\phi$ direction, hence the two charged cylinder will feel a torque</p>

\[\vec{N}_b = \vec{r}\times (-Q\vec{E})= \frac{1}{2}\mu_0n QR^2 \frac{dI}{dt} \ \hat{e}_z\]

    <p>then the angular momentum it has at the end is therefore, using $\vec{N}=d\vec{L}/dt$</p>

\[\vec{L}_{final}-0 = \int_0^t \frac{1}{2}\mu_0n QR^2 \frac{dI}{dt} dt\]

    <p>then do the same for $\vec{N}_a$ of the smaller cylinder.</p>
  </li>
  <li>
    <p><strong>conservation of angular momentum</strong>: you will see from above that $\vec{L}_a + \vec{L}_b$ in the end is not zero. So in the beginning there must be something that is spinning, the field:</p>

\[\vec{l}_{EM} = \vec{r}\times \vec{g}_{EM}\]

    <p>since the fields in the beginning is:</p>

\[\vec{E} = \frac{Q}{2\pi \epsilon_0 l} \frac{1}{s}, \quad a &lt; s &lt; b \\
\vec{B} = \mu_0 n I\ \hat{e}_z,\quad s&lt;R\]

    <p>therefore $\vec{g}_{EM}$ is only non-zero inside the solenoid $s &lt; R$. Hence</p>

\[\vec{g}_{EM} = - \frac{\mu_0 n I Q}{2\pi ls} \hat{e}_\phi\]

    <p>and since we only care about angular momentum in the z-direction:</p>

\[L_{EM}^z = \int_{\text{inside solenoid}} l_{EM}^z\ dV =  \int_{\text{inside solenoid}} (\vec{r} \times \vec{g}_{EM})_z \ dV\]

    <p>you will find this matches up with the angular momentum you found in part 1.</p>
  </li>
</ul>

<hr />

<p><strong>List of good questions:</strong></p>

<ul>
  <li>Problem 8.2: deal with Poynting vector</li>
  <li>Problem 8.7
    <ul>
      <li>(a),(b) also involves $da_{x},da_y$</li>
      <li>(c) = one way to interpret $T^{ij}$</li>
    </ul>
  </li>
</ul>

<h1 id="chapter-9-em-waves">Chapter 9 EM Waves</h1>

<p>Charges and current produce $E$ and $B$ field. The main idea is that in vacuum/material, <strong>field themselves need to obey wave equation</strong> if we were to also consider the <strong>time component</strong> of the field = propagate like waves.</p>

<ul>
  <li>
    <p>we will show that in many physical scenarios, solutions have to obey <strong>wave equation</strong>, e.g. in 1D</p>

\[\frac{\partial^2 y}{\partial x^2} - \frac{1}{v^2}\frac{\partial^2 y}{\partial t^2} = 0\]
  </li>
  <li>
    <p>one common solutions is sinusoidal (and you can do Fourier theorem with it to get other functions), and this chapter basically <strong>focus on sinusoidal solutions in complex notation</strong>, e.g.</p>

\[y(x,t)=\mathrm{RE}[\tilde{A}e^{i(kz-\omega t)} + \tilde{B}e^{i(-kz-\omega t)}]\]
  </li>
  <li>
    <p>therefore the <strong>physics</strong> lies in finding $\tilde{A},\tilde{B},k$ using <mark>boundary conditions</mark> derived from physical principles (e.g. $B^\perp_1 -B^{\perp}_2=0$)</p>
  </li>
</ul>

<h2 id="plane-waves-without-rho-and-vecj">Plane Waves without $\rho$ and $\vec{J}$</h2>

<ul>
  <li>Plane waves is a simple case of having the amplitudes being a constant in space, e.g. $\tilde{A}$</li>
  <li>without $\rho,\vec{J}$ means we are in vacuum for a good <em>insulator</em></li>
</ul>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Using Complex Notation</td>
      <td>$I(t) = \mathrm{RE}[\tilde{I}_0e^{i\omega t}]$</td>
      <td>if you know the solution of $I(t)$ is <strong>in the form of cosine/sine</strong>, then using this will <strong>almost always</strong> make math simpler</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tilde I_0=a+ib = I_0e^{i\phi}$</td>
      <td>so that if $\tilde{I}_0$ is complex, this means we have <strong>phase shifts!</strong> $\omega t \to \omega t + \phi$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304165436626.png" alt="image-20230304165436626" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>this form is also the nicest if you want to take the real part in the end, which becomes just $I_0 \cos(\omega t + \phi)$</td>
    </tr>
    <tr>
      <td>Wave “Definition”</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304165920613.png" alt="image-20230304165920613" /></td>
      <td>a “fixed” shape traveling at some velocity $v$</td>
    </tr>
    <tr>
      <td> </td>
      <td>e.g. $y(x-vt)$, traveling to the right</td>
      <td>the fixed shape would be $y(x)$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>e.g. $y(x-vt) = Ae^{-b(x-vt)^2}$ is a valid wave</td>
    </tr>
    <tr>
      <td>Wave on String</td>
      <td>$\frac{\partial^2 y}{\partial x^2} - \frac{1}{T/\mu}\frac{\partial^2 y}{\partial t^2} = 0$</td>
      <td>so that solution is in the form of $y(x,t)=f(x-vt)+g(x+vt)$ for any function $f,g$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><strong>as a result</strong>, $v^2 = T/\mu$ is the wave’s traveling speed</td>
    </tr>
    <tr>
      <td> </td>
      <td>$F_y = T\sin(\theta’)-T\sin(\theta) = (\mu\Delta z) \frac{\partial^2 y}{\partial t^2}$</td>
      <td>derivation, consider force acting on a small section $\Delta z$ of the string, and using $F_y=ma_y$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tan(\theta(x)) = \frac{\partial y}{\partial x}\vert _x$</td>
      <td>then using small angle approx $\sin \approx \tan$, we obtain the wave equation</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304170338866.png" alt="image-20230304170338866" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Sinusoidal Wave</td>
      <td>$y(x,t)=A\cos[k(x - vt)+\delta]$</td>
      <td>for waves in physics, sinusoidal form for $f,g$ is considered mostly because (a) it works well with B.C. and (b) any function can be built from them = Fourier theorem</td>
    </tr>
    <tr>
      <td> </td>
      <td>$y(x,t)=\mathrm{RE}[\tilde{A}e^{i(kz-\omega t)}]$</td>
      <td>in complex form</td>
    </tr>
    <tr>
      <td> </td>
      <td>$g(x,t)=\mathrm{RE}[\tilde{B}e^{i(-kz-\omega t)}]$</td>
      <td>traveling to the left</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>hence $k$ <mark>indicates direction of propagation</mark></td>
    </tr>
    <tr>
      <td>Important quantities by definition</td>
      <td>$\lambda = 2\pi / k$</td>
      <td>by considering $x\to x+(2\pi/k)$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$kv=\omega$, and $\tilde{A}=Ae^{i\delta}$</td>
      <td><mark>wave/phase velocity $v$</mark>!</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304172127708.png" alt="image-20230304172127708" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>start of physics stuff</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>B.C. simple reflection and transmission</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304173941953.png" alt="image-20230304173941953" /></td>
      <td>string extends to infinity at both ends, and given $\tilde{A}_I$, want to know $\tilde{A}_R,\tilde{A}_T$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>assume $\omega$ is the same on both string, hence $\omega=v_1k_1 = v_2k_2$ since velocity has to be different</td>
    </tr>
    <tr>
      <td> </td>
      <td>$y_&lt;=\tilde{A}_I e^{i(k_1x-\omega t)}+\tilde{A}_R e^{i(-k_1x-\omega t)}$</td>
      <td>waves at $x&lt;x_B$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$y_&gt; =\tilde{A}_T e^{i(k_2x-\omega t)}$</td>
      <td>waves at $x&gt;x_B$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$y(0<em>-, t)=y(0</em>+,t)$, $\frac{\partial y}{\partial x}(0<em>-,t)=\frac{\partial y}{\partial x}(0</em>+,t)$</td>
      <td>no B.C. but <mark>continuity constraints</mark>!</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tilde{A}_R = \frac{k_1 - k_2}{k_1 + k_2}\tilde{A}_I$</td>
      <td>solve and find</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tilde{A}_T = \frac{2k_1}{k_1 + k_2}\tilde{A}_I$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\delta_I=\delta_T$, then if $\mu_1 &gt; \mu_2$, $\delta_R = \delta_I$. Otherwise $\delta_R = \delta_I + \pi$</td>
      <td>this has to be <mark>given</mark> by knowing that $\mu_2 » \mu_1$ gives a standing wave like solution.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>In later questions/setups we will also be able to compute the phase shift $\delta$.</td>
    </tr>
    <tr>
      <td>Wave Equation for EM in vacuum</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304180459157.png" alt="image-20230304180459157" style="zoom:50%;" /></td>
      <td>derived from Maxwell’s eq assuming $\rho=\vec{J}=0$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so $v^2= 1/\mu_0\epsilon_0=c^2$, so EM waves propagate at speed of light</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>notice that this is 3D wave equation. In reality, we will see that when <mark>assuming plane waves solution</mark>, $E,B$ will be <mark>transverse</mark> = this reduces to a 1D wave equation</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304181007050.png" alt="image-20230304181007050" style="zoom:33%;" /></td>
      <td>derivation of the above</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304181024088.png" alt="image-20230304181024088" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Monochromatic Plane Waves</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304181731013.png" alt="image-20230304181731013" style="zoom: 50%;" /></td>
      <td>assuming that the amplitude vector <strong>is constant</strong> in all space, i.e. wave looks like a plane</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\mathbf{\tilde{E}}_0$ means it is a <em>constant <strong>vector</strong> but is complex</em></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304183205944.png" alt="image-20230304183205944" /></td>
      <td><mark>almost always the form to start with when solving plane waves question</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived using the “Property of Plane Waves”</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304181630251.png" alt="image-20230304181630251" style="zoom: 25%;" /></td>
      <td>up and down is modulated by $e^{i(kz-\omega t)}$ term</td>
    </tr>
    <tr>
      <td>Property of Plane Waves</td>
      <td>$\tilde{E}_z=\tilde{B}_z = 0$</td>
      <td>because $\nabla \cdot \vec{E}=\nabla \cdot \vec{B}=0$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so EM plane waves are <strong>transverse</strong> (w.r.t direction of propagation $z$)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{B}}_0 = \frac{1}{c}\hat{z}\times \mathbf{\tilde{E}}_0$</td>
      <td>if $\hat{z}$ is the direction of propagation. So $\mathbf{B}$ is perpendicular to $\mathbf{E}$ and to $\hat{z}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from $\nabla \times \mathbf = -\frac{\partial \mathbf}{\partial t}$</td>
    </tr>
    <tr>
      <td>Wave vector and polarization vector of Plane Waves</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304183620251.png" alt="image-20230304183620251" style="zoom:50%;" /></td>
      <td>$\hat{n}$ is the polarization vector, and $\vec{k}=k\hat{k}$ is the <mark>wave/propagation vector</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304181450411.png" alt="image-20230304181450411" /></td>
      <td>e.g. $\hat{k}=k\hat{z}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304184252710.png" alt="image-20230304184252710" style="zoom:50%;" /></td>
      <td>when you want to know $\vec{E}(\vec{r})$ everywhere in space</td>
    </tr>
    <tr>
      <td>Visualization of Longitudinal Wave</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304182209963.png" alt="image-20230304182209963" /></td>
      <td>this will be useful when considering wave guides = EM waves are not longer purely transverse. But, they are always <strong>transverse w.r.t direction of energy propagation $\vec{S}$</strong></td>
    </tr>
    <tr>
      <td>Energy in EM Waves</td>
      <td>$\lang u_{EM}\rang = \frac{1}{4}(\epsilon_0 \mathbf{\tilde{E}}\mathbf{\tilde{E}}^* + \frac{1}{\mu}\mathbf{\tilde{B}}\mathbf{\tilde{B}}^*)$</td>
      <td>assumes $\mathbf{E}=\mathbf{\tilde{E}}(x,y,z)e^{i(kz-\omega t)}$ and $\mathbf{B}=\mathbf{\tilde{B}}(x,y,z)e^{i(kz-\omega t)}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>time average $\lang u_{EM} \rang = \frac{1}{T}\int_0^T u_{EM}\ dt$ ends up a $1/2$ in front due to $\int \cos^2 dt$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\lang \vec{S} \rang = \frac{1}{2}\mathrm{RE}(\mathbf{\tilde{E}}\times \mathbf{\tilde{B}}^*)\equiv I$</td>
      <td><strong>Intensity</strong> is fact the “average power (E/t) per unit area”!</td>
    </tr>
    <tr>
      <td> </td>
      <td>$u_{EM} = \frac{1}{2}(\epsilon_0 E^2 + \frac{1}{\mu_0}B^2)$</td>
      <td>both of the above are derived from the following</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{S}=\frac{1}{\mu_0}\vec{E}\times \vec{B}$</td>
      <td>previously derived only from Maxwell’s eq, so still holds<br />$E^2 =\vec{E}<em>{total}\cdot \vec{E}</em>{total}$<br />$B^2 =\vec{B}<em>{total}\cdot \vec{B}</em>{total}$</td>
    </tr>
    <tr>
      <td>Energy in Monochromatic Plane Waves</td>
      <td>$\lang u_{EM} \rang = \frac{1}{2}\epsilon_0 E_o^2$</td>
      <td>simpler case since we get $\mathbf{E}=\tilde{E}_0e^{i(kz-\omega t)}\hat{x}$ and $\mathbf{B}=(\tilde{E}_0/c)e^{i(kz-\omega t)}\hat{y}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\lang \vec{S}_{EM}\rang = \frac{1}{2}c\epsilon_0 E_0^2 \hat{z}$</td>
      <td> </td>
    </tr>
    <tr>
      <td>EM Wave Equation in Matter</td>
      <td>$\nabla^2 \mathbf{E} - \epsilon\mu\frac{\partial^2 \mathbf{E}}{\partial t^2}=0, \nabla^2 \mathbf{B} - \epsilon\mu\frac{\partial^2 \mathbf{B}}{\partial t^2}=0$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304204811146.png" alt="image-20230304204811146" style="zoom: 50%;" /></td>
      <td>derived from realizing the difference between this set of Maxwell’s eq and the vacuum is just replacing $\epsilon_0\mu_0 \to \epsilon\mu$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$v^2 = 1/\epsilon\mu &lt; c^2$</td>
      <td>but now <strong>velocity</strong> is different</td>
    </tr>
    <tr>
      <td> </td>
      <td>$v\equiv c/n$</td>
      <td>$n$ is <mark>index of refraction</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>other than the above (and the B.C.), things are the same (e.g. $v=\omega k$)</td>
    </tr>
    <tr>
      <td>B.C. for EM Wave in Matter</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304205332821.png" alt="image-20230304205332821" /></td>
      <td>B.C. when <strong>crossing from medium 1 to medium 2</strong>, assuming both are linear material</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>was the one from Maxwell Eq linear material in <a href="#Chapter 7 Electrodynamics">Chapter 7 Electrodynamics</a></td>
    </tr>
    <tr>
      <td>Setup for EM Wave in Matter + Oblique Incidence</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304211511742.png" alt="image-20230304211511742" /></td>
      <td>still plane waves, but $\mathbf{\tilde{E}}<em>{0_I} = \tilde{E}</em>{0<em>I} \hat{n}</em>{I}$ and we <mark>need to consider $\vec{r}$</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304211518571.png" alt="image-20230304211518571" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304211629030.png" alt="image-20230304211629030" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304211416722.png" alt="image-20230304211416722" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>B.C. for EM Wave in Matter + Oblique Incidence</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304213207452.png" alt="image-20230304213207452" style="zoom:50%;" /></td>
      <td>derived from the B.C. in material + realizing <em>all</em> the exponential cancels out (see below)</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304211823199.png" alt="image-20230304211823199" style="zoom:50%;" /></td>
      <td>additional conclusion for the oblique case, derived by realizing the B.C. yields<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304212820953.png" alt="image-20230304212820953" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so imagine needing the below to hold for all $\vec{r}$ $A\cos(k_I\cdot \vec{r})+B\cos(k_R\cdot \vec{r})=C\cos(k_T\cdot \vec{r})$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304212103599.png" alt="image-20230304212103599" /></td>
      <td>hence at $z=0$ this has to be true for $\forall x,y$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$(k_I)_y=(k_R)_y=(k_T)_y$</td>
      <td>from letting $y=0$,</td>
    </tr>
    <tr>
      <td> </td>
      <td>$(k_I)_x=(k_R)_x=(k_T)_x$</td>
      <td>from letting $x=0$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304212035659.png" alt="image-20230304212035659" /></td>
      <td>derived then aligning $E_I$ into the x-z plane such that $(k_I)_y=0$. Then all other $(k_I)_y=(k_R)_y=(k_T)_y=0$, i.e. all are in x-z plane</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304212007096.png" alt="image-20230304212007096" /></td>
      <td>from $(k_I)_x=(k_R)_x$, and then realizing that $k_I = k_R$ since $\omega = kv$, both $v,\omega$ is the same for $k_I, k_R$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304212016744.png" alt="image-20230304212016744" /></td>
      <td>from $(k_I)_x=(k_T)_x$, and using $v=c/n$</td>
    </tr>
    <tr>
      <td>Solution for EM Wave in Matter + Oblique Incidence</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304213350101.png" alt="image-20230304213350101" style="zoom:50%;" /></td>
      <td>$\alpha= \cos\theta_T/\cos \theta_I$, $\beta = (\mu_1v_1)/(\mu_2v_2)$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304213423495.png" alt="image-20230304213423495" style="zoom:50%;" /></td>
      <td>derived from this and below + Snell’s law, which comes from the B.C., <em>assuming</em> $\vec{E},\vec{B}$ are also in the x-z plane (see below)</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304213427917.png" alt="image-20230304213427917" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230307200713584.png" alt="image-20230307200713584" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Brewster’s angle $\theta_B$</td>
      <td>$\theta_I$ such that there is <strong>no reflected wave</strong></td>
      <td>e.g. in the case above, when $\alpha=\beta$</td>
    </tr>
    <tr>
      <td>Total Internal Reflection</td>
      <td>$\theta_I$ such that there is <strong>no transmitted energy</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td>Reflection and Transmission index</td>
      <td>$R=\frac{I_R}{I_I}, T=\frac{I_T}{I_I}$</td>
      <td>$I=\lang S\rang_z=c\epsilon_0 E_0^2$ for plane waves <strong>striking the area</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$R+T=1$</td>
      <td>basically the <strong>input energy = output energy</strong>, $I_I=I_R+I_T$</td>
    </tr>
  </tbody>
</table>

<p><em>Complex Notation in Circuits</em>: consider a given circuit as follows:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304164428685.png" alt="image-20230304164428685" style="zoom:33%;" /></p>

<p>we want to know $I$ flowing in this circuit, if $V=V_0\cos(\omega t)$. We know that the differential equation for $V$ must be</p>

\[V_0\cos(\omega t) - L \frac{dI}{dt} - IR=0\]

<p>so $I$ will be sinusoidal in form, and relates to the driving frequency $\omega$. Hence we can consider $I(t)=\tilde{I}_0e^{i\omega t}$:</p>

\[V_0 e^{i\omega t} = L \frac{d}{dt}\tilde{I}_0e^{i\omega t} + R\tilde{I}_0e^{i\omega t}\]

<p>then solving you will find:</p>

\[\tilde{I}_0 = \frac{V_0}{i\omega L +R} = \frac{V_0}{\sqrt{\omega^2L^2 + R^2}e^{i\phi}} = = \frac{V_0}{\sqrt{\omega^2L^2 + R^2}}e^{-i\phi}\]

<p>note that from this step, we realize that:</p>

<ul>
  <li>having $L$ in circuit just means a <mark>resistance of $i\omega L$</mark></li>
  <li>having capacitance $C$ in circuit, as you will see later, just means a <mark>resistance of $i C/\omega$</mark>.</li>
  <li>with the above, you can solve circuits without touching differential equations at all, by just treating them as resistors.</li>
</ul>

<p>for $\tan(\phi)=\omega L/R$, by visualizing $a+ib$ in complex plane. Then you are essentially done since:</p>

\[I(t)= \mathrm{RE}[\tilde{I}_0e^{i\omega t}] =\frac{V_0}{\sqrt{\omega^2L^2 + R^2}}\cos(\omega t - \phi)\]

<hr />

<p><em>Standing Wave</em>: given a boundary condition that both ends of a wall has to be zero, e.g. $y(x,0)=A\sin (\pi x/L)$</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304172216985.png" alt="image-20230304172216985" style="zoom:33%;" /></p>

<p>and obviously since this is a string, it needs to satisfy the wave equation</p>

\[\frac{\partial^2 y}{\partial x^2} - \frac{1}{v^2}\frac{\partial^2 y}{\partial t^2} = 0\]

<p>then you should <strong>guess</strong> that the solution $y(x,t)=f(x-vt)+g(x+vt)$ is superposition to two waves:</p>

\[y(x,t) = \frac{A}{2}\left\{ \sin\left[\frac{\pi}{L}(x-vt)\right] +\sin\left[\frac{\pi}{L}(x+vt)\right] \right\}\]

<p>How does this work? Two oppositely traveling waves with <em>same frequency</em> forms standing waves = has <mark>fixed points</mark>!</p>

<table>
  <tbody>
    <tr>
      <td>![Visualizing RF Standing Waves</td>
      <td>Hackaday](https://hackaday.com/wp-content/uploads/2015/08/350px-standing_wave_2.gif)</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>EM Wave in Matter</em>: consider the simple example of reflection + transmission for <em>plane waves</em>. Find all amplitudes as a function of $\tilde{E}_I$</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230304210203014.png" alt="image-20230304210203014" style="zoom: 50%;" /></p>

<p>Since these are plane waves, we have</p>

\[\begin{align*}
\mathbf{E}_{I}(z,t)&amp;={\tilde{E}}_Ie^{i(k_1z-\omega t)}\hat{x}\\
\mathbf{E}_{R}(z,t)&amp;={\tilde{E}}_Re^{i(-k_1z-\omega t)}\hat{x}\\
\mathbf{E}_{T}(z,t)&amp;={\tilde{E}}_Te^{i(k_2z-\omega t)}\hat{x}
\end{align*}\]

<p>then the tricky part is realize the <strong>$\vec{B}_R$</strong> direction has to be consistent such that $\vec{S}=\frac{1}{\mu_0}\vec{E}\times \vec{B}$ gives $\vec{v}_R$:</p>

\[\begin{align*}
\mathbf{B}_{I}(z,t)&amp;=\frac{1}{v_1}{\tilde{E}}_Ie^{i(k_1z-\omega t)}\hat{y}\\
\mathbf{B}_{R}(z,t)&amp;=\frac{\textcolor{red}{-1}}{v_1}{\tilde{E}}_Re^{i(-k_1z-\omega t)}\hat{y}\\
\mathbf{B}_{T}(z,t)&amp;=\frac{1}{v_2}{\tilde{E}}_Te^{i(k_2z-\omega t)}\hat{y}
\end{align*}\]

<p>then just put them into the B.C. at $z=0$ you get</p>

\[\begin{align*}
\vec{E}^{\parallel}_1 - \vec{E}^{\parallel}_2 &amp;= 0 \implies \tilde{E}_I + \tilde{E}_R =\tilde{E}_T \\
\frac{1}{\mu_1}{B}^{\perp}_1 - \frac{1}{\mu_2}{B}^{\perp}_2 &amp;= 0 \implies \frac{1}{\mu_1v_1}\tilde{E}_I -\frac{1}{\mu_1v_1} \tilde{E}_R =\frac{1}{\mu_2v_2}\tilde{E}_T
\end{align*}\]

<p>then to solve $\tilde{E}_R, \tilde{E}_T$, you can realize that letting $k_i \gets \frac{1}{\mu_iv_i}$ gives you the same set of equations to solve for wave on a string.</p>

<h2 id="plane-waves-with-vecj">Plane Waves with $\vec{J}$</h2>

<p>Up to here, we have discussed solutions to wave equations derived</p>

<ul>
  <li><strong>assuming no charges/currents</strong>: now we will have <em>another term</em> in Maxwell’s eq, resulting in a <em>damping term</em> in wave equation (and that $\vec{B}$ is out of phase with $\vec{E}$). You will see this results in $k$ $\to$ $\tilde{k}=k+i\kappa$, so that waves are decaying/not propagating!</li>
  <li><strong>assuming wave $v$ (or rather $\epsilon$) is independent of $\omega$</strong> in material. Before we had $v^2=1/(\epsilon\mu)$ $\to$ you will see $v^2=1/(\tilde{\epsilon}(\omega)\mu)$. Hence this will also affect wave vector $k \to k(\omega)$. In this case, even in a <em>non-conducting material</em> we end up with <em>damping term in solution</em>!</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Maxwell’s Equation inside a conductor</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305003116097.png" alt="image-20230305003116097" style="zoom:50%;" /></td>
      <td>basically we have a $\rho_f$ and a $\vec{J}_{free}=\sigma\vec{E}$ inside conductor</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230301112719153.png" alt="image-20230301112719153" style="zoom:50%;" /></td>
      <td>derived from this using linear media</td>
    </tr>
    <tr>
      <td>Wave Equation used Inside Conductor</td>
      <td>$\nabla^2\mathbf{E} = \mu\epsilon \frac{\partial^2 \mathbf{E}}{\partial t^2}+\underbrace{\mu\sigma \frac{\partial \mathbf{E}}{\partial t}}_{\text{damping term}}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\nabla^2\mathbf{B} = \mu\epsilon \frac{\partial^2 \mathbf{B}}{\partial t^2}+\underbrace{\mu\sigma \frac{\partial \mathbf{B}}{\partial t}}_{\text{damping term}}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305003353475.png" alt="image-20230305003353475" style="zoom: 50%;" /></td>
      <td>this is <em>technically</em> derived from this, i.e. assuming charges has disappeared to boundary</td>
    </tr>
    <tr>
      <td>Charge inside conductor</td>
      <td>$\rho_f(t) = \rho_f(0)e^{-t/\tau}$</td>
      <td>hence the $\rho_f$ term is ignored (only) when deriving the wave equation</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from the continutiy equation $-\partial\rho_f / \partial t = \nabla \cdot \vec{J}_f$, then using $\vec{J}_f = \sigma \vec{E}$ and $\nabla \cdot \vec{D} = \rho_f$</td>
    </tr>
    <tr>
      <td>B.C. inside conductor</td>
      <td>$\epsilon_1E^{\perp}_1 - \epsilon_2E^{\perp}_2 = \sigma_f$, etc.</td>
      <td>derive from the Maxwell’s equation in material, with the presence of both $\rho_f$ and $\vec{J}_f$</td>
    </tr>
    <tr>
      <td>Solution for Wave in Conductor</td>
      <td>$\tilde{k}=k+i\kappa$</td>
      <td>found by plugging our usual solution $\mathbf{\tilde{E}}(z,t)=\tilde{E}_0e^{i(kz-\omega t)}\hat{x}$ into wave equation, and found that $k$ needs to be complex</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{E}}(z,t)=\tilde{E}_0e^{-\kappa z}e^{i(kz-\omega t)}\hat{x}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{B}}(z,t)=\tilde{B}_0e^{-\kappa z}e^{i(kz-\omega t)}\hat{y}=\frac{\tilde{k}}{\omega}\tilde{E}_0e^{-\kappa z}e^{i(kz-\omega t)}\hat{y}$</td>
      <td>notice the coefficient $\tilde{k}/\omega$ is complex, i.e. $\vec{B}$ is <mark>out of phase</mark> with $\vec{E}$. This is derived simply from $\nabla\times \vec{E} = -\partial \vec{B}/\partial t$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>phase difference of $\phi\gets$ converting $\tilde{k}/\omega =Ae^{i\phi}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305004919734.png" alt="image-20230305004919734" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$d\equiv 1/\kappa$</td>
      <td><strong>skin depth</strong>, i.e. an indicator of how far wave can penetrate into conductor</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\alpha \equiv 2\kappa$</td>
      <td><strong>absorption coefficient</strong>, because intensity is proportional to $E^2$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\omega = kv$ holds for the real part of $\tilde{k}$</td>
      <td>because the propagation part is $e^{i(kz-\omega t)}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305004856356.png" alt="image-20230305004856356" /></td>
      <td>if you need to know</td>
    </tr>
    <tr>
      <td>Modeling electron’s motion given a driving field</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305010538065.png" alt="image-20230305010538065" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$m\frac{d^2x}{dt^2}=\vec{F}<em>{electron}=\vec{F}</em>{E}+\vec{F}<em>{damping}+\vec{F}</em>{SHO}$</td>
      <td>where $\vec{F}<em>E = q\vec{E}</em>{ext}$, damping term $-m\gamma \frac{dx}{dt}$, and SHO force due to the <em>atomic attraction</em> for restoration $\vec{F}_{SHO}=-m\omega_0^2x$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305010958944.png" alt="image-20230305010958944" style="zoom:50%;" /></td>
      <td>the equation of motion</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\omega_0$ will be different for each electron config/also relevant to <mark>resonance</mark>!</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tilde{x}(t)=\tilde{x}_0e^{-i\omega t}$ and $\tilde{x}=\frac{q/m}{\omega_0^2-\omega-i\gamma \omega}E_0$   is complex</td>
      <td>solution, by plugging in the steady state $\tilde{x}(t)=\tilde{x}_0e^{-i\omega t}$</td>
    </tr>
    <tr>
      <td>Modeling Polarization given a driving field</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305011452197.png" alt="image-20230305011452197" style="zoom:50%;" /></td>
      <td>notice that $E_0e^{-i\omega t}=\mathbf{\tilde{E}}$ that we supplied</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305011538023.png" alt="image-20230305011538023" style="zoom:50%;" /></td>
      <td>since $\mathbf{P}=N\tilde{p}$, and $N$ is number of molecules per volume</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\omega_j$ comes from $\omega_0$ being different for different electrons. $f_j$ is the number of electrons having the same $\omega_j$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{P}}=\epsilon_0\tilde{\chi}_E(\omega)\tilde{E}$</td>
      <td>rewrite the entire wierd term into $\chi_E$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\tilde{\epsilon}=\epsilon_0(1+\tilde{\chi}_E) = \epsilon_0\left[ 1+\frac{Nq^2}{m\epsilon_0}\sum_j \frac{f_j}{\omega_j^2 - \omega^2 - i \omega \gamma} \right]$</td>
      <td><mark>after all efforts, the net result is that $\epsilon$ becomes complex</mark> and is <mark>a function of $\omega$</mark></td>
    </tr>
    <tr>
      <td>Wave Equation after all these</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305012418254.png" alt="image-20230305012418254" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{E}}(z,t)=\mathbf{\tilde{E}}_0e^{-\kappa z}e^{i(kz-\omega t)}$</td>
      <td>derived from plugging in $\mathbf{\tilde{E}}(z,t)=\mathbf{\tilde{E}}_0e^{i(kz-\omega t)}$ and realizing $\tilde{k} = \sqrt{\tilde{\epsilon}\mu_0}\omega=k+i\kappa$</td>
    </tr>
    <tr>
      <td>Resonance frequency</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305012926612.png" alt="image-20230305012926612" style="zoom: 33%;" /></td>
      <td>so that at $\omega = \omega_j$, absorption peaks = <mark>amplitudes of electron oscillation becomes huge</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305013027929.png" alt="image-20230305013027929" style="zoom: 40%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>The above is a dispersive medium</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305013243995.png" alt="image-20230305013243995" style="zoom:40%;" /></td>
      <td>$n\to n(\omega)$ <mark>index of refraction depends on input wave frequency</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from $\omega = kv,v=c/n$</td>
    </tr>
  </tbody>
</table>

<h2 id="wave-in-wave-guide">Wave in Wave Guide</h2>

<p>No longer plane wave. Now we consider the form</p>

\[\mathbf{E}=\textcolor{red}{\tilde{\mathbf{E}}_0(x,y)}e^{i(kz-\omega t)}\]

<p>which has two changes:</p>

<ul>
  <li>
    <p>our old boundary conditions + now the amplitude is a function of $(x,y)$ results in $E$ and $B$ <strong>cannot both be perpendicular to $\hat{z}$</strong></p>
  </li>
  <li>
    <p>amplitude as a result also has terms:</p>

\[\mathbf{\tilde{E}}_0(x,y) = \tilde{E}_x(x,y)\hat{x} +  \tilde{E}_y(x,y)\hat{y} +  \tilde{E}_z(x,y)\hat{z}\]
  </li>
</ul>

<p>The net result is that the amplitudes/solution becomes more complicated, e.g. TE waves:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305014934219.png" alt="image-20230305014934219" style="zoom:33%;" /></p>

<p>but notice that $\vec{E}$ and $\vec{B}$ still perpendicular to each other</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Setting up Wave Equation <mark>inside Wave Guides</mark></td>
      <td>$\mathbf{E}(x,y,z,t)=\textcolor{red}{\tilde{\mathbf{E}}_0(x,y)}e^{i(kz-\omega t)}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{B}(x,y,z,t)=\textcolor{red}{\tilde{\mathbf{B}}_0(x,y)}e^{i(kz-\omega t)}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305015419912.png" alt="image-20230305015419912" style="zoom:33%;" /></td>
      <td>why this change? assuming the <mark>wave guide is a perfect conductor</mark>.</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305015540452.png" alt="image-20230305015540452" style="zoom:40%;" /></td>
      <td>then this B.C. means we cannot have plane waves (e.g. try to draw it with a rectangular wave guide)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>why this B.C (on the inner surface)? Because $E_{meat}=B_{meat}=0$ for conductor. Then use the usual B.C. from Maxwell’s eq you will get this.</td>
    </tr>
    <tr>
      <td>Generic Solution for Wave in Wave Guide</td>
      <td>$\mathbf{\tilde{E}}_0(x,y) = \tilde{E}_x(x,y)\hat{x} +  \tilde{E}_y(x,y)\hat{y} +  \tilde{E}_z(x,y)\hat{z}$</td>
      <td>our task is <em>just to solve the coefficients</em></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\mathbf{\tilde{B}}_0(x,y) = \tilde{B}_x(x,y)\hat{x} +  \tilde{B}_y(x,y)\hat{y} +  \tilde{B}_z(x,y)\hat{z}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305015732193.png" alt="image-20230305015732193" style="zoom:50%;" /></td>
      <td>by plugging the above into Maxwell’s equation in vacuum. Notice that <mark>all are a function $E_z,B_z$</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305015937140.png" alt="image-20230305015937140" /></td>
      <td>so our only task becomes to solve $E_z,B_z$ from this + specific B.C. in a problem</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>the above is again found in Maxwell Eq.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so <strong>why not transverse wave</strong> = $E_z=B_z=0$? Then we would get both the $\nabla \cdot E=0,\nabla \times E=0$, hence $E=-\nabla \phi$ inside. <br />But a conductor has $\vec{E}^{\parallel}<em>{inner}=0$ since $\vec{E}</em>{meat}=0$. Therefore on the surface $\phi=$constant. <br />Then since this satisfies Laplace equation = both mean and max on surface, $\phi$=constant inside as well. Hence $E=0$ entirely.</td>
    </tr>
    <tr>
      <td>$TE_{MN}$ waves in Rectangular Wave Guide</td>
      <td>$B_z=B_0\cos(k_xx)\cos(k_yy)=B_0\cos(m\pi\frac{x}{a})\cos(n\pi\frac{y}{b})$</td>
      <td>TE because $E_z=0$ hence $E$ is $T$ransverse</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>once $E_z=0$ and $B_z$ is solved, we can solve everything else</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$m=0,1,2,3…$, $n=0,1,2,3,…$, but at least one non-zero so that the solution is not trivial</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305020803974.png" alt="image-20230305020803974" style="zoom:50%;" /></td>
      <td>derived from using $B_z(x,y)=X(x)Y(y)$ and plugging into the  eq for $B_z$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305021240165.png" alt="image-20230305021240165" style="zoom:50%;" /></td>
      <td>since the first term is only a function of $x$, second only a function of $y$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305021309163.png" alt="image-20230305021309163" style="zoom:50%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305020749636.png" alt="image-20230305020749636" style="zoom:50%;" /></td>
      <td>B.C. is in this case is<br />$B_y(y=0)=B_y(y=b)=0$ and $B_x(x=0)=B_x(x=a)=0$</td>
    </tr>
    <tr>
      <td>Properties of $TE_{MN}$ wave above</td>
      <td>$k^2 = \frac{\omega^2}{c^2}\left[ 1-\frac{\omega_{mn}^2}{\omega^2} \right]$, so $\omega_{mn}\equiv \omega_{cutoff}$</td>
      <td>since if $\omega_{cutoff}&gt;\omega$, then $k$ becomes complex and your wave decays in $\hat{z}$/does not propagate</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305021905239.png" alt="image-20230305021905239" style="zoom: 33%;" /></td>
      <td>derived from the equation of $k^2$ from the previous concept</td>
    </tr>
    <tr>
      <td> </td>
      <td>$v=\omega/k = c/\sqrt{1-(\omega/\omega_{mn})^2}$</td>
      <td>this is $v&gt;c$? Because $v=v_{phase}$. The actual $v_g$ of <strong>energy propagation</strong> has $v_g &lt;c$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230305022049463.png" alt="image-20230305022049463" style="zoom:50%;" /></td>
      <td>one way to derive the above is $v_g=c\cos(\theta)$, and $v_{phase}=c/\cos\theta$, with knowing $\cos\theta = k/\vert \vec{k}’\vert$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>and $\vec{k}^\prime=k_x\hat{x}+k_y\hat{y}+k\hat{z}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>shows what does $k_x,k_y$ represent!</td>
    </tr>
    <tr>
      <td>Group Velocity as velocity of energy transfer</td>
      <td>$v_g \equiv d\omega /dk$</td>
      <td>in principle represents velocity of a <strong>wave packet</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$v_g = \frac{\int \lang S\rang_z\cdot d\vec{a} }{\int \lang u\rang_z da}=\frac{\text{energy/time}}{\text{energy/length}}$</td>
      <td>an intuitive “verification” using poynting vector/energy density</td>
    </tr>
    <tr>
      <td> </td>
      <td>$Ae^{i(k+\Delta k/2)z-(\omega + \Delta \omega /2)t}+Ae^{i(k-\Delta k/2)z-(\omega - \Delta \omega /2)t}$<br />$=Ae^{i(kz-\omega t)}\cos(\frac{\Delta k}{2}z-\frac{\Delta \omega}{2}t)$<br />$=Ae^{i(kz-\omega t)}\cos (k(z-v_gt))$</td>
      <td>where it actually comes from = superposing two waves in a specific wave forms a <strong>wave packet traveling at $v_g$</strong> (see visualization in the examples section)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$v_g = \Delta \omega /\Delta k$</td>
      <td>in this example</td>
    </tr>
  </tbody>
</table>

<p><em>Group Velocity v.s. Phase Velocity</em>:</p>

<p>In this example, $v_g&gt;v_{phase}$, but in general, most physical systems will have $v_g &lt; v_{phase}$</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/Recording 2023-03-05 at 01.40.38.gif" style="zoom:33%;" /></p>

<hr />

<p><strong>List of good questions</strong></p>

<ul>
  <li>Problem 9.7
    <ul>
      <li>(b)(c) dealt with understanding more the B.C. and wave equation</li>
      <li>(d) deals with understanding the complex amplitude $\tilde{A}$</li>
    </ul>
  </li>
  <li>
    <p>Problem 9.17: solid question in understanding B.C. and $\vec{E}$ and $\vec{B}$ with directional $\vec{k}\cdot \vec{r}$.</p>
  </li>
  <li>Problem 9.39
    <ul>
      <li>(f) is magical, about total internal reflection</li>
    </ul>
  </li>
  <li>Problem 9.30: what does velocity of “energy propagation” mean, and why group velocity is relevant</li>
</ul>

<h1 id="chapter-10-retarded-potentials-and-fields">Chapter 10 (Retarded) Potentials and Fields</h1>

<p>The goal is to find $\vec{E}$ and $\vec{B}$ given some <strong>really arbitrary $\rho, \vec{J}$</strong>. (Before, we only looked at when $\rho=0$, or when $\vec{J}_f=\sigma\vec{E}$ in a conductor when solving the wave equations.)</p>

<p>In general this is difficult, so here the idea is (like before)</p>

<ul>
  <li>
    <p>solve instead for $\phi, \vec{A}$ under the Lorentz Gauge:</p>

\[\exists \chi,\quad \nabla \cdot \vec{A} + \frac{1}{c^2}\frac{\partial \phi}{\partial t} = 0\]

    <p>then $\phi, \vec{A}$ satisfies:</p>

\[\begin{cases}
\square^2 \phi = - \rho / \epsilon_0\\
\square^2 \vec{A} =  - \mu_0 \vec{J}
\end{cases}\]

    <p>where LHS is <mark>only the sources</mark>, and $\square^2 \equiv \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2}$. Then the solution of $\phi,\vec{A}$ can be <mark>guessed</mark>:</p>

\[\phi(\vec{r},t) = \frac{1}{4\pi\epsilon_0} \int \frac{\rho(\vec{r}',t_r')}{|\vec{r} - \vec{r}'|} d^3r'\\
\vec{A}(\vec{r},t) = \frac{\mu_0}{4\pi} \int \frac{\vec{J}(\vec{r}')}{|\vec{r} - \vec{r}'|} d^3r'\]

    <p>where $t_r = t - \vert \vec{r} - \vec{r}’\vert /c$  is the retarded time.</p>
  </li>
  <li>
    <p>convert to $\vec{E},\vec{B}$ by $\vec{E} =- \nabla \phi - \frac{\partial \vec{A}}{\partial t}$ and $\vec{B}=\nabla \times \vec{A}$</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Potential of Fields in electrodynamics</td>
      <td>$\vec{E} =- \nabla \phi - \frac{\partial \vec{A}}{\partial t}$</td>
      <td>before in electrostatics we had $\vec{E} =- \nabla \phi$ (from $\nabla \times \vec{E} = 0$)but this no longer works if you consider $\nabla \times \vec{E} = - \partial \vec{B} / \partial t$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{B}=\nabla \times \vec{A}$</td>
      <td>unchanged</td>
    </tr>
    <tr>
      <td>Inhomogenous Wave Equation for $\vec{E},\vec{B}$ given generic $\rho,\vec{J}$</td>
      <td>$\nabla^2 \phi - \frac{1}{c^2} \frac{\partial^2 \phi}{\partial t^2} = - \rho / \epsilon_0$<br />$\nabla^2 \vec{A} - \frac{1}{c^2} \frac{\partial^2 \vec{A}}{\partial t^2} = - \mu_0 \vec{J}$</td>
      <td>the first derived from $\nabla \cdot \vec{E}=-\rho / \epsilon_0$, the second derived from $\nabla\times \vec{B}$ and Lorentz Gauge transformation (see below)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>or equivalently  $\square^2 \equiv \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>derived from<br />$\nabla^2 \vec{A} - \frac{1}{c^2}\frac{\partial^2 \vec{A}}{\partial t^2} - \nabla(\nabla \cdot \vec{A} + \frac{1}{c^2}\frac{\partial \phi}{\partial t}) = -\mu\vec{J}$</td>
      <td>which comes from $\nabla \times (\nabla \times \vec{A}) = \nabla \times \vec{B}$ and expand $\vec \times \vec{B}$ using Maxwell’s Eq and $\vec{E} =- \nabla \phi - \frac{\partial \vec{A}}{\partial t}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}’ = \vec{A} + \nabla \chi$, and $\phi’ = \phi-\frac{\partial \chi}{\partial t}$</td>
      <td>for some $\chi$ such that the <mark>corresponding $\vec{E}'=E$ and $\vec{B}'=\vec{B}$ is unchanged</mark></td>
    </tr>
    <tr>
      <td>Lorentz Gauge</td>
      <td>$\chi$ such that $\nabla \cdot \vec{A} + \frac{1}{c^2}\frac{\partial \phi}{\partial t} = 0$</td>
      <td>then you easily get $\nabla^2 \vec{A} - \frac{\partial^2 \vec{A}}{\partial t^2} = - \mu_0 \vec{J}$ using the result 2 cells above</td>
    </tr>
    <tr>
      <td>Retarded Potential</td>
      <td>$\phi(\vec{r},t) = \frac{1}{4\pi\epsilon_0} \int \frac{\rho(\vec{r}’,t_r’)}{\vert \vec{r} - \vec{r}’\vert } d^3r’$</td>
      <td>guessed from the Wave equation, just as we guessed from electrostatics<br /><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507170523791.png" alt="image-20230507170523791" /></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}(\vec{r},t) = \frac{\mu_0}{4\pi} \int \frac{\vec{J}(\vec{r}’)}{\vert \vec{r} - \vec{r}’\vert } d^3r’$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$t_r = t - \vert \vec{r} - \vec{r}’\vert /c$</td>
      <td>retarded time, or better seen from $\vert \vec{r} - \vec{r}’(t_r)\vert  = c(t-t_r)$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507170950514.png" alt="image-20230507170950514" style="zoom:13%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>has relation to relativity since time is a concern, i.e. in another frame, this retarded time would be different</td>
    </tr>
    <tr>
      <td>Lienard-Wiechert Potentials</td>
      <td>$\phi(\vec{r},t) = \frac{1}{4\pi \epsilon_0} \frac{q}{1 - \hat{\mathcal{R}}\cdot \vec{v}(t_r)/c}$</td>
      <td>where $\vec{\mathcal{R}} \equiv \vec{r} - \vec{w}(t_r)$ and $\vec{v}(t_r)$ both are at retarded time</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{A}(\vec{r},t) = \frac{\vec{v}(t_r)}{c}\phi(\vec{r},t)$</td>
      <td>derived from length contraction when seomthing is moving towards you, hence $\tau’ = \tau / (1 - \hat{\mathcal{R}}\cdot \vec{v}(t_r)/c)$ is the volume <em>appears to you</em></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>generic for a point charge moving in any trajectory $\vec{w}(t)$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507173613747.png" alt="image-20230507173613747" style="zoom:23%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>so that $\vert \vec{r}- \vec{w}(t_r)\vert  = c(t-t_r)$</td>
    </tr>
    <tr>
      <td>Fields of a moving point charge</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507175541022.png" alt="image-20230507175541022" /></td>
      <td>where $\vec{u}\equiv c \hat{\mathcal{R}}-\vec{v}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507175955450.png" alt="image-20230507175955450" style="zoom:70%;" /></td>
      <td>derived from the above, since we can compute $\vec{E} = -\nabla \phi - \frac{\partial \vec{A}}{\partial t}$ and $\vec{B} = \nabla \times \vec{A}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>generic for any trajectory</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507175913539.png" alt="image-20230507175913539" style="zoom:15%;" /></td>
      <td>velocity field because when $\vert \vec{v}(t)\vert =v$ is constant, then $\vec{E}$ points to its present location <img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507180250215.png" alt="image-20230507180250215" style="zoom: 50%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>visually:<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507180131681.png" alt="image-20230507180131681" style="zoom:13%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>radiation field because that <mark>term $\propto 1/r$,</mark> hence will dominate at large distances (other terms go like $1/r^2$)</td>
    </tr>
  </tbody>
</table>

<p><em>Example Calculation using Lienard-Wiechert Potentials</em>:</p>

<p>Consider a trajectory of a point charge with constant velocity $\vec{w} = vt$. What is its potential $\phi(\vec{r},t)$?</p>

<p>The idea is to express:</p>

\[\phi(\vec{r},t) = \frac{1}{4\pi \epsilon_0} \frac{q}{1 - \hat{\mathcal{R}}\cdot \vec{v}(t_r)/c}\]

<p>in terms of $q,v, t$, so that we need to figure out what is $\hat{\mathcal{R}}$. Graphically, we know that it is the position of the charge at $t_r$:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507173613747.png" alt="image-20230507173613747" style="zoom:23%;" /></p>

<ol>
  <li>
    <p>find out when is $t_r$:</p>

\[|\vec{r} - vt_r| = c(t-t_r)\]

    <p>and solve for $t_r$</p>
  </li>
  <li>
    <p>find $\hat{\mathcal{R}}$ since we know $t_r$ and $\vec{w}$:</p>

\[\hat{\mathcal{R}} = \hat{\mathcal{R}}(t_r) = \frac{\vec{r} - vt_r}{c(t-t_r)}\]
  </li>
  <li>
    <p>done.</p>
  </li>
</ol>

<p>Surprisingly, you will find in the case of constant velocity <mark>the corresponding $\vec{E}$</mark> points to the <mark>current position of the charge</mark>.</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507180131681.png" alt="image-20230507180131681" style="zoom: 23%;" /></p>

<hr />

<p><em>Thought Experiment of Velocity Field</em>. Consider a point charge moving at $v$ but suddenly stopped at $T$. You are at $\vec{r}$ and that information hasn’t reach you <em>yet</em>. What would the E field look like in space?</p>

<ol>
  <li>Since the information that the charge has stopped hadn’t yet reach me (such information propagates as speed of light), then I would experience a field as if the charge is still moving with $v$</li>
  <li>The field when charge has stopped will be simple, and that will propagate at the speed of light</li>
</ol>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507180720556.png" alt="image-20230507180720556" style="zoom: 33%;" /></p>

<p>so you get a lot of “kinks”</p>

<hr />

<p>List of good questions:</p>

<ul>
  <li>10.17 = draw the space-time diagram version.</li>
  <li>10.21 = mind boggling with field of charge at constant velocity.</li>
</ul>

<h1 id="chapter-11-radiation">Chapter 11 Radiation</h1>

<p>Radiation studies the parts of $\vec{E},\vec{B}$ that goes like $1/r$, such that even for $r\to \infty$ we get $I=\int \vec{S}\cdot d\vec{A} \neq 0$ meaning energy is <strong>radiated and never comes back</strong> = permanently lost.</p>

<p>One example of such a field is the “radiation field” discussed in the previous section. This section will basically focus on studying that field.</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Electric Dipole Radiation</td>
      <td>$\vec{E}_{\text{dipole radiation}} = - \frac{\mu_0 p_0 \omega^2}{4\pi}\frac{\sin(\theta)}{r}\cos(\omega(t-r/c))\hat{\theta}$</td>
      <td>where $p_0 = q_0d$ is electric dipole</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from considering an oscillating dipole constructed with $q(t)=q_0e^{i\omega t}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507182443127.png" alt="image-20230507182443127" style="zoom:50%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>then find its potential $\phi$, discard terms containing any $1/r$, and then compute field $\vec{E}$ to only keep $1/r$ terms</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507182611809.png" alt="image-20230507182611809" style="zoom:50%;" /></td>
      <td>by figuring out $\vec{B}$ as well, then $\vec{S} = (1/\mu_0)\vec{E}<em>{\text{rad}}\times \vec{B}</em>{\text{rad}}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507182750881.png" alt="image-20230507182750881" /></td>
      <td>total power radiated = energy permanently lost per second</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>this power is 1) frequency dependent and 2) is consistent with future derivation of generic radiation loss</td>
    </tr>
    <tr>
      <td>Magnetic Dipole Radiation</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192144887.png" alt="image-20230507192144887" style="zoom:67%;" /><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192154553.png" alt="image-20230507192154553" style="zoom:67%;" /></td>
      <td>derived from considering a current loop such that $\vec{m}(t) = \pi b^2I_0\cos(\omega t)\hat{z}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192050613.png" alt="image-20230507192050613" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>then find its retarded potential $\vec{A}$<br /><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192324974.png" alt="image-20230507192324974" style="zoom:50%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>and since we know $\phi=0$ , can compute $\vec{E},\vec{B}$ by taking derivatives after knowing $\vec{A}$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192434482.png" alt="image-20230507192434482" style="zoom: 67%;" /></td>
      <td>derived from finding $\vec{S}$ and taking integral.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>very similar to power radiated by electric dipole, but it is much smaller due to $c^3$ term.</td>
    </tr>
    <tr>
      <td>Radiation from <strong>Arbitrary</strong> Source</td>
      <td>$\vec{E}_{\mathrm{rad}}(\vec{r},t) = \frac{\mu_0}{4\pi r}[\hat{r}\times (\hat{r}\times \ddot{\vec{p}})]$</td>
      <td>where $\vec{p}$ is dipole moment for arbitrary distribution $\int r’\rho\,\, d^3r’$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\vec{B}_{\mathrm{rad}}(\vec{r},t) = \frac{\mu_0}{4\pi rc}[\hat{r}\times \ddot{\vec{p}}]$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507192728975.png" alt="image-20230507192728975" /></td>
      <td>derived from expand the retarded potentials to contain dipole terms (see left), and discard other terms higher than $1/r$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507193136550.png" alt="image-20230507193136550" style="zoom:67%;" /></td>
      <td>finally, the power radiated is given by $\oint \vec{S}\cdot d\vec{a}$</td>
    </tr>
  </tbody>
</table>

<h2 id="radiation-of-point-charges">Radiation of Point Charges</h2>

<p>Now we focus on the specific case of point charges. Since we already know the fields of point charges in <a href="#Chapter 10 (Retarded) Potentials and Fields">Chapter 10</a>, essentially we use $\vec{E},\vec{B}$ from there but focus on $1/r$ terms.</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Larmor’s Formula</td>
      <td>$P_{\mathrm{rad}} = \frac{\mu_0 q^2 a^2}{6\pi c}$</td>
      <td>derived from assuming $v=0$ at $t_r$ so that $\vec{u}=c \hat{\mathcal{R}}$, but actuallly <mark>holds well for $v \ll c$</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from having a point charge moving with trajectory $\vec{w}$, and only taking $\vec{E}<em>{\mathrm{rad}}$ term to compute $\vec{S}</em>{\mathrm{rad}} = (1/\mu_0 c) E_{\mathrm{rad}}^2$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>then finally $P_{\mathrm{rad}} = \oint \vec{S}_{\mathrm{rad}}\cdot d\vec{A}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><mark>this formula is used most often</mark> as long as it’s not specified we are at relativistic conditions</td>
    </tr>
    <tr>
      <td>Lienard’s Generalization of Larmor’s Formula</td>
      <td>$P_{\mathrm{rad}} = \frac{\mu_0 q^2}{6\pi c}\gamma^6 (a^2 - \left\vert  \frac{\vec{v}\times \vec{a}}{c} \right\vert ^2)$</td>
      <td>takes care of the case when $v\approx c$.</td>
    </tr>
    <tr>
      <td>Radiation Reaction Force</td>
      <td>$\vec{F}_{\mathrm{rad}} = \frac{\mu_0 q^2}{6\pi c}\dot{\vec{a}}$</td>
      <td>derived from realizing that $P_{rad}$ loss needs to be taken away from the <strong>kinetic energy</strong> of a particle. Hence try to find $\vec{F}<em>{rad}\cdot \vec{v} = -P</em>{loss}=-P_{rad}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>since $P_{loss} = \oint \vec{S}\cdot d\vec{A}$ where $\vec{S}$ includes both velocity field and radiation field, here we <em>assume</em> that $P_{loss}=P_{rad}$ by considering a “periodic motion” of the particle os that at $t_2$ it restores its velocity field</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507195052305.png" alt="image-20230507195052305" style="zoom:8%;" />finally substitute in the Larmor’s formula and find $\vec{F}_{rad}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>note that this force is $\propto \dddot{r}!$</td>
    </tr>
  </tbody>
</table>

<p><em>Using Radiation Reaction to find EoM</em>: since this essentially relates energy loss back to change in acceleration/velocity/position, we can use this if find EoM of an object.</p>

<p>Consider radiation damping of a SHO object, attached to some spring with $\omega_0$ and subject to driving force at $\omega$. Then its EoM look like:</p>

\[m \ddot{x} = F_{\mathrm{spring}} + F_{\mathrm{rad}} + F_{\mathrm{drive}} = - m\omega_0^2 x + m \tau \ddot{x} + F_{\mathrm{drive}}\]

<p>where $\tau = \frac{\mu_0 q^2}{6\pi mc}$ from Larmor’s fomula. Then since the solution should be oscillatory, let $x(t) = x_0 \cos(\omega t + \delta)$ to find</p>

\[\ddot{x} = -\omega \dot{x}\]

<p>putting this back the EoM becomes</p>

\[m \ddot{x}+ \underbrace{m \omega\tau \dot{x}}_{\text{damping term}} + m\omega_0^2 x  = F_{\mathrm{drive}}\]

<p>which makes sense as radiation losses energy.</p>

<hr />

<p>List of good questions:</p>

<ul>
  <li>11.14 = how radiation $P_{rad}$ relates to “physical” change in a system</li>
  <li>11.18 = a bit more into how to use $F_{\mathrm{rad}}$ for energy calculation
    <ul>
      <li>(a) try to derive the EoM and the solution</li>
    </ul>
  </li>
  <li>11.19 = normal EoM using $F_{rad}$</li>
</ul>

<h1 id="chapter-12-electrodynamics-and-relativity">Chapter 12 Electrodynamics and Relativity</h1>

<p>Iin previous chapters, we understand how to find $\vec{E},\vec{B}$ in some frame. In this chapter we consider, after knowing some $\vec{E},\vec{B}$, <strong>how do they transform</strong>.</p>

<p>In this chapter you will realize how $\vec{E},\vec{B}$ (and other related quantities such as $\phi, \vec{A}$) can <mark>transform into each other</mark> when you boost into some inertial frame. Some key findings include</p>

<ul>
  <li><strong>space-time diagram</strong> essentially encodes all features of relativity such as time dilation, length contraction, etc</li>
  <li>
    <p><strong>four vectors and four matrices</strong> are keys to help you perform transformations easily</p>
  </li>
  <li><strong>Maxwell’s Eq is already consistent</strong> with relativity</li>
</ul>

<h2 id="special-relativity-and-space-time">Special Relativity and Space Time</h2>

<p>Because speed of light is <strong>universal in any frame</strong>, this basically:</p>

<ul>
  <li>screwed up spontaneity</li>
  <li>screwed up length = length contration</li>
</ul>

<p>and all of the above can be intuitively shown using a space-time diagram.</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>space time coordinate</td>
      <td>$(ct,x,y,z)=(x^0,x^1,x^2,x^3)$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Invariant in space time/hyperbolic space</td>
      <td>$(ct)^2 - x^2 = (ct’)^2 - x’^2$</td>
      <td>signifies length in hyperbolic space uses this minus sign (instead of $x^2 + y^2 = x’^2 + y’^2$ normally)</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>hence define $x^0 \equiv ct$</td>
    </tr>
    <tr>
      <td>Lorentz Transformation with Hyperbolic Functions</td>
      <td>$x^{0’}=\cosh(u)x^0 - \sinh(u)x$<br />$x^{‘}=-\sinh(u)x^0 + \cosh(u)x$<br />$v/c = \tanh(u)$</td>
      <td>where $x=x^1$ above, assuming the $S’$ frame is moving at $v$. $u$ is also called <mark>rapidity</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507201339283.png" alt="image-20230507201339283" style="zoom: 15%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>e.g. transform <strong>into a Bob’s frame moving at $v$</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><mark>this is the more fundamental equations</mark>, and makes math also simpler</td>
    </tr>
    <tr>
      <td>Visualization of Rapidity</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507202354910.png" alt="image-20230507202354910" style="zoom:15%;" /></td>
      <td>derived from thinking about “where to put Bob’s $x^{0^{‘}}$axis” in Alice’s set of axes</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>basically found that $v/c = \mathrm{slope} = \tanh(u)$</td>
    </tr>
    <tr>
      <td>Lorentz Transformation</td>
      <td>$\sinh(u) = \frac{v/c}{\sqrt{1-v^2/c^2}} = \frac{v}{c}\gamma$<br />$\cosh(u) = \frac{1}{\sqrt{1-v^2/c^2}} =\gamma$</td>
      <td>derived from $v/c = \tanh(u)$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$t’ = \gamma (t - \frac{v}{c}x)$<br />$x’ = \gamma (x-vt)$</td>
      <td>same Lorentz transformation as above</td>
    </tr>
    <tr>
      <td>Velocity Addition</td>
      <td>$v_{\mathrm{total}}/c = \tanh(u_{\mathrm{total}})=\tanh(u_1+u_2)$</td>
      <td>when you want to find the velocity $v_2$ relative to you, but only given $v_1$ relative to you and $v_2$ relative to $v_1$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$v_{\mathrm{total}} = \frac{v_1+ v_2}{1 + v_1v_2/c^2}$</td>
      <td>same as above, because<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507202142746.png" alt="image-20230507202142746" style="zoom:15%;" /></td>
    </tr>
    <tr>
      <td>Consequences of living in hyperbolic space</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507202552035.png" alt="image-20230507202552035" style="zoom: 33%;" /></td>
      <td>lines of same lengths becomes wierd</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507202637041.png" alt="image-20230507202637041" style="zoom: 25%;" /></td>
      <td>vector decomposition in a primed/transformed frame</td>
    </tr>
    <tr>
      <td>Relativity of Simultaneity</td>
      <td>two events that are simulatenous to one inertial frame are not, in general, simultaneous in another frame</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507203244450.png" alt="image-20230507203244450" style="zoom:25%;" /></td>
      <td>event $A,B$ are simultaneous to the unprimed frame, but to the primed frame the two events happened at time $b,c$ respectively</td>
    </tr>
    <tr>
      <td>Length Contraction</td>
      <td>Things look longer when moving towards you</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507204037680.png" alt="image-20230507204037680" style="zoom:33%;" /></td>
      <td>note that for things like “length”, we need information to traverse through time, hence the additional photon line</td>
    </tr>
    <tr>
      <td>Lorentz Transformation with Matrix</td>
      <td>$\begin{bmatrix}x^{0^{‘}}\x^{1^{‘}}\x^{2^{‘}}\x^{3^{‘}}\end{bmatrix} = \begin{bmatrix}\cosh(\theta) &amp; -\sinh(\theta) &amp; 0 &amp; 0\-\sinh(\theta) &amp; \cosh(\theta) &amp; 0 &amp; 0\0 &amp; 0 &amp; 1 &amp; 0\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\begin{bmatrix}x^{0}\x^{1}\x^{2}\x^{3}\end{bmatrix}$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\begin{bmatrix}x^{0^{‘}}\x^{1^{‘}}\x^{2^{‘}}\x^{3^{‘}}\end{bmatrix} = \begin{bmatrix}\gamma &amp; -\gamma\beta &amp; 0 &amp; 0\-\gamma\beta &amp; \gamma &amp; 0 &amp; 0\0 &amp; 0 &amp; 1 &amp; 0\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\begin{bmatrix}x^{0}\x^{1}\x^{2}\x^{3}\end{bmatrix}$</td>
      <td>where $\beta = v/c$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>visually, boosting from $S$ to the moving$S’$<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507204437944.png" alt="image-20230507204437944" style="zoom: 50%;" /></td>
    </tr>
    <tr>
      <td>“Inverse” Lorentz Transformation</td>
      <td>$\begin{bmatrix}x^{0}\x^{1}\x^{2}\x^{3}\end{bmatrix} = \begin{bmatrix}\cosh(\theta) &amp; \sinh(\theta) &amp; 0 &amp; 0\\sinh(\theta) &amp; \cosh(\theta) &amp; 0 &amp; 0\0 &amp; 0 &amp; 1 &amp; 0\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\begin{bmatrix}x^{0^{‘}}\x^{1^{‘}}\x^{2^{‘}}\x^{3^{‘}}\end{bmatrix}$</td>
      <td>equivalent of the above but $S$ moving to the left relative to $S’$</td>
    </tr>
    <tr>
      <td>Four vectors</td>
      <td>$x^\mu \equiv \begin{bmatrix}x^{0}\x^{1}\x^{2}\x^{3}\end{bmatrix}$</td>
      <td>(same as above cell, but different representation)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$x^{\mu^{‘}} = \Lambda_\nu^\mu x^\nu \equiv \sum_\nu \Lambda_\nu^\mu x^\nu$</td>
      <td>where $\nu$ is row, $\mu$ is column of the matrix.</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>in this case, the <mark>summation notation is equivalent to perform matrix multiplication</mark></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><mark>any valid four vectors need to satisfy this transformation rule</mark></td>
    </tr>
    <tr>
      <td>Minkovski metric</td>
      <td>$\Delta x_\mu = \eta_{\mu\nu}\Delta x^\nu$</td>
      <td>lowering index</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\Delta x^\mu \cdot \Delta x^\mu = \Delta x_\mu \Delta x^\mu = \text{invariant} = (c^2t^2 - d^2)$</td>
      <td>where $d^2 = x^2 + y^2 + z^2$ (as mentioned before, this is length in space-time)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\eta_{\mu\nu} = \begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\0 &amp; -1 &amp; 0 &amp; 0\0 &amp; 0 &amp; -1 &amp; 0\0 &amp; 0 &amp; 0 &amp; -1\\end{bmatrix}$</td>
      <td>mostly negative convention, called the <strong>metric</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\eta_{\mu\nu}=\eta^{\mu\nu}$</td>
      <td>the RHS is used to raise the index</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>visually, $I$ is the invariant:<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507210223862.png" alt="image-20230507210223862" style="zoom: 15%;" /></td>
    </tr>
    <tr>
      <td>Time like events</td>
      <td>when $I=(c^2t^2-d^2) &gt; 0$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507210335755.png" alt="image-20230507210335755" style="zoom:33%;" /></td>
      <td>if $A$ happened before $B$ in one frame, this order <strong>must hold in any other frame</strong> = <strong>causality is maintained</strong></td>
    </tr>
    <tr>
      <td>Space like events</td>
      <td>when $I=(c^2t^2-d^2) &lt; 0$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507210401599.png" alt="image-20230507210401599" style="zoom:25%;" /></td>
      <td>no causality, and the order of event is no longer absolute</td>
    </tr>
  </tbody>
</table>

<p><em>Graphical Example of Time Dilation</em></p>

<p>In a primed frame, let an event happened at $\Delta t’ = 1$ hour. The same event to an unprimed frame would have happened at $\Delta t &gt; \Delta t’$:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507202901028.png" alt="image-20230507202901028" style="zoom: 25%;" /></p>

<h2 id="relativistic-mechanics">Relativistic Mechanics</h2>

<p>Now, we focus on how “definitions” of <mark>energy, momentum</mark>, and conservation principles <strong>“changed” when we consider relativity</strong>, i.e. do they hold when I transformed into another inertial frame? In sum:</p>

<ul>
  <li>
    <p>energy/momentum redefined through <em>proper time</em></p>
  </li>
  <li>
    <p><em>relativistic energy and momentum</em> is conserved</p>
  </li>
</ul>

<p>why need to “redefine”? It turns out using proper time = can write those quantities into four vectors = much easier to transform = nice properties follows such as conservation of relativistic energy.</p>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Transforming ordinary velocity</td>
      <td>$v_x = v^1 = \frac{\Delta x^1}{\Delta t} = c\frac{\Delta x^1}{\Delta t} = c \frac{\cosh(\theta)\Delta x^{0’} + \sinh(\theta) \Delta x^{1’}}{\sinh(\theta)\Delta x^{0’} + \cosh(\theta) \Delta x^{1’}}$</td>
      <td>given $v^{1’}$ to figure out $v^1$ needs to transform both $\Delta x^1\gets \Delta x^{1’}$ and $\Delta t \gets \Delta t’$</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507213334706.png" alt="image-20230507213334706" style="zoom: 50%;" /></td>
      <td>“consequence” of using ordinary velocities</td>
    </tr>
    <tr>
      <td>Proper Time</td>
      <td>$\Delta \tau$ instead of $\Delta t$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$\Delta x_\mu \Delta x^\mu \equiv \Delta s^2 \equiv (c\Delta \tau)^2$</td>
      <td>$\Delta \tau$ is time for a photon to travel $\Delta s$, which is <strong>invariant under transformation</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$\Delta \tau = \Delta t / \gamma$</td>
      <td>derived from realizing $\Delta \vec{x}\cdot \Delta \vec{x} = (v\Delta t)^2$, and that $\Delta x_\mu \Delta x^\mu = c^2\Delta t^2 - \Delta \vec{x}\cdot \Delta \vec{x}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td><mark>this is actually quite useful later on since it means $d \tau / dt = 1/\gamma$</mark></td>
    </tr>
    <tr>
      <td>Four velocity</td>
      <td>$\frac{\Delta x^\mu}{\Delta \tau}\equiv u^\mu = \gamma (c, \vec{v})$</td>
      <td>is a four vector and transforms with $u^{\mu^{‘}} = \Lambda_\mu^\mu u^\nu$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>$\vec{v}$ is <strong>ordinary velocity</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$u_\mu u^\mu = \gamma^2(c^2 - v^2)=c^2$ is invariant</td>
      <td> </td>
    </tr>
    <tr>
      <td>Four momentum</td>
      <td>$P^\mu \equiv c\cdot m u^\mu$</td>
      <td>we put an extra $c$ so that we get the below</td>
    </tr>
    <tr>
      <td> </td>
      <td>$P^\mu = (\gamma mc^2 , c \gamma \vec{p}) = (E_{relat}, c\vec{p}_{relat})\equiv (E, c \vec{p})$</td>
      <td><mark>relativistic</mark> energy and momentum</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>you can show that $E_{rela} = \gamma mc^2$ includes both $mc^2$ and $(1/2)mv^2$ by expanding $\gamma$. It really is <strong>“total energy”</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td>$P_\mu P^\mu = (mc^2)^2$ is invariant</td>
      <td> </td>
    </tr>
    <tr>
      <td>Mass-Energy equivalence</td>
      <td>$P_\mu P^\mu = E^2 - p^2c^2 = m^2c^4$</td>
      <td>all comes naturally using four vector</td>
    </tr>
    <tr>
      <td>Energy/Momentum of Photon</td>
      <td>$E=pc$</td>
      <td>special case since $m = 0$, then the formula $E=mc^2 \gamma$ doesn’t work as both numerator and denominator has zero</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>hence comes from Mass-energy equivalence</td>
    </tr>
    <tr>
      <td>Conservation of relativistic energy and momentum</td>
      <td>$P^\mu_{\mathrm{IN}}=P^\mu_{\mathrm{OUT}}$</td>
      <td>note that being invariant has nothing to do with being conserved (e.g. mass is invariant but not conserved = some goes into energy)</td>
    </tr>
    <tr>
      <td>Center of mass energy</td>
      <td>find by $P_\mu P^\mu  = P_{CM_\mu} P_{CM}^\mu =E_{CM}^2$</td>
      <td>where $P_{CM}^\mu = (E_{CM},0)$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>utilizing the fact that $P_\mu P^\mu$ is invariant in <em>any frame</em></td>
    </tr>
    <tr>
      <td>Relativistic Force</td>
      <td>$\vec{F} = \frac{d\vec{P}_{relat}}{dt} = \frac{d}{dt} \frac{m\vec{v}}{\sqrt{1-v^2/c^2}}$</td>
      <td>we use the relativistic momentum <strong>but ordinary time</strong></td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>as a result, transformation looks somewhat ugly</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>but only <em>this form</em> is consistent with the force in $\vec{F}=q(\vec{E}+\vec{v}\times \vec{B})$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>can use this to compute trajectory of particles under some force</td>
    </tr>
  </tbody>
</table>

<p><em>Mass is not conserved</em>:</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507215614936.png" alt="image-20230507215614936" style="zoom: 67%;" /></p>

<hr />

<p><em>Conservation of Energy and Momentum using Four vectors</em></p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507220130230.png" alt="image-20230507220130230" /></p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507220041890.png" alt="image-20230507220041890" style="zoom: 33%;" /></p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507220153432.png" alt="image-20230507220153432" style="zoom: 18%;" /></p>

<hr />

<p><em>Trajectory of a particle given some force</em>. Let an object move from rest at $t=0$ under <em>constant force</em>. what is $x(t)$ over time?</p>

<p>The key is to think that $\vec{F} = d\vec{p}_{relat}/dt$, which tells you what is $\vec{v}(t)$, then you can integrate to get $x(t)$</p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507221355367.png" alt="image-20230507221355367" style="zoom:33%;" /></p>

<p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507221417439.png" alt="image-20230507221417439" style="zoom:30%;" /></p>

<h2 id="relativistic-electrodynamics">Relativistic Electrodynamics</h2>

<p>Here you will see that:</p>

<ul>
  <li>magnetic process is a relativistic effect of electric process, i.e. you can have magnetic field in one frame, <strong>boost in another frame</strong>, and obtain no magnetic field but electric field (in the end <strong>what matters is the actual force = affecting particles’ motion</strong>)</li>
  <li>Maxwell’s equation is naturally relativistic</li>
  <li>we will show the above by <strong>re-writing</strong> (no new physics) every related Maxwell quantities in <strong>four vector/matrix formation</strong></li>
</ul>

<table>
  <thead>
    <tr>
      <th>Condition/Name</th>
      <th>Equation</th>
      <th>Comments</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Example of mixing $\vec{E}$ and $\vec{B}$</td>
      <td>$F_M = \frac{1}{\gamma} F_E’$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>frame $S$:<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507222315675.png" alt="image-20230507222315675" style="zoom:15%;" /></td>
      <td>a charge moving with $v$. Wire is charge neutral hence $\vec{E}=0$</td>
    </tr>
    <tr>
      <td> </td>
      <td>frame $S’$:<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507222340931.png" alt="image-20230507222340931" style="zoom:15%;" /></td>
      <td>boost into the charge’s frame, now $\vec{v}\times \vec{B}=0$, but $\vec{E}\neq 0$ now!</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>because the charge density changes: $\rho = #\text{charges}/(\Delta x \Delta y \Delta z)$, and one of the dimension experience Lorentz contraction</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>as a result line density $\lambda = (\rho_+’ - \rho_-‘)A \neq 0$ in frame $S’$, hence obtain electric field</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>essentially, what changed under boosts are the <mark>charge densities</mark></td>
    </tr>
    <tr>
      <td>Transformation of $E,B$ field</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507222905638.png" alt="image-20230507222905638" /></td>
      <td>when boosting into some frame with $v$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from parallel plate, and arguing how surface charge <em>density</em> is the only thing that changed</td>
    </tr>
    <tr>
      <td>Transformation of $E,B$ field in Tensor Notation</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507223148861.png" alt="image-20230507223148861" style="zoom:15%;" /></td>
      <td>equivalent of the above!!</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>note that this summation is different from matrix multiplication</td>
    </tr>
    <tr>
      <td>Field Tensor</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507223216187.png" alt="image-20230507223216187" /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>$F^{\mu\nu’} =\Lambda^{\mu}<em>\lambda \Lambda</em>{\sigma}^{\nu}F^{\lambda \sigma}$</td>
      <td>in summation notation</td>
    </tr>
    <tr>
      <td> </td>
      <td>$F^{\mu\nu} F_{\mu\nu}= 2(E^2/c^2 - B^2)$ is invariant</td>
      <td> </td>
    </tr>
    <tr>
      <td>Dual of Field Tensor</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507223345979.png" alt="image-20230507223345979" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Four current and conservation of charges</td>
      <td>$J^\mu \equiv (c\rho, \vec{J})$</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507223748095.png" alt="image-20230507223748095" /></td>
      <td>is equivalent of <img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507223836807.png" alt="image-20230507223836807" style="zoom: 67%;" /></td>
    </tr>
    <tr>
      <td>(helpers) Transformation of Derivatives</td>
      <td>$\frac{\partial }{\partial x^\mu} \equiv (\frac{\partial }{\partial x^0}, \frac{\partial }{\partial x^1}, \frac{\partial }{\partial x^2}, \frac{\partial }{\partial x^3}) \equiv \partial_\mu$</td>
      <td>lower derivatives for upper index</td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507224333843.png" alt="image-20230507224333843" style="zoom: 6%;" /></td>
      <td><em>transformation</em> of derivatives</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>derived from <img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507224244738.png" alt="image-20230507224244738" style="zoom: 5%;" /></td>
    </tr>
    <tr>
      <td> </td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507224353232.png" alt="image-20230507224353232" style="zoom:33%;" /></td>
      <td><em>raising</em> derivatives</td>
    </tr>
    <tr>
      <td>Derivatives to D’ Lambertian</td>
      <td><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507224513593.png" alt="image-20230507224513593" style="zoom:8%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td>Re-Expressing Maxwell’s equation</td>
      <td>$\frac{\partial F^{\mu\nu}}{\partial x^\nu} = \partial_\nu F^{\mu\nu} = \mu_0 J^\mu$<br />$\frac{\partial G^{\mu\nu}}{\partial x^\nu} = \partial_\nu G^{\mu\nu} = 0$</td>
      <td>same as all four of Maxwell’s equation</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>for example<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507224812071.png" alt="image-20230507224812071" style="zoom:15%;" /></td>
    </tr>
    <tr>
      <td>Re-Expressing Lorentz Force</td>
      <td>$K^\mu \equiv \frac{dp^\mu}{d\tau}$</td>
      <td>proper force (instead of $\vec{F} = \frac{d\vec{P}_{relat}}{dt}$)</td>
    </tr>
    <tr>
      <td> </td>
      <td>$K^\mu = q u_\nu F^{\nu\mu}$</td>
      <td>equivalent of $\vec{F}= q(\vec{E} + \vec{v}\times \vec{B})$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>for example<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507225102436.png" alt="image-20230507225102436" style="zoom:15%;" /></td>
    </tr>
    <tr>
      <td>Re-Expressing Potentials</td>
      <td>$A^\mu = (\phi/c, \vec{A})$</td>
      <td>four potential</td>
    </tr>
    <tr>
      <td> </td>
      <td>$F^{\mu\nu} = \frac{\partial A^\nu}{\partial x_\mu}-\frac{\partial A^\mu}{\partial x_\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu$</td>
      <td>equivalent to $\vec{E} =- \nabla \phi - \frac{\partial \vec{A}}{\partial t}$ and $\vec{B}=\nabla \times \vec{A}$</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td>notice that this is upper index derivative. In a <strong>mostly positive metric</strong> you get an extra minus sign for $x^0$ term:<img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230507230017010.png" alt="image-20230507230017010" style="zoom: 15%;" /></td>
    </tr>
    <tr>
      <td>Re-expressing Gauges and “Wave Equation”</td>
      <td>$A^\mu \to A^\mu + \frac{\partial \chi}{\partial x_\mu}$</td>
      <td>the above hints at Gauge transformation, won’t change $F^{\mu\nu}$</td>
    </tr>
    <tr>
      <td> </td>
      <td>$\partial^\nu\partial_\mu A^\mu = \square^2 A^\mu = -\mu_0 J^\mu$</td>
      <td>equivalent to<br />$\square^2\phi = - \rho / \epsilon_0$<br />$\square^2 \vec{A} = - \mu_0 \vec{J}$</td>
    </tr>
  </tbody>
</table>

<p>List of good questions:</p>

<ul>
  <li>
    <p>12.6 = need to understand the consequence of $t\neq t’$</p>
  </li>
  <li>
    <p>12.8 = not entirely sure how space-time works, potentially needed to deal with hyperbolic space</p>

    <ul>
      <li>one take-away from this is: let me be traveling at $v$ w.r.t. some stationary observer. <strong>When I travelled $d’ = v \Delta t’$ for $\Delta t’$ in my frame,</strong> to other people they <strong>think I travelled $d=v \Delta t \neq d’$!</strong> The “real distance” should be $d’=v\Delta t’$.</li>
    </ul>
  </li>
  <li>
    <p>12.13 = how to draw simultanoues event</p>

    <ul>
      <li>two events simultaneous on a reference frame $\neq$ an observer will see them simultaneously (i.e. no light ray)</li>
    </ul>
  </li>
  <li>
    <p>12.36 = get object <em>velocity</em> from relativistic <em>energy</em></p>
  </li>
  <li>
    <p>professor added question: how <em>rapidity</em> can be used with energies</p>

    <p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230413202735577.png" alt="image-20230413202735577" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>professor added question: Lorentz transfomation and what does a free-fall trajectory look like</p>

    <p><img src="/lectures/images/2023-05-11-PHYS3008_EM_n_Optics/image-20230413225600385.png" alt="image-20230413225600385" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>professor added question of how to transform momentum</p>
  </li>
  <li>
    <p>12.51: $F_{\mu\nu}F^{\mu\nu}$ is really just summing index, not matrix multiplication</p>
  </li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Recap]]></summary></entry><entry><title type="html">POLS1201 Intro to American Politics</title><link href="/lectures/2022@columbia/POLS1201_Intro_to_American_Politics.html/" rel="alternate" type="text/html" title="POLS1201 Intro to American Politics" /><published>2023-05-11T00:00:00+00:00</published><updated>2023-05-11T00:00:00+00:00</updated><id>/lectures/2022@columbia/POLS1201_Intro_to_American_Politics</id><content type="html" xml:base="/lectures/2022@columbia/POLS1201_Intro_to_American_Politics.html/"><![CDATA[<p>Note that a lot of content comes from relevant publications and the <a href="https://openstax.org/books/american-government-3e/pages/1-introduction">OpenStax book</a> (Krutz, G., &amp;  Waskiewicz, S. (2021). <em>American Government 3e</em>. Houston, Texas: OpenStax.)</p>

<h1 id="logistics-and-introduction">Logistics and Introduction</h1>

<ul>
  <li>no recording</li>
  <li>occasional attendance pools/quizzes</li>
  <li>two midterms (02/20 and 03/29) and a cumulative final</li>
  <li>data assignment</li>
</ul>

<p><strong>Approaches within American Politics:</strong></p>

<ul>
  <li>the study of <em>political behavior</em>:
    <ul>
      <li>what does the public actually want?</li>
      <li>how do interest group influence policy?</li>
      <li>what influence who wins an election?</li>
    </ul>
  </li>
  <li>studying specific kinds of organizations - <em>institutions</em>
    <ul>
      <li>Why does congress do so little (or so much)?</li>
      <li>What predicts how the Supreme Court will rule in a lawsuit?</li>
      <li>What pushes state governments to adopt policies?</li>
    </ul>
  </li>
  <li>Yet there is a also a “separate” branch on <em>race, ethnicity, and politics</em>.
    <ul>
      <li>technically those considerations should be embedded into the first two approaches, yet from a developmental perspective this is not what happened</li>
    </ul>
  </li>
</ul>

<p><strong>Approaches in this course</strong>:</p>

<ul>
  <li><em>methodological individualism</em>: explain political phenomenon as a result of (aggregates of) <strong>individual decisions</strong></li>
  <li><em>individual rationality</em>: explain political phenomena as product of people pursuing <strong>subjective interest</strong>
    <ul>
      <li>but of course in reality, <em>what</em> are their “subjective interest” is unknown. (e.g. poor voters voting for candidates could make them even poorer)</li>
      <li>also accidents happen, hence it can be difficult to study only a single event, v.s. <strong>comparing across a category of events</strong> = inference by <strong>drawing parallels</strong> with other similar events!</li>
    </ul>
  </li>
</ul>

<p><strong>Key questions we will ask</strong>:</p>

<ul>
  <li>often decisions are based on a few important actors. Who are they?</li>
  <li>What do they want?</li>
</ul>

<h1 id="theoretical-frameworks-and-political-traditions">Theoretical Frameworks and Political traditions</h1>

<p>Course sign-up and a <strong>prisoner’s dilemma</strong>: because students can choose to sign-up for “shopping” classes</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>$S_2$ No Extra Classes</th>
      <th>$S_2$  Extra Classes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$S_1$ No Extra Classes</td>
      <td>Low Churn (2,2)</td>
      <td>$S_2$ exploits (4,1)</td>
    </tr>
    <tr>
      <td>$S_1$ Extra Classes</td>
      <td>$S_1$ exploits (1,4)</td>
      <td>High Churn (3,3)</td>
    </tr>
  </tbody>
</table>

<p>In this setup:</p>

<ul>
  <li>the numbers indicate rankings for each, e.g. for $S_2$: 1) $S_2$ exploit, 2) low churn, 3) high churn, 4) $S_1$ exploits.</li>
  <li>Signing up for extra classes are <em>always individually rational</em> (at the expense of others)</li>
</ul>

<p>But what if: social norm solution: signing up extra cause social isolation?</p>

<blockquote>
  <p><strong>Model</strong>: a system of concepts that relates series of observable phenomenon to one another. “A way to navigate more efficiently through those concepts”. How is it useful in social science?</p>

  <ul>
    <li>brief summary of existing knowledge/factors and allow for <em>accumulation of additional knowledge easier</em> (e.g. understand what questions/factors are relevant in order to understand novel scenarios)</li>
    <li>can be tested</li>
    <li>can be used to <em>guide decisions</em> we make</li>
  </ul>
</blockquote>

<h2 id="politics-as-collective-action">Politics as Collective Action</h2>

<p>All political action is ultimately <strong>individual action</strong>. However, only when groups of individuals come together to take <strong>collective action</strong> can they make political decisions that lead to change</p>

<h3 id="background">Background</h3>

<blockquote>
  <p><em>Recall</em> that</p>

  <ul>
    <li><strong>Left-wing</strong>: Left-wing believes in that they believe society is best served with an expanded role for the <strong>government</strong>.</li>
    <li>
      <p><strong>Conservative</strong>: People on the right believe that the best outcome for society is achieved when <strong>individual rights</strong> and civil liberties are paramount and the role of gov is minimized</p>
    </li>
    <li>
      <p><strong>Liberal</strong>: Left-wing, federalist. Prefer more regulation and services like free universal health care to be provided by the government to all citizens.</p>
    </li>
    <li><strong>Conservative</strong>: Right-wing, anti-federalist, Prefer smaller government, less regulation, most services to be provided by the private sector in a free market, and a literal interpretation of the Constitution
      <ul>
        <li>conservatives desire security, predictability and authority more than liberals do, and liberals are more comfortable with novelty, nuance and complexity.</li>
        <li>so conservatives opposes gay marriage, abortion and embryonic stem cell research.</li>
      </ul>
    </li>
    <li><strong>Federalists</strong> wanted a <em>stronger national government</em> and the ratification of the Constitution to help properly manage the debt and tensions</li>
  </ul>

  <p>Last but not least</p>

  <table>
    <thead>
      <tr>
        <th style="text-align: center">Democrats</th>
        <th style="text-align: center">Republics</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: center">In a democracy, the <mark>community of people</mark> are considered to hold power over how they are governed. Kings and tyrants are seen as threats to the innate rights of the people. As such, all eligible citizens get equal say in decisions.</td>
        <td style="text-align: center">Republics are in opposition to rulership by a single person. All eligible citizens get equal say in decisions through <mark>elected representatives</mark>. Unalienable rights of individuals are protected by law to safeguard against a majority abusing the minority</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p><strong>Ideas/Insights from “Tragedy of Commons”</strong></p>

<ul>
  <li><strong>morality</strong> of an act is a function of the state of the system at the time it is performed (i.e. time and context dependent).
    <ul>
      <li>As a result, it is very difficult to make laws persistent. Hence the current epicyclic solution is, by law we delegate to bureaus</li>
    </ul>
  </li>
  <li><strong>mutual coercion</strong>. the social arrangement that produce <em>responsibility</em> are arrangements that create coercion
    <ul>
      <li>on tax systems: Who enjoys taxes? We all grumble about them. But we accept compulsory taxes because we recognize that voluntary taxes would favor the conscienceless. We institute and support taxes and other coercive devices to <em>escape the horror of the <strong>commons</strong></em>.</li>
      <li>another example: band-robbing. We thereby <em>infringe on the freedom</em> of would-be robbers we neither deny nor regret.</li>
      <li>“individuals locked into the logic of the commons are free only to bring on universal ruin (i.e. deplete themselves); once they see the ncessity of mutual coecion, they become free to pursue other goals” $\implies$ “Freedom is the recognition of necessity”</li>
    </ul>
  </li>
  <li>why is it so hard for <strong>reforms</strong> to happen? Automatic rejection of proposed is based on two unconscious assumptions a) that the status quo is perfect; b) that the choice we face is between reform and no action $\implies$ <em>no action at all while we wait for the perfect proposal</em>. But to be fair, we should see the status quo <em>as an action</em>, and then we can make better comparison.</li>
  <li>“perhaps the simplest summary of this analysis of man’s population problems is this: the commons is justifiable only under conditions of low population density” $\gets$ we abandoned commons n food gathering, enclosing farms and restricting pastures; somewhat later we saw that the commons as a place for waste disposal would also have to be abandoned $\implies$ humans (<strong>in a large scale</strong>) can’t establish self-organization to make a commons work/sustainable</li>
</ul>

<p><strong>How People become political</strong>?</p>

<ul>
  <li>
    <p>The gradual process of developing values and beliefs, of people becoming who they are as adults, is <strong>socialization</strong>, and the slow development of who a person becomes as a political being is <strong>political socialization</strong>. This process shapes your current political belief, and is influenced by countless factors including partly your genetics but also the <em>environment</em> (e.g. your hometown, school, etc.).</p>

    <p>Your social and physical environments <mark>do not determine</mark> your political personality, but they can have an important influence.</p>
  </li>
  <li>
    <p>The <strong>family</strong> is usually considered the most important influence on both a person’s overall socialization and their political socialization.</p>

    <ul>
      <li>but this could also be a complicated factor. How these changing family structures and living conditions impact political socialization?
        <ul>
          <li>e.g. As of 2016, a higher percentage (52 percent) of 18-to-29-year-olds in the <mark>United States</mark> were living with their parents than at any time since 1900. Among wealthy countries, the percentage of 15-to-29 year-olds living with their parents varied from about 80 percent in <mark>Italy</mark> to 30 percent in <mark>Canada</mark>.</li>
        </ul>
      </li>
      <li>but also broader family environment could affect your political view: In China, <em>caring for one’s parents</em> is a <em>sacred</em> duty; in Norway, it is more often seen as an <em>obligation of the government</em>.</li>
    </ul>
  </li>
  <li>
    <p>Another important factor is <strong>your peers</strong>, especially when you grow older/more mature, people tend to <strong>spend more time with your peers than parents</strong></p>

    <ul>
      <li>To the extent that young people, and indeed all individuals, can <em>choose</em> their social networks rather than being placed in them by virtue of their location, it is more likely that peer networks will <strong>reinforce existing beliefs</strong>, attitudes, and behaviors rather than change them.</li>
      <li>One complicating factor is that now, your peers/social network can be <strong>virtual</strong>, hence “a young person’s peers can be almost anywhere in the world”. In this case, political scientists are still trying to decipher what this means for political socialization.</li>
    </ul>
  </li>
  <li>
    <p>Other interesting factors include:</p>

    <ul>
      <li><strong>Ethnicity</strong>: people from dominant ethnic group may assume that politics and government should favor their interests, as they are the majority. Ethnic minorities, in contrast, may be socialized to feel the sting of discrimination and to view the government as no friend.</li>
      <li><strong>Religion</strong>: atheists are more likely to believe that governmental policy should not be based on religious principles.</li>
    </ul>
  </li>
  <li>
    <p>As people are socialized, they become part of larger groupings of individuals with <strong>common characteristics</strong>. The next sections discuss these larger groupings.</p>
  </li>
</ul>

<p><strong>How people express their political identity</strong></p>

<ul>
  <li>The shared <em>political</em> attitudes, values, goals, and practices common to members of a political group, such as a country, a party, or any other political organization or grouping, is the group’s <strong>political culture</strong>.</li>
  <li>A country’s political culture frames how individuals in that society see themselves, as well as their relation to gov.
    <ul>
      <li>e.g. researchers asked <strong>Americans</strong> and <strong>Europeans</strong>, “What’s more important in our society, that everyone can be free to pursue their life’s goals without interference from the state or that the state plays an active role in society so as to guarantee that nobody is in need?” Almost six in 10 Americans surveyed responded that <em>individual freedom</em> was more important, while nearly eight in 10 Lithuanians, whose country was a part of the collectivist Soviet Union for nearly 50 years, responded that the state’s active role was more important.</li>
      <li>therefore, people from United States <mark>*tend*</mark> to <strong>prioritize personal freedom and individual responsibility over more community-centered values</strong>. Of course, not all Americans favor individual freedom over state intervention = <mark>no stereotyping!</mark></li>
    </ul>
  </li>
  <li>Those within a society who, by virtue of their wealth, status, position, and power, have the <strong>greatest influence over the country’s political agenda</strong>, its policy decisions, and its decision-making cadre are the society’s <strong>political elite</strong>.
    <ul>
      <li>The <em><strong>degree</strong> of influence and domination</em> of elite culture varies from country to country.
        <ul>
          <li>At the extreme, in <mark>North Korea</mark>, the ruling class, led by Supreme Leader Kim Jong-un, controls every aspect of political life.</li>
          <li>At the other side would be <mark>New Zealand</mark>. But even the relatively egalitarian New Zealand, however, those with money, status, and power tend to set the agenda, influence policy decisions, and dominate the decision-making process.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The broadest culture within a country is its <strong>mass culture</strong> (e.g. what movies you like to watch). The lines between elite and mass cultures are not always distinct.
    <ul>
      <li>Prior to the rise of newspapers, radio, and television, <em>mass culture (including political culture) did not exist</em>. All culture was local.</li>
    </ul>
  </li>
  <li>However, as media options proliferate, mass culture diminishes and <strong>minority cultures flourish</strong>. Mass culture, including mass political culture, is <strong>weakening</strong>.
    <ul>
      <li>e.g. About 60 percent of the adult population in America watched the presidential debates between Nixon and Kennedy in 1960. In 2020, even during a highly contentious presidential campaign between President Donald Trump and former Vice President Joe Biden, fewer than 30 percent of adults watched the debates</li>
    </ul>
  </li>
</ul>

<h2 id="collective-dilemma">Collective Dilemma</h2>

<blockquote>
  <p><strong>Key question</strong>: thinking through a collective action framework: How do people organize themselves and behave?</p>
</blockquote>

<p><em>Examples of Collective Actions</em>:</p>

<ul>
  <li>
    <p>Speaker of the House Election in 2023</p>
  </li>
  <li>
    <p>Columbia/Barnard course enrollment</p>
  </li>
</ul>

<p><strong>Making Group Decisions and Collective Dilemma</strong>:</p>

<ul>
  <li>If all people agreed on everything, there would be no collective dilemmas. But because individuals do have differing needs, preferences, and goals, they have to overcome challenges to make a decision. Whenever two or more individuals need to make a plan or resolve a conflict and those involved do not agree on the solution, there is a <strong>collective dilemma</strong>.
    <ul>
      <li>e.g. For 18 days in 2013 and then again for 35 days in 2018–2019, the US government shut down because Congress and the president could not agree on ways to fund federal operations.</li>
    </ul>
  </li>
  <li>causes of collective dilemma include
    <ol>
      <li>
        <p>The first is when the participants disagree because they have <strong>irreconcilable</strong> preferences/differences.</p>

        <ul>
          <li>e.g. reduce debt increase or aid to Ukraine</li>
        </ul>
      </li>
      <li>
        <p>(<strong>coordination problem</strong>) when participants generally agree on what they want to do but <strong>disagree over the details.</strong> Collective action will success only if you <em>do get people doing it</em></p>

        <ul>
          <li>protest march: a million people protested <em>on the same day/time</em></li>
          <li>political institutions make public decision can be seen as a solution to this problem</li>
        </ul>
      </li>
      <li>
        <p>when individual motivations are contrary to the groups’ mutual interests = <strong>collective action problem</strong>. E.g. individuals, acting rationally in pursuit of their <strong>self-interest,</strong> have incentives to make decisions that are harmful to the interests of others</p>
        <ul>
          <li>
            <p>note that the first two can be seen as some kind of <em>coordination costs</em>. This problem, like with prisoner’s dilemmas, is that individuals have strong incentives to do things that are not socially beneficial.</p>
          </li>
          <li>
            <p>this problem can be broadly categorized into:</p>
            <ol>
              <li>
                <p><strong>tragedy of the commons</strong>: depletion of a resource available to all (e.g. threats to environment/health);relevant to a resource that can be <em>exhausted</em></p>

                <ul>
                  <li>e.g. fresh water</li>
                  <li>if everyone has access to a commons, then there is an incentive for them to take more than needed $\implies$ can sell this scarce resource $\implies$ benefit today without consideration of future consequences (depletion)
                    <ul>
                      <li>i.e. users ignores cost imposed on others, focus on own short-term benefits</li>
                    </ul>
                  </li>
                  <li><em>too rapid inclusion of others</em>: new users do not share a similar understanding of how a resource work $\implies$ members of the initial community may feel threatened and hence start to join race to exploit the resource</li>
                </ul>
              </li>
              <li>
                <p><strong>free riding</strong>: not participating in a group activity nonetheless benefit from the <em>public activity</em>; also really hard to <em>exclude people</em> from using it, and <em>not exhausted by use</em></p>

                <ul>
                  <li>
                    <p>e.g. Columbia protest for increasing of pay, where many people can benefit without joining the protest</p>
                  </li>
                  <li>in small groups, this might be fine through mutual peer pressure and you care about others feelings. In larger political world, the problem of free riding is much harder to spot and manage</li>
                  <li>e.g. to help climate change, the Paris Climate Accord of 2015, the agreement of 197 countries to limit global warming. However, in reality only a few really implemented those changes. As a result, change is difficult to achieve, given its tendency to <strong>favor the status quo</strong>. (to suggest that climate change politics are only a matter of free riding would be an oversimplification.)</li>
                </ul>
              </li>
              <li>
                <p><strong>prisoner’s dilemma</strong>: individuals act strategically in ways that ultimately harm themselves, demonstrates why it can be challenging to get allies to work together.</p>

                <ul>
                  <li>
                    <p>consider two people being caught by the police, and is asked separately to point out if the others committed crime:</p>

                    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230121155451809.png" alt="image-20230121155451809" style="zoom:30%;" /></p>

                    <p>This is the prisoner’s dilemma: individuals, acting strategically in their own self-interest (want to accuse others), have incentives that lead them to take actions that result in unnecessarily negative outcomes for both parties.</p>
                  </li>
                  <li>
                    <p>an example in real life is: Two opposing political candidates may each prefer to run only positive campaign ads, but each fears the other will “go negative” to gain an advantage. Both candidates consequently <strong>run negative ads</strong>, which tarnish the reputations of each.</p>
                  </li>
                  <li>
                    <p>one key feature of prisoner’s dilemma is that the <mark>optimal policy for one person $\neq$ the optimal policy for the entire group</mark></p>
                  </li>
                </ul>
              </li>
            </ol>

            <blockquote>
              <p>The tragedy of the commons and the prospect of free riding are especially relevant for slow-growing crises like climate change.</p>
            </blockquote>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>Note that in practice, a bunch of these problems could come up combinatorially.</p>

<h3 id="potential-solutions-to-collective-dilemma">Potential Solutions to Collective Dilemma</h3>

<p><mark>Solutions</mark> to those collective dilemma include include:</p>

<ol>
  <li>
    <p><strong>rules for making a decision</strong> when there is <mark>irreconcilable</mark> preference, e.g. majority vote.</p>

    <ul>
      <li>majority voting: more than 50% agreed</li>
      <li>plurality voting: proposal with most votes</li>
      <li>super-majority: requires sometimes 60%, 67%, or even 75% to agree. Otherwise status-quo = no change
        <ul>
          <li>e.g. supermajority is found in most <strong>US</strong> courtrooms = decide if to jail a person</li>
          <li>e.g. constitutional changes subject to supermajority votes and laws changeable by a simple majority.
            <ul>
              <li>A constitution typically outlines the government’s general powers and duties, while laws fill in the specifics regarding these matters.</li>
            </ul>
          </li>
          <li>One sensible rationale is that the greater the consequences of a decision, the greater the need for a supermajority to guard against making rash or incorrect decisions.</li>
        </ul>
      </li>
      <li>Whenever a <strong>supermajority</strong> rule exists, the status quo is more difficult to change.</li>
    </ul>
  </li>
  <li>
    <p><strong>delegate decision making to a person</strong>, or <strong>do everything by group decision</strong> when there is disagreement over details</p>

    <ul>
      <li>Delegating power to a single person reduces <strong>transaction costs</strong> but increases <strong>conformity costs</strong>. Group decision-making is likely to reduce conformity costs but to increase transaction costs.</li>
      <li><strong>transactional costs</strong>: how much time and effort to make a decision/implement the solution
        <ul>
          <li>e.g. decide amongst 300 people to agree on where to eat = high transactional cost</li>
          <li>e.g. If you are deeply committed to maintain the status quo, then you just need to raise the transaction costs high enough so that no changes can be enacted.</li>
        </ul>
      </li>
      <li><strong>conformity costs</strong> are the “price” those who do not get what they want must
        <ul>
          <li>e.g. in primary/secondary school, high conformity cost as everyone needs to study the same curriculum. In college, lower as you can choose your major/minor, but higher transactional costs as it takes additional registration effort</li>
        </ul>
      </li>
    </ul>

    <p>Political Institutions as solutions to those Problems of Collective Behavior!</p>

    <blockquote>
      <p><strong>Institutions</strong>: a social structure, rule or pattern than can <mark>influence collective behavior</mark>. For example, the supreme court, or the university registrar (determines rule of how you can join the class if you are on the waitlist)</p>
    </blockquote>
  </li>
  <li>
    <p>the most difficult amongst all</p>

    <ol>
      <li>
        <p>resolving tragedy of commons</p>

        <ul>
          <li>
            <p><strong>for small groups</strong>: Each of the three main types of collective action problems is easier to solve, at least in principle, when the problems arise within small groups of people (such as families or tribal units) in which the members know each other well</p>

            <ul>
              <li>provide suitable rewards and enforce appropriate punishments are the keys to avoiding or mitigating collective action problems.</li>
            </ul>
          </li>
          <li>
            <p><strong>for large groups</strong>: collective action problems involving large numbers of people cannot rely on personal relationships</p>

            <ul>
              <li>central institution (the government) the authority to protect the commons through <strong>force</strong>. Alternatively, <strong>privatize</strong> them. But there are also problems</li>
              <li>however, gov may use the resource for the benefits of the elites</li>
              <li>The more politically powerful the group previous exploiting the commons, the more <em>difficult</em> it is for elected officials to protect the resource through privatization. Even if no pressure, it is difficult to set the right price “if the government sets the prices (of fishing permits) too low, the resource will be depleted, and if it sets the prices too high, the community will be deprived of a valuable resource.”</li>
            </ul>
          </li>
          <li>
            <p><strong>for communities</strong>: member roughly know each other</p>

            <ul>
              <li>
                <p>previous solutions are somewhat <em>pessimistic</em> of human’s ability to device long-term, sustainable institutions. In practice, there are a lot of instances when human self-organizations are effective</p>

                <ul>
                  <li>groups of people who can identify one another (e.g. via media) are more likely to draw on trust and reciprocity.</li>
                  <li>e.g. those with reciprocity gain a positive reputation, and others become willing to cooperate = <strong>evolved norms</strong></li>
                  <li>lower the perceived cost: users understand the dynamics of the commons, and hence <strong>value its sustainability</strong></li>
                </ul>
              </li>
              <li>
                <p>Nobel prize winner Elinor Ostrom proposed a relatively effective solution without sovereign or privatization.</p>

                <ol>
                  <li>First, the community must engage in collective decision-making so that all relevant interests can participate.</li>
                  <li>Second, the rules the community makes must be clear so that members know what is allowed and what is not.</li>
                </ol>

                <p>If these conditions are in place, the decisions the community makes are likely to be wise and enforceable</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>resolving free rider: <strong>disincentivize free riders</strong></p>

        <ol>
          <li>e.g. keep groups small, or give punishments to those who don’t. However, this requires additional monitoring cost</li>
          <li>e.g. social solidarity: pay taxes because that is what good, patriotic citizens do.</li>
        </ol>
      </li>
      <li>
        <p>resolving prisoners’ dilemma: <strong>make them cooperate</strong></p>

        <ul>
          <li>A participant is least likely to defect when they know that the other participant will punish them if they do. (e.g. if they each know they will be punished if they defect, then they are <em>more likely to remain silent</em>.)</li>
          <li>Once one of the parties defects in a prisoner’s dilemma setting, it is not easy to get the participants to cooperate later.
            <ul>
              <li>As in the persistent conflict between <mark>Israelis</mark> and <mark>Palestinians</mark>, cases where any two groups are locked in intractable disagreements exemplify how tit-for-tat retaliation dominates any possibility of mutual agreement.</li>
              <li>Avoiding this outcome requires a <strong>third party</strong> that can enforce cooperation or punish those who defect to induce future cooperation.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<p><em>Example</em>: How does <strong>policing</strong> help solve collective action problem by intervention?</p>

<p>Consider for a normal <em>individual</em> to intervene and prevent a bad action $A$ to happen:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Positive</th>
      <th>Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Intervene</td>
      <td>prevent action (A), gain pride (P)</td>
      <td>takes effort (E), may be harmed (H), may face retaliation (R)</td>
    </tr>
    <tr>
      <td>No Intervention</td>
      <td> </td>
      <td>Action occurs</td>
    </tr>
  </tbody>
</table>

<p>Therefore, a rational person will intervene if</p>

\[\text{intervene if:  }\quad P -  E - \pi_1 R - \pi_2 H - (1-\pi_3)A &gt; -A\]

<p>where $\pi_1$ is prob of retaliation happens, $\pi_2$ is when that person is harmed, etc.</p>

<p>But if we have <em>coordination</em>, what could change? (e.g. a group of people intervening at the same time)</p>

<ul>
  <li>probabilities would change (e.g. less likely to be retaliated). $\pi_1,\pi_2 \downarrow$, $\pi_3 \uparrow$</li>
  <li>benefit/costs would change (e.g. less effort needed $E$)</li>
</ul>

<blockquote>
  <p><strong>Written laws</strong> can provide this “improvement”, as it can help people <em>coordinate</em> enforcement.</p>

  <ul>
    <li>but of course, don’t forget <em>conformity</em> cost such as make some people particularly prone to punishments</li>
  </ul>
</blockquote>

<p>What’s different for <strong>designated specialist</strong> (e.g. having a police department)? They can <em>still</em> be modeled as individuals we had before, but</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Positive</th>
      <th>Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Intervene</td>
      <td>prevent action (A), gain pride (P)</td>
      <td>takes effort (E), may be harmed (H), may face retaliation (R)</td>
    </tr>
    <tr>
      <td>No Intervention</td>
      <td> </td>
      <td>Action occurs, <mark>Discipline (D)</mark></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>probabilities change (e.g. $\pi_3$ increases as they are trained)</li>
  <li>you get an extra cost of enforcers, hence <mark>more likely</mark> the action being prevented to happen!</li>
</ul>

\[\text{intervene if:  }\quad P -  E - \pi_1 R - \pi_2 H - (1-\pi_3)A &gt; -A\textcolor{red}{-D}\]

<p>like providing public goods, a good solution is to <em>“force”</em> the production, i.e.</p>

<blockquote>
  <p><strong>Institutions as solutions</strong> to collective action problem. A single institution can solve/raise multiple “problems”:</p>

  <ul>
    <li>e.g. (-) police enforcers can preserve group inequality, e.g. more south American drinking alcohol $\implies$ being policed more often $\implies$ treats unequal treatments to south American $\implies$ being checked on/cautioned more than other ethnic groups</li>
    <li>e.g. (+) written laws can coordinate enforcement AND protect property</li>
  </ul>
</blockquote>

<p>But this beg several questions/understandings:</p>

<ul>
  <li>when and why do institutions change?</li>
  <li>When are they <em>resistant</em> to change?</li>
  <li>How are institutions chosen?</li>
</ul>

<h2 id="institutional-change">Institutional Change</h2>

<blockquote>
  <p>Political Institutions can be modeled as <strong>increasing returns processes</strong>. As a result:</p>

  <ol>
    <li>a wide range of outcomes can result from same initial conditions</li>
    <li>small actions at the right time can gave large implications for institutional form</li>
    <li>when an event occur matter! (esp. <em>early changes</em> matters)</li>
    <li>once begun, change is difficult (hard to <em>reverse</em>, i.e. <em>path dependence</em>)</li>
  </ol>
</blockquote>

<p>What is increasing returns and path dependence? From “Increasing Returns, Path Dependence, and the Study of Politics” by Paul Pierson</p>

<ul>
  <li>
    <p>the concept of <strong>Path Dependence</strong> can be used in various ways. Here, the author means</p>

    <ol>
      <li>specific patterns of timing and sequence matter;</li>
      <li>large consequences may result from relatively “small” or contingent events; (also see <em>increasing returns</em>)</li>
      <li>particular courses of action, once introduced, can be virtually impossible to reverse; (also see <em>increasing returns</em>)</li>
    </ol>

    <p>those are different from other prominent modes of argument and explaination in political science $\implies$ attribute “large” outcomes to “large” causes</p>
  </li>
  <li>
    <p><strong>Increasing returns</strong>, or self-reinforcing process, refers to the case that “preceding steps in a particular direction <strong>induce further movement in the same direction</strong>”. As a result, the relative benefits of the current activity (or the cost of switching) compared with other possible options <strong>increase over time</strong>.</p>

    <ul>
      <li>e.g. the Polya urn process in which each time you pick a ball, you need to add a ball of the same color into the urn $\implies$ <em>small</em>/random picks in the <em>beginning</em> gives a <em>large</em> effect to the <em>final</em> equilibrium configuration of balls.</li>
      <li>therefore, <strong>sequence is crucial</strong> (path dependent), and <mark>earlier events matter much more than later ones</mark></li>
    </ul>
  </li>
  <li>
    <p>insights from increasing returns in <em>economics</em></p>

    <ul>
      <li>
        <p>the most prominent example is perhaps on <strong>technology</strong>: a particular technology may achieve a decisive advantage over competitors, although it is <mark>not necessarily the most efficient alternative</mark> in the long run. This is because when it is subject to increasing returns (higher pay-off when more user uses it), being the <em>fastest out of the gate</em> becomes critical. Then, once positive feedback effects kick in, competitors are excluded and people are locked in this technology (e.g. “QWERTY” keyboard)</p>

        <p>However, not all technologies work like this. Arthur (1994) listed the four criteria</p>

        <ol>
          <li><strong>large set-up/fixed costs</strong>: so that a) it becomes smaller for the company when production increases b) individual users have incentive to stick with a single option</li>
          <li><strong>learning effects</strong>: the more you use it, the more knowledge gained in this system, and this loop continues/spur further innovations (e.g. plugin system)</li>
          <li><strong>coordination effects</strong>: benefits an individual receives from a particular activity increase as others adopt the same option.</li>
          <li><strong>adaptive expectations</strong>: users are willing to <em>adapt their behaviors</em> towards their future expectation of a product (even if it is bad)</li>
        </ol>
      </li>
      <li>
        <p>examples that worked like this include</p>

        <ul>
          <li>initial <em>centers of economic activity</em> may act like a magnet and influence the locational decisions and investments of other economic actor (e.g. silicon valley)</li>
          <li>(North 1990a, 95) <mark>new institutions</mark> often entail high fixed or start-up costs, and they involve considerable learning effects, coordination effects, and adaptive expectations. Established institutions generate powerful inducements that reinforce their own stability and further development.</li>
          <li>Additionally, <mark>institutional arrangements</mark> induce complementary organizational forms, which in turn may generate new complementary institution</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>how/why is <strong>political science</strong> related to this idea of increasing returns?</p>

    <ul>
      <li><strong>collective nature of politics</strong> in economics, I make individual decisions (e.g. switch firms) and the outcome is aggregated by the market. In politics, your decision depend to a considerable degree on your confidence that <em>a large number of other people will do the same</em> $\implies$ positive feedback frequently due to <em>adaptive expectations</em>
        <ul>
          <li>recall that in politics, <strong>creating conditions favorable to collective action</strong> is a principal issue in political life</li>
        </ul>
      </li>
      <li><strong>institutional density of politics</strong>. Institutions and policies may encourage individuals and organizations to invest in specialized skills, deepen relationships with other individuals and organizations, and develop particular political and social identities. These activities <em>increase attractiveness of existing institutional arrangements</em> relative to hypothetical alternatives</li>
      <li><strong>political authority and power asymmetries</strong>. relatively small disparities in political resources (e.g. power) among contending groups may <em>widen dramatically</em> over time as positive feedback sets in. For example, a group with slightly more power could change rules of the game <strong>designed to enhance their power</strong></li>
      <li><strong>complexity and opacity of politics</strong>. in economics, there is often a clear metric (e.g. market price) on how good you/your company is performing. Politics is a <strong>far, far murkier</strong> environment.
        <ul>
          <li>researchers argue that actors who operate in a social context of high complexity and opacity are heavily biased in the way they <em>filter information</em> into existing “mental maps” $\implies$ confirming information is incorporated/dis-confirming is filtered out $\implies$increasing returns</li>
          <li><strong>harder to “reverse course”</strong> in politics than in economics $\implies$ path dependent
            <ul>
              <li>unlike <strong>competition</strong> in economics that would allow a sub-optimal firm eventually be replaced, political institutions rarely confront a <em>dense</em> environment of competing institutions that will instantly capitalize on inefficient performance</li>
              <li><strong>short-time horizon of political actors</strong>. many actors are interested in short-term consequences because the decision of voters are take in the short turn $\implies$ pay little attention to long term consequences</li>
              <li><strong>power asymmetry</strong>. political actors may create rule that make preexisting arrangement hard to reverse (e.g. to protect themselves); and there is often high barriers of reform (e.g. unanimity requirement in EU)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><em>“Research Idea”</em>: how can tech such as VR change the density problem</p>
</blockquote>

<h2 id="research-design">Research Design</h2>

<p>An overview:</p>

<ul>
  <li>
    <p>In legal practice, law determines what facts are relevant, but you have to decide what (e.g. numbers) to demonstrate</p>
  </li>
  <li>in advocacy sector, people care the <em>effect</em> of a proposed policy</li>
  <li>in consulting, what changes would affect certain variables in a system</li>
</ul>

<p>In a quantitative study, you may want to:</p>

<ol>
  <li>come up with a model for a key concept (e.g. poverty)</li>
  <li>develop variables to measure the key concept</li>
  <li>develop hypotheses how variables should relate, and other competing models</li>
  <li>collect data</li>
  <li>test (statistically) whether the relationship holds in your data</li>
</ol>

<p>Keep in mind when you are measuring quantities</p>

<ul>
  <li>
    <p>e.g. annual reports released by institutions might be understating; survey results depend on who you asked</p>
  </li>
  <li>e.g. how to measure democratic participation? e.g. voter turn outs, protest participation, watching presidential debates. All of them measure <em>slightly different things</em>, and you need to be clear if your method is <strong>justified</strong>.</li>
  <li>certain data could be <em>biased</em>. e.g. posters showing low support for a president could be due to only people dislike him is in the pool</li>
</ul>

<p>How do you fix the above problems? Consider a candidate claiming election is going to be fraudulent</p>

<ul>
  <li>gather data on many elections where one candidate claims it will be fraudulent, compare them</li>
  <li>gather data on many elections where one candidate claims it will be fraudulent <mark>and some without</mark>, compare them</li>
</ul>

<blockquote>
  <p>Statements about <mark>causation</mark> make claims about what happens with and without some factor, so must define <mark>both kinds of cases</mark></p>
</blockquote>

<p><em>More Examples</em>: a report of studying 173 mass violence happened in US in the past few years</p>

<ul>
  <li>found in those 173 mass violences in common = at some point had suffered from mental illness $\implies$ <mark>does not mean mental illness will cause those violences!</mark></li>
  <li>also does not teach us about how these acts can be prevented</li>
</ul>

<h1 id="institutions-in-us">Institutions in US</h1>

<p>How successful/unsuccessful <strong>institutions can be</strong> at solving the collective action problem?</p>

<blockquote>
  <p><strong>Origin of Separation of Power</strong>: the US separation of power can be traced back to colonial governments by the British. Why did British monarch create strong legislature (alongside the local governments) in it $N$ American colonies?</p>

  <ul>
    <li>purpose of colonies was resource extraction; but governors have the incentive to pocket more than efficient for the crown</li>
    <li>then, this is used as a way for the crown to <mark>control/constrain those local elites</mark></li>
  </ul>

  <p>so it is lodged in the economic reason of those colonial power</p>
</blockquote>

<p>First of all, several key terms</p>

<blockquote>
  <ul>
    <li>
      <p><strong>confederation</strong> = each state is loosely related, like its separate countries = <em>high degree of freedom/autonomy</em></p>
    </li>
    <li><strong>unitary</strong> = power is concentrated in the hands of the central government, while <em>provinces and regions do not enjoy large autonomy</em>
      <ul>
        <li>has nothing to do with democracy/monarchy</li>
      </ul>
    </li>
    <li>
      <p><strong>federal</strong> = federal government as sovereign entity, but also given <em>lower level units (states) to have their own laws</em></p>

      <p><img src="https://openstax.org/apps/archive/20221219.191545/resources/df7dffd16667f5c2ff8a13c934b26b0dc1afa256" alt="A flow chart depicts the three general systems of government: the unitary system, the federation, and the confederation. The unitary system flowchart starts with the National Government, which flows down to the States. Below the chart, it says, “Authority is concentrated in the central government. Examples: United Kingdom, Japan, Sweden.” The Federation flow chart starts with the People on top. The flow branches down and splits between two boxes; the states, and the National Government. Below this chart, it says, “Authority is divided between central and state governments and is derived from the people. Examples: Canada, India, United States under the Constitution”. The Confederation flow chart starts with the States on top, with an arrow flowing down to the National Government. Under this chart, it says “Authority is concentrated in states. Example: United States under the Articles of Confederation”." style="zoom:33%;" /></p>
    </li>
    <li>
      <p><strong>unicameral</strong> legislature has only one chamber, or body, that makes decisions.</p>
    </li>
    <li><strong>bicameral</strong> legislature has two chambers, often with different procedures and powers, that ultimately must work together to make policy and exercise other legislative powers and responsibilities.</li>
  </ul>
</blockquote>

<h2 id="the-us-constitution-basics">The US Constitution Basics</h2>

<blockquote>
  <p><strong>The US Constitution</strong> The Constitution of the United States established America’s national government and fundamental laws, and guaranteed certain basic rights for its citizens.</p>

  <ul>
    <li>
      <p>Its 7 sections (or <em>Articles</em>) detail the core components of how the framers wanted the government to run the country. For example, no the duties of the three main parts of government: the <em>Executive Branch</em>, the <em>Legislative Branch</em>, and the <em>Judicial Branch</em></p>
    </li>
    <li>
      <p><em>The Bill of Rights</em> were <em>first</em> 10 amendments guaranteeing basic individual protections, such as freedom of speech and religion, that became part of the Constitution in 1791. To date, there are 27 constitutional amendments.</p>
    </li>
  </ul>
</blockquote>

<p>Some short historical facts about it:</p>

<ul>
  <li>
    <p>Under America’s first governing document, the Articles of Confederation, the national government was weak and states operated like independent countries.</p>
  </li>
  <li>
    <p>At the 1787 convention, delegates devised a plan for a stronger federal government with three branches—executive, legislative and judicial—along with a system of checks and balances to ensure no single branch would have too much power. This is the US Constitutional Convention in Philadelphia, signed on September 17, 1787.</p>
  </li>
</ul>

<p>Its content in summary (from https://www.pbs.org/newshour/classroom/app/uploads/2013/11/summary-of-the-US-Constitution.pdf)</p>

<ol>
  <li>Article 1: <strong>Legislative Branch</strong>: the <mark>U.S. Congress</mark> makes the laws for the United States. Congress has two parts, called “<mark>Houses</mark>,” the House of Representatives and the Senate.</li>
  <li>Article 2: <strong>Executive Branch</strong>: the President, Vice-President, Cabinet, and Departments under the Cabinet Secretaries carry out the laws made by Congress.</li>
  <li>Article 3: <strong>Judicial Branch</strong>: the <em>Supreme Court</em> decides court cases according to US Constitution. The courts under the Supreme Court decide criminal and civil court cases according to the correct federal, state, and local laws.</li>
  <li>Article 4: <strong>States’ powers</strong>: States have the power to make and carry out their own laws. State laws that are related to the people and problems of their area. States respect other states laws and work together with other states to fix regional problems.</li>
  <li>Article 5: <strong>Amendments</strong>: The Constitution can be changed. New amendments can be added to the US Constitution with the approval by a <mark>two-thirds vote</mark> in each house of Congress (67, 281) and <mark>three-fourth</mark> vote by the states (38).</li>
  <li>Article 6: <strong>Federal powers</strong>: The Constitution and federal laws are higher than state and local laws. All laws must agree with the US Constitution.</li>
  <li>Article 7: <strong>Ratification</strong>: The Constitution was presented to George Washington and the men at the Constitutional Convention on September 17, 1787, Representatives from twelve out of the thirteen original states signed the Constitution. From September 1787 to July 1788, the states meet, talked about, and finally voted to approve the Constitution</li>
</ol>

<p>A few selected amendments which is important/interesting (IMO):</p>

<ul>
  <li>2nd People have the right to <strong>have a weapon to protect themselves</strong>.</li>
  <li>4th The government <em>cannot</em> arrest a person or <em>search their property</em> unless there is “probable cause.”</li>
  <li>13th Slavery is illegal in the United States. (1865)</li>
  <li>14th Every person <strong>born in the USA is a citizen</strong>. An immigrant can become a naturalized citizen. (1868)</li>
  <li>20th The President is inaugurated in January. Congress begins to meet in January. (1933).</li>
  <li>21st Alcohol is legal. <strong>Each state can make laws</strong> about making, selling, and drinking alcohol. (1933).</li>
  <li>22nd The President <strong>cannot serve for more than two terms</strong>. (1951).</li>
</ul>

<hr />

<p>The <a href="https://openstax.org/books/american-government-3e/pages/2-introduction">textbook</a>’s take on the Constitution:</p>

<ul>
  <li>
    <p>should <em>not</em> be seen as a group of like-minded men aligned in their lofty thinking regarding rights and freedoms; you should not refrain from proposing changes just because you admire the <em>longevity</em></p>
  </li>
  <li>
    <p>was designed largely out of necessity following the failure of the first revolutionary government, and it featured a series of <mark>pragmatic compromises</mark> among its disparate stakeholders. It</p>
  </li>
</ul>

<h3 id="origin-of-the-us-constitution">Origin of the US Constitution</h3>

<blockquote>
  <p>How did the Constitution come about?</p>
</blockquote>

<p>The most significant contributions of <mark>Locke</mark>, a seventeenth-century English philosopher, were his ideas regarding the relationship between <strong>government</strong> and <strong>natural rights</strong>, which were believed to be God-given rights to life, liberty, and property.</p>

<p><img src="https://openstax.org/apps/archive/20221219.191545/resources/142952f76fb24a99b4f7847bb319209606c14d58" alt="A painting shows John Locke." style="zoom:50%;" /></p>

<p>for example:</p>

<ul>
  <li>The English Bill of Rights, heavily influenced by Locke’s ideas, enumerated the rights of English citizens and explicitly guaranteed rights to life, liberty, and property.</li>
  <li>Perhaps the most important of Locke’s ideas was regarding the <strong>origins and purpose of government</strong>.
    <ul>
      <li>Most Europeans of the time believed the institution of monarchy had been created by God, and kings and queens had been divinely appointed to rule. Locke, however, theorized that <em>human beings</em>, not God, had created government.</li>
      <li>Locked believed in <strong>social contract</strong>: people sacrificed a small portion of their freedom and consented to be ruled in exchange for the government’s protection of their lives, liberty, and property.</li>
    </ul>
  </li>
  <li>The desire to limit the power of government is closely related to the belief that people should govern themselves $\iff$ the idea of representative government (people vote for choosing representatives)</li>
</ul>

<hr />

<p>A brief history, starting with <strong>Pre-revolutionary Period</strong></p>

<ol>
  <li><mark>US colonists</mark> lived under the rule of the British government for more than a century</li>
  <li>In 1763, Seven Years War between Great Britain and France came to an end.
    <ul>
      <li>Even though US colonists fought alone side British, they were <em>forbidden to purchase land or settle</em> west of the Appalachian Mountains (belonged to the French).</li>
      <li>To raise revenue for post-war, British also imposed of <em>direct taxes</em>: taxes imposed on individuals, yet North American colonists were <em>not allowed to elect representatives</em> to the British Parliament who made this law.</li>
      <li>other similar regulations include Stamp Act (1765), which required that almost all paper goods, such as diplomas, land deeds, contracts, and newspapers, have revenue stamps placed on them; and Townshend Acts (1767), which imposed taxes on many everyday objects such as glass, tea, and paint.</li>
    </ul>
  </li>
  <li>In 1768, US colonists decided to <em>boycott</em> British goods as a mean to show dissent. The British then sent a warship to the city in 1768.</li>
  <li>on the evening of March 5, 1770, an altercation erupted outside the customs house, and soldiers opened fire on the crowd, killing five colonists and injuring six others. <strong>Boston Massacre</strong></li>
  <li>more resistance in 1773, such as <strong>Boston’s Tea Party</strong>, to threw its cargo of tea, owned by the British East India Company, into the water to protest British policies.</li>
  <li>In 1774, British responded by passing a series of laws called the Coercive Acts, intended to punish Boston for leading resistance
    <ul>
      <li>virtually <strong>abolished</strong> town meetings in Massachusetts and otherwise interfered with the colony’s ability to govern itself.</li>
      <li>This assault on Massachusetts and its economy enraged people throughout the colonies, and delegates from all the colonies except Georgia formed the <strong>First Continental Congress</strong></li>
    </ul>
  </li>
  <li>On July 2, 1776, Congress declared American independence from Britain and two days later signed the <strong>Declaration of Independence</strong>.
    <ul>
      <li>drafted by <strong>Thomas Jefferson</strong>, basically on the belief that “God, he wrote, had given everyone the rights of life, liberty, and the pursuit of happiness. People had created governments to protect these rights and consented to be governed by them so long as government functioned as intended.”</li>
    </ul>
  </li>
</ol>

<p><strong>Articles of Confederation</strong>: basically having</p>

<ul>
  <li>no executive or judicial branches</li>
  <li>unicameral legislature with equal representation</li>
  <li>limited central government powers, state responsible for implementing central laws</li>
</ul>

<ol>
  <li>
    <p>aimed to create a new government strong enough to win the country’s independence but <em>not so powerful that it would deprive people</em>. Thus, a <strong>confederation</strong> was created—an entity in which independent, self-governing states form a union</p>
  </li>
  <li>
    <p>The final draft of the <strong>Articles of Confederation</strong>, which formed the basis of the new nation’s government, was accepted by Congress in November 1777. However, as you will soon see, this faced problem of establishing a <em>(too) weak central government</em> that was unable to fund itself, regulate trade, or enforce laws.</p>
  </li>
  <li>
    <p>However, this soon people realized that the Articles had created a central government <strong>too weak to function effectively.</strong></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: left">Weakness of the Articles of Confederation</th>
          <th style="text-align: left">Why Was This a Problem?</th>
          <th>Collective Action Problem?</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: left">The national government could not impose taxes on citizens. It could only request money from the states.</td>
          <td style="text-align: left">Requests for money were usually not honored. As a result, the national government did not have money to <strong>pay for national defense</strong> or fulfill its other responsibilities.</td>
          <td><strong>free-rider</strong> = central government provides some services, but in return you do not give anything back</td>
        </tr>
        <tr>
          <td style="text-align: left">The national government could not regulate foreign trade or interstate commerce.</td>
          <td style="text-align: left">The government could not prevent <strong>foreign countries</strong> from hurting American competitors by shipping inexpensive products to the United States. It could not prevent states from passing laws that interfered with domestic trade.</td>
          <td><strong>coordination problem</strong> = absent of a single policy for people, hence people act on their own plans</td>
        </tr>
        <tr>
          <td style="text-align: left">The national government could not raise an army. It had to request the states to send men.</td>
          <td style="text-align: left">State governments could choose not to honor Congress’s request for troops. This would make it <strong>hard to defend</strong> the nation.</td>
          <td><strong>free-rider</strong>, etc.</td>
        </tr>
        <tr>
          <td style="text-align: left">Each state had only one vote in Congress regardless of its size.</td>
          <td style="text-align: left">Populous states were <strong>less well represented</strong>.</td>
          <td> </td>
        </tr>
        <tr>
          <td style="text-align: left">The Articles could not be changed without a unanimous vote to do so.</td>
          <td style="text-align: left">Problems with the Articles <strong>could not be easily fixed</strong>.</td>
          <td> </td>
        </tr>
        <tr>
          <td style="text-align: left">There was no national judicial system.</td>
          <td style="text-align: left">Judiciaries are important enforcers of national government power.</td>
          <td> </td>
        </tr>
      </tbody>
    </table>

    <p>Additional collective action problems unaddressed:</p>
    <ul>
      <li>no central currency (coordination problem)</li>
    </ul>
  </li>
  <li>
    <p>In 1786, <strong>Shays’ Rebellion</strong> essentially acted as the trigger for people to find a solution and resolve problems related to commerce, members of Congress called for a revision of the Articles of Confederation.</p>

    <ul>
      <li>Led by Daniel Shays, the heavily indebted farmers marched to a local courthouse demanding relief.</li>
      <li>the incident panicked the governor of Massachusetts, who called upon the national government for assistance.</li>
      <li>However, with no power to raise an army, the government had <em>no troops at its disposal</em>.</li>
    </ul>
  </li>
</ol>

<p><strong>The development of the Constitutions</strong></p>

<blockquote>
  <p>Can be seen as an approach to <strong>solve collective action problems</strong> evident under Articles of Confederation, with a central government being too weak.</p>
</blockquote>

<p>An interesting side note when looking at the debate between those large and small states: <em>Who has the most power in bargaining situation, where agreement is necessary for creation of collective good?</em></p>

<ul>
  <li>a state whose presence for this creation of collective good is <strong>very important</strong>, i.e. will hurt a lot if they decided to drop out</li>
  <li>basically, it depends on the payoff if the agreement is made/fails
    <ul>
      <li>without which: small states more vulnerable to attacks</li>
      <li>with which: large states benefit more (e.g. economically) from stability</li>
    </ul>
  </li>
</ul>

<ol>
  <li>
    <p>Because the shortcomings of the Articles of Confederation proved impossible to overcome, the convention that met in Philadelphia in <strong>1787</strong> decided to create an <strong>entirely new government</strong>.</p>
  </li>
  <li>
    <p>There, a few major points of contention among fifty-five delegates as Philedelphia was</p>

    <ul>
      <li><strong>Small States vs. Large States</strong>: each state one vote (New Jersey Plan) or based on population size (Virginia Plan)?
        <ul>
          <li><strong>Virginia Plan</strong>: preferred by large states and nationalists
            <ul>
              <li>bicameral legislature</li>
              <li>representation based on population</li>
              <li>national government could make any law necessary</li>
            </ul>
          </li>
          <li><strong>New Jersey Plan:</strong>
            <ul>
              <li>unicameral legislature</li>
              <li>equal representation of all states</li>
              <li>limited national government authority, but power of do direct taxation</li>
            </ul>
          </li>
          <li>e.g. still relevant in debate today, whether representation should be proportional to <em>citizen population</em> or <em>just population</em></li>
        </ul>
      </li>
      <li><strong>Slavery and Freedom</strong>: Although some southerners shared similar sentiments, none of the southern states had abolished slavery and none wanted the Constitution to interfere with the institution.</li>
      <li><strong>Federal Supremacy v.s. State Supremacy</strong>: favored a strong national government (necessary for the survival and efficient functioning of the new nation) and those who favored limiting its powers and allowing states to govern
        <ul>
          <li>e.g. overturn of Roe v Wade today.</li>
          <li>e.g. president passing executive orders = president making policy change <em>independent of legislature</em></li>
        </ul>
      </li>
      <li><strong>Individual Liberty v.s. Social Stability</strong> <em>guarantee</em> the rights of life, liberty, and property v.s. more important for the national government to maintain order, and this might require it to <em>limit personal liberty at times</em>.</li>
    </ul>
  </li>
  <li>
    <p>Finally in <mark>1787 September</mark>—after compromising many times—they had worked out a new blueprint for the nation. A overview of US Constitutions have been provided before, and here are the solutions to those major points of contention:</p>

    <ul>
      <li>
        <p><strong>The Great Compromise</strong>. Congress = Senate + House of Representatives = <strong>Bicameral</strong>. (Senate) Each state, regardless of size, would have two senators, making for equal representation as in the New Jersey Plan. (House of Representatives) Representation in the House would be based on population.</p>

        <ul>
          <li>gives collective veto power to smaller states</li>
        </ul>
      </li>
      <li>
        <p><strong>Three-Fifths Compromise</strong>. slaveholding states were allowed to count all their free population, including free African Americans and 60 percent (three-fifths) of their enslaved population.</p>
      </li>
      <li>
        <p><strong>Separation of Power and Checks and Balances</strong>: the idea is to solve the challenge of increasing the authority of the national government while ensuring that it did not become too powerful</p>

        <ul>
          <li><strong>separation of powers</strong> dividing the national government into three separate branches and assigning different responsibilities to each</li>
          <li><strong>checks and balances</strong> by giving each of three branches of government the power to restrict the actions of the others, thus requiring them to work together.</li>
        </ul>

        <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230125231801019.png" alt="image-20230125231801019" style="zoom:40%;" /></p>
      </li>
      <li>
        <p><strong>The Federal System</strong>: power is divided between the federal (or national) government and the state governments.</p>

        <ul>
          <li>Great or explicit powers, called <strong>enumerated powers</strong>, were granted to the federal government (e.g. declare war)</li>
          <li>All powers not expressly given to the national government, i.e. <strong>reserved powers</strong>, are for the states (e.g. intrastate commerce and marriage)</li>
        </ul>
      </li>
      <li>
        <p>How is president chosen?</p>

        <ul>
          <li>state legislature decide how electors are chosen = the electoral voting system today</li>
          <li>without majority, i.e. electoral college is in deadlock, then House chooses president</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>After drafting, the last task is to <strong>be ratified by 9 out of 13 states</strong> in Article VII, but obviously while some people are happy, there are some who would be opposing it $\implies$ Federalists and Anti-Federalists</p>

    <ul>
      <li><strong>Federalists</strong> supported the Constitution. They tended to be among the elite members of society, e.g. being wealthy</li>
      <li><strong>Anti-Federalists</strong> feared the power of the national government and believed state legislatures, with which they had more contact, could better protect their freedoms.</li>
    </ul>
  </li>
  <li>
    <p>In the end after major persuasion efforts, the last two to sign were the wealthy, populous states of Virginia and New York.</p>

    <ul>
      <li>In Virginia, <mark>George Washington</mark>, who wrote letters to the convention, changed the minds of many.</li>
      <li>In New York, <mark>Alexander Hamilton, James Madison, and John Jay</mark> wrote a series of essays, beginning in 1787, arguing for a strong federal government and support of the Constitution</li>
    </ul>
  </li>
</ol>

<p><strong>Constitutional Change:</strong> One of the strengths they built into the Constitution v.s. prior Articles of Confederation</p>

<ol>
  <li>Having drafted nineteen proposed amendments, <mark>James Madison</mark> submitted them to Congress. only ten were accepted by three-quarters of the state legislatures. In 1791, these first ten amendments were added to the Constitution and became known as the <strong>Bill of Rights</strong>.</li>
  <li>The Bill of Rights was intended to quiet the fears of Anti-Federalists that the Constitution did not adequately protect individual liberties and thus encourage their support of the new national government.</li>
  <li>Other important ones include: the Thirteenth, Fourteenth, and Fifteenth Amendments, ratified at the end of the <strong>Civil War</strong>, changed the lives of African Americans who had been held in <strong>slavery</strong>.</li>
</ol>

<h3 id="political-foundation-of-the-us">Political Foundation of the US</h3>

<blockquote>
  <p><em>Some important terms</em>:</p>

  <ul>
    <li><strong>Tocqueville’s Democracy in America</strong>: America has been most shaped by the unusually free and egalitarian ideas and material conditions that prevailed at its found- ing-captures important truths</li>
    <li><strong>Feudalism</strong>: a system where the relationship in society was derived from the holding of land
      <ul>
        <li>collective ownership and control of the means of production.</li>
        <li>land is controlled by a small group of nobles, who hold power and privileges over the common people</li>
        <li>want a <em>stronger government power</em></li>
      </ul>
    </li>
    <li><strong>Socialism</strong>: means of production, distribution, and exchange should be owned or regulated by the community as a whole</li>
    <li><strong>Bourgeoisie</strong>: the middle class/capitalist class who own most of society’s wealth and means of production</li>
    <li><strong>Egalitarianism</strong>: doctrines are generally characterized by the idea that all humans are equal in <em>fundamental worth or moral status</em>.</li>
    <li><strong>Romanticism</strong>: American Romanticism is a frame of thought that places value on the individual above the group, the <em>subjective response and instinct over objective thought</em>, and emotion over logic.
      <ul>
        <li>Romanticism was a literary, artistic, and philosophical movement that first began in Europe late in the 18th century. American Romanticism developed toward the end of the Romantic movement in Europe.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<ul>
  <li><strong>Previously</strong>, analysts have described American political culture as the preeminent example of <strong>modern liberal democracy</strong>, of government by popular consent with respect for the equal rights of all. Here, the idea is that “<strong>inegalitarian ideologies and conditions</strong> that have shaped the participants and the substance of American politics just as deep”.
    <ul>
      <li>for over 80% of U.S. history, its laws declared most of the population to be ineligible for full American citizenship due to race, nationality, or gender.</li>
      <li>Tocqueville is story is centered on relationships among a minority of Americans (e.g. white), and via reference to economic statuses men have held in Europe</li>
      <li>(<strong>race</strong>) Tocqueville treated “Indians and Negros” as <em>tangents</em> to the American nation = exceptions not considered into discussion</li>
      <li>(<strong>gender inequality</strong>) Tocqueville believes that women should be treated in a distinct sphere of action = making them <em>not civic equivalent</em> of men. e.g. denied of ruling at home or taking most professional offices</li>
    </ul>
  </li>
  <li>The author (smith) then gave a few examples work/authors for excluding/<strong>marginalizing</strong> those problems (i.e. see them as minor/external exceptions to the framework) to pretend <em>egalitarian inclusiveness</em> as the norm for US politics. “None of these mainstream approaches to American politics has given prominence to the racial, ethnic, or gender makeup of the American citizenry, though neither have they wholly avoided those issues.”
    <ul>
      <li>e.g. scholars construct the identities of marginal groups as <em>irrational, passionate, dangerous</em> “others,” both to defend their exploitation and to deny the presence of such qualities in mainstream citizens</li>
      <li>but note that colonial British American pursued practices of racial/gender dominance long before any types of liberal and republican idelogies came to play in America $\implies$<mark>racial/gender inegalitarianism is technically *more rooted*</mark> than egalitarian principles.</li>
      <li>e.g. <strong>Higram’s</strong> book had many ingredients to correct the Tocqueville thesis (e.g. American nativism built on ethnocentric attitudes), but did not compel any major reinterpretation of American politics</li>
      <li>e.g. <strong>Fuchs’s</strong> book contained analysis of exclusive civic cultures of America, but does not discuss exclusion of women $\implies$ tend to omit/minimize excluded groups as every author in the Tocquevillian transition do</li>
    </ul>
  </li>
  <li>Therefore, the author offers a <mark>multiple traditions</mark> view of America: we should not presume US politics are rooted in essentially liberal or democratic values and conditions. Instead, we must analyze America as the ongoing <strong>product of often conflicting multiple traditions</strong>.
    <ul>
      <li>i.e. US culture is shaped by a complex and multiple groups interacting with each other, and is thus a result of <strong>recurring conflicts</strong> amongst the apparently inconsistent combinations of the multiple traditions</li>
      <li>additionally, those conflicts often involve each group trying to “<em>valorize their own traits</em>” while “denigrading those of others”</li>
      <li>e.g. for a long time, there is a “separate but equal” treatment to the <strong>blacks</strong> (e.g. segregation of public school) $\implies$ inequal citizens</li>
      <li>e.g. <mark>Chinese</mark> Exclusion Act in 1882 prohibiting all immigration of Chinese laborers for 10 years; in 1889 Chinese Exclusion Case still upheld requirements for Chinese-American to have certificates of citizenship not required of whites</li>
      <li>e.g. <strong>Women</strong> suffrage passed by Congress only on June 4, 1919</li>
    </ul>
  </li>
</ul>

<h2 id="problems-in-the-us-constitution">Problems in the US Constitution</h2>

<p>“How Democratic Is the American Constitution?” by Robert A. Dahl contains a more <strong>critical view</strong> of the US constitutions</p>

<blockquote>
  <ul>
    <li>
      <p>He argues that the Constitution, as originally written and interpreted, has always had <mark>significant undemocratic features</mark>, and that these have been reinforced by subsequent developments.</p>
    </li>
    <li>
      <p>He also points out that the Constitution has been amended over time to address some of its undemocratic features, but that many others remain.</p>
    </li>
  </ul>
</blockquote>

<p>A few selected quotes/key points he mentioned include:</p>

<ul>
  <li>
    <p>The US constitutions was not that “smartly framed”:</p>

    <ul>
      <li>
        <p>framers’ reliable knowledge about constitutions appropriate to a large representative republic was, at best, meager.</p>
      </li>
      <li>
        <p>The necessity for compromise and the opportunities this gave for coalitions and logrolling meant that the Constitution <em>could not possibly reflect a coherent, unified theory of government</em></p>
      </li>
      <li>
        <p>(On why we got the Senate and House of Representative system) The <strong>solution of equal representation</strong> was not a product of constitutional theory, high principle,or grand design. It was nothing more than a practical outcome of a <strong>hard bargain</strong> (e.g. by Delaware) that its opponents finally agreed to in order to achieve a constitution</p>
      </li>
    </ul>
  </li>
  <li>
    <p><mark>Undemocratic</mark> Elements in the Framers’ Constitution</p>

    <ol>
      <li><strong>Slavery</strong>. First, it neither forbade slavery nor empowered Congress to do so</li>
      <li><strong>Suffrage</strong>. the constitution failed to guarantee the right of suffrage, leaving the qualifications of suffrage to the states $\implies$ It implicitly left in placethe exclusion of half the population—women—as wellas African Americans and Native Americans</li>
      <li><strong>Election of the President</strong>. To ensure president election is insulated from both popular majorities and congressional control, the Framer’s solution is a body of presidential electors <em>composed of men of exceptional wisdom</em> and virtue who would choose the chief executive unswayed by popular opinion $\implies$ was almost immediatelycast into the dustbin of history</li>
      <li><strong>Choosing senators</strong>. senators were to bechosen not by the people but by the <em>state legislatures</em> $\implies$ senators would be <em>less responsive to popular majorities</em> and perhaps more sensitive to the needs of property holders</li>
      <li><strong>Equal representation in the Senate</strong>.  each state was, as we have seen, awarded the same number of senators $\implies$ placed and highly privileged minorities—slaveholders, for example—gained disproportionate power</li>
      <li><strong>Judicial power.</strong> failed to <em>limit</em> the powers of the judiciary to declare as unconstitutional laws (i.e. laws opposing the US constitutions) that is passed by the Congress and signed by president $\implies$judges can then affect law making (even though none would have supported this idea)</li>
      <li><strong>Congressional power</strong>. the powers of Congress were <em>limited</em> in a way that could prevent the federal government from regulating or controlling the economy. Without the power to tax incomes, for example, <em>fiscal policy</em>, not to say measures like Social Security, would be <em>impossible</em>.</li>
    </ol>
  </li>
</ul>

<h2 id="why-these-50-states-land-policy-and-state-formation">Why these 50 States? Land Policy and State Formation</h2>

<p>Some facts at current days:</p>

<ul>
  <li>
    <p>Tension in US constitutional design on representation based on population. What percent of US residents (people physically in the US) do no have voting representations in the House or Senates?</p>

    <ul>
      <li>note that you should count in Commonwealth of Puerto Rico, Guam, US Virgin Islands, American Samoa, etc.</li>
      <li>as a result, $1.6$% of US residents have no voting representation (if they physically move to US mainland, then they can vote)</li>
    </ul>
  </li>
  <li>
    <p>When did the US states became “states”? Red is the earliest, and blue the latest</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">When became States</th>
          <th style="text-align: center">Land Acquisition</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="https://www.ereferencedesk.com/resources/statehood/images/us-states-by-date-of-statehood.png" alt="Statehood Order by Dates: Statehood by Dates" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230201164014190.png" alt="image-20230201164014190" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>How to become an independent state? Being a state gives 2 US senators, and representation in US house proportional to share of total population. Therefore, this could impose <em>high influence on future policy making</em>:</p>

    <ul>
      <li>
        <p>new states <em>cannot</em> be granted different privileges than others</p>
      </li>
      <li>requires passage of bill through House and Senate</li>
      <li>it is formed out of a state, then the state legislature has to agree as well</li>
      <li>as statehood changes the balcen of votes in national legislature, hence can be <em>vetoed</em> by national legislative majority</li>
    </ul>
  </li>
  <li>
    <p>Any current movements to change number of states?</p>

    <ul>
      <li>statehood for DC, PR</li>
      <li>cession from California (e.g. state of Jefferson), OR, also WA, UT, etc, as people feels like large cities dominate politics</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>In this section, we consider the question: <strong>does the selection of these statehood have anything to do with policy making in the federal government?</strong> As you shall see soon, why “states” get to be “states” depends a lot on the motivation of reducing conformity cost when implementing government policies.</p>
</blockquote>

<p>Certain backgrounds on relevant US history:</p>

<ul>
  <li>
    <p>scholarly consensus: US government had little ruling capacity <em>before Civil War</em>. As a result, many federal land policy was used to gain control over territory through <strong>private effort</strong> i.e. cannot govern the large territories they owned, hence encourage private individuals to do it</p>

    <ul>
      <li>controlling territory is expensive and difficult, gov has no resource</li>
      <li>can offer reward (e.g. land) to populations with military expertise</li>
      <li>compact settlement enabled effective territorial control</li>
    </ul>
  </li>
  <li>
    <p>however, while all framers agreed to expand territory, they had different <em>vision</em> what their expansion will look like (starting from 13 states). These ideals can be separated into two schools:</p>

    <ul>
      <li>opposition to presense of non-white</li>
      <li>opposition to non-white people with rights</li>
    </ul>

    <p>as a result, this gives</p>

    <ul>
      <li>justification of ethnic cleansing against indigenous population</li>
      <li>
        <p>justified ensalving subset of population</p>
      </li>
      <li>generated opposition to giving political rights to new territory with non-white majorities</li>
    </ul>
  </li>
  <li>
    <p>obviously for federal government to pass a policy, you need <strong>approvals by many states</strong></p>
  </li>
</ul>

<blockquote>
  <p>As a result, lots of strategies arise to <mark>limit conformity cost</mark> in federal gov policy making:</p>

  <ol>
    <li><strong>limit who has right to participate in decision-making</strong> to ensure preferred outcome: e.g. given statehood until there is a sufficiently large white population
      <ul>
        <li>e.g. gerrymandering, franchise restrictions</li>
        <li>e.g. Treaty of Guadalupe Hidalgo, ended war between US and Mexico. then both northerners and southerners are thinking: “How can we get the most territories, with the <em>fewest</em> addition of people”, i.e. “do <em>not</em> want people of Mexico, either as citizens or subjects,” but only land</li>
      </ul>
    </li>
    <li><strong>prevent policy-making/implementation</strong>: limit (e.g. the number of) alternative options to be in favor of your preferred outcome
      <ul>
        <li>e.g. make a policy impossible to happen: e.g. a government weak enough to prevent a future president to pass a law to end slavery</li>
        <li>e.g. limit a government’s action by limiting its fiscal capacity</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>And as a result these factors <strong>shaped</strong> which states are recognized, and which ones are not.</p>

<p><em>For Example</em>, then under this theory, ceteris paribus (i.e. “all other things being equal”), how could Washington DC or Puerto Rico become a state?</p>

<ol>
  <li>white majority</li>
  <li>their policy do not fly in the face of the federal government’s vision</li>
</ol>

<h3 id="frymers-notes-on-land-policy">Frymer’s Notes on Land Policy</h3>

<p>From the paper <em>A Rush and a Push and the Land Is Ours”: Territorial Expansion, Land Policy, and U.S. State Formation</em></p>

<blockquote>
  <p>“The importance of federal land policies in securing and incorporating territorial borders illuminates an under-examined mechanism by which developing nation states, even those with limited bureaucratic and military capacity, can successfully <strong>assert power over a vast and difficult geographic terrain</strong>.”</p>
</blockquote>

<p>Some major points are:</p>

<ul>
  <li><strong>land policy</strong> such as granting veterans land in conflicted areas $\implies$ increase settlement there which could not only <strong><em>expand its territory</em></strong> but also better defend (e.g. serve as buffer zone). Most often this includes “shrinking the Indian territory”. Examples include
    <ul>
      <li>The <strong>Indian Removal Act</strong> was signed into law by President Andrew Jackson on May 28, 1830, authorizing the president to grant unsettled lands west of the Mississippi in exchange for Indian lands within existing state borders (actions of the United States during this period would constitute genocide under current-day international law)</li>
      <li><strong>“Advancing compactly”</strong> = settle the land “progressively” with “compact” settlements and a “formidable” barrier before advancing on the frontier</li>
      <li>In 1842, Congress passed the <strong>Armed Occupation Act</strong> providing 160 acres of land to those settlers who were armed and willing to occupy land south of Gainesville, Florida as a way of ending the Second Seminole War.</li>
      <li>The success of the Armed Occupation Act prompted calls to extend the policy to western territories. Legislation was quickly proposed to induce a volunteer force of mounted men to settle the Oregon Territory that at the time was contested between the US, British, and Indian nations</li>
      <li><strong>Preemption Act and Homestead Act</strong> in 1841 and 1862 fostered an “unparalleled rush for land in Illonois”</li>
    </ul>
  </li>
  <li>the <mark>implication</mark> of those land policies is that those “federal land policies enabled an otherwise constrained American government to <strong>assert authority over the direction and pace of expansion and settlement</strong> and to maintain an official fidelity to constitutional principles while <strong>conquering territory</strong>, removing indigenous populations, and <strong>engineering a dominant racial vision</strong> by manufacturing white majorities in lands populated by diverse peoples.”</li>
  <li>However, as the aim is population movement, you shall also compare against the method of <em>forming sizeable government bureaucracies to mobilize population movements</em>
    <ul>
      <li>The results of American conquest are certainly significant, but the boundaries could well have been <strong>far more expansive</strong> had nation builders not been constrained by a small military and a need for manufacturing white majorities.</li>
      <li>the reliance on land policies—as opposed to more conventional coercive forms—enabled a certain amount of <strong>racial diversity to thrive</strong> on the frontier despite a white hegemonic society. This diversity is not just a result of ideological conflicts and multiple orders. but is also institutionally constructed from weaknesses in the capacity of the American state that enabled these pockets of diversity to withstand eradication, particularly on frontier borderlands.</li>
    </ul>
  </li>
</ul>

<hr />

<p>Note that, as mentioned near the end:</p>

<blockquote>
  <p>“Expansions have been fundamentally opposed/restricted by people motivated to <mark>preserve policy control or other privileges</mark>” hints at certain <em>undemocratic</em>/<em>inegalitarian</em> values in US policy-making in the past</p>

  <ul>
    <li>
      <p>Egalitarian ideas are important for self-identification, but not to determine their collective actions</p>
    </li>
    <li>
      <p>longevity of US government is not explained by abstract commitment to democratic principles. E.g. otherwise to be allowed for statehood you wouldn’t need stuff such as white majority, etc, if you are truly egalitarian</p>
    </li>
  </ul>
</blockquote>

<p>This brings back to America’s political Traditions discussed in Section <a href="#Political Foundation of the US">Political Foundation of the US</a>, so that policy makers are not really “rooted with egalitarianism” but rather:</p>

<ul>
  <li>desire of constrain who gets rights in order to <strong><em>achieve their own policy goals</em></strong></li>
  <li>hence, when looking at a policy, you should think about really: where does this come from? Who made it? What do they want to achieve?</li>
</ul>

<h1 id="american-federalism">American Federalism</h1>

<p>The federal design spelled out in the Constitution divides powers between two levels of government—the <strong>states and the federal government</strong>—and creates a mechanism for them to check and balance one another. As an institutional design, federalism both safeguards state interests and creates a strong union led by a capable central government.</p>

<blockquote>
  <p>This section traces the origins, evolution, and functioning of the <mark>American system of federalism</mark>, as well as its advantages and disadvantages for citizens.</p>
</blockquote>

<p>Recall that other “non-federalist” approach include</p>

<p><img src="https://openstax.org/apps/archive/20221219.191545/resources/df7dffd16667f5c2ff8a13c934b26b0dc1afa256" alt="”." style="zoom:50%;" /></p>

<p>for example</p>

<ul>
  <li>countries such as France, Japan, and Sweden are democratic with unitary systems.</li>
  <li>as with confederation $\implies$ weak national government $\implies$ American system in the past $\implies$ it maximizes regional self-rule at the expense of effective national governance.</li>
</ul>

<h2 id="division-of-powers">Division of Powers</h2>

<p>Modern democracies divide governmental power in two general ways:</p>

<ol>
  <li>the first and more common mechanism shares power among three branches of government—the legislature, the executive, and the judiciary.</li>
  <li>the second, federalism, apportions power between two levels of government: national and sub-national.</li>
</ol>

<p>Some, like the <mark>United States</mark>, use a <strong>combination</strong> of both structures.</p>

<ul>
  <li>US has a sub-national gov = state gov, and national gov = federal gov</li>
  <li>In both level of gov, there are separation of power
    <ul>
      <li>the <em>President</em> assumes executive power, <em>Congress</em> exercises legislative powers, and the <em>federal courts</em> (e.g., U.S. district courts, appellate courts, and the Supreme Court) assume judicial powers.</li>
      <li>In each of the fifty states, a <em>governor</em> assumes executive authority, a <em>state legislature</em> makes laws, and <em>state-level courts</em> assume judicial powers</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Federalism</strong> is an institutional arrangement that creates two relatively autonomous levels of government, each possessing the capacity to act directly on behalf of the people. Although many federalism vary by design, they share a lot of key similarities.</p>

  <ul>
    <li>each unit of government has its own set of officials and independent authority</li>
    <li>existence and authority of each level is protected by a constitution</li>
    <li>each unit of government can <mark>pressure others</mark></li>
  </ul>
</blockquote>

<ol>
  <li>First, all federal systems establish two levels of government, with both levels being elected by the people and <strong>each level assigned different functions</strong>. (e.g. federal gov cares more about national affairs, whereas state gov cares more about matters lie within their region)</li>
  <li>National <em>constitution</em> that cannot be changed without the <strong>substantial consent of <em>sub-national</em> governments</strong>. In the US, approval needs two-thirds of both houses of Congress and three-fourths of the states (supermajority)</li>
  <li>allocate legislative, judicial, and executive authority to the two levels of government in such a way as to ensure each level <strong>some degree of autonomy from the other</strong>. (e.g. state gov has its own governor, legislature, and court)</li>
  <li><strong>national courts</strong> commonly resolve disputes between levels and departments of government.</li>
  <li>Finally, <strong><em>sub-national</em> governments are always represented in the upper house</strong> of the national legislature, enabling regional interests to influence national lawmaking. In the US, the Senate functions as a territorial body by representing the fifty states (those are not the same as state governors or state senates)</li>
</ol>

<blockquote>
  <p>Why these different levels in this Federal Design? Mechanism to <strong>minimize conformity costs</strong></p>

  <ul>
    <li>people being represented under the same level of government (e.g. states) <em>wanted the same thing</em> $\implies$ lower interstate conformity cost
      <ul>
        <li>more homogeneous among people being represented</li>
      </ul>
    </li>
    <li>constitution excluding things that might prevent unification/uniform decision</li>
  </ul>
</blockquote>

<p>And again</p>

<blockquote>
  <p><strong>Collective action framework</strong> helps us to see how political institution make certain things more (or less) possible</p>

  <ul>
    <li>institutions can solve coordination or free rider problems</li>
    <li>but they could also <em>not solve</em> certain problems, i.e. so that having an institution could make that harder to happen
      <ul>
        <li>for example, filibuster = use prolonged speechmaking to delay or prevent a vote on a bill.</li>
        <li>The purpose of the filibuster is to prevent the majority from passing a bill that the minority opposes $\implies$ the <em>minority</em> party can use this tactic to try to <em>block the passage of a bill that they oppose</em> $\implies$ a single senator could veto $\implies$ <strong>large conformity costs</strong> as majority preference is vetoed</li>
        <li>poorly designed to solve problems that aren’t represented by eligible voters</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="federalism-and-constitutions">Federalism and Constitutions</h3>

<p>We have briefly discussed the basics of US constitutions in section <a href="#The US Constitution Basics">The US Constitution Basics</a>. Here we take a more detailed look to see how those articles are related to the US federalist system.</p>

<blockquote>
  <p>In general, some <strong>delineate</strong> the scope of national and state power, while others <strong>restrict</strong> it (e.g. Bill of Rights). The remaining provisions shape relationships among the states and <strong>between</strong> the states and the federal government.</p>

  <ul>
    <li>allow for certain collective actions to be more easily carried out</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p><strong>enumerated</strong> powers of the <strong>national legislature</strong> are found in Article I, Section 8.</p>

    <ul>
      <li>e.g. <strong>elastic clause</strong> or the <em>necessary and proper clause</em>, enables Congress “to make <em>all Laws which shall be necessary and proper</em> for carrying” out its constitutional responsibilities. (this is a rather open construction, hence it did enable national gov to expand authority beyond)</li>
      <li>e.g. <strong>commerce clause</strong> is a provision in the U.S. Constitution (Article I, Section 8, Clause 3) that gives Congress the power “to regulate Commerce with foreign Nations, and among the several States, and with the Indian Tribes.” While this can be interpreted broadly as Congress have a broad power to regulate commerce, the Court has also imposed some limitations on Congress’s power, such as limiting to interstate activity</li>
      <li>establishment of new states</li>
      <li>e.g. <strong>supremacy clause</strong> (see below as well), basically federal laws supersedes all state laws</li>
    </ul>
  </li>
  <li>
    <p>Article I, Sections 9 and 10, along with several constitutional amendments, lay out the <strong>restrictions on federal and state authority</strong>.</p>

    <ul>
      <li>e.g. prevents measures that cause the deprivation of personal liberty (such as slavery, imprisonment without justification)</li>
      <li>e.g. Bill of Rights, including freedom to speech, local police searching you without a warrant</li>
    </ul>
  </li>
  <li>
    <p><strong>supremacy clause</strong> in Article VI of the Constitution regulates <strong>relationships between the federal and state governments</strong> by declaring that the Constitution and federal law are the supreme law of the land.</p>

    <ul>
      <li>however, enforcement is not always that simple.</li>
      <li>In the case of marijuana use, which the federal government defines to be <em>illegal</em>, thirty-six states and the District of Columbia have nevertheless established medical marijuana laws, others have <em>decriminalized its recreational use</em>, and fifteen states have completely <em>legalized</em> it. The federal government could act in this area if it wanted to.</li>
    </ul>
  </li>
  <li>
    <p>powers of the <strong>state governments</strong> were never listed in the original Constitution, but the Tenth Amendment <strong>affirms the states’ reserved powers</strong></p>

    <ul>
      <li>“The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.”</li>
      <li>most important include police powers = establish mechanism to <em>police</em>, such as rules; and provision of public services; system of local government = <em>how local gov should work</em>; and regulation of intra-state commerce</li>
      <li>however, some reserved powers are no longer exclusively within state domain, e.g. boundary between intrastate and interstate commerce has become indefinable as a result of broad interpretation of the commerce clause.</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230202164724977.png" alt="image-20230202164724977" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>Various constitutional provisions <strong>govern state-to-state relations</strong>.</p>

    <ul>
      <li>Article IV, Section 1, referred to as the <strong>full faith and credit clause</strong> or the <em>comity clause</em>, requires the states to accept court decisions, public acts, and contracts of other states. Thus, an adoption certificate or driver’s license issued in one state is valid in any other state.</li>
      <li><strong>privileges and immunities clause</strong> of Article IV asserts that states are prohibited from discriminating against out-of-staters by denying them such guarantees as access to courts, legal protection, property rights, and travel rights. The clause has not been interpreted to mean there cannot be <em>any</em> difference in the way a state treats residents and non-residents. For example, individuals cannot vote in a state in which they do not reside.</li>
    </ul>
  </li>
  <li>
    <p>Some areas <strong>both state and federal governments</strong> can regulate</p>

    <ul>
      <li>militia, both state and federal</li>
      <li>states may act in a federal area, so long as it does not contradict federal law</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>In a sense, Constitution basically provides a framework + freedom for the institutions to work out. Then, people can use whatever levers at their disposal to change a policy in a state/entire United States.</p>
</blockquote>

<p>However, there are “governments” that are missing from this:</p>

<ul>
  <li>local governments has no status in federal constitution
    <ul>
      <li>are granted power by <strong>states</strong> (e.g. local establish police department, but state could take over if it wants, e.g. due to corruption)</li>
      <li>so people say local gov are “creature of the states”</li>
    </ul>
  </li>
  <li>in many states, there is a the concept of “home rule” for local governments
    <ul>
      <li>provisions in many <strong>state constitution</strong> <em>limit action in certain areas</em> of local government</li>
      <li>this must be specifically conferred, in practice given to most larger cities
        <ul>
          <li>e.g. city of Baffalo is allowed to establish police academy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="distribution-of-finances">Distribution of Finances</h3>

<p>Where do national, state, and local government get their money from? A graph below in 2018/2020 illustrates some high level info:</p>

<p><img src="https://openstax.org/apps/archive/20221219.191545/resources/054b305607c99d0a3beb557239a4b7a7d57892bc" style="zoom:50%;" /></p>

<ul>
  <li>for federal government, most of the revenue comes from <strong>income taxes</strong> (includes all kinds of income, paid by you) and <strong>payroll taxes</strong> (includes your payroll, paid by both employer and employee)</li>
  <li>For local governments the <strong>property tax</strong>, a levy on residential and commercial real estate, was the most important source of tax revenue</li>
</ul>

<p>How are those money spent?</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Federal Spending</th>
      <th style="text-align: center">State and Local</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://openstax.org/apps/archive/20221219.191545/resources/e6c3bfa70d0376658434af0fc26ea87e8de7f31d" alt="." style="zoom:40%;" /></td>
      <td style="text-align: center"><img src="https://openstax.org/apps/archive/20221219.191545/resources/bc79aaf6905e8737edba9d131a249cba4b39654e" alt="" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>A look at the federal budget in 2019 shows that the three largest spending categories were <strong>Social Security</strong>; <strong>Medicare</strong>, Medicaid, the Children’s Health Insurance Program; and <strong>defense and international security assistance</strong> (18 percent) $\iff$ Constitution assigns the federal government various powers that allow it to <em>affect the nation as a whole</em>.</li>
  <li>for local and state, we see that <strong>educational</strong> expenditures constitute a major category for both; state also allocate comparatively more funds to public welfare programs, such as <strong>health care, income support, and highways</strong>.</li>
</ul>

<blockquote>
  <p>Note that one interesting point here is about “free-rider problem”, since grants are given to states but <em>come from taxes states collect</em>, then a state can impose the lowest amount of tax and still get grants/findings from government.</p>
</blockquote>

<h2 id="the-evolution-of-american-federalism">The Evolution of American Federalism</h2>

<p>How did US Federalism work in real life?</p>

<blockquote>
  <p>Since the constitution does <mark>not</mark> specify the precise ruling of states and federal gov, this has led to</p>

  <ul>
    <li>
      <p>several clashes between national gov and state gov</p>
    </li>
    <li>
      <p>changes in the configuration of federalism over time that capture distinct balances between state and federal authority.</p>
    </li>
  </ul>
</blockquote>

<p>Examples of national gov v.s. state gov include:</p>

<ul>
  <li>establishment of the Bank of the United States, and are <strong>states</strong> allowed to <strong>tax federal property (e.g. that bank)</strong>?
    <ul>
      <li>In <em>McCulloch v. Maryland</em>, Chief Justice John Marshall argued that Congress could establish “all means which are appropriate” to fulfill “the legitimate ends” of the Constitution.</li>
      <li>therefore, state (in many cases) cannot tax national institutions as “the power to tax is the power to destroy.”</li>
    </ul>
  </li>
  <li>commerce clause of Article I, Section 8; specifically, it had to determine whether the <strong>federal government had the sole authority to regulate the licensing of steamboats</strong> operating between New York and New Jersey.
    <ul>
      <li>in the end, federal law trumped the New York State license-monopoly law</li>
      <li>As Marshall pointed out, “the acts of New York must yield to the law of Congress.”</li>
    </ul>
  </li>
  <li>what about doctrine of <strong>nullification</strong>—that states had the right to reject national laws they deemed unconstitutional.
    <ul>
      <li>ultimate showdown between national and state authority came during the Civil War</li>
      <li>pro-state: Supreme Court ruled that the national government <em>lacked</em> the authority to ban slavery in the territories $\implies$ Civil War</li>
      <li>The defeat of the South had a huge impact on the balance of power between the states and the national government.
        <ol>
          <li>First, the Union victory put an <strong>end to the right of states to secede and to challenge</strong> legitimate national laws.</li>
          <li>Second, Congress imposed several conditions for readmitting former Confederate states into the Union; among them was ratification of the Fourteenth and Fifteenth Amendments.</li>
        </ol>
      </li>
      <li>In sum, after the Civil War the power balance <strong>shifted toward the national government</strong>.</li>
    </ul>
  </li>
</ul>

<p>In sum, With the exception of the Civil War, the Supreme Court settled the power struggles between the states and national government. From a historical perspective, the national supremacy principle introduced during this period did not so much narrow the states’ scope of constitutional authority as restrict their encroachment on national powers.</p>

<h3 id="dual-cooperative-and-new-federalism">Dual, Cooperative, and New Federalism</h3>

<blockquote>
  <p><strong>Dual federalism</strong>, the states and national government exercise exclusive authority in <em>distinctly delineated spheres</em> of jurisdiction, i.e. limited only to perform within the enumerated powers. As a result, state and gov are like “equal” players in politics, hence dual.</p>

  <p>This happened in the late <strong>1870s</strong>, motivated mainly by:</p>

  <ul>
    <li>
      <p>several Supreme Court rulings blocked attempts by both state and federal governments to step outside their jurisdictional boundaries.</p>
    </li>
    <li>
      <p>prevailing economic philosophy at the time <em>loathed government interference</em> in the process of industrial development.</p>
    </li>
  </ul>

  <p>but was dealt a legal blow in 1895 and is no longer used.</p>
</blockquote>

<p>In the late 1870s, industrialization changed the socioeconomic landscape of the United States. One of its adverse effects was the <strong>concentration of market power</strong> $\implies$ lead to several new issues to federal ruling</p>

<ul>
  <li>there was no national regulatory supervision to ensure fairness in market practices, <mark>collusive behavior among powerful firms emerged</mark> in several industries. e.g. anti-competitive practices in the railroad industry.</li>
  <li>Congress passed the Interstate Commerce Act in 1887, which created the Interstate Commerce Commission; then three years later, this is broarden by <strong>Sherman Antitrust Act of 1890</strong>, which made it <strong>illegal to monopolize or attempt to monopolize</strong> and conspire in restraining commerce</li>
  <li>In 1895, in <em>United States v. E. C. Knight</em>, the Supreme Court ruled that the national government <strong>lacked the authority to regulate manufacturing</strong>, arguing that the national government’s regulatory authority applied only to commercial activities. (If manufacturing activities fell within the purview of the commerce clause of the Constitution, then “comparatively little of business operations would be left for state control,” the court argued.)</li>
</ul>

<p>However, things start to change in <mark>Great Depression</mark> of the 1930s, which brought economic hardships the nation had never witnessed before</p>

<blockquote>
  <p><strong>Cooperative federalism</strong> was born of necessity and lasted well into the twentieth century as the national and state governments each found it beneficial. Under this model, both levels of government <strong>coordinated their actions to solve national problems</strong>, such as the Great Depression and the civil rights struggle of the following decades.</p>

  <ul>
    <li>federal gov can attempt to intervene in all areas of local/public policy, not limited to their enumerated powers</li>
    <li>layers of government do not coerce each other, but <em>national can take leadership role</em> (i.e. coerce)</li>
    <li>federal resources give it an upper hand, in practice</li>
    <li>federal government can influence through <mark>carrots and sticks</mark> (see <a href="#Grant and Mandates">Grant and Mandates</a>)</li>
  </ul>

  <p>the more dominant in 1932/37 - present.</p>
</blockquote>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230208162212758.png" alt="image-20230208162212758" style="zoom: 25%;" /></p>

<p>Essentially there is now a <strong>broadening of federal powers</strong> in concurrent and state policy domains, it is also the era of a <strong>deepening coordination</strong> between the states and the federal government in Washington.</p>

<ul>
  <li>Before the Great Depression, the government offered little in terms of financial aid, social benefits, and economic rights. After the New Deal, it provided old-age pensions (Social Security), unemployment insurance, agricultural subsidies, protections for organizing in the workplace</li>
  <li>The unemployment insurance program, also created by the <strong>Social Security Act</strong>, requires states to provide jobless benefits, but it allows them significant latitude to decide the level of tax to impose on businesses</li>
  <li>coordination between state and national gov is clearest in the social welfare and social insurance programs created during the New Deal and Great Society eras, most of which are <strong>administered by both state and federal authorities and are jointly funded</strong>.</li>
</ul>

<p>Thus, the era of cooperative federalism left two lasting attributes on federalism in the United States:</p>

<ol>
  <li><strong>nationalization of politics</strong> emerged as federal legislative activities were aimed at addressing problem such as <em>marketplace inefficiencies</em>, etc $\implies$ again federal governments gain more policy making power</li>
  <li><strong>flexibility</strong> that states and local authorities were given in the implementation of federal social welfare programs $\implies$ cross-state differences in the levels of benefits and coverage (not to confuse with “full faith and credit clause”)</li>
</ol>

<blockquote>
  <p><strong>New federalism</strong> is based on the idea to <em>restore states’ prominence in policy areas</em> into which the federal government had moved in the past. It is also based on the belief that the decentralization of policies enhances administrative efficiency, reduces overall public spending, and improves policy outcomes.</p>
</blockquote>

<p>The most prominent figure championing this was perhaps Ronald Reagan, along with several efforts by the Supreme Court</p>

<ul>
  <li>
    <p>The election of Ronald Reagan heralded the advent of a “<mark>devolution</mark> revolution” in U.S. federalism, in which the president pledged to return authority to the states according to the Constitution.</p>
  </li>
  <li>in United States v. Lopez, the court struck down the Gun-Free School Zones Act of 1990, which banned gun possession in school zones. It argued that the regulation in question did not “substantively affect interstate commerce” $\implies$ gives this ruling back to state</li>
  <li>However, many would say that the years since the 9/11 attacks have swung the pendulum back in the direction of central federal power.</li>
</ul>

<blockquote>
  <p>Note that it is important <mark>not</mark> to see these systems as “permanently replacing” each other. Each tend to leave some footprints in their succeeding system, and hence it is best to see this as an “evolution process” $\implies$ the combined effect of all of them is more important.</p>
</blockquote>

<h2 id="grant-and-mandates">Grant and Mandates</h2>

<p>The national government’s ability to achieve its objectives often requires the <strong>participation of state and local governments.</strong></p>

<blockquote>
  <p>Intergovernmental grants offer positive financial inducements to get states to work toward selected national goals.</p>

  <ul>
    <li>A <strong>grant</strong> is commonly likened to a “carrot” to the extent that it is designed to entice the recipient to do something.</li>
    <li><strong>Mandates</strong> are typically backed by the threat of penalties for non-compliance and provide little to no compensation for the costs of implementation. Thus, a mandate is commonly likened to a “stick.”</li>
  </ul>

  <p>the revenue used for grants are offered through taxing and spending powers</p>
</blockquote>

<p>In the past, examples of grants include “land grants” e.g. University of Delaware, are land-grant institutions because their campuses were built on land donated by the federal government. But today, grants have become a bit more complicated system</p>

<ul>
  <li><strong>Categorical grants</strong> are federal transfers formulated to limit recipients’ discretion in the use of funds and subject them to strict administrative criteria that guide project selection, performance, and financial oversight, among other things. Medicaid and the food stamp program are examples of categorical grants.
    <ul>
      <li>e.g. establishment of criminal justice at local level was funded by federal gov in the past.</li>
    </ul>
  </li>
  <li><strong>Block grants</strong> come with less stringent federal administrative conditions and provide recipients more flexibility over how to spend grant funds. Example include Workforce Investment Act program, which help youths and adults obtain skill sets that will lead to better-paying jobs</li>
</ul>

<blockquote>
  <p>Grant money can change political calculus (e.g. Medicaid: give extra money to extend health insurance to people otherwise inapplicable. Some states took it and hence changed their policy, some did not.)</p>
</blockquote>

<p>However, the national government has <mark>greatly preferred using categorical grants</mark> to transfer funds to state and local authorities because</p>

<ol>
  <li>this type of grant gives them <strong>more control and discretion</strong> in how the money is spent.</li>
  <li>elected officials who sponsor these grants can <strong>take credit</strong> for their positive outcomes</li>
  <li>block grants lack mechanisms to hold state and local administrators <strong>accountable</strong> for outcomes</li>
  <li>once categorical grants have been established, vested interests in Congress and the federal bureaucracy seek to <strong>preserve</strong> them</li>
</ol>

<p>Block grants have been championed for their <mark>cost-cutting effects</mark> $\implies$ placing a ceiling on funding</p>

<ul>
  <li>By eliminating uncapped federal funding, the national government can reverse the escalating costs of federal grant programs.</li>
  <li>Paul Ryan (R-WI), former chair of the House Budget Committee estimated one could save the federal government upwards of $732 billion over ten years if Medicaid is converted to block grant</li>
</ul>

<hr />

<blockquote>
  <p><strong>Unfunded mandates</strong> are federal laws and regulations that impose obligations on state and local governments <em>without fully compensating them for the administrative costs they incur</em>.</p>

  <p>Penalty of non-compliant often includes either a) threatens civil and criminal penalties for state and local authorities b) suspension of federal grant, or c) a combination of both.</p>
</blockquote>

<p>This is as bad as it sounds, but it is difficult to restrict the temptation of not using it:</p>

<ul>
  <li>The widespread use of federal mandates in the 1970s and 1980s provoked a backlash among state and local authorities, which culminated in the <strong>Unfunded Mandates Reform Act (UMRA)</strong> in 1995. However, since the act’s implementation, states and local authorities have obtained limited relief.</li>
  <li><strong>Real ID Act</strong> of 2005, a federal law designed to beef up homeland security by replacing driver’s license and identification cards (DI/ID) with standardized machine-readable cards. However, while the cost to states of re-issuing DL/IDs is estimated to be $11 billion, the government only reimburse a small portion of the cost. As a result, since 2016 and only thirty-eight were in full compliance with Real ID as of December 2018.</li>
  <li>other examples: National Voter Registration Act.</li>
</ul>

<p>The continued use of unfunded mandates clearly <mark>contradicts new federalism’s call</mark> for giving states and local governments more flexibility in carrying out national goals. As a result, there have been more instances of confrontational interactions between the states and the federal government.</p>

<hr />

<blockquote>
  <p><strong>Preemption</strong>: federal law that assets national government control over an area. Intent is to limit state government authority, hence many of them are also litigated in courts.</p>
</blockquote>

<p>Examples include</p>

<ul>
  <li>recently, over the issue of abortion</li>
  <li>Defensive of Marriage Act (1996) where federal govt took over</li>
</ul>

<h3 id="competitive-federalism-today">Competitive Federalism Today</h3>

<p>One aspect of competitive federalism today is that some policy issues, such as immigration and the marital rights of LGBTQ people, have been redefined as the <strong>roles that states and the federal government play in them have changed</strong> $\iff$ both state and federal government can influence each other.</p>

<p><strong>Contending Issues</strong>: In sum, as the immigration and marriage equality examples illustrate, constitutional disputes have arisen as states and the federal government have sought to <mark>reposition themselves on certain policy issues</mark>, disputes that the federal courts have had to sort out.</p>

<ul>
  <li>before, it was understood that the federal government handled immigration and states determined the legality of marriage.</li>
  <li>Since the late 1990s, <strong>states</strong> have asserted a right to make <strong>immigration policy</strong> on the grounds that they are enforcing, not supplanting, the nation’s immigration laws.
    <ul>
      <li>In 2005, twenty-five states had enacted a total of thirty-nine laws related to immigration;</li>
      <li>In 2012, in <em>Arizona v. United States</em>, the Supreme Court affirmed <strong>federal supremacy on immigration</strong>, as Arizona passed Senate Bill 1070, which sought to make it so difficult for undocumented immigrants to live in the state that they would return to their native country</li>
    </ul>
  </li>
  <li>By passing the Defense of Marriage Act (DOMA) in 1996, the <strong>federal government</strong> stepped into policy making on <strong>marriage</strong>.
    <ul>
      <li>DOMA considered denying same-sex couples from various federal provisions and benefits—such as the right to file joint tax returns and receive Social Security survivor benefits.</li>
      <li>In <em>United States v. Windsor</em>, the Supreme Court changed the dynamic established by DOMA by ruling that the federal government had no authority to define marriage (i.e. laws cannot discriminate between same-sex and different-sex couples based on the equal protection clause of the Fourteenth Amendment)</li>
    </ul>
  </li>
</ul>

<p><strong>Strategizing about new issues</strong>: By creating <mark>two institutional access points</mark>—the federal and state governments—the U.S. federal system enables interest groups such as MADD to strategize about how best to achieve their policy objectives.</p>

<ul>
  <li>Mothers Against Drunk Driving (MADD) was established in 1980 by a woman whose thirteen-year-old daughter had been killed by a drunk driver, and they aim to raise the drinking age and impose tougher penalties
    <ul>
      <li>tried lobbying <strong>state</strong> legislators, but did not succeed</li>
      <li>redirect its lobbying efforts at <strong>Congress</strong>, and <strong>succeeded</strong>. In 1984, the federal government passed the National Minimum Drinking Age Act (NMDAA), raising minimum purchase age to 21.</li>
    </ul>
  </li>
  <li>anti-abortion advocates (in 1973 <em>Roe v. Wade</em> Supreme Court decision making abortion legal nationwide) used the same strategy of venue shopping
    <ul>
      <li>initially targeted at the Congress, but failed</li>
      <li>shift their focus to <strong>state</strong> legislators, where their advocacy efforts have been more successful. By 2015, for example, thirty-eight states required some form of parental involvement in a minor’s decision to have an abortion, etc.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Surely as a result of the borderline between state v.s. federal policy become intermingled.</p>
</blockquote>

<h3 id="advantages-and-disadvantages-of-federalism">Advantages and Disadvantages of Federalism</h3>

<blockquote>
  <p>[discussion] means this point was made during the discussion section</p>
</blockquote>

<p>Among the <strong>merits</strong> of federalism (i.e. separated power) are that it</p>

<ol>
  <li><strong>limits</strong> ability of federal government to oppress (e.g. people)</li>
  <li>promotes policy <strong>innovation</strong>, “a single courageous state may, if its citizens choose, serve as a <em>laboratory</em>;” = <mark>laboratory of democracy</mark>
    <ul>
      <li>For example, a number of New Deal breakthroughs, such as child labor laws, were <strong>inspired by state policies</strong>.</li>
      <li>[discussion] e.g. police training before enforcement (before, all you need is a badge and a gun. In 1959, California and NY required police training/provided financial incentive)</li>
      <li>[discussion] in general might find a better policy</li>
    </ul>
  </li>
  <li>promotes political <strong>participation</strong>, as it creates a government closer to the people (e.g. only for people in this region)</li>
  <li>accommodates <strong>diversity</strong> of opinion.
    <ul>
      <li>the system of checks and balances in our political system often prevents the federal government from imposing uniform policies across the country.</li>
      <li>As a result, states and local communities have the latitude to address policy issues based on the specific needs and interests of their citizens.</li>
    </ul>
  </li>
  <li>states can act when federal government <strong>can’t agree</strong> (if urgent) = less <strong>coordination and transactional cost</strong> when making <strong>intrastate</strong> policies
    <ul>
      <li>[discussion] but of course, interstate coordination becomes more difficult (see #5 below)</li>
    </ul>
  </li>
</ol>

<p><strong>Drawbacks</strong> include (or adv of centralized power):</p>

<ol>
  <li>economic disparities across states, <strong>race-to-the-bottom</strong> dynamics (i.e., states compete to attract business by lowering taxes and regulations)
    <ul>
      <li>economic disparities include
        <ul>
          <li>e.g. in 2017, Maryland had the highest median household income (80,776), while West Virginia had the lowest (43,469).</li>
          <li>In 2016, New York spent 22,366 per student for elementary and secondary education, while Utah spent 6,953.</li>
        </ul>
      </li>
      <li>The economic strategy of using race-to-the-bottom tactics in order to compete with other states in attracting new business growth also carries a <strong>social cost</strong>.
        <ul>
          <li>For example, workers’ safety and pay can suffer as workplace regulations are lifted, and the reduction in payroll taxes for employers has led a number of states to end up with underfunded unemployment insurance programs.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>the difficulty of taking action on issues of <strong>national importance</strong>.
    <ul>
      <li>federal design of our Constitution and the system of checks and balances has jeopardized or outright blocked federal responses to important national issues.</li>
      <li>but with a national government, this becomes much easier</li>
    </ul>
  </li>
  <li><strong>free-riding</strong>: some states rely on contribution of others, e.g. on the grants issue, or raising military for national defense
    <ul>
      <li>can be decreased if having a national government = third party supervising the prisoner’s dilemma</li>
    </ul>
  </li>
  <li>duplication of efforts $\implies$ inefficiency, as you have so many layers of govt and each has its own authority, <strong>making decisions can be much slower</strong>
    <ul>
      <li>lower <strong>transactional cost</strong> to enforce policies/make decisions</li>
    </ul>
  </li>
  <li>interstate <strong>coordination problem</strong> exists = to coordinate the same traffic laws across different states, it is very difficult and still varied today
    <ul>
      <li>[discussion] becomes even greater today as we have more population and more complicated dynamics such as powerful econ people</li>
    </ul>
  </li>
</ol>

<h1 id="urban-politics">Urban Politics</h1>

<p>What counts as a government?</p>

<blockquote>
  <p><strong>US census of Governments</strong> uses the three criteria</p>

  <ul>
    <li>existence of organized entity</li>
    <li>governmental character: power to levy <mark>taxes</mark>, issue debt, i.e. actions that governments usually take</li>
    <li>substantial <em>autonomy</em></li>
  </ul>
</blockquote>

<p>Examples of local governments that <em>satisfy</em> this definition</p>

<ul>
  <li>
    <p>county government/parish</p>
  </li>
  <li>
    <p>city government</p>
  </li>
  <li>
    <p>township</p>
  </li>
  <li>
    <p>school district: raise taxtes, make policies</p>
  </li>
  <li>
    <p>other <strong>special purpose districts</strong>/governments: e.g. levy taxes and provide funding. This include</p>

    <ul>
      <li>airports, cemeteries, corrections, jails</li>
      <li>electric power, fire protecion, gas supply district</li>
      <li>highways, hospitals, housing, and community development</li>
    </ul>

    <p>to what degree can they tax? This is often restricted/predefined by the state government.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Special district</strong> governments are independent, special purpose governmental units, other than school district governments, that exist as separate entities with substantial administrative and fiscal independence from general purpose local governments.</p>
</blockquote>

<p>Why are we discussing this? Why so many special form of governments?</p>

<blockquote>
  <p>When you want to get something new done, you can</p>

  <ul>
    <li>get it done with an existing governemnt</li>
    <li>create a new government with that responsibility</li>
  </ul>

  <p>Hence this is to explain why are we having <strong>90,126</strong> governments (a lot, because <mark>people want different things</mark>) in the US at the time of this note, according to the census.</p>
</blockquote>

<p>A more detailed breakdown:</p>

<table>
  <thead>
    <tr>
      <th>Type of Govt</th>
      <th>Number</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>County</td>
      <td>3,031</td>
    </tr>
    <tr>
      <td>City</td>
      <td>19,522</td>
    </tr>
    <tr>
      <td>Township</td>
      <td>16,364</td>
    </tr>
    <tr>
      <td>Independent School District</td>
      <td>12,884</td>
    </tr>
    <tr>
      <td>others</td>
      <td>37,381</td>
    </tr>
    <tr>
      <td>etc</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>With this in 2010, this <em>gives 1 govt per 3800 people.</em></p>

<p>But is having so many forms of government good or bad?</p>

<blockquote>
  <p><strong>Proliferation of government</strong> can result in</p>

  <ul>
    <li>[+] allows for diversity of policy</li>
    <li>[-] raises cost of oversight = hard to keep up with what they are doing, and who to support $\implies$ problem for democratic control</li>
    <li>[-] can create coordination problems/free rider .e.g people inaction</li>
    <li>[+] can allow for greater expertise development = governors becomes better at their job in govt = improves quality of action</li>
    <li>[-] surely duplication effort</li>
    <li>[-] allows for greater preference matching and disparity, as shown in the next section</li>
  </ul>
</blockquote>

<h2 id="political-boundaries">Political Boundaries</h2>

<p>Consider you are the <em>green</em> local government, and you want you policy to pass. ‘O’ indicates people who supports your policy, and ‘X’ against it.</p>

<p>Interestingly, you can get policy change if you <mark>change the boundary</mark> of your affected people/region</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Deadlock</th>
      <th style="text-align: center">3/5 Majority</th>
      <th style="text-align: center">3/4 Super Majority</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230208180859085.png" alt="image-20230208180859085" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230208180906170.png" alt="image-20230208180906170" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230208180916479.png" alt="image-20230208180916479" /></td>
    </tr>
  </tbody>
</table>

<p>In fact, you can “steal” an election</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230215161716888.png" alt="image-20230215161716888" style="zoom:15%;" /></p>

<blockquote>
  <p><strong>Gerrymandering</strong> (illegal): manipulate the boundaries of (an electoral constituency) so as to favor one party or class. This is especially done by people who are already in charge. (think of path dependence in <a href="#Institutional Change">Institutional Change</a>)</p>
</blockquote>

<ul>
  <li>e.g. exists, such as in Chicago, it was made so that republicans cannot get enough support</li>
  <li>e.g. NY, which is controlled by democrats, and wanted to redraw on a boundary to increase its chance</li>
</ul>

<p>Therefore, changing jurisdiction boundaries can be beneficial to <mark>control what policy is passed</mark>. This can actually be done in two ways</p>

<blockquote>
  <p><strong>Preference sorting</strong> = change policies that are politically possible, achieved by mechanism indicated above</p>
</blockquote>

<blockquote>
  <p><strong>Economic sorting</strong> = change policies financially possible. Sort the geographical regions such that certain district becomes restricted in funds $\implies$ can no longer enact certain policies. (e.g. many schools are funded by <em>large</em> public money)</p>
</blockquote>

<h2 id="limits-on-local-government">Limits on Local Government</h2>

<p>What can the local government <mark>not do</mark>, given that it seems powerful with all the different mechanics above?</p>

<ul>
  <li><strong>state preemption</strong> = (recall) federal law that asserts national government control over an area. Intent is to limit state government authority</li>
  <li>wealth in jurisdiction/resources</li>
  <li><strong>state restrictions on tax revenue</strong>: state laws restrict what kind of tax revenue or rate a local jurisdiction can set
    <ul>
      <li>hence restricts fiscal ability, and limits what policies are possible</li>
    </ul>
  </li>
  <li><strong>population and business flight</strong> = when you <em>change policy</em> in a region, business and population can just leave = restrict local government’s choices</li>
  <li><strong>elections</strong>: need to cater to people’s wills, otherwise step down of power
    <ul>
      <li>however this is a “twist” to this, being anti-democratic:  can create policies so that a) other policies are much harder to achieve = transactional cost, or b) difficult for people who are against it to “represent” themselves in elections</li>
    </ul>
  </li>
  <li><strong>free-rider</strong> problems (e.g. due to the proliferation of local gov) and <strong>coordination problem</strong> still exists.
    <ul>
      <li>e.g. land policies: local gov has the right to set which area is zoned for what kind of houses. However, they tend to prefer zoning for luxury houses than for affordable houses (wishing other local govs to establish) $\implies$ difficult to build enough affordable housing</li>
    </ul>
  </li>
</ul>

<h2 id="more-on-new-york-city">More on New York City</h2>

<p>ref: <a href="https://clio.columbia.edu/catalog/14759712">Community power in a postreform city: politics in New York City</a>; Pecorella, Robert F, Chapter 1</p>

<blockquote>
  <p>The author presents a <strong>contextual approach</strong> to urban politics, which suggest that</p>

  <ul>
    <li>periodic fiscal crisis $\implies$ regime change in New York’s governance a lot</li>
    <li>politics unfolds within a <strong>social-economic</strong> environment that constraints the breadth of options to public officials</li>
    <li>so that during periods of crisis, local politics becomes <strong>more a function of local economics</strong></li>
  </ul>
</blockquote>

<p>But it is also worth noting the <em>other popular schools of urban politics</em>:</p>

<blockquote>
  <p><strong>Pluralism</strong>, in political science, the view that in liberal democracies power is (or should be) <mark>dispersed among a variety of economic and ideological pressure/interest groups</mark> and is not (or should not be) held by a single elite or group of elites.</p>
</blockquote>

<p><strong>Pluralism believes that</strong></p>

<ul>
  <li><em>diversity</em> is beneficial to society and that autonomy should be enjoyed by disparate functional or cultural groups within a society, including religious groups, trade unions, professional organizations, and ethnic minorities.</li>
  <li><em>continual competition</em> among diverse groups, with none able to accumulate sufficient re­sources to monopolize the political game, is conducive to the development of polyarchy where democratic norms govern</li>
  <li>this plural system thus moves incrementally in <em>balancing</em> the pressures for stability and change</li>
</ul>

<p>Under the pluralist view (in some sense resembles the <a href="#Political Foundation of the US">multi-tradition view of US politics</a>):</p>

<ul>
  <li>New York emphasized group competition when analyzing both periods of normal politics $\implies$ viewed as a <strong>diverse political arena</strong> where competing interest groups interact with city officials to secure some share of the prizes of local politics</li>
  <li>the 1975 fiscal crisis resulted from economic decline following a temporary <strong>imbalance in the city’s interest group configuration</strong></li>
</ul>

<blockquote>
  <p><strong>Statism</strong> is the doctrine that the political authority of the state is legitimate to some degree. This may include economic and social policy, especially in regard to taxation and the means of production.</p>
</blockquote>

<p>Statism differs with Pluraist in</p>

<ul>
  <li>From the statist perspective, then, urban fiscal crises are the predictable con­ sequence of <em>government’s inability</em> to exercise authority and choose among dif­ ferent claims on public resources.</li>
  <li>although resources are noncumulative from a system-wide perspective, the agency-client relationships that characterize group entrenchment represent <em>cumulative power</em> within a particular policy area. The ability of entrenched interests to exclude countervailing groups from their policy domains negates the pluralist concepts of political competition.</li>
</ul>

<blockquote>
  <p><strong>Stratification approach</strong>: local politics is essentially the domain of eco­ nomic elites whose policy influence overrides that of any other group or coali­ tion in the city.</p>
</blockquote>

<p>They differ from previous groups in that:</p>

<ul>
  <li>instead of concentrating their attention on public decision­ making processes group, stratification believes the <em>determinant role that economic elites play</em> in local politics.</li>
  <li><em>elitists</em> researcher thus believes that a) an <em>economic elite</em> controls most major public policy decisions; b) city politics includes a visible political class whose members contest for office but who are, in the final analysis, subservient to the local economic elites</li>
  <li>from the perspective of elite theorists, urban fiscal crises are the consequence of the <em>self-interested policies pursued by the economic elites</em> who control cities</li>
</ul>

<h1 id="us-civil-rights">US Civil Rights</h1>

<p>The U.S Constitution and its founding principles of liberty, equality, and justice are admired and emulated the world over. However, not everyone living in the U.S. has enjoyed the same treatment and freedoms the law promises: e.g. women, immigrants, people of color, LGBTQ people, people with disabilities, and other groups, a majority of Americans have been <em>deprived of basic rights and opportunities</em>.</p>

<p>The belief that people should be treated <strong>equally under the law</strong> is one of the cornerstones of political thought in the United States. Yet not all citizens have been treated equally throughout the nation’s history. Some types of unequal treatment are considered acceptable in some contexts, while others are clearly not.</p>

<blockquote>
  <p>No one would consider it acceptable to allow a ten-year-old to vote, because a child lacks the ability to understand important political issues, but all reasonable people would agree that it is wrong to mandate racial segregation or to deny someone voting rights on the basis of race. It is important to understand <mark>which types of inequality are unacceptable and why</mark>.</p>
</blockquote>

<h2 id="what-are-civil-rights">What are Civil Rights?</h2>

<p>Before we dive in on what is Civil Rights, it is good to know</p>

<blockquote>
  <p>We typically envision <strong>civil liberties</strong> as <mark>limitations on government power</mark>, intended to <mark>protect freedoms</mark> upon which governments may not legally intrude.</p>
</blockquote>

<p>For example</p>

<ul>
  <li>the First Amendment denies the government the power to prohibit “the free exercise” of religion.</li>
  <li>the Eighth Amendment prohibits the application of “cruel and unusual punishments” to those convicted of crimes,</li>
</ul>

<blockquote>
  <p><strong>Civil rights</strong>, on the other hand, are guarantees that government officials will treat <mark>citizens equally</mark> and that decisions will be made on the basis of merit rather than race, gender, or other personal characteristics.</p>

  <p>In a sense this is <em>also limitation on government</em>, but <mark>limiting the government’s ability to discriminate</mark> or treat some people differently, unless the unequal treatment is based on a valid reason, such as age.</p>
</blockquote>

<p>For example:</p>

<ul>
  <li>
    <p><strong>Fifth Amendment</strong>:  “all men are created equal” by providing <em><mark>de jure equal treatment</mark></em> under the law.</p>
  </li>
  <li>
    <p><strong>equal protection clause</strong> of the <strong>Fourteenth</strong> Amendment, which states, in part, that “No State shall . . . deny to any person within its jurisdiction the equal protection of the laws.”; also ensure that the states would respect the civil liberties of <em>formerly enslaved</em> people (from Civil War).</p>
  </li>
</ul>

<p>But how do you identify discrimination? What count as discrimination? Consider the following examples:</p>

<ul>
  <li>need a a minimum age for driving $\to$ age discrimination?</li>
  <li>school only enroll students have a high school diploma or a particular score on the SAT or ACT $\to$ discriminate students with weaker grades?</li>
</ul>

<p>How can the federal, state, and local governments “discriminate” in all these ways even though the equal protection clause seems to suggest that everyone be treated the same?</p>

<blockquote>
  <p>The decision between what is discrimination and what is not lies in the <strong>purpose</strong> of the discriminatory practice, and really, <strong>how justifiable it is</strong>.</p>
</blockquote>

<p>The simple, most general rule is the <strong>rational basis test</strong>. That is, as long as there’s a reason for treating some people differently that is “rationally related to a legitimate government interest,” the discriminatory act or law or policy is acceptable.</p>

<ul>
  <li>e.g. universities discriminate against students with weaker grades and test scores because these students most likely do not yet possess the knowledge or skills needed to do well in their classes</li>
</ul>

<p>However, depending on what group of people is discriminated, courts apply more stringent rules to policies, laws, and actions</p>

<blockquote>
  <p>Discrimination based on <strong>gender or sex</strong> is generally examined with <strong>intermediate scrutiny</strong>.</p>

  <ul>
    <li>need to demonstrate such discrimination is “<em>substantially</em> related to an important governmental objective.”</li>
    <li>e.g. laws that treat men and women differently are <em>sometimes</em> upheld,</li>
  </ul>
</blockquote>

<blockquote>
  <p>Discrimination against members of <strong>racial, ethnic, or religious groups</strong> or those of <strong>various national origins</strong> is reviewed with the <strong>strict scrutiny</strong> standard</p>

  <ul>
    <li>there is a compelling governmental interest in treating people from one group differently</li>
    <li>if there is a <em>non-discriminatory way to accomplish the goal</em> in question, discrimination should not take place.</li>
    <li>e.g. laws and actions that are challenged under strict scrutiny have rarely been upheld</li>
  </ul>
</blockquote>

<p>In summary, a simple helper method for you to <mark>identify true discrimination</mark></p>

<ol>
  <li><em>Which groups?</em> First, identify the group of people who are facing discrimination.</li>
  <li><em>Which right(s) are threatened?</em> Second, what right or rights are being denied to members of this group?</li>
  <li><em>What do we do?</em> Third, what can the government do to bring about a fair situation for the affected group? Is proposing and enacting such a remedy realistic?</li>
</ol>

<h2 id="the-african-american-struggle-for-equality">The African American Struggle for Equality</h2>

<p>Here I summarize some major events related to civil rights and civil war:</p>

<p><em>Antebellum</em>: 1787-1865</p>

<ol>
  <li>In the Declaration of Independence, Thomas Jefferson made the radical statement that “all men are created equal” and “are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.”</li>
  <li>But at his time, Jefferson also owned dozens of other human beings as his personal property. He recognized this contradiction, and agreed to free those upon his death</li>
  <li>but still, framers of the Constitution <strong>chose not to address the issue in any definitive way</strong>. Political support for abolition was very much a minority stance in the United States at the time</li>
  <li>As US expanded westward $\to$ They feared the expansion of slavery would lead to the political dominance of the South over the North and would deprive small farmers in the newly acquired western territories who could not afford to enslave others.</li>
  <li>President Abraham Lincoln had been willing to allow slavery to continue in the South to preserve the Union, he changed his policies regarding abolition over the course of the war.</li>
  <li><strong>Emancipation Proclamation</strong> on January 1, 1863: Although it stated “all persons held as slaves . . . henceforward shall be free,” the proclamation was limited in effect to the states that had rebelled</li>
</ol>

<blockquote>
  <p>Basically a period where you have:</p>

  <ul>
    <li><strong>anti-slavery thoughts</strong>: abolitionists, black religious, ed, prof, business groups; small farmres, merchants, markers, esp. N &amp; border
      <ul>
        <li>note that they are here for <em>different reasons</em>, e.g. purely economical, ideological, moral.</li>
      </ul>
    </li>
    <li><strong>pro-slavery thoughts</strong>: slave owners, textile; industrialists in N, artisans and working-class, European immigrates</li>
  </ul>
</blockquote>

<hr />

<p><em>Civil Rights in the Court</em>: 1877-1960s</p>

<ol>
  <li>
    <p>After the civil war, changes wrought by Fifth and Fourteens Amendment introduced banning of slavery and equal rights/treatments</p>
  </li>
  <li>
    <p>But soon, violence in the hands of white men was used to discourage Black people from exercising the rights they had been granted.</p>
  </li>
  <li>
    <p>The revocation of voting rights, or <strong>disenfranchisement</strong>, took a number of forms</p>

    <table>
      <thead>
        <tr>
          <th>Method</th>
          <th>How it works</th>
          <th>How it discriminates</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>literacy tests</strong> and <strong>understanding tests</strong></td>
          <td>call on voter to demonstrate his (and later, her) ability to read a particular passage of text.</td>
          <td>more difficult passages to those whose registration they wanted to deny (typically, Black people).</td>
        </tr>
        <tr>
          <td><strong>grandfather clause</strong></td>
          <td>exempted those who had been allowed to vote in that state prior to the Civil War and their descendants from literacy and understanding tests.</td>
          <td>to protect in some states, poorer, less-literate white voters feared being disenfranchised due to the literacy/understanding test</td>
        </tr>
        <tr>
          <td> </td>
          <td> </td>
          <td>allowed most illiterate white people to vote while leaving obstacles in place for Black people who wanted to vote as well.</td>
        </tr>
        <tr>
          <td><strong>poll tax</strong></td>
          <td>pay to register to vote.</td>
          <td>Because formerly enslaved people were usually quite poor, they were less likely than White men to be able to pay poll taxes.</td>
        </tr>
        <tr>
          <td><strong>white primary</strong></td>
          <td>they held primary elections to choose the Democratic nominee in which only White citizens were allowed to vote.</td>
          <td>make the votes from Black people meaningless since since White voters can agree beforehand to support whoever won the Democrats’ primary</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>alongwith disenfranchisement, there is also <strong>discrimination of treatment</strong> - “<strong>separate but equal</strong>”</p>
  </li>
</ol>

<ul>
  <li>As long as nominally equal facilities were provided for both races, it was legal to require members of each race to use the facilities designated for them.</li>
  <li>state and local governments passed laws limiting neighborhoods in which Black and White people could live.</li>
  <li>Collectively, these discriminatory laws came to be known as <mark>Jim Crow laws = legalized racial segregation</mark></li>
</ul>

<ol>
  <li>
    <p>Then comes a series of <strong>court rulings and accusations</strong> made by organizations such as National Association for the Advancement of Colored People (NAACP) for equal treatment</p>

    <ul>
      <li>e.g. overturning segregation in education.</li>
      <li>Beyond these favorable court rulings, however, progress toward equality for African Americans remained slow in the 1950s.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>Basically a period where you have:</p>

  <ul>
    <li><strong>anti-Jim Crow thoughts</strong>: liberal democrats and liberal republicans; black business; most non-white advocacy organizations</li>
    <li><strong>pro-Jim Crow thoughts</strong>: conservative south democrats and republicans, most white business, and most white academic institution</li>
  </ul>
</blockquote>

<hr />

<p><em>Legislating Civil Rights and Post Civil Rights Movement</em>: 1970s - (2016?)</p>

<ol>
  <li>After Rosa Parks refused to give up her bus seat to a White person and was arrested, Civil rights pioneers adopted these measures in the 1955–1956 Montgomery bus boycott.
    <ul>
      <li>prefer more confrontational approaches, including the use of <strong>direct action</strong> campaigns relying on marches and demonstrations.</li>
      <li>The strategies of nonviolent resistance and <strong>civil disobedience</strong>,</li>
    </ul>
  </li>
  <li>As the campaign for civil rights continued and gained momentum, President John F. Kennedy called for Congress to pass new civil rights legislation, which began to work its way through Congress in 1963 $\implies$ for the first time <mark>outlawed segregation/discrimination</mark> and other forms of discrimination by most businesses that were open to the public</li>
  <li>Progress in registering African American voters remained slow in many states despite increased federal activity supporting it, so <mark>civil rights leaders including Martin Luther King, Jr</mark>. decided to draw the public eye to the area where the greatest resistance to voter registration drives were taking place.
    <ul>
      <li>planned a march from <strong>Selma</strong> to Montgomery in March 1965.</li>
      <li>The events at Selma galvanized support in Congress for a follow-up bill solely dealing with the right to vote. The <strong>Voting Rights Act</strong> of 1965 went beyond previous laws by requiring greater oversight of elections by federal officials.</li>
    </ul>
  </li>
  <li>But of course, there are other approaches to black rights. <mark>Malcolm X</mark> expressed significant distrust of White people, and advocated for their <strong>separation</strong> from the United States through eventual emigration to Africa.
    <ul>
      <li>His position was attractive to many young African Americans, especially after Martin Luther King, Jr. was assassinated in 1968.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>Basically a period where you have:</p>

  <ul>
    <li><strong>Race-Conscious</strong>: abolitionists; most democrats; non-white advocacy orgs
      <ul>
        <li>recognizes and takes into account the significance of race in society, including its impact on individuals and groups.</li>
        <li>This approach acknowledges that race is an important factor in shaping experiences and outcomes and seeks to address racial disparities and inequities.</li>
      </ul>
    </li>
    <li><strong>Color-blind</strong>: most republicans; conservative democrats; most federal and state judges
      <ul>
        <li>assumes that race should not be a factor in how people are treated or perceived</li>
      </ul>
    </li>
  </ul>

  <p>which is like trying to answer the question: <em>how do you treat</em> people equally</p>
</blockquote>

<hr />

<p>So in short:</p>

<ul>
  <li>
    <p>south wanted slavery, 19th century northern and <strong>westerners</strong> had to fight to end it</p>
  </li>
  <li>
    <p>south wanted to limit black rights post-emancipation - <strong>federal</strong> courts had to stop it</p>
  </li>
  <li>
    <p>civil rights movement removed the last elements of white supremacy from law</p>
  </li>
</ul>

<blockquote>
  <p><strong>Substance of policy debates determines coalition possibilities</strong>. i.e. depending on different time periods/context, there are different racial problems (e.g. due to economic grounds, etc) $\implies$ different support groups, some of which might seem confounding</p>
</blockquote>

<h3 id="geography-of-white-supremacy">Geography of White Supremacy</h3>

<p>It is widely held (but <strong>false</strong>) that racial conflicts in the US caused by <strong>Southern racism</strong>. Why?</p>

<ol>
  <li>federalism allowed variety of tactics to preserve white supremacy</li>
  <li>black population concentrated in South (by design) until 20th century</li>
  <li>Federal restricted non-Black citizenship advocated by western states</li>
</ol>

<p>Another piece of data would be the probability of being <strong>lynched</strong> = 1889 - 1918</p>

<ul>
  <li>physical violence as a tactic to suppress the political rights of people $\implies$ show others to obey more</li>
  <li>a period where white people trying to deter black from political participation</li>
</ul>

<p>There is also discrimination against other groups:</p>

<ul>
  <li><strong>Racial Restrictions on Citizenship</strong>: In 1780-1870: only ‘free whites’ were allowed to naturalize, and for example, Chinese, were only allowed to naturalize in 1943. From 1952, finally no racial restrictions.</li>
</ul>

<blockquote>
  <p>Racial hierarchy is an <strong>American legacy</strong>, not Southern.</p>
</blockquote>

<p>But at the same time:</p>

<blockquote>
  <p>Population change since 1965 has opened new frontiers</p>
</blockquote>

<p>Some of the <strong>very recent issues include</strong></p>

<ul>
  <li><strong>Redistricting</strong>: Changes in population can require redistricting, or the redrawing of electoral boundaries, which can lead to political battles over how to allocate political power and representation.</li>
  <li><strong>Immigration policies:</strong> As immigration patterns have changed, there have been debates and political conflicts over how to manage borders, regulate immigration, and balance concerns around national security and economic growth with issues of human rights and social justice.</li>
  <li><strong>Urban-rural divides:</strong> Population change has also contributed to the urban-rural divide in many countries, with different political priorities and values emerging in urban and rural areas, and political leaders struggling to bridge these differences.</li>
</ul>

<h3 id="voting-rights">Voting Rights</h3>

<p>Several related courts:</p>

<ul>
  <li>Dred Scott v Sanford (1857): held back people with enslaved ancestor could not be citizen. But then supemacy court decided that black <em>does not have the right to sue</em></li>
  <li>Plessy v. Ferguson (1896): whether or not state mandated segregation is a violation of equal protection. Decision: 14th amendment.</li>
  <li>Korematsu v. United States (1944): upheld internment of Japanses-Americans as allowed</li>
  <li>Shelly v. Kraemer (1948): previously allowed private agreements to involve racial restrictions. Now unconstitutional under this one
    <ul>
      <li>the issue here was that private owners created a covenant and the Fourteenth Amendment applies to state action.</li>
      <li>However, the Court reasoned that the Fourteenth Amendment applies to judicial enforcement of such covenants, as that is state action. Thus, the Court concluded that the state is taking action in this case, therefore such an <em>private racial agreement cannot be allowed</em>.</li>
    </ul>
  </li>
  <li>Brown v. Board of Education (1954): banned segregation in public schools</li>
  <li>Shelby County v. Holder (2013): certain states who were racially restrictive are require to pre-clear before elections. Ruling is that those pre-clearance requirement is unconstitutional
    <ul>
      <li>i.e. previously required certain states and localities with a history of discrimination in voting to obtain federal approval, or “preclearance,” before changing their voting laws or procedures</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Sometimes the judgment is conservative, and sometimes liberal.</p>
</blockquote>

<p>More on <strong>white primary</strong> (Texas style):</p>

<ul>
  <li>Texas banned white people voting in primary, but then it is sued to be unconstitutional</li>
  <li>In 1935, they won a rule “party rule banning Black voting in primary was private action, so it is <em>allowed</em>”</li>
  <li>In 1944, delegating power to parties to run primary still a form of state action, so it is <em>unconstitutional again</em></li>
</ul>

<p>What does this show us?</p>

<ul>
  <li>white supremacists persistence, that people even try to move discriminatory actions into private sphere to prevent scrutiny</li>
</ul>

<blockquote>
  <p>therefore, it is very difficult to use fixed actions or policy positions to treat discriminatory intent.</p>
</blockquote>

<h2 id="the-fights-for-womens-rights">The Fights for Women’s Rights</h2>

<ol>
  <li>At the time of the American Revolution, women had few rights. Although single women were allowed to own property, married women were not.
    <ul>
      <li>e.g. all personal property they owned legally became their husbands’ property.</li>
      <li>e.g. their husbands were entitled to their wages.</li>
    </ul>
  </li>
  <li>Following the Revolution, women’s conditions did not improve. Women were not granted the right to vote by any of the states except New Jersey.</li>
  <li>In 1848, Stanton and Mott called for a women’s rights convention, the first ever held specifically to address the subject, at Seneca Falls, New York.
    <ul>
      <li>Stanton wrote the <strong>Declaration of Sentiments,</strong> which was modeled after the Declaration of Independence and proclaimed women were equal to men and deserved the same rights.</li>
      <li>The Declaration passed, but the resolution demanding <strong>suffrage</strong> was the only one that <strong>did not pass</strong> unanimously.</li>
    </ul>
  </li>
  <li>The more radical <strong>National Woman’s Party</strong> (NWP), led by Alice Paul, advocated the use of stronger tactics. The NWP held public protests and picketed outside the White House
    <ul>
      <li>Finally, in 1920, the triumphant passage of the Nineteenth Amendment granted all women the <strong>right to vote</strong>.</li>
    </ul>
  </li>
</ol>

<p>Civil Rights and the Equal Rights amendment</p>

<ol>
  <li>Just as the passage of the Thirteenth, Fourteenth, and Fifteenth Amendments did not result in equality for African Americans, the Nineteenth Amendment <em>did not end discrimination</em> against women in education, employment, or other areas of life</li>
  <li>A second women’s rights movement emerged in the 1960s to address these problems. <strong>Title VII of the Civil Rights Act</strong> of 1964 prohibited discrimination in employment on the basis of sex as well as race, etc
    <ul>
      <li>Nevertheless, women continued to be denied jobs because of their sex and were often sexually harassed at the workplace.</li>
    </ul>
  </li>
  <li>National Organization for Women (NOW) = NOW promoted workplace equality, including equal pay for women. NOW also declared its support for the <strong>Equal Rights Amendment</strong> <strong>(ERA)</strong>, which mandated equal treatment for all regardless of sex.
    <ul>
      <li>but until today, ERA failed to be ratified</li>
      <li><strong>Title IX</strong> of the United States Education Amendments of 1972 passed into law as a federal statute (not as amendment) $\implies$ if a school receives federal aid, it cannot spend more funds on programs for men than on programs for women.</li>
    </ul>
  </li>
</ol>

<p>Pressing Issues today</p>

<ol>
  <li><strong>Roe v. Wade,</strong> (1973) was a landmark decision of the U.S. Supreme Court conferred the right to choose to have an abortion. In June 2022, the Supreme Court <mark>overruled</mark> Roe in <em>Dobbs v. Jackson Women’s Health Organization</em> on the grounds that the substantive right to abortion was not “deeply rooted in this Nation’s history or tradition”.</li>
  <li><strong>glass ceiling</strong>, an invisible barrier caused by discrimination, prevents women from rising to the highest levels of American organizations, including corporations, governments, academic institutions, and religious groups. <strong>Women earn less money than men for the same work.</strong></li>
</ol>

<h2 id="civil-rights-for-indigenous-groups">Civil Rights for Indigenous Groups</h2>

<blockquote>
  <p>This includes group such as Native Americans, Alaskans, and Hawaiians</p>

  <ul>
    <li>Ironically, Native Americans were not granted the full rights and protections of U.S. citizenship until long after African Americans and women were, with many having to wait <strong>until the Nationality Act of 1940 to become citizens</strong>.</li>
  </ul>
</blockquote>

<p>NATIVE AMERICANS LOSE THEIR LAND AND THEIR RIGHTS</p>

<ol>
  <li>From the very beginning of European settlement in North America, Native Americans were abused and exploited.</li>
  <li>As White settlement spread westward over the course of the nineteenth century, Indian tribes were forced to move from their homelands.</li>
  <li>In 1830, Congress passed the Indian Removal Act, which forced Native Americans to move west of the Mississippi River.
    <ul>
      <li>Not all tribes were willing to leave their land, however. The <strong>Cherokee</strong> in particular resisted</li>
      <li>Between 1831 and 1838, members of several southern tribes, including the Cherokees, were forced by the U.S. Army to move west</li>
      <li>The forced removal of the Cherokees to Oklahoma Territory, which had been set aside for settlement by displaced tribes and designated Indian Territory, resulted in the death of one-quarter of the tribe’s population. The Cherokees remember this journey as the <strong>Trail of Tears</strong>.</li>
    </ul>
  </li>
  <li>By the time of the Civil War, most Indian tribes had been relocated west of the Mississippi. However, once large numbers of White Americans and European immigrants had also moved west after the Civil War, Native Americans once <em>again found themselves displaced</em>.</li>
  <li>In 1898, the Curtis Act dealt the final blow to Indian sovereignty by <strong>abolishing all tribal governments</strong>.</li>
</ol>

<p>THE FIGHT FOR NATIVE AMERICAN RIGHTS</p>

<ol>
  <li>As Indians were removed from their tribal lands and increasingly saw their traditional cultures being destroyed over the course of the nineteenth century, a movement to protect their rights began to grow.
    <ul>
      <li>e.g. Sarah Winnemucca, member of the Paiute tribe, lectured throughout the east in the 1880s in order to acquaint White audiences with the injustices suffered by the western tribes.</li>
    </ul>
  </li>
  <li>In 1924, the <strong>Indian Citizenship Act</strong> granted citizenship to all Native Americans born after its passage.</li>
  <li>In 1934, Congress passed the <strong>Indian Reorganization Act</strong>, which ended the division of reservation land into allotments. It returned to Native American tribes the right to institute self-government on their reservations.
    <ul>
      <li>However, most tribes remained <em>impoverished</em>, and many Native Americans, despite the fact that they were now U.S. citizens, were <strong>denied the right to vote</strong></li>
    </ul>
  </li>
  <li>In the 1960s, a modern Native American civil rights movement, inspired by the African American civil rights movement, began to grow.
    <ul>
      <li>In 1969, a group of Native American activists from various tribes, took control of <strong>Alcatraz</strong> Island in San Francisco Bay</li>
      <li>In 1973, members of the <strong>American Indian Movement</strong> <strong>(AIM)</strong>, a more radical group than the occupiers of Alcatraz, temporarily took over the offices of the Bureau of Indian Affairs in Washington, DC.</li>
    </ul>
  </li>
  <li>The current relationship between the U.S. government and Native American tribes was established by the <strong>Indian Self-Determination and Education Assistance Act</strong> of 1975.
    <ul>
      <li>tribes assumed control of programs that had formerly been controlled by the BIA, such as education and resource management, and the federal government provided the funding.</li>
    </ul>
  </li>
  <li>In addition to gains in <strong>water rights and land rights</strong>, Native American tribes made other gains in recent decades. Tribes have robust and well-recognized governing institutions based on democratic principles.</li>
  <li>Finally, the appointment by President Biden, and subsequent Senate confirmation, of <strong>Rep. Deb Haaland (D-NM)</strong> as Secretary of the Interior was a powerful and pathbreaking moment. She is the first Native American to hold that position at Interior, which includes the Bureau of Indian Affairs.</li>
</ol>

<h2 id="equal-protection-for-other-groups">Equal Protection For Other Groups</h2>

<p>Many groups in American society have faced and continue to face challenges in achieving equality, fairness, and equal protection under the laws and policies of the federal government and/or the states.</p>

<blockquote>
  <p>Some of these groups are often overlooked because they are <strong>not as large of a percentage of the U.S. population</strong> as women or African Americans, and because organized movements to achieve equality for them are <strong>relatively young</strong>.</p>
</blockquote>

<p><strong>Hispanic/Latino Civil Rights</strong></p>

<ul>
  <li><em>Hispanic</em> usually refers to native speakers of Spanish or those descended from Spanish-speaking countries. <em>Latino</em> refers to people who come from, or whose ancestors came from, Latin America. Not all Hispanics are Latinos and vice versa. People from Spain are Hispanic but are not Latino, while people from Brazil are Latino but not Hispanic.</li>
  <li>Many Latinos became part of the U.S. population following the annexation of <mark>Texas</mark> by the United States in 1845. The Spanish-speaking population of the United States increased following the Spanish-American War in 1898 with the incorporation of <mark>Puerto Rico</mark> as a U.S. territory.
    <ul>
      <li>In the early twentieth century, waves of violence aimed at Mexicans and <strong>Mexican Americans</strong> swept the Southwest. Mexican Americans in Arizona and in parts of Texas were denied the right to vote,</li>
    </ul>
  </li>
  <li>Today, Latinos constitute the largest minority group in the United States. They also have one of the highest birth rates of any ethnic group.</li>
</ul>

<p><strong>Asian American Civil Rights</strong></p>

<ul>
  <li><strong>Asian Americans</strong> have also often been discriminated against and denied their civil rights. Often stereotyped as the “the model minority” (because it is assumed they are generally financially successful and do well academically), the truth is that Asian Americans have long faced discrimination.</li>
  <li>The <mark>Chinese</mark> were the first large group of Asian people to immigrate to the United States.
    <ul>
      <li>Their willingness to work for less money than White workers led White workers in California to call for a ban on Chinese immigration. In 1882, Congress passed the <strong>Chinese Exclusion Act</strong>, which prevented Chinese from immigrating to the United States for ten years</li>
      <li>With the passage of the <strong>Immigration Act</strong> of 1924, all Asian people, with the exception of Filipinos, were prevented from immigrating to the United States or becoming naturalized citizens.</li>
    </ul>
  </li>
  <li>Discrimination against Asian Americans, regardless of national origin, increased during the <strong>Vietnam War.</strong>
    <ul>
      <li>Chinese, Japanese, Koreans, and Vietnamese caused members of these groups to unite around a shared <strong>pan-Asian identity</strong>, much as Native Americans had in the Pan-Indian movement.</li>
      <li>(<em>history</em>) The Vietnam war is fought between the communist forces of North Vietnam and the government forces of South Vietnam (with eventual allies of US troops and Chinese=strong ally of north Vietnam as well). The main cause of the Vietnam war was the spread of communism and the desire of the North Vietnamese to reunify the country under a communist government. The war was characterized by guerrilla tactics and unconventional warfare, and resulted in a significant loss of life and widespread destruction.</li>
    </ul>
  </li>
  <li>Unfortunately, recently racist vitriol related to the origin of COVID-19 has recently highlighted discrimination against Asian Americans again</li>
</ul>

<h1 id="civil-liberties">Civil Liberties</h1>

<p>In writing the Declaration of Independence in 1776, Thomas Jefferson drew on the ideas of English philosopher John Locke to express the colonists’ belief that they had certain <strong>inalienable or natural rights</strong> that no ruler had the power or authority to deny to their subjects.</p>

<blockquote>
  <p>The framers of the Constitution wanted a government that <em>would not repeat the abuses of individual liberties</em> and rights that caused them to declare independence from Britain.</p>
</blockquote>

<p>What are those freedoms? And how should we balance them against the interests of society and other individuals?</p>

<h2 id="what-are-civil-liberties">What are Civil Liberties?</h2>

<p>Recall that we defined a distinction between civil liberties and civil rights in <a href="#What are Civil Rights?">What are Civil Rights?</a>: <strong>civil liberties</strong> as limitations on government power, intended to <mark>protect freedoms upon which governments may not legally intrude</mark>.</p>

<blockquote>
  <p>Two general <strong>form of protection</strong></p>

  <ul>
    <li><strong>substantive</strong> restraints (i.e. to a particular action). For example, get contraception; neither states nor the national government can forbid people to follow a religion of their choice, even if politicians and judges think the religion is misguided.</li>
    <li><strong>procedural</strong> restraints. For example, police must follow certain procedure when arresting/interrogating people</li>
  </ul>

  <p>That said, the <strong><em>way</em></strong> you practice your religion, like any other practice, may be regulated if it impinges on the rights of others.</p>
</blockquote>

<p>The first tool you think of as to “restrict the government on regulating civil freedom” could be the “<mark>bill of rights</mark>”:</p>

<ol>
  <li>
    <p>The Constitution as drafted in 1787 <strong>did not include a Bill of Rights</strong>, although the idea of including one was proposed and, after brief discussion, dismissed in the final week of the Constitutional Convention.	The framers of the Constitution believed they</p>

    <ul>
      <li>faced much more pressing concerns—most notably keeping the fragile union together in the light of internal unrest and external threats.</li>
      <li>had adequately covered rights issues in the main body of the document
        <ul>
          <li><strong>bills of attainder</strong>: prohibit convicts or punishes someone for a crime without a trial,</li>
          <li><em>prohibiting</em> <strong>ex post facto laws</strong>: prohibit the retroactive effect that it can be used to punish crimes that were not crimes at the time they were committed,</li>
          <li>limiting the ability of Congress to suspend the <strong>writ of habeas corpus</strong>: allow a neutral judge decide whether someone has been lawfully detained.</li>
        </ul>
      </li>
      <li>Hamilton went on to argue that listing some rights might actually be dangerous, because it would provide a pretext for people to claim that rights <em>not</em> included in such a list were not protected.</li>
    </ul>
  </li>
  <li>
    <p>However, many large states—New York and Virginia in particular—believed the Constitution’s lack of specified rights became a serious point of contention, and hence did not want to ratify the constitution. As a result, the framers agreed to consider incorporating provisions (i.e. later the <strong>Bill of Rights</strong>). Ultimately, <mark>James Madison</mark> proposed ten of the amendments were successfully ratified</p>

    <table>
      <thead>
        <tr>
          <th>First Amendment</th>
          <th>Right to freedoms of religion and speech; right to assemble and to petition the government for redress of grievances; right to a free press</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Second Amendment</td>
          <td>Right to keep and bear arms to maintain a well-regulated militia</td>
        </tr>
        <tr>
          <td>Third Amendment</td>
          <td>Right to not house soldiers during time of war</td>
        </tr>
        <tr>
          <td>Fourth Amendment</td>
          <td>Right to be secure from unreasonable search and seizure</td>
        </tr>
        <tr>
          <td>Fifth Amendment</td>
          <td>Rights in criminal cases, including due process and indictment by grand jury for capital crimes, as well as the right not to testify against oneself</td>
        </tr>
        <tr>
          <td>Sixth Amendment</td>
          <td>Right to a speedy trial by an impartial jury</td>
        </tr>
        <tr>
          <td>Seventh Amendment</td>
          <td>Right to a jury trial in civil cases</td>
        </tr>
        <tr>
          <td>Eighth Amendment</td>
          <td>Right to not face excessive bail, excessive fines, or cruel and unusual punishment</td>
        </tr>
        <tr>
          <td>Ninth Amendment</td>
          <td>Rights retained by the people, even if they are not specifically enumerated by the Constitution</td>
        </tr>
        <tr>
          <td>Tenth Amendment</td>
          <td>States’ rights to powers not specifically delegated to the federal government</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<blockquote>
  <p>BoR can be thought of a way to limit conformity cost. i.e. to limit the ability of the majority to impose their preferences on minority groups through legislation or other means. $\implies$ certain <strong>minority preference/rights are still preserved</strong></p>

  <p>However, the meaning and intent of BoR is often unclear.</p>
</blockquote>

<p>For example:</p>

<ul>
  <li><strong>ambiguous language</strong>
    <ul>
      <li>in 8th amendment What is “curel and unusual punishment”</li>
      <li>establishment clause and prayer (does praer in public school count as religious )</li>
    </ul>
  </li>
  <li><strong>practical contradictions</strong>
    <ul>
      <li>press freedom (1st), but public trial with impartial jury (8th) $\implies$ very hard if facts have been exposed in media, even illegal, can make it hard to make impartial decisions</li>
    </ul>
  </li>
  <li><strong>omissions</strong>
    <ul>
      <li>who does the BoR restrict? Does it apply to state governments? What about local governments?</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>As a result, the Supreme Court had to make a lot of decisions.</p>
</blockquote>

<ol>
  <li>the Barron Case (1830) ruled that BoR does not applied to state $\implies$ then BoR has no meaning really as states are the implementations</li>
  <li>then, after the Civil War, succeeded states had to agree to amendments (e.g. 14th) in order to rejoin US</li>
  <li>in the 14th amendment, obligated states to provide equal protectoin, due procecss
    <ul>
      <li>national citienship (e.g. for black people)</li>
      <li>equal protection: but note that police department was not there</li>
      <li>due process: without due proces of law, state cannot deprice any person of life and liberty</li>
    </ul>
  </li>
  <li>establishment of a monopoly of slaughter house in Arizona
    <ul>
      <li>decided that BoR does not apply to states $\to$ the decision limited the ability of the federal government to protect the rights of citizens against state abuses, and it helped to reinforce the power of state governments to regulate the activities of their citizens.</li>
      <li>It was only later, through other landmark cases such as Brown v. Board of Education and Roe v. Wade, that the Supreme Court began to expand the scope of federal power and protect the rights of citizens against state infringement.</li>
    </ul>
  </li>
  <li>over time, more and more elements of BoR gets incorporated into state $\to$ **selective incorporation **(see below as well)</li>
</ol>

<hr />

<p>EXTENDING THE BILL OF RIGHTS TO THE STATES</p>

<ol>
  <li>In the decades following the Constitution’s ratification, the Supreme Court <strong>declined to expand the Bill of Rights to curb the power of the states</strong></li>
  <li>The festering issue of the rights of enslaved persons and the convulsions of the <strong>Civil War</strong> and its aftermath forced a reexamination of the prevailing thinking about the application of the Bill of Rights to the states.
    <ul>
      <li>e.g. after the civil war, states passed “Black codes” that restricted the rights of formerly enslaved people</li>
    </ul>
  </li>
  <li>Their long-term solution was to propose and enforce two amendments to the Constitution to guarantee the rights of freed men and women.
    <ul>
      <li>With the ratification of the <strong>Fourteenth Amendment</strong> in 1868, the scope and limits of civil liberties became clearer: “<strong>no State</strong> shall make or enforce any law which shall abridge the privileges or immunities of citizens of the United States”</li>
      <li>second provision of the Fourteenth Amendment pertaining to the application of the Bill of Rights to the states is the <strong>due process clause</strong>, which requires fair treatment and procedural safeguards (e.g. the right to a trial, and that people be treated <em>fairly and impartially by government officials</em>) before the government can deprive a person of life, liberty, or property.</li>
    </ul>
  </li>
  <li>there has been a process of <strong>selective incorporation</strong> of the Bill of Rights into the practices of the states: c<em>ertain provisions must be upheld by the states</em>, even if their state constitutions and laws (and the Tenth Amendment itself) do not protect them
    <ol>
      <li><strong><em>when issue arises</em></strong>, the Supreme Court decides whether state laws violate the Bill of Rights and are therefore unconstitutional.</li>
      <li>it this still consistent with the supremacy clause? The key is that <em>if there is a conflict</em>, then supremacy clause overrules.</li>
      <li>e.g. It was only in the <em>McDonald v. Chicago</em> case two years later that the Supreme Court incorporated the Second Amendment (keep and bear arms) into state law.</li>
    </ol>
  </li>
</ol>

<h2 id="securing-basic-freedoms">Securing Basic Freedoms</h2>

<p>We can broadly divide the provisions of the Bill of Rights into three categories</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230215170855266.png" alt="image-20230215170855266" style="zoom:25%;" /></p>

<ul>
  <li>protect basic individual freedoms;</li>
  <li><strong>criminal procedural protections</strong>:  protect people suspected or accused of criminal activity or facing civil litigation;</li>
  <li>express the view that Bill of Rights is not necessarily an exhaustive list of all the rights people have and guarantees a role for state as well as federal government</li>
</ul>

<p>A bit more details into the different amendments</p>

<ol>
  <li><strong>The first amendment</strong>: guarantees both religious freedoms and the right to express your views in public.
    <ul>
      <li><strong>establishment clause</strong>. Congress is prohibited from creating or promoting a state-sponsored religion (this now includes the states).</li>
      <li><strong>free exercise clause</strong>, on the other hand, limits the ability of the government to control or restrict religious practices.</li>
      <li>basically, it not only forbids the creation of a “Church of the United States” or “Church of Ohio” it also forbids the government from favoring one set of religious beliefs over others or favoring religion (of any variety) over non-religion.</li>
    </ul>
  </li>
  <li><strong>The second amendment</strong>: the right of the people to keep and bear Arms
    <ul>
      <li>however, due to school shootings and gun violence. As a result, gun rights have become a highly charged political issue.</li>
    </ul>
  </li>
  <li><strong>The third amendment</strong>: “No Soldier shall, in time of peace be quartered in any house, without the consent of the Owner, nor in time of war, but in a manner to be prescribed by law.”
    <ul>
      <li>citizens remembered having their cities and towns occupied by British soldiers and mercenaries during the Revolutionary War, and they viewed the British laws that required the colonists to house soldiers particularly offensive</li>
    </ul>
  </li>
  <li><strong>The fourth amendment</strong>: indicates that government officials are required to apply for and receive a <strong>search warrant</strong> prior to a search or seizure;
    <ul>
      <li>sits at the boundary between general individual freedoms and the rights of those suspected of crimes.</li>
      <li>can be seen as to protects us from overzealous efforts by law enforcement to root out crime by ensuring that police have good reason before they intrude on people’s lives with criminal investigations.</li>
      <li><em>however</em>, the courts have found that police <strong>do not</strong> generally need a warrant to search the passenger compartment of a car, or to <strong>search people entering the United States from another country</strong>.
        <ul>
          <li>but they must demonstrate to a judge that there is probable cause to believe a crime has been committed or evidence will be found.</li>
          <li><strong>Probable cause</strong> is the legal standard for determining whether a search or seizure is constitutional or a crime has been committed; it is a lower threshold than the standard of proof at a criminal trial.</li>
        </ul>
      </li>
      <li><strong>exclusionary rule</strong>: obtained <em>without</em> a warrant could not be counted as evidence in a trial</li>
    </ul>
  </li>
</ol>

<h2 id="rights-of-suspects">Rights of Suspects</h2>

<blockquote>
  <p>In addition to protecting the personal freedoms of individuals, the Bill of Rights <strong>protects those suspected or accused of crimes</strong> from various forms of unfair or unjust treatment.</p>
</blockquote>

<p>The next four amendments pertain to those suspected, accused, or convicted of crimes, as well as people engaged in other legal disputes.</p>

<ol>
  <li><strong>the fifth amendment</strong>:
    <ul>
      <li>protection against <strong>self-incrimination</strong>: you have the right <em>not to give evidence</em> in court or to law enforcement officers that <em>might constitute an admission of guilt</em> or responsibility for a crime.</li>
      <li>protects individuals against <strong>double jeopardy</strong>, a process that subjects a suspect to prosecution twice for the same criminal act.</li>
    </ul>
  </li>
  <li><strong>the sixth amendment</strong>: contains the provisions that govern criminal trials, i.e. after which a person is charged with crime
    <ul>
      <li>the right to have a <em>speedy, public trial by an impartial jury</em> (i.e. excessively lengthy delays must be justified and balanced against the potential harm to the defendant.)</li>
      <li><strong>plea bargain</strong>, an agreement between the defendant and the prosecutor in which the defendant pleads guilty to the charge(s) in question, or perhaps to less serious charges, in exchange for more lenient punishment than they might receive if convicted after a full trial.</li>
    </ul>
  </li>
  <li><strong>the seventh amendment</strong>: deals with the rights of those engaged in civil disputes</li>
  <li><strong>the eighth amendment</strong>: cannot impose bail or fines that are unreasonably high or disproportionate to the alleged offense, and prohibits any cruel and unusual punishment
    <ul>
      <li>e.g. drawing and quartering, burning people alive, and the electric chair—are prohibited by this provision.</li>
    </ul>
  </li>
</ol>

<p>Examples include (criminal procedural protections)</p>

<ul>
  <li><em>Mapp v. Ohio</em> - materials obtained by unconstitutional searches cannot be used in criminal courts</li>
  <li><em>Gideon v. Wainwright</em> -  Can’t afford lawyers? State required to pay for lawyer for anyone who cannot afford it</li>
  <li><em>Miranda v. Arizona</em> - police must notify suspects of rights prior to interrogation</li>
  <li><em>In re Gault</em> - can this 15 year old be protected? Decision: procedural constitutional protections apply to juveniles as well</li>
</ul>

<h2 id="interpreting-the-bill-of-rights">Interpreting the Bill of Rights</h2>

<blockquote>
  <p>Ninth and Tenth Amendments indicate <strong>how the Constitution and the Bill of Rights should be interpreted</strong>, and lay out the residual powers of the state governments - these two amendments affect our understanding of the Constitution as a whole.</p>
</blockquote>

<ol>
  <li>
    <p><strong>the ninth amendment</strong>: “The enumeration in the Constitution, of certain rights, shall not be construed <em>to deny or disparage others</em> retained by the people.”</p>

    <ul>
      <li>i.e. James Madison and the other framers were aware they might endanger some rights if they listed a few in the Constitution and omitted others.</li>
    </ul>
  </li>
  <li>
    <p><strong>the tenth amendment</strong>: “The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.”</p>

    <ul>
      <li>also allows states to guarantee rights and liberties more fully or extensively than the federal government does</li>
      <li>however, by the supremacy clause, if the federal government passes a law or adopts a constitutional amendment that restricts rights or liberties, or a Supreme Court decision interprets the Constitution in a way that narrows these rights, the state’s protection no longer applies.</li>
    </ul>
  </li>
  <li>
    <p><mark>the right to privacy</mark>: scholars have interpreted <em>several Bill of Rights provisions</em> as an <em>indication</em> that James Madison and Congress sought to protect a common-law right to privacy: a right to be free of government intrusion into our personal life, particularly within the bounds of the home.</p>

    <ul>
      <li>
        <p>stem from the 9th amendment, i.e. implicitly reasoned from the other amendments/laws</p>
      </li>
      <li>
        <p><strong>sexual privacy</strong>: including right to obtain contraception, <strong>abortion</strong> rights (overturned recently), the rights for adults to have noncommercial, consensual sexual relationships in private.</p>

        <ul>
          <li><em>Dobbs v. Jackson Women’s Health Organization</em>: now abortion is not protected by constitution (overruled <em>Joe v. Wade</em>)</li>
        </ul>
      </li>
      <li>
        <p><strong>privacy of communication and property</strong>: this has been complicated in modern era where the society is under pervasive surveillance.</p>

        <ul>
          <li>pervasive use of GPS; and research shows that even metadata—information about the messages we send and the calls we make—can tell governments and businesses a lot about what someone is doing.</li>
          <li>increased use of drones, small preprogrammed or remotely piloted aircraft.</li>
          <li>In the United States, many advocates of civil liberties are concerned that laws such as the USA <strong>PATRIOT Act</strong> (i.e., Uniting and Strengthening America by Providing Appropriate Tools Required to Intercept and Obstruct Terrorism Act), passed weeks after the 9/11 attacks in 2001, have given the federal government too much power by making it easy for officials to seek and obtain search warrants or, in some cases, to bypass warrant requirements altogether.</li>
        </ul>

        <p>as a result, the emergence of these technologies (while unarguably beneficial) means calls for vigilance and limits on what businesses and governments can do with the information they collect and the length of time they may retain it.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>other “found” rights</strong> (implied by interpreting other laws)</p>

    <ul>
      <li>to procreate, i.e. people do have fundamental right to have children
        <ul>
          <li>living with extended family</li>
          <li>To control own child’s upbringing (e.g. <em>Meyer v NE</em>, to teach a child a foreign language)</li>
          <li>etc.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>One takeaway message from the privacy issues. e.g. abortion case with Dobbs</p>

  <ul>
    <li>the same conclusion can be reasoned to by multiple paths</li>
    <li>part of the US Supreme Court reasoning is very political palatable, given the current context
      <ul>
        <li>Hence to make a case successful it largely depends on *how you reason your case</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>And fundamentally, both the Constitutions and the BoR can be seen as an attempt to solve the collective action problem.</p>

<blockquote>
  <p>By having the Constitutions and BoR:</p>

  <ol>
    <li>Constitution $\to$ a separation of powers $\to$ ensures that decisions are made through a <strong>collective process</strong> involving multiple branches of government
      <ul>
        <li>by requiring cooperation and compromise among different groups and branches of government, it encourages coordination and limiting the ability of any one group to dominate the decision-making process</li>
      </ul>
    </li>
    <li>the Bill of Rights provides protections for individual rights and freedoms that can help to <strong>reduce conformity costs</strong> and <strong>encourage political participation.</strong>
      <ul>
        <li>By guaranteeing freedoms such as free speech, freedom of assembly, and the right to petition the government, the Bill of Rights helps to ensure that minority groups are able to have their voices heard in the decision-making proces</li>
      </ul>
    </li>
    <li>the Constitution and Bill of Rights establish a legal framework $\to$ disputes can be resolved without resorting to violence or coercion $\to$ providing a means for <strong>resolving conflicts and coordinating actions</strong>
      <ul>
        <li>e.g. as there is a law, you have knowledge of what other people can or not do to you</li>
      </ul>
    </li>
  </ol>
</blockquote>

<h1 id="problem-of-delegation">Problem of Delegation</h1>

<p>we will discuss agency models,and then discuss house and senate system in US</p>

<h2 id="agency-model">Agency Model</h2>

<blockquote>
  <p><strong>Agency Model</strong>: developed most in economics, to analyze the phenomenon that one person <strong>delegates some task to another</strong> to get something done (in stead of doing it yourself)</p>
</blockquote>

<p><em>For Example</em>:</p>

<ul>
  <li>voters delegate policy making to governments</li>
  <li>government don’t provide service directly, but sometimes ask private companies</li>
  <li>congress delegates enforcement of laws (but makes a lot of laws) to president and bureaucracy</li>
  <li>school departments delegates teaching to professors</li>
  <li>professor delegate teaching instructors/discussion sections to TAs</li>
  <li>students/parents delegate education and safety to the college they attend</li>
</ul>

<blockquote>
  <p>Sometimes really consequential decisions/responsibilities being delegated to others</p>
</blockquote>

<p>in general, leaders in an organization delegate certain decisions and responsibility to lower members $\implies$ occurs everywhere. <mark>But why did it occur?</mark></p>

<ul>
  <li>take advantage of <strong>specialization</strong> = those delegated with narrow set of tasks can develop better skills at it</li>
  <li>overcome individual <strong>time/resource constraints</strong> = simply too difficult for a single person to control everything
    <ul>
      <li>e.g. all polices report to the chief $\to$ at some point too overwhelming for that single chief to understand what is happening $\to$ hierarchical</li>
    </ul>
  </li>
  <li><strong>mitigate collection action problems</strong> (e.g. free rider) . for example, this <em>small</em> group of people has this responsibility</li>
  <li><strong>deflect responsibility</strong>: let other people enact certain harsh decisions/implementations
    <ul>
      <li>e.g. let IRS collect taxes, but when using it to build infrastructure such as bridges, MC shows up</li>
    </ul>
  </li>
</ul>

<p>While this sounds great, <mark>what can go wrong in this agency model</mark>?</p>

<ul>
  <li>they might not do what you want $\to$ <strong>moral harzard problem</strong> = personal interest for those agents could come first</li>
  <li>they might just be bad at it $\to$ <strong>adverse selection problem</strong></li>
</ul>

<blockquote>
  <p>This basically comes down to <strong>Agency Loss</strong> = different between the quality of you doing it v.s. someone else doing it = i.e. cost of due to delegating power to others</p>
</blockquote>

<p>(note that this is applicable to the proliferation of local government = different lens to the same problem)</p>

<hr />

<p>How do you reduce agency loss?  (i.e. cost of delegating power to others)</p>

<ul>
  <li><strong>Intentional disobedience</strong> can be <strong>mitigated</strong> by
    <ul>
      <li>set clear rules and instructions, be explicit what the agent should do</li>
      <li>punishment/reward, e.g. monitoring</li>
    </ul>
  </li>
  <li><strong>incompetence</strong> can be <strong>mitigated</strong> by
    <ul>
      <li>careful selection of agent with right skills</li>
      <li>dismissing agents without skills, e.g. competitions</li>
    </ul>
  </li>
</ul>

<p>But of course, this might not be easy to implement in real life</p>

<ul>
  <li>how do you keep certain FBI work on law enforcement but not CIA, so that if you tried to remove them they threaten with some compromised documents?</li>
</ul>

<h2 id="the-us-congress">The US Congress</h2>

<p>Recall that:</p>

<ul>
  <li>Instead of having all power delegate to a president (after years of tyranny under a king), framers, while recognizing the need for centralization in terms of a stronger national government with an elected executive wielding its own authority, those at the Constitutional Convention <strong>wanted a strong representative assembly at the national level</strong>: the congress</li>
  <li>Thus, Article I of the Constitution <strong>grants several key powers to Congress</strong>, which include overseeing the budget and all financial matters, introducing legislation, confirming or rejecting judicial and executive nominations, and even declaring war.</li>
</ul>

<blockquote>
  <p>Background:</p>

  <ol>
    <li><strong>Speaker of the House</strong>: The Speaker of the House is the presiding officer of the House of Representatives and is responsible for overseeing the House’s proceedings, setting the agenda, and managing its operations. The Speaker is also second in line to the <em>presidency</em>, after the Vice President. The <em>Speaker is elected by the members of the House</em> and is <strong>typically the leader of the majority party.</strong></li>
    <li><strong>Majority Leader</strong>: The Majority Leader is responsible for managing and scheduling the House’s legislative agenda. They work with the Speaker of the House to set the legislative agenda and help guide legislation through the House. The Majority Leader is typically the second-ranking member of the majority party in the House.</li>
    <li><strong>Minority Leader</strong>: The Minority Leader is responsible for representing the minority party in the House and works with the Majority Leader and Speaker of the House to negotiate and craft legislation. The Minority Leader is typically the leader of the minority party in the House.</li>
    <li><strong>Whip</strong>: The Whip is responsible for counting votes and ensuring that members of their party <strong>vote in accordance with the party’s position</strong>. They work closely with the Majority and Minority Leaders to build support for legislation and to ensure that their party’s position is reflected in the House’s votes.</li>
  </ol>

  <p>What’s special about the Congress system in the US?</p>

  <ul>
    <li>The bicameral system established at the Constitutional Convention and still followed today requires the two houses to pass <strong>identical bills</strong>, or proposed items of legislation. This is not easy, hence
      <ul>
        <li>reduce hasty decisions</li>
        <li>large-scale dramatic reform is exceptionally difficult to pass and that the status quo is more likely to win the day</li>
        <li>difficult for a single faction or interest group to enact laws and restrictions</li>
      </ul>
    </li>
    <li>The congress is a <em>bicameral</em> system
      <ul>
        <li>the <strong>Senate</strong> every state will have two senators who each serve a six-year term.</li>
        <li>the <strong>House of Representatives</strong> are distributed among the states based on each state’s population</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Certain <strong>powers of the congress</strong> include (granted by the Constitution)</p>

<ol>
  <li><strong>levy taxes</strong>: quite possibly the most important power Congress possesses, i.e. controls the money.</li>
  <li>setting budget and regulate interstate and international commerce</li>
  <li><strong>introduce legislation</strong></li>
  <li><strong>oversight</strong> of the actions of the president and the administration: the Senate’s final say on many presidential nominations and treaties signed by the president, and the House’s ability to impeach or formally accuse the president or other federal officials of wrongdoing</li>
  <li><strong>war declaration</strong>: they don’t run military, but they can decide whether if war can happen</li>
  <li>also have a “necessary and proper clause”</li>
</ol>

<p>How does <strong>Constitution limit Congress’ power</strong> (therefore incurs conformity costs)</p>

<ul>
  <li>need periodic (2y) <strong>elections</strong> = congressmen needs to respond to people’s needs</li>
  <li>separation of power
    <ul>
      <li>president being able to veto legislations</li>
      <li>house + senate</li>
    </ul>
  </li>
  <li>different elections
    <ul>
      <li>some people could be representing parts of NY, and certain the entire state of NY</li>
      <li>different goals</li>
    </ul>
  </li>
  <li>BoR limiting what congress can do</li>
</ul>

<p>The traditional process by which a bill becomes a law is called the <em>classic legislative process</em>. How does a bill get passed in the congress?</p>

<ol>
  <li>legislation must be drafted</li>
  <li>majority leadership consults with the parliamentarian about which committee to send it to.</li>
  <li>hold a hearing on the bill.
    <ul>
      <li>If the chair decides to not hold a hearing, this is tantamount to killing the bill in committee.</li>
    </ul>
  </li>
  <li>Once hearings have been completed, the bill enters the <strong>markup</strong> stage.
    <ul>
      <li>This is essentially an amending and voting process.</li>
      <li><em>Tabling a bill</em> typically means the bill is dead, but there is still an option to bring it back up for a vote again.</li>
      <li>If the committee decides to advance the bill, however, it is printed and goes to the chamber, either the House or the Senate.
        <ul>
          <li>House can debate and add amendments. Once the limits of debate and amendments have been reached, the House holds a vote. A majority (51%) is needed to pass</li>
          <li>in the Senate, the bill is placed on the calendar so it can be debated. Typically, senators allow each other to talk and debate as long as the speaker wants = the filibuster problem. To invoke <em>cloture</em> (prevent filibuster), the Senate had to get a two-thirds majority.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Post-fixing:</strong> recall that for a legislation to pass, both House and the Senate need to pass identical bills. Typically one of the two approach is taken:
    <ul>
      <li>the chamber to simply accept the bill that ultimately makes it out of the second chamber.</li>
      <li>first chamber to further amend the second chamber’s bill and send it back to the second chamber.</li>
    </ul>
  </li>
  <li>Pass to <strong>president for signature</strong>.
    <ul>
      <li>If the president does veto the bill, both chambers must muster a two-thirds vote to overcome the veto and make the bill law without presidential approval</li>
    </ul>
  </li>
</ol>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230224232743479.png" alt="image-20230224232743479" style="zoom:30%;" /></p>

<h2 id="congress-with-agency-model">Congress with Agency Model</h2>

<p>How do <strong>collective action problem</strong> limit Congress</p>

<ul>
  <li>need for information
    <ul>
      <li>free-rider: the acquisution of knowledge incurrs some costs, but once you know it everybody can take advantage of it</li>
    </ul>
  </li>
  <li>compromise/building coalitions
    <ul>
      <li>legislation need majorities to make decisions</li>
    </ul>
  </li>
  <li>need to <strong>decide</strong> what to prioritize, since workload is inhuman (coordination problem)</li>
  <li>getting members of congress to work towards the <strong>same goal</strong>. Arises since individual and collective goals could be different
    <ul>
      <li>individual goals: reelection/higher office; power in Washington. i.e. MC might be better of
        <ul>
          <li>doing casework = directly helping a person = getting support back</li>
          <li>focus on pet legislation = what you personally really cared about</li>
          <li>just spend a lot of time campaigning in the district</li>
          <li>not drafting legislation (which is technically the job as congress member)</li>
        </ul>
      </li>
      <li>collective goal: good public policy, maintaining the institution</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Collective action of the congress (e.g. drafting good polict) significantly <strong>undermined by individual interests</strong></p>
</blockquote>

<p>But this is not without hope:</p>

<ul>
  <li><strong>Specific members</strong> (i.e. a hierarchy) have special powers to <strong>induce coordination</strong> (solve coordinate action)
    <ul>
      <li><strong>legislative parties</strong>: Members of Congress are organized into political parties, and party leaders play a critical role in p<em>romoting coordination among members</em>.
        <ul>
          <li>Party leaders work to <strong>build consensus</strong> (balance individual interests) and ensure that their party’s positions are <strong>reflected in legislative outcomes</strong>.</li>
          <li>reduce the cost of information exchange and promote the exchange of information and ideas</li>
        </ul>
      </li>
      <li><strong>committee system</strong>: allows for members to work together to craft policy and can help to <strong>balance individual interests</strong> with the interests of the legislative body as a whole; can also seen as being <strong>delegated</strong> some power
        <ul>
          <li>so that this <em>smaller group of people</em> can do collective action stuff more easily, e.g. gathering information</li>
          <li>but the tradeoff is of course that there is still agency loss? How do we mitigate that?
            <ul>
              <li>select the right members (e.g. to control what kind of bills show up at the first place, e.g. giving leadership positions to loyal members)</li>
              <li>deploy reward and punishments (e.g. reward specific members things they are interested in)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>shift some of the benefits of individual goals/powers</strong> to speakers of the house/whips
    <ul>
      <li>Speaker of the House and the Majority Leader have the ability to decide which bills are brought to the floor for consideration, and they have the power to shape the legislative agenda. This allows them to prioritize certain issues and work to build consensus among members, rather than allowing individual members to dictate the agenda based solely on their own interests.</li>
      <li>the Whips do focus on getting votes for their party’s positions, they also work to ensure that party members are represented and that their interests are taken into account in the legislative process.</li>
      <li>MC give up some individual freedom for collective benefits</li>
      <li>but given speaker of house’s power, this means that:
        <ul>
          <li>[-] e.g. Gaetz v.s. McCarthy. Gaits said we are not voting for you unless you change the powers to limit yourself. Note that in republican there are extreme factions = members want restraints on the speaker of house.</li>
          <li>[+] e.g. can use reward/punishment = essentially their power to achieve what they want. Nacy Pelosi to get AOC (Alexandria Ocasio-Cortez) if you do what;
            <ul>
              <li>threats = cut you out on majority committee</li>
              <li>get another candidate to compete with you</li>
            </ul>
          </li>
          <li>in summary, leader use reward and penalty to motivate the party. But party can change what power leaders have correspondingly</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><mark>agency model as a solution:  delegation of power</mark> from the house, and intended to use them to solve the collection problem
    <ul>
      <li>Resources of delegated to Party Leaders
        <ul>
          <li><strong>scheduling (agenda control)</strong>: setting rules of <em>how to make decisions</em> can affect the final decision!
            <ul>
              <li>e.g. preference for three senators A&gt;B&gt;C v.s. B&gt;C&gt;A v.s. C&gt;A&gt;B</li>
              <li>then A v.s. B will have A win; A v.s. C will have C win = what actually gets passed</li>
              <li>hence, this could <em>also be misused</em> for bad purposes</li>
            </ul>
          </li>
          <li>can control campaign money = solve coordination problem by giving reward and threats = controlling priority and working towards the same goal
            <ul>
              <li>e.g. Pelosi denying party members of Campaign funds when they don’t do their job (in Pelosi’s eyes)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Resource for Party in Senate (some what weaker and much more limited)
        <ul>
          <li>no rules committee</li>
          <li>unlimited debate and amendments
            <ul>
              <li>less efficient = small member of senate can kill legislation</li>
            </ul>
          </li>
          <li>decide committee leadership by seniority</li>
        </ul>
      </li>
      <li>delegate work to a committee: as explained before, a committee with less people have a smaller collective action problem. But</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>In the collective action framework, those are strategies to get one policy/preference passed among many conflicting preferences.</p>
</blockquote>

<hr />

<blockquote>
  <p>In general, political change is hard</p>

  <ul>
    <li>federal system has created a lot of veto points, so people can maintain policies they liked</li>
    <li>entrenched political power extensive, used to preserve status quo (e.g. the conservatism in the bicameral Congress)</li>
  </ul>
</blockquote>

<p>Hard to change, but not impossible. It is important for you to understand <em>what it takes/you need to do</em> to change the world.</p>

<ul>
  <li>abortion now banned 12 states now</li>
  <li>slavery and Jim Crow segregation ended</li>
</ul>

<h2 id="typology-of-legislation">Typology of Legislation</h2>

<blockquote>
  <p>crucial to explain <em>what</em> legislation can be passed more easily in the congress. A lot of this depends on:</p>

  <ul>
    <li>who/how many is going to <em>come to you</em> and complain (e.g. people being unhappy)</li>
    <li>who/how many is going to is going to <em>come to you</em> and reward/support you (e.g. people being happy)</li>
  </ul>
</blockquote>

<p>To see why this happens, we can first understand how <strong>targeted a particular legislation will be</strong></p>

<ul>
  <li>e.g. if you take a <em>particular group</em> benefit away/give, people will notice. If everybody pays a little, is fine = diffuse cost</li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Concentrated Costs</th>
      <th>Diffuse Costs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Concentrated Benefits</td>
      <td>NA Free Trade Agreement<br />Cigarette tax to fund inner city education</td>
      <td>Farm subsidies (farmers get money for keeping land but funded by taxation)<br />Automotive subsidies</td>
    </tr>
    <tr>
      <td>Diffuse Benefits</td>
      <td>Environmental Protection<br />Eminent domain</td>
      <td>Military<br />Public education</td>
    </tr>
  </tbody>
</table>

<p>why the above:</p>

<ul>
  <li>NA Free Trade Agreement: any business can have the entire NA for trade; all those business that depends on the tariff needs to bear a huge costs</li>
  <li>Environmental Protection: business who pollutes need to bear the immense costs; benefit is for everyone in general = diffuse</li>
  <li>Eminent domain: government taking over some property (e.g. land) for a public project</li>
  <li>Military: the US entirely benefits form the strong military; paid by the broad tax based</li>
</ul>

<p>In general:</p>

<ul>
  <li>if costs/benefits are concentrated = leads to recipients <em>very likely</em> to provide feedback/show up</li>
  <li>if costs/benefits are diffuse = leads to recipients <em>unlikely</em> to notice</li>
</ul>

<blockquote>
  <p>Hence</p>

  <ul>
    <li><strong>MC biased toward</strong> well organized <strong>narrow interests</strong> = concentrated benefits and diffuse costs $\to$ benefited people will more likely show up to support the bill</li>
    <li><strong>MC will be bad passing</strong> at diffuse benefits and concentrated cost = not a lot of people show up to support it, but a lot showing up to condemn</li>
  </ul>
</blockquote>

<h1 id="the-presidency">The Presidency</h1>

<p>Some background</p>

<ul>
  <li>
    <p><strong>Election Process: primaries and electoral college</strong></p>

    <ul>
      <li>The rise of the <strong>presidential primary</strong> and <strong>caucus</strong> system as the main means by which presidential <mark>candidates</mark> are selected has had a number of anticipated and unanticipated consequences.
        <ul>
          <li>i.e. the political <strong>party system</strong>, + primaries, which are elections in which candidates vied for the support of state delegations to the party’s nominating convention.</li>
          <li><em>caucus</em> or large-scale gathering was made up of legislators in the Congress who met informally to decide on nominees from their respective parties.</li>
          <li>Hence, to win in primaries $\to$ candidates seek to align themselves with committed partisans,</li>
        </ul>
      </li>
      <li>then, the candidate who wins the popular vote in November receives <strong>all the state’s electoral votes</strong>.
        <ul>
          <li>the Electoral College consists of a body of 538 people called electors, each representing one of the fifty states or the District of Columbia.</li>
          <li>this system created certain irregularities: e.g. Donald Trump comfortably won the Electoral College by narrowly winning the popular vote in several states, while Hillary Clinton collected nearly 2.9 million more votes nationwide.</li>
          <li>Should no candidate receive a majority of the votes cast, the <em>House of Representatives</em> would select the president, with each state casting a single vote, while the <em>Senate</em> chose the vice president.</li>
        </ul>
      </li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230225003637956.png" alt="image-20230225003637956" style="zoom:33%;" /></p>
  </li>
  <li>
    <p><strong>Impeachment</strong>: removing the president over serious wrongdoing</p>

    <ol>
      <li>the House of Representatives could impeach the president by a simple <strong>majority</strong> vote.</li>
      <li>the Senate could remove the president from office by a <strong>two-thirds majority</strong></li>
    </ol>

    <p>e.g. the most recent impeachments were of President Donald Trump, who was impeached in the House twice. However, support for removal in the Senate did not meet the super-majority requirement,</p>
  </li>
  <li>
    <p><strong>Power of the president</strong>: a few note-worthy ones include</p>

    <ol>
      <li><strong>commander-in-chief</strong> of the armed forces of the United States
        <ul>
          <li>today, this has been expanded greatly, with presidents waging undeclared wars,</li>
        </ul>
      </li>
      <li>negotiate treaties with the advice and consent of the Senate, and receive representatives of foreign nations</li>
      <li><strong>nominating federal judges, including Supreme Court justices</strong>, as well as other federal officials, and making appointments to fill military and diplomatic posts.</li>
      <li><strong>veto</strong> legislation if necessary, although a two-thirds supermajority in both houses of Congress could override that veto;</li>
      <li>(later developed) <strong>executive privilege</strong>, the right to withhold information from Congress, the judiciary, or the public.</li>
    </ol>

    <p>The rather vague wording in Article II, which says that the “<em>executive power shall be vested</em>” in the president, has been subject to broad and sweeping interpretation in order to <strong>justify actions beyond those specifically enumerated</strong></p>
  </li>
</ul>

<p>In general, we now see a growth of presidential power attributable to the growth of the United States $\to$ rising profile of the United States on the international stage has meant that the president is a far more important figure as leader of the nation.</p>

<hr />

<p>After winning an election:</p>

<ul>
  <li>
    <p><strong>appointments</strong></p>

    <ul>
      <li>most importantly selection of a <strong>cabinet</strong>.
        <ul>
          <li>today in total there are 15 members</li>
          <li>inner cabinet—the heads of the Departments of Defense, Justice, State, and the Treasury (echoing Washington’s original cabinet)—receive the most attention from the president, the Congress, and the media.</li>
          <li>president nominate those positions, while the <strong>Senate confirms or rejects these nominations</strong>. Though most of the time it is confirm.</li>
        </ul>
      </li>
      <li>selection of <strong>the West Wing of the White House</strong> = president’s staff, and the name is where they worked
        <ul>
          <li>e.g. political advisers, speechwriters, and a press secretary to manage the politics and the message of the administration</li>
          <li><em>not</em> subject to Senate approval</li>
        </ul>
      </li>
      <li>selection of <strong>vice president</strong>: before, vice presidents were often sent on minor missions or used as mouthpieces for the administration. But now they are taking a much more active role + collaboration with the president.</li>
    </ul>
  </li>
  <li>
    <p><strong>forging an agenda</strong>: decide how to deliver upon what was promised during the campaign.</p>
    <ul>
      <li>most presidents do recognize that they must address their major initiatives during their <strong>first two years</strong> in office.</li>
      <li>the delivery of an <strong>inaugural address</strong>: set forth priorities within the overarching vision of what they intend to do.</li>
    </ul>
  </li>
</ul>

<hr />

<p>The public presidency: can you use media + IT to get more political popularity?</p>

<ul>
  <li>before, using radio to broadcast the president’s voice into many of the nation’s homes.</li>
  <li>now, <strong>television and the internet</strong>, but it remains a question <mark>whether choosing to go public actually enhances a president’s political position</mark> in battles with Congress.
    <ul>
      <li>[-] polarize political debate</li>
      <li>[-] increase public opposition to the president</li>
      <li>[-] complicate the chances to get something done.</li>
      <li>[+] rallying supporters</li>
    </ul>
  </li>
  <li>but also the <strong>first lady</strong>: help to gain public support and help their husbands
    <ul>
      <li>e.g. increasing public political role of the first lady continued in the 1980s with Nancy Reagan’s “Just Say No” antidrug campaign</li>
    </ul>
  </li>
</ul>

<hr />

<p>President’s action during office:</p>

<ul>
  <li><strong>direct action</strong>, it may break a policy deadlock or establish new grounds for action
    <ul>
      <li>exercises the power of <strong>pardon</strong> without conditions, i.e. fully forgiven for their offense, and their criminal record is effectively erased</li>
      <li><strong>line-item veto</strong> is a type of veto that keeps the majority of a spending bill unaltered but nullifies certain lines of spending within it.</li>
      <li><strong>signing statements</strong> are statements issued by a president when agreeing to legislation</li>
      <li><strong>executive orders</strong> or proclamations to achieve policy goals = direct government agencies to pursue a certain course, but are subject to court rulings or changes in policy enacted by Congress
        <ul>
          <li><mark>often used in cases of national security</mark>, e.g. aggressively deploy U.S. military force.</li>
          <li><strong>rally around the flag effect</strong>, in which presidential popularity spikes during international crises = during national emergencies and war, presidents need to act independently and vigorously</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>informal powers</strong> of <strong>persuasion</strong> and negotiation essential to working with the legislative branch $\to$ to secure policy achievements in cooperation with Congress.
    <ul>
      <li>e.g. when the president nominates and the Senate confirms persons to fill vacancies on the Supreme Court</li>
      <li>e.g. In times of divided government (when one party controls the presidency and the other controls one or both chambers of Congress), it is up to the president to cut deals and make compromises that will attract support from at least some members of the opposition party</li>
    </ul>
  </li>
</ul>

<h2 id="how-powerful-is-the-us-president">How Powerful is the US President</h2>

<blockquote>
  <p>19th century president had weak power, more like clerks than leaders</p>

  <ul>
    <li>limited role in domestic policy making</li>
    <li>limited capacity in federal agencies</li>
    <li>cabinet (parties) made most of the policy choices</li>
    <li>exceptions of this include
      <ul>
        <li>Lincoln: assumed unpredence tpower during civil <em>war</em></li>
        <li>Jackson - incredibly popular so can go against Congress and courts</li>
      </ul>
    </li>
  </ul>

  <p>20 century - instead of lead by congress, federal government lead by the <strong>executives</strong></p>

  <ul>
    <li>due to industrial revolution = a lot happened and <strong>Congress appointed</strong> those stuff to the <em>executives</em></li>
    <li>New Deal programs expanded purview of <em>executives</em></li>
    <li>Great Society continued</li>
    <li>eventually the Congressional monitoring resource did not keep pace with executives work, could not monitor anymore</li>
  </ul>
</blockquote>

<p><strong>Advantage/Disadvantage of extensive delegation of power to executives</strong> by the Congress</p>

<ul>
  <li>[+] specialization: possibly greater expertise acquisition</li>
  <li>[+] reduced a lot of workload congress</li>
  <li>[+] more flexible policy-making
    <ul>
      <li>faster to change = congress might want certain policy changes more <em>up-to-date</em> with economic, medical, advances in science situations today</li>
      <li>but of course in the Congress’s perspective, you are <em>giving up power</em> = delegation problem</li>
    </ul>
  </li>
  <li>[+] avoid contentious policy issues (a political benefit for the congress, deflect responsitbility)</li>
  <li>[-] policy control becomes harder (one solution they employed is to make <em>rules</em> on how those policy can be made)</li>
</ul>

<p>As a result, the <strong>president’s power today can hugely affect policy-making</strong></p>

<ul>
  <li><strong>veto</strong> power of president
    <ul>
      <li>even if president doesn’t veto = still huge impact because policies being written, hidden in mind, want to comply to president = but incredible difficult to measure its impact</li>
      <li><strong>veto bargining</strong></li>
    </ul>
  </li>
  <li><strong>persuasion</strong>: if the congress passed something the president doesn’t like, one choice is to appeal to/persuade the public and let the public put pressure on the congress! (it is often very easy for the president to gather media attention)</li>
  <li><strong>unilateral executive action</strong>: allow presidents to “make” policy outside of the regular lawmaking process, allowing them to <a href="https://doi.org/10.1111/j.1741-5705.2005.00258.x">move first and act alone</a> in policy-making
    <ul>
      <li>i.e. “if the congress is not doing it, I will do it”.</li>
    </ul>
  </li>
</ul>

<h2 id="veto-bargaining">Veto Bargaining</h2>

<p>What will happen to the <em>bill devised by the Congress leaders</em> (ignoring Senator for now), if they know it can be vetoed by president?</p>

<ul>
  <li>sometimes it is <strong>not about passing a policy</strong>, but to <strong>send message</strong>
    <ul>
      <li>e.g. objective was to signal to policy maker that they are committed to change the policy</li>
    </ul>
  </li>
  <li>ordinarily, presidential veto is taken into account <strong>when coming up with a policy</strong>.
    <ul>
      <li>
        <p><strong>spatial model:</strong> a single dimension of <code class="language-plaintext highlighter-rouge">&lt;----- P ------- C -----&gt;</code>. Let SQ be the status quo policy, and distance being how disliked it is</p>

        <ol>
          <li><code class="language-plaintext highlighter-rouge">&lt;--SQ-- P ------- C -----&gt;</code>. Congress can draft a policy $p$ <em>towards their ideal upto</em> here <code class="language-plaintext highlighter-rouge">&lt;--SQ-- P --p---- C -----&gt;</code> and president won’t veto.</li>
          <li><code class="language-plaintext highlighter-rouge">&lt;----- P ------- C --SQ--&gt;</code>. Congress can draft a policy <em>exactly at their ideal</em>, to <code class="language-plaintext highlighter-rouge">&lt;----- P ------- Cp --SQ--&gt;</code>, and the president will also pass it</li>
          <li><code class="language-plaintext highlighter-rouge">&lt;----- P ---SQ--- C -----&gt;</code>. Then status quo cannot be changed = congress <em>cannot</em> propose a new legislation different from this = <mark>gridlock</mark>. (i.e. <strong>P and C want to move the policy in opposite direction</strong>). For example,
            <ul>
              <li>immigration reform = Republican want to make immigrant less pleasant, and democrats want to improve</li>
              <li>Environmental policy = Biden want to increase control on env, but Congress want to decrease environmental monitor;</li>
              <li>Abortion</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>what can politicians do if it is in gridlock?
        <ul>
          <li>omnibus = pack something the president like into the policy, but also something you like so that president more likely pass it</li>
          <li>C: best option is to pass/think about other policies, or if they can, change what P wants</li>
          <li>P: can perform <mark>unilateral executive action</mark> = change policy on their own, and wait for congress to stop them. Why <mark>executives are so powerful today</mark></li>
        </ul>
      </li>
      <li>from the spatial model
        <ul>
          <li><strong>if $P$ and $C$ are aligned</strong>, the interval of opposite pull will be smaller = pass policy easier as they have the same interest</li>
          <li><strong>greater polarization</strong>: the gridlock interval becomes wider = president party very very different from the Congress = harder to make policy</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>but of course, several nuances in the spatial model</p>

<ul>
  <li>
    <p>Congress can overrule veto with super-majority = then this model doesn’t work = need to also model senators</p>
  </li>
  <li>
    <p><strong>First-Mover Advantage</strong>: whoever passes something first will <em>force the other to either veto it or pass it</em>. Consider the three scenarios above</p>

    <ul>
      <li>Congress moves first (because they are legislative anyway):
        <ul>
          <li>case 1) [+] congress can draft a policy $p$ <em>towards their ideal upto</em> here <code class="language-plaintext highlighter-rouge">&lt;--SQ-- P --p---- C -----&gt;</code></li>
          <li>case 2) [+] can draft <em>exactly at their ideal</em>, to <code class="language-plaintext highlighter-rouge">&lt;----- P ------- Cp --SQ--&gt;</code></li>
          <li>case 3) no gain no loss</li>
        </ul>
      </li>
      <li>President move first by passing <mark>executive order</mark>, so Congress is forced to either veto or pass it
        <ul>
          <li>case 1) [+] president can move directly at his/her ideal</li>
          <li>case 2) [+] can move to <code class="language-plaintext highlighter-rouge">&lt;----- P ----p-- C --SQ--&gt;</code> hence becomes closer to the president than before</li>
          <li>case 3) no gain no loss</li>
        </ul>
      </li>
    </ul>

    <p><mark>in all cases, whoever moves first gains advantage!</mark> An example IRL is Biden’s 400billion student loan relief program passed using executive order. Now congress/USSC is unhappy and checking it.</p>
  </li>
</ul>

<p>Therefore: agreements will be much harder to reach, but it depends the above strategies</p>

<h2 id="unilateral-executive-action">Unilateral Executive Action</h2>

<blockquote>
  <p><strong>Executive orders</strong>: direct/ask agencies in how they should carry out their job but technically cannot touch on law-making</p>
</blockquote>

<p>Example: Case of DACA: how executive orders can be used to “change” policy</p>

<ul>
  <li>DACA stands for Deferred Action for Childhood Arrivals. It is a program that offers work permits and deportation relief to more than 640,000 undocumented immigrants <strong>brought to the U.S. when they are children</strong></li>
  <li>One memo signed by President Biden <strong>ordered</strong> the Departments of Homeland Security to safeguard the Obama-era DACA program</li>
</ul>

<p>Legislative orders:</p>

<ul>
  <li>immigration reform has been an issue = federal gov failed to pass any significant policy in immigration</li>
  <li>2012- Obama admin issued rule on discretoin over deportatoin</li>
  <li>USSC said Trump had this power but the way he uses it is wrong</li>
</ul>

<blockquote>
  <p>Conclusion: president is powerful, but</p>

  <ul>
    <li>limited on political circumstances.</li>
    <li>mismatch in expectations bending the laws = innovation</li>
  </ul>
</blockquote>

<h1 id="the-courts">The Courts</h1>

<p>Some backgrounds:</p>

<ul>
  <li>along with part of the <strong>check and balance system</strong>, the judiciary is arguably the branch where the <strong>individual has the best chance to be heard</strong>. For the most part, the Supreme Court is an appeals court, operating under <strong>appellate jurisdiction</strong> (i.e. hearing appeals from the lower courts)</li>
  <li>
    <p>the power of <strong>judicial review</strong>: as part of the system of checks and balances, to look at actions taken by the other branches of government and the states and determine whether they are constitutional.</p>
  </li>
  <li>
    <p>before, the basic structure of the judicial branch was:</p>

    <ul>
      <li>At the lowest level are the <strong>district courts</strong>, where federal cases are tried, witnesses testify, and evidence and arguments are presented.</li>
      <li>A losing party who is unhappy with a district court decision may appeal to the circuit courts, or <strong>U.S. courts of appeals</strong>, where the decision of the lower court is reviewed.</li>
      <li>Still further, appeal to the U.S. Supreme Court is possible, but of the thousands of petitions for appeal, the Supreme Court will typically hear fewer than one hundred a year.</li>
    </ul>
  </li>
  <li>
    <p>the judiciary today continues as a <strong>dual court system</strong></p>

    <ul>
      <li>
        <p>with courts at <mark>both the national and state levels</mark>.</p>
      </li>
      <li>
        <p>Both levels have three basic tiers consisting of <strong>trial court</strong>, appellate court, and finally courts of last resort, typically called supreme courts</p>

        <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230225023717252.png" alt="image-20230225023717252" style="zoom:22%;" /></p>

        <p>in case of federal courts:</p>

        <ul>
          <li>there are ninety-four U.S. <strong>district courts</strong> in the fifty states and U.S. territories, of which eighty-nine are in the states (at least one in each state).</li>
          <li>On the U.S. Supreme Court, there are nine justices—one chief justice and eight associate justices. Circuit courts each contain three justices, whereas federal district courts have just one judge each.</li>
          <li>court system operates on the principle of <strong><em>stare decisis</em></strong> (Latin for <em>stand by things decided</em>), which means that today’s decisions are based largely on rulings from the past = if legal facts of today’s court is the same as before, then it should be treated the same way. <mark>court interpretations can change as times and circumstances change</mark>, hence changes is still possible.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Courts and Public Policy</p>

<ul>
  <li>United States has a <strong>common law</strong> system. With code law in place, as it is in many nations of the world, it is the job of judges to simply apply the law. Under common law, as in the United States, they <em>interpret</em> it</li>
  <li><strong>last resort</strong> for individuals or a group believing there as been a wrong
    <ul>
      <li>e.g. employment <strong>discrimination</strong> based on their religious attire,</li>
      <li>e.g. put limits on the ability to impose the death penalty, ruling, for example, that the government may not execute a person with cognitive disabilities</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Courts and Federalism</strong></p>

<ul>
  <li>
    <p>State courts really are the core of the <strong>U.S. judicial system</strong>, and they are responsible for a huge area of law.</p>

    <ul>
      <li>Although the Supreme Court tends to draw the most public attention, it typically hears fewer than one hundred cases every year.</li>
      <li>The federal courts, on the other hand, will hear any case that involves a foreign government, patent or copyright infringement.</li>
    </ul>

    <table>
      <thead>
        <tr>
          <th style="text-align: left">State Courts</th>
          <th style="text-align: left">Federal Courts</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: left">Hear most day-to-day cases, covering 90 percent of all cases</td>
          <td style="text-align: left">Hear cases that involve a “federal question,” involving the Constitution, federal laws or treaties, or a “federal party” in which the U.S. government is a party to the case</td>
        </tr>
        <tr>
          <td style="text-align: left">Hear both civil and criminal matters</td>
          <td style="text-align: left">Hear both civil and criminal matters, although many criminal cases involving federal law are tried in state courts</td>
        </tr>
        <tr>
          <td style="text-align: left">Help the states retain their own sovereignty in judicial matters over their state laws, distinct from the national government</td>
          <td style="text-align: left">Hear cases that involve “interstate” matters, “diversity of citizenship” involving parties of two different states, or between a U.S. citizen and a citizen of another nation (and with a damage claim of at least $75,000)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>this federal design has both pluses and minuses</p>
    <ul>
      <li>[+] more than just one court system ready to protect that</li>
      <li>[-]  judicial rulings about what is legal or illegal may differ from state to state $\leftarrow$ state law that governs the authority of state courts
        <ul>
          <li>different courts in which a person could face charges for a crime or for a violation of another person’s rights.</li>
          <li>Just as the laws <strong>vary</strong> across the states, so do <strong>judicial rulings and interpretations</strong>, and the judges who make them.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>The federal court system</strong>: the basics has been mostly covered before with the three level. Here we discuss the <strong>selection of judges</strong></p>

<ul>
  <li>
    <p>At the federal level, the <strong>president nominates a candidate</strong> to a judgeship or justice position, and the <strong>nominee must be confirmed</strong> by a majority vote in the U.S. Senate</p>

    <ul>
      <li>through such <strong>senatorial courtesy</strong>, senators exert considerable influence on the selection of judges in their state,</li>
    </ul>
  </li>
  <li>
    <p>All judges and justices in the <em>national courts</em> (from US District to USSC) serve <strong>lifetime</strong> terms of office. This is to provide the judicial branch with enough independence such that it could not easily be influenced by the political winds of the time.</p>
  </li>
  <li>
    <p>What was once a predominately White, male, Protestant institution is today much more diverse:</p>

    <table>
      <thead>
        <tr>
          <th>First Catholic</th>
          <th>Roger B. Taney (nominated in 1836)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>First Jew</td>
          <td>Louis J. Brandeis (1916)</td>
        </tr>
        <tr>
          <td>First (and only) former U.S. President</td>
          <td>William Howard Taft (1921)</td>
        </tr>
        <tr>
          <td>First African American</td>
          <td>Thurgood Marshall (1967)</td>
        </tr>
        <tr>
          <td>First Woman</td>
          <td>Sandra Day O’Connor (1981)</td>
        </tr>
        <tr>
          <td>First Hispanic American</td>
          <td>Sonia Sotomayor (2009)</td>
        </tr>
      </tbody>
    </table>

    <p>However, the number of women and people of color on the courts <strong>still lags behind the overall number of White men</strong>.</p>

    <ul>
      <li>As of 2021, the federal judiciary consists of 67 percent men and 33 percent women.</li>
      <li>In terms of race and ethnicity, 74 percent of federal judges are White, 12 percent African American, 8 percent Latinx, and 4 percent Asian American.</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>The US Supreme Court</strong></p>

<ul>
  <li>structure
    <ul>
      <li>There is one <strong>chief justice</strong>, who is the lead or highest-ranking judge on the Court, and eight <strong>associate justices</strong></li>
      <li>Each justice has three or four <strong>law clerks</strong>: law clerks’ work and recommendations influence whether the justices will choose to hear a case</li>
    </ul>
  </li>
  <li>selecting cases
    <ul>
      <li>Case names, written in italics, list the name of a petitioner versus a respondent, as in <em>Roe v. Wade</em>, for example. Since the petitioner is the one bringing up the case and the party unhappy with the decision of the lower court will be the one $\to$ can tell which party lost at the lower level of court (e.g. Roe, for example)</li>
      <li>accepts <strong>fewer than 2 percent</strong> of the as many as ten thousand cases it is asked to review every year.
        <ul>
          <li>most often it is <strong>writ of <em>certiorari</em></strong>, a request that the lower court send up its record of the case for review. Then four of the nine justices must vote to accept a case. This is called the <strong>Rule of Four</strong>.</li>
          <li><strong>solicitor general</strong> is the lawyer who represents the federal government before the Supreme Court: He or she decides which cases (<strong>in which the United States is a party</strong>) should be appealed from the lower courts and personally approves each one presented. Most of the cases the solicitor general brings to the Court will be given a place on the docket.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>supreme court procedures
    <ul>
      <li>Once a case has been placed on the docket, <strong>briefs</strong>, or short arguments explaining each party’s view of the case, must be submitted
        <ul>
          <li>first by the petitioner and respondent</li>
          <li>other non-involving groups may also file may file an <strong>*amicus curiae*</strong> (“friend of the court”) brief giving their opinion, analysis, and recommendations about how the Court should rule.</li>
        </ul>
      </li>
      <li>After brief filed, USSC hears <strong>oral arguments</strong> in cases from October through April. The proceedings are quite ceremonial.
        <ul>
          <li>each side’s lawyers have thirty minutes to make their legal case, though the justices often interrupt the presentations with questions.</li>
          <li>decide the case in a (private) conference by discussions and then voting</li>
          <li>Oral arguments are open to the public.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Judicial opinions
    <ul>
      <li>along with the decision, there will be a published <strong>majority opinion</strong> = explanation of the majority vote, and a <strong>dissenting opinion</strong>, being written by the people assigned by most senior justice in the dissenting group</li>
    </ul>
  </li>
  <li>Influencing the decision
    <ul>
      <li>courts must rule on its facts. Although the courts’ role is interpretive, judges and justices are still constrained by the facts of the case, the Constitution, the relevant laws, and the courts’ own precedent.</li>
      <li>but in reality <strong>many factors play a role in its decision-making</strong>
        <ul>
          <li>including law clerks, the solicitor general, interest groups, and the mass media. But additional legal, personal, ideological, and political influences weigh on the Supreme Court and its decision-making process.</li>
          <li>judges personal beliefs and political attitudes also matter</li>
          <li>affected by another “court”—the court of public opinion, e.g. swayed by special-interest pressure, the mass media, etc.</li>
        </ul>
      </li>
      <li>Both the <strong>executive and legislative branches check and balance</strong> the judiciary in many different ways.
        <ul>
          <li>President appoint nominees, Congress retains the power to modify the federal court structure and its appellate jurisdiction, and the Senate may accept or reject presidential nominees to the federal courts.</li>
          <li>court rulings matter only to the extent they are heeded and followed = <strong>judicial implementation</strong>. While it is true that courts play a major role in policymaking, they have <em>no mechanism to make their rulings a reality</em> = <mark>relies on the executive to implement</mark> or enforce its decisions.</li>
          <li>in general both the president and other branches <em>tend to provide support</em> rather than opposition $\to$ what becomes of court decisions is largely due to their credibility, their viability, and the assistance given by the other branches of government.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Therefore, <strong>US legal system is federalized</strong> with the legislatures:</p>

<ul>
  <li>50 state court systems and 50 state constitutions</li>
  <li>federal courts can declare <strong>state laws invalid</strong> if they conflict with the federal constitution (supremacy clause)
    <ul>
      <li>e.g. abortion is explicitly protected in some states while not in the constitution = allowed under this framework</li>
    </ul>
  </li>
</ul>

<h2 id="judiciary-court-and-the-agency-problem">Judiciary Court and the Agency Problem</h2>

<p>What’s the aim of federal court?</p>

<ul>
  <li><strong>resolving disputes</strong>: often <em>state</em> and <em>federal</em> governments disagree, or <em>congress</em> and <em>president</em> disagree
    <ul>
      <li>e.g. Trump admin threatening California air quality standards (i.e. your cars need to meet a particular efficiency threshold, and they set their own standards). California wanted to raise the standard due to the large amount of cars there, but Trump disagrees.</li>
      <li>e.g. ACA medicaid expansion</li>
      <li>e.g. Trump v. Hawaii</li>
    </ul>
  </li>
  <li><strong>judicial review</strong> = declare laws invalid/enforce state agencies to do stuff = <mark>most important for political science</mark></li>
</ul>

<p>Structure: USSC is really the court of <strong>final appeal</strong></p>

<ul>
  <li>justices vote whether or not to accept a case, usually important ones such as from solicitors (represent US gov)</li>
  <li>judgments from USSC set precedent for the entire country = so laws are <strong>even across states</strong></li>
</ul>

<p>Agency problem and USSC:</p>

<ul>
  <li>government chooses USSC members, hence delegation = <strong>judges act as agents of Congress and President</strong>
    <ul>
      <li>to help implement decisions</li>
      <li>to help adjudicate bureaucracti obedience</li>
      <li>but of course, the aim of USSC is to reduce agency loss (e.g. congress has 800+ people)</li>
    </ul>
  </li>
  <li>Regular criticism: life appointment inconsistent with democratic policy control</li>
  <li>Regular defense: protect the rights to unpopular people/minority (i.e. even against popular idea, you will not be removed = less impetus to conform simply to majority)</li>
</ul>

<p><strong>How can the agency loss be <em>limited</em></strong>?</p>

<ul>
  <li>careful selection: president appoints judges and senate confirms</li>
  <li>reasonably <strong>controlled/monitored</strong> by the congress and other branches:
    <ul>
      <li>congress sets structure of fedearl courts</li>
      <li>if congress is unhappy with it, congress can change laws, e.g. start the process of amending the constitution</li>
    </ul>
  </li>
</ul>

<p>What factors <strong>motivate USSC’s decision</strong> in a case?</p>

<ol>
  <li>follow the <strong>rules/constitution/federal statutes</strong>, which can be vague sometimes, and sometimes <em>contradictory</em>/inconsistent with others
    <ul>
      <li>how do they interpret those complex laws? (a) Intent of <em>authors</em> (b) purpose of the <em>law</em> (c) the plain meaning of the text</li>
    </ul>
  </li>
  <li>follow their <strong>own ideology</strong> = a preconceived notion of what is good or bad
    <ul>
      <li>this can be a big influencer, the cases that <em>reached USSC are often hard to decide by just following the rules</em></li>
    </ul>
  </li>
  <li>care about the <strong>legitimacy</strong> of the institution itself = want it to be revered and respected</li>
</ol>

<p><strong>Why might USSC still care about public opinion/maintaining legitmacy?</strong></p>

<ul>
  <li>they have no enforcement power, so what happens in reality depends on if policy enforcers will do it (e.g. only if it is popular)</li>
  <li>themselves suffer from delegation problem, e.g. they <mark>cannot control the president to actually perform those decisions</mark></li>
</ul>

<h2 id="how-ussc-is-affected-by-public-opinion">How USSC is affected by Public Opinion</h2>

<blockquote>
  <p><strong>Key idea</strong>: supreme court cares about their public approval, even though they will never be thrown out = wants to maintain legitimacy in the <strong>eyes of the public</strong></p>
</blockquote>

<p>But if an external event/some social forces affects both the public opinion and the USSC in a same way, it will look like public opinion affecting USSC but in reality this is a <strong>spurious correlation</strong>.</p>

<p>In the paper, they show that</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230306220636520.png" alt="image-20230306220636520" style="zoom: 33%;" /></p>

<ul>
  <li>e.g. positive correlation of “Public Mood” is $1.59^*\pm0.78$ being statistically significant</li>
  <li>therefore, changing public mood and court ideology drive USSC decision making</li>
</ul>

<p>in the first section, they concluded that USSC will follow public opinions when:</p>

<ul>
  <li><strong>high profile decisions</strong> mattered the most, so justices care the most = had to stick to their personal ideologies/principles = most <strong><em>costly</em></strong> for the justice to just go with public opinion</li>
  <li>so for <strong>low profile decisions</strong> = USSC care more about public opinion</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Table 2</th>
      <th style="text-align: center">Figure 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230306220736599.png" alt="image-20230306220736599" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230306220711393.png" alt="image-20230306220711393" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>so that in this table:</p>

<ul>
  <li>following “public mood” is <strong>correlated with non-salient court cases</strong>, but no clear correlation with salient court cases</li>
</ul>

<blockquote>
  <p><strong>Conclusion</strong>: good evidence that USSC sometimes responds to public opinions = justices are balancing between public opinion and their legitimacy</p>
</blockquote>

<h2 id="democratic-trade-offs">Democratic Trade-offs</h2>

<p>To some extent, this is also related to the agency problem/limiting agency loss:</p>

<blockquote>
  <p>USSC <strong><em>can</em> prevent</strong> majority actions by elected branches = in some sense an anti-democratic institution. But, in those cases, many USSC blocks can be overcome through other means:</p>

  <ul>
    <li>US Constitution can be amended</li>
    <li>over time, USSC justices can be replaced</li>
  </ul>
</blockquote>

<p>What are the mechanisms that federal courts can use to <strong>affect policy making</strong></p>

<ol>
  <li>restrain federal/state action, e.g. your action violates some rights of people = <strong>blocks</strong> majoritarian policy making</li>
  <li>remove/change the set of restraints on federal/state actions, e.g. Dobbs v. Jackson on Women’s health = <strong>changes venue</strong> for majoritarian policy making</li>
  <li>change the rules of electoral competition = <strong>changes the means</strong> for majoritarian policy making
    <ul>
      <li>e.g. USSC have expanded the set of question they adjudicate = <strong>can determine the rules on how parties compete in redistricting</strong> = can affect the policy that comes out!
        <ul>
          <li>previously, the court cannot tampered with political actions, and the constitution does not specify this power as well. Now, USSC can review redistricting</li>
          <li>(Wesberry v. Sanders) USSC brings in 14th amendment ad requires US <em>house</em> districts on basis of population</li>
          <li>(Reynolds v. Sims) 14th Amendment requires <em>state</em> legislative districts on the basis of population = cannot have a city has only 1 senate for million, and a rural area 1:1000.</li>
        </ul>
      </li>
      <li>the above example results in massive shift in the balance of power between city and rural areas = came out from the USSC decisions even though they did not explicit tamper with the redistricting process itself</li>
    </ul>
  </li>
</ol>

<p>Note that:</p>

<ul>
  <li>it is often difficult for the USSC to entirely prevent a societal goal from happening (e.g. segregation), but to only <strong>block/remove a particular mechanism</strong> from achieving a goal:
    <ul>
      <li>e.g. recall the segregation fight between people (education segregation, white primaries, racial covenant, etc) and the courts</li>
      <li>i.e. difficult to fix all bugs all at once, but a few at a time when it pops up</li>
    </ul>
  </li>
  <li>if USSC had been weaker = would not block majoritarian policy making. e.g. DACA would be gone
    <ul>
      <li>recall that DACA is an administrative relief that protects eligible immigrants who came to the United States when they were children from deportation. DACA gives undocumented immigrants: 1) protection from deportation, and 2) a work permit</li>
      <li>DACA is created by the executive branch/Obama, but Trump admins want to revoke = <em>USSC decided</em> that is inconsistent with administrative rule, hence kept it in place</li>
    </ul>
  </li>
</ul>

<p><strong>A democratic bargain</strong> = whatever you do to the opposition when in power, that party can do the same to you when you step our of power</p>

<ul>
  <li>this alternation of power mechanism therefore act as a deterrent = a threat to tyranny</li>
  <li>but works if that party can wield the power when in place</li>
  <li>therefore, this alternation of power can protect <em>powerful</em> groups</li>
</ul>

<blockquote>
  <p><strong>Conclusion</strong>: courts are part of the rules of the game = e.g. decide how policies are made. Solve certain problems but also introduces others</p>
</blockquote>

<h1 id="the-bureaucracy">The Bureaucracy</h1>

<p>the many arms of the federal <strong>bureaucracy</strong>, often considered the fourth branch of government, are valuable components of the federal system.</p>

<ul>
  <li>elevated certain types of <mark>nonelected workers</mark> (i.e. hired) are elevated to positions of relative <strong>power</strong> within the governmental structure.</li>
  <li>Collectively, these essential workers are called the bureaucracy.</li>
  <li>examples in include: The Interstate Commerce Commission, Federal Reserve Board, etc</li>
</ul>

<blockquote>
  <p>A <strong>bureaucracy</strong> is an administrative group of nonelected officials charged with carrying out functions connected to a series of policies and programs.</p>

  <ul>
    <li>fundamentally, implementing the laws (e.g.) passed by Congress. Also include <strong>writing the more specifics laws, enforcing them,</strong> etc.</li>
    <li>public administration: the implementation of <strong>public policy</strong> as well as the academic study that prepares civil servants to work in government</li>
    <li>but, their roles are <mark>barely mentioned in the Constitution</mark>, despite being seen as “the <mark>fourth branch</mark> of the government”</li>
  </ul>
</blockquote>

<p><em>The origin of US Bureaucracy</em></p>

<ul>
  <li>For example, Article II, Section 2, provides the president the power to appoint officers and department heads. In the following section, the president is further empowered to see that the laws are “faithfully executed.” $\to$ Granting the president and Congress such responsibilities appears to anticipate a bureaucracy of some size, yet the design of the bureaucracy is <mark>not described</mark>.</li>
  <li>first developed in 1820s with the rise of <strong>centralized party politics</strong> is the <strong>spoils system</strong>: political appointments were transformed into political patronage = appointing federal posts as rewards for supporters swelled over the following decades.</li>
  <li>next, there is <strong>industrial revolution</strong> that increases the economic size of the US and brought people together with railroads and telegraphs</li>
  <li>President Woodrow Wilson believes that administrative activities should be <mark>devoid of political manipulations</mark> (while politics does set tasks for administration, public administration should be built on a science of management)</li>
</ul>

<p><em>Fall of political patronage</em></p>

<ul>
  <li>supporting the patronage system held that their positions were well earned; those who condemned it argued that federal legislation was needed to ensure jobs were awarded on the <strong>basis of merit</strong>.</li>
  <li><strong>Civil Service Reform Act</strong> of 1883 (<mark>Pendleton</mark> Act). The act established the Civil Service Commission, a centralized agency charged with <mark>ensuring that the federal government’s selection</mark>, retention, and promotion practices were based on open, competitive examinations in a <mark>merit system</mark></li>
</ul>

<p><em>The Bureaucracy comes of Age</em></p>

<ul>
  <li>With the onset of the Great Depression in 1929, President Roosevelt and U.S. Congress rapidly reorganized the government’s problem-solving efforts into a series of programs designed to revive the economy $\to$ In the 1930s, the <strong>federal bureaucracy grew</strong>, and by 1940, approximately 700,000 U.S. workers were employed in the federal bureaucracy.</li>
</ul>

<p><em>The Civil Service Commission</em></p>

<ul>
  <li>the Pendleton act had important consequences due to
    <ol>
      <li>the law attempted to reduce the impact of politics on the civil service sector by making it <strong>illegal to fire</strong> or otherwise punish government workers <strong>for strictly political reasons</strong>.</li>
      <li>raised the qualifications for employment in civil service positions by <strong>requiring applicants to pass exams</strong> designed to test their competence in a number of important skill and knowledge areas.</li>
      <li>allowed for the creation of the United States <strong>Civil Service Commission (CSC)</strong>, which was charged with enforcing the elements of the law.</li>
    </ol>
  </li>
  <li>Congress and the president responded (to prior skepticism on bureaucracy) with the <mark>Civil Service Reform Act of 1978</mark>, which abolished the Civil Service Commission. In its place, the law created two new federal agencies:
    <ul>
      <li><strong>Office of Personnel Management (OPM)</strong> has responsibility for recruiting, interviewing, and testing potential government employees in order to choose those who should be hired.</li>
      <li><strong>Merit Systems Protection Board (MSPB)</strong>, responsible for investigating charges of agency wrongdoing and hearing appeals when corrective actions are ordered.</li>
    </ul>
  </li>
</ul>

<p><em>Merit-Based Selection</em></p>

<ul>
  <li>
    <p>In this system, the large majority of jobs in individual bureaucracies are <strong>tied to the needs of the organization</strong> rather than to the political needs of the party bosses</p>
  </li>
  <li>
    <p>Before, a civil service exam is required, stipulated by the Pendleton Act. This mandatory testing has since been <strong>abandoned</strong>, and now approximately <strong>eighty-five percent of all federal government jobs are filled through an examination</strong> of the applicant’s education, background, knowledge, skills, and abilities. (amongst them a few still requires testing)</p>
  </li>
  <li>
    <p>Civil servants receive pay based on the U.S. Federal General Schedule.</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230319022934917.png" alt="image-20230319022934917" style="zoom: 25%;" /></p>

    <p>this system is to create an environment in which those most likely to succeed are in fact those who are ultimately appointed. However</p>

    <ul>
      <li>[+] naturally result in organizations composed of experts who dedicate their lives to their work and their agency.</li>
      <li>[-] permanent employees can become too independent of the elected leaders. While a degree of separation is intentional and desired, too much can result in <strong>bureaucracies that are insufficiently responsive to political change</strong>.</li>
      <li>[-] accepted expertise of individual bureaucrats can sometimes <strong>hide their own chauvinistic impulses</strong>.</li>
    </ul>
  </li>
</ul>

<p><em>Models of Bureaucracy</em></p>

<ul>
  <li>The patronage system tied the livelihoods of civil service workers to their party loyalty and discipline. Without the patronage network (spoils system), <strong>bureaucracies form their own motivations</strong>.</li>
  <li>models for understanding how bureaucracy works
    <ul>
      <li><strong>Weberian Model</strong>:
        <ul>
          <li>ideal type of bureaucracy, the Weberian model, was one in which <strong>agencies are apolitical, hierarchically organized, and governed by formal procedures</strong>.</li>
          <li>specialized bureaucrats would be better able to solve problems through <strong>logical reasoning</strong></li>
          <li>as a result, it would impose order and efficiency, create a clear understanding of the service provided, reduce arbitrariness, ensure accountability, and limit discretion.</li>
        </ul>
      </li>
      <li><strong>Acquisitive Model</strong>: bureaucracy compete for limited resources
        <ul>
          <li>bureaucracies are naturally competitive and power-hungry $\to$ they recognize that limited resources are available to feed bureaucracies, so they will work to <strong>enhance the status of their own bureaucracy to the detriment of others</strong>.</li>
          <li>e.g. attempt to emphasize their work to Congress and maximize its budget by depleting all its allotted resources each year</li>
          <li>In this way, the bureaucracy will eventually grow far beyond what is necessary and <strong>create bureaucratic waste</strong></li>
        </ul>
      </li>
      <li><strong>Monopolistic Model</strong>: <em>absence</em> of competition in bureaucracy
        <ul>
          <li>recognize the similarities between a bureaucracy like the Internal Revenue Service (IRS) and a private monopoly like a regional power company</li>
          <li>there are rare bureaucratic exceptions that typically compete for presidential favor, most notably organizations such as the CIA, the National Security Agency, and the intelligence agencies in the Department of Defense. Apart from these, bureaucracies have <strong>little reason to become more efficient or responsive</strong> due to absence of competition.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><em>Types of Bureaucractic Organizations</em></p>

<ul>
  <li>
    <p>In the U.S. government, there are four general types: <strong>cabinet departments</strong>, <strong>independent executive agencies</strong>, <strong>regulatory agencies</strong>, and <strong>government corporations</strong>.</p>
  </li>
  <li>
    <p><strong>Cabinet departments</strong> are major executive offices that are directly accountable to the president.</p>

    <ul>
      <li>
        <p>they include the Departments of State, Defense, Education, Treasury, and several others = <strong>headed by a single person nominated by the pres</strong>.</p>
      </li>
      <li>
        <p>usually have many smaller agencies within them, e.g. FBI within DoJ.</p>
      </li>
      <li>
        <p>individual cabinet departments are composed of numerous levels of bureaucracy. These levels descend from the department head in a mostly <strong>hierarchical</strong> pattern</p>

        <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230319143221201.png" alt="image-20230319143221201" style="zoom: 50%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Independent Executive Agencies and Regulatory Agencies</strong></p>

    <ul>
      <li>Like cabinet departments, independent executive agencies report directly to the president.</li>
      <li><strong>Unlike</strong> cabinet, independent executive agencies:
        <ul>
          <li>they are assigned far <strong>more focused tasks</strong>.</li>
          <li>These agencies are considered independent because they are <strong>not subject to the regulatory authority</strong> of any specific department.</li>
          <li>located outside of cabinet departments (obv.)</li>
        </ul>
      </li>
      <li>examples of <em>Independent Executive Agency</em> include Central Intelligence Agency (CIA), National Aeronautics and Space Administration (<strong>NASA</strong>), and Environmental Protection Agency (EPA)</li>
      <li><strong>Independent Regulatory commissions</strong>
        <ul>
          <li>to grant control and limit agency loss = Congress writes a law, these are the people who can oppose it even if popular
            <ul>
              <li>e.g. a federal reserve system = people insulated from popular anger; in the long term people will be better-off</li>
            </ul>
          </li>
          <li>independent from President as well</li>
        </ul>
      </li>
      <li>examples of <em>Regulatory Agency</em> include Interstate Commerce Commission (ICC), charged with regulating that most identifiable and prominent symbol of nineteenth-century industrialism, the railroad; Securities and Exchange Commission (SEC) expanded significantly in the digital era beyond mere regulation of stock floor trading</li>
    </ul>
  </li>
  <li>
    <p><strong>Government Corporations</strong></p>

    <ul>
      <li><strong>subject to market forces</strong> and tend to generate enough profit to be self-sustaining, but they also <strong>fulfill a vital service the government has</strong> an interest in maintaining.</li>
      <li>Unlike a private corporation, a government corporation does not have stockholders. Instead, it has a board of directors and managers; they are also exempted from taxes</li>
      <li>examples: most widely used government corporation is the <strong>U.S. Postal Service (USPS)</strong>. Another widely used government corporation is the National Railroad Passenger Corporation, which uses the trade name <strong>Amtrak</strong></li>
    </ul>
  </li>
</ul>

<p><em>Bureaucracy Rule-Making</em>: the processes of <strong>rulemaking and bureaucratic oversight are equally complex</strong> (since bureaucracy is complex)</p>

<ul>
  <li>before, <strong>notice-and-comment rulemaking</strong>: bureaucracy publicize its proposals, and allow for the public to comment.
    <ul>
      <li>if you want to deletage a task to an agent, what would you do? = <mark>monitoring</mark>: notice and comment rule-making</li>
      <li>e.g. <strong>FDA</strong> = which packaged food can be labeled healthy. This rule was put up , and would limit added sugars and other changes
        <ul>
          <li>of course, some unhappy as certain cereals would be <em>not</em> be labeled as healthy</li>
          <li>several manufacturing lobbied and this ruling is <em>still</em> in process = <mark>allows interest groups to "self-fix" them</mark> = interest groups to monitor bureaucrat compliance = and also policy to be influenced by science and the public!</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>now, some uses the <strong>negotiated rulemaking process</strong>: neutral advisors known as convenors put together a committee of those who have vested interests in the proposed rules $\to$ then, with the help of neutral mediators, the committee eventually reaches a general consensus on the rules.</li>
</ul>

<p><em>Government Bureaucratic Oversight</em></p>

<ul>
  <li>The ability for bureaucracies to develop their own rules and in many ways control their own budgets has often been a matter of great concern for elected leaders. As a result, elected leaders have employed a number of strategies and devices to <strong>control public administrators in the bureaucracy</strong>.</li>
  <li><strong>The Congress</strong>
    <ul>
      <li>because of its power to <strong>control funding</strong> and approve presidential appointments.</li>
      <li>perhaps the most powerful oversight tool is the Government Accountability Office (GAO) $\to$ produce <strong>reports</strong>, mostly at the insistence of Congress.
        <ul>
          <li>e.g. from Bureau of Prisons = needed to report the number of prisoners they incarcerate, and Congress found that 45% of the prisoners who are released are re-arrested within 3 years.</li>
          <li>Congress thinks this is too high, and decided to require BoP create programs to fix this (e.g. to help them during incarceration), and also metrics to evaluate its effectiveness</li>
          <li>BoP has still yet implemented an evaluation plan due to various difficulties = <strong>in reality you might need to do more to push bureaucracies to do what you want</strong></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>The president</strong>:
    <ul>
      <li>Most directly, the president controls the bureaucracies by <strong>appointing the heads</strong> of the fifteen cabinet departments and of many independent executive agencies, such as the CIA, the EPA, and etc.</li>
      <li>other way to conducts oversight over the federal bureaucracy is the Office of Management and Budget (OMB) $\to$ produce the president’s annual budget for the country</li>
      <li><strong>repealing bureaucratic rules</strong> = if you are appointed new president but disagree with previous admins. What can you do with the bureuacracies?
        <ul>
          <li>wanted to make sure that <mark>TODO</mark></li>
          <li>new President directed agencies to review <em>every single rule</em> that was promulgated between that period</li>
          <li>Department of Labor issued a new rule = note that Presidents can’t write rules themselves, <mark>make new rules through agencies under control</mark></li>
          <li>Congress passed resolution to overrule the rule</li>
          <li>President vetoed that resolution on Monday, so this new rule will probably stand</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>The citizens</strong>
    <ul>
      <li><mark>Freedom of Information Act of 1966</mark> = provides journalists and the general public the right to <strong>request records from various federal agencies</strong>. These agencies are required by law to release that information unless it qualifies for one of nine exemptions.</li>
      <li><mark>Government in Sunshine Act of 1976</mark> = requires all multi-headed federal agencies to <strong>hold their meetings in a public forum on a regular basis</strong>.
        <ul>
          <li>this name comes from the belief that corruption thrive in secrecy but shrink when exposed to the light of public scrutiny.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Within Bureaucracy itself</strong>
    <ul>
      <li>When Congress drafted the Civil Service Reform Act of 1978, it specifically included rights for federal <strong>whistleblowers</strong>, those who <strong>publicize misdeeds committed within a bureaucracy</strong> or other organization, and set up protection from reprisals.</li>
      <li>Over time, Congress and the president have strengthened protections of those whistleblowers with Whistleblower Protection Act of 1989 and the Whistleblower Protection Enhancement Act of 2012.</li>
    </ul>
  </li>
</ul>

<p><em>Government Privitization</em>: a more controversial solution to the perceived and real inefficiencies</p>

<ul>
  <li>[+] rhetoric of privatization—that market competition would <strong>stimulate innovation and efficiency</strong>—sounded like the proper remedy to many people and still does.</li>
  <li>[-] certain government functions are simply <strong>not possible to replicate in a private context</strong>.</li>
  <li>ways of privitization
    <ul>
      <li><strong>Divestiture, or full privatization</strong>, occurs when government services are transferred, usually through sale, from government bureaucratic control into an entirely market-based, private environment. Rare but: <strong>Student Loan Marketing Association</strong> in 1973 to full privatization in 2004</li>
      <li><strong>Issuing government contracts to private companies</strong> in order for them to provide necessary services. e.g. By 2006, reliance on contracting to run the war was so great that contractors outnumbered soldiers. = a <strong>very routine form of privatization</strong></li>
      <li><strong>Third-party financing</strong> = the federal government signs an agreement with a private entity so the two can form a <em>special-purpose vehicle</em> to take ownership of the object being financed. The special-purpose vehicle is empowered to reach out to private financial markets to borrow money.</li>
    </ul>
  </li>
</ul>

<h2 id="bureaucratic-structure-and-independence">Bureaucratic Structure and Independence</h2>

<blockquote>
  <p>Structure of the bureaucracy depends a lot on the leadership (who is in charge), funding, and mandate</p>
</blockquote>

<p>How does Congress decide on structure?</p>

<ul>
  <li>how similar are the goals: if a bureaucracy is often making decision on political <em>unpopular</em> matters</li>
  <li>can change the legislation of bureaucracy to prevent tyranny = achieve 51% $\gets$ may want a group of people in charge instead of a single person on sensitive/contradictory issues.</li>
</ul>

<p>Creating bureaucracy creates <strong>agency problem</strong></p>

<ul>
  <li>[-] a hired bureaucrat can have a different incentive compared to what you wanted</li>
  <li>[-] hard to watch everything = monitoring problem
    <ul>
      <li>e.g. the stuff they do is so complicated, it is difficult to assess how well they are doing it</li>
    </ul>
  </li>
  <li>As a result
    <ul>
      <li>[-] (w.r.t President and Congress) making policies President and the Congress dislikes, especially if staffed with extreme preferences</li>
      <li>[-] corruption can happen</li>
      <li>[-] shirking, difficult to fire a person due to political reason by design (as those jobs are politically protected to decouple from politics)</li>
    </ul>
  </li>
</ul>

<p>How can congress control this agency loss:</p>

<ul>
  <li>Pass new laws; senate confirmation when appointing offices</li>
  <li>hold hearing where bureaucrats must answer</li>
  <li><strong>Congressional Review Act</strong> = allows congress to <strong>override a rule</strong> through joint resolution</li>
  <li><strong>Administrative Procedures Act</strong> = establishes rules for <strong>how bureaucracies make policy</strong>, and requires them to solicit comments, allow time, and etc.</li>
  <li><strong>Freedom of Information Act</strong> = requires certain records from the bureaucrats be made public</li>
</ul>

<p><strong>Bureaucratic Autonomy</strong> - where does it come from?</p>

<ul>
  <li>
    <p>bureaucracies develop a <strong>reputation</strong> for competence</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230323120545278.png" alt="image-20230323120545278" style="zoom:40%;" /></p>

    <p>where interests groups are the people who vote/have interest</p>
  </li>
  <li>
    <p>with unique organizationl abilities</p>

    <ul>
      <li>have well-resourced mezzo-level bureaucrats to figure out what people likes, and do more of that</li>
    </ul>
  </li>
  <li>
    <p>has distinct political agendas</p>
  </li>
</ul>

<p><strong>Why bureaucracy are really the fourth branch</strong></p>

<ul>
  <li>bureaucracies also have constituencies = people supporting them = very specific group of people who are benefited</li>
  <li>this is different from the constituency for electing president/Congress</li>
</ul>

<h2 id="bureaucratic-private-enforcement-regime">Bureaucratic Private Enforcement Regime</h2>

<p>Why was there a massive increase in federal litigation before?</p>

<blockquote>
  <p><strong>Private enforcement regime</strong> refers to the system of legal mechanisms and procedures that allow <strong>individuals or private entities to enforce</strong> their legal rights against other individuals, companies, or government entities. This can involve seeking compensation for harm suffered or seeking an injunction to prevent harm from occurring.</p>

  <p>This is to be in contrast to public enforcement, where regulatory bodies such as the <em>government or law enforcement agencies are responsible</em> for enforcing the law.</p>
</blockquote>

<ul>
  <li><strong>allow private citizens to be able to sue</strong> private entities = stop violation of laws; rather have federal bureaucracy agents to enforce this</li>
  <li>e.g. law in Taxes that empowered individuals to sue people</li>
  <li>motivates people because people will be receiving benefits (e.g. suing employment inequality)</li>
  <li>emerged as a strategic choice (see benefits and drawbacks below)</li>
</ul>

<p>Why PER is beneficial?</p>

<ul>
  <li>[+] decouples with president = president who disagrees <strong>cannot prevent</strong> this</li>
  <li>[+] tie the enforcement process with the American population = ensure it is proportional to public preference
    <ul>
      <li>i.e. <strong>self-correction</strong> relative to the interest groups of that entity</li>
    </ul>
  </li>
  <li>[+] by creating individual benefit from those private enforcement process (e.g. money), <strong>less free-rider</strong> = people waiting others to sue</li>
</ul>

<p>but also comes with certain costs</p>

<ul>
  <li>[-] more litigation, hence need <em>more judges</em> to hear them, or the entire process become <em>slow</em></li>
  <li>[-] selective use of PER: people who are more wealthy gets more attention; an uneven advantage <em>favoring the more powerful</em> = <mark>more inequality</mark>
    <ul>
      <li>so uncertain areas are more vulnerable to judicial backlogs = e.g. not getting attention and went into backlogs to be “waitlisted”</li>
    </ul>
  </li>
  <li>[-] gives judiciary large role in determining policy (bad if you want more unpolitical opinions)</li>
</ul>

<p>How to <strong>motivate more people to do PER</strong>:</p>

<ul>
  <li>
    <p>if you win a PER:</p>

    <ul>
      <li>=’win damages, legal fees to be paid, time lost’</li>
      <li>therefore, you can increase probability of wining, reduce legal fees, etc, to motivate people do PER more</li>
      <li>basically find ways to increase the expected benefit by changing various components/burdens</li>
    </ul>
  </li>
  <li>
    <p>if you lose:</p>

    <ul>
      <li>=’no damages, legal fees to be paid, time lost’</li>
    </ul>
  </li>
  <li>
    <p>hence there is a balance between how to “assign” those burdens</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230323115914771.png" alt="image-20230323115914771" style="zoom:33%;" /></p>
  </li>
</ul>

<p>But in general, PER has been used a lot. The grey line being US plaintiffs litigation has been shifted to private people.</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230323115653828.png" alt="image-20230323115653828" style="zoom: 40%;" /></p>

<blockquote>
  <p>Bureaucracy is not the only option to stop violation of law/promote equality, but PER can also have drawbacks.</p>

  <ul>
    <li><strong>inequality occurs in PER</strong>.
      <ul>
        <li>e.g. greater employment inequality due to employers know, depending on how wealthy/powerful the employee is, how likely they are being sued</li>
        <li>e.g. to uphold environmental protection standards, company will select poorer community to dump all the environmental wastes</li>
      </ul>
    </li>
    <li>a strong bureaucracy could prevent the above from happening. But there is the <strong>other problem of president oversights, etc</strong></li>
  </ul>
</blockquote>

<h2 id="progressive-reformers">Progressive Reformers</h2>

<blockquote>
  <p>A <strong>trade-off</strong> between controlling (e.g. bureaucracies) with expertise v.s. controlling with democracy</p>

  <ul>
    <li>more expertise = to protect policy from being changed by simply winning a majority vote</li>
    <li>more democracy = less corruption/tyranny</li>
  </ul>
</blockquote>

<p>During the progressive era, people worked to change governments at all levels from 1890s to 1920s and wanted gov to run like a business. They focused on the bureaucracies to have:</p>

<ul>
  <li>less corruption = fix voting systems</li>
  <li>more efficient</li>
  <li>immorality = e.g. alcohol ban</li>
  <li>social improvement and safety = e.g. more on women’s suffrage, so that they care more about limiting corruption = who supported their goal</li>
</ul>

<p>During this era, changes to achieve this include</p>

<ul>
  <li>16th = can levy taxes and require more from healthier states</li>
  <li>17th = popular electrion of senators = less corruption</li>
  <li>18th = prohibition of alcohol</li>
  <li>19th = women’s right to vote</li>
</ul>

<p>And this gives some <strong>hint to “ideal” bureaucratic designs</strong></p>

<ul>
  <li>need a mix of bureaucrats with expertise/political independence + bureaucrats with “democratic” opinions
    <ul>
      <li>e.g. <strong>Federal Reserve System</strong> reflects this. There are 12 regional banks getting regional interests (democracy), but 7 governors serving 14-years term to be less affected by politics (expertise)</li>
      <li>e.g. <strong>Council-Manager form of city government</strong>. Professional manager serves longer in charge of government functions (expertise), and manager reports to council who does final decision incorporating public opinions</li>
    </ul>
  </li>
  <li>so basically there is a <strong>trade-off</strong> between “listening to people” and “making expert decisions”
    <ul>
      <li>if main threat is incompetence, the “solution” is political independence = <mark>complex/technical policy areas often fit this</mark></li>
      <li>if main concern is subversion/extremism, then more political control is a good solution</li>
    </ul>
  </li>
</ul>

<h2 id="welfare-state">Welfare State</h2>

<blockquote>
  <p>Welfare state = A series of agencies through which <strong>government</strong> takes care of health and well-being of the people</p>

  <ul>
    <li>you will see how this becomes an example of agency loss and a debate/conflict between fed and state</li>
  </ul>
</blockquote>

<p>What are the major elements of US welfare state?</p>

<ul>
  <li>social security = pension for older people</li>
  <li>Medicare and Medicaid = health insurance</li>
  <li>Unemployment insurance</li>
  <li>etc.</li>
</ul>

<p>Examples <em>not</em> counted as welfare include: public education, mortgage assistance, policing, etc. Hence as you can see:</p>

<blockquote>
  <p>Surly, boundaries of ‘welfare state’ is politically contested.</p>
</blockquote>

<p>But who runs the welfare state?</p>

<ul>
  <li>social security = by federal government</li>
  <li>medicare = by fed government</li>
  <li>medicaid = by state governments</li>
  <li>earned income tax credit = by federal</li>
  <li>etc.</li>
</ul>

<p>Why are some programs managed by fed, some by state? Specifically, why give states power to decide criterion for eligibility for Medicaid, for example?</p>

<ul>
  <li>reduces agency loss for political majorities in states = they might not trust the federal bureaucrat</li>
  <li>increases agency loss for majority in congress = as states can do what they want</li>
  <li>
    <p>a lower level government in general also lower agency loss of voter = more responsive.</p>
  </li>
  <li>e.g. max income for medicaid for parent eligibility is different for different states (e.g. Texas very low, hence few Medicaid eligibility)</li>
</ul>

<blockquote>
  <p>This therefore also results in long history of conflict over <strong>who should be eligible for the benefits</strong> $\gets$ government and states want different group of people to be receiving the benefits</p>
</blockquote>

<h2 id="police-reform">Police Reform</h2>

<p>This topic is not on midterm, but has a lot on bureaucracy and agency problems, also a research project by the professor.</p>

<blockquote>
  <p><strong>Research question</strong>: how well did police policy-making process serve Black people?</p>

  <ul>
    <li>how democratic responsible are they = are they listening to the people?</li>
    <li>how well did police policies actually protect black people.</li>
  </ul>
</blockquote>

<p>A policy-making process for police:</p>

<ol>
  <li>(many) black people unhappy with policing</li>
  <li>advocacy groups propose solutions, e.g. more training, Police Community Relations (PCR), more black cops, break policy department into smaller units = more accountability</li>
  <li>policing experts settle on PCR and hiring Black officers</li>
  <li>other solutions pruned</li>
</ol>

<p>Empirical questions answered by the reading/paper:</p>

<ul>
  <li>Did PCR programs change arrest rates for Black people?</li>
  <li>Did PCR programs reduce crime rates?</li>
</ul>

<p>The short summary is a decreased low level arrest for white, <strong>not</strong> a lot for black people.</p>

<hr />

<p>Historical context: 1945-70</p>

<ul>
  <li>Popular constraints in 1955:
    <ul>
      <li>complaint that black victims were not concerned</li>
      <li>brutality, e.g. police hurting the suspect during arrest</li>
      <li>unreliable protetion from white vigilantes</li>
      <li>wide-spread complaints on disrespect</li>
    </ul>
  </li>
  <li>reform proposals
    <ul>
      <li>training = give officers a different set of skills
        <ul>
          <li>e.g. introduce police officers to prejudices and make them aware of those</li>
          <li>e.g. teach cops to see crimes not as race but environemntal efforts</li>
        </ul>
      </li>
      <li>hiring black officers = believing it happened due to adversarial selection problem</li>
      <li>civilian review board</li>
      <li>police community relations = began in 1955
        <ul>
          <li>how do we get black leaders and police together, work together and solve</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>during this time, there was also a lot on women reform, but also on Vietnam war</li>
  <li>this results in the most common features of PCR programs
    <ul>
      <li>training</li>
      <li>community advisory councils or meetings; residents come to police and can file complaints</li>
      <li>specialized outreach officers= sell the police department to residents of various colors</li>
    </ul>
  </li>
  <li>however, the result is
    <ul>
      <li>Dallas had no PCR and ended with decreasing arrests</li>
      <li>so we are not uniforming seeing PCR helping it. Raises the question: what effect does PCR have? Control variables?</li>
    </ul>
  </li>
</ul>

<p>Related works</p>

<ul>
  <li>no studies of PCR program effect on punishment</li>
  <li>black perference $\neq$ low punishment; black people wanted a bunch of diverse things to be safe. But
    <ul>
      <li>high rates of crime victimization drive support for punitive approaches</li>
      <li>black officers took these jobs to be police officer, often <em>not</em> to change the racial-police relationship = should not expect that just putting black police will make them act different from other polices</li>
    </ul>
  </li>
  <li>conjecture that policy-making environment may bias against black preference</li>
</ul>

<blockquote>
  <p>Hypothesis:</p>

  <ol>
    <li>these programs effects will depend on city racial conposition</li>
    <li>PCR increase arrest rates for low-level offenses, if
      <ul>
        <li>reduced risk of intervention for officers = don’t want to be involved with brutal cases to hurt themselves</li>
        <li>increased service for crime victims, i.e. more police is answering your help</li>
      </ul>
    </li>
    <li>PCR decrease arrest rates for low-level offenses, if
      <ul>
        <li>greater deference to community preferences, e.g. want less arrests for gambling</li>
        <li>officers working less hard = greater shirking</li>
        <li>effort switched to serious crimes</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>The empirical study then considers data of PCR program status, population, black and white minor crimes arrest rates. Then perform mixed effect models and measure how PCR changed things.</p>

<blockquote>
  <p>Results summary:</p>

  <ul>
    <li>PCR units reduced minor crime arrest rates for white, no clear effect on Black arrests rates</li>
    <li>PCR units reduced violent crime rates</li>
  </ul>

  <p>so unfortunately one of the major policy used in policy departments, PCR, had no clear effect in helping the black</p>
</blockquote>

<p>But why? This is still being worked on.</p>

<p>But be careful that arrest rates can mean different things, e.g. if they are not perpetrators, then this indicates police wrongdoings.</p>

<ul>
  <li>e.g. can look at dismissal rates <em>in addition</em> to arrest rates = a lot of people who were arrested have not committed the crime.</li>
</ul>

<h1 id="public-opinion">Public Opinion</h1>

<blockquote>
  <p><strong>Public opinion</strong>: aggregation of people’s view about issues, situations, &amp; public figures.</p>

  <ul>
    <li>e.g. when situations arise internationally, polling companies survey whether citizens support U.S. intervention in places like Syria or Ukraine. These individual opinions are collected together to be analyzed and interpreted for politicians and the media.</li>
  </ul>

</blockquote>

<p>But where do people’s opinions come from? Most citizens base their political opinions on their <strong>beliefs</strong> and their <strong>attitudes</strong></p>

<p><strong>Belief</strong>: closely held ideas that support our values and expectations about life and politics.</p>

<ul>
  <li>for example, the idea that we are all entitled to equality, liberty, freedom, and privacy is a belief most people in the United States share.</li>
  <li>i.e. what we <em>think</em> are true about the world, e.g. who do we <em>think</em> are responsible for xxx.</li>
  <li>can change rapidly over time</li>
</ul>

<p><strong>Attitude</strong>: surveys want to measure people’s view on an issue = measuring their attitude</p>

<ul>
  <li>represent the <em>preferences</em> (e.g. like or dislike) we form based on our life experiences and values.
    <ul>
      <li>more specific than ideologies; people of the same ideology can have different attitudes on the same issue (e.g. due to a variety of factors)</li>
      <li>can be <strong>more effective affecting survey responses</strong> than belief (e.g. I believe this is true, but I still dislike it.)</li>
    </ul>
  </li>
  <li>organized and consistent manner of thinking, feeling, and reacting to people/groups/social issues
    <ul>
      <li>but <strong>most</strong> people’s attitudes are <strong>quite losely structured</strong>/inconsistent</li>
      <li>but still more stable than beliefs</li>
      <li>e.g. consumer confidence = belief in economy changes rapidly during Trump/Pandemic/Biden, despite in reality the real change in economy is much less = <em>people’s attitudes on presidency/certain events affect their belief on the economy</em></li>
    </ul>
  </li>
  <li>attitude might be strong/weak = can change over time or not = depends on people and on issues
    <ul>
      <li>e.g. racial attitudes</li>
    </ul>
  </li>
</ul>

<p><strong>Ideology</strong>/Belief system: configuration of your beliefs and attitudes such that it can <strong>explain/connect one’s belief and attitudes</strong></p>

<ul>
  <li>established beliefs and ideals about how government and public policy should work</li>
  <li>in theory, this promotes consistency among political attitudes</li>
  <li>in practice, ideology combine attitudes linked by policy groups</li>
</ul>

<hr />

<p>Some examples of contemporary idelogies</p>

<ul>
  <li><strong>liberal</strong> positions
    <ul>
      <li>pro-choice</li>
      <li>higehr and more progressive taxation</li>
      <li>environmental protection</li>
      <li>social welfare programs</li>
    </ul>
  </li>
  <li><strong>conservative</strong> positions
    <ul>
      <li>pro-life</li>
      <li>lower taxes</li>
      <li>‘traditional’ values</li>
    </ul>
  </li>
  <li>one interesting issue is on <strong>military intervention</strong>
    <ul>
      <li>it seems party identification (democrat or republican) is <em>not</em> a good predictor for military intervention</li>
      <li>party identification is not identical to ideology but <em>a reasonably good proxy recently</em></li>
    </ul>
  </li>
</ul>

<p>On gross, what are majorities ideologies? Liberal-Conservative Self-Identification shows:</p>

<ul>
  <li>
    <p><strong>about half of the population don’t identify</strong> with any of those labels</p>
  </li>
  <li>
    <p>(a survey of 7 point scale, extremely conservative; …; extreme liberal; don’t know)</p>
  </li>
</ul>

<p><strong>Influence of Public Opinion</strong> on public policy</p>

<ul>
  <li>US constitution give mechanism to make it easier for folks to express and develop their public opinons
    <ul>
      <li>broad suffrage</li>
      <li>freedom of speech and press</li>
    </ul>
  </li>
  <li>public opinion affect politicians making decisions = they need to monitor public opinions
    <ul>
      <li>e.g. need to deal with regular elections</li>
    </ul>
  </li>
</ul>

<p>Origin/factors that <strong>influence public opinion</strong></p>

<ul>
  <li>they have shared political <strong>values</strong> (e.g. equlity of opportunity, individual freedom)
    <ul>
      <li><strong>basic model: people form opinion using those values they have</strong></li>
      <li>but those values are not <em>born with those values</em> = political socializatoin</li>
    </ul>
  </li>
  <li><strong>political socialization</strong>
    <ul>
      <li>influenced by family, social groups (e.g. unions and churches)</li>
      <li>education = years of formal education associated with more tolerance; more participation in politics</li>
      <li>major political events = people who have gone through those major events (e.g. Civil War, new Deals) have a shared opinions; this makes their opinions more similar to each other than people from other times</li>
    </ul>
  </li>
</ul>

<h2 id="us-opinion-over-time">US Opinion over time</h2>

<p>Many events which has <strong>opinions change over time</strong> (measured from public opinion pool)</p>

<ul>
  <li><strong>abortion</strong>: in the past (1988) about 40% opposing abortion, now only 10-15% opposes it
    <ul>
      <li>a further breakdown for recent opinions = about 30-40% thinks it abortion should be permitted when rape, etc. And another 25-40% thinks it should be woman’s freedom</li>
    </ul>
  </li>
  <li><strong>guns</strong>: about 5% people thinks it should be made easier to acquire guns; about 50% more difficult, and 45% keep same.
    <ul>
      <li>interestingly, this trend has been remarkably stable</li>
      <li>why more stable? several possible reasons
        <ul>
          <li>this is a distant issue, v.s. abortion is recent = unstable</li>
          <li>in reality, there had been decisions <em>making gun purchases easier</em>.
            <ul>
              <li>e.g. recently expired ban on buying assault rifles = more easier;</li>
              <li>e.g. USSC also passed laws to make it easier</li>
            </ul>
          </li>
          <li>an example where decisions can go against political opinion</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>income difference</strong>: should government take action to reduce the difference
    <ul>
      <li>about 50% agree that government should try to reduce income diff, about 20% thinks status quo</li>
    </ul>
  </li>
  <li><strong>public schools</strong>: about 70% wanted increase in public schools</li>
  <li><strong>social security</strong>: about 50% wanted increase in social security</li>
  <li><strong>more aid to poor</strong>: about 50% thinks that federal should spend more to aid the poor, and about 40% thinks status quo</li>
  <li><strong>reduce deficit</strong>: about 75% people think it is very to extremely important</li>
</ul>

<p>The above is very weird: everybody wanted more spending on income/social security/more aid, but also reduce deficit = less spending = <strong>inconsistency</strong>.</p>

<ul>
  <li>
    <p>wrong question wording/wrong measurement</p>
  </li>
  <li>maybe public opinon should influence but <em>not used to determine</em> policy</li>
  <li>elite notions of policy coherency should be abandoned</li>
</ul>

<blockquote>
  <p><strong>Interpretation of those surveys are often complicated</strong>, and <strong>consistency</strong> in aggregate opinion is rare (e.g. see above).</p>
</blockquote>

<h2 id="what-can-go-wrong-in-public-pooling">What can go wrong in Public Pooling?</h2>

<p><strong>Can public opinion be manipulated/what can affect people’s attitudes?</strong></p>

<ul>
  <li>
    <p>first of all, this is difficult because many contributing factors are fixed, i.e. are steadfast and hard to change</p>

    <ul>
      <li>e.g. culture, religion. Have impact on your opinons, and they don’t change much over time</li>
    </ul>
  </li>
  <li>
    <p>opinions on contradictory issues can manipulate through <strong>framing</strong></p>

    <ul>
      <li>given an issue, how you frame the issue affects how people think about it (e.g. religious freedom problem v.s. discrimination problem)</li>
      <li>basically answer could change based on how you frame the question</li>
    </ul>
  </li>
  <li><strong>sampling error</strong></li>
  <li>
    <p>drawing random samples is subject to <strong>trade-off</strong> between more data/less error v.s. more time/money costly</p>
  </li>
  <li><strong>selection bias</strong>, often selective non-response: might have missed certain populations.
    <ul>
      <li>e.g. survey in college and apply it to real life</li>
      <li>
        <p>e.g. the survey results we did in class is <em>different</em> than the results in public poll</p>

        <ul>
          <li>e.g. folks don’t trust the government won’t answer surveys, hence missed</li>
        </ul>
      </li>
      <li>
        <p>solution is a random sample of the entire population where everyone has chance of selection, and over-sampling plus re-weighing</p>

        <ul>
          <li>but this is difficult to implement in real life. e.g. no single directory to entire US population, and no reliable way to reach everyone (e.g. no internet access)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>measurement error</strong>: people’s answer might not be what they actually think</p>

    <ul>
      <li>you asked the wrong question, so data you collected <strong>does <em>not</em> answer what you wanted to measure</strong></li>
      <li>often this happens when your question is unclear/too simple
        <ul>
          <li>e.g. LA Times poll in 1980s “Do you think a pregnant women should or should not be able to get a legal abortion, <em>no matter what the reason?</em>” 57% says no.</li>
          <li>e.g. CBS poll a few months later “If a woan wants to have an abortion, and here doctor agrees to it, should she be allowed to have an abortion?” 58% said yes</li>
          <li>yea, questions are a bit different, but the point is that people’s majority opinion changes on the issue of abortion.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Measurement Validation</strong>: But how do you even <mark>quantify how good your methodolgy is</mark>, given that there is no “ground-truth to begin with”?</p>

<ul>
  <li>the simple idea is to ask the public if they think the methodology will answer the question you have in mind</li>
  <li>e.g. want to research how changing a priming text can change how people want to run for office
    <ul>
      <li>idea is to manipulate sense of racial discrimination, group solidarity, difficult etc.</li>
      <li>then by this validation technique, you can <strong>reduce (some) measurement errors</strong> (i.e. people did not answer the way you wanted) by:
        <ul>
          <li>e.g. just survey/interview people what they <em>think</em> the answer is asking (vast majority to just make sure this)</li>
          <li>e.g. in this research ask people in survey if they think those text approximately induce what you wanted to do</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>e.g. want to research: does the exposure of racial identity (e.g. people of Asian descent) change your support to the republican party?
    <ul>
      <li>Idea is to have a two sets of people: White and Asian American. Then random subset of both groups were treated with micro-aggression “I forgot that this study is only for US citizens. Are yuo a US citizen I cannot tell.” = cue a sense of exclusion to Asian Americans. The hypothesis is that this should only affect results for Asian Americans, not white.</li>
      <li>Result: Asian descent after micro-aggression had less support for republicans, v.s. White descent after micro-aggression. = <em>exposure to racialized micro-aggression caused Asian-identifiers to be more supportive of Democrats</em></li>
      <li><strong>Why is this useful?</strong> Implication that trend in democratic voting among Asian America may due to experiences of discrimination. = Hence republicans can have more support if they break this association with discrimination.</li>
    </ul>
  </li>
</ul>

<h1 id="elections">Elections</h1>

<p>How do you select a candidate? <strong>What are some “metrics” you would use to evaluate competence</strong>?</p>

<ul>
  <li>their policy proposals</li>
  <li>
    <p>policy priorities and party alignments; endorsements (who sponsored them)</p>
  </li>
  <li>their identity (age, race, gender, class)</li>
  <li>political track records/resume</li>
  <li>ask yourself: how are things going for you/others given that he/she will be in office?</li>
</ul>

<p>What are elections for?</p>

<ul>
  <li>
    <p><strong>mechanism for principals (votres) to keep agents in line/accountable</strong> (reduce agency loss)</p>

    <ul>
      <li>allow selection of candidates with desirable qualities</li>
      <li>prospect of fture electrions creates incentive for office-holders to behave themselves/care about eopple</li>
      <li>possibility of unseating incumbent creates incentive to monitor</li>
    </ul>

    <p>basically tie policy to what voters are interested in</p>
  </li>
  <li>
    <p>voting produces collective goods</p>

    <ul>
      <li>victory for party</li>
      <li>signal voter (dis)satisfaction to officials (so they can react)</li>
      <li>an opportunity to remove agents who are not performing well</li>
    </ul>
  </li>
</ul>

<h2 id="rational-theory-of-elections">Rational Theory of Elections</h2>

<p><strong>Elections/voting as a free-rider problem</strong>: it can be costly</p>

<ul>
  <li>
    <p>if voters wanted to maximize their individual well-being, would you vote?</p>

    <ul>
      <li>benefits: voting gives greater tie to the community</li>
      <li>costs: you have to do all the effort/studying, or the party you don’t like wins</li>
    </ul>
  </li>
  <li>
    <p><mark>a classical model of voting</mark></p>

\[V = P*B - C + D\]

    <p>where $V$ whether to vote, $P$ the prob of individual vote will change outcome, $B$ the benefit if candidate wins, $C$ the cost of voting, and $D$ duty of psychological gratification. Therefore in this simple model, you would vote if:</p>

\[P*B + D &gt; C\]

    <p>some factors</p>

    <ul>
      <li>if a coup/vote manipulation occurs, then $P\to 0$.</li>
      <li>$P$ would go up if: the vote is currently tied, there is a small voting population</li>
      <li>$B$ would go up if there are big difference between candidate policy goals</li>
      <li>$C$ would go up if: the financial status of the person, registration/voting laws, intimidation/violence</li>
      <li>$D$ would go up if: peer/social group pressure want you to vote</li>
    </ul>
  </li>
  <li>
    <p><strong>low-info rationality</strong>. In reality, it is found that <strong>on average, people know little about politics</strong></p>

    <ul>
      <li>does this mean that they are bad voters? Not necessarily</li>
      <li>learning all these info is time-consuming, but good substitutes around
        <ul>
          <li>can rely on cues
            <ul>
              <li>e.g. relying on price to infer quality when buying clothes</li>
              <li>hence, can making choosing an agent (who is better for you) much simpler</li>
            </ul>
          </li>
          <li>examples of cues involve
            <ul>
              <li>party labels/involvement</li>
              <li>endorsements: what kind of organization is backing him/her
                <ul>
                  <li>American Civil Liberties Union: civil liberties focused, on the rights of racial minorities</li>
                  <li>Newspaper can give endorsement as well (e.g. editorial board)</li>
                </ul>
              </li>
              <li>identity feature such as gender or race</li>
              <li>candidate experience</li>
            </ul>
          </li>
          <li>it turns out that providing this informational is <strong>indeed a major objective of campaigns</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="research-example-candidate-evaluation">Research Example: Candidate Evaluation</h2>

<p>2021 World Bank data: US ranks below other rich liberal democracy in the proportional of national legislative seats (senate+house) held by women.</p>

<blockquote>
  <p>Does this indicate US is behind in terms of gender discrimination? This is a complicated question.</p>
</blockquote>

<ul>
  <li>
    <p>Q: if imbalance in office matches imbalance in candidate pool, does this mean no discrimination?</p>

    <p>Not if candidates are strategic</p>
  </li>
  <li>
    <p>Q: does euqal win rates in general elections mean on discrimination?</p>

    <p>Not if selection process produces different candidate pools</p>
  </li>
  <li>
    <p>What would be the “good” questions to ask? We need to consider: <strong>would the voter’s evaluation change if gender were different</strong>?</p>
  </li>
</ul>

<p>Path to (federal) office:</p>

<ol>
  <li>decision to run for office
    <ul>
      <li>women might have a harder time getting their main messages out due to discrimination in voters</li>
    </ul>
  </li>
  <li>wining in primary (need more fund-raising)</li>
  <li>winning general election</li>
</ol>

<p>The discrimination process <em>may</em> happen <em>anywhere</em> in this trajectory. Therefore even if in the end you have “equal win rates” there might still be discrimination. (e.g. discrimination against women in 1 and 2, but reversed in 3)</p>

<blockquote>
  <p>Takeaway: total effect of social features like gender is <strong>more expansive</strong> than quantitative studies we have today</p>

  <ul>
    <li>quantitiative tools are useful for testing a <em>single change</em></li>
    <li>therefore, all the estimates of effects we measure will be partial</li>
  </ul>
</blockquote>

<blockquote>
  <p>Research Question: Association of the task with gender implications can affect women’s win rate</p>
</blockquote>

<p>Hypothesis:</p>

<ol>
  <li>women have a larger advantage over men in <em>legislative</em> than in executive (women better at talking)</li>
  <li>women have a larger advantage where policy domain are <em>perceived towards women’s task</em> (e.g. school boards)</li>
  <li>women have  a larger advantage over men in constituents that are more <em>liberal</em> (so voters have less attachment with traditional gender values = more likely to vote for women)</li>
</ol>

<p>The idea is to study this from the election result in 2021. Assuming there is</p>

<ul>
  <li>
    <p><strong>no candidate quality different between genders</strong></p>
  </li>
  <li>
    <p><strong>similar selection process in the different elections</strong> (e.g. school board, mayor=more executive, city council=more legislative)</p>
  </li>
</ul>

<p>The result is:</p>

<ul>
  <li>women win rate does seem higher in school boards, and also higher in city council than mayor</li>
  <li>women’s win rate gaped more than men in school boards when there are more liberal voters. But there is a mixed effect in council elections and mayor</li>
</ul>

<h2 id="campaigns-and-persuasion">Campaigns and Persuasion</h2>

<p>How do I gather more more votes?</p>

<ul>
  <li>should I persuade those who disagree (involves democratic value of listening to people)?</li>
  <li>should I mobilize those who agree (just want them to show up and actually vote for me)?</li>
</ul>

<p>How do you decide voting?</p>

<ul>
  <li><strong>issue-based voting</strong>:
    <ul>
      <li>a person’s vote depends on if you agree with the candidate’s take on it</li>
      <li>a person’s vote depends on the weights of those issues (how important)</li>
      <li>but in reality, you will see that many people also vote <em>just for the candidate</em> regardless of his positions</li>
    </ul>
  </li>
  <li>how can campaigns change things?
    <ul>
      <li>can change weights of those issues</li>
      <li>can add new considerations to the issue
        <ul>
          <li>not an easy task = voters can be saturated with considerations</li>
          <li>there is only a small number of people who are persuadable</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>J. Kalla and Broockman in 2017 did a study on <strong>if campaigns are helpful in changing people’s votes</strong>
    <ul>
      <li>finding: little evidence of impact = all those money spent on campaigns have a net effect of zero</li>
      <li>why?
        <ul>
          <li>persuasion <mark>TODO</mark></li>
          <li>there may be an impact in the beginning, but wears off when actually voting</li>
          <li>odd cases <mark>TODO: how does this relate to spending money</mark>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Then why do campaigns spend so much money?
    <ul>
      <li>maybe: <strong>in the real world campaigns are to “cancel” each other out</strong>. One party says bad things about others, and vice versa = zero net output.</li>
      <li>the change of the campaigns <strong>might have long term effects</strong> (e.g. decade), hence measuring in a single season is not suitable</li>
      <li>even if peoples votes didn’t change, those campaigns can <strong>affect people’s values/belief about the world</strong></li>
    </ul>
  </li>
</ul>

<h2 id="does-policy-derive-how-people-vote">Does Policy Derive how People Vote?</h2>

<p><strong>Voter Competence Test</strong>:</p>

<ul>
  <li>should we see elections as driven by voters, or do voters follow their politicians they prefer</li>
  <li>elections not by what voters want, but what the campaigns</li>
</ul>

<p>Research:</p>

<ul>
  <li>
    <p>focus on respondents who only learned the policy positions during campaigns</p>
  </li>
  <li>
    <p>when people <em>realized</em> their candidates is different from position they support, they still voted for them = it looks like <strong>people are following politicians</strong> instead of the policies</p>

    <ul>
      <li>e.g. Bush v Gore, who is social security funds handled</li>
      <li>
        <p>e.g. Carter v. Ford - whether federal gov should fund more <mark>TODO</mark></p>
      </li>
      <li>then ask people: Q: do you know who is supporting what position? Tell them the info, and ask again. Some people who knows about this issue got it right in the first round. Some people (learners) learns it afterwards.</li>
    </ul>
  </li>
  <li>
    <p>Finding: voters (learners) <strong>don’t change their votes even when there’s changes in their candidate support</strong></p>
    <ul>
      <li>Does this mean candidates <em>face no pressure</em> to take certain issue positions? No
        <ul>
          <li>the study is only on voters who <em>learned</em> the positions taken by the candidate. Hence this excludes the cases of those voters who already know the positions (and changed their votes)</li>
          <li>excludes interest groups, who typically take a strong hold/attention to the positions taken</li>
        </ul>
      </li>
      <li>If pressure comes mostly from interest groups, is it bad?</li>
    </ul>
  </li>
</ul>

<p>What are politician punished for?</p>

<ul>
  <li>strong evidence that <em>shark</em> attacks depressed Wilson vote in NJ (beach state) = people blindly attribute this to Wilson</li>
  <li>strong evidence <em>drought</em> reduces incumbent party vote share</li>
</ul>

<blockquote>
  <p>So based on all those “bad” findings, <strong>are US voters incompetent</strong>? A really complicated question.</p>
</blockquote>

<blockquote>
  <p>This shows that many voters’ voting behavior <strong>does not really represent their interest</strong> (e.g. blind rationale, learners, etc). Therefore, we may need to <strong>re-think elections</strong> = collecting votes as a mechanism to <strong>aggregate people/voters’s interest</strong> to form policies</p>
</blockquote>

<ul>
  <li>Politics not as competition between policies but competitions between groups for government</li>
  <li>politics as policy competition leaves more room for compromise</li>
</ul>

<h2 id="democratic-benchmarks">Democratic Benchmarks</h2>

<p>Whenever assessing the performance of a political system, you should to think about alternatives (what it <em>can</em> do). Why?</p>

<p>It turns out there is a <mark>limit on democratic choice system</mark>. <mark>No choice aggregation rule can satify four conditions simultaneously</mark></p>

<ol>
  <li>if everyone prefers A to B, the rule will pick A</li>
  <li>choice between two optins not influenced by third</li>
  <li>responds to more than one person’s wants</li>
  <li>decision cannot be manipulated by choice order</li>
</ol>

<blockquote>
  <p>All institutions doesn’t do at least some of the four.</p>
</blockquote>

<p>As a result, this means all of the following features of US political system are not “really” democratic = influence the final aggregated result</p>

<ul>
  <li>influence the aggregation process</li>
  <li>eventually all procedures are flawed</li>
</ul>

<p>all of the above features influence what public policy is.</p>

<h1 id="political-party">Political Party</h1>

<p>What is a political party, and how would you tell if the party is changing?</p>

<blockquote>
  <p><strong>political parties</strong> are groups of people with similar interests who work together to create and implement policies.</p>
</blockquote>

<p>On a high level: what do they do? Essentially they gain control over the government by winning elections.</p>

<ul>
  <li><strong>recruit</strong> candidates = exercise control over who they want
    <ul>
      <li>v.s. why not let candidates apply by themselves? = the party might have a different set of interest than candidates who are applying</li>
      <li>e.g. Trump candidacy in 2015/16</li>
    </ul>
  </li>
  <li><strong>nominate</strong> candidates (run for state government, Congress, and the presidency.)
    <ul>
      <li>different state has different rules (e.g. top two in votes end up in a runoff)</li>
      <li>in the end, state laws affect this selection progress significantly</li>
    </ul>
  </li>
  <li>guide members of Congress in <strong>drafting</strong> legislation.</li>
  <li>etc.</li>
</ul>

<p>But it is also helpful to break down political party into the three categories</p>

<ul>
  <li>party in government</li>
  <li>party in electorate,</li>
  <li>party organizations (fight for what the rules should be)</li>
</ul>

<p><strong>What can political party do for politicians?</strong> solve <strong>collection action</strong> problems</p>

<ul>
  <li>provide a host of funds/resources (people needed to) to help a politician win an office
    <ul>
      <li>especially to gather votes and sure <em>people</em> vote consistently if they are in the party</li>
    </ul>
  </li>
  <li>compromise/coordination when lawmaking = parties guide proposed laws through Congress and inform party members how they should vote on important issues.
    <ul>
      <li>e.g. you can ask party to pressure people if they want things different than what you want</li>
    </ul>
  </li>
  <li>agency problem/trade-off: politicians only want help to achieve office position
    <ul>
      <li>e.g. politicians want parties to leave them alone after helping, but party members prefer the opposite!</li>
    </ul>
  </li>
</ul>

<h2 id="who-identifies-with-which-party">Who identifies with which party?</h2>

<p>This is decided over a range of empirical studies, and it is found that</p>

<blockquote>
  <p>People align with a political party <strong>not because of its policies</strong>, but to <strong>compete</strong> against/dislike groups who share different values/identity = really for their own interest.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Data 1</th>
      <th style="text-align: center">Data 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417202032980.png" alt="image-20230417202032980" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417202150189.png" alt="image-20230417202150189" /></td>
    </tr>
  </tbody>
</table>

<p>here we see that there is a</p>

<ul>
  <li>fair amount of stability in party memberships = those party members usually vote for their parties</li>
  <li>but one place you see change is “how intensely” are you partisan
    <ul>
      <li>see a rise of strong partisan to 44% from around 35% and less weak partisanship</li>
      <li>intensity of people’s commitment to the identity increases = shift in partisan system is not more people coming but <strong>more committed to partisanship</strong> = affect how politicians make electoral strategy</li>
    </ul>
  </li>
</ul>

<p>But what do people/voters really think of the political party as?</p>

<ul>
  <li>Q: which party is more conservative? 80% answered republicans correctly, but 20% either don’t know or answered wrongly
    <ul>
      <li>= hints that some people align with a party without really knowing its policy</li>
    </ul>
  </li>
  <li>Q: do you think the two parties are different? a lot of people think the parties are the same in the past, now mostly different, but still
    <ul>
      <li>= hints that some people align with a party without really knowing its policy</li>
      <li>making polarization increasing, but people at least know</li>
    </ul>
  </li>
  <li>
    <p>Q: how do you feel about the rival party? On average 19.3% happy with members of the other party, and 71.5% happy with own members.</p>

    <ul>
      <li>^ but specifically what traits are different? e.g. close-mindedness/unintelligent
        <ul>
          <li>are very parallel observation that both parties share the same trend of disliking other parties</li>
          <li>but it is also found that in general, people dislike <em>any other</em> groups, not necessarily rival</li>
          <li>yet still, the <strong>increase in dis-likeness=polarization</strong> is important</li>
        </ul>
      </li>
    </ul>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Anti-Group</th>
          <th style="text-align: center">Increasing Polarization</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417202830606.png" alt="image-20230417202830606" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417202620818.png" alt="image-20230417202620818" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Q: what groups of people tilt the politician group? Here we member within each party <em>grouped by their traits</em></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Tilt Republican</th>
          <th>Tile Democratic</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417203202854.png" alt="image-20230417203202854" style="zoom:33%;" /></td>
          <td><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230417203212104.png" alt="image-20230417203212104" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>= increasing <strong>separation</strong> of the identities between people in each parties = how social group change opinion = politics as <mark>competition between groups</mark> (a newer theory today) v.s. competition between policy (older)</li>
      <li>also implies politician within group may only care about moving powers to people who think like yourself</li>
    </ul>
  </li>
  <li>
    <p>finding: misconceptions of the parties can influence how people feel of the other party = also support the above conclusion that <strong>party alignment may have not much to do with the real policy they put out</strong> = people are bit egoistic</p>

    <ul>
      <li>perceived of the republican supporters:
        <ul>
          <li>folks in the survey think the republican are 40% over 65 aged people, in reality 20%, etc.</li>
          <li>in conclusion: many folks don’t have an accurate impression of the party composition</li>
        </ul>
      </li>
      <li>perceived of the democratic supporters:
        <ul>
          <li>folks in the survey think the democrats are 30% gay, lesbian, or bisexual, and 40% black, but in reality the proportion is much lower</li>
          <li>same conclusion as above.</li>
        </ul>
      </li>
      <li><strong>when corrected of those impressions, people’s view of the other party improved</strong></li>
      <li>and that those mis-perceptions are most skewed among those who look at political news most often</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Conclusion: today we may have a shift in how we understand people’s stance for their political party/competition between them</p>
</blockquote>

<ul>
  <li>older models: members of each party have <em>many overlapping interest/identities</em> (but conflicts/argues over a few ideologies)</li>
  <li>today: parties are more not very related/very polarized/huge difference between political groups
    <ul>
      <li>perhaps due to <strong>political sorting</strong> = people are much more polarized = members inside the same group aligned / <strong>hate other groups</strong></li>
      <li>also results in <strong>stronger partisanship</strong> = greater/more negative affects against other parties</li>
      <li>partisan identification and sense of difference has increased</li>
      <li>but this increasing political difference may come at a cost (TBD)</li>
    </ul>
  </li>
</ul>

<h2 id="median-voter-theorem">Median Voter Theorem</h2>

<blockquote>
  <p><strong>Median Voter Theorem</strong>: in a majority-rule voting system, the outcome will be determined by the preferences of the median voter.</p>
</blockquote>

<p>Why would this happen? Consider the following leader position, where <code class="language-plaintext highlighter-rouge">x</code> goes with the Left and <code class="language-plaintext highlighter-rouge">o</code> goes with the right</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xxxLxxxoooRooo
</code></pre></div></div>

<p>assuming that</p>

<ul>
  <li>candidates <code class="language-plaintext highlighter-rouge">L</code> and <code class="language-plaintext highlighter-rouge">R</code> want to win electrions</li>
  <li>people vote for candidates whose policy is <em>closest</em> to their ideal</li>
</ul>

<p>then you have on a <strong>single dimension</strong></p>

<ul>
  <li>
    <p>if <code class="language-plaintext highlighter-rouge">L</code> moves further from the center then <code class="language-plaintext highlighter-rouge">L</code> will be <strong>losing</strong>: <code class="language-plaintext highlighter-rouge">Lxxxx-ooooRooo</code></p>
  </li>
  <li>
    <p>if <code class="language-plaintext highlighter-rouge">R</code> moves to the closer to the center then <code class="language-plaintext highlighter-rouge">R</code> will be winning: <code class="language-plaintext highlighter-rouge">xxxLxxooRooooo</code></p>
  </li>
  <li>
    <p>therefore, both parties would need to move closer to the center so that the other party cannot gain advantage. This results in</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xxxxxxLRoooooo
</code></pre></div>    </div>
  </li>
</ul>

<p>therefore, rational parties will converge to center = <mark>2-party competition results in moderate parties</mark></p>

<p><strong>How well does this describe actual elections in US? Is this model too simplified?</strong></p>

<ul>
  <li>
    <p>but of course, the above assumes that voter is still enthusiastic to vote despite this sad situation, etc.</p>
  </li>
  <li>At district level this is plausible, but there are at least two violations
    <ul>
      <li>candidates of parties are selected by the <em>primary</em> system = primary candidates really diverge from each other</li>
      <li>elections in states influence each other</li>
      <li>as seen last time (<a href="#Who identifies with which party?">Who identifies with which party?</a>), people often vote <em>not because</em> of similar policies to their idea, but if they heard of them, or if they share similar physical identity, etc.</li>
    </ul>
  </li>
  <li>in general, many voters might not vote based on “whose policy is <em>closest</em> to their ideal”, but
    <ul>
      <li>if they have heard of them</li>
      <li>if they have endorsement/trusted sources support them (i.e. weird reason people vote)</li>
      <li>if they share identity with the voters (i.e. weird reason people vote)</li>
      <li>and candidate recruits (e.g. party organizations) can <em>manipulate</em> all these things in a district</li>
    </ul>
  </li>
  <li>this means, <strong>in reality, to recover MVT</strong> you would need at least:
    <ul>
      <li>name-recognition not too skewed towards a candidate</li>
      <li>endorsement not to skewed</li>
      <li>candidate ideneity not too skewed</li>
    </ul>
  </li>
</ul>

<h2 id="a-theory-of-parties-as-interest-group-coalitions">A Theory of Parties as Interest Group Coalitions</h2>

<p><strong>Who affects</strong> the political parities?</p>

<ul>
  <li>
    <p><strong>politician</strong> centered theory: politicians (when selected in office) drive changes in political parties</p>
  </li>
  <li>
    <p><strong>interest group</strong> centered theory: interest groups exert huge influence on <em>who</em> gets elected during primaries (e.g. fund a guy to be prominent) = can exert influence in political parties</p>
  </li>
</ul>

<blockquote>
  <p><strong>Provisional conclusions</strong></p>

  <ul>
    <li>interest groups play a major role shaping the political party, especially in affecting the <em>candidate</em> before he/she is elected in office</li>
    <li>once politicians are in office, they themselves surely exert large influence the political party</li>
  </ul>
</blockquote>

<p>(Democratic) Parties mimic levels of national government</p>

<ul>
  <li>you have democratic party in Manhattan; democratic party in New York; democratic party in the National level
    <ul>
      <li>in theory, all those political parties should be <strong>independent</strong> = e.g. Manhatton GoP more liberal than Bonner Counter GoP.</li>
      <li>this held true mostly in the past</li>
      <li>but since the 1970s, state parties have become much more <em>similar</em> = they don’t respond to voters in their district, but <strong>aimed at a national agenda</strong> = “How national parties transformed state parties”</li>
    </ul>
  </li>
</ul>

<p>Why might interest groups and parties shift policy making to <strong>focus on a state level</strong> instead of a national level?</p>

<ul>
  <li>[+] some policies only apply to certain states
    <ul>
      <li>no easy “one size fits all” given variantions betweem states</li>
    </ul>
  </li>
  <li>[+] you can get more done in state houses than national houses = more money efficient</li>
  <li>[+] many folks do not pay attention to issues on the state level
    <ul>
      <li>in theory, the lower the government (e.g. local/state) the more attention people pay</li>
      <li>but empirically many people do the reverse: people don’t pay much attention to state levels but more to national affairs</li>
      <li>hence easier to get (bad) policies passed if nobody pays attention</li>
    </ul>
  </li>
</ul>

<h3 id="federalism-and-parties">Federalism and Parties</h3>

<p>What are some <strong>evidences</strong> that lower level political parties are do not listen much to the public/become more centralized to <strong>national agenda</strong>?</p>

<ul>
  <li>
    <p>since 1970s, <em>states</em> are taking in more policy making spending</p>
  </li>
  <li>
    <p>state parties are becoming more <em>similar to each other</em> = responding more to a national agenda than public opinon</p>

    <ul>
      <li>evidence = direction of <strong>policy change</strong> is increasingly predicted by <strong>whether or not the party has uniform control</strong></li>
      <li>graph: if a party has uniform control, what would the average state policy look like?</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173436164.png" alt="image-20230424173436164" style="zoom: 33%;" /></p>

    <ul>
      <li>
        <p>findings 1: before, states “do what they want” hence averages out to zero despite national control; nowadays, which party is taking charge greatly shapes <em>individual state’s policies</em></p>
      </li>
      <li>
        <p>findings 2: some exceptions are civil rights and criminal justice</p>

        <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173529209.png" alt="image-20230424173529209" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>conclusion = national party control is a <strong>strong predictor of policies on a state level</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p>test the counter-argument: do changes/trend in state policy follow changes/trend in public opinion policy?</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173602178.png" alt="image-20230424173602178" style="zoom:50%;" /></p>

    <ul>
      <li>most policy areas doesn’t work = state policies affected by other factors
        <ul>
          <li>abortion: public opinion looks flat, but significant shits in policies;</li>
          <li>civil rights: the other way around; in both cases the two look unrelated</li>
        </ul>
      </li>
      <li>but there are ones that works, e.g. LGBTQ</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Conclusion: divergence in state level policy is <strong>not explained by changes in public opinion</strong>, but increasingly by <strong>partisan control</strong>.</p>
</blockquote>

<p>Why? <strong>Interest groups</strong> involvement in <strong>parties</strong> can be one explanation as a strong force</p>

<ul>
  <li>
    <p>regular donors are more extreme (than ordinary donors)</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173810661.png" alt="image-20230424173810661" style="zoom:33%;" /></p>

    <p>where those who donates to Interest Groups (IG) and/or legislation are more extreme in their political ideology (most liberal and most conservative)</p>
  </li>
  <li>
    <p>contacted with the legislators more (than ordinary donors)</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173855509.png" alt="image-20230424173855509" style="zoom:33%;" /></p>

    <p>similar to above, those who donated to IG and/or legislations contacted more</p>
  </li>
  <li>
    <p>better predictor of legislative ideologies (than ordinary donors)</p>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230424173930676.png" alt="image-20230424173930676" style="zoom:33%;" /></p>

    <p>there is a strong association between <strong>identity of donors and characteristics of the candidates</strong></p>
  </li>
</ul>

<blockquote>
  <p>Conclusion:</p>

  <ul>
    <li>
      <p>parties responsiveness seem to <strong>follow money</strong> (from specific sources), not public opinion = political party evolving with interest group system</p>
    </li>
    <li>
      <p>party system being more <strong>nationalized</strong> = <strong>polarized</strong></p>
      <ul>
        <li>nationalized parties <em>at state level</em> mean polarized policies not driven by state-level opinion anymore</li>
        <li>MVT theorem disrupted = left and right position being more affected by <strong>other factors than just policy preferences</strong></li>
        <li>state has less freedom to find policy through experimentation = <strong>less creative</strong> policy making</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h1 id="interest-groups-and-lobbying">Interest Groups and Lobbying</h1>

<blockquote>
  <p><strong>Interest groups</strong>: <mark>individuals</mark> would band together in <mark>an attempt to use government in their favor</mark>. In general formal association of individuals or organizations that attempt to influence government decision-making and/or the making of public policy.</p>
</blockquote>

<p>How is interest groups different from political parties?</p>

<ul>
  <li>interest groups are much <strong>narrower</strong> in their pursuits. e.g. focused on areas like taxes, the environment, and gun rights or gun control, or their membership is limited to specific professions.</li>
  <li><strong>do not</strong> function primarily to <strong>elect candidates</strong> under a certain party label or to directly control the operation of the government</li>
</ul>

<p>Categories of IG</p>

<ul>
  <li><strong>trade association</strong>: any economically based grouping. consumer brands association (e.g. for food regulation)</li>
  <li><strong>labor union</strong>: AFL-CIO, FOP, highly active in politics</li>
  <li><strong>individual membership associations</strong> (grassroot): random people who joined that cared abotu the same thing</li>
  <li><strong>professional associations</strong>: membership is individual, and goal is to participate in that occupation group. AMA, ABA</li>
</ul>

<p>What do IGs do?</p>

<ul>
  <li><strong>elections</strong>: mobilize voters to vote for campaigns, and act as <mark>endorsements for candidates</mark> (see previous sections)</li>
  <li><strong>legislatures</strong>: help draft bills, etc.</li>
  <li><strong>courts</strong>: if something bad happened, threatening to sue; file briefs when brought by others</li>
  <li><strong>bureaucracies</strong>: certain professional licensing run by interest group</li>
  <li>A very diverse branching = IGs use every aspect of the US political system to achieve their goal.</li>
</ul>

<p>Imagine you are a legislator and you heard the following from a lobbyist:</p>

<p><em>“Our research indicates the accessibility of guns is leading to unnecessary deaths”</em></p>

<ul>
  <li>if you heard this from a national rifle association v.s. Mom’s association of Gun sense…; former more convincing</li>
</ul>

<p><em>“Our research indicates that carbon emissions need to be significantly curtailed in the next few yeras or the US will face significant ecnomic har from climate change”</em></p>

<ul>
  <li>if you heard this from greenpeace v.s. american petroleum association; latter more convincing</li>
</ul>

<blockquote>
  <p>Intuition: <strong>source bias</strong> makes some statement harder to believe = less persuasive if it comes from IG who anyway wanted you to do this.</p>
</blockquote>

<p>But then <strong>why spend money to lobbying</strong>, if everyone knows what your group wants?</p>

<ul>
  <li>persuade those who <strong>disagree</strong> with you: often hard</li>
  <li>convince those who <strong>agree</strong> with you to <strong>focus on the issues</strong> you care the most.
    <ul>
      <li>i.e. subsidizing the efforts of those who agree with you = reduce your efforts</li>
      <li>lobbists do more things than persuasion = providing resource subsidies for people who want to sue</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em> Colorado Juvenile Defend Center</p>

<ul>
  <li>2012 they found that 45% kids charge with offenses had no council representing them</li>
  <li>2013 contacted sympathetic legislator, convinced them to form an interim committee</li>
  <li>2014 participated in negotiations and re-drafting of bill = the law changed <strong>because of this IG’s efforts</strong></li>
</ul>

<p>But given this effort, does this <em>process correct for poor representation through elections</em>, or <em>subvert democracy of people</em>? (interesting arguments to consider)</p>

<hr />

<p>Lobbying at the state level: (i.e. groups that lobby)</p>

<ul>
  <li>municipal leagues</li>
  <li>county commissioner associations</li>
  <li>school board associations</li>
  <li>local government: e.g. Colorado wanted bars to be closed at 2am. Then people and local government/mayor lobbied state legislators for change.</li>
  <li>etc.</li>
</ul>

<p>California require every lobby group to file forms. From data collected, it is found that</p>

<ul>
  <li>
    <p>local government spent by far the most among IGs in California ;</p>
  </li>
  <li>many cities do lobby, and especially in California cities lobby a lot</li>
  <li>and it is not just cities, over 500 cases are lobbied by counties, and over 1500 from special districts.</li>
  <li>lobbying at the state government is most common, at national government reasonably common</li>
</ul>

<blockquote>
  <p><strong>On local lobbying:</strong></p>

  <ul>
    <li>
      <p>it is a pervasive practice, and targets a lot on state government</p>
    </li>
    <li>
      <p>has the potential to compound disadvantage for poor regions = don’t have money for IGs to toss around</p>
    </li>
  </ul>

  <p><strong>In general, lobbying</strong></p>

  <ul>
    <li>is baked in to democracy = is a <strong>crucial part of US politics</strong></li>
    <li>but has a trade-off between the influence of interest groups/anti-democracy</li>
    <li>interest group environment may not be a leveling playing field
      <ul>
        <li>not every issues have an interest group</li>
        <li>IGs have <strong>different resource distribution</strong>; some group has more power/money = features of the group can also affect if how far can you get this issue solved</li>
        <li>Strolovitch finds that organizations are substantially less active when it comes to issues affecting <strong>disadvantaged subgroups</strong> than they are when it comes to issues affecting more advantaged subgroups.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>e.g. current measures to restrict IG’s power:</p>

<ul>
  <li>requires lobbying data be recorded and <strong>disclosed</strong></li>
  <li><strong>cooling-off period</strong> = laws that require a person who works for a legislator cannot lobby for two years after they leave; to prevent them lobbying on things that are advantageous for them (but they can ask others to lobby)</li>
  <li>campaign donation limits</li>
  <li>tax disclosure laws</li>
</ul>

<h2 id="collective-action-and-free-riding-in-igs">Collective Action and Free Riding in IGs</h2>

<p>As mentioned before, an example of IGs in US is the labor organization/unions. But think about this:</p>

<ul>
  <li>The benefits sought by <strong>unions</strong>, such as higher wages, collective bargaining rights, and safer working conditions, are often enjoyed by <strong>all workers regardless of whether they are members</strong>.</li>
  <li>therefore, <strong>free riders</strong> can receive the benefit of the pay increase without helping defray the cost by paying dues, attending meetings or rallies, or joining protests,</li>
</ul>

<blockquote>
  <p>If <strong>free riding</strong> is so prevalent, why are there so many interest groups and <strong>why is interest group membership so high</strong> in the United States?</p>
</blockquote>

<p>One explanation is that free riding is overcame by:</p>

<ol>
  <li>Groups with <strong>financial</strong> resources/patrons outside the group have an advantage in mobilizing in that they can <strong>offer incentives</strong> or hire a lobbyist.</li>
  <li>opinions within <strong>smaller</strong> IGs may be
    <ul>
      <li><em>easier</em> to reach consensus</li>
      <li>if you don’t voice your opinion and the policy is not what you liked, high chance your <em>preference will not be taken into account</em></li>
      <li>easier to <em>spot if you are not contributing</em></li>
    </ul>
  </li>
  <li>Group <strong>leaders</strong> also play an important role, for example
    <ul>
      <li>offer <strong>material incentives</strong>, which are tangible benefits of joining a group. (AARP, for example, offers discounts on hotel accommodations and insurance rates for its members)</li>
      <li>offer <strong>solidary incentives</strong>, which provide the benefit of joining with others who have the same concerns or are similar in other ways, as people are naturally drawn to others with similar concerns.</li>
    </ul>
  </li>
  <li><strong>disturbance theory</strong>: why groups mobilize <strong>due to an event</strong> in the political, economic, or social environment = people will naturally join groups in response to <strong>disturbances</strong>. for example
    <ul>
      <li>in 1962, Rachel Carson published <em>Silent Spring</em>, a book exposing the dangers posed by pesticides such as DDT $\to$ many individuals start to worry about environment and dangers of pesticides $\to$ an increase in both the number of environmental interest groups, such as Greenpeace and American Rivers,</li>
      <li>In May 2020, George <em>Floyd</em> died shortly after police officer Derek Chauvin leaned his knee on Floyd’s neck for nine and half minutes, while Floyd was handcuffed and laying face down on the ground $\to$ massive protests across US</li>
    </ul>
  </li>
</ol>

<h1 id="the-media">The Media</h1>

<p>Freedom of the press and an independent media are important dimensions of a liberal society, and the media can have a huge impact in how we see the society today.</p>

<p>Some basic terms:</p>

<ul>
  <li>The collection of all forms of media that communicate information to the general public is called <strong>mass media</strong> (e.g. television, print, radio, and Internet.)</li>
  <li>The Internet also facilitates the flow of information through <strong>social media</strong>, which allows users to instantly communicate with one another and share with audiences that can grow exponentially.</li>
</ul>

<blockquote>
  <p>Regardless of where we get our information, the various media avenues available today, versus years ago, make it <strong>much easier for everyone to be politically and socially engaged</strong>.</p>
</blockquote>

<p>But who controls the media we rely on? Suprisingly today, most media is controlled by a limited number of conglomerates (a collection of companies, organizations, and media networks)</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230503164855628.png" alt="image-20230503164855628" style="zoom: 25%;" /></p>

<p>what does this mean? When a media conglomerate has <strong>policies or restrictions</strong>, they will apply to all stations or outlets under its ownership, potentially limiting the information citizens receive.</p>

<ul>
  <li>This raise the question whether the media still operate as an independent source of information. Is it possible that <strong>corporations and CEOs now control the information flow</strong>, making profit more important than the impartial delivery of information?</li>
  <li>The reality is that media outlets, whether newspaper, television, radio, or Internet, are <strong>businesses</strong>, they need to raise revenue! How do they get revenue? Get advertising and sponsors. How do they get them? You need active viewers. In the end, <strong>what attracts viewers and advertisers is what survives</strong>.</li>
</ul>

<h2 id="functions-of-the-media">Functions of the Media</h2>

<p>So what do those media do? What can they control?</p>

<ul>
  <li><strong>help maintain democracy</strong> and keeps the government accountable for its actions, even if a branch of the government is reluctant to open itself to public scrutiny.</li>
  <li>
    <p><strong>promote the public good</strong> by offering a platform for public debate and improving citizen awareness. Network news informs the electorate about national issues, elections, and international news.</p>
  </li>
  <li><strong>agenda setting</strong>, which is the act of choosing which issues or topics deserve public discussion. e.g. In the spring of 2015, when the Dominican Republic was preparing to exile Haitians and undocumented (or under documented) residents, major U.S. news outlets remained silent.</li>
</ul>

<blockquote>
  <p>Large network newscasts and major newspapers are still more powerful at <mark>initiating or changing a discussion</mark>.</p>
</blockquote>

<h2 id="regulating-the-media">Regulating the Media</h2>

<p>The approval of the First Amendment, as a part of the Bill of Rights, demonstrated the framers’ belief that a <strong>free and vital press was important</strong> enough to protect = serves as the basis for the political freedoms of the United States. It said:</p>

<blockquote>
  <p>“Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the government for a redress of grievances.”</p>
</blockquote>

<p>Although the media are independent participants in the U.S. political system, their liberties are not absolute and <mark>there are rules they must follow</mark>.</p>

<ol>
  <li>First, the media do not have the right to commit <strong>slander</strong>, speak false information with an intent to harm a person or entity, or <strong>libel</strong>, print false information with an intent to harm a person or entity. But it seems that newspaper has been doing this a lot?
    <ul>
      <li>libel and slander occur only in cases where false information is presented as <strong>fact</strong>.</li>
      <li>When editors or columnists write <em>opinions</em>, they are protected from many of the libel and slander provisions because they are not claiming their statements are facts.</li>
      <li>the defamed individual or company to bring a <em>lawsuit</em>, and the courts have different standards depending on whether the claimant is a private or public figure.</li>
    </ul>
  </li>
  <li>media have only a limited right to publish material the <strong>government says is classified</strong>.
    <ul>
      <li>If the journalist calls the White House or Pentagon for quotations on a classified topic, the president may order the newspaper to stop publication in the interest of national security. The <strong>courts</strong> are then asked to rule on what is censored and what can be printed.</li>
      <li>e.g. in 19721, US gov sued New York Times and Washington Post to stop release information from a classified study of the Vietnam war. In the end, the court gave the newspapers the right to publish much of the study, but revelation of troop movements and the names of undercover operatives are some of the few approved reasons for which the government can stop publication or reporting.</li>
    </ul>
  </li>
  <li>television and radio broadcasters are monitored by both the courts and a <strong>government regulatory commission</strong>.
    <ul>
      <li>Radio Act of 1927 was the first attempt by Congress to regulate broadcast materials.</li>
      <li>Communications Act of 1934 replaced the Radio Act and created a more powerful entity to monitor the airwaves—a seven-member <strong>Federal Communications Commission (FCC)</strong> to oversee both radio and telephone communication.</li>
      <li>the idea is to ask radio/TV stations to apply for <strong>licenses</strong>, which is granted only if stations follow rules about limiting advertising, providing a public forum for discussion, and serving local and minority communities. The licensing requires
        <ul>
          <li><strong>equal-time rule</strong>: registered candidates running for office must be given equal opportunities for airtime and advertisements at non-cable television and radio stations beginning forty-five days before a primary election and sixty days before a general election.</li>
          <li>While the idea behind the equal-time rule is fairness, it may <em>not apply</em> to supporters of that candidate or of a cause. Hence, there potentially may be a loophole in which broadcasters can give free time to just one candidate’s supporters.</li>
          <li><strong>indecency regulations</strong>: limit indecent material and keep the public airwaves free of obscene material.
            <ul>
              <li>However, broadcasters can show indecent programming or air profane language between the hours of 10 p.m. and 6 a.m.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>net neutrality</strong>: required internet service providers to give everyone equal access to their services and disallowed biased charging of internet access fees.</li>
    </ul>
  </li>
  <li>and many more</li>
</ol>

<h2 id="impact-of-the-media">Impact of the Media</h2>

<p>Since media release information consumed by the people, this information may <strong>affect what we think and the actions we take.</strong></p>

<p>There are many theories debating how much impact the media has, e.g.</p>

<ul>
  <li>Lippmann’s statements led to the <strong>hypodermic theory</strong>, which argues that information is “shot” into the receiver’s mind and readily accepted.</li>
  <li><strong>minimal effects theory</strong>, which argues the media have little effect on citizens and voters. Information is transmitted in two steps, with one person reading the news and then sharing the information with friends.</li>
  <li><strong>cultivation theory</strong>, hypothesized that media develop a person’s view of the world by presenting a perceived reality, and the media can set norms for readers and viewers by choosing what is covered or discussed.</li>
</ul>

<p>and some common techniques the Media use to sway peoples opinions:</p>

<ul>
  <li><strong>framing</strong>: the creation of a narrative, or context, <em>for</em> a news story.</li>
  <li><strong>priming</strong>: when media coverage predisposes the viewer or reader to a particular perspective on a subject or issue.
    <ul>
      <li>e.g. If a newspaper article focuses on unemployment, struggling industries, and jobs moving overseas, the reader will have a negative opinion about the economy. If then asked whether they approve of the president’s job performance, the reader is primed to say no.</li>
    </ul>
  </li>
</ul>

<p>In the end, the consensus among observers is that media have some effect, even if the effect is subtle. Some <mark>important ways</mark> include</p>

<ul>
  <li>effects on governance and caompaigns</li>
  <li>effects on the society</li>
</ul>

<h3 id="medias-effect-on-governance-and-campaigns">Media’s Effect on Governance and Campaigns</h3>

<p>Some historical examples:</p>

<ul>
  <li>in 1972, candidates with the most media coverage build momentum and do well in the first few primaries and caucuses.</li>
  <li>In the 1980s, campaigns learned that tight control on candidate information created more favorable media coverage.</li>
  <li>In 1992, both Bush’s and Bill Clinton’s campaigns maintained their carefully drawn candidate images by also limiting photographers and television journalists to photo opportunities at rallies and campaign venues.</li>
</ul>

<p>However, <strong>campaign coverage now focuses on shallow reports</strong></p>

<ul>
  <li>i.e. colorful personalities, strange comments, lapse of memories, and embarrassing revelations are more likely to get air time than the candidates’ issue positions.</li>
  <li>i.e. citizens want to see updates on the race and electoral drama, not boring issue positions or substantive reporting.</li>
</ul>

<p>As a result, all these factors have likely led to the shallow press coverage we see today, sometimes dubbed <strong>pack journalism</strong> because journalists follow one another rather than digging for their own stories.</p>

<ul>
  <li>
    <p>In 1968, the average sound bite from Richard Nixon was 42.3 seconds, while a recent study of television coverage found that sound bites had decreased to only eight seconds in the 2004 election.</p>
  </li>
  <li>
    <p>that study also found the news showed images of the candidates, but for an average of only twenty-five seconds while the newscaster discussed the stories.</p>
  </li>
  <li>
    <p><strong>media’s discussion of campaigns has also grown negative</strong>.</p>

    <ul>
      <li>During the 2012 campaign, seventy-one of seventy-four MSNBC stories about Mitt Romney were highly negative</li>
      <li>FOX News’ coverage of Obama had forty-six out of fifty-two stories with negative information</li>
      <li>major networks—ABC, CBS, and NBC—were somewhat more balanced, yet the overall coverage of both candidates tended to be negative.</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230504013541363.png" alt="image-20230504013541363" style="zoom: 25%;" /></p>
  </li>
</ul>

<blockquote>
  <p>Due in part to the lack of substantive media coverage, campaigns increasingly use <strong>social media</strong> to relay their message.</p>
</blockquote>

<p>So this includes:</p>

<ul>
  <li>using Facebook, Twitter, and YouTube accounts to provide information to voters. The best example would be Donald Trump, who took social media posts to a new level, both in terms of the number of posts and the intensity.</li>
  <li>Yet, on social media, candidates still need to combat negativity.</li>
</ul>

<h3 id="medias-effects-on-society">Media’s Effects on Society</h3>

<blockquote>
  <p>The media choose what they want to discuss. This <strong>agenda setting</strong> creates a reality for voters and politicians that affects the way people think, act, and vote.</p>
</blockquote>

<p>Even if the crime rate is going down, for instance, citizens accustomed to reading stories about assault and other offenses still perceive crime to be an issue. More dominant examples today include, especially on issues of <mark>race and gender</mark>:</p>

<ul>
  <li>
    <p>that local news shows were more likely to show pictures of <strong>criminals</strong> when they were African American,</p>
  </li>
  <li>Network news similarly <strong>misrepresents the victims of poverty</strong> by using more images of African Americans than White people in its segments.</li>
  <li>media coverage of <strong>women</strong> has been similarly biased.
    <ul>
      <li>The media’s historically uneven coverage of women continues in its treatment of <mark>women candidates</mark>. Early coverage was sparse.</li>
      <li>Women were often seen as a <em>novelty</em> rather than as serious contenders who needed to be vetted and discussed.</li>
    </ul>
  </li>
</ul>

<h1 id="side-notes">Side Notes</h1>

<p>A few topics that is not on the US politics, but related</p>

<h2 id="data-and-descriptive-representation">Data and Descriptive Representation</h2>

<p>Republicans become more conservative over time.</p>

<p>(vertical graph here)</p>

<p>How do you rate if a policy is conservative v.s liberal?</p>

<ul>
  <li>
    <p>a few easy ones:</p>

    <ul>
      <li>
        <p>tax rates</p>
      </li>
      <li>
        <p>military spending</p>
      </li>
    </ul>
  </li>
  <li>
    <p>some hard ones</p>

    <ul>
      <li>economic regulation = republicans are not all against it, but certain kinds of it</li>
    </ul>
  </li>
</ul>

<p>On what basis do you decide <strong>if a person is liberal or conservative</strong>?</p>

<ul>
  <li>self-identification</li>
  <li>policy goals, using fixed defintions</li>
  <li>who they vote with</li>
</ul>

<p><strong>DW-Nominate:</strong> spatially representing where people stand on the political spectrum, and explain what factor causes the clustering</p>

<ul>
  <li>
    <p>procedure sketch:</p>

    <ul>
      <li>takes all members and their votes for 2 yeras</li>
      <li><mark>who they vote with</mark> and who they vote against (similarity embedding between each other)</li>
    </ul>
  </li>
  <li>
    <p>found that</p>

    <ul>
      <li>just using the economic dimension + race can almost explain the clustering</li>
      <li>the clustering is changing over time, being now very <strong>polarized</strong></li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230228193056148.png" alt="image-20230228193056148" style="zoom:33%;" /></p>

    <ul>
      <li>the polarization movement is <strong>asymmetric</strong>: <strong>republican</strong> have moved towards polarization/clustering earlier</li>
      <li><strong>high predictability of political decision</strong>/policy as groups are very polarized = less variety in a pro-econ people voting for a con-econ policy</li>
    </ul>
  </li>
</ul>

<p><strong>Polarization as a feature of congress</strong>: more agreement inside group and aggression outside. This</p>

<ul>
  <li>makes legislation a lot harder since we need votes across the entire house</li>
  <li>hence explains why policies are polarized</li>
  <li>but note that the definition/baseline changes over time = e.g. democrats worried by econ problems could have voted with republicans</li>
</ul>

<p>To note: important to think about descriptive representation from different <strong>dimensions</strong>:</p>

<p>e.g. consider the following data representing women in congress</p>

<p><img src="/lectures/images/2023-05-11-POLS1201_Intro_to_American_Politics/image-20230227170321928.png" alt="image-20230227170321928" style="zoom:33%;" /></p>

<ul>
  <li>doesn’t show the proportion of how is running and how is a candidate</li>
  <li>other dimension include profession of those women, economic background, etc.</li>
</ul>

<h2 id="reading-empirical-research-paper">Reading Empirical Research Paper</h2>

<blockquote>
  <p>What are the goals of quantitivative emprical research?</p>

  <ul>
    <li><strong>reduce</strong> the number of competing explanations (i.e. which existing theories might be wrong)</li>
    <li><strong>refine</strong> theories that mostly work (i.e. we had a good theory, but we need to patch it a bit to explain this as well)</li>
  </ul>
</blockquote>

<p>Therefore, we will first</p>

<ul>
  <li><strong>experiments</strong>: good at identifying causal effects, have controlled variables, but it is very difficult to know if your variable is indeed the cause</li>
  <li><strong>observational studies</strong>: a big dataset, but is that correlation driven by stuff in the dataset or something else</li>
</ul>

<p>We want:</p>

<ul>
  <li><strong>explanation</strong> for a particular event, e.g. <em>rule out</em> theories</li>
  <li><strong>prediction</strong> of what will happen in the future</li>
</ul>

<hr />

<p>Therefore, for quantitative empirical research paper will be organized like:</p>

<ol>
  <li><strong>introduction</strong>: previews of the arguments to be made in this paper</li>
  <li><strong>theory</strong>: situates question and aguments
    <ul>
      <li>also contains justifications why they used this particular approach</li>
      <li>also look for defensive language = reviewers had criticism on those</li>
    </ul>
  </li>
  <li><strong>data and methods</strong></li>
  <li><strong>results/discussion</strong>: here is what we think we have learned</li>
</ol>

<blockquote>
  <p>Therefore, <strong>just with abstract and introduction</strong>, you can get a descent summary of what is happening</p>

  <ul>
    <li>but this will only give you what the author <em>wants you to remember</em>. to critique you will have to read the entire paper</li>
  </ul>
</blockquote>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Note that a lot of content comes from relevant publications and the OpenStax book (Krutz, G., &amp; Waskiewicz, S. (2021). American Government 3e. Houston, Texas: OpenStax.)]]></summary></entry><entry><title type="html">UN1494 Intro to Exp</title><link href="/lectures/2022@columbia/UN1494_Intro_to_Exp.html/" rel="alternate" type="text/html" title="UN1494 Intro to Exp" /><published>2023-05-11T00:00:00+00:00</published><updated>2023-05-11T00:00:00+00:00</updated><id>/lectures/2022@columbia/UN1494_Intro_to_Exp</id><content type="html" xml:base="/lectures/2022@columbia/UN1494_Intro_to_Exp.html/"><![CDATA[<h1 id="introduction-and-logistics">Introduction and Logistics</h1>

<ul>
  <li><strong>Lecturer</strong>: Emily Tiberi ect2158@columbia.edu. Office hours: TBA or by appointment</li>
  <li><strong>Expectations</strong>
    <ul>
      <li>your data analysis should be on <code class="language-plaintext highlighter-rouge">Python</code>  or <code class="language-plaintext highlighter-rouge">Mathematica</code>, but <em>not</em> <code class="language-plaintext highlighter-rouge">Excel</code></li>
      <li>Background physics: have taken any physics class at college level</li>
      <li>Time commitment: expected to spend on average <em>less than 10 hours on your lab report each week</em></li>
    </ul>
  </li>
  <li><strong>Absence Policy</strong>
    <ul>
      <li>Attendance includes lab participation and lab report submission</li>
      <li>You are allowed two excused absences and one unexcused absence</li>
    </ul>
  </li>
  <li><strong>Lab sessions</strong>:
    <ul>
      <li>Each lab session will begin with a brief recap with the TA.</li>
      <li>Your lab group will collect raw data</li>
      <li>There will be in-lab discussions. Work in groups, analyze data and answer the discussion questions.</li>
      <li>You will have <mark>a week</mark> to finish the lab report</li>
      <li>bring your laptop, recommend recording <em>raw data</em> using excel and export</li>
    </ul>
  </li>
  <li><strong>Lab report</strong>: your lab report should be your own, but all the others will be/<em>can be</em> collaborative
    <ul>
      <li>your first week lab report will only be personal feedback, no grade</li>
      <li>there <mark>will be a rubric</mark> for lab reports</li>
    </ul>
  </li>
  <li><strong>Grading</strong>: no quizzes, 90% lab report, and 10% in-lab discussion and participation (discussion with the TA)</li>
</ul>

<h1 id="scientific-writing">Scientific Writing</h1>

<p>Readers interpret prose more easily when it flows smoothly… From background rationale conclusion</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230123150215405.png" alt="image-20230123150215405" style="zoom:33%;" /></p>

<ul>
  <li>Don’t force the reader to figure out your logic – clearly state the rationale.</li>
  <li>Clear writing is also concise writing. The report should be fairly brief (<mark>up to 4 pages</mark>).</li>
</ul>

<p>Your report should look like:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230123150517402.png" alt="image-20230123150517402" style="zoom:50%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>“<strong>References</strong>” most of the time empty</p>
  </li>
  <li>
    <p><strong>Abstract</strong> can be there, but is optional. For example</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230123150706887.png" alt="image-20230123150706887" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>the <strong>introduction</strong> should usually be <mark>no more than half a page in single column</mark></p>

    <ul>
      <li>describe the research question, why it is important, and what approaches (briefly) you used (optional: briefly mention the results if you want)</li>
    </ul>
  </li>
  <li>
    <p><strong>method</strong>: you really want to talk about your setup, including your apparatus and instruments used, any techniques, etc.</p>

    <ul>
      <li>this <em>should be brief</em> in general</li>
    </ul>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230123151119020.png" alt="image-20230123151119020" style="zoom:50%;" /></p>
  </li>
  <li>
    <p><strong>results and analysis</strong>: the <mark>most important section</mark> in your report. You should <mark>succinctly</mark> give</p>

    <ul>
      <li>data obtained, graphs and tables</li>
      <li>how you calculated those quantities (if not obvious)</li>
      <li>comments on uncertainties. Why are certain error bars so big?</li>
      <li>is your result good?</li>
    </ul>
  </li>
  <li>
    <p><strong>conclusions</strong>: should be short. Contain your interpretation of the result, what you expected</p>
  </li>
</ul>

<p>Other notes for writing <mark>these reports</mark></p>

<ul>
  <li>example, annotated lab reports are also provided in Courseworks</li>
</ul>

<h1 id="error-analysis">Error Analysis</h1>

<blockquote>
  <p>When we measure any quantity, we <em>cannot</em> expect to measure it exactly = we will have <strong>errors/uncertainties</strong> in our experiment!</p>
</blockquote>

<p>Therefore, instead of giving each of your friend $2.3684$ slices of pizza, you might say:</p>

\[2.37 \pm 0.01 \quad \mathrm{slices}\]

<p>note that</p>

<ul>
  <li>you may want to <em>match</em> the number of significant figures in your number and your uncertainty</li>
  <li>usually keep up to 2 sig. figs.</li>
</ul>

<h2 id="type-of-errors">Type of Errors</h2>

<p>There are mainly two types of errors you get in your experiment</p>

<blockquote>
  <ul>
    <li><strong>Statistical/Random Errors</strong> = no going around this, but are <strong>quantifiable</strong>
      <ul>
        <li>Due to random fluctuations from measurement to measurement</li>
        <li>Can be reduced by taking <em>more measurements</em>, computing their <mark>mean and std</mark></li>
      </ul>
    </li>
    <li><strong>Systematic Errors</strong> = e.g. how you are measuring
      <ul>
        <li>Always bias the data in one direction</li>
        <li>“do you best” to identify and correct it</li>
        <li><em>Hard</em> to quantify</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>How do we quantify statistical/random errors?</p>

<ul>
  <li>
    <p>calculate mean $\bar{x}$, std $\sigma$</p>
  </li>
  <li>
    <p>but also Standard Error of the Mean $\sigma_\bar{x}$ = uncertainty we have on our measured average = <mark>uncertainty in your lab report</mark></p>

\[\sigma_\bar{x} = \frac{s}{\sqrt{N}}\]

    <p>because each time one more measurement is taken, your mean will change. So here is that measure of “<strong>precision in our measurement of the mean</strong>”</p>
  </li>
</ul>

<blockquote>
  <p>What happens when you have different measurements all with different precisions and you want to combine them into a unique result?</p>
</blockquote>

<p>For example, consider you are measuring a length with:</p>

<ul>
  <li>4 different rulers, each time measure once = $x_1\pm \sigma_1, x_2\pm \sigma_2,x_3 \pm \sigma_3,x_4 \pm \sigma_4$</li>
  <li>4 different rulers, with each ruler you measured a lot of times, hence four averages $\bar{x}<em>1 \pm \sigma</em>{1}, \bar{x}<em>2 \pm \sigma</em>{2}, \bar{x}<em>3 \pm \sigma</em>{3}, \bar{x}<em>4 \pm \sigma</em>{4}$</li>
</ul>

<p>And you want to report a <em>single length</em> measurement by a <strong>weigthed average</strong> of them. Then you can do this:</p>

\[\bar{x} = \frac{\sum_i w_i x_i}{\sum_i w_i}, \quad w_i = \frac{1}{\sigma_i^2}\]

<p>and then your <strong>error</strong> for this weighted mean is:</p>

\[\frac{1}{\sigma^2} = \sum_{i=1}^N \frac{1}{\sigma_i^2}\]

<p>(the more general way to derive those uncertainty is in the section: <a href="#Error Propagation">Error Propagation</a>)</p>

<h2 id="confidence-intervals">Confidence Intervals</h2>

<blockquote>
  <p>You want to see <strong>if</strong> a theoretically calculated = true average, $\mu$, <strong>falls at a certain distance from the statistical mean=your measured mean, $\bar{x}$</strong> in your experiments.</p>
</blockquote>

<p>The idea is, say, your measurement follows a <strong>Gaussian distribution</strong></p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130155232209.png" alt="image-20230130155232209" style="zoom:50%;" /></p>

<p>then we can “<em>estimate</em>” whether a certain result is reasonable:</p>

<ul>
  <li>if within $1\sigma_x$, then in general you can say it is good agreement</li>
  <li>within $1\sigma \sim 2\sigma$, it is consistent (but probably need more measures)</li>
  <li>beyond that it is not very good</li>
</ul>

<p>note that <strong>if the error bar is very large</strong>, it does not mean your measurement is automatically good = caveat to mention in your report</p>

<h2 id="error-propagation">Error Propagation</h2>

<p>Given some function $f(x,y,z)$, where each variable has uncertainty $x \pm \sigma_x$ etc, then:</p>

\[\sigma_f^2 = \left( \frac{\partial f}{\partial x} \right)^2 \sigma_x^2 + \left( \frac{\partial f}{\partial y} \right)^2 \sigma_y^2 + \left( \frac{\partial f}{\partial z} \right)^2 \sigma_z^2\]

<p>which is valid if the errors on x, y and z are uncorrelated.</p>

<blockquote>
  <p><strong>Note</strong>: you may want to automate this calculation in your python code.</p>
</blockquote>

<hr />

<p><em>Examples</em>:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130152506629.png" alt="image-20230130152506629" style="zoom: 33%;" /></p>

<p>where notice that</p>

<ul>
  <li>relative uncertainty = $1\pm (\sigma_x / \bar{x})$, absolute uncertainty $\sigma_x$</li>
  <li>relative uncertainty is useful when you are doing multiplication</li>
</ul>

<h2 id="graphical-analysis-of-data">Graphical Analysis of Data</h2>

<p>When plotting data, you want to show <em>at minimum</em> a) uncertainty = error bars; b) axis labels</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130152733661.png" alt="image-20230130152733661" style="zoom:50%;" /></p>

<p>But often you would also want to <strong>fit a linear model</strong> given this data, i.e.</p>

\[y_i = ax_i + b\]

<p>where your measurements are $(x_i, y_i)$. Obviously you will not be able to get a perfect fit, so we consider:</p>

\[\hat{y}_i = ax_i + b\]

<p>and hope to pass as close as possible to the highest number of points = <strong>linear regression</strong> = <strong>minimize least square errors</strong></p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130152902895.png" alt="image-20230130152902895" style="zoom:50%;" /></p>

<p>given the error = residuals $\Delta y_i = y_i - \hat{y}_i$, then you consider a</p>

\[\min_\beta J = \min_\beta \frac{1}{2N}\sum_{i=1}^N \left( y_i - \hat{y}_i \right)^2 = \min_\beta \frac{1}{2n}\sum_{i=1}^N \Delta y_i^2\]

<p>where $\beta = (a, b)$ in this example.</p>

<ul>
  <li>
    <p>the more general case if to of course consider linear algebra formulation</p>

\[\mathbf{y} = \mathbf{X\alpha + \beta} = \mathbf{[1,X] \beta} \equiv \mathbf{X\beta}\]

    <p>where $\beta = [\alpha_1, \alpha_2…,\alpha_n, \beta]$. This is so that you can more easily find your least square solution.</p>
  </li>
  <li>
    <p>one interpretation of <em>why</em> your model is not perfect is to consider <em>how $y$ are constructed</em>. Consider a process where you have a <strong>random Gaussian noise</strong> to generate those data</p>

\[\mathbf{y} = \mathbf{X\beta} + \epsilon\]

    <p>meaning the residuals you have $\Delta y_i = \epsilon_i$, so that if the your model is correct you should see</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130153058507.png" alt="image-20230130153058507" style="zoom:50%;" /></p>

    <p>which would be a good way to perform <strong>sanity check</strong>.</p>
  </li>
</ul>

<p>But there are cases when you see that plotting $\Delta y_i$ gives (not limited to only those two cases):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Case 1</th>
      <th style="text-align: center">Case 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130171539106.png" alt="image-20230130171539106" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230130171552081.png" alt="image-20230130171552081" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>in case 1 it could be you are fitting a <em>wrong model</em>, or you have some <em>systematic error</em></li>
  <li>in case 2, you might need a some form of a <em>weighted fit</em></li>
</ul>

<h1 id="lab-1">Lab 1</h1>

<p>Basically we are re-imagining Galileo’s ramp experiment, where basically we are considering:</p>

<ul>
  <li>motion under constant acceleration</li>
  <li>forces will affect the acceleration of a mass (newton’s second law)</li>
</ul>

<p>The apparatus we will be using is the Fritionless Air-Track</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230206150149419.png" alt="image-20230206150149419" style="zoom:50%;" /></p>

<p>where Timing is performed by Sonic Ranger.</p>

<ul>
  <li>Sonic Ranger: Measuring Velocity: It simply computes the average velocity over very small time intervals</li>
</ul>

<p>Then, one can then have a measure of the <strong>elasticity</strong> of a collision by computing the <strong>coefficient of restitution</strong></p>

\[e = \left| \frac{v_f}{v_i} \right| = \begin{cases}
1 &amp; \text{elastic}\\
&lt;1 &amp; \text{non elastic}
\end{cases}\]

<p>Three main parts</p>

<ol>
  <li>
    <p>Leveling the ramp</p>
  </li>
  <li>
    <p>Measuring the elasticity of the bumper (under constant velocity)</p>

    <ul>
      <li>
        <p>By comparing the velocity before and after, you can compute elasticity (note to propagate uncertainty to compute $e$)</p>
      </li>
      <li>
        <p>note that since $e = \vert v_f / v_i\vert$, then the uncertainty is:</p>

\[\sigma_e^2 = \frac{1}{v_i^2}\sigma_{v_f}^2 + \frac{v_f^2}{v_i^4}\sigma^2_{v_i}\]
      </li>
      <li>
        <p>will measurements/error of $v_i$ be dependent on $v_f$? In this experiment, theses two are treated as independent variables so intuitively no. But practically you need to be careful about this.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Measuring the acceleration due to gravity (under constant acceleration</p>

    <ul>
      <li>
        <p>When you incline the ramp, the size of the gravitational force along the ramp is proportional to the angle of the ramp incline</p>

        <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230206152530984.png" alt="image-20230206152530984" style="zoom:50%;" /></p>

        <p>for which you can estimate acceleration with both $v(t)=a_x t$ and $x(t)=…$.</p>
      </li>
      <li>
        <p>then, once you have measured $a_x$, you can compute your $g$ by</p>

\[a_x = g \sin(\theta) = g\frac{h}{L}\]
      </li>
      <li>
        <p>Do you expect steeper slopes to be more accurate than shallower ones? Why or why not?</p>
        <ul>
          <li>measurement readings error as there is less time when $h$ is high $\implies$ $a_x$ is high $\implies$ less time to collect data $\implies$ more error introduced</li>
          <li>if there is systematic uncertainty, then relative uncertainty of height <em>becomes less</em></li>
          <li>the higher velocity means other contributions such as air resistance becomes more significant</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Challenge: estimate friction by looking at the loss of energy</p>
  </li>
</ol>

<blockquote>
  <p><strong>Check</strong>: when you doing linear regression for this lab, you are <em>probably</em> going to just do the unweighted version $\implies$ do not need to capture uncertainty</p>
</blockquote>

<h1 id="lab-2-projectile-motion-and-conservation-of-energy">Lab 2: Projectile Motion and Conservation of Energy</h1>

<p>Overview: basically <strong>launching a ball of a ramp</strong>, so that you can</p>

<ul>
  <li>estimate friction</li>
  <li>predicting landing position</li>
</ul>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230213144921560.png" alt="image-20230213144921560" style="zoom:33%;" /></p>

<blockquote>
  <p><strong>Primary objectives</strong></p>

  <ul>
    <li>understanding distributions of data</li>
    <li>Gaussian statistics</li>
    <li>write (at least some part) of your derivation in your lab report</li>
  </ul>
</blockquote>

<p>So how does this work? We are essentially exchanging potential and kinetic energy in a closed system</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230213145210803.png" alt="image-20230213145210803" style="zoom: 33%;" /></p>

<p>Then, since kinetic energy is</p>

\[E_{k} = \frac{1}{2}mv^2 + \frac{1}{2}I\omega^2\]

<p>for rolling without slipping, $\omega R = v$. and potential energy is</p>

\[E_{p}(r) = -G \frac{M_e m }{r} \approx mgh\]

<p>But of course, often we get energy “leaked” to friction, e.g. $W_f$ work done by friction</p>

\[E_{k}^{ini} = E_k^{fin} + W_f\]

<p>or the canonical form from work-energy theorem</p>

\[W = \frac{1}{2}mv_f^2 - \frac{1}{2}mv_i^2\]

<p>Then, the position of the ejected ball can be described by</p>

\[x(t) = x_0 + v_{x,0}t\\
y(t) = y_0 + v_{y,0}t - \frac{1}{2}gt^2\]

<p>This should summarize all you need. You basically get</p>

<ul>
  <li>$E$</li>
  <li>$\Delta E_p = mg \Delta h$</li>
  <li>$\Delta E_k = (7/10) m v_0^2$</li>
  <li>$W_f = mg \Delta h’$ for $\Delta h’$ being the height at which the ball stops</li>
  <li>..blablabla</li>
</ul>

<p>with which you can find out the final landing position $x(t)$</p>

<blockquote>
  <p><strong>Note that</strong></p>

  <ul>
    <li>the friction force we are estimating is technically velocity dependent. So measuring friction at one configuration might not mean you have the same friction in later setups</li>
    <li>The experiment is <mark>extremely sensitive to the value of $W_f$</mark>, so measure it as carefully as possible!</li>
  </ul>
</blockquote>

<hr />

<p><strong>Taking Data</strong></p>

<p>Basically you will record the position of the landing by having the ball hitting a carbon paper:</p>

<ul>
  <li>draw a marker of where you <em>expect</em> it to land</li>
  <li>compare against the actual data</li>
</ul>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230213151427100.png" alt="image-20230213151427100" style="zoom: 33%;" /></p>

<p>Then, we can look at the <strong>distribution of the data</strong></p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230213151532695.png" alt="image-20230213151532695" style="zoom:28%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>you should expect your data to look like a normal distribution (even if your model is wrong) if you are testing stuff in the <em>same configuration</em></p>
    </li>
    <li>
      <p>error propagation in this lab is onerous. It is then highly recommended that you define</p>

\[x_{app} = \frac{D\sqrt{2h_2h_E}}{L};\quad h_E = \frac{10}{7}(\Delta h - \Delta h')\]

      <p>$h_2$ and $h_E$ are <mark>not independent</mark>. Therefore you should really calculate $\sigma_u$ for $u = h_2h_e$</p>
    </li>
  </ul>
</blockquote>

<h1 id="lab-4">Lab 4</h1>

<p>Primary Learning Goals:</p>

<ul>
  <li><strong>Weighted</strong> linear regressions</li>
  <li><strong>Weighted</strong> means of population of data</li>
</ul>

<p>And we will be probing <strong>atomic structures</strong>. Before, we understood the <strong>classical macroscopic force</strong> between matter, but <strong>not</strong> really what the constituents of matter were</p>

<ul>
  <li>JJ Thomson shows that matter has constituents that are negatively charged and whose charge/mass ratio is <strong>constant</strong> = <mark>quantization</mark>!</li>
  <li>proposed the plum pudding model (not quite right, improved by latter models)</li>
  <li>today, quantum mechanics! Ann the standard model of elementary particles</li>
</ul>

<p>Experiment: <strong>measure $e/m$</strong> by using lorentz force giving a circular motion</p>

<ol>
  <li>
    <p>given a loop current, we can compute the magnetic field created</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230227145721191.png" alt="image-20230227145721191" style="zoom: 33%;" /></p>

    <p>specifically, we will use a <strong>Helmholtz coil</strong>, which is basically two coils:</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230227150611603.png" alt="image-20230227150611603" style="zoom:33%;" /></p>

    <p>and $C$ will be given.</p>

    <p>magnetic field at the <mark>center</mark> of experimental apparatus. Finally we also want to <em>not have contribution from external field</em>, hence you want to <strong>align your apparatus with any external field</strong> (<mark>very important</mark>)</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230227150800379.png" alt="image-20230227150800379" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Then, in this field (around the axis of the loop) <strong>if we send in a moving charge</strong>:</p>

\[F = q\vec{v}\times \vec{B} = m \frac{v^2}{R}\]

    <p>which performs a circular motion with radius $R$. Hence</p>

\[\frac{q}{m} = \frac{2v}{R^2 B^2}\]
  </li>
  <li>
    <p>but we don’t have an easy way to measure the velocity $v$. One solution to this problem is to speed the electrons up thanks to a known potential difference</p>

\[K_{\mathrm{gain}} = e V = \frac{1}{2}mv^2\]

    <p>assuming $v_0=0$ is at rest and accelerated across $V$.</p>

    <p>Spefically:</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230227150903458.png" alt="image-20230227150903458" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>measure the radius of curvature</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230227151108697.png" alt="image-20230227151108697" style="zoom:33%;" /></p>
  </li>
</ol>

<p>Finally, plot with a <strong>line of best fit</strong> to a equation = shows that the equation is linear hence has more <em>physical meaning</em> (e.g. than computing $e/m$ for each of your trial and averaging across)</p>

<ul>
  <li>note that you will need to pick different $I$, one good way is to pick it such that it hits one of the markers</li>
  <li>what about the error of the $I$? You will notice that the marker is quite thick. So you can take $I_\min$ and $I_\max$ that hits that marker, and then take $\bar{I}\pm \sigma_I$ as uncertainty</li>
</ul>

<h1 id="lab-5-polarization-and-interference">Lab 5: Polarization and Interference</h1>

<p>A few background:</p>

<ul>
  <li>
    <p>light as EM wave = oscillating E and B fields</p>
  </li>
  <li>
    <p>Electric and magnetic fields <strong>are always perpendicular to each other</strong></p>
  </li>
  <li>
    <p>Everyday light is usually <strong>unpolarized</strong>. All directions of the electric field are equally probable</p>

    <ul>
      <li>
        <p>a <strong>polarized</strong> light, e.g. electric fields points in one direction only</p>

        <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306145053943.png" alt="image-20230306145053943" style="zoom:50%;" /></p>
      </li>
      <li>
        <p>so a better definition/visualization of <strong>unpolarized light = cannot define a plane where it oscillates in</strong></p>
      </li>
    </ul>
  </li>
</ul>

<p>Then how much field will be transmitted is given by a <mark>linearly polarized light</mark>:</p>

\[|\vec{E}_{trans}| = | \vec{E}_0 | \cos\theta\]

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306145519617.png" alt="image-20230306145519617" style="zoom: 33%;" /></p>

<p><strong>Experiment 1 Background Mauls’ law</strong>: but all we care is <strong>its intensity</strong> because that is what <em>our eyes can observe</em></p>

\[I \propto | \vec{E} |^2\]

<p>Therefore we get <strong>Malus’ Law</strong> (at the second polarizer)</p>

\[I=I_0 \cos^2\theta\]

<p>so you should see something like this</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306145638472.png" alt="image-20230306145638472" style="zoom:33%;" /></p>

<p>So how do you measure this? You will get a setup with a rotatable polarizer:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306153933205.png" alt="image-20230306153933205" style="zoom: 33%;" /></p>

<p>So that you can</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Measure this</th>
      <th style="text-align: center">Plot this</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306154027663.png" alt="image-20230306154027663" /></td>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306154048970.png" alt="image-20230306154048970" /></td>
    </tr>
  </tbody>
</table>

<p>where in the plot you will need to extract at least 20 points = 20 different values of $\cos^2\theta$</p>

<hr />

<p><strong>Experiment 2 Background: Young’s Double Slit</strong></p>

<ul>
  <li>
    <p>constructive interference = same propagation direction, same frequency, in phase</p>
  </li>
  <li>destructive interference = same propagation direction, same frequency, but out of phase</li>
  <li>(recall that standing wave = <em>opposite</em> propagation direction, same frequency)</li>
</ul>

<p>Here we focus on the constructive and destructive interference</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306150046067.png" alt="image-20230306150046067" style="zoom:33%;" /></p>

<p>the idea is to measure <mark>where the peaks/dark spots</mark> are. The key insight is that:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306150331011.png" alt="image-20230306150331011" style="zoom: 33%;" /></p>

<p>assumption to make thing easier: two waves are parallel, as $D » d$</p>

<ul>
  <li>
    <p>for <strong>bright spot to appear</strong>, then the $\Delta l$ must be an <em>integer multiple of wavelength</em> = $m\lambda$</p>
  </li>
  <li>
    <p>for <strong>dark spot to appear</strong>, then $\Delta l$ will be $(m+1/2)\lambda$</p>
  </li>
</ul>

<p>Then for the positions for the bright spot is</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306150627704.png" alt="image-20230306150627704" style="zoom:50%;" /></p>

<p>How do you measure this? There will be a sensor you can move <em>along $x$</em>, and record intensity $I(x)$ so that you can find $x_m$</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306151618379.png" alt="image-20230306151618379" style="zoom:50%;" /></p>

<hr />

<p><strong>Experiment 3 Background: Diffraction</strong>: can be thought of as self-interference:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306151014923.png" alt="image-20230306151014923" style="zoom:50%;" /></p>

<p>where the <strong>diffraction single-slit minima</strong> occurs at</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306150927858.png" alt="image-20230306150927858" style="zoom:50%;" /></p>

<p>so that you can <em>overlay</em> your single-slit minima envelop on top of your double slit experiment. How did this happen?</p>

<ul>
  <li>in an ideal double slit, all the amplitudes will be constant. In an “ideal” single split, you get your envelope</li>
  <li>therefore in the practical double split, the observed intensity is actually an <em>ideal double split $\times$ single slit</em></li>
</ul>

<p>How would yuo measure this? Once again take measurements by moving the sensor in the transverse direction</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230306151823319.png" alt="image-20230306151823319" style="zoom:50%;" /></p>

<hr />

<p>What are some questions to think about:</p>

<ul>
  <li><strong>What limits the precision of these measurements with light?</strong> theory, physical limitations, aberration, diffraction, ambient lights</li>
</ul>

<p>Some tips:</p>

<ol>
  <li>For all the three parts: Move the RMS slowly when recording data.</li>
  <li>For the polarizer part: Try your best to minimize the amount of environmental light coming in. E.g. using the dim light in the room, move components closer to the sensor, etc.</li>
  <li>For the last part: move the laser closer to the light sensor to see more fringes but remember to <strong>record the value of D</strong>!</li>
</ol>

<h1 id="lab-6-interferometer">Lab 6: Interferometer</h1>

<p>The idea that we can use light as a precision measurement tool. Recall that some key properties include</p>

<ul>
  <li>
    <p><strong>interference</strong></p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320145027594.png" alt="image-20230320145027594" style="zoom:33%;" /></p>

    <p>basically when <em>two</em> waves interfere under different conditions</p>
  </li>
  <li>
    <p><strong>refraction</strong>, governed by snell’s law</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320145320632.png" alt="image-20230320145320632" style="zoom: 33%;" /></p>

    <p>so that basically, as Maxwell’s equation is “different” in medium, we get</p>

\[v = c/n,\quad n \text{ being index of refraction}\]
  </li>
  <li>
    <p><strong>optical path length</strong>: how many “cycles” the light spend during the a physical distance</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320150452652.png" alt="image-20230320150452652" style="zoom:33%;" /></p>

    <p>in the case of different medium (but same physical distance), then you can simply calculate the <strong>number of wavelength we can fit</strong> in each “box” (i.e. physical distance)</p>

    <p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320150625225.png" alt="image-20230320150625225" style="zoom: 33%;" /></p>

    <p>why would this quantity be useful? Recall that in the previous lab this difference in wavelength relates to constructive/destructive interf.</p>
  </li>
</ul>

<p>Finally, <strong>interferometry</strong>, the idea being</p>

<ol>
  <li>Split single beam into two, then recombine.</li>
  <li>Observer sees interference patterns projected onto a small screen.</li>
  <li>By moving one of the mirrors, the observer can change the path length difference  . The consequence is a shift in the interference pattern.</li>
</ol>

<p>so that as you move the $M_1$ length back and forth, you will see a different interference pattern</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320151553834.png" alt="image-20230320151553834" style="zoom:33%;" /></p>

<p>where basically you get two rays because <strong>one is being transmitted and other being reflected</strong></p>

<p>measure the gravitational wave=stretching/compressing space=<strong>changes the physical distance=interference pattern</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Inteferometers Today</th>
      <th>Recorded Interference due to Gravitational Wave</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320151329891.png" alt="image-20230320151329891" style="zoom:50%;" /></td>
      <td><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320151437742.png" alt="image-20230320151437742" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Alternatively, the beams that are transmitted and reflected+transmitted will have interference = can measure the gap</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320152244339.png" alt="image-20230320152244339" style="zoom:50%;" /></p>

<blockquote>
  <p>If we change the path length by a certain distance</p>

\[2d_m = m \lambda,\]

  <p>where $d$ is the distance you moved the mirror w.r.t the original position, then you will <strong>restore the original intereference pattern</strong>.</p>

  <ul>
    <li>be careful that the drawings we did are “assuming” a single light ray, but of course in reality is a “spherical wave front”</li>
    <li>therefore, if you moved $d_{10}$, then you would have seen the bright spot reappeared $10$ times <em>during the time you moved it</em></li>
  </ul>
</blockquote>

<hr />

<p><strong>Experiment: you will use both the Michelson and the Fabry-Perot interferometers</strong></p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320152916248.png" alt="image-20230320152916248" style="zoom:33%;" /></p>

<p>How to read the micrometers</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320152419592.png" alt="image-20230320152419592" style="zoom:33%;" /></p>

<p>You will turn the knob and count how many fringes have passed.</p>

<p>Then you will do this again:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320153053961.png" alt="image-20230320153053961" style="zoom:33%;" /></p>

<blockquote>
  <p>To think about:</p>

  <ul>
    <li>in the end both setup measures the same thing. Which configuration is better?</li>
    <li>sometimes count the first fringe might be hard. It could be better to count the second ring, etc = the <mark>largest error is most likely mis-counting the number of fringes</mark>. So that should be <mark>included in your uncertainty measurement</mark></li>
  </ul>
</blockquote>

<p>The last part is we can change the optical distance <strong>by changing the medium</strong> (before we are changing the physical distance)</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320153256202.png" alt="image-20230320153256202" style="zoom:50%;" /></p>

<p>so that we can <strong>change the pressure=change index of refraction</strong> and observe different interference patterns.</p>

<p>Therefore, since the interference patterns re-occur every integer number of wavelengths apart:</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320153435220.png" alt="image-20230320153435220" style="zoom: 50%;" /></p>

<p>Then finally from this we can compute $n_{air}$ by</p>

<p><img src="/lectures/images/2023-05-11-UN1494_Intro_to_Exp/image-20230320153539028.png" alt="image-20230320153539028" style="zoom: 50%;" /></p>

<blockquote>
  <p><strong>Tips</strong></p>

  <ul>
    <li>start turning the knob <strong>from</strong> the 500 $\mu m$ mark</li>
  </ul>
</blockquote>

<h1 id="feedback-received">Feedback Received</h1>

<p>Feedback for Lab 1:</p>

<p>Total score: 13.5/20</p>

<p><strong>Overall Communication/Organization rubric items: Full points</strong></p>

<p><strong>Data Vis: -1</strong>
Plots must have titles as well as captions.
Note: A short table or two with some of your measurements would not have been out of place here, but it’s not strictly necessary and in this case, you wouldn’t lose points</p>

<p><strong>Data Manipulation &amp; Error Analysis rubric items: -1</strong>
- Uses Python, Mathematica, or similar: Yes
- Only most relevant equations or analyses are explicitly shown: Yes
- Analysis and modeling are connected to physical concepts: Yes
- Uncertainties are justified: There is <mark>no such thing as “human error.” This will fall under “random error”</mark> but never use the phrase “human error” or anything like it.</p>

<p><strong>Discussion rubric items: -3.5</strong>
- Comparison of results to expectation with justification: <mark>Agreement of measured e with expected value (1) is not reported</mark>. When discussing whether results are in agreement with predictions, don’t say that your results are “satisfactory” or “close” or anything like that (e.g. when you talked about your result for b, the y-intercept). When it comes to analyzing results, there’s only one thing that matters. <mark>Are your measurements the same as the predicted values within 3 sigma?</mark> If yes, they’re in agreement with predictions. If not, there’s a statistically significant error and they’re not in agreement. Once you have established this, you go into <mark>discussing the quality of your results (why they were or weren’t in agreement), and then you can potentially use more qualitative descriptions</mark>, but only once the quantitative actual analysis has been established.
- Discussion of quality (quantitative and qualitative) of results: Whether or not your results are in agreement with expectation, need to comment on why. This hasn’t been done fully.
- Sources of errors reference model, assumptions, and technical limitations: Sources of error for e not commented on.
- Responds to discussion questions: Not all</p>

<p>Narrative flow &amp; context rubric items: -1
- Sufficiently contextualize results and discussion: Yes
- Include only necessary and relevant background: Yes. If you want it to be perfect, you need to include just one or two sentences that <mark>capture the really big picture (why do we even care about any of the things we investigated in this experiment).</mark> You’re really almost there, but think of the intro as the part that will convince the reader that this is worth reading. Think of a scientific journal article where the intro always has something to make the project sound <mark>immediately relevant to everyone</mark>, no matter their background.
- Separate information appropriately: Yes
- Summarize concisely: Yes</p>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Introduction and Logistics]]></summary></entry><entry><title type="html">APPH4010 Intro to Nuclear</title><link href="/lectures/2022@columbia/APPH4010_Intro_to_Nuclear.html/" rel="alternate" type="text/html" title="APPH4010 Intro to Nuclear" /><published>2022-12-20T00:00:00+00:00</published><updated>2022-12-20T00:00:00+00:00</updated><id>/lectures/2022@columbia/APPH4010_Intro_to_Nuclear</id><content type="html" xml:base="/lectures/2022@columbia/APPH4010_Intro_to_Nuclear.html/"><![CDATA[<p>Equations and Concepts for Intro to Nuclear</p>

<h1 id="1-basic-concepts">1. Basic Concepts</h1>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Isotopes</td>
      <td style="text-align: left">same atomic number but different neutrons</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Electron Volt</td>
      <td style="text-align: left">energy equal to the amount gained to accelerate rest electron through a potential difference of 1 volt</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Atomic Mass Unit/Dalton</td>
      <td style="text-align: left">1/12 of the mass of a neutral atom of $^{12}_6C$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Magnetic Moment</td>
      <td style="text-align: left">Magnetic Dipole Moment associated with spin of nucleus</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Spin Quantum Number</td>
      <td style="text-align: left">nucleons have spin of 1/2 in units of $h / (2\pi)$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Mass to Energy</td>
      <td style="text-align: left">$E^2 = p^2c^2 + m_0^2 c^2$</td>
      <td style="text-align: left">$m_0$ is rest mass, and if at rest, $p=0$</td>
      <td style="text-align: left">$E_{\mathrm{electron}}=511keV$</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$E_{\mathrm{amu}}=931.5keV$</td>
    </tr>
    <tr>
      <td style="text-align: center">Binding Energy</td>
      <td style="text-align: left">Energy required to separate its constituent nucleus</td>
      <td style="text-align: left">using $E=mc^2$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$B = [Z\cdot m_H + N\cdot m_N - m(A,Z)]c^2$</td>
      <td style="text-align: left">energy used for binding, so the actual atomic mass $m(A,Z)$ is smaller</td>
      <td style="text-align: left">consider for carbon $^{12}_6C$ we have $m(A,Z)=12$, but $m_H=1.007825$ and $m_N=1.008665$</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$m_H$ is mass of hydrogen so that it includes weight of electron</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Binding Energy Curve</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221013004334846.png" alt="image-20221013004334846" /></td>
      <td style="text-align: left">$B/A$ plotted because binding energy increases just as there are more proton to hold</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">the more $A$, much much more $B$ is needed due to short range of internuclear force</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">but the above only works until a point where there is only a fixed number of neutrons affecting a proton</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">since higher $B$ also means more energy required to break = more energy released when making bonds, we want fission to go from right</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclide Stability Curve</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221013005606999.png" alt="image-20221013005606999" /></td>
      <td style="text-align: left">more neutrons is needed for more protons, hence this stability curve in the middle</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">so above it is more proton then needed = proton rich</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Properties of Radioactivity</td>
      <td style="text-align: left">Decay is a random process, but the probability of occurring can be modelled, and macro quantity such as $t_{1/2}$ can be computed</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">usually an unstable nuclide “parent” transforms into a more stable nuclide “daughter”</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">radioactivity measured in Becquerel = 1 decay per second</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">often heavy nuclei gives $\alpha$, and light nuclei $\beta$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Alpha Emission Properties</td>
      <td style="text-align: left">$(A,Z) \to (A-4, Z-2)$</td>
      <td style="text-align: left">preferred by heavy nuclides</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\alpha$-particle is preferred because it is <strong>very</strong> stable, hence net effect of decay releases energy</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">(see below)</td>
    </tr>
    <tr>
      <td style="text-align: center">Decay Energy Released</td>
      <td style="text-align: left">$Q = \Delta m c^2 =  (m_{\mathrm{left}}-m_{\mathrm{right}})c^2$</td>
      <td style="text-align: left">if net mass loss = net energy released = RHS more stable = RHS less actual mass</td>
      <td style="text-align: left">U(238)$\to$ Th(234) + He(4)</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$Q_\alpha = \mathrm{KE}<em>{\mathrm{daughter}}+ \mathrm{KE}</em>{\mathrm{\alpha}}$</td>
      <td style="text-align: left">for alpha decay</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$Q_\alpha = E_\alpha[1+ \frac{m_\alpha}{m_D}]$</td>
      <td style="text-align: left">conservation of momentum</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">since $Q_\alpha,m_\alpha/m_D$ is known, this means KE energy spectrum for $\alpha$ will be discrete in this case</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Beta Decay Properties</td>
      <td style="text-align: left">for proton-rich nuclides, often see $\beta^+$ decay or electron capture to convert $P \to N$</td>
      <td style="text-align: left">$p \to n + e^+ + \nu$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$p+e^- \to n + \nu$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">otherwise, light nuclides often have $\beta^-$, which is $N\to P$</td>
      <td style="text-align: left">$n \to p + e^- + \bar{\nu}$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221013014759439.png" alt="image-20221013014759439" /></td>
      <td style="text-align: left">KE Spectrum of electron becomes continuous because you have three particles released</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Gamma Decay Property</td>
      <td style="text-align: left">occurs when excited nucleus lose energy $\Delta E$ as photons</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Rate of Radioactive Decay</td>
      <td style="text-align: left">$\frac{dN}{dt} = -\lambda N$</td>
      <td style="text-align: left">Given that macroscopically the rate is proportional to number of radioactive nuclei</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$N(t)=N_0e^{-\lambda t}$</td>
      <td style="text-align: left">$\lambda$ is probability per unit time that a nuclide decays</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$A=\lambda N$</td>
      <td style="text-align: left">since $\lambda$ is prob/time, activity is rate of decay of a sample, measured in Becquerel</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Relativistic Effect</td>
      <td style="text-align: left">$\Delta T’ = \gamma \Delta T$</td>
      <td style="text-align: left">high energy particle emitted so that $v \approx c$, then the actual half life becomes $\Delta T’$</td>
      <td style="text-align: left">$v=0.995c$ when $\nu$ is released with large energy</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\gamma = \frac{1}{\sqrt{1-v^2/c^2}}$</td>
      <td style="text-align: left">is Lorentz fraction</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<h1 id="5-interaction-of-radiation-and-matter">5. Interaction of Radiation and Matter</h1>

<p>Nuclear radiation normally consists of <mark>any particle with energy</mark> or photons (particle radiation is the <strong>radiation of energy by means of fast-moving subatomic particles</strong>). Its <strong>interactions with matter</strong> gives us opportunity for all experimental work. Therefore, this chapter we consider:</p>

<ul>
  <li>how charged particle interact with matter</li>
  <li>how uncharged particle interact with matter</li>
  <li>how photons interact with matter</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Num Atoms and Atom Density</td>
      <td style="text-align: left">$N = \frac{m}{M}N_A$</td>
      <td style="text-align: left">$m$ is mass of the substance you care, $M$ is the atomic weight of it</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$N/V = \frac{m}{V\cdot M}N_A = \frac{\rho}{M}N_A$</td>
      <td style="text-align: left">$\rho$ is the density of the substance you are</td>
      <td style="text-align: left">$H_2O$ has $\rho \approx 1g/\mathrm{cm}^3$, and $M=18.0153$</td>
    </tr>
    <tr>
      <td style="text-align: center">Heavy Charged Particles Interaction Properties</td>
      <td style="text-align: left">Two Ways of Interaction:<br />1) its electric field can ionize atoms in its passage<br />2) collision (e.g. with electrons)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Deflect very little from path because its heavy</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Well-defined range, depending on its energy, mass, charge, and the stopping medium</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\Delta E \approx E_\alpha(\frac{4m_e}{M_\alpha})$</td>
      <td style="text-align: left">max energy lost per collision</td>
      <td style="text-align: left">incident alpha particle</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Though short range, as it can produce ionization = excited nuclei produces x-rays = need thicker $&gt;R$ protection layer</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Stopping Power</td>
      <td style="text-align: left">Rate at which a particle loses energy per unit path length</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221013235440171.png" alt="image-20221013235440171" /></td>
      <td style="text-align: left">Bethe-Bloch Formula</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Energy Dependence of Stopping Power</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221013235351944.png" alt="image-20221013235351944" style="zoom:50%;" /></td>
      <td style="text-align: left">variation of stopping power with energy if incident proton. This general shape works for any charged ion</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221025164843434.png" alt="image-20221025164843434" style="zoom: 33%;" /></td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\frac{dE}{dx} \approx \frac{\mathrm{const}}{E^k},k\approx 0.8$</td>
      <td style="text-align: left">the faster the ion travels, the less chance it interacts with stopping medium, hence negative slope</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$R=\int_{E}^0 dE/(dE/dx) \propto E^{k+1}$</td>
      <td style="text-align: left">derived from above</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Projectile Dependence of Stopping Power</td>
      <td style="text-align: left">$dE/dx \propto z^2 f(v)$</td>
      <td style="text-align: left">depends on particle charge</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$R \propto (m/z^2) F(v)$</td>
      <td style="text-align: left">range depends on incident particle mass and charge, derived from above</td>
      <td style="text-align: left">if two charged particle has the same energy, then $R_1/R_2 = (m_1/m_2)(z_2/z_1)^2$ because velocity is the same</td>
    </tr>
    <tr>
      <td style="text-align: center">Stopping Medium Dependence of Stopping Power</td>
      <td style="text-align: left">$\frac{R_1}{R_2} \approx \frac{\rho_2 \sqrt{A_1}}{\rho_1 \sqrt{A_2}}$</td>
      <td style="text-align: left">depends on stopping medium’s density and mass number</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Bragg Curve</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014000422271.png" alt="image-20221014000422271" /></td>
      <td style="text-align: left">utilize the fact that stopping power increases when energy decreases</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">particle travels slower = can produce more ions per unit path</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">hence Brag Peak = peak stopping power = peak ionization energy</td>
      <td style="text-align: left">hit tumor at this distance</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">when all energy is lost, it does nothing and stops/halts</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Electrons/$\beta$-particle Properties</td>
      <td style="text-align: left">Two Ways of Interaction:<br />1) its electric field can ionize atoms in its passage<br />2) collision (e.g. with electrons)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">a zig-zag path</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">a much faster speed = less stopping force</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">less well-defined range, hence uses mean-free-path</td>
      <td style="text-align: left">mean free path is the average distance over which a moving particle travels before substantially changing its direction or energy</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">you can also use max-range $R_{\max}$ as a measure, in which case$\rho  R_{\max} \propto E_{\max}$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Stopping Power v.s. Electron Energy</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014142135607.png" alt="image-20221014142135607" /></td>
      <td style="text-align: left">as electrons accelerate by rapidly changing directions, it emits a lot of Bremsstrahlung radiation meaning $-\frac{dE}{dx}_{\mathrm{rad}}$ contributes a lot to stopping power</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Reaction Cross Sections</td>
      <td style="text-align: left">measures area within which if you hit the particle near the target $T$, reaction will occur</td>
      <td style="text-align: left">can be used for both Neutron and Photon</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">hence measures how strongly target $T$ reaction will occur</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Microscopic Cross Section</td>
      <td style="text-align: left">$\sigma = \sigma(E)$</td>
      <td style="text-align: left">$E$ is energy of incident particle</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">unit is uslaly $\mathrm{cm}^2$ or in Barnes ($10^{-24}\mathrm{cm}^2$)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\sigma_{\mathrm{total}} = \sigma_{\mathrm{abs}}+\sigma_{\mathrm{sca}}+\sigma_{\mathrm{n,2n}}+…$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Reaction Rate</td>
      <td style="text-align: left">$R=N \cdot \sigma \phi = \Sigma \phi$</td>
      <td style="text-align: left">$N$ is atomic density, $\phi$ is particle flux (particle per cm$^2$ per second)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$\sigma \phi$ measures number of reactions triggered per second</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Macroscopic Cross Section</td>
      <td style="text-align: left">$\Sigma \equiv N \sigma$</td>
      <td style="text-align: left">is the macroscopic cross section</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Gamma Ray Interaction Properties</td>
      <td style="text-align: left">Primary ways of interaction:<br />1) photo-electric effect<br />2) Campton scattering<br />3) pair production</td>
      <td style="text-align: left">because gamma rays are very energetic and they are photons</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">we consider attenuation coefficient $\mu_m$ as a measure of <strong>probability the photon interact with material = photon vanished</strong></td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Photoelectric Effect</td>
      <td style="text-align: left">photon absorbed by electrons (near nucleus for conservation of momentum) to be ejected</td>
      <td style="text-align: left">hence that photon disappeared = attenuated</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$T=E_\gamma - B_e = hv - B_e$</td>
      <td style="text-align: left">$T$ is the emitted photoelectron and $B_e$ is the binding energy of the electron</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">secondary reactions might occur if<br />1) rest of the electrons rearrange themselves for de-excitation and release $\gamma$-ray<br />1) eject low energy electrons for de-excitation and hence <strong>Auger</strong> electrons  to deal with the excess energy</td>
      <td style="text-align: left">The Auger effect is a physical phenomenon in which the filling of an inner-shell vacancy of an atom, by an electron, is accompanied by the <em>emission of another electron</em> from the same atom <em>instead of releasing energy</em>.</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\sigma_{PE} \propto z^5 / E_\gamma^{3.5}$</td>
      <td style="text-align: left">free electron = low probability of reaction</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">higher electron binding energy = more tightly bound electron has a higher chance (also for <strong>conservation of momentum</strong>)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Campton Scattering</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014150725380.png" alt="image-20221014150725380" /></td>
      <td style="text-align: left">$\gamma$ ray scattered off electron and hence we get an electron recoiling and a lower energy photon</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$T = E_\gamma - E_{\gamma ‘} = E_{\text{KE of elec}}$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$E_{\gamma ‘} = E_{\text{KE of elec}} = E_e - m_ec^2$</td>
      <td style="text-align: left">for $E_e$ is the total energy of the recoil electron</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$E_{\gamma ‘} = \frac{E_\gamma}{1+(E_\gamma/mc^2)(1-\cos \theta)}$</td>
      <td style="text-align: left">energy of scattered photon from conservation of energy and momentum</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">a problem for shielding in real life as $\gamma$ photon didn’t disappear</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Pair Production</td>
      <td style="text-align: left">creates an electron-positron pair (when heavy nucleus is near to conserve momentum)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$E_\gamma = 2m_ec^2 + T_- + T_+$</td>
      <td style="text-align: left">$2m_ec^2$ is the rest mass energy of positron and electron</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">has secondary effect when positron recombine with electron $\to$ annihilate and produce two oppositely traveling photon $\to$ can do PE or scattering</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Photon Attenuation</td>
      <td style="text-align: left">from the three mechanism above, photon either disappear or scattered = not observed by detector = attenuated</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014151515070.png" alt="image-20221014151515070" /></td>
      <td style="text-align: left">can measure and find out attenuation dependence on the three mechanism</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014151526416.png" alt="image-20221014151526416" /></td>
      <td style="text-align: left">difficult to measure</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Collimated Photon Attenuation</td>
      <td style="text-align: left">$dI = - N\sigma I dx$</td>
      <td style="text-align: left">$x$ is thickness of the material, $N$ is material atomic density, and $\sigma$ is interaction cross section</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\mu \equiv N\sigma$</td>
      <td style="text-align: left">linear attenuation coefficient</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">represents the probability per unit path length of a photon undergoing an interaction that would remove it from the beam</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$I = I_0 e^{-N\sigma x} = I_0 e^{-\mu x}$</td>
      <td style="text-align: left">intensity observed is $I$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\mu_m = \mu / \rho$</td>
      <td style="text-align: left">mass attenuation coefficient</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Photon Attenuation Coefficient Graph</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014152131327.png" alt="image-20221014152131327" /></td>
      <td style="text-align: left">again, there are three phenomenon contributing to $\sigma$ hence $\mu_m$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Neutron Interactions</td>
      <td style="text-align: left">A variety of interactions but mostly nuclear reactions:<br />1) fission if neutrons at few MeV<br />2) scattering<br />3) slowed neutrons can give neutron absorption</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Fission Fragments</td>
      <td style="text-align: left">When $N$ and $_{238}U$ react, it will create two <strong>fission fragment</strong> and 2-3 neutrons</td>
      <td style="text-align: left">hence neutron attenuated</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">those neutrons can then start chain reactions</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">requires low neutron energy</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Neutron Moderation</td>
      <td style="text-align: left">if elastic collision, can calculate energy $E$ of neutron with initial energy $E_0$ <strong>after colliding</strong> with some target nucleus at rest</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$n = (1/\varepsilon) \ln(E_0 / E_n)$</td>
      <td style="text-align: left">$n$ is the number of collisions of neutrons</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$E_n$ is the energy of neutron after $n$ collisions</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\varepsilon = (2/A) - (4/(3A^2))$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">if reached low neutron energy = thermal neutron, can do fission</td>
      <td style="text-align: left">hence attenuated (see above)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Attenuation in Neutrons</td>
      <td style="text-align: left">$\sigma_T \approx \sigma_a + \sigma_s$</td>
      <td style="text-align: left">mostly scattering and absorption</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$I = I_0 e^{-N \sigma_T x}$</td>
      <td style="text-align: left">same equation as photon attenuation</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$I = I_0 e^{-\Sigma x} = I_0 e^{- x/\lambda}$</td>
      <td style="text-align: left">same $\Sigma = N\sigma_T$ as before</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\lambda = 1 / \Sigma$</td>
      <td style="text-align: left">mean attenuation length = <strong>mean free path</strong></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$1/\lambda = (1/\lambda_a) + (1/\lambda_s)$</td>
      <td style="text-align: left">since $\Sigma = \Sigma_a + \Sigma_s$ from the first equation</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<h1 id="6-detectors-and-instrumentation">6. Detectors and Instrumentation</h1>

<p>Here, we consider the principle at systems for</p>

<ul>
  <li>detection radiation</li>
  <li>producing controlled beams of radiation</li>
</ul>

<p>In general, any detector gets its signal from the <strong>interaction of radiations with matter</strong>:</p>

<ul>
  <li>collect charge released by ionization of gas (cased by radiation)</li>
  <li>excitation of electrons in semi-conductors (cased by radiation)</li>
  <li>observing fluorescent photons emitted due to de-excitation (cased by radiation)</li>
  <li>making ionization trails visible in film/solid gas (cased by radiation)</li>
</ul>

<p>and in addition to detecting those radiations, we also want detectors to tell us more information such as <strong>energy of the radiation</strong>, type of the radiation, dose rate, etc.</p>

<ul>
  <li>
    <p><strong>dose rate</strong> is quantity of radiation absorbed or delivered per unit time</p>
  </li>
  <li>
    <p>Dose equivalent (or effective dose) combines the amount of radiation absorbed and the medical effects of that type of radiation.</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Observe Current by Collecting Charge</td>
      <td style="text-align: left">$Q = \int_{0}^t i(t)dt$</td>
      <td style="text-align: left">typically, radiation interact with matter and produces an electric charge (and ion)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014155430476.png" alt="image-20221014155430476" /></td>
      <td style="text-align: left">can collect that charge using an electric field = observe a current</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">notice the <strong>pulse like shape</strong></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014155617597.png" alt="image-20221014155617597" /></td>
      <td style="text-align: left">In reality, since radiation is random, it is better described by Poisson statistics</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Poisson statistics = probability of a given number of events occurring in a fixed interval of time if the events occur with a known average rate and independently of the time since the last event</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Modes of Detector Operation</td>
      <td style="text-align: left">Pulse mode: when low radiation flux hence each radiation can be recorded as separate pulse</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Current mode: high particle flux hence average current is recorded</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Detectors Figures of Metric</td>
      <td style="text-align: left">Things to check for a detector:<br />1) Energy Resolution<br />2) Detection Efficiency<br />3) Dead Time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Energy Resolution</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014161258795.png" alt="image-20221014161258795" /></td>
      <td style="text-align: left">$E_0$ peak is the source, the Gaussian curve is the fitted energy spectrum <em>measured</em></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">measures Full-Width at Half Maximum (FWHM)</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014161426020.png" alt="image-20221014161426020" /></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">measured in units of energy or percent of its peak energy $E_0$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">since it is usually first fitted to a Gaussian curve, then FWHM$=2.35\sigma$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014161543558.png" alt="image-20221014161543558" /></td>
      <td style="text-align: left">in reality due to all different effects <em>reaching the device</em></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Detection Efficiency</td>
      <td style="text-align: left">Measures how much radiation it can capture, and if captured, how much it can record</td>
      <td style="text-align: left">capture = radiation reaches it<br />record = radiation reached AND recorded</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">usually easy to detect charged particles $\alpha$ and $\beta$, but be careful as they have short range</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">harder to deal with $\gamma, n$ because they have deposit little energy per unit path</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014161857546.png" alt="image-20221014161857546" /></td>
      <td style="text-align: left">depends on geometry of the device</td>
      <td style="text-align: left">if device is liquid so that source submerges in it, then it is $4\pi$ full coverage</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014161905351.png" alt="image-20221014161905351" /></td>
      <td style="text-align: left">actual performance, dependent on detector material and thickness, and radiation type and energy</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Dead Time</td>
      <td style="text-align: left">due to physical/electronical problem, there will be dead-time when device becomes <strong>unresponsive</strong> between events</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">e.g. takes time for the captured electron to travel to the cathode</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">events almost overlap</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\tau$ =  dead time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014162158239.png" alt="image-20221014162158239" /></td>
      <td style="text-align: left">so non-paralyzable can recover 4</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">paralyzable has the problem of <strong>extended dead time</strong></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Dead Time Corrections</td>
      <td style="text-align: left">Paralyzable detector: $n=me^{n\tau}$</td>
      <td style="text-align: left">$n=$ true interaction rate, $m$ = recorded interaction rate, $\tau$ = deadtime</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Nonparalyzable detector: $n= m/(1-m\tau)$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014162313018.png" alt="image-20221014162313018" /></td>
      <td style="text-align: left">when true radiation $n$ is high, paralyzable can mistake it for a low $m$!</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Principles of Gas Detectors</td>
      <td style="text-align: left">detectors by using gas to be ionized by the radiation $\to$ record those ions</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">three types of detectors on this principle<br />1) ionization chamber<br />2) proportional counter?<br />3) GM counter</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Ionization Chamber</td>
      <td style="text-align: left">works by measuring ionization (of gas molecules) produced <em>solely</em> by incident ionizing particles (e.g. $\alpha$ radiation)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014162757533.png" alt="image-20221014162757533" /></td>
      <td style="text-align: left">ionizing particles creates ionized gas $\to$ have a high enough field to prevent recombination $\to$ those ions complete the circuit by having electron goes to anode and positive ion to cathode</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">but usually needs an amplifier as current produced could be small</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">need a certain amount of electric field applied</td>
      <td style="text-align: left">see above</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$A$ = #ions per sec produced $\times$ charge per ion</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">source disposing $1GeV\,s^{-1}$ energy per second and air in the chamber ionizes with $34$eV</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">#ions per sec produced = $10^9 / 34$</td>
    </tr>
    <tr>
      <td style="text-align: center">Pulse Amplitude v.s. Voltage Applied in Gas Detector</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014163205531.png" alt="image-20221014163205531" /></td>
      <td style="text-align: left">the fundamental reason why we have three types of gas detectors</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">pulse <strong>amplitude</strong> can give you information of the <strong>energy</strong> of ionizing particles!</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">region I: increase $V$ means less recombination of ions</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">region II: full charge collection = no recombination</td>
      <td style="text-align: left">Ionization Chamber</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">region where output is independent of applied voltage</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Gas Amplification Factor (GAF) = 1</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">region III: electrons become more energized $\to$ can cause secondary ionization during collision. Those secondary ionized electrons can further produce ionizations</td>
      <td style="text-align: left">proportional chamber</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">amplification of current = Townsend Avalanche</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">GAF up to about $10^5$, but still proportional to the original ionization</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">region V: electrons so energized that it can excite inner electrons $\to$ UV radiation from de-excitation $\to$ ionizes other irrelevant atoms in the chamber</td>
      <td style="text-align: left">Geiger-Mueller Counter</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">can’t distinguish initial input energy of those ionizing particles</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">hence can only know the presence of those radiations</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Proportional Chamber</td>
      <td style="text-align: left">electric field increased beyond region II, so that secondary ionizations occur</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">pulse amplitude still tells you energy of input ionizing particles</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221014164005297.png" alt="image-20221014164005297" /></td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Geiger-Mueller Counter</td>
      <td style="text-align: left">electric field increased so much that everything is ionized = pulse amplitude does not depend on the energy of input ionizing particles</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">hence can only measure the presence of radiation</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Scintillation Detector Mechanics</td>
      <td style="text-align: left">energy of radiation $\to$ excitation of electrons of scintillation material $\to$ de-excitation/Compton scattering which emits UV/visible light</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">those light photons are directed to the photosensitive surface $\to$ emit photo-<strong>electrons</strong> $\to$ amplified in PMT $\to$ observe pulse of current</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221021173557853.png" alt="image-20221021173557853" /></td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Scintillation Materials</td>
      <td style="text-align: left">need high effiiency of converting energy to photons</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">linear conversion: output proportional to deposited energy from radiation</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">short decay time = quick flash = short dead time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">maximize conversion to output fluorescence</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">transparent to its own emission (which is photon, which is <em>also</em> energy)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Scintillation Types</td>
      <td style="text-align: left">Fluorescence=emit visible radiation with emission time approx. 10 ns</td>
      <td style="text-align: left">preferred</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Delayed fluorescence=above but loner emission time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Photofluorescence=longer wavelength and longer emission time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Semi-Conductor Detector Mechanics</td>
      <td style="text-align: left">energy of radiation creates electron-hole pair $\to$ electron move in the direction of applied field $\to$ current</td>
      <td style="text-align: left">works only if those e-h pairs do not recombine or get trapped in regions of impurity</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Semi-Conductor Detector Properties</td>
      <td style="text-align: left">only requires 3-4 $eV$ to create e-h pair, whereas to create ion pair in gas requires 30 $eV$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">better energy resolution</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">faster charge collection = shorter dead time</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Semi-Conductor Detector Types</td>
      <td style="text-align: left">diode</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">high purity $Ge$</td>
      <td style="text-align: left">see above</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">lithium drifted $Si$ or $Ge$</td>
      <td style="text-align: left">alternatives to high purity, use $Li$ for drifting as dopant atoms</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Thermoluminscent Detector</td>
      <td style="text-align: left">operates by accumulating radiation energy and read altogether at the end</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">therefore, we want crystals to de-excite as slow as possible when absorbed radiation</td>
      <td style="text-align: left">opposite of scintillation</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">mechanism: electron and holes are elevated but below conduction band, hence “trapped”</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Neutron Detectors Properties</td>
      <td style="text-align: left">cannot detect neutrons directly, but secondary radiation, such as $(n,p)$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">depending on how fast the neutrons are, there are two types of neutron detectors</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Slow Neutron Detectors Mechanism</td>
      <td style="text-align: left">for slow, thermal neutrons, nuclear reactions such as $(n,p),(n,\alpha), (n,f)$ have large cross section $\sigma \propto 1/v$</td>
      <td style="text-align: left">basically can be triggered easily</td>
      <td style="text-align: left">$(n,p)$ means the reaction of $A+n \to B+p$</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">mechanics: neutron $\to$ nuclear reactions $\to$ charged outputs (e.g. fission fragments) cause ionization $\to$ which can be measured</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">$^{10} B(n,\alpha)^7 Li$</td>
    </tr>
    <tr>
      <td style="text-align: center">Slow Neutron Detector Types</td>
      <td style="text-align: left">Proportional Counter</td>
      <td style="text-align: left">$BF_3$ proportional counter performs $^{10} B(n,\alpha)^7 Li$ which a large cross section of 4010 b for thermal neutrons, and $Q=2.79MeV$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">utilizes $(n,\alpha)$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Fission Counters: coat detector with fissionable material</td>
      <td style="text-align: left">utilizes $(n,f)$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Activation Counter: using activation foils composed of material sensitive of neutron of different energies</td>
      <td style="text-align: left">utilizes neutron capture</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Fast Neutron Detector</td>
      <td style="text-align: left">use plastic or liquid organic scintillation material instead</td>
      <td style="text-align: left">$1/v$ means slow neutron detectors become not efficient</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">in general, materials of rich hydrogen $\to$ recoiling protons produce energy for scintillation</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Particle Identification</td>
      <td style="text-align: left">since radiation = any particle with energy, we might also want to know which particle it is</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">PI: Counter Telescope</td>
      <td style="text-align: left">stack two or more detectors, and can measure $\Delta E$ between detectors and $E$ total</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\Delta E$ can tell you stopping power</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$E\times \Delta E \propto mz^2$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">PI: Time of Flight</td>
      <td style="text-align: left">measure time between detectors, hence determine $v$</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">if radiation is pulsed beam, then you can just measure arrival time at detector and actual beam pulse</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">PI: Magnetic Analysis</td>
      <td style="text-align: left">use spectrometer with magnetic field to measure the deflection of charged particles</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$r = mv/qB$, can measure mass to charge if known velocity</td>
      <td style="text-align: left">based on Lorentz Force $F=qv\times B$</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<h1 id="midterm-concepts">Midterm Concepts</h1>

<p>Chapter 1</p>

<ul>
  <li>atomic structure
    <ul>
      <li>plum pudding model, Rutherford model and its problems, Quantum Theory and its model, Bohr Model and its energy levels</li>
      <li>Rutherford scattering experiment, result</li>
    </ul>
  </li>
  <li>equivalence of energy and momentum
    <ul>
      <li>mass-energy equation</li>
    </ul>
  </li>
  <li>uncertainty principle</li>
  <li>nuclear models
    <ul>
      <li>liquid drop, shell model, their differences</li>
    </ul>
  </li>
  <li>electron volt, alternative mass unit</li>
  <li>binding energy, mass excess, and Q calculation</li>
  <li>binding energy curve (reproduce)</li>
  <li>nuclear stability curve (reproduce)</li>
  <li>radioactivity equation and activity</li>
  <li>secular equilibrium</li>
  <li>alpha decay
    <ul>
      <li>energy is discrete, etc</li>
    </ul>
  </li>
  <li>beta decay
    <ul>
      <li>continuous energy up to a cut-off</li>
    </ul>
  </li>
  <li>auger electron</li>
  <li>number of atoms and atomic densities calculations</li>
</ul>

<p>Chapter 5</p>

<ul>
  <li>stopping power
    <ul>
      <li>two components, use Bremsstrahlung</li>
    </ul>
  </li>
  <li>Bethe-Bloch that
    <ul>
      <li>stopping power goes $z^2 / v^2$</li>
    </ul>
  </li>
  <li>stopping power v.s. energy curve
    <ul>
      <li>bragg peak curve</li>
    </ul>
  </li>
  <li>heavy charged particles
    <ul>
      <li>how it loses energies, ionizing them and/or exciting them</li>
    </ul>
  </li>
  <li>light charged particles
    <ul>
      <li>zig zag, no well-defined range</li>
    </ul>
  </li>
  <li>cross sections
    <ul>
      <li>reaction rates, linear attenuation coefficient $\mu$</li>
      <li>attenuation and its differential equation</li>
    </ul>
  </li>
  <li>photon ways of interacting
    <ul>
      <li>three ways of how they work, but not formula</li>
    </ul>
  </li>
  <li>neutrons interactions
    <ul>
      <li>three ways, elastic, inelastic, or reaction</li>
      <li>thermal neutrons</li>
    </ul>
  </li>
</ul>

<p>Chapter 6</p>

<ul>
  <li>
    <p>detector two operation mode</p>
  </li>
  <li>
    <p>detectors figure of merit</p>

    <ul>
      <li>equation of FWHM/$E_0$</li>
      <li>efficiency, especially uncharged particles</li>
      <li>definitions of absolute/intrinsic effiency</li>
    </ul>
  </li>
  <li>
    <p>dead time</p>

    <ul>
      <li>paralyzable v.s. non-paralyzable</li>
      <li>dead time correction equation</li>
      <li>dead time curves</li>
    </ul>
  </li>
  <li>
    <p>will need to draw a detector</p>
  </li>
  <li>
    <p>three types of ionization chambers</p>
  </li>
  <li>
    <p>gas-field detector v.s voltage</p>

    <ul>
      <li>its five regions and the graph itself</li>
    </ul>
  </li>
  <li>
    <p>scintillation detector mechanism</p>
  </li>
  <li>
    <p>semi-conductor detector mechanism</p>
  </li>
  <li>
    <p>TLDs</p>
  </li>
  <li>
    <p>Neutron Detectoros</p>

    <ul>
      <li>slow.v.s fast needs hydrogen = moderates</li>
      <li>fission counter</li>
    </ul>
  </li>
  <li>
    <p>particle identification</p>

    <ul>
      <li>
        <p>time-of-flight gives velocity + magnetic anlaysis</p>
      </li>
      <li>
        <p>none of the detector mechanisms except in the particle identification to tell us particle type</p>
      </li>
    </ul>
  </li>
</ul>

<h1 id="nuclear-structure">Nuclear Structure</h1>

<p>Aim:</p>

<ul>
  <li>understanding what happens <strong><em>inside</em></strong> nucleus
    <ul>
      <li>e.g. understand its properties by understanding what <strong>forces are responsible</strong></li>
    </ul>
  </li>
  <li>no complete theory today fully describes the structure and behavior of complex nuclei</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">liquid drop model</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">nucleus regarded as a <strong><em>collection</em></strong> of neutrons and protons forming a droplet of incompressible fluid</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">good for <strong>systematic behaviors</strong> such as nucleon binding energy</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">discrepancies in liquid drop model = there is an ORDERED STRUCTURE within the nucleus in which neutrons and protons are arraged in stable quantum states in a potential well</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Shell (Single-particle Model)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">loosely held individual outer nucleons, which account for many of the nucleus’s properties</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">very alike the electron shell model and how electrons arrange themselves</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Force</td>
      <td style="text-align: left">binds nucleons in nucleus, n-n, p-p, p-n</td>
      <td style="text-align: left">extremely complicated, derivation from first principle</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Force Properties</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">short range</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214025855146.png" alt="image-20221214025855146" style="zoom:50%;" /></td>
      <td style="text-align: left">for very small separations, nucleons begin to repel = no clump</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">nuclear density approx constant for different sized nuclei = liquid drop</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Force Charge Dependency</td>
      <td style="text-align: left">charge symmetric: same nuclear force for  p-p as for n-n</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Force Spin Dependency</td>
      <td style="text-align: left">Average force for p-n &gt; p-p or n-n by a factor of about 2</td>
      <td style="text-align: left">both n and p are fermions = obey Pauli Exclusion for spin</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">so for p-p and n-n, you have to have different spin, net $S=0$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">n-p can have be either anti-parallel or paralell. <strong>Force in $S = 1$ state is stronger</strong> than force in $S = 0$ state. Therefore, avg. force for p-n is greater than that for p-p or n-n (by about factor of 2).</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Explains why can have bound n-p (deuteron), but not bound n-n or p-p (i.e., nuclear force not strong enough to bind the latter two configurations)</td>
      <td style="text-align: left">p-p repulsion;  n-n free particles + not strong enough force</td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Force Spin-Orbit Coupling</td>
      <td style="text-align: left">$\text{Spin-orbit Force}=L \cdot S$</td>
      <td style="text-align: left">$L,S$ being angular momentum and spin, respectively</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Force is attractive if S and L are parallel, and repulsive if they are anti-parallel</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">zero on average inside an atom</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Semi-Empirical Mass Formula</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214030640303.png" alt="image-20221214030640303" /></td>
      <td style="text-align: left">estimating <strong>binding energy</strong>, which can then be used to estimate the <strong>actual nuclear masses</strong> for unknown nuclei (but known $A$ and $N$)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">based on liquid-drop = estimates <strong>collective properties of a nucleus</strong></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">SEMF Volume Energy</td>
      <td style="text-align: left">$a_vA$ term</td>
      <td style="text-align: left">nucleon feels the force only from its nearest neighbors and the nucleon density is approx constant = force is constant = <strong>B/A is approx constant</strong> in the interior of the nucleus.</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">SEMF Surface Term</td>
      <td style="text-align: left">$A^{2/3}$ since the first $A\propto \mathrm{Volume}$</td>
      <td style="text-align: left"><strong>reduced</strong> by a factor proportional to the <strong>surface area</strong> of the nucleus</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214110550694.png" alt="image-20221214110550694" /></td>
      <td style="text-align: left">nucleons on the surface of the nucleus experience the nuclear force from nucleons inside the nucleus, but no force from the outside. hence reduced</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">SEMF Coulomb Term</td>
      <td style="text-align: left">$\propto {Z^2}/{A^{1/3}}$</td>
      <td style="text-align: left">Coulomb repulsion would further reduce binding energy</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">protons repel each other with a long-range Coulomb force; <br />Mean radius of the nucleus is proportional to $R\propto A^{1/3}$<br />Coulomb force also $\propto Z^2$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">SEMF Symmetry Term</td>
      <td style="text-align: left">$\propto (N-Z)^2$</td>
      <td style="text-align: left">nucleus becomes more <strong>unstable</strong> the greater the difference between <strong>Z and N</strong>. Has the most effect for <strong>light nuclei</strong></td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">shell model result. The term is zero for $Z = N$ and becomes less important for heavy nuclei (high A), where $N &gt; Z$</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">SEMF Pairing Term</td>
      <td style="text-align: left">$\Delta$</td>
      <td style="text-align: left">nucleons tend to <strong><em>couple pairwise</em></strong> into more stable configurations</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Δ &gt; 0 if N and Z are both even</td>
      <td style="text-align: left">from Pauli Exclusion Principle</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Δ &lt; 0 if N and Z are both odd</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Δ = 0 if either N or Z is odd (i.e., A odd)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Fission Energy Barrier</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214111507480.png" alt="image-20221214111507480" /></td>
      <td style="text-align: left">barrier to fission is called the fission barrier or activation energy</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">for <strong>Heavy</strong> Nuclei</td>
      <td style="text-align: left">As s $A$ increases, the relative importance of the Coulomb repulsion term increases = it becomes energetically possible for the nucleus to split if it becomes <strong>deformed</strong> enough</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Odd nuclei (($U_{92}^{235}$)) have very low activation energies, <strong>since the pairing term is zero</strong>, and can fission with low energy neutrons.</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Isobaric Nuclei</td>
      <td style="text-align: left">same number of total nucleons but swapped #neutrons and #protons</td>
      <td style="text-align: left">will affect the coulomb term in SEMF</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Vibrational Model</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214112515740.png" alt="image-20221214112515740" /></td>
      <td style="text-align: left"><strong><em>SEMF assumed sphere shape,</em></strong> but in reality can deform</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">vibration can actually be modelled with liquid-drop model</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Rotational States</td>
      <td style="text-align: left">rotation doesn’t produce any change of state</td>
      <td style="text-align: left">Collective rotational states can <strong>only occur in non-spherical nuclei</strong> (otherwise how do you know it is rotating?)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Shell (Independent Particle) Model</td>
      <td style="text-align: left">aims to model energy level of nucleons</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">neutrons and protons fill energy level in the nucleus according to the Pauli Exclusion Principle</td>
      <td style="text-align: left">similar to electron energy levels</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">those structure like properties are not predicted by SEMF, which only deals with collective states</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">“islands of stability” corresponding to “closed shells,” also called “magic numbers,”</td>
      <td style="text-align: left">2 (1s), 8 (2s, 1p), 20, 28, …</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Potential Energy</td>
      <td style="text-align: left">predict <strong><em>energy levels</em></strong> in the nucleus, need to know $V(r)$</td>
      <td style="text-align: left">arises from interactions with other nucleons.</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">infinite well = simplest model</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Woods-Saxon Potential</td>
      <td style="text-align: left">$V(r) = -\frac{V_0}{1+e^{(r-R)/a}}$</td>
      <td style="text-align: left">quite a good approximate</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center">Spin-Orbital Potential</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221214113711983.png" alt="image-20221214113711983" /></td>
      <td style="text-align: left">more complicated but <strong>accounted for more splittings</strong> of energy level</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">splittings due to spin $s$ and angular momentum $l$. If <strong>aligned</strong>, $s\cdot l$ positive hence binding energy is increased</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<h1 id="nuclear-instability">Nuclear Instability</h1>

<p>Aim: Predict/explain <strong><em>why</em></strong></p>

<ol>
  <li>some <em><strong>half-lives are short</strong> and others are long</em>, and</li>
  <li>why <strong><em>certain energy transitions take place</em></strong> and others don’t</li>
</ol>

<p>In general there will be a) electromagnetic force; b) weak force; c) strong force</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Gamma Emission Mechanism</td>
      <td style="text-align: left">Excited nucleus may de-excite through $\gamma$-emission</td>
      <td style="text-align: left">help understand why certain gamma decay have longer half-lives than another</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">gamma is “everywhere” in the sense that many radioactive decay accompanies gamma emission (see below, due to excited daughter nucleus)</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Characteristic $\gamma$-emission</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">actually come from the transitions among the energy levels of the <strong>daughter</strong> nucleus.</td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218182726846.png" alt="image-20221218182726846" style="zoom:67%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td>the above means we have two decays:<br /><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218182834669.png" alt="image-20221218182834669" style="zoom: 15%;" />and then<img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218182846700.png" alt="image-20221218182846700" style="zoom:10%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Selection rules</td>
      <td style="text-align: left">derived from consideration of conservation of <mark>angular momentum and parity</mark>, <strong><em>specify the allowable transitions</em></strong> among energy levels</td>
      <td style="text-align: left">answers: “why are some gamma more likely to happen than others?”</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">1. Photon that carries away energy has angular momentum $L&gt;0$</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. Conservation of Angular Momentum: $\vert l_i - l_f\vert \le L \le \vert l_i + l_f\vert$</td>
      <td style="text-align: left">$l_i$, $l_f$ is the angular momentum of the initial and final nuclear state</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">3. Parity (wavefunction even or odd) is also conserved in EM transitions</td>
      <td style="text-align: left">whether or not a parity changed decides $\to$</td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218204404711.png" alt="image-20221218204404711" style="zoom:50%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218204529542.png" alt="image-20221218204529542" style="zoom:50%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">do <strong><em>not</em></strong> provide information on the probability of its occurrence, only if it <em>can</em> occur</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Competing Process in $\gamma$</td>
      <td style="text-align: left">as gamma usually comes from de-excitaton, other processes such as de-excite with electron is competing (also comes from de-excitation)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">e.g. Internal Conversion</td>
      <td style="text-align: left">transfer energy to an orbital electron (K-shell or further out), ejecting it from the nucleus</td>
      <td>another way to de-excite = the parent and child would be the same <em>as if performed gamma decay</em></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"><strong>Single energy peak</strong> for each orbital electron transition unlike continuous β spectrum.</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">$\gamma$ Transition Rates</td>
      <td style="text-align: left">Weisskopf Single Particle γ Transition Rates</td>
      <td style="text-align: left"> </td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218204742112.png" alt="image-20221218204742112" /></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$E_1$ is greatly favored</td>
      <td style="text-align: left"> </td>
      <td>Selection rules allow E2, M3, E4, M5, and E6 transitions, but the E2 radiation is strongly favored.</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218214350669.png" alt="image-20221218214350669" /></td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">If transitions have <em>high multipolarity</em>, T$_{1/2}$ may be considerable</td>
      <td style="text-align: left">basically for E6, M6, etc above,  the probabiility is low = long half lives</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Mixed $\gamma$ transitions</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">can substantially <strong><em>raise the probability</em></strong> that a particular energy γ-ray will be emitted</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">can’t characterize transition probability using the single particle model</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Beta Decay Equations</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218214832570.png" alt="image-20221218214832570" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">allowed reactions = conservation of <strong><em>energy/momentum/charge/lepton number</em></strong></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">can find the end point of the continous $\beta$ spectrum</td>
      <td style="text-align: left">recall that $Q$ gives the kinetic energy difference $Q = K_f - K_i = (m_i -m_f)c^2$</td>
      <td>$T_{end} = E_0 - m_e c^2$, the end point of the β spectrum. $E_0$ = total energy of the transition.</td>
    </tr>
    <tr>
      <td style="text-align: center">Fermi’s Golden Rule</td>
      <td style="text-align: left">Probability of beta decay, determine the <em>transition rate</em> between an initial state (i) and a final state (f) for $\beta$ decay</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218220413602.png" alt="image-20221218220413602" /></td>
      <td style="text-align: left">$\lambda$ measures the transition probability</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Electron Capture</td>
      <td style="text-align: left">competes with β+-decay when both modes are possible<img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218220610791.png" alt="image-20221218220610791" style="zoom:25%;" /></td>
      <td style="text-align: left">all $\beta$ are due to <mark>weak interaction</mark></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Factor affecting EC probability/rate</td>
      <td style="text-align: left">depends on the <strong>overlap</strong> of the electron’s and the nucleus’s wave functions</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">1. electron is most likely to be captured when it is <strong><em>closer</em></strong> to the nucleus</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. higher Z = being attracted more = size of $K$-orbit electrons orbit is smaller = being captured</td>
      <td style="text-align: left">EC importance over β+ decay increases with Z</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Alpha Decay</td>
      <td style="text-align: left">common decay mode for <strong>heavy radionuclides</strong></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Shed four units of mass (two neutrons and two protons) in one decay.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">QM “tunneling” in Alpha Decay</td>
      <td style="text-align: left">heavy nucleus like uranium has barrier of 20MeV, but emitted alpha particle from it can be <em>as low as 5 MeV</em>. now we know it is due to non-zero wave function</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">requires $Q&gt;0$</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218221142073.png" alt="image-20221218221142073" /></td>
      <td style="text-align: left">explanation of QM tunneling</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Preformation Probability</td>
      <td style="text-align: left">for alpha decay to happen, you need to first form the alpha particle in the nucleus</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218221306606.png" alt="image-20221218221306606" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="nuclear-reactions">Nuclear Reactions</h1>

<p>Study a bit further on <strong><em>how/when reaction happens</em></strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Types/Classification of Reactions</td>
      <td style="text-align: left"><strong>Elastic</strong> scattering: a + A $\to$ a + A</td>
      <td style="text-align: left">scattering = Incident and outgoing particles are the same</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Inelastic</strong> scattering: a + A $\to$ a + $A^*$</td>
      <td style="text-align: left">some energy goes into exciting internal levels in A, and later will go off $\gamma$ decay</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Knockout</strong>: a particle is emitted (“knocked out”) from the nucleus</td>
      <td style="text-align: left"> </td>
      <td>e.g. stripping of a proton from a carbon nucleus</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Stripping</strong> reaction if the transfer is from the projectile to the target.</td>
      <td style="text-align: left">Transfer reaction: 1 or 2 nucleons are transferred between the projectile and target.</td>
      <td>C-12 + alpha -&gt; C-8* + alpha*</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Pickup</strong> reaction if the transfer is from the target to the projectile.</td>
      <td style="text-align: left">Transfer reaction, i.e. target nucleus gained</td>
      <td>He-3 + p -&gt; He-4* + gamma</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Direct</strong> nuclear reactions: formation of a “new” nucleus typically involve the transfer of <em>just a few nucleons</em> (protons or neutrons) between the colliding nuclei.</td>
      <td style="text-align: left">without the creation of an intermediate compound nucleus.</td>
      <td>e.g. scattering</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Compound</strong> nuclear reactions: results in the formation of an intermediate compound nucleus, and can result in the emission of several particles</td>
      <td style="text-align: left">involve the transfer of <em>many nucleons</em> between the colliding nuclei</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Resonance</strong> reaction: incoming particle has right energy to excite an energy level in the target nucleus, greatly <em>increasing the cross section</em></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Discern what Reaction happened with Energy Spectrum</td>
      <td style="text-align: left">can distinguish different mechanism because they give rise to outgoing particles have different energy</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218222924675.png" alt="image-20221218222924675" /></td>
      <td style="text-align: left"><strong>Discrete</strong> energy peaks at high energies from direct reactions</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">At lower energies, peaks correspond to more closely spaced energy levels can’t be resolved</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">At still lower energies, compound nuclei are formed, where neutrons and protons <strong>share</strong> the incoming particle energy and “evaporate” from the nucleus in a continuous spectrum</td>
      <td>evaporate: formed compound nucleus $\to$ an equilibrium is reached so the compound nucleus <em>loses its energy slowly over time by emitting particles,</em> mostly protons and neutrons</td>
    </tr>
    <tr>
      <td style="text-align: center">Angular Distributions</td>
      <td style="text-align: left">angle of output particles relative to input particle</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Angular Distributions for Direct Reactions</td>
      <td style="text-align: left"><em>Direct</em> collisions (few nucleons take part) usually produce forward peaked reaction products</td>
      <td style="text-align: left">forward peak = products traveling in the <strong>same direction</strong> as input</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><em>Direct</em> collisions = exhibit <em>oscillations</em> as a function of scattering angle due to the wave nature of the particles</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Angular Distributions for Compound Reactions</td>
      <td style="text-align: left">Angular spectrum of evaporated particles from a <em>compound</em> nucleus is more <em>isotropic</em></td>
      <td style="text-align: left">since the emitted particles “<strong>have no memory</strong>” of the direction of the incoming particle</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218223426199.png" alt="image-20221218223426199" /></td>
      <td style="text-align: left">y-axis = prob of observing this</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Reaction rate</td>
      <td style="text-align: left">each reaction has its own cross-section = own reaction rate</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$R =\sigma N_A \phi=\Sigma \phi$</td>
      <td style="text-align: left">σ = reaction cross section<br />φ = particle (e.g., neutron) flux<br />$N_A$ = the number of target atoms per unit volume</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218223905002.png" alt="image-20221218223905002" /></td>
      <td>DD reaction cross section = higher E better because this is <em>fusion</em></td>
    </tr>
    <tr>
      <td style="text-align: center">Classical Estimate of Reaction Cross Section Assumptions</td>
      <td style="text-align: left">calculate cross section itself (before we were given this)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">assumptions = reaction happens when come close enough together for the strong nuclear force to act</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Classical Estimate of Uncharged Particles</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218224237587.png" alt="image-20221218224237587" style="zoom: 33%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$\sigma= \pi (R_1 + R_2)^2 = \pi R^2$</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Classical Estimates of Charged Particles</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218224450325.png" alt="image-20221218224450325" style="zoom:150%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Impact parameter, $b$ replaces $R$.<br />$\sigma = \pi b^2 = \pi R^2 (1-B/E)$</td>
      <td style="text-align: left">means if $B&gt;E$ reaction cannot occur</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">QM Estimate</td>
      <td style="text-align: left">classical works when particles is more ‘particle-like’</td>
      <td style="text-align: left">classical approximation of reaction cross section is only decent for particles with <strong>de Broglie wavelength less than the size of the nucleus</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">also kind of works if we are <em>heavy ions</em> = wavelength often smaller than nuclear dimensions</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">but if wave functions coincide, then reaction occur = can occur even if $B&gt;E$</td>
      <td style="text-align: left">in reality, high energy = looks like particle; low energy = looks like wave</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Usage of Elastic Scattering for Nuclear Structure</td>
      <td style="text-align: left">force causing scattering depends on the <em>spatial distribution of nucleus</em> =&gt; by analyzing the way particles scatterd = know about the size and distribution of <em>force field</em> = know about the <em>nucleus</em></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Electrons</strong> make good probes of the nucleus since they are not absorbed and interact via the well-know electromagnetic force with the protons</td>
      <td style="text-align: left">if we take particles such as $\alpha$, then it interacts strongly once inside the nucleus = lose its identity and not reappear in the entrance channel</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Compound Nuclear Reaction Mechanism</td>
      <td style="text-align: left">Many nuclear reactions proceed in two or more steps</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">1.  the <em>incoming</em> particle is absorbed by, and excites, the nucleus</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. the nucleus <em>loses</em> its excitation energy (decays) through one of several different, possible exit channels (decay branches or decay channels)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Compound Nuclear Reaction Rate</td>
      <td style="text-align: left"><em>Each decay channel</em> is characterized by a probability and mean lifetime, $\lambda = 1/\tau$</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">for compound reaction $A+a\to C\to B+b$<br /> reaction rate is $\sigma_{\alpha, \beta} = \sigma_c (\Gamma_\beta / \Gamma)$</td>
      <td style="text-align: left">$\sigma_c$ is the cross section of forming the compound nucleus<br />$\Gamma_\beta / \Gamma$ is fractional decay width into the final channel $B+b$</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">basically, need $C$ to happen, and then also $\beta$ to happen</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Energy width of Compound Nuclear Reaction</td>
      <td style="text-align: left">due to the Uncertainty Principle, uncertainties in $\lambda$ and $\tau$ $\to$ <em>uncertainties in energies of the states</em> $\to$ energy spread</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218230238702.png" alt="image-20221218230238702" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Cross Section Thresholds</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218230654380.png" alt="image-20221218230654380" /></td>
      <td style="text-align: left">each decay mode <em>also has its own ‘excitation energy’</em> required</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Heavy ion particle accelerator’s goal</td>
      <td style="text-align: left">Heavy ion particle accelerators are used to cause reactions where the <em>target nuclei are blown apart</em> or to attempt to create <em>new heavy elements through absorption</em>.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="fission">Fission</h1>

<p>This section will focus on <strong>induced fission from neutron absorption</strong></p>

<ul>
  <li>Some heavy “transuranic” (near uranium) nuclei can undergo induced fission if supplied with sufficient energy (7-8 MeV)</li>
  <li>about 6 MeV comes from binding of an extra <strong><em>neutron</em></strong> by the <strong><em>strong force</em></strong> and the rest from external sources</li>
  <li>an extra 1-2 MeV from pairing (depends on if your $N$ is odd or even)</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Why is Fission Energetic Useful?</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221218235727825.png" alt="image-20221218235727825" style="zoom: 33%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Fissile nuclei can fission with <em>low energy, “thermal,” neutrons</em></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Energy during/Mechanism of Fission</td>
      <td style="text-align: left">1. normally, the SEMF surface term provides a restoration force, like disturbing surface tension on water</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. if sufficient energy is supplied, the shape will deform to such an extent that the Coulomb repulsion force will dominate = strong force only works locally = nucleus starts to break</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">3. repulsive force will drive the (usually) 2 fission fragments apart and potential energy is <em>converted into kinetic energy</em></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219000721638.png" alt="image-20221219000721638" style="zoom: 33%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Fission Activation Energy</td>
      <td style="text-align: left">The energy required to overcome the fission barrier = deform</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219000459450.png" alt="image-20221219000459450" style="zoom: 33%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Properties of Fission</td>
      <td style="text-align: left">Energetically preferred to have one heavy group and one light group as fission fragment</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219000902306.png" alt="image-20221219000902306" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">When a fissionable nucleus absorbs a neutron and forms a compound nucleus, there is a <strong>competition between fission and gamma emission</strong> to release excitation energy, which doesn’t lead to fission</td>
      <td style="text-align: left">$\sigma_a = \sigma_f + \sigma_\gamma$</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Discovery of Neutron</td>
      <td style="text-align: left">James Chadwick found that if the energetic alpha particles emitted from polonium fell on certain light elements, specifically beryllium, an <em>unusually penetrating radiation</em> was produced (not gamma ray)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219001726268.png" alt="image-20221219001726268" style="zoom:33%;" /></td>
      <td style="text-align: left">this radiation’s range, etc. can be measured by placing paraffin a distance away to <strong>perform (n,p) reaction</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">the first ‘weird’ event that happened about this was: Walther Bothe and Herbert Becker = discovered that when energetic α-particles from the decay of polonium impinged on certain light materials, they would <strong><em>eject high energy, very penetrating, neutral radiation</em></strong>; they thought they were γ-rays</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219002009283.png" alt="image-20221219002009283" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">‘Discovery of Fission’</td>
      <td style="text-align: left">Otto Hahn and Frederick Strassmann</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Observed presence of barium and other middle-weight elements in a uranium sample bombarded by neutrons and noted <em>large release of energy</em>.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">12/2/1942 – Enrico Fermi – 1st controlled fission chain reaction in Chicago Pile 1</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Start of Nuclear Weapon</td>
      <td style="text-align: left">7/16/1945 – 1st nuclear weapon test: <strong><em>Trinity</em></strong> in White Sands Proving Grounds, New Mexico.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">called “The Gadget.” 20 kT (84 TJ). Pu implosion device</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Types of Nuclear Weapons Mechanism</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219002342388.png" alt="image-20221219002342388" style="zoom: 25%;" /></td>
      <td style="text-align: left">normal <strong>plutomium</strong> okay, but densely pressed is not = can go over critical mass</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219002349437.png" alt="image-20221219002349437" style="zoom:25%;" /></td>
      <td style="text-align: left">simply have separate halfs of sphere = explosion <strong>combine</strong> them = increase density = go react<br />now it is <strong>uranium</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Why no Pu+Gun Barrel? <br />1. Because plutonium is less dense than HEU, it would require a larger mass to reach criticality<br />2. even trace amounts of Pu-240 in the plutonium would release enough neutrons from <strong>spontaneous fission</strong> = wouldn’t explode but fizzle</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Nuclear Power Plant</td>
      <td style="text-align: left">12/20/1951 – <strong>EBR-1</strong>, Idaho National Lab, Idaho Falls, Idaho. First generation of electric power from nuclear power plant</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Delayed Energy and Delayed Neutrons</td>
      <td style="text-align: left">nuclear reactor core and spent fuel <em>continue to emit considerable energy</em> even after the nuclear chain reaction process has <em>stopped</em>. This happens because</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">1. fission fragments as a by-product are themselves every radio-active = release energy</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. delayed neutrons play a critical role in the control of nuclear fission reactions because they are emitted at a <em>slower rate than prompt neutrons</em>, which are emitted immediately following the fission event = helpful to make power change smoother = but still are neutrons</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219003325304.png" alt="image-20221219003325304" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Neutron Economics</td>
      <td style="text-align: left">each fission produces 2-3 neutrons of about 2MeV, but <em>prompt fission neutrons must be thermalized</em> to about 0.025 MeV</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">In the slowing down process, must avoid <br />1. absorption by other nuclei <br />2. leakage out of the system.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">3. (uncontrollable) absorbed neutron but gave non-fission reaction</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Chain Reaction</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219003615062.png" alt="image-20221219003615062" style="zoom:33%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Neutron Multiplication Factor</td>
      <td style="text-align: left">measure if your chain reaction is sustaining/increasing/decreasing</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219004107957.png" alt="image-20221219004107957" style="zoom: 25%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219004140291.png" alt="image-20221219004140291" style="zoom:25%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219004201393.png" alt="image-20221219004201393" style="zoom:25%;" /></td>
      <td style="text-align: left">recall the section on neutron economics</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Thermal Fission Factor</td>
      <td style="text-align: left"><em>probability of a fission event occurring</em> when a neutron collides with a nucleus</td>
      <td style="text-align: left">Not all neutrons absorbed in fuel cause fission</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219004515942.png" alt="image-20221219004515942" /></td>
      <td style="text-align: left">basically $\sigma_f + \sigma_a = \sigma_T$</td>
      <td>if 99.3% U-238 and 0.7% U-235, then the ratio is<br />$\frac{0.7*\sigma_f(^{235}U)}{(0.7\sigma_T(^{235}U)+99.3\sigma_T(^{238}U))}$</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"> </td>
      <td>because only $^{235}U$ can fission = Most nuclear reactors and all nuclear weapons <strong>require higher U-235 enrichment</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">Methods for U-235 Enrichment</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219005126907.png" alt="image-20221219005126907" style="zoom: 25%;" /></td>
      <td style="text-align: left">Gaseous Diffusion Enrichment: since U238 is a <strong>little bit heavier</strong> (cannot separate them chemically as they are the same, but physicaly not). Therefore smaller would go through more readily at the top</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">and you can repeat this process over and over</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219005222054.png" alt="image-20221219005222054" style="zoom:25%;" /></td>
      <td style="text-align: left">Gas Centrifuge Enrichment: again, U238 is a little bit heavier than U235, so another approach is to spin it very fast</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Enrichment Definitions</td>
      <td style="text-align: left">LEU – Low Enrichment Uranium: &lt; 20% U-235</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">HEU – High Enrichment Uranium: &gt; 20% U-235</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Weapons Grade Uranium: &gt; 90% U-235</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Components in a Nuclear Power Plant</td>
      <td style="text-align: left"><strong>Fuel pellets</strong> – typically, sintered UO2 enriched to 3-5% in 235U.</td>
      <td style="text-align: left">Fuel pellets and fuel rods are both used to store and transport nuclear fuel in nuclear reactors.</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Fuel rods</strong> – typically, clad in Zircalloy to contain fission products and gases</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Control</strong> rods – neutron absorbers among fuel assemblies</td>
      <td style="text-align: left">to regulate the number of neutrons available to sustain the chain reaction</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Reactor core – typically, “square cylinder” shape</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Coolant/<strong>moderator</strong> – light water most common</td>
      <td style="text-align: left">to slow down neutrons to thermal</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><strong>Structural</strong> material – alloys of steel (absorber/reflector)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">PWR and BWR</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219005748886.png" alt="image-20221219005748886" style="zoom: 33%;" /></td>
      <td style="text-align: left">pressurized, transfer of heat energy</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219005817822.png" alt="image-20221219005817822" style="zoom: 33%;" /></td>
      <td style="text-align: left">could have secondary radiation, but more efficient</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">PWR Schematic</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219005940524.png" alt="image-20221219005940524" style="zoom:50%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="fusion">Fusion</h1>

<p>Why could fusion be useful if we had fission already?</p>

<ul>
  <li>much less radioactive waste</li>
  <li>but the major problem is it is <strong>technically harder</strong></li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Name of Concept/Equation</th>
      <th style="text-align: left">Definition/Equation</th>
      <th style="text-align: left">Notes</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Why is Fusion Energetic Useful?</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219013327992.png" alt="image-20221219013327992" style="zoom:50%;" /></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Extra Requirement for Fusion</td>
      <td style="text-align: left">since nuclei are positively charged, so must impart <strong>high kinetic energies</strong> to the to overcome the repulsive Coulomb barrier = close enough for <strong>strong force</strong> to form new nucleus</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">1. high temperature (i.e., kinetic energy)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. high density (close enough)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">3. confinement time: amount of time that the plasma must be confined in order for the reactions to occur</td>
      <td style="text-align: left">later on will see that density and confinement determines how much energy can be gained</td>
      <td><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219014048587.png" alt="image-20221219014048587" style="zoom: 50%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">Magnetic Confinement Basics</td>
      <td style="text-align: left">confine plasma using magnetic fields = quite complicated setup</td>
      <td style="text-align: left">because plasma = lot of <strong><em>charged</em></strong> particles in gaseous form</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">works based on Lorentz Force</td>
      <td style="text-align: left">but will lose them if they move in parallel of the field!</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219015741263.png" alt="image-20221219015741263" style="zoom:33%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Inertial Confinement</td>
      <td style="text-align: left">Heat and fuse ions so rapidly that they do not have time to escape.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">typically use laser beams to compress and heat fuel pellets.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Fusion Reaction Examples</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219014333643.png" alt="image-20221219014333643" /></td>
      <td style="text-align: left">Most uses isotopes of hydrogen to <mark>minimize repulsion</mark></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">the highlighted ones have high cross section, and non-gamma energy release = can be more easily retrained</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Helium as a product is <mark>very stable</mark>, hence releases a lot of energy!</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Properties of DT Fusion Reactions</td>
      <td style="text-align: left">high cross section</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">produces a lot of energy</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">produces an alpha particle, which can be <strong><em>re-used to heat up the plasma</em></strong></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">shield against neutrons, which are also very energetic</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">tritium (T) don’t occur naturally, so often need to be bred in the reactor factory</td>
      <td style="text-align: left">can be bred from Lithium</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Why would DT be preferred?</td>
      <td style="text-align: left">1. requires lower energy<br />2. produces more energy</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219015116694.png" alt="image-20221219015116694" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Reaction Rate of Fusion</td>
      <td style="text-align: left">Reaction prob per unit time = $n_2 \sigma v$</td>
      <td style="text-align: left">$v$ speed of the particles<br />$n_2$ is density of particle 2</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">considers prob of particle 1 interacting with particle 2</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">total reaction rate per unit volume: $R=n_1n_2\sigma v$</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">$R=n_1n_2 \lang \sigma v \rang$</td>
      <td style="text-align: left">since plasma = gas particles = velocity is defined by Boltzmann distribution</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Fusion Energy Output and Break-Even</td>
      <td style="text-align: left">Fusion energy output = $E_f=n_1n_2\lang \sigma v\rang Q \tau$</td>
      <td style="text-align: left">$\tau$ is <mark>confinement time</mark>!<br />$Q$ is energy released per reaction</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">also includes <mark>density $n_1,n_2$</mark></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">Break-Even $n\tau &gt; \frac{12kT}{\lang v \sigma \rang Q}$</td>
      <td style="text-align: left"><strong>Lawson Criteria</strong>: need to achieve this to get more energy out than input</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219015523087.png" alt="image-20221219015523087" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Magnetic Bottle Confinement</td>
      <td style="text-align: left">1. Closed-field geometry (e.g., torus)</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. <strong>Toroidal field</strong> (TF) – Coils external and perpendicular to toroidal containment vessel generate TF</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">3. <strong>Poloidal field</strong> (PF) – pass current around axis of torus. Compensates for weakening TF with increasing $r$</td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219015910587.png" alt="image-20221219015910587" style="zoom: 25%;" /></td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><mark>Tokamak</mark> is one famous example!</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Inertial Confinement Mechanism</td>
      <td style="text-align: left">1. the energy from the laser or particle beam pulses is absorbed by the fuel target, heating it to extremely high temperatures and causing it to undergo a rapid <strong>expansion</strong>.</td>
      <td style="text-align: left">difficulty is to generate sufficient power to achieve this</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">2. This expansion creates a shock wave that <strong>compresses</strong> the fuel target, increasing the density and temperature of the plasma and creating conditions suitable for fusion reactions.</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left">therefore, requires lasers and <strong>fuel pellets</strong></td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">National Ignition Facility</td>
      <td style="text-align: left">NIF: 192 laser beams focused onto small target, uses Indirect target</td>
      <td style="text-align: left"> </td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"><img src="/lectures/images/2022-12-20-APPH4010_Intro_to_Nuclear/image-20221219020446322.png" alt="image-20221219020446322" /></td>
      <td style="text-align: left">Laser pulse vaporizes the heavy metal case, generating intense <strong>x-rays inside the hohlraum</strong>, which compresses and heats the DT fuel</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Hohlraum = the metal cylinder</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="final">Final</h1>

<ul>
  <li>Nuclear sturcture
    <ul>
      <li>spin has an effect on energy</li>
      <li>know every term in SEMF, what it represents</li>
      <li>pairing term</li>
      <li>vibrational harmnoics shapes</li>
      <li>nuclear potential energy</li>
    </ul>
  </li>
  <li>Nuclear Instability
    <ul>
      <li>gamma emission comes from nucleus, actually comes fro the daughter</li>
      <li>selection rules for gamma emission, including the table</li>
      <li>transition ratess</li>
      <li>Decay schemes
        <ul>
          <li>high polarity and high half life</li>
        </ul>
      </li>
      <li>internal conversion (eject electron)</li>
      <li>beta decay three possible transitions
        <ul>
          <li>tell by excess protons or neutrons whether if it goes beta plus or beta minus</li>
        </ul>
      </li>
      <li>alpha decay
        <ul>
          <li>QM consideration that as long as Q is positive it will happen</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Nuclear Reactions
    <ul>
      <li>classification
        <ul>
          <li>scattering, inelastic scattering; knockout</li>
          <li>direct reactions and compound-nucleus decay</li>
        </ul>
      </li>
      <li>cross section and density equation and reaction rate
        <ul>
          <li>estimate the cross section such as assuming no coulomb interactions with $\pi R^2$; but with intearaction $\pi b^2$</li>
          <li>cross section calculation for charged particles</li>
          <li>no need to know partial waves</li>
          <li>compound nuclear reactions equation</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Fission
    <ul>
      <li>binding energy curve and why fission is possible</li>
      <li>high cross section with thermal neutrons</li>
      <li>fission activation energy - when you pass the poit of this yuo can fission</li>
      <li>fission history and discovery of neutron</li>
      <li>chain reaction graph of uranium
        <ul>
          <li>neutron multiplication factor</li>
          <li>reactors would want to stay at $k=1$, which is controlled by control rods</li>
          <li>why enrichment is needed</li>
        </ul>
      </li>
      <li>nuclear reactor basics
        <ul>
          <li>fuel pellets</li>
          <li>Pressurized WR and BWR designs; difference is that water doesn’t boil and goes into heat exchange</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Fusion
    <ul>
      <li>why fusion is possible in binding energy curve
        <ul>
          <li>but the hard part is to have a high temperature, etc.</li>
          <li>magnetic confinement v,s, inertial confinement: heating them so quickly</li>
        </ul>
      </li>
      <li>reaction rate for fusion equation with $R=n_1n_2 \sigma v$</li>
      <li>triple product and lawson criterion</li>
      <li>toroidal fields, know stuff like this exists
        <ul>
          <li>tokamak</li>
        </ul>
      </li>
      <li>inertial confinement
        <ul>
          <li>using lasers</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>some shared topisc shuch as alpha and beta decay</p>
  </li>
  <li>Energy calculation, very practical</li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Equations and Concepts for Intro to Nuclear]]></summary></entry><entry><title type="html">ELEN6885 Reinforcement Learning part2</title><link href="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2.html/" rel="alternate" type="text/html" title="ELEN6885 Reinforcement Learning part2" /><published>2022-12-20T00:00:00+00:00</published><updated>2022-12-20T00:00:00+00:00</updated><id>/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2</id><content type="html" xml:base="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning_part2.html/"><![CDATA[<h1 id="rl-packages">RL packages</h1>

<p>Now we start to discuss using DL for modeling. Some commonly used packages include</p>

<p><strong>Mainly for this course</strong></p>

<ul>
  <li>openAI <code class="language-plaintext highlighter-rouge">gym</code> mainly an online environment for bench-marking for algorithm performance</li>
</ul>

<p><strong>Other</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">stable-baselines3</code> implementations of many modern RL algorithms</li>
  <li><code class="language-plaintext highlighter-rouge">rllib</code> includes ray tuning</li>
</ul>

<blockquote>
  <p>Resources</p>

  <ul>
    <li>colab tutorial: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb</li>
    <li>openai doc: https://www.gymlibrary.dev/</li>
    <li>stable-baselines3 doc: https://stable-baselines3.readthedocs.io/en/master/</li>
  </ul>
</blockquote>

<h2 id="openai-gym">OpenAI Gym</h2>

<blockquote>
  <ul>
    <li>Separate the <mark>implementation of the environment</mark> from the implementation of the learning algorithm.</li>
    <li>Make it easy to compare various RL algorithms on the same/standardized environment.</li>
  </ul>
</blockquote>

<ul>
  <li>
    <p>installation, e.g. <code class="language-plaintext highlighter-rouge">gym[mujoco]</code></p>
  </li>
  <li>
    <p>has many environments which can be treated as <strong>benchmark</strong> “datasets”</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Control</th>
          <th style="text-align: center">Atari</th>
          <th style="text-align: center">Mujoco</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028153609045.png" alt="image-20221028153609045" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028161012225.png" alt="image-20221028161012225" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028161048421.png" alt="image-20221028161048421" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>and many more</p>
  </li>
</ul>

<p>As basically they are interactive environments, the <code class="language-plaintext highlighter-rouge">environment</code> is what is implemented, meaning your job is to <strong>determine what action to take</strong> (i.e. your model) given your state (e.g. history of rgb pixels of the game), and <strong>environment gives you a reward and next state</strong></p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028161310313.png" alt="image-20221028161310313" style="zoom:50%;" /></p>

<p>Specifically the <code class="language-plaintext highlighter-rouge">environment</code> will give you</p>

<ul>
  <li><strong>observation</strong>: the new state simulated by the environment</li>
  <li><strong>reward</strong>: defined and computed by the environment</li>
  <li><strong>done</strong>: A boolean indicating if episode has finished</li>
  <li><strong>info</strong>: additional info used only for debugging</li>
</ul>

<p>And the most important is the <mark>observation &amp; action space</mark> for each environment:</p>

<ul>
  <li>it will have an <code class="language-plaintext highlighter-rouge">action_space</code> specifying what are the legal actions
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Discrete</code>: A discrete space where $a(s) \in {0,1,…,n}$</li>
      <li><code class="language-plaintext highlighter-rouge">Box</code>: An n dimensional continuous space with optional bounds</li>
      <li><code class="language-plaintext highlighter-rouge">Dict</code>: A dictionary of space instances such as <code class="language-plaintext highlighter-rouge">Dict({"position": Discrete(2), "velocity": Discrete(3)})</code></li>
    </ul>
  </li>
  <li>it will have an <code class="language-plaintext highlighter-rouge">observation_space</code> specifying what will be returned
    <ul>
      <li>e.g. RGB pixel values of the screen</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>Examples: Cliff Walking Environment</em></p>

<p>The board is a 4x12 matrix, with (using NumPy matrix indexing):</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">[3, 0]</code> as the start at bottom-left</li>
  <li><code class="language-plaintext highlighter-rouge">[3, 11]</code> as the goal at bottom-right</li>
  <li><code class="language-plaintext highlighter-rouge">[3, 1..10]</code> as the cliff at bottom-center</li>
</ul>

<p>If the agent steps on the cliff, it returns to the start. An episode terminates when the agent reaches the goal.</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028161926749.png" alt="image-20221028161926749" style="zoom: 50%;" /></p>

<p><strong>Reward</strong></p>

<ul>
  <li>Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward.</li>
</ul>

<p><strong>Action Space</strong>: There are 4 discrete deterministic actions:</p>

<ul>
  <li>0: move up</li>
  <li>1: move right</li>
  <li>2: move down</li>
  <li>3: move left</li>
</ul>

<p><strong>Observation Space</strong>: returned as a flattened index to represent the coordinate of the system</p>

<ul>
  <li>There are 3x12 + 1 possible states. In fact, the agent cannot be at the cliff, nor at the goal (as this results in the end of the episode). It remains all the positions of the first 3 rows plus the bottom-left cell.</li>
  <li>The observation is simply the current position encoded as flattened index. You can easily convert back to coordinate system with <code class="language-plaintext highlighter-rouge">np.unravel_index</code></li>
</ul>

<hr />

<p><em>Example: Mujoco Humanoid</em></p>

<p>The 3D bipedal robot is designed to <strong>simulate a human</strong>. It has a torso (abdomen) with a pair of legs and arms. The legs each consist of two links, and so the arms (representing the knees and elbows respectively). The goal of the environment is to walk forward as fast as possible without falling over.</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221028162424416.png" alt="image-20221028162424416" /></p>

<p><strong>Action space</strong> is a <code class="language-plaintext highlighter-rouge">Box(-1, 1, (17,), float32)</code>. An action represents the <strong>torques</strong> applied at the hinge joints.</p>

<ul>
  <li>
    <p>action is a box has 17 different joints, each controlling torque from <code class="language-plaintext highlighter-rouge">[-1.0,1.0]</code></p>
  </li>
  <li>
    <p>example joints you can control: <code class="language-plaintext highlighter-rouge">hip_1 (front_left_leg), angle_1 (front_left_leg), hip_2 (front_right_leg), right_hip_x (right_thigh), right_hip_z (right_thigh), ...</code></p>
  </li>
</ul>

<p><strong>Observation Space</strong></p>

<ul>
  <li>Observations consist of <strong>positional</strong> values of different body parts of the Humanoid, followed by the <strong>velocities</strong> of those individual parts (their derivatives) with all the positions ordered before all the velocities</li>
</ul>

<p><strong>Rewards</strong>: the reward consists of four parts:</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">healthy_reward</code>: Every timestep that the humanoid is alive</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">forward_reward</code>: A reward of walking forward</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ctrl_cost</code>: A negative reward for penalizing the humanoid if it has too large of a control force.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">contact_cost</code>: A negative reward for penalizing the humanoid if the external contact force is</p>
  </li>
</ul>

<h2 id="using-openai-gym">Using OpenAI Gym</h2>

<p>A minimal working example would be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span> 
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>  <span class="c1"># show a window on your screen. In colab notebook more needs to be done
</span>    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># random policy
</span>    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> 
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span> 
<span class="n">env</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="stable-baselines-3">Stable-Baselines-3</h2>

<blockquote>
  <p>Stable Baselines3 (SB3) is a set of reliable <strong>implementations of reinforcement learning algorithms</strong> in PyTorch.</p>
</blockquote>

<p>Where OpenAI gym focuses on the environment, stable-baselines-3 focuses on the learning. But of course it is easily integratable with OpenAI <code class="language-plaintext highlighter-rouge">gym</code>. Smoe additional features include</p>

<ul>
  <li>access to most popular deep reinforcement learning algorithms. Ex: DQN, PPO, A2C, DDPG</li>
  <li>allow you to implement deep reinforcement learning solutions in just a few lines</li>
  <li><mark>vectorized training in parallel</mark> (e.g. instantiate parallel copies of agents)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">"CartPole-v1"</span><span class="p">)</span>  <span class="c1"># use mode='rgb_array' to return game image as state
</span>
<span class="c1"># training
</span><span class="n">model</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="s">"MlpPolicy"</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># multilayer perceptron + feedin environment
</span><span class="n">model</span><span class="p">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">10_000</span><span class="p">)</span>

<span class="c1"># testing
</span><span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> 
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span> 
<span class="n">env</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<h1 id="function-approximation">Function Approximation</h1>

<p>Recall that all evaluation and control algorithms we discussed are <strong>tabular methods</strong> (e.g. $Q(s,a)$)</p>

<p><img src="https://miro.medium.com/max/622/1*fe1eyEAcfj6KHWFXHUNHbQ.png" style="zoom:50%;" /></p>

<ul>
  <li>Each update will only change the value of one state or one state-action pair, i.e., an entry in the lookup table</li>
  <li>The lookup table <strong>may become unmanageable</strong> when the number of “states” or “state-action” pairs goes up. For instance, what if we have a <strong>continuous action space</strong>?</li>
  <li>also needed a way to keep exploring, because we needed to estimate values of a lot of states to be accurate</li>
</ul>

<blockquote>
  <p><strong>Use function approximation</strong> to treat $V(s)$ or $Q(s,a)$ as functions, i.e. we consider instead of tables, <mark>parameters of a function</mark> such that:</p>

  <ul>
    <li>for $V$: $S\to \mathbb{R}$</li>
    <li>for $Q: S,A \to \mathbb{R}$</li>
  </ul>

  <p>so we consider $V(s,w)$ and $Q(s,a,w)$ for $w$ being the weights/parameters of the function</p>
</blockquote>

<p>Function approximation methods has <strong>advantages</strong> for cases such as</p>

<ul>
  <li>evaluate/predict $v(s)$ and $q(s, a)$ for large or high-dimension state space</li>
  <li>evaluate/predict $v(s)$ and $q(s, a)$ for continuing tasks (non-episodic)</li>
  <li>evaluate/predict $v(s)$ and $q(s, a)$ for partially observable problems
    <ul>
      <li>because we can <strong>extrapolate</strong> from the experienced samples (e.g. states)</li>
    </ul>
  </li>
</ul>

<p>and is usally trained with <mark>supervised training</mark></p>

<ul>
  <li>e.g. given a $(s,G_t(s))$ pair, ask the model to do learn it</li>
  <li>use SGD to minimize the objective function such as MSE between $\hat{V}(s,w)$ and $V(s)$</li>
</ul>

<h2 id="value-function-approximation">Value Function Approximation</h2>

<p>We would like to consider $\hat{V}(s,w)$ and $\hat{Q}(s,a,w)$ parameterized by $w$:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221104083159491.png" alt="image-20221104083159491" style="zoom:33%;" /></p>

<p>where the target $V_\pi(s)$ and $Q_\pi(s,a)$ are basically predefined by the specific algorithms we want to use and our model aims to model them.</p>
<ul>
  <li>In general we can have three types of modelling, where the last one modelling $Q$ from $V$ is not used often in practice</li>
  <li>for MC methods, bascially $V_\pi(s) = G_t$ is used as target, and it differs if you do TD(0), for instance</li>
  <li>since those are aimed to improve on tabular methods, dimensoinaltiy of $w$ is typically much smaller of states $\vert S\vert$, and highly related to how many features you want to encode in each state</li>
  <li>finally, notice that updating one weight will influence value for all states (cf. tabular methods)</li>
</ul>

<p>There are <strong>many ways</strong> to construct your function approximator, e.g.:</p>

<ul>
  <li>
    <p><strong>Linear:</strong> consider we having <em>three features</em> of a state $s=[s_1,s_2, s_3]^T$.  Then we can do:</p>

\[V(s,w) = s_1 w_w + s_2 w_2 + s_3 w_3 = [s_1, s_2,s_3]^T 
\begin{bmatrix}w_1\\
w_2\\
w_3\end{bmatrix}\]

    <p>or you can construct nonlinear features usch as $w_1*w_2$</p>
  </li>
  <li>
    <p><strong>non-linear:</strong> using neural networks</p>
  </li>
  <li><strong>decision tree</strong></li>
  <li>etc.</li>
</ul>

<blockquote>
  <p>Here we consider <mark>differentiable</mark> function approximation methods.</p>
</blockquote>

<p>For instance, let $w$ be a column vector. Then we can consider</p>

\[J(w) = \mathbb{E}[ (\hat{V}(s,w) - V_\pi(s) )^2 ]\]

<p>to minimize this loss using SGD (single sample per batch) type approach to consider</p>

\[\Delta w = - \frac{1}{2} \alpha \nabla_w J(w)\]

<p>if you perform the chain rule and take derivatives you get:</p>

\[\Delta w = \alpha \mathbb{E}_\pi [(v_\pi(s) - \hat{v}(s,w)) \nabla_w \hat{v}(s,w)]\]

<p>Finally, as we are doing SGD:</p>

\[\Delta w = \alpha  \underbrace{(v_\pi(s) - \hat{v}(s,w))}_{\text{prediction error}} \underbrace{\nabla_w \hat{v}(s,w)}_{\text{gradient}}\]

<p>note that</p>
<ul>
  <li>the above form is <mark>generic if you use MSE</mark></li>
  <li>$v_\pi(s)$ depends on what algorithm you use, e.g. MC means $v_\pi(s) = G_t$ is calculated at the end of episode.</li>
</ul>

<hr />

<p><em>Example: Simple SGD updates</em></p>

<p>Let the target be $y$, and the input features are 1-D $x$. So that you get the following dataset</p>

\[(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\]

<p>Suppose your approximator has two parameters:</p>

\[\hat{V}(x) = w_1  + w_{2} x\]

<p>Then your MSE loss is simply:</p>

\[J(w) = \sum\limits_{i=1}^{n} (\hat{V}(x_i) - y_i)^2 = \sum\limits_{i=1}^{n} (w_1 + w_2 x_i - y_i)^2\]

<p>Therefore, in the case of SGD your per step weight update with learning rate $\eta$ is</p>

\[\begin{bmatrix} 
    w_1\\
    w_2 
\end{bmatrix} \gets
\begin{bmatrix} 
    w_1\\
    w_2
\end{bmatrix} - \eta \nabla_w J(w)
= \begin{bmatrix} 
    w_1\\
    w_2
\end{bmatrix} - \eta
\begin{bmatrix} 
    2(w_1 + w_2 x_i - y_i)\\
    2x_i(w_1 + w_2 x_i - y_i)
\end{bmatrix}\]

<h2 id="rl-prediction-with-value-approximation">RL Prediction with Value Approximation</h2>

<p>Now, we employ VFA  methods to perform RL. Recall that in general we need two things:</p>

<ul>
  <li><strong>evaluation/prediction</strong> of $v_\pi(s)$ and $q_\pi(s,a)$</li>
  <li><strong>control/planning</strong> to find optimal policy (usually by using $q_\pi(s,a)$)</li>
</ul>

<p>Here, we first discuss the problem of prediction. As essentially it becomes supervised learning</p>
<ol>
  <li>agent interact using policy $\pi$ to collect experience/dataset</li>
  <li>since the collected signals are only <strong>rewards</strong>, you usually need to then specify a <strong>target</strong> for the model to learn</li>
  <li>now you have a supervised dataset, and you can use SGD to train your model</li>
</ol>

<p><em>For instance</em>:</p>

<ul>
  <li>
    <p>if you are using MC methos, then your computed target is simply $G_t = \sum\limits_{k=t}^{T} \gamma^{k-t} r_k$ so that your weight updates become</p>

\[\nabla w = \alpha \underbrace{( \textcolor{red}{G_t} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)\]
  </li>
  <li>
    <p>if you are using TD methods, then <mark>technically your target is $R_{t+1}+\gamma V_\pi(S_{t+1})$</mark>, but since you don’t know $V_\pi$, you consider:</p>

\[\nabla w = \alpha \underbrace{( \textcolor{red}{R_{t+1}+\gamma \hat{v}(S_{t+1},w)} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)\]

    <p>note that as our target is technically also parametrized by $w$ but we did not take its gradient, it is also called <strong>semi-stochastic gradient descent</strong>. (e.g. in DQN, we would use old $\hat{v}(S_{t+1},w^-)$ as targets and update it over time)</p>
  </li>
  <li>
    <p>for TD($\lambda$), the target is $G_t^\lambda$:</p>

\[\nabla w = \alpha \underbrace{( \textcolor{red}{G^{\lambda}_t} - \hat{v}(S_t,w) )}_{\text{prediction error}} \nabla_w \hat{v}(S_t,w)\]

    <p>which is the forward-view. The <strong>backward-view</strong> update is shown in the section <a href="#TD($\lambda$)_with_Value_Function_Approximation">TD($\lambda$) with Value Function Approximation</a></p>
  </li>
</ul>

<h3 id="linear-function-approximation">Linear Function Approximation</h3>

<p>We can consider a simple case study of using linear function approximation to predict $v_\pi(s)$.</p>

<p>Your general pipeline would be:</p>

<ol>
  <li>determine your feature representation $x(s)$ for each state $s \in S$:</li>
</ol>

\[\boldsymbol{x}(S) = \begin{bmatrix} 
      x_1(S)\\
      \vdots\\
      x_n(S) 
  \end{bmatrix}\]

<ol>
  <li>
    <p>define your <strong>model</strong>. Here we consider a linear approximator, we consider:</p>

\[\hat{v}(S, w) = \boldsymbol{x}(S)^{T} \boldsymbol{w} = \sum\limits_{j=1}^{n} x_{j} (S)w_j\]
  </li>
  <li>
    <p>define your <strong>objective</strong>: consider given the true value $v_\pi(S)$:</p>

\[J(\boldsymbol{w}) = \mathbb{E}_\pi[(v_\pi(S) - \boldsymbol{x}(S)^{T} \boldsymbol{w} )^2]\]
  </li>
  <li>
    <p>perform gradient descent. Using SGD the update rule for weights become:</p>

\[\begin{align*}
 \nabla_{w} \hat{v}(S, w) &amp;= \boldsymbol{x}(S)\\
 \Delta w &amp;= \alpha \underbrace{(v_\pi(S) - \boldsymbol{x}(S)^{T} \boldsymbol{w} )}_{\text{prediction error}} \boldsymbol{x}(S)\\
\end{align*}\]
  </li>
</ol>

<p>Note that in practice:</p>
<ul>
  <li>very important to choose the right features and the representations</li>
  <li>for linear approximators, the gradient $\nabla_{w} \hat{v}(S, w) = \boldsymbol{x}(S)$ is just the feature vectors</li>
</ul>

<p><strong>Special Example: modelling table look up</strong></p>

<p>We can actually use a linear approximator and <mark>get back tabular methods</mark>.</p>
<ul>
  <li>we can make feature to be onehot of states</li>
  <li>then weights are basically the values in the table lookup</li>
</ul>

\[\boldsymbol{x}(S) = \begin{bmatrix} 
    1(S=s_1)\\
    \vdots\\
    1(S=s_n)
\end{bmatrix}\]

<p>Then a linear approximator gives</p>

\[\hat{v}(S, w) = \boldsymbol{x}(S) = \begin{bmatrix} 
    1(S=s_1)\\
    \vdots\\
    1(S=s_n)
\end{bmatrix} \cdot  \begin{bmatrix} 
    w_1\\
    \vdots\\
    w_n 
\end{bmatrix}\]

<p>so that essentially $\hat{v}(S_i) = w_i$ is like a table lookup.</p>

<h3 id="mc-with-value-function-approximation">MC with Value Function Approximation</h3>

<p>Recall that for MC methods, we consider a target of $G_t$. Therefore:</p>

<ol>
  <li>
    <p>experiences $(S_i, A_i, R_i)$ and compute $(S_i, G_i)$ pairs to get training data</p>

\[(S_1, G_1), (S_2, G_2), \dots, (S_T, G_T)\]
  </li>
  <li>
    <p>then just perform supervised learning to train your model</p>
  </li>
</ol>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221104092845712.png" alt="image-20221104092845712" style="zoom: 50%;" /></p>

<p>Note that</p>

<ul>
  <li><strong>theoretically</strong>, in order to guarantee convergence, $\alpha$ should be decreasing but needs infinite visit of all states (the usual stochastic assumption) $\sum \alpha = \infty, \sum \alpha^2 &lt; \infty$, for instance $\alpha = \frac{1}{t}$</li>
  <li>but in reality people just use constant $\alpha$ and it works fine</li>
</ul>

<h3 id="td0-with-value-function-approximation">TD(0) with Value Function Approximation</h3>

<p>Recall that the TD Target $R_{t+1} + \gamma \hat{v}(S_{t+1},w)$ is <strong>biased</strong> as we are using $\hat{v}(S_{t+1},w)$ to estimate $v\pi(S_{t+1})$.</p>

<p>But you can still regardless compute the supervised signal as $(S_i, R_{i+1} + \gamma \hat{v}(S_{i+1},w))$ hence</p>

<ol>
  <li>
    <p>collect experience hence a dataset of</p>

\[(S_1, R_2 + \gamma \hat{v}(S_2,w)), (S_2, R_3 + \gamma \hat{v}(S_3,w)), \dots, (S_{T-1}, R_T + \gamma \hat{v}(S_T,w))\]
  </li>
  <li>
    <p>then again supervised training on your $\hat{v}(S,w)$ with SGD:</p>

\[\nabla \boldsymbol{w} = \alpha \underbrace{(R_{t+1} + \gamma \hat{v}(S_{t+1},w) - \hat{v}(S_t,w))}_{\text{prediction error}} \nabla \hat{v}(S_t,w)\]

    <p>or sometimes people just write (as TD is used often)</p>

\[\nabla \boldsymbol{w} = \alpha \delta \nabla \hat{v}(S_t,w)\]
  </li>
</ol>

<p>Then the rest of the algorithm is basically the same as the MC version. Note that</p>
<ul>
  <li>again, this is not fully stochastic = semi-stochastic gradient as that target $R_{i+1} + \gamma \hat{v}(S_{i+1},w)$ is parameterized yet no gradient is taken</li>
  <li>examples of this type of algorithm is DQN, and active research is on improving this semi-gradient algorithm</li>
</ul>

<h3 id="tdlambda-with-value-function-approximation">TD($\lambda$) with Value Function Approximation</h3>

<p>The target for TD($\lambda$) is $G_t^\lambda$, which is a weighted sum of TD targets and is still biased of the true value $v_\pi(S_t)$.</p>

<p>But regardless:</p>
<ol>
  <li>
    <p>collect experience hence a dataset of</p>

\[(S_1, G_1^\lambda), (S_2, G_2^\lambda), \dots, (S_T, G_T^\lambda)\]
  </li>
  <li>
    <p>then again supervised training on your $\hat{v}(S,w)$ with SGD. First the <mark>forward view</mark> is simply:</p>

\[\nabla \boldsymbol{w} = \alpha \underbrace{(G_t^\lambda - \hat{v}(S_t,w))}_{\text{prediction error}} \nabla \hat{v}(S_t,w)\]
  </li>
  <li>
    <p>then we derive the update rule being the <mark>backward view</mark>:</p>

\[\begin{align*}
\delta_{t} &amp;= R_{t+1} + \gamma \hat{v}(S_{t+1},w) - \hat{v}(S_t,w)\\
E_{t} &amp;= \gamma \lambda E_{t+1} + \nabla \hat{v}(S_t,w) \\
\nabla \boldsymbol{w} &amp;= \alpha \delta_{t} E_{t}
\end{align*}\]
  </li>
</ol>

<p>So we get the algorithm being:
<img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221104155114.png" style="zoom:50%;" /></p>

<h3 id="convergence-of-prediction-algorithms">Convergence of Prediction Algorithms</h3>

<p>In general TD methods are fast to learn but are unstable.</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221104155252.png" style="zoom:50%;" /></p>

<h2 id="rl-control-with-function-approximation">RL Control with Function Approximation</h2>

<blockquote>
  <p>How do we find the <strong>optimal policy</strong> once we estimated the value or action-value function using VFA?</p>
</blockquote>

<p>Recall that as we are in the case of model-free control, we generally consider:</p>
<ul>
  <li>estimate <mark>action-value</mark> function $Q_\pi(S,A)$</li>
  <li>policy improvement using $\epsilon$-greedy policy of $Q_\pi(S,A)$</li>
</ul>

<p>Hence the general policy iteration (still applies) basically looks like</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221104093515112.png" alt="image-20221104093515112" style="zoom:50%;" /></p>

<blockquote>
  <p>As policy improvement is based on $Q_\pi(S,A)$, all we need to do is to <strong>estimate $Q_\pi(S,A)$ using VFA.</strong> This essentially is the same as fitting $V_\pi(S)$ but use $Q$ instead in formulas.</p>
</blockquote>

<p>Hence our model considers</p>

\[\hat{q}(S,A,w) \approx \hat{q}_\pi(S,A)\]

<p>then to fit it, we consider</p>

\[J(\boldsymbol{w}) = \mathbb{E}_{\pi}[(\hat{q}(S,A,w) - q_\pi(S,A))^2]\]

<p>and using SGD the gradient is</p>

\[\nabla w = \alpha \underbrace{(q_\pi(S,A) - \hat{q}(S,A,w))}_{\text{prediction error}} \nabla \hat{q}(S,A,w)\]

<p>and based on different algorithms you use different targets to compute $q_\pi(S,A)$</p>

<p>As it is basically the same as the value function case, we will quickly show:</p>

<ul>
  <li>
    <p>for MC methods, the target is $G_t$</p>

\[\nabla w = \alpha \underbrace{(\textcolor{red}{G_t} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)\]
  </li>
  <li>
    <p>for TD(0), the target is the estimated TD target $R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w)$</p>

\[\nabla w = \alpha \underbrace{(\textcolor{red}{R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w)} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)\]
  </li>
  <li>
    <p>for <em>forward-view</em> TD($\lambda$) the target is $G_t^\lambda$</p>

\[\nabla w = \alpha \underbrace{(\textcolor{red}{G_t^\lambda} - \hat{q}(S_t,A_t,w))}_{\text{prediction error}} \nabla \hat{q}(S_t,A_t,w)\]

    <p>and the <strong>backward view update</strong> rule is</p>

\[\begin{align*}
\delta_{t} &amp;= R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},w) - \hat{q}(S_t,A_t,w)\\
E_{t} &amp;= \gamma \lambda E_{t+1} + \nabla \hat{q}(S_t,A_t,w) \\
\nabla \boldsymbol{w} &amp;= \alpha \delta_{t} E_{t}
\end{align*}\]
  </li>
</ul>

<p>the algorithms are again basically the same as the value function case.</p>

<h3 id="convergence-of-control-algorithms">Convergence of Control Algorithms</h3>

<p>Problem mostly occurs when you do TD and Non-linear function approximation.</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221104160154.png" style="zoom:40%;" /></p>

<h2 id="batch-method-for-rl-applications">Batch Method for RL Applications</h2>

<p>Before, we are talking about SGD which uses a sample and throw it away after updates.</p>

<blockquote>
  <p><strong>Aim</strong>: We can be more sample efficient by sampling a batch and perform updates. Additionally, it also provides a better estimate to solve the least square problem.</p>
</blockquote>

<p>Consider a given experience dataset $D$ consisting of state-value pairs provided:</p>

\[\mathcal{D} = \{ (s_1, v_1^{\pi}), (s_2, v_2^{\pi}), ..., (s_T, v_T^{\pi}) \}\]

<p>then the best parameter that <mark>best fits $v_\pi$</mark> obviously needs to perform least square on <mark>all of them</mark></p>

\[LS(\boldsymbol{w}) = \sum_{t=1}^T (v_t^{\pi} - \hat{v}(s_t,w))^2
= T \cdot  \mathbb{E}_{\pi}[(v^{\pi} - \hat{v}(s,w))^2]\]

<p>or $T$ times the average error you make. Naive SGD for each sample would not work as recall</p>
<ul>
  <li>each piece of experience would be correlate as they will come from real experience</li>
  <li>but fitting VFA assumes that the samples are <strong>independent</strong></li>
</ul>

<p>therefore, they will be techniques such as experience replay to shuffle the order so data appears IID:</p>

<ol>
  <li>
    <p>given an experience consisting of state value pairs</p>

\[\mathcal{D} = \{ (s_1, v_1^{\pi}), (s_2, v_2^{\pi}), ..., (s_T, v_T^{\pi}) \}\]
  </li>
  <li>repeat
    <ol>
      <li>
        <p>sample state, value for experience</p>

\[(s_t, v_t^{\pi}) \sim \mathcal{D}\]
      </li>
      <li>
        <p>update the parameters using SGD</p>

\[\Delta w = \alpha (v_t^{\pi} - \hat{v}(s_t,w)) \nabla \hat{v}(s_t,w)\]
      </li>
    </ol>
  </li>
  <li>
    <p>converges to LS solution:</p>

\[\boldsymbol{w} = \arg\min_{w} LS(\boldsymbol{w})\]
  </li>
</ol>

<p>This basically is the SGD version of <strong>experience replay</strong> in DQN.</p>

<hr />

<p><em>Brief Introduction of DQN</em>:</p>

<p>Here we will deal with non-linear function approximation and the problem of <strong>correlation</strong>.</p>

<blockquote>
  <p><strong>Non-linear neural networks</strong>: needs a static training set + uncorrelated IID data during training</p>
</blockquote>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221104090857675.png" alt="image-20221104090857675" style="zoom: 33%;" /></p>

<p>DQN uses <mark>experience replay</mark> to remove correlations between consecutive samples of data and <mark>fixed Q-target</mark> to stabilize the learning process.</p>

<p>The overall algorithm consists of:</p>

<ol>
  <li>take action $a_t$ in state $s_t$ and observe reward $r_t$ and next state $s_{t+1}$</li>
  <li>store transition $(s_t, a_t, r_t, s_{t+1})$ in replay memory $D$</li>
  <li><strong>sample random minibatch</strong> of transitions $(s_j, a_j, r_j, s_{j+1})$ from $D$</li>
  <li>
    <p>compute Q-learning targets w.r.t to <strong>old network $\boldsymbol{w}^-$</strong> and optimize MSE:</p>

\[L(\boldsymbol{w}) = \mathbb{E}_{(s_j, a_j, r_j, s_{j+1}) \sim D}[(r_j + \gamma \max_{a'} Q(s_{j+1}, a', \boldsymbol{w^{-}}) - Q(s_j, a_j, \boldsymbol{w}))^2]\]
  </li>
  <li>batch gradient descent on $L(\boldsymbol{w})$ to update $\boldsymbol{w}$</li>
</ol>

<h1 id="policy-gradient">Policy Gradient</h1>

<p>Recall that our task is to find the best policy given some environment that we can interact with:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221112013422.png" style="zoom:30%;" /></p>

<p>The environment transit and returns new $s_{t+1}, r_{t+1}$ to the agent, <strong>and agent needs to spit out a $a_t$ given a state $s_t$</strong>.</p>

<p>The previous section considers first learnig an action-value function $Q_\theta(s,a)$ and then using it to find the optimal policy $\pi_\theta(a\vert s)$, e.g. doing greedy/epsilon-greedy policy, e.g. learn a $Q$ using MC, TD, SARSA, etc.</p>

<blockquote>
  <p><strong>can we perhaps directly learn a policy $\pi(a\vert s)$?</strong></p>
  <ul>
    <li>though we often still need some estimate of $Q$ to do policy improvement, but we can <mark>avoid doing $\arg\max$ over all actions to get a policy</mark></li>
  </ul>
</blockquote>

<p><strong>Advantages</strong></p>

<ul>
  <li>now action probability changes more smoothly, meaning better convergence</li>
  <li>easly for encoding exploratoin</li>
  <li>avoid the need of $\arg\max q(s,a)$ which can be costly if $\vert S\vert \times \vert A\vert$ is large</li>
</ul>

<p><strong>Disadvantages</strong></p>

<ul>
  <li>value-based could more efficient for a small number of states and actions</li>
  <li>hard to get an unbiased estimate through sampling (you will see that the gradient has an expected value term)</li>
  <li>many current approaches are still based on MC, hence many has high variance and could lower convergence speed</li>
</ul>

<h2 id="policy-objective-functions">Policy Objective Functions</h2>

<blockquote>
  <p>The first thing is to design an objective function to minimize. Note that we <strong>do not know a true policy</strong> but only an environment</p>

  <ul>
    <li>for $Q$/$V$ estimates, we can easily compute a loss from a target such as $r+\gamma Q$</li>
  </ul>
</blockquote>

<p>Approaches: we need some measure <mark>(a single score $\in \mathbb{R}$)</mark> of how good a policy then:</p>

<ul>
  <li>
    <p>in episodic environments, we can use the <strong>value function of the start state</strong></p>

\[J_1(\theta) = V^{\pi_\theta}(s_0) = \mathbb{E}_{\pi_\theta}[v_1]\]
  </li>
  <li>
    <p>in continuing environments, we can use the <strong>expected return</strong> (which is the average of all possible returns)</p>

\[J_{avV}(\theta) = \sum\limits_{s} d^{\pi_\theta} (s) V^{\pi_\theta}(s)\]

    <p>which is basically the average of all possible returns, where $d^{\pi_\theta}(s)$ is the state distribution of the states under the policy $\pi_\theta$</p>
  </li>
  <li>
    <p>in general, we can also use an average reward per time-step:</p>

\[J_{avR}(\theta) = \sum\limits_{s} d^{\pi_\theta} (s) \mathbb{E}_{\pi_\theta}[r]
 = \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a}\]
  </li>
</ul>

<p>and our aim is to <mark>maximize them</mark>. Given this, we now need to do gradient ascent:</p>

\[\Delta \theta = \alpha * \nabla_\theta J(\theta)\]

<p>and no minus sign in front. But what is this gradient?</p>

<ul>
  <li>to perform back-propagation, you need to have a derivative form of each operation = backed by math equation</li>
  <li>but considering the gradient above it is unclear what to do with the <strong>distribution of states $d^{\pi_\theta}(s)$ is also parameterized by $\theta$</strong></li>
</ul>

<h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2>

<blockquote>
  <p><strong>Aim</strong>: manipulate the expression of gradient so that it does not need to do the summations</p>
</blockquote>

<p>In a single step MDP problem we can fix the distribution of states:</p>

\[\begin{align*}
   \nabla J(\theta) 
   &amp;= \nabla \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &amp;= \sum\limits_{s} d^{\pi_\theta} (s) \nabla \sum\limits_{a} \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &amp;= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \nabla \pi_\theta(a|s) \mathcal{R}_s^{a} \\
   &amp;= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \frac{\nabla \pi_\theta(a|s) }{ \pi_\theta(a|s) } \mathcal{R}_s^{a}\\
   &amp;= \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) \underbrace{\nabla \ln \pi_\theta(a|s) }_{\text{score function} } \mathcal{R}_s^{a}\\
   &amp;= \sum\limits_{s} d^{\pi_\theta} (s) \mathbb{E}_{\pi_\theta}[\nabla_\theta \ln \pi_\theta(a|s) \mathcal{R}_s^{a}]
\end{align*}\]

<p>Overall (proof in the book), the theorem looks like:</p>

<blockquote>
  <p><strong>Policy Gradient Theorem</strong>: we can estimate the gradient for any differentiable policy $\pi_\theta$ and for any policy objective functions $J=J_1, J_{avR}$ or $\frac{1}{1-\gamma} J_{avV}$</p>

\[\begin{align*}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} \pi_\theta(a|s) Q^{\pi_\theta}(s,a) \\
&amp;\propto \sum\limits_{s} d^{\pi_\theta} (s) \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s)
=\mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]
\end{align*}\]

  <p>so that <mark>gradient does not flow through distribution $d$</mark>! note that the proportional up to a constant is fine because that can be absored into the learning rate $\alpha$</p>
</blockquote>

<hr />

<p><em>Example</em>: gradient of Softmax policy using linear function approximation</p>

<p>Consider an naive modelling of $\pi$:</p>

\[\pi_\theta(s,a) = x(s,a)^T \theta\]

<p>but recall that $\pi \in [0,1]$, yet this does not satisfy such a constraint. Hence we can consider</p>

\[\pi = \mathrm{softmax}(x(s,a)^T \theta)\]

<p>for $\theta$ is a vector here. In this specific case, the <strong>score function of this form</strong> can be further computed as</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111085101851.png" alt="image-20221111085101851" style="zoom: 33%;" /></p>

<p>and you can use it do the policy gradient.</p>

<p>However, <strong>what if your action is continous?</strong> For a continous probability, prob is zero given a state/action. Therefore, we need to instead model a PDF:</p>

\[a \sim \mathcal{N}(\mu(s), \sigma^2), \quad \mu(s) = x(s,a)^T \theta\]

<p>essentially modelling the mean to be modelled using VFA, and variance can either be pre-defined or also parametrized.</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221112025330.png" style="zoom:40%;" /></p>

<p>where each state has its own distribution, and the x-axis is the available actions you can take. Now, we can <strong>choose which action to take given a state we are at by sampling</strong> from the distribution.</p>

<p>Finally, in this case the score function is:</p>

\[\nabla_\theta \ln \pi_\theta(a|s) = \frac{a-\mu(s)}{\sigma^2} x(s,a)\]

<h2 id="mc-policy-gradient-reinforce">MC Policy Gradient (REINFORCE)</h2>

<p>Now we have already learned:</p>
<ol>
  <li>objective function for policy</li>
  <li>gradient ascent methods</li>
  <li>can compute gradient using gradient theorem
    <ul>
      <li>also specific forms if policy being softmax or Gaussian if continous</li>
    </ul>
  </li>
</ol>

<p>Recall that by policy gradient theorem:</p>

\[\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]\]

<blockquote>
  <p><strong>Reinforce</strong>: MC policy gradient method considers</p>
  <ul>
    <li>use return $G_t$ as an unbiased estimator of $Q^{\pi_\theta}(s,a)$</li>
    <li>use one state $S_t$ and action $A_t$ to estimate the gradient</li>
    <li>therefore stochastic gradient ascent and consider</li>
  </ul>

\[\Delta \theta_t = \alpha \nabla_\theta \ln \pi_\theta(A_t|S_t) G_t\]

</blockquote>

<p>and the algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111090125607.png" alt="image-20221111090125607" style="zoom:33%;" /></p>

<h2 id="actor-critic-methods">Actor-Critic Methods</h2>

<blockquote>
  <p><strong>Aim:</strong> as MC methods using $G_t$ have a high variance, we can perhaps directly estimate $Q^{\pi_\theta}(s,a)$ and use it to estimate the gradient</p>
  <ul>
    <li>this can also tell you how good is the new policy/improved</li>
  </ul>
</blockquote>

<p>Use a <mark>critic</mark> to estimate the action-value function:</p>

\[Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\]

<p>So we can directly compute the policy gradient:</p>

\[\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q_w(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]\]

<blockquote>
  <p>Therefore <strong>Actor-Critic methods</strong> are:</p>
  <ol>
    <li>Critic: update action-value function parameters $w$ (e.g. using TD(0) or TD($\lambda$), basically our previous section on VFA)</li>
    <li>Actor: update policy parameters $\theta$ using policy gradient suggested by critic</li>
  </ol>
</blockquote>

<p>Finally we approximate the policy gradient by stochastic sampling:</p>

\[\Delta \theta = \alpha \nabla_\theta \ln \pi_\theta(A_t|S_t) Q_w(S_t,A_t)\]

<p>An example using TD(0)-based estimate of Q, the Actor-Critic Algorithm looks like</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111091952917.png" alt="image-20221111091952917" style="zoom:40%;" /></p>

<p>where basically:</p>

<ul>
  <li>
    <p>line 5, 6 is data collection through interaction</p>
  </li>
  <li>
    <p>line 7 is TD error and with line 9 for improving the critic (derived from gradient descent of MSE objective)</p>
  </li>
  <li>
    <p>line 8 is policy gradient update</p>
  </li>
</ul>

<h3 id="compatible-function-approximation-theorem">Compatible Function Approximation Theorem</h3>

<p>A problem with actor-critic methods above is that approximating the policy gradient could <strong>introduce bias</strong>.</p>

<p>But it turns out that if we can choose VFA carefully, we can avoid introducing any bias</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111092902627.png" alt="image-20221111092902627" style="zoom:40%;" /></p>

<p>(proof skipped)</p>

<h2 id="policy-gradient-with-advantage-function">Policy Gradient with Advantage Function</h2>

<blockquote>
  <p><strong>Aim:</strong> we can <mark>reduce the variance</mark> of this existing policy gradient:</p>

\[\nabla_\theta J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]\]

  <p>because variance could come from how you estimate $Q^{\pi_\theta}(s,a)$, and the stochastic sampling of $S_t$ and $A_t$. This can be done by subtracting the baseline:</p>

\[Q^{\pi_\theta}(s,a) \to Q^{\pi_\theta}(s,a) - b(s)\]

  <p>so that intuitively, picking $b(s)=V(s)$ can “re-center” all the $Q^{\pi_\theta}(s,a)$ and therefore reduce the variance</p>
</blockquote>

<p>However, we could like our performance to <strong>still be unbiased</strong>, and it can be shown that this holds as long as baseline <strong>$b(s)$ a function not of $a$.</strong></p>

<p><em>Proof</em>: Consider the new gradient:</p>

\[\nabla \ln \pi_\theta (s,a) * [Q_w(s,a) - B(s)]\]

<p>we want to make sure the gradient is the same as the original one, therefore, $\mathbb{E}[\nabla \ln \pi * B(s)] = 0$ needs to be proved.</p>

<p>By definitino of expectation</p>

\[\begin{align*}
   \mathbb{E}_{s\sim \pi_\theta}[\nabla \ln \pi_\theta(s,a) * B(s)] 
   &amp;= \sum\limits_{s} d(s) \sum\limits_{a} \pi_\theta(a|s) \nabla \ln \pi_\theta(s,a) * B(s) \\ 
   &amp;= \sum\limits_{s} d(s) \sum\limits_{a} \pi_\theta(a|s) \frac{\nabla \pi_\theta(a|s)}{\pi_\theta(a|s)} B(s) \\
   &amp;= \sum\limits_{s} d(s) \sum\limits_{a} \nabla \pi_\theta(a|s) B(s) \\
   &amp;= \sum\limits_{s} d(s) B(s) \nabla \sum\limits_{a} \pi_\theta(a|s) \\
   &amp;= 0
\end{align*}\]

<p>since $\sum \pi = 1$ is constant, therefore the last $\nabla$ is zero. <mark>Notice that this worked because $B(s)$ does not depend on action</mark></p>

<p>What is a good $B(s)$ to choose? One example is to use $V(s)$, hence the advantage funtion</p>

<blockquote>
  <p><strong>Policy Gradient with Advantage Function</strong>:
the advantage function using $V(s)$ is:</p>

\[A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)\]

  <p>then the policy gradient is:</p>

\[\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} A^{\pi_\theta}(s,a) \nabla_\theta \ln \pi_\theta(a|s) \right]\]

</blockquote>

<p>So how do we estimate the advantage function?</p>

<ul>
  <li>
    <p>estimate both $V$ and $Q$ separately, need two more set of paramters</p>
  </li>
  <li>
    <p>or we can estimate only using one (i.e. the value function)</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111093932628.png" alt="image-20221111093932628" style="zoom:40%;" /></p>

    <p>so you only need to estimate $V$</p>
  </li>
</ul>

<h3 id="actor-and-critic-at-different-time-scales">Actor and Critic at Different Time Scales</h3>

<p>A final nuiances is that the we can vary how <strong>far ahead we look ahead</strong> by changing the time scale of actor and critic.</p>
<ul>
  <li>critic: estimating the critic can use TD($0$) until $G_t$</li>
  <li>actor: estimating  the actor can use different estiamte based on $V$</li>
</ul>

<p>When estimating $Q$, we can also have actor at different time scales</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221111094240856.png" alt="image-20221111094240856" style="zoom:40%;" /></p>

<p>We estimating the policy gradient, if we have only a value estimate $V^{\pi_\theta}$:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221112033819.png" style="zoom:38%;" /></p>

<h2 id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</h2>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221112033925.png" style="zoom:35%;" /></p>

<h1 id="planning-and-learning">Planning and Learning</h1>

<p>Here, we shift focus back to the problem of <strong>planning/control</strong>. We consider situations where:</p>

<ul>
  <li>
    <p>if you have access to a model, but your state/action space is enormous (DP is not a good idea), what is a <strong>good planning algorithm</strong>?</p>
  </li>
  <li>
    <p>if you have enough data and you can fit a model reasonably well, how do you use it to <strong>further improve your planning algorithms</strong> (such as SARSA)?</p>

    <ul>
      <li>
        <p>in reality the convergence of model could be faster than fitting value/action function because the later needs information to propagate</p>
      </li>
      <li>
        <p>but of course, fitting a model is not an easy task</p>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Model-based</strong> methods rely on planning as their primary component, while <strong>model-free</strong> methods primarily rely on learning.</p>

  <ul>
    <li>Model-based: DP, and MCTS (this chapter). Also called the problem of <em>planning</em></li>
    <li>Model-free: Q-learning, SARSA, etc. Also called the problem of <em>learning</em></li>
  </ul>

  <p>Of course, there are also hybrids, so that you can use a model/learned model to improve model-free algorithms, such as Dyna (this chapter)</p>
</blockquote>

<h2 id="rollout-algorithms-and-tree-search">Rollout Algorithms and Tree Search</h2>

<p>First, we discuss the model-based planning where model of the world is given.</p>

<p>For instance, consider the rule of Go game</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118132038782.png" alt="image-20221118132038782" style="zoom:50%;" /></p>

<p>so that to <strong>plan each step, we need to think ahead</strong>. Therefore, it might make sense to consider <mark>simulation as part of planning</mark>: if I take this step here, what happens next and can I win? So rollout algorithms in general involve</p>

<ul>
  <li>try <strong>all</strong> potentially high reward possibilities/moves <strong>starting from the current position</strong></li>
  <li><strong>estimate those moves=actions</strong> by simulation=rollout, and sometimes in combination with some value function estimate</li>
  <li>pick the best move (e.g. UCB), and repeat</li>
</ul>

<blockquote>
  <p>Rollout algorithms are <mark>decision-time planning algorithms</mark> based on Monte Carlo control applied to simulated trajectories that all <mark>begin at the current environment state</mark>.</p>

  <ul>
    <li>They estimate action values for a given policy by averaging the returns of many simulated trajectories that start with each possible action and then follow the given rollout policy.</li>
    <li>When the action-value estimates are considered to be accurate enough, the action (or one of the actions) having the highest estimated value is executed, after which the process is carried out anew from the resulting next state.</li>
  </ul>
</blockquote>

<p>This is also important as we talk about the details of an rollout algorithm: MCTS, which is that</p>

<blockquote>
  <p>It can be said the aim of a rollout algorithm (e.g. MCTS) is to <mark>improve upon the rollout policy; not to find an optimal policy</mark>.</p>

  <ul>
    <li>In some applications, a rollout algorithm can produce good performance even if the rollout policy is completely random.</li>
    <li>But the performance of the improved policy depends on properties of the rollout policy and the ranking of actions (e.g. Upper Confidence Bound) produced by the Monte Carlo value estimates.</li>
    <li>Intuition suggests that the <strong>better the rollout policy and the more accurate the value estimates</strong>, the <mark>better the policy produced</mark> by a rollout algorithm (e.g. MCTS) is likely be (but see Gelly and Silver, 2007, and in AlphaGo it more human-like policy instead of better policy may be better).</li>
  </ul>
</blockquote>

<h3 id="upper-confidence-bounds">Upper Confidence Bounds</h3>

<blockquote>
  <p><strong>Upper Confidence Bound:</strong> get a <em>better</em> balance between exploration and exploitation, besides e-greedy and softmax approach we have seen before.</p>

  <ul>
    <li>e-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain.</li>
    <li>It would be better to select among the non-greedy actions <strong>according to their potential for actually being optimal</strong>, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.</li>
  </ul>
</blockquote>

<p>The ultimate form of the simple version of UCB looks like, for a one-step MDP problem (i.e. without states)</p>

\[a_t = \arg\max_a \left[ Q_t(a) + c \sqrt{\frac{\ln n_t}{N_t(a)}} \right]\]

<p>where:</p>

<ul>
  <li>$n_t$ is the total number of actions you have taken until time $t$, and $N_t(a)$ is the total number of times an action $a$ is taken up to time $t$</li>
  <li>
    <p>$Q_t(a)$ is your current estimated return of action $a$</p>
  </li>
  <li>the $\arg\max$ is because we are doing the <em>upper confidence bound</em></li>
  <li>essentially the left term is your <strong>exploitation</strong>, and right term is <strong>exploration</strong> trying to visit less-visited states</li>
</ul>

<p>This form is derived from modeling the regret of each action you made, and <mark>minimize your regret</mark>.</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118134055442.png" alt="image-20221118134055442" style="zoom: 50%;" /></p>

<p>What does this have to do with UCB?</p>

<ul>
  <li>Lai and Robbins showed that the regret for the multi-armed bandit has to <strong>grow at least logarithmically w.r.t. the number of plays $n$</strong></li>
  <li>UCB is proved to <mark>grow the regret logarithmically in that case</mark> (by choosing actions from UCB), hence is optimal in that case</li>
</ul>

<p>We can visualize what UCB is doing to be first estimating the “confidence” of your current estimate, and then pick the ones that are most promising</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Confidence Bound</th>
      <th style="text-align: center">Upper Confidence Bound Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118084108912.png" alt="image-20221118084108912" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118084255212.png" alt="image-20221118084255212" style="zoom:25%;" /></td>
    </tr>
  </tbody>
</table>

<p>But in reality, UCB might not be easy to work with. Consider a realistic problem where you could have a large state space:</p>

\[a_t = \arg\max_a \left[ Q_t(s,a) + c \sqrt{\frac{\ln n_t}{N_t(s,a)}} \right]\]

<p>the $N_t(s,a)$ for many combinations will be zero until explored. Hence in those cases simple e-greedy of softmax could also work as well and is much simpler.</p>

<h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>

<p>Now we can talk about MCTS. The objective is to do planning given a model, and specifically to improve upon a rollout policy (by significant margin, e.g. improvement in computer Go from a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015.)</p>

<blockquote>
  <p>At its base, <strong>MCTS is a rollout algorithm</strong> as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively <strong>direct simulations toward more highly-rewarding trajectories</strong>.</p>
</blockquote>

<p>At its core, on high level the following is happening</p>

<ul>
  <li>MCTS is executed after encountering <strong>each new state</strong> to select the agent’s action for that state; it is <strong>executed again to select the action for the next state, and so on</strong>.</li>
  <li>MCTS incrementally extends the tree by <strong>adding nodes representing states that look promising</strong> based on the results of the simulated trajectories</li>
  <li>Outside the tree and at the leaf nodes the rollout policy is used for action selections, but at the states inside the tree something better is possible. For these states we have value estimates for at least some of the actions, so we can <strong>pick among them using an informed policy, called the tree policy, that balances exploration (e.g. using UCB)</strong>.</li>
</ul>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118135457036.png" alt="image-20221118135457036" style="zoom: 50%;" /></p>

<p>In more details and to explain what the above means, consider the overall algorithm iteratively performing</p>

<ol>
  <li>MCTS continues executing these four steps, starting each time at the tree’s root node, until no more time is left
    <ol>
      <li><strong>Selection</strong>. Starting at the root node, a <mark>tree policy (e.g. UCB) based on the action values</mark> attached to the edges of the tree traverses the tree to <mark>select a leaf node</mark></li>
      <li><strong>Expansion</strong>. The tree is expanded <strong>from the selected leaf node</strong> by <mark>adding one or more child nodes</mark> reached from the selected node via unexplored actions</li>
      <li><strong>Simulation</strong>. From the selected node, or from one of its newly-added child nodes (if any), <mark>simulation of a complete episode</mark> is run with actions selected by the <mark>rollout policy</mark>.
        <ul>
          <li>The result is a Monte Carlo trial with <strong>actions selected first by the tree policy and beyond the tree by the rollout policy</strong>.</li>
        </ul>
      </li>
      <li><strong>Backup</strong>. The <mark>return</mark> generated by the simulated episode is <mark>backed up to update, or to initialize, the action values</mark> attached to the edges of the tree traversed by the tree policy in this iteration of MCTS. <strong>No values are saved for the states and actions visited by the rollout policy</strong> beyond the tree, as shown by the little triangle branch being dotted and discarded. Simulation result is only used to <strong>backup the existing tree</strong>
        <ul>
          <li>basically update my estimates of how good each action is based on simulation (+ value estimate in AlphaGO)</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>Then, finally, an <strong>action</strong> from the root node (=<strong>the current state</strong> of the environment) is <strong>selected</strong> according to some mechanism that depends on the accumulated statistics in the tree; for example, e.g. largest action value accumulated by UCB</li>
  <li>After the environment transitions to a <strong>new state, MCTS is run again from the first step</strong> (often starting with a tree containing any descendants of this node left over from the tree constructed by the previous execution of MCTS)</li>
</ol>

<blockquote>
  <p>What does the accumulated estimate $Q(s,a)$ at each action node correspond to? In rollout algorithms where $s$ is the current state and <mark>$\pi$ is the rollout policy</mark>, averaging the returns of the simulated trajectories produces <mark>estimates of $q_\pi(s,a)$</mark> for each action $a$.</p>

  <p>Then the policy that selects an action in s that maximizes these estimates and thereafter follows $\pi$ (during simulation) is a good candidate for a policy that <strong>improves over $\pi$.</strong> The result is <mark>like one step of the policy-iteration algorithm of dynamic programming</mark></p>

  <ul>
    <li>you will see in AlphaGO we can potentially do better to estimate $q_*(s,a)$ hence choose the optimal action instead of just improvement</li>
  </ul>
</blockquote>

<hr />

<p><em>For example</em>: Let us be playing a game of GO. Let us say we have some tree already, and we would like to use UCB based on action value to be our <strong>tree policy</strong> to select a node</p>

<ol>
  <li>
    <p>Selection:</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118145758200.png" alt="image-20221118145758200" style="zoom:33%;" /></p>

    <p>where here, we have already made 7 simulations/rollouts hence the top node stores 7.</p>

    <ul>
      <li>
        <p>the $2/3$ at black node (at second level) means it’s <strong>black’s turn to act</strong>, but after this white action <strong>white has won $2/3$ times</strong> in the simulation</p>
      </li>
      <li>
        <p>if we are computing the UCB of each node for instance at the second level:</p>

\[\mathrm{UCB}(2/3) =\frac{2}{3} + \sqrt{2} \sqrt{\frac{\ln 7}{3}}\]

        <p>for using $c=\sqrt{2}$.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Expansion: we have selected a node and we want to explore some actions by adding a child node:</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118150319119.png" alt="image-20221118150319119" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Simulation: we simulate from the <strong>newly added node</strong> using a rollout policy (e.g. a <em>fast</em> agent trained supervised from online GO games)</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118150456863.png" alt="image-20221118150456863" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Back-Propagation: we <strong>discard the simulation trajectory</strong> but utilize the results to update our existing tree</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118150603388.png" alt="image-20221118150603388" style="zoom:33%;" /></p>

    <p>notice that all we need to update is the <mark>branch</mark>. (Recall that the white $1/1$ means its white’s turn but black has won $1/1$, hence the previous black node becomes $1/2$ as there are now in total $1+1=2$ simulations done and white lost one of them)</p>
  </li>
</ol>

<p>But of course, no algorithm is yet perfect:</p>

<table>
  <thead>
    <tr>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tree growth focuses on more promising areas</td>
      <td>Memory intensive: entire tree in memory</td>
    </tr>
    <tr>
      <td>Can stop algorithm anytime to get search results</td>
      <td>For complex problems, enhancement needed for good performance</td>
    </tr>
    <tr>
      <td>Avoid the problem of globally approximating an action-value function (as in DP)</td>
      <td> </td>
    </tr>
    <tr>
      <td>Convergence to minimax solution</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>High controllability = can insert your prior domain knowledge into tree policy, for instance</strong></td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="alphago">AlphaGO</h3>

<p>The famous AlphaGO in essence is a MCTS algorithm, but <strong>enhanced by several changes</strong>, e.g. make the GO simulation computationally fast using value functions.</p>

<p>The overall pipeline of AlphaGO involves:</p>

<ol>
  <li>
    <p>train a <strong>supervised policy $p_\sigma$</strong> cloning the plays of experts.</p>
  </li>
  <li>
    <p>use <strong>policy gradient</strong> to train another agent $p_\rho$, which can consistently beats (more than 80% of the time) $p_\sigma$</p>
  </li>
  <li>
    <p>obtain <strong>value function $V_\rho$</strong> from the strongest player $p_\rho$, hoping that $V_\rho \approx V_*$ of the optimal play</p>
  </li>
  <li>
    <p>use MCTS to play by</p>

    <ul>
      <li>
        <p>tree policy is UCB</p>
      </li>
      <li>
        <p><mark>expansion is done using the prior from $p_\sigma$</mark>, which is found to perform better than $p_\rho$ as the former is more diverse and human-like</p>
      </li>
      <li>
        <p><mark>backpropagation uses both simulation results and $V_\rho$</mark>, so that the value for each leaf node is</p>

\[V(s_L) = (1-\lambda) v_\theta (s_L) + \lambda z_L\]

        <p>and each edge/action is the mean evaluation of all simulations through that edge</p>

\[Q(s,a) = \frac{1}{N(s,a)}\sum_{i=1}^n \mathbb{1}(s,a,i)V(s_L^i)\]

        <p>so that $\mathbb{1}(s,a,i)$ indicates if the edge $(s,a)$ was traversed during the $i$-th simulation.</p>
      </li>
      <li>
        <p>at the end of simulation/MCTS, <mark>play policy</mark> is to select action with most visited state:</p>

\[a = \arg\max_{a} N(s,a)\]
      </li>
    </ul>
  </li>
</ol>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118152145169.png" alt="image-20221118152145169" style="zoom:40%;" /></p>

<p>more details see paper: https://www.nature.com/articles/nature16961</p>

<h2 id="sample-based-planning">Sample-Based Planning</h2>

<p>Now we have covered planning if given model: for small spaces DP can be gone, but more often MCTS which selectively expands search space is more tractable. However, what if we don’t know the model, i.e. model-free? Do we have to only use Q-learning/SARSA etc?</p>

<blockquote>
  <p>If model is unknown, we can <strong>first learn the model</strong> and then <strong>plan</strong>.</p>

  <ul>
    <li>once the model is fitted, we can do planning using DP, value iteration, policy iteration, MCTS, etc.</li>
    <li>or, we can use the model to <strong>generate <em>more</em> samples</strong> to fit $Q$ functions</li>
  </ul>
</blockquote>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118152715124.png" alt="image-20221118152715124" style="zoom:33%;" /></p>

<p>Recall that for a model, we need to <mark>spit out next state and reward</mark>:</p>

\[S_{t+1} \sim \mathcal{P}_\eta (S_{t+1}|S_t, A_t)\\
R_{t+1} \sim \mathcal{R}_\eta (S_{t+1}|S_t, A_t)\\\]

<p>for a model parameterized by $\eta$, and we typically can assume independence so that:</p>

\[\mathbb{P}(S_{t+1},R_{t+1}|S_t, A_t) = \mathbb{P}(S_{t+1}|S_t, A_t)\mathbb{P}(R_{t+1}|S_t, A_t)\]

<p>This then would be useful when learning a distribution model.</p>

<blockquote>
  <p>Some models produce a description of all possibilities and their probabilities; these we call <mark>distribution models</mark>. Other models produce just one of the possibilities, sampled according to the probabilities; these we call <mark>sample models</mark>.</p>

  <ul>
    <li>e.g. consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution.</li>
  </ul>

  <p>For algorithms such as Q-learning and SARSA, we just need a <strong>sample model</strong>. But for DP, we would need a <strong>distribution model</strong>.</p>
</blockquote>

<p>It is typically easier to learn a sample model, which can be done using <strong>supervised learning</strong></p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118092918058.png" alt="image-20221118092918058" style="zoom: 50%;" /></p>

<hr />

<p><em>For Example</em>: Small Table Lookup Example.</p>

<p>For small state spaces, you can learn a <strong>distribution model</strong> by simply counting:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118153701401.png" alt="image-20221118153701401" style="zoom: 45%;" /></p>

<p>So in the case of the following experience/data, you can easily fit a distribution model:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Experience</th>
      <th style="text-align: center">Model Learned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118093255770.png" alt="image-20221118093255770" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118093308625.png" alt="image-20221118093308625" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>in this case we calculated the model without supervised learning but directly computing, but the idea is the same.</p>

<p>Then, once we have a model, we can <em>sample</em> from it to do MC control, for instance:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118153938347.png" alt="image-20221118153938347" style="zoom: 40%;" /></p>

<p>Or of course, we can plan given a model directly by doing DP or MCTS.</p>

<h2 id="integrating-learning-and-planning">Integrating Learning and Planning</h2>

<p>There is also an approach to try to do <strong>learning and planning</strong>:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118154633165.png" alt="image-20221118154633165" style="zoom:33%;" /></p>

<blockquote>
  <p>Within a planning agent, there are at least two roles for real experience:</p>

  <ul>
    <li><strong>model-learning:</strong> it can be used to improve the model (to make it more accurately match the real environment)</li>
    <li><strong>direct RL:</strong> it can be used to directly improve the value function and policy using the kinds of reinforcement learning methods we have discussed</li>
  </ul>

  <p>Note how experience can improve value functions and policies either directly or <strong>indirectly via the model</strong>. It is the latter, which is sometimes called <mark>indirect reinforcement learning</mark>, that is involved in planning.</p>
</blockquote>

<p>An example algorithm is Dyna-Q: which combines Q-learning and model learning as shown below</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118095038634.png" alt="image-20221118095038634" style="zoom: 45%;" /></p>

<p>notice that step a)-d) is basically Q-learning, and step e)-f) further update your Q function using the model</p>

<ul>
  <li>
    <p>the intuition is that if you can explore more from your previous $(S,A)$, you could <strong>make better decisions sooner</strong> in the future when you met similar states, whereas just doing Q-learning makes that part slower</p>
  </li>
  <li>
    <p>For instance, as you can see here the more simulated experienced you gave to Q, it makes better decisions earlier and hence converge faster. i.e. <strong>While walking around the world according to the experience</strong>, I am changing dynamically/<strong>planning at each step as well</strong></p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118095318251.png" alt="image-20221118095318251" style="zoom: 40%;" /></p>

    <p>the idea of updating a bit sooner than until the end is like TD v.s. MC.</p>
  </li>
</ul>

<h2 id="unified-view-of-planning-and-learning">Unified View of Planning and Learning</h2>

<p>Just like how we Generalized Policy Iteration to contextualize model-free algorithms, the overall framework of planning and learning <strong>also shares high level similarity</strong>. The unified view we present in this chapter is that all state-space planning methods share a common structure</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221118155344361.png" alt="image-20221118155344361" style="zoom: 45%;" /></p>

<ol>
  <li>all state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and</li>
  <li>they compute value functions by updates or backup operations applied to simulated experience.</li>
</ol>

<h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>

<p>Deep L can model any components in RL such as policy, model, value function, but Deep L + RL is unstable and hard to get convergence</p>

<p>Current approaches, modeling:</p>

<ul>
  <li>value-based RL
    <ul>
      <li>approximating value function $Q^<em>$ or $V^</em>$ using a NN with weight $w$ or $\theta$</li>
      <li>e.g. Q-learning, SARSA, MC methods, TD methods, etc.</li>
    </ul>
  </li>
  <li>policy-based RL
    <ul>
      <li>search for and approximate the optimal policy function $\pi^*$</li>
      <li>stochastic policy = policy model outputs a distribution of actions (e.g. by Softmax, or Gaussian distribution), and then you sample from it</li>
      <li>deterministic policy = find a function that gives the action directly</li>
    </ul>
  </li>
  <li>model-based RL
    <ul>
      <li>build a model (transition function and reward) from samples</li>
      <li>when obtained a model, do planning</li>
    </ul>
  </li>
</ul>

<h2 id="dl-for-value-functions">DL for Value Functions</h2>

<p>Here we will discuss on algorithm that has been successful and can be used for control (by simply taking $a = \arg\max_a Q(s,a)$)</p>

<h3 id="deep-q-network-dqn">Deep Q-Network (DQN)</h3>

<p>As we have mentioned before, this aims to model the Q network</p>

\[Q(s,a,w)\approx Q^\pi (s,a)\]

<p>and your target is simply based on the Q-learning target, which gives you a loss function:</p>

\[\mathcal{L}(w) = \mathbb{E}\left[ \left( \underbrace{r+\gamma \max_{a'} Q(s',a',w)}_{\text{target}} - Q(s,a,w)  \right)^2 \right]\]

<p>notice that you need to forward pass twice for $(s,a)$ and $(s’,a’)$.</p>

<ul>
  <li>
    <p>Could there be some correlation there? Keep that in mind</p>
  </li>
  <li>
    <p>note that $a’$ will be computed by the 1) can model $Q(s,a) \to \mathbb{R}^{\vert A\vert }$ and then <em>index into the action $a$ to get the $Q$ value</em>. 2) then in this method, picking $\arg\max Q$ can be one <strong>using only one forward pass</strong></p>
  </li>
</ul>

<p>then you perform gradient descent, which would look like the same descent in Q-learning</p>

\[\frac{\partial \mathcal{L}(w)}{\partial w} = \mathbb{E}\left[ \left( r+\gamma \max_{a'}Q(s',a',w)-Q(s,a,w) \right) \frac{\partial Q(s,a,w)}{\partial w} \right]\]

<blockquote>
  <p>However, <strong>naive Q-learning oscillates or diverges with neural nets</strong></p>

  <ul>
    <li>data is sequential, hence there are correlations between successive samples</li>
    <li>policy could change rapidly with slight change in $Q$-values</li>
    <li>correlation between $Q$-value and target value during the loss computation</li>
    <li>scale of rewards and $Q$-values could vary and hence gradient might be unstable</li>
  </ul>
</blockquote>

<p>There are the core issues that DQN attempts to solve:</p>

<blockquote>
  <p>DQN provides a stable solution to deep value-based RL</p>

  <ul>
    <li><mark>experience replay</mark> to break correlation in <em>data/experience</em></li>
    <li>freeze <mark>target $Q$-network</mark> (periodically, for example update only after 10k steps) to break correlations between $Q$-network and target, and avoid policy changes too rapidly</li>
    <li><mark>clip or normalize reward</mark> so network can work adaptively to ranges of Q-values = robust gradients</li>
  </ul>
</blockquote>

<p>Therefore the algorithm below</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202090538120.png" alt="image-20221202090538120" style="zoom:40%;" /></p>

<h2 id="dl-for-policy-functions">DL for Policy Functions</h2>

<p>Here we aim to directly learn a policy, and we will cover methods such as</p>

<ul>
  <li>Actor-critic methods: DDPG, A3C</li>
  <li>Optimization methods: TRPO, GPS</li>
</ul>

<p>In general, policy gradient based methods are quite popular today, especially when continuous action spaces.</p>

<h3 id="deterministic-policy-gradient">Deterministic Policy Gradient</h3>

<p>Recall that during stochastic policy approximation, we considered modeling the (real) policy:</p>

\[\pi_\theta(a|s) \to \mathbb{P}[a|s;\theta]\]

<p>where we could do this by taking a Softmax of the output, or modeling parameters of a Gaussian distribution for continuous action space. But here, we might consider a <mark>deterministic policy</mark>:</p>

\[\mu_\theta(s)\to a\]

<p>Why does this matter? As $\mu_\theta$ is the network, and it <strong>no longer models a probability</strong> the stochastic policy gradient theorem we had before (using probability):</p>

\[\nabla J(\theta) \propto \mathbb{E}_{s \sim \pi_\theta} \left[ \sum\limits_{a} Q^{\pi_\theta}(s,a) \nabla_\theta \ln \underbrace{\pi_\theta(a|s)}_{\text{prob.}} \right]\]

<p><strong>no longer makes sense</strong> as using $\ln \mu_\theta(s)$ does not make sense any more. Therefore, people proved that there is a policy gradient for deterministic case:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202110405667.png" alt="image-20221202110405667" style="zoom:50%;" /></p>

<p>where the objective function in this case would be:</p>

\[J(\theta) = \mathbb{E}[r_1 + \gamma r_2 + \gamma^2 r_3 + ....] = \mathbb{E}_{\mu_\theta}[G]\]

<p>basically the expected return if we follow policy $\mu_\theta$</p>

<blockquote>
  <p>But why use/prefer deterministic policy? Difference in practice?</p>

  <ul>
    <li>
      <p>when the variance in stochastic tends to zero, it <strong>became the deterministic case</strong></p>
    </li>
    <li>stochastic needs gradient descent in $\mathbb{E} [\nabla \pi_\theta(s,a)*G_t ]$, meaning we for a single state we need action spaces combined for a good estimate v.s. deterministic case in practice <strong>needs only to scan over a much smaller actions</strong>
      <ul>
        <li>therefore can outperform significantly <strong>when action space is high dimensional</strong></li>
      </ul>
    </li>
    <li>
      <p>stochastic encourages more exploration as in the end you are sampling from the distribution v.s. deterministic policy <strong>might not explore other possibilities</strong>. For instance</p>

\[a = \mu_\theta(s) + \text{Noise}\]

      <p>so then all your exploration is the noise term</p>
    </li>
  </ul>
</blockquote>

<h3 id="deep-deterministic-policy-gradient">Deep Deterministic Policy Gradient</h3>

<p>The overall idea is basically <strong>combining DL + DPG + Actor Critic Method</strong> by:</p>

<ul>
  <li>use actor-critic method to compute DPG gradient</li>
  <li>model your actor and critic using deep learning</li>
  <li>(solve instability issues with soft updates)</li>
</ul>

<p>Therefore, first we consider modeling a policy/actor with DL:</p>

\[a = \pi(s,u)\]

<p>with weights $u$, and a critic network $Q(s,a,w)$ with weights $w$. Then the gradient updates are simply:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202093643427.png" alt="image-20221202093643427" style="zoom: 40%;" /></p>

<p>so in total here we have two networks, where the critic update is basically no $\max$ version of DQN updates, then use that to calculate gradient for actor</p>

<blockquote>
  <p>Again, naive actor-critic method oscillates or diverges with neural network</p>
</blockquote>

<p>Then in DDPG, we use</p>

<ul>
  <li>
    <p><strong>experience replay</strong> as in DQN for <strong>both actor and critic</strong></p>
  </li>
  <li>
    <p>four networks where <strong>soft target update</strong> is used instead of freezing</p>
  </li>
</ul>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202093702826.png" alt="image-20221202093702826" style="zoom:60%;" /></p>

<h3 id="asynchronous-advantage-actor-critic-a3c">Asynchronous Advantage Actor-Critic (A3C)</h3>

<p>Again, building on top of actor-critic method but being</p>

<ul>
  <li><strong>Asynchronous</strong>: initiate multiple agents, each interact with the environment independently, and report update back to global network</li>
  <li><strong>Advantage</strong>: using advantage function instead of $Q$ to update policy</li>
</ul>

<p>Therefore overall it looks like:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202093940740.png" alt="image-20221202093940740" style="zoom: 40%;" /></p>

<p>So in a sense data correlation is removed automatically because <strong>each agent has independent interactions</strong> = no need to maintain an experience buffer.</p>

<p>The overall algorithm then looks like:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202094115629.png" alt="image-20221202094115629" style="zoom: 60%;" /></p>

<p><strong>Why A3C over other actor-critic methods?</strong></p>

<ul>
  <li>naturally has <strong>decoupled</strong> correlation, v.s. in DQN and DDPG an experience buffer is used</li>
  <li>runs many agents in parallel to collect samples for updates, hence <strong>fast</strong> in computing/updates as well</li>
  <li>each agent can use <strong>different exploration policy</strong>, which can maximize diversity and further decorrelate</li>
  <li>can run on multi-core CPU threads on a single machine to reduce communication overhead</li>
</ul>

<h3 id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</h3>

<p>Basically a new form of loss function/objective, other than the simple ones such as $J(\theta)=\mathbb{E}<em>{\mu</em>\theta}[G]$</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221202102143093.png" alt="image-20221202102143093" style="zoom: 50%;" /></p>

<h2 id="dl-for-model-functions">DL for Model Functions</h2>

<p>Now, we can also use a model to learn the world model, i.e. the transition functions</p>

\[p(r,s'|s,a)\]

<p>so that after this, we can:</p>

<ul>
  <li>use it to collect more data and improve value function estimates</li>
  <li>do planning directly, such as MCTS</li>
</ul>

<h3 id="alphago-zero">AlphaGo Zero</h3>

<blockquote>
  <p>Recall that the first version <a href="#AlphaGO">AlphaGO</a>, we used</p>

  <ul>
    <li>
      <p>one network for roll-out policy, another for SL policy, both from <strong>human data</strong></p>
    </li>
    <li><strong>self-play to improve policy network</strong>: learn a RL policy network <em>and a value network</em> as well</li>
    <li><strong>MCTS</strong> based on the tree policy (RL network) and UCB
      <ul>
        <li>expand using rollout policy and combine with value function</li>
        <li>pick action based on UCB</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>For AlphaGo Zero, the emphasis is to learn <mark>without human knowledge entirely</mark>. The core idea is that it is solely based on reinforcement learning and MCTS to improve.</p>

<ol>
  <li>
    <p>start with a <strong>single model</strong> producing both the move probability/policy and the value function</p>

\[f_\theta(s) = (p_\theta(a|s), V_\theta(s))\]
  </li>
  <li>
    <p>since the model does not care which player you are, it can self-play. But additionally, here we consider self-play using MCTS instead of just the policy $f_\theta$ because MCTS can usually result in selecting a much stronger move, hence an be treated as a <strong>policy improvement operator</strong></p>
  </li>
  <li>
    <p>Then, we have effectively gathered $(\pi,z)$, where $\pi$ is the MCTS probability and $z$ is the game winner. This game winner $z$ can be seen as a <strong>policy evaluation operator</strong>, which we can use to improve $V_\theta(s)$ estimate as well.</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221207111925758.png" alt="image-20221207111925758" style="zoom: 40%;" /></p>
  </li>
  <li>
    <p>but how exactly do we model $\pi$ probabilities in the MCTS? AlphaGo Zero models this to be:</p>

\[\pi_a \propto N(s,a)^{1/ \tau}\]

    <p>so that the more popular a move is the higher the probability. Then how does it select those moves?</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221207112607316.png" alt="image-20221207112607316" style="zoom: 40%;" /></p>

    <p>essentially it uses $f_\theta$ to help expand the tree so that each edge stores the $P(s,a),N(s,a),Q(s,a)$ where</p>

    <ol>
      <li>each simulation step <mark>selects</mark> a move that maximizes upper confidence bound $Q(s,a)+U(s,a)$, where $U(s,a)\propto P(s,a)/(1+N(s,a))$ until a leaf node is encountered. This basic form is also used in Alpha Go</li>
      <li>the leaf node is then evaluated and <mark>expanded</mark> only once using the prior probability and value $(P(s’,\cdot), V(s’) = f_\theta(s’))$. Notice no rollout here!</li>
      <li>each edge $(s,a)$ traversed is then <mark>updated</mark> to increment its visit count $N(s,a)$ and its action value to be the mean evaluation $Q(s,a)=\sum V(s’)/N(s,a)$.</li>
      <li>once done, a winner $z={+1,-1}$ is stored, and the MCTS probability $\pi_a$ is computed</li>
    </ol>
  </li>
  <li>
    <p>Finally, you have basically had a improved policy and value $(\pi,z)$, you <strong>update your network</strong> to match those parameters to achieve policy improvement and policy evaluation</p>

\[\mathcal{L}=(z-v_\theta)^2 - \pi^T \log p_\theta + c||\theta||^2\]

    <p>where the last term is simply a regularization.</p>
  </li>
  <li>
    <p><mark>Note</mark> that at the end of training/during actually play, you <mark>also perform the same MCTS procedure above</mark> and select move based on the play policy $\pi$</p>
  </li>
</ol>

<blockquote>
  <p><strong>Note that</strong> some key difference with Alpha Go include</p>

  <ul>
    <li>
      <p>learn RL network + value network during <strong>self-play from scratch</strong> without human knowledge</p>
    </li>
    <li>
      <p>modified MCTS to</p>

      <ul>
        <li>
          <p>use value network <strong>without roll-out</strong></p>
        </li>
        <li>
          <p>modified UCB $\mathrm{UCB}(s,a)=Q(s,a)+U(s,a)$, where $U(s,a)\propto P(s,a)/[1+N(s,a)]$</p>
        </li>
        <li>
          <p>final play policy $\pi$ becomes <strong>stochastic</strong> (is <em>not</em> the tree policy which is UCB)</p>

\[\pi(a|s) = \frac{N(s,a)^{1/\tau}}{\sum_b N(s,b)^{1/\tau}}\]

          <p>which is then used to instruct the model $p_\theta$ to achieve policy improvement.</p>
        </li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>And just to re-iterate the key aspects in the paper (borrowing the author’s words)</p>

<blockquote>
  <p>The AlphaGo Zero self­play algorithm can similarly be understood as an <strong>approximate policy iteration</strong> scheme in which MCTS is used for both policy improvement and policy evaluation.</p>

  <ul>
    <li>Policy improvement starts with a neural network policy, executes an MCTS based on that policy’s recommendations, and then projects the (much stronger) search policy back into the function space of the neural network.</li>
    <li>Policy evaluation is applied to the (much stronger) search policy: the outcomes of self­play games are also projected back into the function space of the neural network. These projection steps are achieved by training the neural network parameters to match the search probabilities and self­play game outcome respectively.</li>
  </ul>
</blockquote>

<h1 id="other-rl-topics">Other RL Topics</h1>

<p>There are many other topics of RL we haven’t discussed, including:</p>

<ul>
  <li>various other methods for <strong>balancing exploration and exploitation</strong></li>
  <li>
    <p><strong>Federated Reinforcement Learning</strong>, e.g. using edge devices</p>
  </li>
  <li><strong>Multi-agent Reinforcement Learning</strong>.
    <ul>
      <li>Recent breakthrough achieving human level performance in Stratego, “solving” incomplete information problem</li>
    </ul>
  </li>
</ul>

<h2 id="more-on-exploration-and-exploitation">More on Exploration and Exploitation</h2>

<p>Online decision has a fundamental trade-off between exploration and exploitation. Here we want to discuss various <strong>different schemes</strong> that you can use to balance such as trade-off</p>

<ul>
  <li>e.g. MCTS contains search using tree policy = exploitation, but expands using <strong>UCB</strong>, which helps exploration</li>
</ul>

<p>Some design principles for exploration include</p>

<ul>
  <li><strong>Naive Exploration</strong>. e.g. DDPG adding <strong>noise</strong> to a policy for exploration</li>
  <li><strong>Optimistic Initialization:</strong> initialize $\hat{Q}$ to be larger than expected reward in the beginning</li>
  <li><strong>Optimistic in the face of uncertainty</strong>: UCB as an example
    <ul>
      <li>compute uncertainty over time, and prefer actions with uncertain values for exploration</li>
    </ul>
  </li>
  <li><strong>Probability Matching</strong>: choose action according to some “model” you have about the reward distribution</li>
  <li><strong>Information state search</strong>: lookahead search</li>
</ul>

<blockquote>
  <p>For many examples below, we will <mark>assume a Multi-arm Bandit</mark> problem, meaning there will be no state for simplicity</p>
</blockquote>

<h3 id="optimistic-initialization">Optimistic Initialization</h3>

<p>One very simple and practical idea to guarantee exploration is to <strong>initialize $Q(a)$ to be high</strong>(er than expected reward). So that:</p>

<ul>
  <li>for actions that perform lower than that, we <strong>automatically explore</strong> other actions</li>
</ul>

<p>In practice, we can do this by:</p>

<ol>
  <li>
    <p>initialize high $Q$</p>
  </li>
  <li>
    <p>Update action value by incremental MC evaluation</p>

\[\hat{Q}_t(a_t) = \hat{Q}_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat{Q}_{t-1})\]
  </li>
</ol>

<p>However, this can <strong>still lock onto sub-optimal actions over time</strong>, as you will see later that both greedy/$\epsilon$-greedy + optimistic initialization will have <mark>linear</mark> total regret.</p>

<h3 id="regret">Regret</h3>

<blockquote>
  <p>Recall that regret aims to measure the <strong>opportunity loss</strong>, which can be measured by the difference between the optimal and your chosen action.</p>

\[l_t = \mathbb [ V^* - Q(a_t)]\]

  <p>then the total regret over time is</p>

\[L_t = \mathbb{E} \left[ \sum_{\tau=1}^t V^* - Q(a_\tau) \right]\]

  <p>so minimizing regret = maximize cumulative reward</p>
</blockquote>

<p>But regret can then be reformulated using the gap $\Delta_a = V^*-Q(a)$ for every action:</p>

\[\begin{align*}
L_t 
&amp;= \mathbb{E} \left[ \sum_{\tau=1}^t V^* - Q(a_\tau) \right]\\
&amp;= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)] (V^* - Q(a)) \\
&amp;= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)] \Delta_a
\end{align*}\]

<p>So for a good algorithm, we want to</p>

<ul>
  <li><strong>have large $N$ for small $\Delta_a$, and vice versa</strong> to minimize this (of course you need to get an accurate measure of $\Delta_a$ first)</li>
  <li>idea is so that you would want to choose that small $\Delta_a$ action over and over.</li>
</ul>

<p>How does some simple approach perform in this metric? For multi-arm bandit:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209083335260.png" alt="image-20221209083335260" style="zoom:50%;" /></p>

<p>so that we would like to look for a way to get <strong>sub-linear total regret</strong>.</p>

<ul>
  <li>
    <p>if you never explore, your regret will increase lienearly.</p>
  </li>
  <li>
    <p>for epsilon greedy, similar but slightly better</p>
  </li>
  <li>
    <p>sub-linear: We graduating decrease $\epsilon$ over time, having more exploitation over time.</p>

    <ul>
      <li>
        <p>decaying needs some decaying schedule. An example would be</p>

        <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209083723566.png" alt="image-20221209083723566" style="zoom:33%;" /></p>

        <p>the longer time/steps we have sampled, the less exploration. Also if $d$ is big, meaning currently making <strong>big mistakes</strong>, we want to reduce exploration</p>
      </li>
    </ul>
  </li>
</ul>

<p>But remember that $d$ or $\Delta_a$ is still hard to calculate in practice as we don’t know $V^*$</p>

<h3 id="upper-confidence-bound">Upper Confidence Bound</h3>

<blockquote>
  <p>What is the lower bound on Multi-arm bandit in terms of regret? Maybe we can get an algorithm <strong>if we know the lower bound?</strong></p>
</blockquote>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209084140380.png" alt="image-20221209084140380" style="zoom: 40%;" /></p>

<p>For instance, let us say we have three actions, and let us model $\hat{Q}(a)$ and we are <strong>unsure of our estimate</strong>:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209084355759.png" alt="image-20221209084355759" style="zoom:50%;" /></p>

<p>so that uncertainty for each action would be this breadth. The idea is <mark>the more uncertain we are about an action, the more important it is to explore it</mark></p>

<ul>
  <li>
    <p>action $a_1$ has the highest breadth/uncertainty, hence pick this first</p>
  </li>
  <li>
    <p>after picking blue, we would be less uncertain as this <strong>distribution shifts</strong>, and more likely to pick other actions</p>

    <p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209084614203.png" alt="image-20221209084614203" style="zoom:33%;" /></p>
  </li>
</ul>

<blockquote>
  <p>Therefore, we are <mark>optimistic in the face of uncertainty</mark> by hoping those highly uncertain actions to be potentially very beneficial</p>
</blockquote>

<p>An example algorithm to do this is UCB</p>

<ul>
  <li>$U_t(a)$, if you never tried an action $a$, then this will be very high/high uncertainty</li>
  <li>combine with current estimate $Q$ to combine exploration and exploitation $\hat{Q}_t(a) + \hat{U}_t(a)$</li>
</ul>

<p>so we select action maximizing UCB</p>

\[a_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a)\]

<p>But what is $\hat{U}_t$? It is motivated by the Hoeffding’s Inequality</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209085118183.png" alt="image-20221209085118183" style="zoom:40%;" /></p>

<p>which makes sense in the context of MAB because if we have large uncertainty, then this probability is small. Then we can use this to solve for a form of $U$:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209114450959.png" alt="image-20221209114450959" style="zoom:40%;" /></p>

<p>and we knw that this UCB1 achieves sublinear regret on on Multi-arm bandit</p>

<h3 id="baysesian-bandits">Baysesian Bandits</h3>

<p>So far made no assumptions about the reward distribution, and this methods basically <strong>exploit prior knowledge of rewards</strong> by specifying a prior distribution</p>

<blockquote>
  <p>Have a prior to shape the reward distribution, and use <strong>posterior</strong> to guide exploration</p>
</blockquote>

<p>So that we essentially estimate the posterior based on the (prior probability and the sample) you get:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209090030872.png" alt="image-20221209090030872" style="zoom:50%;" /></p>

<p>where $\alpha$ is number of times the arm gives you success, $\beta$ number of times if failed. Consider $\theta$ modeling reward we will get form this action. Consider a “success” being a reward $+1$ is given, failure gives $0$ reward</p>

<ul>
  <li>in the begnning, no idea so $\alpha=\beta=1$ meaning the machine can give you any reward</li>
  <li>if we lost, then the average value drifts to the left</li>
  <li>in the end, you see the average reward is about $0.7$</li>
</ul>

<p>Once done, now we have a distribution an action. We can repeat this to get a <strong>distribution for each action</strong>.</p>

<p>Then, how do we decide which action to pick? Since we are doing <strong>probability matching</strong>, we want take actions respecting those probability. For instance, say you have got three distribution with mean $0.1,0.7,1.0$</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209115006625.png" alt="image-20221209115006625" style="zoom: 25%;" /></p>

<ol>
  <li><strong>sample</strong> a reward from each curve, say $r_1=0.1,r_2=0.9,r_3=0.7$</li>
  <li>choose an action according to the <strong>biggest reward in the sample</strong>. Hence action $a_2$</li>
</ol>

<p>Other more well-fleshed idea is <strong>Thompson Sampling</strong>:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209115340909.png" alt="image-20221209115340909" style="zoom: 33%;" /></p>

<p>which also achieves the lower bound!</p>

<p>Intuitively, this way you can <strong>naturally achieve exploration</strong>.</p>

<blockquote>
  <p>This is also <mark>optimistic in the face of uncertainty</mark>, because those uncertain actions could have higher probability being $\max$ during sampling</p>
</blockquote>

<h2 id="federated-reinforcement-learning">Federated Reinforcement Learning</h2>

<blockquote>
  <p>Federated reinforcement learning is a type of reinforcement learning in which multiple agents or agents <strong>across multiple devices</strong> learn to solve a common problem while maintaining the privacy and autonomy of their local data.</p>
</blockquote>

<p>This can be useful in situations where the data that is used to train the learning algorithm is distributed across multiple devices or agents, and where it is not feasible or desirable to centralize the data. (somewhat similar to A3C idea)</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209092706109.png" alt="image-20221209092706109" style="zoom: 60%;" /></p>

<ul>
  <li><strong>Advantages</strong>: No need for centralized training data collection and centralized, data privacy, edge intelligence, learning quality</li>
  <li><strong>Disadvantages</strong>: New threats and attacks to distributed participants (e.g., <strong>backdoor attack</strong>), <strong>communications overhead</strong>, varying data quality, selection of FL participant</li>
  <li>to some extent this is similar to A3C</li>
</ul>

<p>Another approach is learning on the edge:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209093137300.png" alt="image-20221209093137300" style="zoom: 33%;" /></p>

<p>Or you could also have a fully distributed workflow</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209115720811.png" alt="image-20221209115720811" style="zoom:33%;" /></p>

<h2 id="multi-agent-reinforcement-learning">Multi-Agent Reinforcement Learning</h2>

<p>Categories in MARL:</p>

<ul>
  <li>agents are <strong>cooperative</strong></li>
  <li>agents are <strong>competitive</strong></li>
  <li><strong>hybrid</strong>, intra-group cooperation but inter-group competitive</li>
</ul>

<p>Architectures can look like:</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209115820150.png" alt="image-20221209115820150" style="zoom:50%;" /></p>

<p>An example success if DeepNash, where the there is <strong>incomplete information</strong> in the game as well</p>

<p><img src="/lectures/images/2022-12-20-ELEN6885_Reinforcement_Learning_part2/image-20221209093550846.png" alt="image-20221209093550846" style="zoom: 33%;" /></p>

<blockquote>
  <p>From the paper abstract:</p>

  <ul>
    <li>We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level.</li>
    <li>Stratego requires long- term strategic thinking as in chess, but it also requires dealing with <strong>imperfect information</strong> as in poker.</li>
    <li>The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, <strong>without search</strong>, that learns to master Stratego through <strong>self-play from scratch</strong>.</li>
  </ul>
</blockquote>

<h1 id="rl-part-2-review">RL Part 2 Review</h1>

<ul>
  <li>VFA: estimate $Q \gets \hat{Q}$
    <ul>
      <li>linear approach doing $\hat{Q}=x(s,a)W$</li>
      <li>define loss function using MSE</li>
      <li>gradient has the form $\mathbb{E}[2(\cdot ) \nabla x]$
        <ul>
          <li>if using Q-learning, then the omitted part will be $r+\max_a Q(s’,a’)-Q(s,a)$, etc</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Policy Gradient: score function that the gradient of the loss for improving policy is $Q\cdot\nabla \ln \pi / \mu$
    <ul>
      <li>probably a question on calculating this</li>
    </ul>
  </li>
  <li>Planning and Learning: planning and learning loop, MCTS</li>
  <li>DRL: differences between DPG and PG, the latter being stochastic policy. More efficient since the action space don’t need to integrated over</li>
  <li>This chapter: trade off between exploration and exploitation, and different methods</li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[RL packages]]></summary></entry><entry><title type="html">ASCE1359 Intro to China</title><link href="/lectures/2022@columbia/ASCE1359_Intro_to_China.html/" rel="alternate" type="text/html" title="ASCE1359 Intro to China" /><published>2022-12-19T00:00:00+00:00</published><updated>2022-12-19T00:00:00+00:00</updated><id>/lectures/2022@columbia/ASCE1359_Intro_to_China</id><content type="html" xml:base="/lectures/2022@columbia/ASCE1359_Intro_to_China.html/"><![CDATA[<p>Intro to East Asia: China</p>

<p>My timeline: https://time.graphics/line/691984</p>

<h1 id="introduction">Introduction</h1>

<p>Order of Major Dynasty you should now:</p>

<ul>
  <li>商，周，秦，汉；隋，唐，宋；元，明，清，Republic of China</li>
</ul>

<p>And that China has evidences of many pre-history humans living:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908161314858.png" alt="image-20220908161314858" style="zoom: 33%;" /></p>

<h1 id="neolithic-age-ca-8000-2000-bce">Neolithic Age (ca. 8000-2000 B.C.E)</h1>

<p>Neolithic Age, 新石器时代, is discovered mostly because of the <strong>distinct style of jar</strong> unearthed (Neolithic Period (3300 - 2050BC):</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908161504480.png" alt="image-20220908161504480" style="zoom: 25%;" /></p>

<p>Additionally, features of Neolithic age except for jars include its</p>

<ul>
  <li>
    <p><strong>Agriculture</strong>: the most important factor for what to plant is <mark>weather + water</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908161923132.png" alt="image-20220908161923132" style="zoom: 50%;" /></p>

    <p>note that already in the Neolithic age:</p>

    <ul>
      <li><strong>rice</strong> (水稻) grows in sunny + area with lots of irrigation. Hence near river</li>
      <li><strong>wheat/millet</strong> (to make noodles) grows in area that prefers a moderate level of irrigation. Hence more near the north
        <ul>
          <li>note that in the past (i.e. Neolithic age), the north of China has <em>not</em> been arid like now: its reasonably wet</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Domestication of Animals</strong> (e.g. dogs)</p>
  </li>
  <li>
    <p><strong>Fixed Human Settlements</strong> (see next section)</p>
  </li>
</ul>

<h2 id="major-neolithic-cultures">Major Neolithic Cultures</h2>

<p>Around 5000 B.C.E, we have a brief map of major cultures:</p>

<p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163052642.png" alt="image-20220908163052642" style="zoom: 33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163117632.png" alt="image-20220908163117632" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

<p>where our focus would be the 仰韶 and 龙山 culture, how they are related and different.</p>

<hr />

<p><strong>Yangshao Culture</strong> (5000-3000 BCE), who lives many on agriculture with meat supplies from hunting</p>

<ul>
  <li>
    <p><mark>colorful</mark>, painted pots</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163446439.png" alt="image-20220908163446439" style="zoom: 25%;" /></th>
          <th style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163450866.png" alt="image-20220908163450866" style="zoom: 25%;" /></th>
          <th style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163454693.png" alt="image-20220908163454693" style="zoom: 25%;" /></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center">3500-3000 B.C.E</td>
          <td style="text-align: center"> </td>
          <td style="text-align: center"> </td>
        </tr>
      </tbody>
    </table>

    <p>with many emphasis on fish/animals and interesting shapes such as spirals.</p>
  </li>
  <li>
    <p>human settlement: <mark>Banpo</mark></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Banpo Model (c.a. 4000 B.C.E)</th>
          <th style="text-align: center">Second Bury</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163705259.png" alt="image-20220908163705259" style="zoom: 25%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908163711564.png" alt="image-20220908163711564" style="zoom: 50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where we notice that:</p>

    <ul>
      <li>its various size and placement indicates a <strong>lineage/kinship</strong> relationship</li>
      <li>there are no walls for the Banpo model. This could be indicative of them being <strong>non-violent</strong> and friendly people, no protection is needed from outsiders</li>
      <li>already practices of <strong>burial</strong>: bury once to wait until decomposition, and bury again with others</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Longshan Culture</strong> (3000 - 2000 B.C.E)</p>

<ul>
  <li>
    <p><mark>black</mark> potteries, distinct from Yangshuo (hence indication of a separate culture)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908164202703.png" alt="image-20220908164202703" style="zoom: 33%;" /></p>

    <p>created from rustic material, but has very sophisticated patterns while being extremely thin (0.2mm).</p>

    <ul>
      <li>production of which surely requires high effort, hence indication of <mark>social stratification</mark>/concentration of wealth</li>
      <li>but it could also be a practice of certain religion, yet the previous would be more supported by other evidences</li>
    </ul>
  </li>
  <li>
    <p><mark>Shimao</mark> site, Longshan Settlement:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908164516923.png" alt="image-20220908164516923" style="zoom: 33%;" /></p>

    <p>note that we begin to see</p>

    <ul>
      <li>
        <p>a structure of inner city and outer/taller city</p>
      </li>
      <li>presence of walls</li>
      <li>has a lot of spear heads and other weapons</li>
    </ul>

    <p>indicating <strong>social stratification</strong> (as compared to Yangshuo) and <strong>foreshadowing the bronze age</strong></p>
  </li>
</ul>

<hr />

<p>Finally, note that it appears Longshan culture immediately proceeds Yangshuo culture, cultural interactions is always a non-linear process:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908165024512.png" alt="image-20220908165024512" style="zoom: 33%;" /></p>

<p>where there are many other cultures being present, and theories explaining those in relation to the two major culture include:</p>

<ul>
  <li><strong>nuclear area theory:</strong> all starts from Yangshuo, and other cultural development radiated outwards</li>
  <li><strong>interactive sphere theory:</strong> each culture has its own sphere, but they trade and interact, and eventually, converge.</li>
  <li>etc.</li>
</ul>

<p>Without decisive evidences, it is hard to say what IS really the origin of all.</p>

<h1 id="xia-dynasty-ca-2100---1600-bce">Xia Dynasty (ca. 2100 - 1600 B.C.E)</h1>

<blockquote>
  <p>The Xia dynasty was the first of many ancient Chinese ruling houses, thought to exist from around 2070 B.C.E. until 1600 B.C.E. Yet <strong>the actual existence of this dynasty and culture has been debated</strong>.</p>

  <ul>
    <li>in China, a lot of government funding has been provided to prove the existence of Xia dynasty.</li>
    <li>however, one should be careful as those evidences could be biased as well.</li>
  </ul>
</blockquote>

<p>One major culture that are hypothesized to be related to Xia Dynasty and anyway preceeds to the Shang Dynasty is the <mark>Erlitou Culture</mark> (c.a. 1750-1520 BCE), located in 河南洛阳, 二里头遗址</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908165557551.png" alt="image-20220908165557551" style="zoom: 33%;" /></p>

<p>Some interesting aspect of the Erlitou Culture include:</p>

<ul>
  <li>
    <p>Erlitou’s <mark>settlement</mark> layout:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Erlitou Layout</th>
          <th style="text-align: center">Central Palace Reconstruction</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908165818245.png" alt="image-20220908165818245" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908165842753.png" alt="image-20220908165842753" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where many scholars hypothesize that the central palace could be the <strong>capital of Xia</strong></p>
  </li>
  <li>
    <p>Start of <mark>Bronze</mark> vessels (proceeds Bronze Age) and some made of <strong>Jade</strong></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Turquoise-inlaid Bronze Plaques (板)</th>
          <th style="text-align: center">Turquoise Dragon and Bronze Bell</th>
          <th style="text-align: center">Clay Vessels</th>
          <th style="text-align: center">Bronze ding Vessel</th>
          <th style="text-align: center">Yue Jade</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170113582.png" alt="image-20220908170113582" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170118996.png" alt="image-20220908170118996" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170123643.png" alt="image-20220908170123643" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170129141.png" alt="image-20220908170129141" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170135071.png" alt="image-20220908170135071" /></td>
        </tr>
      </tbody>
    </table>

    <p>all of which are highly likely to be produced by the workshop near the palace, found in large quantity in Erlitou. This could indicate that:</p>

    <ul>
      <li>Erlingtou must be a central and powerful place, perhaps like that of a government</li>
      <li>however, without much written records those are still speculations</li>
    </ul>
  </li>
  <li>
    <p><strong>Lack of written records</strong></p>

    <p>the possibility of Xia dynasty comes from 史记 (司马迁), where he mentioned annals of Xia:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Shiji (91BC)</th>
          <th style="text-align: center">Symbols Excavated from Near Erlitou</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170705110.png" alt="image-20220908170705110" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220908170725577.png" alt="image-20220908170725577" style="zoom: 25%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where we see some lack of written records for Xia, only left with some symbols on potteries, hence no way of confirming whether if Xia really existed.</p>
  </li>
</ul>

<blockquote>
  <p><em>According to tradition</em>, the Xia dynasty was established by the legendary <strong>Yu the Great (禹)</strong>, after Shun (舜), the last of the Five Emperors (三皇五帝), gave the throne to him. In traditional historiography, the Xia was later succeeded by the Shang dynasty.</p>

  <ul>
    <li>三皇五帝is regarded as the demi-gods/emperors who significantly improved peoples lives by teaching them how to use fire and etc.</li>
  </ul>
</blockquote>

<h1 id="shang-dynasty-ca-17661600---1045-bce">Shang Dynasty (ca. 1766/1600 - 1045 B.C.E)</h1>

<p>Between Shang and Xia (if it existed), there is a piece of text from the Shiji 史记 that is worth to know. Basically:</p>

<ul>
  <li>Xie (偰) of Yin (殷) was the son of Jiandi (简狄), who (Jiandi) is a women of the Song nomad tribe</li>
  <li>Xie was born because Jiandi became the second consort of the Emperor Ku. This happened because “Once, when Jiandi was bathing with two other women, a <strong>dark bird (玄鸟) flew past and dropped an egg down to them</strong>. Jiandi retrieved it and swallowed it whole. Accordingly, she became pregnant”</li>
  <li>When Xie grew up, he <strong>assisted Yu</strong> (considered the founding father of <mark>Xia</mark>) in taming the great flood. Thereupon, Emperor Shun charged Xie with the following orders: “The hundred clans do not cleave to one another and the five ranks are not in accord. Assume the office of Governor of Conduct and attentively spread the five teachings, whose essential lesson is broad tolerance. Then Shun bestowed upon Xie <strong>a patrician estate in <mark>Shang</mark> and the surname Zi</strong>.</li>
</ul>

<p>Note that 殷 is often refered to as 安阳, which is the captial of the last period of Shang. Hence, 殷 can often be referred to as <mark>Shang</mark> as well.</p>

<blockquote>
  <p><mark>Note</mark>: Most of the content below show be viewed together with summaries of the book chapter  <a href="#Ch.1 Beginnigns of the Written Record">Ch.1 Beginnigns of the Written Record</a>.</p>
</blockquote>

<blockquote>
  <p>Even though Shang is universally considered as actually existed today, it was not in the past, until recent excavations at Anyang and Sanxingdui.</p>

  <ul>
    <li>basically Shang’s existence was establiashed by the large quantity of text found on <strong>oracle bones</strong>.</li>
    <li>a lot of doubts were there for Shang because it is portrayed during the Zhou that times before were “barbaric” and “uncultured”</li>
  </ul>
</blockquote>

<p><strong>Discovery of Oracle Bones:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Location of Yin</th>
      <th style="text-align: center">Oracle Bone Pit, Anyang</th>
      <th style="text-align: center">Shang Ox Scapula</th>
      <th style="text-align: center">Shang Plastron</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913215118561.png" alt="image-20220913215118561" style="zoom: 25%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913215121937.png" alt="image-20220913215121937" style="zoom: 25%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913215241025.png" alt="image-20220913215241025" style="zoom:25%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913215314497.png" alt="image-20220913215314497" style="zoom:25%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the scripts on the oracle bones are largely reflective of the <strong>elite life at Shang</strong>, since using them requires the Emperor</p>

<ul>
  <li>
    <p>the word 卜 comes from this, because its sound “Puk!” mimics that of the cracking of oracle bones. Hence 卜 has the meaning of divination now.</p>
  </li>
  <li>
    <p>an example of text from the oracle bone:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220911155732457.png" alt="image-20220911155732457" style="zoom: 67%;" /></p>

    <p>however, we are not to conclude that women are having less more than men, as there are cases where giving birth to daughters are considered auspicious according to records.</p>
  </li>
</ul>

<p><strong>Warefare and Polity</strong>: consider from the extracted text:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913220045932.png" alt="image-20220913220045932" style="zoom: 33%;" /></p>

<p>where Di refers to 帝, a Shang deity; suffix of “fang” means enemies. From here we also see the role of oracle bones to warfare. But what about polity?</p>

<ul>
  <li>Qiang people were being slaves and used for sacrifise. In many Shang’s work large labor is needed. Qiang people might be the man power.</li>
  <li>Concentration of power and wealth was greatly at the elite. And even artisan (e.g. making potteries) are much better off than peasants.</li>
</ul>

<p><strong>The role of Women: the case of Lady Hao:</strong></p>

<ul>
  <li>it is unclear what normal women’s life are like at Shang due to lack of records (all oracle bones with scripts are used by/for the Emperor)</li>
  <li>but what is known is that elite women (e.g. Lady Hao) and elite dead ancestors (e.g. Emperor’s Grandmother) had influence and power
    <ul>
      <li>e.g. Lady Hao’s tomb had great amount of lavish goods</li>
      <li>Lady Hao during her time was “<strong>hard as the bronze, soft as the jade</strong>” as she was great both at war/fighting to extend Shang’s border, but also raising the kids. She was also being well respected by the people.</li>
    </ul>
  </li>
</ul>

<p><strong>Shang’s Piece-mold Technique</strong>: for more details check out the book chapter  <a href="#Ch.1 Beginnigns of the Written Record">Ch.1 Beginnigns of the Written Record</a>.</p>

<ul>
  <li>very pottery based, hence explains why artisans could have a better life</li>
  <li>bronze casting required much material and technique. Hence it is <strong>mostly restricted to elites’ use</strong>. Most people use stone vessels as alternatives.</li>
</ul>

<p><strong>Shang’s Bronze: Ritual Purposes</strong></p>

<ul>
  <li>
    <p>the bronze used at Shang typically had intricate patterns featuring animals such as fish, snakes, dragons, etc. This is because they believed animals to be related to Shaman, and hence are an intermediary for connecting <strong>human and the spirits</strong></p>
  </li>
  <li>
    <p>among the animal motifs, one dominates: tao-tie 饕餮 (a beast with insatiable greed)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913221801597.png" alt="image-20220913221801597" style="zoom: 25%;" /></p>

    <p>until today we are not entirely sure why this is preferred/appear so often on the bronze vessels. Religious purposes? Artistic purpose?</p>
  </li>
</ul>

<p><strong>Masks of Sanxingdui</strong>: made of bronze but appears completely different from that of Shang. It seems to require less technical intricacy, but also very different in look.</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913222138172.png" alt="image-20220913222138172" style="zoom: 25%;" /></p>

<ul>
  <li>what could this be used for? We don’t know either.</li>
  <li>it is found to the south west of China (四川), where people at the center (中原) sees as uncultured.</li>
  <li>after examination, it appears to be techniques mixed from many different regions.</li>
</ul>

<p><strong>Chinese Texts</strong>: its feature of being pictographic and having a 1) radical and 2) phonetic component is discussed in the book chapter summary  <a href="#Ch.1 Beginnigns of the Written Record">Ch.1 Beginnigns of the Written Record</a>.</p>

<ul>
  <li>
    <p>but it is interesting to see the evolution from past texts to today</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220913222715903.png" alt="image-20220913222715903" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Summary</strong>:</p>

<ul>
  <li>Oracle bones: disc. by accident, tortoise plastrons and ox scapula used in ritual divination, contain archaic form of Chinese writing and corroborate later written historical accounts, tells the Shang political and religious system</li>
  <li>Ritual bronzes: primarily for ritual purposes, funerary/ancestral sacrifice, taotie</li>
  <li>Lady Hao’s tomb</li>
  <li>Sanxingdui: another Bronze civilization in China</li>
  <li>Bronze age: development of city-states that control territory; <strong>Erlitou and Yin are dense urban centers</strong> and centers of a regional economy, ruled by ‘royal houses’ (connecting Xia and Shang)</li>
</ul>

<h1 id="western-zhou-ca-1046---771-bce">Western Zhou (c.a. 1046 - 771 B.C.E)</h1>

<p>On a broader picture, this is the rough timeline of how Shang relates to Western Zhou.</p>

<ul>
  <li>Zhou Kingdom, once tributary state to the Shang, deposes the Shang in 1045 BCE</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915195207991.png" alt="image-20220915195207991" style="zoom:50%;" /></p>

<p>But before diving into evidences and stories of Zhou, we need to take a note of the difference between how Western Historians see Chinese histories and how Chinese scholars investigates it:</p>

<ul>
  <li><strong>Western</strong>: “wie es eigentlich gewesen ist (“how it really was”). They are interested in knowing what <strong>exactly</strong> has happened</li>
  <li><strong>Chinese</strong>: moralistical interpretation of the past, want to use history as a moralistic lesson to descendants (in fact many histories ARE written that way)</li>
</ul>

<p><strong>Records related to Zhou</strong>: in general there are texts that are recorded by the Zhou people/people alive near the Zhou period, and the ones written much later</p>

<ul>
  <li>Confucian Classics
    <ul>
      <li>Shangshu 尚書 – contains alleged speeches of the early W. Zhou kings</li>
      <li>Shijing 詩經 – Liturgical hymns (W. Zhou) and folk songs (Spring &amp; Autumn)</li>
      <li>Zhouyi 周易 – Divination manual</li>
    </ul>
  </li>
  <li>Later Histories
    <ul>
      <li>Shiji 史記 by Sima Qian 司馬遷 (145 – c. 86 BCE) – synthetic Chinese history of first century BCE</li>
      <li>Bamboo Annals (zhushu jinian 竹書紀年) – annalistic history of bamboo slips composed in 298 BCE and rediscovered in 279 C</li>
    </ul>
  </li>
</ul>

<p><strong>Bronze Vessel = Power</strong>: though often used for ritual purposes (still), making exquisite vessels still takes time and money</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915200737872.png" alt="image-20220915200737872" style="zoom:33%;" /></p>

<ul>
  <li>therefore, bronze vessels is still an <strong>embodiment of power</strong>, making them suitable for gifts when Kings awarded to lords</li>
  <li>mostly concentrated and controlled by the elites. This can also be inferred from the high uniformity of style across vessels = is regulated/controlled</li>
  <li>many inscribed text on the bronze vessels, which can be used to study Zhou’s culture (but <strong>whether if this is a faithful</strong> representation of the history is debatable, as those vessels are used for ritual purposes)</li>
</ul>

<p><strong>From Shang to Zhou</strong>:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915201853489.png" alt="image-20220915201853489" style="zoom:50%;" /></p>

<ul>
  <li><strong>Battle of Muye</strong> was a battle fought between the rebel Zhou state (King Wu of Zhou 周) and the defending army King Zhou (纣) of Shang, captured their capital Yin and ended the Shang dynasty.</li>
  <li>Zhou then settled their captical in Xi’an, near wastern side of the Wei river.</li>
</ul>

<p><strong>Settlements of Western Zhou</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Settlements</th>
      <th style="text-align: center">Overall “Border”</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915202742768.png" alt="image-20220915202742768" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915202924427.png" alt="image-20220915202924427" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>as you can see, most settlements are to the west/north of the Wei river.</li>
  <li><mark>Chengzhou</mark> (modern-day Luoyang) constructed soon after the Zhou conquest and served as the administrative center of the Western Zhou state on the eastern plain
    <ul>
      <li>later when Western Zhou got defeated and had to move to the east, they went to Chengzhou, captial of <mark>Eastern Zhou</mark></li>
    </ul>
  </li>
</ul>

<p><strong>Western Zhou Kings and Shiqiang pan 史墙盘</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Western Zhou Kings</th>
      <th style="text-align: center">Shiqiang Pan</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915203525367.png" alt="image-20220915203525367" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915203534257.png" alt="image-20220915203534257" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>Shiqiang Pan contains two pages of texts, seemed as the “first concious attempt in China to write history”</p>
  </li>
  <li>
    <p>on the Shiqiang pan, it contained, in order: a) extol of former kings b) extols of former ancestors c) dedication of the vessel</p>

    <ul>
      <li>but there are attemps found that recorded texts are too “good”</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915203753379.png" alt="image-20220915203753379" style="zoom:25%;" /></p>
  </li>
</ul>

<p><strong>Reducing Power in Late Westerm Zhou</strong>:</p>

<ul>
  <li>in Lai Pan, the structure became to extol ancestors first, but not the Kings</li>
</ul>

<p><strong>Invasion of the Rong</strong> (non-chinese people)</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915204207788.png" alt="image-20220915204207788" style="zoom:33%;" /></p>

<ul>
  <li>along with Zhou, Zhou’s territory including Qin state (秦, which later took over and start <mark>Qin Dynasty</mark>) relocated towards their other captial Chengzhou (today Luoyang)</li>
</ul>

<h1 id="eastern-zhou-770---256-bce">Eastern Zhou (770 - 256 B.C.E)</h1>

<p>Again, a brief outline of the periods:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920131742743.png" alt="image-20220920131742743" style="zoom:50%;" /></p>

<p>even though Zhou dynasty is the longest in history, <mark>Zhou</mark> Kings has <strong>declining power starting/becomes a figure head</strong> from 771 B.C.E.</p>

<ul>
  <li>Qin King called him self Qin Emperor, hence imperial period</li>
</ul>

<p>Geograohically, the two period looks like:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920132124104.png" alt="image-20220920132124104" style="zoom:50%;" /></p>

<ul>
  <li>Western Zhou: capital in area of modern day <strong>Xi’an</strong></li>
  <li>Eastern Zhou: capital in west sacked, moved east to <strong>Luoyang</strong></li>
</ul>

<p>And then Easter Zhou is (long hence) divided into two halves: 春秋 and 战国</p>

<h2 id="spring-and-autumn-period-722---481-bce">Spring and Autumn Period (722 - 481 B.C.E)</h2>

<p>At the Start of 春秋, the distribution of polities (resulting from declining Zhou King influence):</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920132300350.png" alt="image-20220920132300350" style="zoom:50%;" /></p>

<ul>
  <li>decline of Zhou power, rise of vassal states</li>
  <li>early china composed of a large collection of different polities</li>
</ul>

<p>Among the several vassal states, the major ones include 晋,齐,秦,楚:</p>

<p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920132542198.png" alt="image-20220920132542198" style="zoom: 40%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920133003347.png" alt="image-20220920133003347" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

<ul>
  <li>these polities are also the hegemons
    <ul>
      <li>hegemon: something (such as a political state) having dominant influence or authority over others</li>
      <li>this means that those states can interefere with sucecesion, hold meeting inter-states, etc.</li>
    </ul>
  </li>
  <li>one important hegemon is the <mark>晋文公</mark>, who was in “exile” for 19 years in Di, but came back and become the Duke for Jin, and reformed Jin’s military power to become one of the hegemons</li>
  <li>another important hegemon is the <mark>Qin</mark>, who were initially non-chinese/considered by people at the central plain as less cilivilized. They were adopted as “chinese” by donaintg troops to Zhou King
    <ul>
      <li>the Qin state <strong>absorbed cultures from two of the Four Barbarians from the west and north</strong>, which made the other warring states see their culture in low esteem</li>
    </ul>
  </li>
  <li>overall, this period is very violent, due to inceases in tension between states, insecurity, and betrayal. Yet there are some courtesy for battles and benevolence (e.g. not wipe out the entire lineage)</li>
</ul>

<p>Besides the central states 中国, the <strong>four important non-chinese neighbors</strong> are important to mention:</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920133632653.png" alt="image-20220920133632653" style="zoom: 33%;" /></p>

<p>where mostly the boundaries are made for geographical terms, meaning their is still culture diversity/similarity between them</p>

<ul>
  <li>e.g. Duke Wen of Jin born by a woman in <em>Rong</em>, went to <em>Di</em> during exile</li>
  <li>The Master said, “The Yi and Di tribes, even with their rulers, are still inferior to the Chinese states without their rulers.” ––Analects, 3.5</li>
</ul>

<p>453 BC Jin fell apart, partition of <mark>Jin</mark> between rival families into the three states of <mark>Han</mark>, <mark>Zhao</mark> and <mark>Wei</mark></p>

<ul>
  <li>this event is also the <strong>watershed between the Spring and Autumn and Warring States periods</strong>, refers to the division of the State of Jin between</li>
</ul>

<p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920134035087.png" alt="image-20220920134035087" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920134420548.png" alt="image-20220920134420548" style="zoom:50%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920134431303.png" alt="image-20220920134431303" style="zoom: 33%;" /> |
| ———————————————————— | :———————————————————-: | :———————————————————-: |</p>

<p>Under this turmultous condition gives birth of thinkers: Confucious (ca.551–479BCE), Kong zi, Master Kong</p>

<ul>
  <li>
    <p>born in <em>Lu</em>, full of internal division within. <em>Lu</em> later becomes very weak and is absorbed by Qi</p>
  </li>
  <li>Confucious is perhaps the author of <em>The Spring and Autumn Period</em> and <em>the Odes</em>.</li>
  <li>wanted to be advisor, but never succeeded, hence became a private tutor and <strong>earn a living teaching</strong> (a lot are aristocrates learning some courtesy)</li>
  <li>want students to be actively learning</li>
  <li>always think about “why did China become like this? How can we restore the order?”
    <ul>
      <li>believe that this is because the Zhou’s ritual and order is not properly practiced by people, hence this mess</li>
      <li>hence also aspires one of the first rulers in Zhou, leading the people with benevolent and virtue</li>
      <li>gives rise to the <mark>ru儒 class</mark>, being people who are specialized in practicing rituals</li>
    </ul>
  </li>
</ul>

<p><strong>Analects 论语</strong>: collection of conversations assembled after Confucious death</p>

<ul>
  <li>
    <p>“analects” means fragments—requires reader to connect fragments.  The Chinese title, Lun Yu, means “assorted conversations.”</p>
  </li>
  <li>
    <p>it is <em>not</em> an essay that follows an argument structure, but structured around some key terms (e.g. <em>li</em> 礼)</p>
  </li>
  <li>
    <p>some <strong>most important concepts</strong> involve</p>

    <ul>
      <li>
        <p><em>li</em> (ritual), some aspects of which this key concept is about includes:</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920135228733.png" alt="image-20220920135228733" style="zoom:33%;" /></p>

        <ul>
          <li>worship: how to make offertings to gods</li>
          <li>rites: how to marry; how to mourn the dead, etc.</li>
          <li>daily decorum: how to interact with one’s lord, parents, partners, etc.</li>
          <li>morally appropriate, proper, right.</li>
        </ul>
      </li>
      <li>
        <p><em>ren</em> (benevolence, humane, etc)</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920135607372.png" alt="image-20220920135607372" style="zoom: 33%;" /></p>
      </li>
      <li>
        <p><em>junzi</em>: the gentleman/noble person is not a vessel</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920135724904.png" alt="image-20220920135724904" style="zoom:33%;" /></p>

        <p>he believes a gentlemen is not a person of a specific capacity— not limited to one capability. Instead, the gentleman attends to the entirety and integrity of his person—he tends to his de (moral force; magnitude of character)</p>

        <ul>
          <li>redefined that nobility does not come with birth, but through learning</li>
        </ul>
      </li>
      <li>
        <p><em>social roles</em>:</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920135955877.png" alt="image-20220920135955877" style="zoom:33%;" /></p>

        <p>against social flux, even though his teaching could be a proponent for it.</p>

        <ul>
          <li>not the first time there is “some” contradiction within him</li>
          <li>another one mentioned in the book is his claim of 述而不作</li>
        </ul>
      </li>
      <li>
        <p><em>morality and legality</em></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920140105597.png" alt="image-20220920140105597" style="zoom: 33%;" /></p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>How did tradition</strong>, transmitted by classicists such as Confucius, <strong>view the first Zhou kings</strong>?</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220920140523978.png" alt="image-20220920140523978" style="zoom:33%;" /></p>

<ul>
  <li>Duke of Zhou consolidating the foundation Western Zhou, being a very capable leader</li>
  <li>seems him as a very <strong>upright</strong> person: succeeded power from young cousin (normally father-to-son succession) and promised to gave away when he is older. In the end he made his promise.</li>
  <li>Confucius is inspired by Duke of Zhou (paragon of <em>de</em> (virtue) and human excellence) to define his Dao/way of teaching</li>
  <li>Confucius thinks we should <strong>return</strong> to the period of order/patrician society. But rulers at this time rejected this idea as it is too dangerous to their own establishments power. Therefore, unfortunately during his period, his teaching remain a <strong>minority</strong> view</li>
</ul>

<p><strong>Servicemen (shi 士) class during the Zhou period</strong></p>

<ul>
  <li>constant warefare in Spring and Autumn experience a <mark>decline in aristorcracy</mark>, with infantry + crossbows being effective army = transformed the <em>shi</em> class
    <ul>
      <li><em>shi</em> started as the nobility/aristocrats for warriors in Western Zhou</li>
      <li>early Zhou (feudal order): mounted knights; special retainers. late Zhou (Warring States): need specialists, technicians in all kinds of fields (e.g. defense)</li>
      <li>Therefore, there are many social fluxes, including warriors to advisors, ministers, etc, and enables man of humble origin to climb = <em>**shi</em> class ** becomes a mix of people from non-noble classes specialized at different things (using education)</li>
      <li>after <mark>Qin</mark> dynasty (imperial era), especially in the <mark>Song</mark>, <em>shi</em> beame officials; scholars</li>
    </ul>
  </li>
  <li>the changing definition of <em>Shi</em> shows a <mark>decline of nobility</mark> (and is later critical for the emergence of 科举), that throughout periods it is <strong>talent and knowledge people are seeking</strong>.</li>
</ul>

<h2 id="warring-states-453221-bce">Warring States (453–221 B.C.E.)</h2>

<p><strong>Previously in 春秋</strong></p>

<ul>
  <li>If Western Zhou was “golden age” portrayed by <mark>Confucian</mark> (who wants to go back to those glory Zhou period) school, Eastern Zhou is a time of <strong>political fragmentation, incessant warfare, and moral decline</strong></li>
  <li>Confucius’ portrays himself as a restorer of tradition, but could be viewed as reformer; his legacy is continually contested, reinterpreted by followers
    <ul>
      <li>yet remember that his school of thought remain a minority view during his period</li>
    </ul>
  </li>
  <li>key Confucian ideas captured in fragments through the Analects, compiled by disciples (bamboo strips 3rd c./books 2nd c.); emphasizing ideas of ritual (<em>li</em>), benevolence (<em>ren</em>), filial piety (<em>xiao</em>), and the virtues of the gentleman <em>junzi</em>.</li>
</ul>

<p><strong>Overall situation in Warring States</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922131924653.png" alt="image-20220922131924653" style="zoom: 33%;" /></p>

<ul>
  <li>disintegration of <mark>Zhou</mark> order and inter-state competition &amp; warfare</li>
  <li>states such as Chu and <mark>Qin</mark> became very powerful (most smaller, weak states are already conquered)
    <ul>
      <li>“ally your self with distant enemies, and conquer near enemies”</li>
    </ul>
  </li>
  <li>at the end of the period, we know that Qin conquered all other states</li>
</ul>

<p><strong>Major changes in the Warring States</strong></p>

<ul>
  <li>
    <p>during 春秋, battles involve using chariots and training few elite warriors; but during 战国, concentrates on <strong>training large infantry</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922132256947.png" alt="image-20220922132256947" style="zoom:33%;" /></p>

    <p>and the importance of <strong>strategies</strong> (e.g. induce the enemy to destroy themselves). In <em>The Art of War</em>, a lot of idea is about how to “outsmart” your competitor without physically engaging, hence useful in many modern situations as well</p>
  </li>
  <li>
    <p><strong>usage of iron</strong> both for agriculture and weapons</p>
  </li>
  <li>
    <p>need money to maintain large army</p>

    <ul>
      <li>increase in the practice of <strong>taxation</strong></li>
      <li>argiculture and commercial development: the use of <strong>money</strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922132826183.png" alt="image-20220922132826183" style="zoom:33%;" /></p>

    <p>note that Qin used the round coin. Later we found different coins scatter everywhere, meaning existance of wide trading.</p>
  </li>
  <li>
    <p>dynamic market for <strong>specializations</strong></p>

    <ul>
      <li>lots of trading, city grows in size, population grows</li>
      <li>noble class no longer hold monopoly on intelligence, much change in social hierarchy (e.g. advisor, the <em>shi</em> class)</li>
    </ul>
  </li>
</ul>

<p><strong>Features of <em>Shi</em></strong> 士</p>

<ul>
  <li>these specialists were itinerant, traveling from state to state for employment.
    <ul>
      <li>most attractive were methods of state-strengthening and governance.</li>
      <li>emphasis on governance is a common feature of Chinese thought.</li>
    </ul>
  </li>
  <li><strong>famous specialists</strong> with pupils were referred to with the “<em>zi</em>” suffix after their surname, which means the “Master” of a teaching</li>
</ul>

<p><strong>Different “schools” (jia 家) of thought</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922133324153.png" alt="image-20220922133324153" style="zoom:50%;" /></p>

<ul>
  <li>not necessarily an actual institutional school—instead, think of a school as a taxonomic “family” whose members exhibit family <strong>resemblance</strong>.</li>
  <li>many ideas are shared across groups:
    <ul>
      <li><mark>filial piety</mark> not exclusive to “Classicists” or followers of Confucius.</li>
      <li>“Daoists” teach about the “Way” (<mark>dao 道</mark>)—the principles of the world and how to live in it—but all masters use this term, but in different ways.</li>
    </ul>
  </li>
  <li>most concern with how to <mark>govern</mark></li>
</ul>

<p><strong>Mozi: <em>兼爱</em></strong></p>

<ul>
  <li>most followeres are commoners/artisans</li>
  <li>very influential during the period (perhaps because they are specialists in artisan = defense), but died out durint Qin</li>
  <li>兼爱: love for all, no special regard for parents, etc. <strong>Against confucious</strong>, who believe that family relationship is the most importatnt.
    <ul>
      <li>hence rejection of offensive wars; military engineering and technology</li>
    </ul>
  </li>
  <li>additionally, ritual and music is for ruling class, and is fundementally <em>selfish</em> since you are mourning your own lineage. Hence Mozi is against it.</li>
  <li>Developed systematic rules of <mark>logic</mark> to persuade people
    <ul>
      <li>deep commitment to reason: argue in a rational fashion</li>
      <li>Why obsession of logic: let people of different origin understand and accept their ideas</li>
    </ul>
  </li>
</ul>

<p><strong>Mencius: <em>Human Nature is Good</em></strong></p>

<ul>
  <li>namd the second sage after Confucious
    <ul>
      <li>regarded himself as a follower of Confucious</li>
      <li>his book <em>Mencius</em> is later used for 科举</li>
      <li>occupied high political power in Qi</li>
    </ul>
  </li>
  <li>believes that “<strong>Human Nature is Good</strong>”
    <ul>
      <li>everyone has the potential to become a sage (developed their moral throgh long and hard study and reflection)</li>
      <li>human are born with <em>benevolence, rightenousness, ritual, wisdom</em>. But you need to cultivate them through learning.</li>
      <li><strong>believes in every man’s potential to become a sage</strong></li>
    </ul>
  </li>
  <li>against Mozi, as Mencius believes it is impossible to show same love to others, because human beings are inclined to love their own families more</li>
</ul>

<p><strong>Laozi: <em>Dao道</em></strong> (and Daodejing)</p>

<ul>
  <li>
    <p>Dao today refers to idea from <em>Daodejing</em>, and the book <em>Zhuangzi</em></p>
  </li>
  <li>
    <p>many messages in Daodejing is debatable and mysterious</p>

    <ul>
      <li>
        <p>Dao cannot be described</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922135121167.png" alt="image-20220922135121167" style="zoom: 33%;" /></p>
      </li>
      <li>
        <p>but Dao is the way of nature</p>
      </li>
    </ul>
  </li>
  <li>
    <p><em>Confucious</em> Dao: the way for <strong>human beings to behave</strong> ethically and moral. <em>Laozi</em> and <em>Zhuangzi’s</em> Dao: <strong>the way how universe/nature works.</strong></p>

    <ul>
      <li>Laozi/Zhuangzi believes that Confucious exaggerated the importance of human being. Others ranging from the useful and the useless in nature should also be considered as well</li>
      <li>“human is a small part of the nature”, and in the end achievement of certain goals cannot be done without nature</li>
    </ul>
  </li>
  <li>
    <p>therefore, emphasis on <em>wuwei</em> <mark>无为</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922135532501.png" alt="image-20220922135532501" style="zoom:33%;" /></p>

    <ul>
      <li>non-interference, and have no excessive desires</li>
      <li>believes that persuing non-intereference will have positive impacts on the society</li>
    </ul>

    <p>for example, for government you should also not interfere with people too much</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922135712574.png" alt="image-20220922135712574" style="zoom:33%;" /></p>

    <p>so the ruler should stop acting when the system is inplace.</p>
  </li>
  <li>
    <p><em>relativity of value</em></p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922135857820.png" alt="image-20220922135857820" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922140019729.png" alt="image-20220922140019729" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <ul>
      <li>advantage of the weak, strong and weak can become each other</li>
    </ul>
  </li>
</ul>

<p><strong>Zhuangzi: 自然</strong> (another Daoist)</p>

<ul>
  <li>dreamt himself of butterfly, <mark>questions knowledge and beliefs</mark> we taken as granted</li>
  <li>the narrative style of Zhuangzi is different from other philosophers. Other philosophers teach in an opaque way, but he does it with playfulness and examples</li>
  <li>Confucious portrayed as a proper learner, but Zhuangzi appears eccentric and unpredictable</li>
  <li>both Laozi and Zhuangzi see <em>Dao</em> as the nature they must accord with, but there are differences
    <ul>
      <li>Laozi discusses about governness, and protecting the weak</li>
      <li>Zhuangzi is not interested in politics (recluse), celebrated ritual freedom. Searching for conception of life</li>
    </ul>
  </li>
</ul>

<p><strong>Different thoughts of Masters</strong>: “when your parents die, how should they be mourned?”</p>

<ul>
  <li>
    <p><strong>Confucious</strong>: three year mourning period</p>
  </li>
  <li>
    <p><strong>Mencius</strong>: just do what <em>your heart believes</em> is correct.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922141619545.png" alt="image-20220922141619545" style="zoom:33%;" /></p>
  </li>
  <li>
    <p><strong>Mozi</strong>: a limited resource should be used to the right places/<mark>frugality</mark></p>
  </li>
  <li>
    <p><strong>Zhuangzi</strong>: life and death is part of the natural process, it just part of the natural changes. So it would be foolish to attach yourself to changing things.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220922141924289.png" alt="image-20220922141924289" style="zoom:33%;" /></p>

    <p>i.e. you should not try too hard to change things.</p>
  </li>
</ul>

<p><strong>Legalist: 法</strong> (very <mark>pragmatic</mark>)</p>

<ul>
  <li>
    <p>Han Feizi as an example of Legalist, a student of <em>Xunzi (Xunzi thinks human nature is evil, and coersion, strict teaching is needed</em></p>

    <ul>
      <li>has a high noble origin</li>
      <li>the only ruler took his teaching is Qin, which also become the emperor and united China</li>
      <li>but note that “the <strong>Legalist teachings</strong> 法家 in Qin was so repelled, by Xunzi, but they proposed a radically new way of organizing the state and its subject that allows its adherets to <mark>unite the realm for the first time</mark>”. So while Xunzi believes in the important of education to make the society behave, later legalist believed in coersion/law/punishmnets.</li>
    </ul>
  </li>
  <li>
    <p>believes that people are naturally anti-social (e.g. killed girls to favor boys at birth)</p>

    <ul>
      <li>evilness of human nature the root of trouble = political unrest</li>
      <li>so <strong>ruler must be always on guard</strong> even for his advisors, officials, etc</li>
    </ul>
  </li>
  <li>
    <p>disagree that Confucious notion that government should be based on moral and ritual, and <mark>emphasized laws and praticality</mark></p>

    <ul>
      <li>must be easy to follow</li>
      <li>must be consistent</li>
      <li>must be backed by substantial reward and heavy punishment</li>
    </ul>

    <p>people should be controlled by punishments and rewards</p>
  </li>
  <li>
    <p>different interpretation of <em>wuwei</em></p>

    <ul>
      <li>The ruler should <strong>conceal</strong> his motives and desires, for then the true characters of the ministers will be plain; otherwise the ministers will “polish themselves accordingly”</li>
      <li>Let the ministers do the work: the ruler take the credit; ministers take the blame</li>
      <li>in a sense defend the position of power. note that Han Feizi is noble.</li>
    </ul>

    <p>essentially for a ruler, <strong>law applied to everybody except the ruler</strong></p>
  </li>
  <li>
    <p>but note that the aim is <mark>not to punish people</mark>, but to <mark>improve/correct people via laws/controls</mark></p>
  </li>
</ul>

<h1 id="qin-dynasty-221---207-bce">Qin Dynasty (221 - 207 B.C.E.)</h1>

<p><strong>Previously on the Warring States</strong></p>

<ul>
  <li>
    <p>political disunity, lots of wars $\to$ disversity of political thought</p>
  </li>
  <li>
    <p>many thinkers (Daoist, Han Feizi, etc.) have <em>different idea as compared to Confucious</em></p>

    <ul>
      <li>Followers of Confucius: <em>Mencius</em> (Meng Zi) and <em>Xun Zi</em> systematize Confucian framework, elaborate on theories of governance and human nature (good/bad)</li>
      <li><em>Mozi</em> (the Mohists) known for doctrine of universal compassion</li>
      <li><em>Laozi</em> and <em>Zhuangzi</em>—contemplative recluse, philosophical relativist—presents counterpoint to classic Confucian ideas</li>
      <li><em>Han Feizi</em> —the rule of law, all men treated equally except the ruler
        <ul>
          <li>Diversity of philosophical approaches during Warring States is unsurpassed in Ch. hist; some schools–Legalism, Daoism–peak later; others (<em>Mohists</em>) disappear</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Repeated <strong>cycle of union and division</strong>, which is a basic paradigm in chinese history</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927132049739.png" alt="image-20220927132049739" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>The rise of the Qin state</strong></p>

<ul>
  <li>
    <p>Qin became the main power during the Warring period, but were seen as a barbaric state by other states</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927132433325.png" alt="image-20220927132433325" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>however, because this is a place easy to defend as it is on the <em>outskirt</em>, and it is easy to launch attacks</p>

    <ul>
      <li>can weaken power of nobility, later on realized by Lord Shangyang $\to$ perform <strong>talent recruitment</strong></li>
      <li>western troops interaction/contact made Qin advanced in <strong>military technology and capability</strong>
        <ul>
          <li>later on Qin developed a system of non-hereditary ranks, where one can rise in rank purely from achievement during war</li>
          <li>Qin king maintained a centralized control over all people, especially the troops</li>
        </ul>
      </li>
    </ul>

    <p>all of which is only possible because the power of nobility is weak/no hereditary hierarchy, and contributed to the rise of Qin</p>
  </li>
  <li>
    <p>the entire conquest of Qin on other states happened during 230 - 221 BC, which is <strong>very quick</strong>!</p>
  </li>
</ul>

<p><strong>Qin’s foreigner minister: Lord Shang (Shang Yang)</strong></p>

<ul>
  <li>in 361BC he become the <strong>chief minister of Qin</strong>, and wanted to make Qin a more powerful state $\to$ <strong>Legalist State</strong></li>
  <li>changes he introduced include
    <ul>
      <li>suppressed the hereditary nobility; ranks achieved by military merit; established bureaucracy</li>
      <li>country divided into county + officials, state penetration into the villages
        <ul>
          <li>i.e. local farmers no longer controlled the local nobility, but by the government</li>
        </ul>
      </li>
      <li>supervision between each other in a family unit</li>
      <li>distribution of land by offering to farmers, but <strong>land still belongs to the country</strong> = lending ownership temporarily</li>
      <li>direct taxation from people ot the state, instead of given to local nobility</li>
      <li>shrunk the family into atomic family units $\to$ can tax more people; $\to$ prevent large group which could start rebellion</li>
      <li>laws applied equally except for the emperor</li>
    </ul>
  </li>
  <li>overall employed harsh rules, which effectively made <strong>Qin a legalist state and laid a foundation of Qin’s sucess</strong>. But had a tragic death due to harsh rules as well.</li>
</ul>

<p><strong>King Zheng (247-221BC)</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927134150999.png" alt="image-20220927134150999" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>First <mark>Emperor</mark> of Unified China, King Yin Zheng. Named himself Emperor instead of King signifies his power presiding over the entire China</p>

    <ul>
      <li>
        <p>some viewed him as a passive beneficiary of smart officials and weak other states, hence sucess of reunification</p>
      </li>
      <li>
        <p>but it could also be that King Yin Zheng is a controlling figure: failure of assassination prior to the unifcation of China</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927134402347.png" alt="image-20220927134402347" style="zoom:33%;" /></p>

        <p>where in the figure, the person on the left that has a dagger is the assassin. The assassin Jin Ke in from the <strong>Yan state</strong> to <mark>fake for a peace treaty</mark> hence surrender, but he failed and King Zheng became infuriated and speeded up the conquer later on. This might reflect King Zheng of being a powerful figure/driving force of the country as:</p>

        <ul>
          <li>usually if ministers are the smart/driving forces, then the simples way is to bribe ministers/assassinate ministers</li>
          <li>but assassinating King Zheng means he is the driving force, hence he could be powerful</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>rised into power when he is age of 13</p>
  </li>
</ul>

<p><strong>Qin Dynasty</strong>: lasted for only 14 years but impact is long-lasting</p>

<ul>
  <li>
    <p>established the political and law structure until the end of imperial era in 1911</p>
  </li>
  <li>
    <p>notice the difference betweem <mark>Fengjian and Central</mark> system</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927135211385.png" alt="image-20220927135211385" style="zoom:33%;" /></p>

    <p>notice that now the Emperor almost had a direct control over the people.</p>

    <ul>
      <li>the Fengjian has the structure of dispersing sons/relatives with lands $\to$ let them control diverse pieces</li>
      <li>While this was designed to maintain Zhou authority as it expanded its rule over a larger amount of territory, many of these became major states when the dynasty weakened.</li>
      <li>Li Si opposed it, and believed the Central Bureaucracy by Shang Yang is better, as it could <strong>prevent the dispersion of power/rebels</strong></li>
    </ul>
  </li>
  <li>
    <p>Later Li Si’s plan is favored, and the following organization of the territory is used</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927135459829.png" alt="image-20220927135459829" style="zoom:33%;" /></p>

    <p>now the emperor has the peak power and <strong>everybody in this hierarchy has to listen to him</strong></p>

    <ul>
      <li>
        <p>the entire bureucracy is funded by the universal agricultural taxation system</p>
      </li>
      <li>at the same time, weaken wealthy clans/party, so no one can influence King’s power. e.g. sometimes taken extreme measures such as <em>reallocated all powerful clans/local nobility to captical</em></li>
      <li>all walls for military defense is abolished, and melt down all weapons (so no more civil war)</li>
    </ul>
  </li>
</ul>

<p><strong>Economic Standardization in Qin</strong></p>

<ul>
  <li>
    <p>erased diversity by <strong>unifying weights and measures</strong></p>

    <ul>
      <li>produce in markets can be compared and priced equivalently</li>
      <li>weights measured by weigh</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927140044752.png" alt="image-20220927140044752" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>another important standardization is <strong>currency</strong>: faciliates trading</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Other Curreny to Qin</th>
          <th style="text-align: center">Qin’s Currency</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927140136547.png" alt="image-20220927140136547" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927140220842.png" alt="image-20220927140220842" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>unification of <strong>script</strong>. For instance the word for “horse”</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927140308102.png" alt="image-20220927140308102" style="zoom:33%;" /></p>

    <p>which is advantageous for <strong>communications</strong> between each other/different states</p>

    <ul>
      <li>done by Li Si, by rationalizing the Zhou script</li>
      <li>symbolizes the <mark>cultural unity</mark> that Qin intended</li>
    </ul>
  </li>
  <li>
    <p>more than standardization: <mark>knowledge control</mark></p>

    <ul>
      <li>burying of scholars alive (uncertain): Confusious preferred Zhou’s structure, but Li Si is the chancelor and is totally against it
        <ul>
          <li>Confucious hated Qin, hence people afterwards, e.g. in Han dynasty, who are followers of Confucious could be biased</li>
        </ul>
      </li>
      <li>buring of books (confirmed, 213BC): burned all Confucian classics, and whoever buys it will be executed (view this as an enemy of Qin’s progress).</li>
    </ul>

    <p>happens because Qin wants people to forget about Zhou/past, and corroberates current policies. But it does not mean to abandon all values from the past as well, e.g. lack of filiaty is still punished</p>
  </li>
  <li>
    <p><strong>legal standardization</strong>: standarized rules for all (except ruler)</p>

    <ul>
      <li>
        <p>later Han dynasty described Qin rules being very harsh</p>
      </li>
      <li>
        <p>but the truth is complicated: low ranking offical Xi’s tomb in 睡虎地 found</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927141158609.png" alt="image-20220927141158609" style="zoom:33%;" /></p>

        <p>Qin books on laws, which is <strong>not that harsh but rather detailed and meticulous</strong></p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Logics on Punishment</th>
              <th style="text-align: center">Women v.s. Men</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927141404034.png" alt="image-20220927141404034" style="zoom:33%;" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927141535014.png" alt="image-20220927141535014" style="zoom:33%;" /></td>
            </tr>
          </tbody>
        </table>

        <p>additionally, we also see some <strong>much more power of man than wives</strong></p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Qin Transportation System:</strong> e.g. road network (4000+ miles)</p>

<ul>
  <li>
    <p>strength economic ties between regions</p>
  </li>
  <li>
    <p>wall built during Ming Dynasty, but some foundation was laid out during Qin</p>

    <ul>
      <li>as expression for shielding off enemies, like in Longshan period</li>
      <li>wanted to <strong>wall out non-chinese tribes to north (Xiongnu)</strong> and wests</li>
      <li>Qin recognizes its limits of power, by delineating is boundary <em>clearly</em> for the first time</li>
    </ul>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Existing Walls during Qin</th>
          <th style="text-align: center">Great Wall we see today</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927141911239.png" alt="image-20220927141911239" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927141955179.png" alt="image-20220927141955179" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where essentially Qin is linking the existing walls together. Note that during this period, Qin assigned a <strong>military official</strong> because</p>

    <ul>
      <li>those walls are on the outskirts, hence skirmishes would happen a lot</li>
      <li>need to managed a large group of people, which is a skill more common within military</li>
    </ul>
  </li>
</ul>

<p><strong>Qin’s Death: Terra-Cotta Army</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220927142246854.png" alt="image-20220927142246854" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>used for Qin emperor to fight during the underworld (belief in afterlife, that military he defeated will prusue him in afterlife)</p>
  </li>
  <li>
    <p>soldiers were originally colorful</p>
  </li>
</ul>

<h1 id="han-dynasty-202bc---220ad">Han Dynasty (202BC - 220AD)</h1>

<p><strong>Previous on Qin Dynasty</strong></p>

<ul>
  <li>230-221 BCE: <mark>Qin</mark> conquest ends the <mark>Warring States period</mark>, results in unification</li>
  <li>221-210 BCE: King Ying Zheng of <mark>Qin</mark> proclaims himself ‘emperor’ Huangdi, reigns as first emperor (Qin Shihuang)
    <ul>
      <li><strong>Liu Bang</strong> (Emperor Gaozu): the founder of the Han</li>
      <li>Qin state began as a politically and culturally marginal power, which is strengthened by <strong>Shang Yang’s reform</strong></li>
      <li>then Han is corroberates by <strong>Emperor Wu (Wudi)</strong></li>
      <li>Shang Yang’s reforms majorly included bureaucratic government, merit-based appointment, and administrative and tax reforms—direct relation to ruled</li>
      <li><strong>Li Si,</strong> after the conquest, builds upon Shang Yang’s model, extends bureaucratic framework to standardization of weights and measures, currency, script, thought control</li>
    </ul>
  </li>
  <li><strong>Laws of Qin</strong> exresses legalist principles, but seen as were harsh by later people
    <ul>
      <li>portrayed as repressive and tyrannical, but recent scholars are rejecting prejudices and acknowledges its centralizing measures and its legal codes far from arbitrary</li>
    </ul>
  </li>
</ul>

<p><strong>The Collapse of the Qin Empire</strong></p>

<ul>
  <li>
    <p>In brief: 221BC Qin unification. 210BC Shi Huangdi died; his unpopular son, Huhai, succeeded the throne. 209BC Rebellions broke out. 207BC Qin ended by Liu Bang and Xiang Yu, both from <mark>Chu state</mark> (Liu Bang won at the very end).</p>
  </li>
  <li>
    <p>Qin suffered from strong resistance on the conquered people (tax, lots of construction work), because the <strong>restructure/unification of China within a short period</strong> is bound to invoke dissent</p>

    <ul>
      <li>Especially <mark>Chu</mark> state had grudges, since Qin once broke alliance with Chu and masaacre a lot of people in Chu</li>
    </ul>
  </li>
  <li>
    <p>Rebellions broke out. Some key rebellion leaders (both from Chu) include:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Xiang Yu’s Military Capabilty</th>
          <th style="text-align: center">Liu Bang’s Social Capability</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929132146778.png" alt="image-20220929132146778" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929132155103.png" alt="image-20220929132155103" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>and the interesting is they have exactly different personalities and abilities</p>
  </li>
  <li>
    <p>Liu Bang won at the very end, and started <mark>Han dynasty</mark>, named himself <strong>Han Gaozu</strong>汉高祖</p>
  </li>
  <li>
    <p>Han is also divided into <strong>two periods</strong>, almost identical to the shift for Western Zhou and Easter Zhou</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929132518552.png" alt="image-20220929132518552" style="zoom:33%;" /></p>

    <p>essentially almost identical to Zhou, and we <strong>start with Western Han</strong>, but later moved captial to Luoyang, hence <strong>Eastern Han</strong></p>
  </li>
</ul>

<p><strong>Liu Bang’s New Empire</strong></p>

<ul>
  <li>
    <p>Liu Bang’s first decision was whether to keep the bureaucray of Qin or start a new one (most people associated centralization of power with tyrannay at this point, due to Qin dynasty)</p>
  </li>
  <li>
    <p>in the end, he retained most of the structure, central bureucracy, still uses commanderies, counties, appointed officials by talent, but on <strong>had a benign implementation</strong> (rules are more flexible)</p>
  </li>
  <li>
    <p>decided to share power with his relatives, starting vassal states.</p>

    <ul>
      <li>hence only western portion, about 1/3 of the entire territory is under direct control</li>
      <li>easter ones are controlled by his relatives (so when he died, family still has control)</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929132736301.png" alt="image-20220929132736301" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>also didn’t like Confucious, so prohibitted Confucious didactic books</p>
  </li>
  <li>
    <p>so in general, most policties are aligned with Lord Shang, with few changes:</p>

    <ul>
      <li>relaxed the laws of Qin a bit, by relaxing the implemenation. e.g. you can plead to the judge</li>
      <li>Liu Bang hated Confucious but judges of crime valued filial piety. This is no direct contraversy because the idea of filial piety is in fact common on lots of idealities, including legalism (e.g. violating filial piety will be punished)</li>
      <li>but thinks Confucious scholar is useful</li>
    </ul>
  </li>
</ul>

<p><strong>Emperor Wudi</strong> (Liu Che刘徹)</p>

<ul>
  <li><mark>strongest of all Han emperoros</mark>: lots of achievements during his reign, especially his expansion of China</li>
  <li>Wudi is a <strong>posthumouns title</strong> due to his achievements in military, and china’s territoy expansion</li>
  <li>had much control over vassal states</li>
  <li>made Confucianism state ideoligy</li>
</ul>

<p><strong>Emperor Wudi: reshaping economy and politics</strong></p>

<ul>
  <li>centralizing the government
    <ul>
      <li>instead of forcibly remove the large estate landowners, decreed lord’s land shared with heirs (so that in the long run Emperor wins)</li>
      <li>hence smaller control, and Wudi have more power</li>
      <li>periodically, rich and influential family relocated to the captial to be under supervision (similar to Qin)</li>
    </ul>
  </li>
  <li>significant revenues come from <strong>state monopolies</strong>
    <ul>
      <li>intent was to not overburden farmers</li>
      <li>hence increased <strong>taxes on private businesses</strong></li>
      <li>monopolies on salt and iron, and later coinages as well
        <ul>
          <li>later debated if salt and iron monopoly is a good idea as they are critical for the people, but very profitable for the Emperor</li>
          <li>in the long run, such monopoly disrupted the private sector of economy, and a political controversy: debate v.s. <strong>Confucianism scholars</strong> at that time on what is proper government</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Fight Against Xiongnu</strong></p>

<ul>
  <li>
    <p>Wudi puhsed out <strong>Xiongnu</strong> (problem since Shang, exacerbated by Zhou, e.g. making peace through gifts and “peace marriage”)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929134528429.png" alt="image-20220929134528429" style="zoom:33%;" /></p>

    <p>but at the same time, Xiongnu tribes fromed the first confederation</p>

    <ul>
      <li>Xiongnu might have been ancestors of European Turkish</li>
    </ul>
  </li>
  <li>
    <p>also had campaigns and subdugate the various kingdoms, defeated Ferghana, got a lot of horses</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929134933688.png" alt="image-20220929134933688" style="zoom:33%;" /></p>

    <p>and this campaigm <strong>secured trade routes later to central asia</strong></p>
  </li>
  <li>
    <p>other campaigns aimed to make <strong>more tribute states</strong> hence allies to fight against Xiongnu</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929135055258.png" alt="image-20220929135055258" style="zoom:33%;" /></p>

    <p>some famours campaign leaders:</p>

    <ul>
      <li><strong>Huo Qubing:</strong> defeated 5 Xiongnu tribes in 6 days, killed over 70000 Xiongnu. (but we didn’t find Xiongnu’s records to confirm this)</li>
      <li><strong>Zhang Qian:</strong> his route established later <mark>silk road</mark>, and later where <mark>buddism</mark> came in</li>
    </ul>
  </li>
  <li>
    <p>Consequences of Wudi’s expansions</p>
    <ul>
      <li>Positive:
        <ul>
          <li>military security</li>
          <li>established contact with the larger world - later on the silk road</li>
          <li>ambassators of many tribute states $\to$ send princesses to China and returns with periodic gift to vassal states</li>
        </ul>
      </li>
      <li>Problem:
        <ul>
          <li>nearly 10% of state revenue spent on gifts</li>
          <li>Stretched China’s resources; economic well-being weakened</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Han Confucianism as State Orthodoxy</strong></p>

<ul>
  <li>
    <p><strong>Dong Zhongshu</strong> developed Confucious that matched the mood of the time</p>

    <ul>
      <li><strong>synthesizes</strong> different ideas to Confucian metaphysics, e.g. adapted <em>yin-yang</em> and the <em>five forces</em> (fire, water, earth, metal, and wood) into Confucian metaphysics. Note that Confucious origianlly did not address supernatural</li>
      <li>harmonize the contrasting idea of Mencius and Xunzi, so human’s nature is good but man needs to be guided by the ruler (personification of heaven)</li>
      <li>remade emperor-cnetered, hence Wudi liked it</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929140431707.png" alt="image-20220929140431707" style="zoom:33%;" /></p>

    <ul>
      <li>essentially, Confucianism became <mark>more eclectic and emperor-centered</mark></li>
    </ul>
  </li>
  <li>
    <p>he then made the proposal to Wudi, who liked his idea and adopted it</p>

    <ul>
      <li>even though Wudi’s established rule is in a way very similar to Qin’s legalist principle on military and laws,</li>
      <li><strong>ruled by legalist, but decorated/surfaced by Confucianism idea</strong></li>
      <li>so practically, every state claimed that they follow confucianism even though rules by legalist ideality (due to Qin)</li>
    </ul>
  </li>
  <li>
    <p>another important change include that the government officials needed Confucianism classics as required knowledge</p>

    <ul>
      <li>invenstion of paper in Easter Han helped expanded Confucianism</li>
      <li>but the overall Confucianism has diverged some significant ways</li>
    </ul>
  </li>
</ul>

<p><strong>Emperor Lv</strong>吕后 and <strong>Ban 班Family</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929140951519.png" alt="image-20220929140951519" style="zoom:33%;" /></p>

<ul>
  <li><mark>first woman ruler</mark> (dowager) in imperial china, wife of Liu Bang</li>
  <li>power of empresses family threatened the power of emperor family, hence seen as dangerous
    <ul>
      <li>so historians such as Sima Qian cover her achievments up</li>
    </ul>
  </li>
  <li>Another first woman historican: <mark>Ban Zhao</mark>
    <ul>
      <li>completed the history of Western Han</li>
      <li>wrote book on <em>Lessons for Women</em>, which is very popular
        <ul>
          <li>emphasized women obligations to husband, cultivation of virtues appropriate to women.</li>
          <li>women and men have different status, hence can be seen as Confucious idea to stick to their role</li>
          <li>but also advocated for equal education for women</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Sima Qian, the “historian”</strong></p>

<ul>
  <li>
    <p>our vision of ancient china is being shaped a lot from his view of it</p>
  </li>
  <li>
    <p>at that time, historian were more known as Archivist, Astrologer, and/or Scribe</p>

    <ul>
      <li>duties may have included recording events, or keeping “minutes.”</li>
      <li>could access palace archives, such as chronologies, confidential documents.</li>
    </ul>
  </li>
  <li>
    <p><em>Shiji</em> started by his father, finished <mark>privately</mark> by him (hence less biased v.s. Ban’s version, which is state sponcered)</p>
  </li>
  <li>
    <p><em>Shiji</em> narrates history from beginning to time to Sima Qian’s time</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220929141604353.png" alt="image-20220929141604353" style="zoom:33%;" /></p>

    <p>where all the history was unearthed by him, checking archives, going to places physically, etc.</p>
  </li>
  <li>
    <p>however, there could still be biases:</p>

    <ul>
      <li>during ruling of Han Wudi, he had a castration due to political mistake, hence could have grudge to Han</li>
      <li>dislikes Emperess Lv</li>
      <li>student of Dong ZHongshu, had Confucious influences</li>
    </ul>

    <p>but today seen as he tried his best to be impartial</p>
  </li>
</ul>

<p><strong>Wang Mang Interloper</strong>:</p>

<ul>
  <li>recall that Liu Bang, Empress Lv, and Empreor Wu are all classfied as early Han</li>
  <li>Wang Mang started a rebellion and separated out of the royal clan to start his <mark>Xin dynasty</mark> (which lasted very shortly) besides <mark>Han</mark>.
    <ul>
      <li>however, due to the flooding of yellow river at the time and other reasons, Wang Mang’s plan is shortlived</li>
    </ul>
  </li>
  <li>due to rebellion, Han people relocated to Luoyang, <strong>restored the power</strong>, had the <mark>Eastern Han</mark>
    <ul>
      <li>Eastern Han formally began on 5 August AD 25, when Liu Xiu became Emperor Guangwu of Han, with <strong>capital moved eastward to Luoyang</strong></li>
      <li>still the <strong>Liu</strong> clan being the royalty, but the eunuchs has rose up in power</li>
    </ul>
  </li>
</ul>

<p><strong>End of Han</strong></p>

<ul>
  <li>The end of the Han dynasty refers to the period of Chinese history from 189 to 220 CE, which roughly coincides with the tumultuous reign of the Han dynasty’s last ruler, Emperor Xian.</li>
  <li>During this period, the country was thrown into turmoil by the Yellow Turban Rebellion (184–205). Meanwhile, the Han Empire’s institutions were destroyed by the <mark>warlord Dong Zhuo</mark> and <mark>fractured into regional regimes ruled by various warlords</mark>, some of whom were <strong>nobles and officials of the Han imperial court</strong>.</li>
  <li>One of those warlords, Cao Cao, was gradually reunifying the empire, ostensibly under Emperor Xian’s rule</li>
</ul>

<h1 id="six-dynasties-220---589">Six Dynasties (220 - 589)</h1>

<p><strong>Previously on Han Dynasty</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004131407632.png" alt="image-20221004131407632" style="zoom: 50%;" /></p>

<ul>
  <li>
    <p><strong>Liu Bang</strong>, or Han Gaozu, founded the Han dynasty</p>
  </li>
  <li><mark>Han</mark> Dynasty is split into two halves, distinguished by their capitals:
    <ul>
      <li>206BC - 9 CE: Western/Former Han (Chang’an). Then problem incurred by <mark>Wang Mang</mark> and forced the move of the royal family</li>
      <li>25-220 CE: Eastern Han/Later Han (Luoyang)</li>
    </ul>
  </li>
  <li>under the reign of <strong>Emperor Wu</strong>: we had great territorial expansion, but weakened the economy of empire. Several milestones include:
    <ul>
      <li><strong>Han-Xiongnu War</strong>: Han expansion creates a multilingual, multiethnic empire, enduring political and cultural ties to inner Asia; foundation of silk road and exchange of goods/ideas; but weaken the local economy due to warefare
        <ul>
          <li>note that silk road in reality are a series of <em>short trades</em>, and that it is important to know that many tradings existed <em>before silk road</em> as well. It is just that Wudi made <em>silk road even more trade intensive</em>.</li>
        </ul>
      </li>
      <li><strong>Dong Zhongshu</strong> works to install Confucianism as official state ideology, but draws upon Daoist cosmology; this syncretic Confucianism legitimizes Han order</li>
      <li><strong>Sima Qian</strong>, court astrologer to E. Wu, writes monumental Records of the Grand Historian in the Confucian mold, considered founder of Chinese historiography</li>
      <li><strong>Salt &amp; Iron</strong> monopoly debates (held 81BC) on Emperor Wu’s economic policies</li>
    </ul>
  </li>
  <li>
    <p><mark>Han</mark> governance marks continuation of Qin law and institutions, <mark>legalist foundation with new syncretic Confucianist state ideology laid over it</mark>. =&gt; Called the “Han Synthesis,” but the Han is essentially a Legalist state.</p>
  </li>
  <li>
    <p>also recall that in the song, this missing period is essentially the topic of this section: the <mark>Six Dynasties</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004131733439.png" alt="image-20221004131733439" style="zoom: 33%;" /></p>
  </li>
</ul>

<p><strong>Timeline After Han</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004131810873.png" alt="image-20221004131810873" style="zoom:50%;" /></p>

<ul>
  <li>first split into Wu吴国, Wei魏国, Shu蜀国, the three kingdom period</li>
  <li>then north have been controlled by alien rulers, e.g. Northern Wei. The southern are controlled by aristocrats migrated from north (e.g. to <mark>seek refuge</mark>)
    <ul>
      <li>recall that south had high humidity = lots of malaria</li>
      <li><strong>oldest form of spoken Chinese in the South</strong>, such as Fujian, because south had been “stale” while north was changing, e.g. interaction with steppe people.</li>
    </ul>
  </li>
  <li>the name “Six dynasties” come from the six dynasty that had capitals in <mark>Nanjing</mark> (Eastern Wu, Eastern Jin, Song, Qi, Liang, Chen)
    <ul>
      <li>it also overlapped the <mark>Sixteen Kingdoms</mark> period = warring in Northern China after fall of Western Jin</li>
      <li>the period of Northern Wei + Southern Song to before Sui is also called the <mark>Northern and Southern period</mark> 南北朝</li>
    </ul>
  </li>
  <li>Sui dynasty reunified china</li>
</ul>

<p><strong>The Three Kingdoms</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004132335086.png" alt="image-20221004132335086" style="zoom:33%;" /></p>

<ul>
  <li>the three rulers slowly rose to power at the end of Han, by being warlords</li>
  <li>by 205, Cao Cao had made themselves dictator of North China. Since there are so many Wei, we added the surname of the leader to give Cao Wei</li>
  <li>later on <strong>Cao Cao</strong> also defated <strong>Xiongnu</strong>, but never able to reunite the other empires, which ended with the <strong>Battle of the Red Cliff 赤壁之战</strong>
    <ul>
      <li>defeated by alliance between Shu and Sun, and the tiredness of his troops due to long distance marching</li>
      <li>he didn’t die there, but just failed to reunite</li>
    </ul>
  </li>
  <li>but <strong>eventually his Wei prevailed</strong> as it has twice the population, and won over the other state</li>
  <li>near the last of Wei’s heirs, Cao Shuang was executed and royal power weakened = rise and eclipsed by the <strong>Sima family</strong>.
    <ul>
      <li>They eventually took over change the dynasty to <mark>Jin 晋</mark></li>
      <li>first time a small reunion of China, but fell apart quickly by the <strong>War of Eight Princes</strong> (i.e. family member seeking power)</li>
      <li>subsequently, in 304, civil war within and northern invasion of the <mark>non-Chinese tribes</mark> collapsed</li>
      <li>Therefore, the north became the <mark>Sixteen Kingdoms period</mark> and the south is the <mark>Eastern Jin dynasty</mark></li>
    </ul>
  </li>
</ul>

<p><strong>Sixteen Kingdoms and Eastern Jin</strong></p>

<ul>
  <li>
    <p>Non-chinese joined the conflicts and made north a battle ground.</p>
  </li>
  <li>
    <p>life in the north was dangerous hence many people, especially Jin, packed up and fled south = <mark>Eastern Jin</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004133417147.png" alt="image-20221004133417147" style="zoom:50%;" /></p>

    <p>but since there are aboriginal people there:</p>

    <ul>
      <li>the local populartion either pushed away or absorbed</li>
      <li>essentially Eastern Jin is still under the Sima Family</li>
    </ul>
  </li>
  <li>
    <p>one reason why Jin so shorted lived is because its choice of Government officials. The rise of aristocracy from Han through Tang</p>

    <ul>
      <li><mark>Warring States</mark>: on the basis of merit (<em>shi</em> class)</li>
      <li><mark>Qin</mark>: merit, as a firm principle</li>
      <li><mark>Han</mark>: by recommendation (rule of avoidance, so officials there are non-local) + exams
        <ul>
          <li>in reality, since officals are non-local, they basically asked people of importance <strong>at that local area</strong> for recommendation</li>
          <li>but later this became a problem, that it still became local recommendation by locals themselves</li>
        </ul>
      </li>
      <li><mark>Wei</mark> (under Cao Cao): and the whole era of disunity: the <strong>Nine-Grade</strong> system
        <ul>
          <li>every family will be assigned a rank = position in government, hence hereditary in nature</li>
          <li>this rank assignment depends in virtue, reputation, talent</li>
          <li><strong>long survival of elite status</strong>, and many of those clans persisted to the Tang dynasty
            <ul>
              <li>to preserve this, aristocrat should only marry aristocrat</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><mark>Sui</mark>: exams
        <ul>
          <li>realize that the exam is the fairest way</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Elite Aesthetics</strong>:</p>

<ul>
  <li>
    <p>in South China, the educated and the well-off <strong>competed in extravagence</strong>, instead of the virtue Confucious wanted</p>
  </li>
  <li>
    <p>these people are interested in mythical things, divination and supernatural (“Seven Sages of the Bamboo Grove” below)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004134535212.png" alt="image-20221004134535212" style="zoom:33%;" /></p>

    <p>for example, they loved drinking, and some also took drug.  Hence those elite people always try to be naked</p>

    <ul>
      <li>challenged public moral at the time, of what is virtuely right</li>
      <li>e.g. one was crying when an unmarried man died, but was eating and drinkg wine at his mother funeral = violated Confucious values</li>
    </ul>
  </li>
  <li>
    <p>but also a burst of self-expression in art and poetry</p>
  </li>
</ul>

<p><strong>Northwen Wei Dynasty</strong></p>

<ul>
  <li>
    <p><mark>Tuoba</mark>, which included tribes who spoke turkish, mongolish, etc. They invaded and controlled he nothern China after the Western Jin</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004135051914.png" alt="image-20221004135051914" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>however, Tuoba interested in the process of <mark>Sinicization</mark> (to Sinify: to make Chinese. Latin “Sinai”=Qin=Chinese)</p>

    <ul>
      <li>how can a minority population of Tuoba ($\approx$ 1 million) contorl the vast population of Chinese $\approx$ 20 million?</li>
      <li>one approach is Sinicization, which is wierd but largely because <strong>Emperor Xiaowen himself is admiring Chinese since childhood</strong></li>
      <li>proposed a new system of Equi-field, which is reminiscent of Han to tax individual cultivators
        <ul>
          <li>individual family assigned land depending on how many people they have</li>
          <li>this is to ensure that no-land is neglected, and that <strong>no people wander off</strong> (e.g. don’t want merchant and nomads)</li>
          <li>hence is useful for building a fiscal base</li>
        </ul>
      </li>
      <li><strong>Emperor Xiaowen:</strong> new captial in Luoyang and wanted to <mark>transform Xianbei鲜卑 into Chinese</mark>
        <ul>
          <li>used chinese surnames</li>
          <li>ordered Chinese clothings in court</li>
          <li>preferred Sinicized Chinese = elites of mixed ethnic backgrounds increasingly became the norm
            <ul>
              <li>Luoyang became popular, many Xianbei nobilities became versed in Chinese</li>
              <li>The cultural and ethnic integration <mark>laid the foundation for later reunification of the North and South by the Sui</mark></li>
            </ul>
          </li>
          <li>however, this eventually lead to the problem that military who are mostly Xianbei are being <mark>treated unfairly</mark> compared to mixed court officials. This can be said to tbe the root cause of the throw off for Emperor Xiaowen</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>the <strong>Ballad of Mulan</strong>, when the Northern Wei (where Mulan is born) was in war with ruan-ruan</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004135710984.png" alt="image-20221004135710984" style="zoom:33%;" /></p>

    <ul>
      <li>in the Ode the concept of <strong>Khan</strong> is equivalented to the concept of <strong>Son of Heaven</strong></li>
      <li>indicates the sinicization of north</li>
    </ul>
  </li>
  <li>
    <p>Resistance of Sinicization,</p>

    <ul>
      <li>this acceptance of Chinese culture would lead of loss of identities, and Xianbei in military did not enjoy status that sincized Xianbei in the court had</li>
      <li>since those unequal treatment made militaries unhappy, and lead to rebellion in 524</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004135907298.png" alt="image-20221004135907298" style="zoom: 67%;" /></p>
  </li>
  <li>
    <p>but later they all suffered from sinified Chinese, that a mix of culture differences induced alot of conflict</p>
  </li>
</ul>

<p><strong>Buddhism</strong>:</p>

<ul>
  <li>
    <p>provided a different way of conceving life, death, and cosmos</p>
  </li>
  <li>
    <p>Buddhism originated in India, a lot went to China from the trade routes</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004140127938.png" alt="image-20221004140127938" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>instead of a single creed, chinese Buddhism foundd a lot of ideas and practices</p>

    <ul>
      <li>met both <mark>Theravada</mark> (early buddihism, lesser vehicle) and <mark>Mahayana</mark> (later buddism, greater vehicle ‘that can carry more people to salvation’)</li>
      <li>problem of translation then was critical</li>
    </ul>
  </li>
  <li>
    <p>since Buddhism is in sanskrit, early translators used Daosim concepts to explain Buddhism</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004140458180.png" alt="image-20221004140458180" style="zoom: 33%;" /></p>

    <ul>
      <li>in the beginning, had lots of problems such as Nirvana $\to$ Wuwei</li>
      <li>then with <mark>Kumarajiva</mark> and his translation project coining new terms in Chinese (e.g. Niepan, to preserve the foreign phoenic), it became much more accurate</li>
    </ul>
  </li>
  <li>
    <p>how did <strong>Buddhism change Chinese culture at large</strong>?</p>

    <ul>
      <li>the idea of rebith, and people cycling through life is new to the Chinese</li>
      <li>added the idea of <strong>death is also suffering</strong>, hence Nirvana is important</li>
      <li>reshaped the <mark>relationship between the living and the ancestors</mark>
        <ul>
          <li>ancestors become “weak” beings needing to be saved (v.s. before, ancestors are made offerings and then gave help in return)</li>
          <li>how do you save them? Death can be lifted by merit, and <mark>merit can be transferred</mark>, e.g. by <mark>Buddhist clergy</mark></li>
          <li>remade the relationship between living and dead. Idea of ancestor are powerful has diminished</li>
        </ul>
      </li>
      <li>From the above, induced the rise of the <strong>importance of Buddhist clergy</strong></li>
    </ul>
  </li>
  <li>
    <p>how did <strong>China change Buddhism</strong>?</p>

    <ul>
      <li>
        <p>Mahayna buddhism became the famous one, China magnifies Mahayana. e.g. extending the influence to Japan as well</p>

        <ul>
          <li>e.g. Chan, or Zen in Japanese, all purely Chinese inventions</li>
        </ul>
      </li>
      <li>
        <p>developed specific Buddhist schools, all Mayhayana sects</p>

        <ul>
          <li><strong>Pure Land</strong>: preach by faith, hence very easy to pratice. Only need to recite the name of Amitabha. Hence welcomed by commoners</li>
          <li><strong>Zen buddism</strong>: needs meditation, more attractive to literate elite</li>
        </ul>
      </li>
      <li>
        <p>an example of locaization of Buddhism deity, Guan Yin of mercy and compassion.</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Original</th>
              <th style="text-align: center">In China</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004170004118.png" alt="image-20221004170004118" style="zoom:33%;" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004141817604.png" alt="image-20221004141817604" style="zoom:25%;" /></td>
            </tr>
          </tbody>
        </table>

        <ul>
          <li>initially Guan Yin is a male in India, but in China, became female</li>
          <li>promoted the idea of <strong>equal gender</strong>: “presentation of gender is provisional, situational, audience-oriented”</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Rise of Daoism</strong>:</p>

<ul>
  <li>Though Han dynasty with popular with Confucious, there are still believers of Daoism and the notion of <strong>immortality</strong> (e.g. Lady Dai)</li>
  <li>during the Religious Daoism, those sporadic believers (e.g. farmers) suffered from taxations and wars. They <mark>wish for peaceful lives</mark> (near the end of Han), hence grouped up and <strong>formed a more powerful sect</strong> and started to gain governence in their own regions = the Yellow Turban, etc.</li>
</ul>

<p><strong>Religious Daoism</strong></p>

<ul>
  <li>
    <p>gained insights from Cihinese superstitions, and is for pursuing immortality (e.g. Qin and Emperor Wudi)</p>
  </li>
  <li>
    <p>but in 2nd century, there ermges religous Daoism: organized commual religous due to conflicts/dissent <mark>near end of Han</mark> (notice that <strong>something new happens near the end of a dynasty is pretty common by now</strong>)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004142119564.png" alt="image-20221004142119564" style="zoom:50%;" /></p>

    <ul>
      <li>Laozi is now made into a deity</li>
      <li>had two sects, by Zhang Daoling (五斗米道), and by Zhang Jiao (Yellow Turbans)</li>
      <li>those leaders claim they obtained new revelations and gained a lot believers</li>
    </ul>
  </li>
  <li>
    <p>eventually the growth of Daosim and the incoming Buddhistm lead to strong competition</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221004170739423.png" alt="image-20221004170739423" style="zoom: 33%;" /></p>

    <ul>
      <li>Daoist built temples v.s. Buddhist monasteris</li>
      <li>Daoist claim that they have better spells, soceries for protection and ailness, etc.</li>
    </ul>
  </li>
  <li>
    <p>There have been much efforts of one religion try to use the emperor to <strong>expel religion</strong>: but never had a single religion to dominate</p>

    <ul>
      <li>many western misconception therefore though Chinese religion had high tolerance, e.g. the exclusiveness of Islam and Christian</li>
      <li>but that is not true, the reality is that are also trying to exclude but just never works out</li>
    </ul>
  </li>
</ul>

<h1 id="sui-and-tang-581907">Sui and Tang (581–907)</h1>

<p><strong>Previously on the Age of Division</strong></p>

<ul>
  <li><mark>Eastern Han</mark>: falls from socioeconomic and environmental strain, and religious (e.g. Yellow Turf), peasant movements. Fractures into short-lived, celebrated Three Kingdoms period.</li>
  <li><mark>North-South</mark> division remains significant;
    <ul>
      <li>South underdeveloped, only has 1/6 of N.’s population;</li>
      <li>North: had mostly <strong>proto-Mongol or Turkic nomadic people</strong> including 1. Xiongnu, 2. Xianbei, 3. Tuoba establish series of “kingdoms” which are usually brief; Northern kingdoms see a degree of <mark>Sinicization</mark>, but reverse also occurs; ethnicity and culture are <mark>fluid</mark></li>
      <li>Differences in geography, climate, trade routes; the rise of aristocracy</li>
      <li>One of the legacies of the <mark>Northern Wei</mark> (Tuoba branch of Xianbei) is patronage of Buddhism, which was translated ‘through’ Daoism, (i.e. wuwei and nirvana)</li>
      <li><mark>Sui</mark> had a series of measures trying to unite and resolve the difference:
        <ul>
          <li>a law code equally applied to all</li>
          <li>the <strong>Grand Canal</strong>, which linked the Huang He and the Ch’ang Rivers and encouraged north-south trade within China</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><mark>Buddhism</mark> transforms culture; <mark>Chinese</mark> culture transforms Buddhism
    <ul>
      <li>Religious Daoism: in competion with Buddhism, wanted to get exclusive patronage by the emperors but never work out</li>
    </ul>
  </li>
</ul>

<p><strong>Reunification of Sui</strong></p>

<ul>
  <li>
    <p>some key milestones of Sui and later</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006132308299.png" alt="image-20221006132308299" style="zoom: 50%;" /></p>

    <ul>
      <li>Reunification under the Sui (581-618) <strong>Yang Jian 杨坚</strong></li>
      <li>Yang Guang杨广, the first emperor’s son when successed build the <strong>Grand Canal</strong> (try to keep China together) but also had extravagant rebuilding of capital at Luotang (critisized)</li>
      <li>Yang Guang’s campaign to Korea later was failed, and rebelliions broke out in China = emperor killed = <mark>Li Yuan founded Tang</mark></li>
      <li>Early Tang: mainly <mark>Emperor Taizong (Li Shimin)</mark> &amp; empire-building+conquering – Tang Cosmopolitanism: Chang’an and material culture</li>
      <li><mark>Empress Wu (Zetian)</mark>, who reigned for a long time, and the role of women</li>
      <li><strong>An Lushan Rebellion</strong> forced center to cede much power</li>
    </ul>
  </li>
  <li>
    <p><strong>rescined the ban</strong> on Buddhism and Daoism passed by the previous Northern Zhou dynasty in 574</p>

    <ul>
      <li>
        <p>monks were already working on Buddhist text protection in Zhou: digging caves and putting Buddhist texts incised in stone</p>
      </li>
      <li>
        <p>but Sui founder was himself a <mark>grand donor to Buddhism</mark>, built a lot of monasteries</p>
      </li>
    </ul>
  </li>
  <li>
    <p>mixed nordic and Chinese, a reunification of Chinese northern and southern states</p>

    <ul>
      <li>north and southern could have became a permanent separation</li>
      <li>new centralized bureucracy even stronger than in Han, and enforced it in a short periof of time (like Qin)</li>
    </ul>
  </li>
  <li>
    <p>later in Tang, the division seemed a much undesired situation</p>

    <ul>
      <li>as shown by Tang, unification = longer dynasty e.g. <strong>Han and Tang became an aim of every later ruler</strong></li>
    </ul>
  </li>
  <li>
    <p>since again, <mark>Sui</mark> are doing a lot but too soon (alike Qin)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006132844907.png" alt="image-20221006132844907" style="zoom:33%;" /></p>

    <p>where</p>

    <ul>
      <li>dig a canal to connect north and south, and to ship south resources to north</li>
      <li>faciliated and <strong>symbolizes the political reunification</strong> of south and north</li>
    </ul>
  </li>
  <li>
    <p>new system by exam in <mark>Sui</mark> = <strong>intended to deprive high aristocracy of power</strong></p>

    <ul>
      <li>opened to a somewhat larger population to serve in government</li>
      <li>but many successful candidates <strong>still correlates to power of rich families</strong></li>
    </ul>
  </li>
  <li>
    <p>second emperor wanted to rebuild the great wall, and build a road (much different from the first emperor of Sui, and very alike Qin)</p>
    <ul>
      <li>he also conmissioned lots of army to vietnam, turko-mongol in the north, and korea</li>
      <li>he did too much, <strong>distrupt dometic ecnomony, and with the failure of Korea, and became overthrown</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Tang Founders</strong>: cosmopolitanism about elite culture</p>

<ul>
  <li>with the disintegration of the Sui dynasty in July 617, <mark>Li Yuan</mark> – urged on by his second son Li Shimin (李世民, the eventual Emperor Taizong) – rose in rebellion.
    <ul>
      <li>Using the title of “Great Chancellor” (大丞相), Li Yuan installed a puppet child emperor, Yang You, but eventually removed him altogether and established the Tang Dynasty in 618 as Emperor.</li>
      <li>His son and successor <mark>Li Shimin honoured him as Gaozu ("high founder")高祖</mark> after his death.</li>
      <li>and later <mark>Li Shiming become Tang Taizong</mark>, achieved by killing his brother and forced his father to step down
        <ul>
          <li>unethical as it violated Confucian norms, and hence a “legend” arose that emperor was summoned before the King of the dead</li>
          <li>the king of the dead asked why did you do it</li>
          <li>the emperor was terrified, and in the end sacrificied his family for the good of the empire</li>
          <li>shows the belief in afterlife and <mark>juristication of the courts of the dead</mark>, which is a norm of justice at that time</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ruling in the north become multi-ethnic
    <ul>
      <li>lots of Tuoba intermariage, different families made different choices</li>
      <li>already in Sui: e.g. wife of the founder of Sui was Turkish, and the crazed son of Emperor Taizong (because his father killed his homosexual lover with a palane entertainer) insist on only speaking <strong>Turkish</strong>. Eventually his wife Wu Zhao took over and became <strong>Empress Wu</strong></li>
      <li>in general, very <strong>mixed identity in the north</strong></li>
    </ul>
  </li>
  <li>Tang followed a lot of Sui policies
    <ul>
      <li>both had a modified form of the <mark>equal-field system</mark>, which originated from Northern Wei by Dowager Empress Feng</li>
      <li>both retrained militia system of nothern dynasties (very ingrained into the population)</li>
    </ul>
  </li>
  <li>Tang’s founder is the <strong>Li family</strong>
    <ul>
      <li>also mixed ethnic background, spoke both Turkish and Chinese</li>
      <li>hence with <strong>a heavy Tuoba element and steppe culture</strong>, e.g. liked horse riding and hunting
        <ul>
          <li>steppe nomads learned to ride from a young age and were expert horsemen</li>
        </ul>
      </li>
      <li>after An Lushan rebellion, people wanted to draw a more clear ethnic lines</li>
    </ul>
  </li>
  <li>The tang was an age not just for cultural openenss but of <strong>political strength</strong>, and had much control over its citizens
    <ul>
      <li>official supervised markets carefully (e.g. periodically monitored prices), and also kept a close watch on rural residents</li>
    </ul>
  </li>
</ul>

<p><strong>Tang’s military culture: part of the elite identity</strong></p>

<ul>
  <li>
    <p>e.g. from Han, Wudi was a emperor of strong military</p>
  </li>
  <li>
    <p>in case of Tang, it is different from Han because</p>

    <ul>
      <li>
        <p>it is a militarized society, mixed of military <strong>and civilist cultures</strong></p>
      </li>
      <li>
        <p>the <strong>militia system 服兵</strong> made almost every person had some kind of military knowledge</p>

        <ul>
          <li><mark>Sui and Tang</mark> developed this system, which a way to privde a national army from a local level</li>
          <li>based off from equal-field system, so that given land = tax and <strong>to supply one soldier,</strong> but that soldier is exempt from taxes</li>
        </ul>
      </li>
      <li>
        <p>soldiers are serving by <mark>rotations</mark> (hence no permanent soldier = less concentrated regional power)</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006134140581.png" alt="image-20221006134140581" style="zoom:33%;" /></p>

        <ul>
          <li>when off duty = training</li>
          <li>when on duty = go to war if there are wars, otherwise patrols in captials, etc.</li>
          <li>don’t receive salaries, and so need to prepare their own equipmenet, horse, etc, but will have rewards from <strong>won battles</strong>
            <ul>
              <li>e.g. recall Mulan, who had to buy her own horse</li>
            </ul>
          </li>
          <li>won battles for rewards in land, cash, ranks, hence can be a way some family desires</li>
        </ul>
      </li>
      <li>
        <p>using the above system, the cost of launching wars is minimized. Hence lead to a high degree of military knowledge penetrating throughout the population</p>
      </li>
    </ul>
  </li>
  <li>
    <p>using this militia system, Tang got a lot of <strong>expansion success</strong></p>

    <ul>
      <li>
        <p>in 630, conquered Mongols in <strong>Mongolian</strong>. This victory gave Taizong a title “Heavenly Khan”<mark>天可汗</mark></p>
      </li>
      <li>
        <p>also campaigned western states</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006134530315.png" alt="image-20221006134530315" style="zoom:33%;" /></p>

        <p>which become mordern day Xinjiang</p>
      </li>
      <li>
        <p>but since a large part of Turkish soldier who were sent from tang’s army settled there, hence they people in Xinjiang speak turkish a lot</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Established Six Protectorates</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006134820782.png" alt="image-20221006134820782" style="zoom:33%;" /></p>

<ul>
  <li>a protectorates has a local ruler and autonomy</li>
  <li>the protectorate submits to the Tang emperor as the highest sovereign.</li>
  <li>a Tang was to pacified te region</li>
  <li>the protectorates also paied tributes ot the Tang, in return, emperor gave them licenses for trade = trade relationship</li>
</ul>

<p><strong>State-building at home</strong></p>

<ul>
  <li><mark>Tang Code</mark> of 653 was the earliest code to survive directly, read and copied (before other code was dug up + bamboo)
    <ul>
      <li>principles of crimilal laws, etc.</li>
      <li>many other countries (e.g. Japanase, Korea) adopted a lot of Tang code not for its content, but for the symbol of prosper</li>
      <li>but punishment depends on social status: priviledged, commoners, and inferior people (e.g. slaves)</li>
      <li>the code was also calibrated based on sex, age, and disabilities</li>
      <li><strong>reflected Legalist on appropriate treatment, but also espoused Confucianism values</strong> (e.g. killing own father much worse than killing a stranger)</li>
    </ul>
  </li>
  <li>promoted <mark>Confusion education</mark>, and expanded the <mark>civil exam</mark>
    <ul>
      <li>but still Tang’s offcials is only at the early state of change, most still come from wealthy family</li>
      <li>only 20-30 men passes the exam per year, hence bureaucracy still mostly domoinated by aristocrates</li>
      <li>the change is <strong>most intensified during Song</strong></li>
      <li>at the start of the Song, those great families started to fade away, perhaps rebellion targeted at aristocracy at the end of Tang = genocide.</li>
    </ul>
  </li>
</ul>

<p><strong>Tang’s openness</strong>: Changan长安 = eternal peace</p>

<ul>
  <li>recall that <strong>Chengzhou成周=Luoyang, Changan长安= Xi’an</strong></li>
  <li>Chang’an was rebuilt by Sui as a political statement, and was rebuilt by the Tang for the same reason = symbol of power</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006135233371.png" alt="image-20221006135233371" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>the city layout is highly planned, had 1 million living inside and approx another million just outside.</p>
  </li>
  <li>
    <p>a lot of walls for security purposes (had curfews, patrols, etc.)</p>
  </li>
  <li>
    <p>today none of the Tang outer walls remain, but only the Ming wall survived, which is approximate only around the palace city area</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006135408226.png" alt="image-20221006135408226" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>emperor palace in the north, and people lived in the rectangular units = a unit. Rich people lives closer ot the palace, and there are also two <strong>notable markets</strong>, where</p>

    <ul>
      <li>west markets are for foreign trades = most foreign merchant come form west</li>
      <li>east markets are for trading local goods</li>
    </ul>
  </li>
  <li>
    <p>the southern gate also had a <strong>very wide avenue: wide enough for much traffic</strong> (gone today. but below is reconstruction)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006135627741.png" alt="image-20221006135627741" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Emperor Taizong is very interested by Xuanzang玄奘, a Buddhist in search of Buddhist scripture and came back</p>

    <ul>
      <li>He is known for the epoch-making contributions to Chinese Buddhism, the travelogue of his journey to India in 629–645 CE, his efforts to bring over 657 Indian texts to China, and his translations of some of these texts</li>
      <li>inspired the modern novel: <strong>The journey to the West (西天取经)</strong>, to depict Xuanzang’s journey in search of the Buddhist script</li>
    </ul>
  </li>
  <li>
    <p>knowledge of outside world largely <strong>influenced by those exotic goods, central asian musicians, magician</strong>, etc.</p>

    <ul>
      <li>attest the love of Tang elite for cosmopolitan cultures and especially those of the central asia</li>
      <li>but they had a hard time diferentiate those foreign people, <strong>so called them “Hu”</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Who are the Hu 胡?</strong></p>

<ul>
  <li>
    <p>general term for foreigners from the north or west. The origin might be related to homophone “beard.”</p>
  </li>
  <li>
    <p>North influences: steppe peoples, Xiongnu, Xianbei, Mongols.</p>
  </li>
  <li>
    <p>West influences: Sogdians, Parthians, Persians, Turkic.</p>

    <ul>
      <li><strong>Sodigans</strong> was an ancient <strong>Iranian</strong> civilization between the Amu Darya and the Syr Darya, and in present-day Uzbekistan, Turkmenistan, Tajikistan, Kazakhstan, and Kyrgyzstan.</li>
      <li>sodigans were quite useful - important <strong>middle-men of Tang and Byzantine empires.</strong></li>
      <li>Sodigans are also <strong>promininet among military</strong> (e.g. soldiers) and merchants in Tang</li>
      <li>hence have a lot of Sogdian figurines: stablemen, soldiers, musicians.</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006140016892.png" alt="image-20221006140016892" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>those <mark>Sogdian people also brought a major changes to Chinese music</mark> (and dance)</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006140105446.png" alt="image-20221006140105446" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006140445166.png" alt="image-20221006140445166" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <ul>
      <li>pipa was from western asia, but now is a major musical instrument in China</li>
    </ul>
  </li>
  <li>
    <p>before Tang, all chinese sit on the ground with the formal sitting position, but then in Tang people begins to <strong>sit in the chair</strong> (slowly shifts away to a new pose)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006140624154.png" alt="image-20221006140624154" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Tang Trade</strong></p>

<ul>
  <li>
    <p>some people say it is buddhist monks <strong>brought chairs in for trade</strong></p>
  </li>
  <li>
    <p>Samarkand-based Sogdiana and Ferghana supplied fine <strong>horses</strong></p>
  </li>
  <li>
    <p>many exotic foods</p>

    <ul>
      <li>from Central Asia: peas, spinach, garlic.</li>
      <li>from Persia: sesame, pistachios.</li>
      <li>from India: black pepper, cane sugar (native sweetener: malt sugar &amp; honey) • from Mediterranean: coriander.</li>
      <li>grape wine available in Chang’an.</li>
      <li>lychee delivered daily from warm southlands for Consort Yang.</li>
    </ul>
  </li>
  <li>
    <p>Tea became a national drink and a major trade item.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006141031697.png" alt="image-20221006141031697" style="zoom:33%;" /></p>

    <ul>
      <li>throught the history people had their own “culture” during their time. In Tang, a specialized tea set above</li>
      <li>added ingredients such as salt, citric peel, etc.</li>
    </ul>
  </li>
</ul>

<p><strong>Life of Commoners and Merchants</strong></p>

<ul>
  <li>merchants much well off than commoners, who need to to menial jobs such as maintaingin gardens</li>
  <li>with the “Tale of Li Wa”, which talked about a <strong>high officials family background</strong> young man came for examination $\to$ attracted to Li Wa, a prostitute $\to$ used up all his money and was abandoned $\to$ became a begger, almost died, saved by employees at funeral shop $\to$ became a famous professional mourner $\to$ father found out and beated him almost to death (assuming the status of polluted, inferior person) $\to$ saved by Li Wa $\to$ studied hard and <strong>became a official</strong></li>
  <li>shows that <mark>flux of social class is not that encouraged</mark></li>
</ul>

<p><strong>Examinations in Tang</strong></p>

<ul>
  <li>exams are used to recruit men of good character</li>
  <li>therefore, taking examinations = regularly <mark>visiting examiners and giving them samples of your writing</mark></li>
  <li>but the good writings = poetry = only those brought up in a good family could be expected to know</li>
  <li>importance of poetry also lead to <strong>explosion in poetry writing</strong></li>
  <li>but it ended still with majority of the court composing people from families of high social status</li>
</ul>

<p><strong>Religion in Tang</strong></p>

<ul>
  <li>
    <p><strong>Buddhist</strong> and <strong>Daosit</strong> are the major players, and <strong>Confucian</strong> central to states, but more patronized Daoism</p>

    <ul>
      <li>Confucianism thought more like “framework”/structure for organizing the governments, e.g. <mark>train candidates in Confucian classics</mark>.</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006141208343.png" alt="image-20221006141208343" style="zoom:33%;" /></p>

    <ul>
      <li>more support for Daoism, because imperial famly claimed their origin can be traced back to Laozi</li>
      <li>influnce of Buddism is still everywhere, a lot of monarsteries</li>
    </ul>
  </li>
</ul>

<p><strong>Tang “soft power” and Japan</strong></p>

<ul>
  <li>
    <p>properous of China attracted much foreign countries who also seek to build an empire (e.g. many <strong>Korean and Japanese</strong> in Chang’an)</p>
  </li>
  <li>
    <p>Japanese connection also became much deeper</p>
    <ul>
      <li>Japan rulers were also tring to establish empire, since Tang had been so powerful</li>
      <li>based their policy on Japanese pepole who travelled to China study stuff and taking back to Japan with them, and relied on a lot of Buddhists</li>
      <li>the capital of Nara built in 618, and then <strong>Kyoto built in 794 are smaller versions of Chang’an</strong>: square in shape, geometrically aligned on a north-south axis; grid structure.</li>
      <li>Kimono is from China, and lots of <strong>Tang scriptures are importanted to Japanese</strong> (who has spoken laguage but not written)</li>
    </ul>
  </li>
</ul>

<p><strong>Dynasty Names Adopted as Ethnic Term for Chinese</strong></p>

<ul>
  <li><mark>Han</mark> mainly used in mainland China
    <ul>
      <li>in 20th century mainland, China’s national ethnicity is constructed and called “Han”</li>
      <li>national language also called “Han language”</li>
    </ul>
  </li>
  <li><mark>Tang</mark>, more popular used for the Western, e.g. Tangren Jie=China Town
    <ul>
      <li>because first migration was Cantonese speaking diaspora (control of them was solidified during Tang)</li>
      <li>Tang also had a much broader contact with the <strong>rest of the world</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Empress Wu: only woman emperor in China</strong></p>

<ul>
  <li>previous empres are mostly regent, dowager</li>
  <li>personal name <mark>Wu Zetian</mark>
    <ul>
      <li>regent, 660-690; as the Emperor of Zhou, 690-705</li>
      <li>ruled very capably, but there exists evidence her approach are sometimes ruthless</li>
    </ul>
  </li>
  <li>her real look must be beutiful (praised by emperor), was borned into a wealth family
    <ul>
      <li>encouraged to read and write in family</li>
      <li>married to emperor family</li>
      <li>interesting story of how she came to power, e.g. was concubine of Taizong, but when Taizong died, bore the first child of Gaozong (from whom she took power from), which indicates incest</li>
    </ul>
  </li>
  <li>
    <p>She successfully succeeded the throne largely because lots of officials also supported her, and <mark>justified by using Buddism</mark></p>

    <ul>
      <li>a monk wrote a commentary that told of a female goddess (hint at Empress Wu) who would be reborn as a princess in a small kingdom, etc.</li>
    </ul>
  </li>
  <li>later <strong>Xuanzong</strong> made China the most properperous time, but was built by Empress Wu</li>
  <li>but still since Empress = <strong>upset a balance of nature, hence some scholars are still concerned</strong>
    <ul>
      <li>Confucious idea that natural diasters = emperor problem (Dong Zhongshu, a ruler who did not fullfill = nature diaster)</li>
      <li>earth quake happened, and ministers took it as a problem</li>
      <li>hence, some portray her as a ruthless ruler (e.g. because scribes are mane)</li>
    </ul>
  </li>
  <li>
    <p>avent sponsor of Buddhistm, but to user religion in her favor (e.g. justify her throne)</p>
  </li>
  <li>
    <p>in general Tang people had freeodm and under her, <strong>women (tried) to have better freedom</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221006142234989.png" alt="image-20221006142234989" style="zoom:33%;" /></p>

    <ul>
      <li>e.g. shifts in beauty standards: women on horse also appealing.</li>
    </ul>
  </li>
  <li>
    <p>The importance to history of Wu Zetian’s period of <strong>political and military leadership includes the major expansion of the Chinese empire</strong>, extending it far beyond its previous territorial limits, deep into Central Asia, and engaging in a series of wars on the Korean Peninsula, first allying with Silla against Goguryeo, and then against Silla over the occupation of former Goguryeo territory.</p>
  </li>
  <li>
    <p>other notable changes in value include:</p>

    <ul>
      <li>as <strong>wife</strong>
        <ul>
          <li>main wife but not cocubine</li>
          <li>men have shorter lifespans than women, so the wife often becomes the only head of the household.</li>
        </ul>
      </li>
      <li>as <strong>mother</strong>
        <ul>
          <li>should continue have influence even when sons become adults</li>
          <li>cocubine is lower in status, hence concubine’s son should regard main wife as mother</li>
        </ul>
      </li>
      <li>as <strong>clan advodate</strong>:
        <ul>
          <li>encouraged women’s position in government, but still install family males to most positions especially to military</li>
          <li>this is anyway too big of a change for a complete equality of gender here</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Interesting Note</strong> from primary sources:</p>

  <ul>
    <li>Emperor Taizong’s advice on effective government: land is large, need cooperation = give relatives land. Zhou is a successful model lasted several centuries
      <ul>
        <li>Qin had zero relatives to rely on (Li Si’s plan) hence failed</li>
        <li>Han enfeoffed the land too generously: the Six Kingds harbored ambitions of overthrowing throne</li>
        <li>so the best way is to enfeoff many relatives to even up their power and to have them regulate one another and share responsibilities</li>
        <li>and for government officials: 适才所用</li>
        <li>military cannot be eliminated nor used all the time</li>
        <li>emphasis of literary arts and military arts</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="the-decline-of-tang">The Decline of Tang</h2>

<p><strong>Previously on Sui and Tang</strong></p>

<ul>
  <li>
    <p>Sui-Tang continuity: Tang was a successor to the Sui and Northern Dynasties</p>
  </li>
  <li>
    <p>Three features of Tang culture (mostly elite)</p>

    <ul>
      <li>
        <p>cosmopolitanism: ethnic diversity, Tang ruling family (Li) as a classic example</p>
      </li>
      <li>
        <p>importance of military pursuits and military cultures to elite identity; a more militarized society than later period</p>
      </li>
      <li>
        <p>aristocracy remained dominant in government (the system of examination is only fully utilized for social flux at Song Dynasty)</p>
      </li>
    </ul>
  </li>
  <li>Tang Empire (618-907) marks <strong>culmination of the turn toward Central Asia</strong> that began in the Han Dynasty, and continued through age of disunity. Tang military institutions, urban culture, silk road trade and religious life look westward
    <ul>
      <li>however, in Late Tang due to much political disunity they withdrawn their troops and lost most of the control in Central Asia</li>
    </ul>
  </li>
  <li>Emperor Wu Zetian is the only woman to rule under her own name; powerful and competent (some ruthless approaches), expanded empire, but disparaged by later generations because she is a woman</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011132556798.png" alt="image-20221011132556798" style="zoom: 50%;" /></p>

<p><strong>Emperor Xuanzong: the peak of the Tang</strong></p>

<ul>
  <li>
    <p><strong>Emperor Xuanzong:</strong> early half of his reign: astute ruler, the empire expanded to the high point; patronage of the arts (especially poets)</p>

    <ul>
      <li>Empress Wu had his daughter tried to poison her, trying to become the next empress. So there were some instability, but Emperor Xuanzong made it</li>
      <li>recall that Empress Wu had a lot of her family relatives in government posts, and sponsored a lot of Buddhism, e.g. claimed that she is the incarnation of Bodhisattva. Emperor Xuanzong tried to <strong>redress/reform</strong> those
        <ul>
          <li>he also reformed Equal-Field system due tax problems encountered during their time</li>
        </ul>
      </li>
      <li>there were also threats from the Turks, Tibetans, so he restructured military system and give commanders great authority (among them he particularly gave <strong>An Lushan much power</strong>)</li>
    </ul>
  </li>
  <li>
    <p>change of women’s clothing, don’t reveal shoulders or backs</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011132658550.png" alt="image-20221011132658550" style="zoom:33%;" /></p>

    <p>yet this type of dress (rightest) is not for commoners, and it is more popular for them to wear men’s clothing = meaning more physical mobility. An example below are all women:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011132739433.png" alt="image-20221011132739433" style="zoom:33%;" /></p>

    <p>hence this tells something about the change of attitudes towards women</p>

    <ul>
      <li>
        <p><mark>freedom of women wearing riding trousers with faces uncovered</mark>, where as in later centuries women of equivalent status would always be veiled, ear robes, and <mark>be restricted to sedan chairs (Song dynasty)</mark></p>
      </li>
      <li>
        <p>but unfortunately due to decline of Tang later, it reverted to conversative clothing</p>
      </li>
    </ul>
  </li>
  <li>
    <p>however, at late ages had some problem. He fell in love with Prized-Consort Yang杨贵妃, and gave too much power to her</p>

    <ul>
      <li>falling in love with 杨贵妃, who lacked political sense, even when he is 60</li>
      <li>gave too much power to Yang and her relatives, so they almost monolpolized the government</li>
      <li>basically becomes a femme fatale</li>
      <li>
        <p>An Lushan liked by Emperor, but relative from Yang’s family disliked his power, so <mark>arrested and executed An Lushan's supporters in court</mark></p>
      </li>
      <li>this oral history and gossip recorded in Bai Juyi’s “A Song of Unending Sorrow”</li>
    </ul>
  </li>
  <li>
    <p>however, Yang did show us the standard of beauty at that time is to be a little bit chubby</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011134021391.png" alt="image-20221011134021391" style="zoom: 33%;" /></p>
  </li>
</ul>

<p><strong>Battle at Talas</strong>: in modern Khazakhstan</p>

<ul>
  <li>suprised by the sudden defection of China’s tribal allies, the Chinese <strong>lost the battle to the Islamic Abbasid caliphate</strong></li>
  <li>marked the beginning when <mark>region of Central Asia turned away from China towards the Islamic world</mark></li>
</ul>

<p><strong>General An Lushan’s rebellion (755)</strong></p>

<ul>
  <li>
    <p>recall that Yang Guozhong (cousin of Prized-Consort Yang, chief minister) began to purge members of General An Lushan’s faction due to Yang’s relative’s dominance in court</p>
  </li>
  <li>
    <p>also at that time, due to malfunctioning of equal-field system, <mark>state relies on soldiers from alien tribes instead of collected from each field</mark></p>

    <ul>
      <li>problem 1), they <strong>don’t pledge alliance to Tang</strong></li>
      <li>problem 2), they become <strong>long-lasting</strong>, whereas equal-field collected soldiers will go back home</li>
      <li>problem 3), An Lushan was also particularly powerful due to prior likings of Emperor Xuanzong</li>
    </ul>
  </li>
  <li>
    <p>General An Lushan:</p>

    <ul>
      <li>commanded two large frontier armies in the northeast, in modern-day Beijing.</li>
      <li>father was Turkish; mother was Sogdian.</li>
      <li>alarmed that his supporters at court were being arrested and executed.</li>
      <li><mark>in 755, leads an army of 150K troops towards the capital cities</mark>.</li>
      <li>he first captures Luoyang, the ‘eastern capital’ on the edge of the plains.</li>
      <li>from Luoyang, the fortified Tong Pass blocked westward passage to Chang’an.</li>
      <li>but Yang Guozhong pressed imperial forces to leave the Pass and do battle—and were destroyed by An Lushan. - <strong>frontier troops were professional, war veterans</strong>; capital troops, unseasoned.</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011134349910.png" alt="image-20221011134349910" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>as a result, Emperor Xuanzong flees from Chang’an to Chengdu (modern-day Sichuan), and eventually abdicates the thrown to his son.</p>

    <ul>
      <li>note that Chengdu is also the capital of Shu, and hence “The road to Shu is steep” from Li Bai is relevant</li>
    </ul>
  </li>
</ul>

<p><strong>Consequences of this rebellion</strong>? A major turning point for Tang: decline of Tang stability</p>

<ul>
  <li>Tang’s forces in central asia has to be recalled for rescue. <mark>Tibetan Empire and Muslim Caliphate</mark> are also encroaching those areas, so a retreat made Tang a lost of control there, which eventually caused the lost of control of the Silk Road
    <ul>
      <li>Xinjiang ceded to Tibetan empire and Muslim Caliphate</li>
      <li>central asia then changed from Buddhism to Islam</li>
      <li>cut-off from horse supplies due to lost of Silk Road</li>
      <li>became less cosmopolitan in western eyes</li>
      <li>without this buffer zone frontier, capial Changan is more exposed to incursion</li>
    </ul>
  </li>
  <li>rise of regional warlordism: to quell the rebellion, Xuanzong <strong>gave a lot of power for regional governors</strong>.
    <ul>
      <li>then those regional governments <strong>later</strong> made themselves <strong>autonomous</strong>, hence results in political chaos</li>
      <li>but also caused an even further increase in local military knowledge</li>
    </ul>
  </li>
  <li>rebellion resulted from, caused, or simply coincided with
    <ul>
      <li><strong>collapse of the equal-field system</strong> (for collecting tax revenue)
        <ul>
          <li>during quenching rebellions, system of household registration collapsed = does not know how many residents are there = cannot collect tax</li>
          <li>because there is a shortage of land (due to population increase, aristocrats gathered too much land). Government cannot raise enough revenue.</li>
          <li>lost hold of household registrar = <mark>lost direct control of the people</mark>! Then then had to rely on regional governments to report those numbers, which never works out.</li>
        </ul>
      </li>
      <li><strong>collapse of the militia system</strong>, since no money, hence has to relie on <mark>foreign solders</mark> who pledged alligence to comamnder bu tnot dynasty</li>
    </ul>
  </li>
  <li>all of those contributed to the collapse of the down fall of Tang</li>
  <li>this is also a period of time hard for ordinary people to survive, e.g. as deadly as first world war
    <ul>
      <li>as the battle happed in the capital, Du Fu and his family fled. This also becomes a major turning point in Du Fu’s life</li>
    </ul>
  </li>
</ul>

<p><strong>Poetry: Li Bai and Du Fu</strong></p>

<ul>
  <li>
    <p>they are friends but differ greatly in work. Li Bai write about nature, especially wine, but <mark>Du Fu is more for his social consciousness</mark> (e.g. events happening at Tang)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011135256440.png" alt="image-20221011135256440" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>the rebellion was also a turning point of Du Fu</p>

    <ul>
      <li>no longer a confident scholar, but become an ordinary person, hence his <strong>writing style becomes much heavier and realistic</strong></li>
      <li>became a helpless pawn, and wanted to write the things he wanted to say (<em>春望</em>: 国破山河在, …)</li>
    </ul>
  </li>
</ul>

<p><strong>Change of Territory</strong></p>

<ul>
  <li>
    <p>Tang is the first dynasty where knowledge of the outside world is really accessible for China commoners lives (e.g. in west markets, and pilgrim, merchants, caravans have been traveling back and forth). Before in Zhou dynasties, and even in Tang, those knowledges are mostly exclusive to the court</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011140358147.png" alt="image-20221011140358147" style="zoom:33%;" /></p>

    <p>but now, China can no longer see himself as the only civilized nation (e.g. Zhou King once said the others are barbarians). Several other great civilizations exist, for the control of Central Asia trade routes</p>
  </li>
  <li>
    <p>Muhhamad preaches <mark>Islam</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011140615505.png" alt="image-20221011140615505" style="zoom:33%;" /></p>
  </li>
  <li>
    <p><mark>Arab Caliphate</mark> v.s. Tang, wanted Tang’s submission</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011140727895.png" alt="image-20221011140727895" style="zoom:33%;" /></p>

    <p>which was before An Lushan, where Tang had some initial success but suffered a decisive defeat shortly before An Lushan Rebellion.</p>

    <ul>
      <li>
        <p>Defeat = those Chinese painters, craftsmen are taken as prisoners in Caliphate  = introduction of paper making and gave technology away</p>
      </li>
      <li>people argue that this also marks the end of Tang’s west expansion</li>
      <li>sooner or later Islam also spread in thos area</li>
    </ul>
  </li>
  <li>
    <p>another force was <mark>Tibetan Empire</mark> in the mid-7th century</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011141035756.png" alt="image-20221011141035756" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011141316127.png" alt="image-20221011141316127" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <p>the birth of this Empire comes from Songtsen Gampo, lead to organized state and powers</p>

    <ul>
      <li>Tang wanted a peaceful relation, hence had consort sent to Tibetan King, and made alliance with Tibet</li>
      <li>this princess is said to bring agricultural technologies, and to introduce Buddhism</li>
      <li>later, Songtsen Gampo also married a Nepal princess, who had their own version of Buddhism</li>
      <li>after Songtsen Gampo, (esoteric) Buddhism became Tibet’s religion, but very differnt from Buddhism in China</li>
    </ul>

    <p>even though there are some peaceful times, it eventually deteriorated. So after An Lushan, <strong>Tibetan Empire recaptured the land and tried to press on deeper in China</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011141657620.png" alt="image-20221011141657620" style="zoom: 50%;" /></p>

    <p>this also caused the direct routes for Silk Road is no longer at Chinese hands</p>
  </li>
</ul>

<p><strong>Examination System In Late Tang</strong>: Bai Juyi</p>

<ul>
  <li>even after all those territorial changes, the system of examination stayed basically the same as in Early Tang</li>
  <li>exams tested on
    <ul>
      <li>classics and literary ability/merit: write essays on a given topic</li>
      <li><mark>social ability: establish good relations with examiners</mark>
        <ul>
          <li>sons of powerful official families often given high scores, hence still mostly elite families in offices</li>
          <li><strong>In 800, for once the exam is just based on merit, and Bai Juyi passed</strong></li>
          <li>a different structure in Song</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>The Rise of Eunuchs</strong></p>

<ul>
  <li>usually eunuchs are for serving women’s quarters (guard the women’s living areas), and some for messengers and some for spies on military commanders</li>
  <li>but after An Lushan’s rebellion, they began to
    <ul>
      <li>gain control of the emperor’s personal army</li>
      <li>placing adopted sons into different offices, and forming factions with each other</li>
      <li><mark>hence eventually obtained high positions and controls</mark></li>
    </ul>
  </li>
  <li>over the ninth century, there are eight more changes of emperor where each accession was <strong>engineered by eunuchs</strong> working in concert with political factions
    <ul>
      <li>hence, life at court such as Bai Juyi depended a lot on <strong>which faction is in current power</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Mogao Caves and Dunhuang Library</strong></p>

<ul>
  <li>The ‘Caves of the Thousand Buddhas’ (Qianfodong), also known as <mark>Mogao</mark>, are a <mark>magnificent treasure trove of Buddhist art</mark>. They are located in the desert, about 15 miles south-east of the town of Dunhuang
    <ul>
      <li>By the late fourth century, the area had become a busy desert crossroads on the caravan routes of the Silk Road linking China and the West. Traders, pilgrims, and other travelers <strong>stopped at the oasis town to secure provisions, pray</strong> for the journey ahead or give thanks for their survival.</li>
    </ul>
  </li>
  <li>
    <p>adjacent to one of the caves was the <strong>hidden Dunhuang Library</strong> discovered by Daoist Wang: a secret library of 40,000 documents was discovered inside cave #16 by Wang Yuanlu. The library is referred to as cave #17</p>

    <ul>
      <li>almost half of the text where Buddhist text in Chinese or Tibetan</li>
      <li>first time purely <mark>original documents not for public use</mark> (compared to books written after the fact) to reveal the real life - religious and civil in Tang.</li>
      <li>but because papers are <mark>recycled, the margins and backs display all kinds of documents</mark>, including literacy work, contracts, government documents, etc.</li>
    </ul>
  </li>
  <li>
    <p>some civil and religious life revealed include</p>

    <ul>
      <li>
        <p>During the thousand years of artistic activity at Dunhuang, the <strong>style of the wall paintings</strong> and sculptures changed. The early caves show greater Indian and Western influence, while during the Tang dynasty (618–906 C.E.) the influence of the Chinese painting styles of the imperial court is apparent.</p>
      </li>
      <li>
        <p>donor portraits with <strong>Xianbei ethnic feature</strong></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013215226656.png" alt="image-20221013215226656" style="zoom:33%;" /></p>
      </li>
      <li>
        <p><strong>monasteries at Dunhuang</strong> also ran a variety of money-making enterprises, including oil presses</p>
      </li>
      <li>
        <p>Sodigan letters, Irq Bitig (8th-9th c), the only Old Turkic manuscript partially overwritten with Buddhist verses in Chinese</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>The Beginning of Printing</strong></p>

<ul>
  <li>
    <p>the first printed book found in Dunhang library cave.</p>
  </li>
  <li>
    <p>during Tang there were a high demand for texts (e.g. religions), hence lead to invention of woodblock printing, and later in Song 活体印刷</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011142335264.png" alt="image-20221011142335264" style="zoom: 33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011142345272.png" alt="image-20221011142345272" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <p>where the problem with the old technique is that if you made a mistake, the whole mold is useless</p>
  </li>
</ul>

<p><strong>Persecution of Buddhism</strong> (840 - 846)</p>

<ul>
  <li>many aristocrats were using land as Buddhism temples to <strong>evade tax</strong></li>
  <li>but also a lot of conflict between Buddhism v.s. Daoism and Confucianism</li>
  <li>at the end of Tang <strong>persecution of Buddhism provoked by Daoist</strong>, but also <strong>rationalized by Confucians</strong>
    <ul>
      <li>sees those temples asked commoners to buy weird goods and donate money while <strong>undermining tax revenues</strong></li>
      <li>see Buddhism to undermine family ties, give up your own responsibility and become a monk, hence violates Confucians ideal</li>
      <li>during (840-846), also as emperor favored Daoist, Buddhism was persecuted =  lots of monks and nuns have to be returned to their normal social status</li>
      <li>while Buddhism did not diminish, <strong>their impact in China never rose again to the peak in early times</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Huang Chao Rebellion</strong></p>

<ul>
  <li>
    <p>ruined Chang’an, never again as China’s capital</p>
  </li>
  <li>
    <p>put an end to high aristocracy in court by <mark>targeting the aristocrats of Tang</mark></p>
  </li>
  <li>
    <p>contributed to the fall of Tang in 907, and the distribution of power in China after the rebellion becomes</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221011165258535.png" alt="image-20221011165258535" style="zoom: 50%;" /></p>
  </li>
</ul>

<h1 id="song-960-1276">Song (960-1276)</h1>

<p><strong>Previous only the downfall of Tang</strong></p>

<ul>
  <li>Tang critically weakened by the An Lushan rebellion (755-63), allegedly caused by intrigue surrounding the Precious Consort Yang, Tang dynasty enters decline
    <ul>
      <li>retreat of forces in Central Asia lost control of it</li>
    </ul>
  </li>
  <li>Eurasian Empires in the 8th century: Tang, Tibetans, Muslims, Uyghur-Turks
    <ul>
      <li><strong>Tibetan Empire</strong>: Songtsen Gampo, Princess Wencheng. Eventually took over the Hexi corridor which is critical for Tang’s access to Central Asia</li>
      <li><strong>Abbasid Empire (Muslims)</strong>: the Battle of Talas (751) between the Tang and Abbasids, signals <mark>Tang loss of control over silk road and central Asia</mark></li>
    </ul>
  </li>
  <li>The <strong>Mogao Caves</strong> in Dunhuang: provide an abundance of vivid materials depicting Buddhist arts, religion, culture (4th-14th c), the largest body of Buddhist art in China; the <strong>library</strong> found there includes many secular texts that tell about politics, economics, ethnic relations of the local society in western China.</li>
  <li>Persecution of Buddhism: an example of conflicts between religion and state
    <ul>
      <li>Buddhist monasteries before operated oils, loans, banking services, and hold a large land being tax-exempted</li>
      <li>hence the central government became annoyed especially when they don’t have enough tax revenue</li>
      <li>Buddhism survived the persecution, but never reached its peak status as in Tang until today</li>
    </ul>
  </li>
  <li>Regional <mark>rise of warlords in late Tang</mark> (just like the final breakup in the Han dynasty)
    <ul>
      <li>government became decentralized during rebellion, and many elites fleed and clustered in Chang’an and Luoyang.</li>
      <li>then those elites are obliterated/almost compteletly wiped out during <strong>Huang Chao Rebellion</strong></li>
      <li>hence a period of disunion in 907 - 960, and the Wei river valley, <mark>Chang'an + Luoyang never again reach their prosper in the past</mark></li>
      <li>therefore, this removal of stale upper-class aristocrats also <strong>contributed to the success of the civil service exam</strong> in Song = really based on merit = based on your ability to read, understanding Confucian classics</li>
    </ul>
  </li>
</ul>

<h2 id="an-early-modernity-in-song">An Early Modernity in Song</h2>

<p><strong>China’s Population throughout History</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013132228281.png" alt="image-20221013132228281" style="zoom: 33%;" /></p>

<ul>
  <li>
    <p>inability of government in Tang might not mean bad economy. In fact, late Tang stimulated economy: rise of population, almost doubled even during Tang decline</p>
  </li>
  <li>
    <p>also a significant <strong>shift of population from north to south</strong>.</p>

    <ul>
      <li>Before North:South population is 2:1 in Early Tang</li>
      <li>After An Lushan Rebellion 1:1</li>
      <li>In Southern Song 1:2, when their capital in Kaifeng is sacked</li>
    </ul>
  </li>
  <li>
    <p>late Tang also have lots of <strong>technological developments, such as paddle-wheel Ship and Woodblock Printing</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013132427835.png" alt="image-20221013132427835" style="zoom:50%;" /></p>
  </li>
</ul>

<p><strong>Song Territory</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013132547104.png" alt="image-20221013132547104" style="zoom: 50%;" /></p>

<ul>
  <li>Song’s territory is the smallest among all of Tang, Song, Yuan, Ming, Qing</li>
  <li>also divided in two periods.
    <ul>
      <li>Early/Northern Song rules both north and south, including Kaifeng</li>
      <li>Late/Southern Song rules only capital at Hangzhou (Kaifeng is sacked by the northern enemies, i.e. the <strong>Jurchen forces</strong>)</li>
    </ul>
  </li>
  <li>but Song in general have a <mark>modern economy</mark> being industrialized and monetized</li>
</ul>

<p><strong>Agricultural Leap in Song</strong>: factors contributing to rise/improvement in agriculture</p>

<ul>
  <li>
    <p>new strains of rice from Champa/Vietnam = more rice yield as it can harvest twice a year</p>
  </li>
  <li>
    <p>mass iron production for plows and farm tools</p>
  </li>
  <li>
    <p>mobilization of bio-power (human and animal labor inputs).</p>
  </li>
  <li>
    <p>improved, intensive farming techniques of rice plots.</p>

    <ul>
      <li>wider application of fertilizers</li>
    </ul>
  </li>
  <li>
    <p>dikes, canals to convert wetlands to farmland.</p>
  </li>
  <li>
    <p><mark>advances in water/irrigation control</mark>, including pump and water mills</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Pump</th>
          <th style="text-align: center">Mill</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013133119636.png" alt="image-20221013133119636" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013133217766.png" alt="image-20221013133217766" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>hence families can get more rice then needed = can sell the rest and exchange goods</p>
  </li>
  <li>
    <p>as a result, population doubles to 120 million, from 60-65 million in Tang</p>
  </li>
</ul>

<p><strong>Commericial Modernization in Song</strong></p>

<ul>
  <li>
    <p>new/more agriculture techniques + invention means more <strong>specialization</strong></p>

    <ul>
      <li>
        <p>peasants can just <mark>exchange</mark> for other goods instead of trying to produce it themselves</p>
      </li>
      <li>
        <p>in late Song cottons=cash crop</p>
      </li>
    </ul>
  </li>
  <li>
    <p>as compared to regulated trades in Chang’an in Tang, in early Song, <strong>merchants and shops are spread out in town to intermix with residence</strong></p>

    <ul>
      <li>some even had the stock system = manager+owner of business</li>
      <li><strong>credit systems</strong> adopted by traders</li>
      <li>an increase of <mark>people in middle class</mark></li>
    </ul>
  </li>
  <li>
    <p><mark>encouraged maritime trade</mark> (as compared to Tang), due to cut off of silk road</p>

    <ul>
      <li>
        <p>encourage foreign traders to come to China as well</p>
      </li>
      <li>
        <p>to achieve maritime trade, needs improved ship technologies</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013133731794.png" alt="image-20221013133731794" style="zoom:33%;" /></p>

        <p>as well as the <mark>invention of compass</mark></p>
      </li>
    </ul>
  </li>
  <li>
    <p>as trade increase, <strong>demands of money increased</strong>. In addition, silk was also used as currency</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013134014492.png" alt="image-20221013134014492" style="zoom:33%;" /></p>

    <p>the largest output in Tang was 320 million coins a year. But in late Song, government maintains <strong>more than 6 billion coins</strong>. But there was still not enough, as there is still no inflation.</p>

    <ul>
      <li>government requires more money in circulation</li>
      <li>demand has been so great and hence <strong>issued the first paper money</strong>, which can be created out of nothing</li>
    </ul>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Paper Money</th>
          <th style="text-align: center">Government Logistics</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013134255718.png" alt="image-20221013134255718" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013134353502.png" alt="image-20221013134353502" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>hence <strong>Song’s economic change to China is almost revolutionary</strong></p>
  </li>
</ul>

<p><strong>Wang Anshi vs. Sima Guang</strong></p>

<ul>
  <li>
    <p>despite overall economic boom, <strong>some farmers still suffer</strong> from interest rates and tax</p>
  </li>
  <li>
    <p>The Emperor Shenzong of Song faced <strong>declining taxes and an increasingly heavy burden of taxation on commoners</strong> due to the development of large estates, whose owners managed to evade paying their share of taxes.</p>

    <ul>
      <li>Therefore, he sought for advices, and comes Wang Anshi</li>
    </ul>
  </li>
  <li>
    <p>Wang’s primary objectives of New Policies (<em>xinfa</em>) were</p>

    <ul>
      <li>cut government expenditure and <mark>strengthen the military in the north</mark> (which requires funds)</li>
      <li>To do this, Wang advocated for policies intended to <mark>alleviate suffering among the peasantry</mark> and to <mark>prevent the consolidation of large land estates</mark> which would deprive small peasants of their livelihood = aimed to help improve the wealth gap</li>
      <li>in reality, his policies is <strong>effectively the last effort by Song government to control the economy</strong> = last strand</li>
      <li>but this <mark>failed</mark> because the local officials implemented this policy were rewarded for how much loans they gave out $\to$ local officials benefited more than the farmers</li>
    </ul>
  </li>
  <li>
    <p>Specifically, his policies include:</p>

    <ul>
      <li>fixing fiscal policy: <strong>Green Sprout reform</strong>, which give peasants money in the beginning and ask for repay with 40% interest at harvest time (so they don’t need to rent from landlords)</li>
      <li>schemes of transporting money</li>
      <li>pumped more currency into the economy</li>
    </ul>

    <p>recevied initial success but failed in the end</p>
  </li>
  <li>
    <p>Sima Guang (not related to Sima Qian) is one guy opposing Wang’s policy: wanted incremental reform instead of all processes done at once = too fast</p>

    <ul>
      <li>Wang Anshi almost like Legalist, as he is <strong>making state intervening deeply to people</strong>, hence denounced as un-Confucian</li>
      <li>Wang Anshi therefore needed more support in court = reformed civil exam system to teach his thoughts</li>
      <li>further exacerbated the problem</li>
    </ul>
  </li>
</ul>

<p><strong>Organization in Song</strong>: Idealized urban life</p>

<ul>
  <li>
    <p>Qingming scroll, idealized urban life in Kaifeng (debatable), but anyway <strong>exemplifies Song’s rigor and commerce</strong></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Full Scroll</th>
          <th style="text-align: center">Center Piece</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013135132538.png" alt="image-20221013135132538" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013135141379.png" alt="image-20221013135141379" /></td>
        </tr>
      </tbody>
    </table>

    <p>notice the overall city structure is very different than Tang: symmetrical + structure, but Song = more organic. This hints at the difference between Tang and Song: Song is moving away from society full of aristocracy to <strong>more middle class based</strong> society, hence different city layout, etc.</p>
  </li>
  <li>
    <p>transportation in Song: important people on horse backs and in caravan</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013140017927.png" alt="image-20221013140017927" style="zoom:50%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013140025275.png" alt="image-20221013140025275" style="zoom:50%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
  <li>
    <p>but notice the lack of female figures in the scroll: if this were Tang’s time, women on streets with beautiful dresses, <mark>why such changes</mark>?</p>

    <ul>
      <li>recall that Empress Wu tried to improve women’s status, and during Emperor Xuanzong women can come out homes and wear men clothes</li>
      <li>but in Song, there is
        <ul>
          <li>commercialization=market economy, so that with coinage + paper money $\to$ women <strong>became commodified again</strong> (e.g. prostitutes, sold daughters, etc)</li>
          <li><strong>foot binding in Song</strong>, women having small feet = standard of beauty = object of sexual desire, because this means it feet becomes a <mark>private part of body</mark> only seen by herself and husband
            <ul>
              <li>correlates with the growth of Song’s commercialization, and that families sell daughters + people kidnap women</li>
              <li>women transformed into commodities who were bought and sold on the basis of appearance</li>
              <li>women of good families compete with women with bound feet, hence 卷起来了</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>however, in general women do exert greater autonomy at home by handling the household estate as men die earlier</li>
    </ul>
  </li>
  <li>
    <p>some unrealistic aspects of the scroll is</p>

    <ul>
      <li>notice that there is no trash on street</li>
      <li>no fire stations, weird because lots of houses should be made of wood</li>
      <li>another modern visualization from some text include</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013140223138.png" alt="image-20221013140223138" style="zoom:50%;" /></p>

    <p>notice a fire station and soldiers repairing canal</p>
  </li>
</ul>

<p><strong>Capital at Hangzhou</strong>: south already contained a lot of population but then with capital shifted to Hangzhou in late Song, living in south becomes popular</p>

<ul>
  <li>recall that in Late Song, Kaifeng is sacked, so people moved to Hanzhou</li>
  <li>since Late Tang people are migrating, but recall that
    <ul>
      <li>south was hard to irrigate = solved by building canals</li>
      <li>north much exposed to steppe culture, and to the northern enemies, hence much more chaos = less preferred eventually</li>
    </ul>
  </li>
</ul>

<p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013140442737.png" alt="image-20221013140442737" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013140431572.png" alt="image-20221013140431572" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

<p><strong>Urbanization in Song</strong></p>

<ul>
  <li>
    <p>as advancement in agriculture = food surplus = people can have other occupations</p>
  </li>
  <li>movement of both people and good into cities
    <ul>
      <li>both Kaifeng and Hangzhou were linked by the Grand Canal (built in Sui)</li>
      <li>both Kaifeng and Hangzhou had population reaching at least 1 million</li>
    </ul>
  </li>
  <li>
    <p>in mid Song, 5% population already urban</p>
  </li>
  <li>
    <p>however, this raid commercialization can have <mark>adverse effect on government</mark></p>

    <ul>
      <li>technology advancement so fast that government cannot keep up with it</li>
      <li>e.g. in Tang, a small elite group in government is easy to control. But now in Song there is an emerging elite class/middle class with new found wealth = push <strong>against the centralized control Tang had</strong></li>
      <li>this diffusion of “resources” also relates to the <strong>“laicization” of Song religion</strong>. Practicing religion becomes shifted away from institution but more towards individual activities any lay people can do, e.g. building their own shrines
        <ul>
          <li>very similar diffusion of governmental control, so it now the religious institutions</li>
        </ul>
      </li>
    </ul>

    <p>therefore, the general theme of rising middle class and a <mark>"anti-centralization" of resources as compared to Tang is also a distinct factor in Song</mark></p>
  </li>
</ul>

<p><strong>New Elite: Scholar-Officials</strong> (<em>shi</em>士, or <em>shidafu</em>士大夫)</p>

<ul>
  <li>
    <p>recall that <em>shi</em> was in Zhou as mlitary people</p>

    <ul>
      <li>In the first half of the Chunqiu period, the feudal system was a stratified society, divided into ranks as follows: the ruler of a state; the feudal lords who served at the ruler’s court as ministers; the <strong>shi (roughly translated as “gentlemen”)</strong> who served at the households of the feudal lords as <strong>stewards, sheriffs, or simply warriors</strong>; and, finally, the commoners and slaves.</li>
      <li>In Warring States period, they become more specialized forces (e.g. in defense) that includes people of humble origin (climbed up by education)</li>
      <li><em>shi</em> in Han = Confucian scholars, but <em>shi</em> still dominated by aristocrats since Han officials are appointed by local recommendations. However, there is a shift towards <strong>more literal skills</strong>/non-military such as poetry.</li>
    </ul>
  </li>
  <li>
    <p>in Song, <em>shi</em> are <strong>scholar-officials certified through the civil service exams</strong></p>

    <ul>
      <li>this new civil service exam <mark>aims to guarantee that elite families cannot simply occupy government</mark></li>
      <li>success in exam = prestige in family. If passed the highest level become <em>jinshi</em></li>
      <li>without passing exam, you will just be officials of low rank</li>
      <li>therefore became very competitive, 400,000 people taking but government post is very limited</li>
      <li>firs time in China history that <strong>ended large dominance of northerners in the officialdom</strong> (e.g. since Sui and Tang, a lot of prestigious family members have mixture of northern roots and are in the government)
        <ul>
          <li>recall that Sui and Tang has a lot officials still from north + exam system are initial and social status are very fixed</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>the exam curricululm includes (ultimiate aim is for governing)</p>

    <ul>
      <li>elementary education: Analects, Classics of Filia Devotion</li>
      <li>ancillary exams, such as math and legal codes</li>
      <li><strong>core content: classics and history</strong>
        <ul>
          <li>write eesays on policy (chief examiners can take people who side with their positions)</li>
        </ul>
      </li>
      <li>poetry composition
        <ul>
          <li>ability to allude to historical events and personages</li>
          <li>easily gradable as it is standarized/ryhmed, etc.</li>
        </ul>
      </li>
      <li>hence a candidate needs to memorize a lot of classics: (word count)</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013141330999.png" alt="image-20221013141330999" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>recall that Tang candidates know their examiners, but <mark>Song insist on the anonymity of candidates</mark></p>

    <ul>
      <li>candidates writing will be re-copied by clerks to remove hand-writings styles</li>
      <li>no consideration of personality of candidate, e.g. as in Tang, and hence a very restricted version of merit</li>
    </ul>
  </li>
  <li>
    <p>more state and local academies are built in Song, and increased literacy made this exam popular and hence work</p>
  </li>
  <li>
    <p>the <mark>scholar-official (*shi*) class came to be defined by classical education and scholarly activities</mark></p>
  </li>
  <li>
    <p>supported also a <strong>thriving market for books, art, and connoisseur cultures</strong>. Therefore the invention of movable printing enable this remarkable demand</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221013141800645.png" alt="image-20221013141800645" style="zoom:33%;" /></p>
  </li>
</ul>

<h2 id="cultural-and-intellectual-changes-in-the-song">Cultural and Intellectual Changes in the Song</h2>

<p><strong>Previously on the Song</strong></p>

<ul>
  <li>
    <p>The Tang-Song transition sees a major shift of China’s population centers and economic activity southward. Population doubles, rapid <strong>urbanization</strong>, more trade (emphasis on the maritime)</p>
  </li>
  <li>
    <p>Social change ushers the Song <strong>commercial revolution</strong>. Trade facilitated by <strong>paper currency</strong> (technology of printing) and technological innovations in agriculture, water transport, and metallurgy.</p>

    <ul>
      <li>
        <p>key innovations: paper money and woodblock printing</p>
      </li>
      <li>
        <p>increased agriculture output = farmers produce more than they need = can specialize and trade</p>
      </li>
    </ul>
  </li>
  <li>Wang Anshi’s <strong>new policies</strong>: set of interventionist political-economic reforms designed to improve rural welfare, solidify tax base, etc. But meets Confucian opposition because implementations are legalists
    <ul>
      <li>Qingming scroll aims to depict idealized life at Kaifeng, but we know that benefits did not apply to everyone.</li>
    </ul>
  </li>
  <li><strong>Civil service exams</strong> create a new bureaucracy and a new scholar-elite <em>shi</em> class as compared to before</li>
</ul>

<p><strong>Crisis on the frontier: Tanguts, Khitans, Jurchens</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018132234600.png" alt="image-20221018132234600" style="zoom: 50%;" /></p>

<ul>
  <li>the policies from Wang was also to cover the spending to military
    <ul>
      <li>recall that Tuoba clan of the Xianbei during Six dynasties had a large army</li>
      <li>Tang recruited soldiers to push out China’s border, but in Song times, progressively more northern land is conquered. Specifically the Khitan, Liao, and Jurjan Jin, and Tanguts (Xi Xia).</li>
      <li>Eventually <mark>Yuan</mark> ruled over: Yuan was a <strong>Mongol-led</strong> imperial dynasty of China</li>
    </ul>
  </li>
</ul>

<p><strong>Tanguts and China</strong></p>

<ul>
  <li>
    <p>Tanguts (people of Tibetan origin) after Tibetan fell apart, took Hexi corridor which is critical for the path to Central Asia</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Tanguts v.s. China</th>
          <th style="text-align: center">Horse Farm, Hexi Corridor</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018132606077.png" alt="image-20221018132606077" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018153346036.png" alt="image-20221018153346036" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>however</p>

    <ul>
      <li>has no official history because Mongol attempted to genocide the Tanguts in 1227, hence most of their text and architectures are gone</li>
      <li>Tanguts had a lot of heavy calvary</li>
    </ul>
  </li>
  <li>
    <p>Xi Xia (Tanguts) people look lie</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Costumes</th>
          <th style="text-align: center">Scripts</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018133021480.png" alt="image-20221018133021480" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018133140857.png" alt="image-20221018133140857" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>notice that</p>

    <ul>
      <li>Xi Xia aristocrat looks very alike China, because they adopted much Chinese policies and had <strong>Sinicization</strong></li>
      <li>but later their Emperor mandate their specific form of hair style to keep their culture</li>
      <li>
        <p>their scripts are also modelled from Chinese, but much more complicated</p>
      </li>
      <li>
        <p>Tanguts people made beautiful Buddhist artifacts in the Mongol caves as well</p>

        <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018133404190.png" alt="image-20221018133404190" style="zoom: 25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018153627755.png" alt="image-20221018153627755" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Tangut as Xi Xia</strong> (1038 - 1227)</p>

<ul>
  <li>
    <p>In 881 they helped Tang to suppress Huang Chao rebellion</p>
  </li>
  <li>
    <p>religion fusion: Chinese + Tibetan Buddhism</p>
  </li>
  <li>
    <p>in early Song, lots of trade between Tanguts and Song, and many silk road goods went through them. But their relations with Song is volatile: in later Song they asked for equal status with Song Emperor:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018133734437.png" alt="image-20221018133734437" style="zoom:50%;" /></p>

    <p>notice that</p>

    <ul>
      <li>before 1038, Tangut was polite and referred to itself being a vassal state. But after 1038, no justification is needed</li>
      <li>before, mentioned much culture/music as justification = imply that <strong>Confucianism is a standard for civilization</strong> = acceptance of Chinese’ idea of civilization</li>
    </ul>
  </li>
</ul>

<p><strong>Khitans (Liao)</strong>, 907 - 1125</p>

<ul>
  <li>Abaoji established the <mark>tribal federation</mark> = connecting many tribes together = the Liao Dynasty</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018153958544.png" alt="image-20221018153958544" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>Khitans are not friendly to Tanguts either, but they posed more <strong>threat to Song</strong></p>
  </li>
  <li>
    <p>they conquered and occupied the Sixteen Prefectures, <mark>which also included Beijing</mark>, hence a lot of sentiment in Song</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018134323478.png" alt="image-20221018134323478" style="zoom:33%;" /></p>

    <ul>
      <li>many Song emperor tried to recover this land from Khitans, but failed</li>
      <li>this strip had strategic North- South chokepoints between central plains and northern steppes, Manchuria.</li>
      <li>was not re-captured until Yuan</li>
    </ul>
  </li>
  <li>
    <p>Song was defeated by Khitan, hence signed a <mark>Peace Treaty of Chanyuan</mark></p>

    <ul>
      <li>clear boundaries, sixteen prefecture that Song wanted is kept by Liao</li>
      <li>Song paid indemnity for peace, which is a <strong>reasonably cheap price</strong> hence gives little incentive for them to fight back</li>
      <li>Song was also forced to recognize Liao as peers (etc. Khitan would address Song emperor as elder brother)
        <ul>
          <li>this is rejected by Chinese people = now you have <strong>two sons of Heaven</strong></li>
          <li>sense of humiliation, major source of <mark>strong sentiment in Song</mark></li>
        </ul>
      </li>
      <li>but at least this avoided wars between them. Maintained until 1125 until Song invited Jurchens to attack Liao</li>
    </ul>
  </li>
  <li>
    <p>government in North is a mobile civilization = nomadic culure. In the southern = 16 prefecture, so <strong>people kept the Chinese style</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135019332.png" alt="image-20221018135019332" style="zoom:33%;" /></p>

    <p>so essentially Chinese lived under Khitan rules</p>
  </li>
  <li>
    <p>Khitan also, used by Jurchens who conquered Khitan and later adopted their own scripts based on Chinese</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135138990.png" alt="image-20221018135138990" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Jurchen Jin</strong></p>

<ul>
  <li>
    <p>further east of the Khitans, being half nomad utilizing both hunting and farming</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135207765.png" alt="image-20221018135207765" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Originally they were subjects of Khitan, but later Aguda formed a <mark>confederation of tribes</mark> and rebelled = Jurchan</p>

    <ul>
      <li>nomad usually do not pose threats until formed confederation. But this also means that in them people are <strong>not from the same tribe</strong></li>
      <li>they named themselves <mark>Jin金</mark> dynasty</li>
    </ul>
  </li>
  <li>
    <p>Jurchen was also the reason Song became Southern Song in later periods. Since every Song wanted to <strong>retake 16 prefectures</strong> from Khitan but non worked, Emperor Huizong decided to be <strong>allied with Jurchan</strong>. In 1123, Juchen defeated Khitan, but in 1127, they turned on their Song ally, and <mark>Kaifeng is lost</mark>. After that, Huizong abdicated and Song had to <mark>move south to Hangzhou</mark>.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center"> </th>
          <th style="text-align: center"> </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135528141.png" alt="image-20221018135528141" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135611298.png" alt="image-20221018135611298" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>is a great painter (e.g. <strong>three perfections, painting, poetry and calligraphy</strong>)</li>
      <li>Emperor Huizong wished to retake the “Chinese” territory of the Sixteen Prefectures (area including Beijing) from the Khitan Liao, but is responsible for losing the North and hence criticized by later historians for his <strong>indulgence in aesthetic pursuits</strong></li>
    </ul>
  </li>
  <li>
    <p>hence, Jurchan moved their capital to Kaifeng. But unlike Khitan who kept a duo system, <mark>Jurchan went Sinicization</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135811632.png" alt="image-20221018135811632" style="zoom:50%;" /></p>

    <p>so a lot Jurchan spoke Chinese, dress Chinese, like <mark>Northern Wei</mark></p>
  </li>
  <li>
    <p>in total, Song at this point had to sign treaties including:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018135943098.png" alt="image-20221018135943098" style="zoom:33%;" /></p>

    <p>but was actually not a big burden, as they did not exceed 2% of state avenue, because Song’s modernization and is rich. However, most went to pay for military, which took up $3/4$ of the revenue for just maintaining the army.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018140053433.png" alt="image-20221018140053433" style="zoom: 33%;" /></p>
  </li>
</ul>

<p><strong>Essentials of Military Classics (<em>Wujing Zongyao</em> 武經總要)</strong></p>

<ul>
  <li>
    <p>commissioned by Song emperor and had instructions for a broad range of weapons</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Siege Ladders</th>
          <th style="text-align: center">Triple Crossbow</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018140220582.png" alt="image-20221018140220582" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018140230536.png" alt="image-20221018140230536" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>even though Song had advancements in military and spent a lot on its army, it did not conquer their lost territories back:</p>

    <ul>
      <li>Jurchan are sinicized anyway, and payment for peace was cheap</li>
      <li>however, you still need to keep a strong army in case of future infringement</li>
    </ul>
  </li>
  <li>
    <p>this book also recorded the invention/production of <strong>gunpowder</strong>, which was found from Daoist alchemy accidentally</p>
  </li>
  <li>
    <p>but even with those, it didn’t give much advantages because artisans are captured by enemies. e.g. those siege machines are also employed by Jurchen</p>
  </li>
</ul>

<p><strong>Neo-Confucianism</strong>: in response to Chinese cultural identity crisis at borders</p>

<ul>
  <li>“It was perverse for Chinese to forget their ancestors and abandon sacrifices to them, serving instead barbarian ghosts.” —Shi Jie ⽯介</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018140623628.png" alt="image-20221018140623628" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>Buddhism was already in 1000 years, but now they start to <strong>reject the religion as it is not indigenous and practiced by their enemies</strong></p>

    <ul>
      <li>
        <p>recall in Tang times Buddhism was wide spread. But in Song, due to changes of international politics, such as the Khitan Liao and Tanguts are buddhists, Song scholars thought of <mark>Buddhism as religion of enemy</mark></p>
      </li>
      <li>
        <p>story about Cai Wenji who lived in Han but abducted by Xiongnu, to show the barbarian culture of the northern people as compared ot Chinese architectures</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018140826760.png" alt="image-20221018140826760" style="zoom:33%;" /></p>

        <p>however, notice that here they are dressing in Khitan = used as a symbol of foreignness = barbarianism of their northern enemies</p>
      </li>
    </ul>
  </li>
  <li>
    <p>late Song had a lots of <strong>problem</strong>: big armies taking much expenditures, officials not selected properly, and taxes not collected properly $\to$ scholars believe that everything <mark>needs reform</mark></p>

    <ul>
      <li>
        <p>but unlike Wang Anshi’s new policy, Song scholars aims to <strong>revitalize in Confucianism $\to$ Neo-Confucianism</strong></p>

        <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018141134032.png" alt="image-20221018141134032" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018141216014.png" alt="image-20221018141216014" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

        <ul>
          <li>
            <p>even during Age of Division and Tang when there are a lot of Buddhist, Confucianism technically is still used as texts for exams</p>
          </li>
          <li>
            <p>but those scholars dismissed them and said Confucianism is lost since after Zhou, that it was all about Buddhism and Taoism. Hence they considered themselves as revivalists = <strong>reinterpret the traditional Confucianism core = becomes a dominant idea among Song intellectual</strong></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Zhu Xi</strong> (as a representative and consolidator) for Neo-Confucianism</p>

    <ul>
      <li>
        <p>Buddhism had life cycles, and hence Confucian also had Metaphysics about <em>li</em> 理 (principle) and <em>qi</em> (vital energy)</p>

        <ul>
          <li>believes that nothing can exist without <em>li</em></li>
        </ul>
      </li>
      <li>
        <p>used the two concepts to explain human’s good nature. Humans born with good <em>li</em> but <em>qi</em> is impure = hence people do bad things</p>

        <ul>
          <li>sages had perfect <em>qi</em>, so people needs to work on this</li>
        </ul>
      </li>
      <li>
        <p>Zhu Xi’s take is that <em>li</em> is more important $\to$ how do you <strong>investigate things</strong></p>

        <ul>
          <li>rigorous study to probe <em>li</em> in things</li>
          <li>your methods of study should include nurture reverence + pursue knowledge</li>
        </ul>
      </li>
      <li>
        <p>the ultimate goal is <strong>self-cultivation</strong>, which is believed to lead to harmonious society (if everyone does it). But how to cultivate it?</p>

        <ul>
          <li>Zhu Xi’s idea is to investigate things, hence his school = <mark>*Li Xue* = a mainstream Neo-Classical school</mark></li>
        </ul>
      </li>
      <li>
        <p>since Zhu Xi believed one needs to study, he helped establish private academies = rise of private schooling for transmitting new ideas</p>

        <ul>
          <li>also wrote <em>Commentaries on Analects</em>, which is later made an official text in 1241 until 1905</li>
        </ul>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221018142312659.png" alt="image-20221018142312659" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>since Zhu Xi wanted to revive, he called himself <em>Daoxue</em> or <em>Lixue</em>, they don’t called themselves Neo-Confucianism</p>
      </li>
    </ul>
  </li>
  <li>
    <p>even though those new thoughts is a response of the time of crisis to attack Buddhism or Taoism, but they are <strong>not immune to their influence</strong>,</p>

    <ul>
      <li>e.g. still use Buddhist practices such as medication and adopted Buddhist terms in Neo-Confucianism</li>
      <li>i.e. has a lot of elements drawn from them</li>
    </ul>
  </li>
</ul>

<p><strong>Song Women’s Status</strong></p>

<ul>
  <li>pretty complicated
    <ul>
      <li>commodification of women, selling daughters gives farmers a ton of money, to be prostitutes</li>
      <li>foot-binding, which started in the sing-song class, but eventually expanded throughout = small foot had sexual allure for men</li>
      <li>but women can also use that to their advantages, and some had autonomy, such as in the reading that some women had words that judges listen to</li>
    </ul>
  </li>
  <li>foot binding in Song. women having small feet-only seen by herself and husband (264)
    <ul>
      <li>correlates with the growth of Song’s commercialization, and that families sell daughters + people kidnap women</li>
      <li>women transformed into commodities who were bought and sold on the basis of appearance</li>
      <li>women of good families compete with women with bound feet, hence 卷起来了</li>
    </ul>
  </li>
</ul>

<h1 id="midterm">Midterm</h1>

<ul>
  <li>three main parts</li>
  <li>Identifications of people, places, key concepts
    <ul>
      <li>e.g. equal field system</li>
      <li>a list will be given out, also explain the historical importance of this key term</li>
    </ul>
  </li>
  <li>short answer questions
    <ul>
      <li>explain the deeper meaning/significance of a passage</li>
      <li>e.g. a paragraph long</li>
    </ul>
  </li>
  <li>one long essay
    <ul>
      <li>e.g. half of the test</li>
      <li>e.g. interaction with steppe normadic culture, e.g. explain <em>shi</em> class in warring states and implications in later dynasties</li>
      <li>e.g. bidirectional influences between stuff</li>
      <li>like three to fourth paragraphs, and you need to provide a structured, argument-based response</li>
    </ul>
  </li>
</ul>

<h1 id="yuan-dynasty-1271---1368">Yuan Dynasty (1271 - 1368)</h1>

<blockquote>
  <p>Yuan was a <strong>Mongol-led imperial dynasty of China</strong> and a successor state to the Mongol Empire after its division. It was established by <mark>Kublai</mark> (Emperor Shizu), leader of the Borjigin clan, and lasted from 1271 to 1368.</p>

  <ul>
    <li>Although <mark>Genghis Khan</mark> had been enthroned with the Han-style title of Emperor in 1206 and the Mongol Empire had ruled territories including modern-day northern China for decades, it was not until 1271 that <mark>Kublai Khan</mark> officially proclaimed the dynasty in the traditional Han style
      <ul>
        <li>Genghis Khan = Chinggis Khan later in text</li>
      </ul>
    </li>
    <li>Kublai’s conquest was not complete until <strong>1279 when the Southern Song dynasty</strong> was defeated in the Battle of Yamen. His realm was, by this point, isolated from the other Mongol-led khanates and controlled most of modern-day China and its surrounding areas, including modern-day Mongolia.</li>
    <li>Mongol rule was <mark>cosmopolitan under Kublai Khan</mark>. He welcomed foreign visitors to his court, such as the Venetian merchant Marco Polo, who wrote the most influential European account of Yuan China</li>
    <li>In 1368, following the <strong>defeat of the Yuan forces by the Ming dynasty</strong>, the Genghisid rulers retreated to the Mongolian Plateau and continued to rule until 1635 when they surrendered to the Later Jin dynasty (which later evolved into the Qing dynasty).</li>
  </ul>
</blockquote>

<p>How did the <mark>Mongols</mark> managed to conquer all China during <mark>Yuan</mark>?</p>

<ul>
  <li>they didn’t have literati, didn’t have language, didn’t have permanent houses</li>
</ul>

<p><strong>Raising Warriors</strong>: especially women warriors</p>

<ul>
  <li>can reveal Mongols political, cultural and social priorities</li>
  <li>Eurasian steppe: resource scare, nature is volatile, human is small compared to nature
    <ul>
      <li>generating only a minimal surplus, and internal disputes arises due to resource scarcity</li>
    </ul>
  </li>
  <li>therefore perpetual uncertainty, <strong>hard on both women and man, little time on inequality</strong>
    <ul>
      <li>nomad = <mark>group is prized more</mark> important than individual survival = young children were taught a universal skill set</li>
      <li>has highly non-gendered training, e.g. horsemanship, women and men equal competence</li>
      <li>herd and hunt typically taught by mothers, e.g. Chinggis Khan’s mother</li>
    </ul>
  </li>
  <li>learned bravery during hunt = <strong>bravery in battle</strong>
    <ul>
      <li>horsemanship, bravery, shooting skills for all</li>
      <li>e.g. Alaqa Beki, a <strong>women leader of large army</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Groups Win Wars</strong>: prizing group advantages</p>

<ul>
  <li>
    <p>military tactics through surprise, ambush. Hence also requires soldiers strong in acrobatic skills</p>
  </li>
  <li>
    <p><strong>caracole</strong> tactic:</p>

    <p><img src="https://qph.cf2.quoracdn.net/main-qimg-9a7d2ad9f1f7cd2e072ed4f3f53e32c4-lq" style="zoom:33%;" /></p>

    <ul>
      <li>horses are faster in group</li>
      <li>shoot at 40-50m = general kill zone, so every warrior can contribute to the arrows</li>
    </ul>
  </li>
  <li>
    <p><strong>feigned</strong> retreat</p>

    <p><img src="https://rud.is/khan/uploads/4/6/9/3/46935069/876421879_orig.jpg" style="zoom:33%;" /></p>

    <ul>
      <li>appear to be overwhelmed and flee, to <strong>lure</strong> enemies to ambush location</li>
    </ul>
  </li>
</ul>

<p><strong>Climatic Changes</strong></p>

<ul>
  <li>15 year <strong>Pluvials (increased rainfall=wet) climatic</strong> favored Mongol
    <ul>
      <li>to being able to support a <strong>record-holding amount of animals</strong> (e.g. horses) due to biomass flourishing. So many perhaps more than the rest of the horse in the world combined</li>
      <li>zero foot-soldiers, or supply chain, but each horseman have <strong>4-5 well-trained horses</strong></li>
      <li>those horses would respond to the unique whistle of the owner</li>
    </ul>
  </li>
  <li>soldiers can sleep on their horses
    <ul>
      <li>place their meat underneath saddle</li>
      <li>can also train blood from horses as calorie intake if needed</li>
    </ul>
  </li>
</ul>

<p><strong>Semi-private/public Ger</strong></p>

<p><img src="https://www.youngpioneertours.com/wp-content/uploads/2020/01/Ger-construction.jpg" style="zoom:33%;" /></p>

<ul>
  <li>Ger is a single room structure (tent)
    <ul>
      <li>often entire family lives in a single Ger. This means sometimes many people all into 100 sqrft space, for both women, men, and kids</li>
      <li>hence also little <em>room</em> for inequality</li>
    </ul>
  </li>
  <li>Ger is Mongol’s only structure shielding from nature
    <ul>
      <li>place for everything, all resources</li>
      <li>technically <strong>owned by the female</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Buying Loyalty</strong></p>

<ul>
  <li>Chinggis dissolved conquered population to create a structure for him as head of power
    <ul>
      <li>then perform military integration = some of them become part of the army</li>
      <li>each person loyal to <strong>their direct superior</strong>, and is recursive until the top, which is Chinggis</li>
    </ul>
  </li>
  <li>so ultimately each gifted population (along with their stuff) is still owned by Chinggis
    <ul>
      <li>those owners then need to populate the armies</li>
    </ul>
  </li>
  <li>so source of income is plunder or tribute</li>
</ul>

<p><strong>Women in Mongols’ Political Sphere</strong></p>

<ul>
  <li>chiggis wealth transfer to the youngest child</li>
  <li>not only were women are entitled to power, men also believed in their power
    <ul>
      <li>women power has nothing to do with proximal relationship with men, as in early China (e.g. Zhou dynasty)</li>
    </ul>
  </li>
  <li>women in general (including Chinese) enjoyed a more fairer status:
    <ul>
      <li>mongol named a women <strong>Yang Miaozhen</strong> (successful and fearful leader of army) to the position of provincial governor - probably the highest power ever to be held by a civilian women in imperial China</li>
      <li>women has power in conducting family affairs (334-336). e.g. Zhao’s wife, <strong>Guan Daosheng</strong>, has a commanding role in household management</li>
    </ul>
  </li>
</ul>

<p><strong>Extreme Violence Saves Lives</strong></p>

<ul>
  <li>e.g. Persia killed Mongol’s envoys, so Mongol went on to decimate them</li>
  <li>believed violence during warfare is a necessity, so they wanted to <strong>spread their image of violence</strong></li>
  <li>so a town under their attack can
    <ul>
      <li>surrender and pay tribute (survive)</li>
      <li>not useful become slaughtered (decimated)</li>
      <li>the useful, such as artisans were saved to improve techniques. Or a few terrified were allowed to flee and spread this terror (survive)</li>
    </ul>
  </li>
  <li>mongol invasion in south is slow
    <ul>
      <li>instead of plunder/kill farmers, tax them was proposed</li>
    </ul>
  </li>
</ul>

<p><strong>Mongol Ruling China</strong></p>

<ul>
  <li>
    <p>The Yuan dynasty created a <mark>"Han Army"</mark> (漢軍) out of defected Jin troops and an army of <strong>defected Song troops</strong> called the “<mark>Newly Submitted Army</mark>” (新附軍).</p>
  </li>
  <li>as many women (still physically strong) start to shift more to political poewr
    <ul>
      <li>initially 30% are Mongol women in army, but now forces are not even many Mongols</li>
    </ul>
  </li>
  <li>Kublai wanted to also focus on governing in China
    <ul>
      <li>eventually adopting alternating policies favoring their own steppe culture and sedentary culture of the vast population of Chinese</li>
    </ul>
  </li>
  <li>no evidence that Kublai converted to Buddhism, nor Confucianism
    <ul>
      <li>only a rudimentary knowledge of Chinese, hence certainly not Confucianism</li>
      <li>sensitive not to rely on Chinese, but military ones in western/central asia</li>
      <li><strong>suspended the examination system</strong> = no group can challenge Mongols</li>
      <li>only later <mark>emperor Ayurbarwada</mark> <strong>restored</strong> the civil exam system due to his tutor teaching Confucius and he liked it</li>
    </ul>
  </li>
  <li>Kublai considered <strong>South Chinese</strong> the least trust worthy group, hence at the bottom of four divisions
    <ul>
      <li>Specifically, the <mark>four classes</mark> of people by the descending order were Mongolian people, Semu people, Han people (in the northern areas of China) and Southern Chinese (people of the former Northern Song Dynasty)</li>
      <li>perhaps because Southern Chinese are the latest to be conquered</li>
      <li>prioritize economic growth, some</li>
    </ul>
  </li>
  <li>Kublai nevertheless <strong>sought to keep good relation to both Chinese and Mongols</strong>
    <ul>
      <li>as ruler for Mongol sought to preserve martial values, and having mongol women as concubines</li>
      <li>as ruler for Chinese, established capital in China and subsidized Chinese art</li>
      <li>in reality, even only for 100 years it is still <strong>hard</strong> to both rule sedentary China and steppe Mongol.</li>
    </ul>
  </li>
</ul>

<p><strong>Fall of Yuan</strong></p>

<ul>
  <li>However, eventually depopulation in China and increasing stress on people caused <strong>rebellion</strong>
    <ul>
      <li>Kublai launched huge campaign using sedentary Chinese are army = many <strong>died</strong></li>
      <li>increasing amount of burden + natural disasters (drought) + ineffective government policies inflicted <strong>struggle, famine in people</strong></li>
    </ul>
  </li>
</ul>

<h1 id="ming-dynasty-1368---1644">Ming Dynasty (1368 - 1644)</h1>

<p><strong>Previously on the Mongols and the Yuan</strong> (from Chinese perspective)</p>

<ul>
  <li>Tribal confederations became a major threat
    <ul>
      <li>Throughout the period of the Southern Song (1127-1279) Mongol confederation under Genghis Khan gaining strength in the north.</li>
      <li>Mongols being effective in warfare, e.g. brutality</li>
      <li>embrace new war technologies from captured artisans</li>
    </ul>
  </li>
  <li>migration possibly encouraged by climate changes
    <ul>
      <li>high productivity of horses, on average each mongol solider has 3-5 horses</li>
      <li>Eurasian steppe nature, hence a small population = lose control when large territory</li>
      <li>i.e. problem of <strong>minority rule</strong>: assimilation v.s. their elite identity also needed to be balanced</li>
    </ul>
  </li>
  <li>Mongol’s nomadic culture + small population = both women and men trained equally
    <ul>
      <li>no need a supply chain, just plunder and adopt technology</li>
      <li>but also had succession crises</li>
    </ul>
  </li>
  <li>Kublai Khan defeats Southern Song and established Yuan dynasty
    <ul>
      <li>first non-Chinese who rule the entire China both north and south</li>
      <li><strong>divided people into 4 ethnic group and favored non-Chinese</strong> = arise Chinese ethnic consciousness and desire to regain China = eventually lead to Chinese rebellions = <mark>downfall of Yuan</mark>
        <ul>
          <li>recall that they were military strong in horseback but as the creation Han Army and Newly Submitted Army made many mongols become sedentary + a much smaller population, it becomes hard for them to hold the vast population of Chinese</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Yuan was overall political unstable but flourishing time for culture
    <ul>
      <li>e.g. Yuan drama, literature, blue and white porcelain</li>
    </ul>
  </li>
</ul>

<h2 id="ming-autocracy">Ming Autocracy</h2>

<p><strong>The strangeness of Early Ming</strong>: despotism and an agrarian vision of economy and society</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027132235822.png" alt="image-20221027132235822" style="zoom: 33%;" /></p>

<ul>
  <li>
    <p>smaller than Yuan as Mongolia is not in Ming’s control, and captial at <mark>Nanjing</mark> (capital during Six dynasties)</p>

    <ul>
      <li>rule both north and south from a southern city for the first time</li>
    </ul>
  </li>
  <li>
    <p>Founder <mark>Zhu Yuanzhang</mark>朱元璋, with reign title <mark>Hongwu</mark> (1368-1398)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027132453072.png" alt="image-20221027132453072" style="zoom: 33%;" /></p>

    <ul>
      <li>Hongwu reign year 1 to represent 1368</li>
      <li>reign title chosen by himself, because of his <strong>military achievement: Ming has expanded</strong> and included corridor area in north and sixteen prefectures (later Beijing become capital under YongLe)</li>
    </ul>
  </li>
  <li>
    <p>An extraordinary life experience of <strong>Zhu Yuanzhang</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027132703443.png" alt="image-20221027132703443" style="zoom:33%;" /></p>

    <ul>
      <li>parents were poor, and was a wondering monk for several years</li>
      <li>Mongols burnt down the monasteries, and he joined rebels</li>
      <li><strong>commoner to become an emperor since Liu Bang</strong></li>
    </ul>
  </li>
  <li>
    <p>part of his experience in his own words (explains why he shaped a <mark>conservative</mark> vision as government/<mark>weird early Ming</mark>)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027132909702.png" alt="image-20221027132909702" style="zoom:33%;" /></p>

    <p>he claimed to “restore S. Song style”, but <strong>kept many Yuan policies</strong> (conservatives)</p>

    <ul>
      <li>
        <p>emperor to bureaucracy below him is much more hierarchical/autocratic, and strong personal subordination to emperor</p>

        <ul>
          <li>did not hesitate to have officials beaten openly = very despotic</li>
          <li>became a regular mean to <strong>punish officials</strong>: rarely used in Tang, but <strong>Mongol did use it frequently</strong></li>
        </ul>
      </li>
      <li>
        <p>paranoid for treason: frequent execution of officials</p>

        <ul>
          <li><strong>great purges:</strong> zero toleration of corruption and disloyalty. Executed over 30,000 victims</li>
          <li><strong>didn’t trust prime ministers</strong>, made himself the chief director for large and small matters and <strong>removed “Great Chancellor” as a rank</strong></li>
        </ul>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027133324396.png" alt="image-20221027133324396" style="zoom: 33%;" /></p>

        <p>for example, the first and fourth is due to (he believes is) disloyalty, and the 2nd and 3rd for corruption.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>emperors are now complete autocrat. related to Mongol’s style,</p>

    <ul>
      <li>comparison of the past: officials were somewhat confident and powerful in decision making</li>
      <li>In Tang they sat in chairs, in Song not required to kneel (also vowed not to execute officials even bad news/disagreement). But in <strong>Ming, a lot more <mark>autocratic</mark></strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027133538707.png" alt="image-20221027133538707" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Despotism</strong>: new government structure</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027175108011.png" alt="image-20221027175108011" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>this despotism in emperor dominating over bureaucracy is highly related/<mark>originated</mark> by</p>

    <ul>
      <li>decline of hereditary aristocracy since Huang Chao rebellion = recruit based on merit/little family influence = earlier those <strong>aristocrat families are well venerated</strong> and hence easily <strong>exerts strong influence even in court</strong></li>
      <li>the rise of civil service exam and the above = aristocrat “class” are flexible, can <strong>hardly create equal influence</strong></li>
      <li>the only <strong>special case is during Song</strong>, when emperor explicitly said not to execute scholars if bad news/disagreements hence those recruited by merit base can have influence</li>
      <li>
        <p>his peasant background also made him have dislikes of anti-intellectuals</p>
      </li>
      <li>though Emperor Zhu Yuanzhang did not have civil system for a decade but eventually needed help = depended on them, so a <strong>power balance is subtle</strong></li>
    </ul>
  </li>
  <li>
    <p>since Tang to Yuan, government structure were somewhat similar. But in Ming</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Tang and Song</th>
          <th style="text-align: center">Ming</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027134122694.png" alt="image-20221027134122694" style="zoom: 50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027134246693.png" alt="image-20221027134246693" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>before, <strong>grand chancellor</strong> (developed in Song, e.g. Wang Anshi) have a high degree of influence in decision making, but now is removed</li>
      <li>strong centralization of authority in the throne
        <ul>
          <li>abolished Grand Chancellor = Prime Minister</li>
          <li>military bureau divided into 5, and censorate into 12. In total 23 ministries <strong>directly reported to Emperor</strong></li>
          <li><strong>grand secretariat</strong> = mid level officials = good at writings as helpers
            <ul>
              <li>after Zhu Yuanzhang, they gain power and even reject emperor’s decisions in the long run = basically functioning as prime minister</li>
            </ul>
          </li>
          <li>this induced a <strong>large workload</strong> for emperor, and seem only possible with Zhu Yuanzhang</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Social reorganization at the local level</strong></p>

<ul>
  <li>
    <p>Zhu Yuanzhang liked rural areas as being less corrupt and simple + close community of farmers</p>

    <ul>
      <li>very different in Song time’s commercialized farmers</li>
    </ul>
  </li>
  <li>
    <p>therefore, developed an <strong>ideal economical life being “rural”</strong></p>

    <ul>
      <li>aimed for rural self-sufficiency</li>
      <li>developed the <strong>Lijia 里甲 system</strong> (+ registration system for direct control of people, like Qin period)</li>
      <li>developed the “<strong>Sacred Edict of Six Maxims</strong>” (for spreading Confucius teachings)</li>
    </ul>
  </li>
  <li>
    <p><mark>Lijia (hundred-and-tithing): village registration system</mark></p>

    <ul>
      <li>assign people with land and get taxes as well. so that
        <ul>
          <li>10 households = 1 <em>jia</em>, and 10 <em>jia</em> = 1 <em>li</em>. Hence 100 households per <em>li</em></li>
          <li>additional 10 wealthy ones in charge of managing hence 110 households</li>
        </ul>
      </li>
      <li>the idea of <strong>self-supervision + direct report</strong> to emperor was raised by Shangyang in Qin</li>
      <li>registering households as done similarly by Mongol, which
        <ul>
          <li>origins as early as Qin (which was for recruiting <strong>military</strong>).</li>
          <li>In Song, Wang Anshi also employed a a similar policy for governmental interference in <strong>economy</strong></li>
          <li>but Zhu Yuanzhang wanted this more for <mark>control</mark> <strong>of people</strong> (the book yellow registers)</li>
        </ul>
      </li>
      <li>effectively transfer govern responsibility to local community. Emphasized community self-management including tax-collecting, transportation, judicial, police services, etc.
        <ul>
          <li>intent is a small government as peasants manage themselves = no need for high tax as well</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>he also uses Lijia to teach people about the laws and Confucianism</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027135538558.png" alt="image-20221027135538558" style="zoom:33%;" /></p>

    <ul>
      <li>“sacred edict” read aloud regularly, people urged to behave with <strong>Confucian values such as Filial Piety</strong></li>
      <li>very effective mean of imbuing people (including illiterate) with Confucius values</li>
    </ul>
  </li>
</ul>

<p><strong>Four Categories of People</strong></p>

<ul>
  <li>
    <p>Mongol had this division: 1) Mongol; 2) Semu = colored-eye people, from west/central Asia; 3) Northern Chinese; 4) Southern. Reveals how <em>much</em> Mongol confederation trusted each group</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027135758073.png" alt="image-20221027135758073" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>in Ming, he also <strong>categorized peopled base on occupation SPAM</strong></p>

    <ul>
      <li>but later this <mark>doesn't work for economy</mark>, and broke down with high commericialization later</li>
      <li>attempted to demarcate social boundaries, e.g. merchants, farmers, must do different things. But since Song, <strong>people are highly dynamic</strong>, as in commercialization, many specialization and trading/work across occupations. So that was a failure of his policies</li>
    </ul>
  </li>
  <li>
    <p>recall that exams during Yuan was hard, so a lot of scholars changed direction and went to medicine</p>

    <ul>
      <li>of course, getting a degree is still valuable, but as more and more people receive education and exam even more intense, a lot of <strong>novel occupations/diversified future of scholars</strong></li>
      <li>but still, Southern literati elites regardless held immense influence in local society = <strong>localist turn</strong> (started in Southern Song). e.g. story of cruel family getting hold of the entire <em>local</em> community
        <ul>
          <li>before, passing the exam = route for wealth, status and fame. But now they realize this is not the only route and can <strong>exert influence on a local level</strong>. (which is very related to neo-Confucianism - build society from local level) = establishment of <mark>local roots</mark></li>
          <li>for them, civil exam is now just among the many exams, but just sitting for the exam still gives status.</li>
          <li>so south weathered the Mongol invasion much better than the north thanks to their established local roots = as they anyway didn’t rely on the civil exam to get wealth = your life stays hereditary</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>again, even as merchants at the bottom category, they are often the wealthiest, e.g. Shen Wansan (a merchant)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027140231319.png" alt="image-20221027140231319" style="zoom:33%;" /></p>

    <p>since education requires money, they are at an advantage: merchant can infiltrate by marriage into gentry (shidafu) family, or just adopt their sons</p>
  </li>
</ul>

<p><strong>Zhu Yuanzhang’s successors</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027140532108.png" alt="image-20221027140532108" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>succession crisis after death. The correct line of succession should be Zhu Yunwen, but Zhu Di = <strong>his uncle got it</strong> (hence seen as “illegitimate”)</p>
  </li>
  <li>
    <p><mark>Zhu Di</mark> gives himself the reign title <mark>Yongle</mark>, and did several important things</p>
  </li>
  <li>
    <p>one major change is Nanjing being demoted to secondary captial and <mark>Beijing becomes capital</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027140744214.png" alt="image-20221027140744214" style="zoom:33%;" /></p>

    <ul>
      <li>he felt insecure in the south (as he is an illegitimate ruler), and his generals are all from northern areas</li>
      <li>demographically it is <strong>strategically important to protect central plain from north but not safe</strong></li>
      <li>it is also not very developed yet, so resources needed to shift from Nanjing/the south</li>
      <li>but Beijing became the capital for the rest of the 7 centuries until today</li>
    </ul>
  </li>
  <li>
    <p>Yong Le’s achievements</p>

    <ul>
      <li>
        <p>be ready for the Mongol: rebuilt sections of <mark>great wall</mark>, and <strong>grand canal</strong> for transportation, expanded <strong>military</strong></p>

        <ul>
          <li>successive dynasties built and maintained multiple stretches of border walls. The best-known sections of the wall were built by the Ming dynasty</li>
        </ul>
      </li>
      <li>
        <p>his palace at Beijing = the <mark>Purple Forbidden City</mark></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027183224061.png" alt="image-20221027183224061" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>also extensive <strong>maritime voyages: Zheng He</strong></p>
      </li>
      <li>
        <p>sponcered a lot of <strong>Buddhism</strong>, but unclear if he is loyal to Buddhism as sometimes he pursues as Daoist. Perhaps as a tool to establish trust in his subjects</p>

        <ul>
          <li>but did forge a strong relationship with Tibet</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Yong Le’s sponser of Zheng He’s voyage</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027141241577.png" alt="image-20221027141241577" style="zoom:33%;" /></p>

    <ul>
      <li>the aim was to mask his illegitimacy of ruling (as an uncle took power from the direct grandson of emperor), by <strong>announcing to the maritime world that he is emperor</strong></li>
      <li>therefore, voyage was for diplomatic reasons</li>
    </ul>
  </li>
</ul>

<p><strong>Admiral Zheng He</strong></p>

<ul>
  <li>
    <p>the mission was diplomatic: to all tributary states that Yong Le is now the emperor. Hence Zheng He also had military force brought along</p>
  </li>
  <li>
    <p>as a by product of the voyage</p>

    <ul>
      <li><strong>enlarged trading routes</strong></li>
      <li><strong>furthered Ming’s tribute system</strong> (see next subsection)</li>
    </ul>
  </li>
  <li>
    <p>brought back some exotic info/products, e.g. giraffe is named as 麒麟</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027141918382.png" alt="image-20221027141918382" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>some of Zheng He’s fleets routes</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221027141319992.png" alt="image-20221027141319992" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>The Tribute System</strong></p>

<ul>
  <li>was a old system. Two mechanisms of Chinese regimes’ <strong>relationships with foreign states: tribute and trade</strong>.</li>
  <li>the system:
    <ul>
      <li>foreign rulers to send embassies to China bearing tribute in the form of local exotica</li>
      <li>emperor in turn gives gifts of equal or greater value; bestowed titles on tributary rulers; and permitted a controlled volume of <strong>trade</strong></li>
    </ul>
  </li>
  <li>as it is mutually beneficial:
    <ul>
      <li>a device for <strong>mutual recognition</strong>, and to establish <strong>political and cultural prestige of China internationally</strong></li>
      <li>more <strong>trade</strong> opportunities</li>
      <li>in most times both parties are happy</li>
    </ul>
  </li>
</ul>

<p><strong>End of Yongle Fleets</strong>: only YongLe emperor promoted maritime trade/voyage</p>

<ul>
  <li>expeditions cost a lot of money: in 1424, Yongle died, and new court considered the expeditions as <strong>wasteful expenditures</strong> that did not benefit the people’s welfare.</li>
  <li>eventually people switched focus to their northern land: shifted policy to <mark>focus on northern steppe enemies</mark>
    <ul>
      <li>1449 Tumu disaster: Mongol Oirats capture the boy emperor and threaten Beijing—this marks permanent shift in foreign policy away from the sea and towards the steppe.</li>
    </ul>
  </li>
  <li>hence this end also signals the <strong>ended the state-sponsored sea voyages and foreign contacts/trade overseas</strong>. Instead of maritime trade, a lot became used for defending</li>
</ul>

<h2 id="ming-and-the-global-economy">Ming and the Global Economy</h2>

<p><strong>Previously on early Ming</strong></p>

<ul>
  <li><strong>Hongwu (1368-98):</strong> <strong>autocracy</strong> and inheritance of Mongol polices + social reorganization (agrarian view)
    <ul>
      <li>autocracy of emperor in court is supported by the decline of aristocracy + rise of civil exam in Song</li>
      <li>social reorganization involves demarcation of people into four classes $\to$ did not work at the end as society often have boundary crossings</li>
    </ul>
  </li>
  <li><strong>Yongle</strong> (1402-24): usurper = deeply concerned of his ruling = moved capital from Nanjing to Beijing
    <ul>
      <li>build the forbidden city in Beijing</li>
      <li>eager to pursue <strong>tributary relations</strong> with new states = recognize his legitimacy of ruling</li>
      <li>for a similar reason above, sponsored Zheng He’s voyage</li>
    </ul>
  </li>
  <li><strong>Zheng He</strong>’s voyage: massive expedition primarily as a diplomatic tool
    <ul>
      <li>a by-product is to encouraged <strong>private maritime trade</strong> (commercialization later)</li>
      <li>but costly + enemies in the north made China shift focus</li>
    </ul>
  </li>
</ul>

<p><strong>Monetization in mid-Ming</strong>: second wave of commercialization after Song dynasty</p>

<ul>
  <li>
    <p>e.g. description of the prosperity and commercialized economy in south</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101131958629.png" alt="image-20221101131958629" style="zoom: 33%;" /></p>

    <p>material property + commercialization = non-agrarian society starts to come back again</p>

    <ul>
      <li>Hangzhou was also reasonably prosperous, as previous Song capital</li>
      <li><mark>Jiang'nan</mark> = south of Yangzi river = <strong>most prosperous region</strong> in mid-Ming = lots of records come from this area</li>
    </ul>
  </li>
  <li>
    <p>recall that <em>Lijia</em> 里甲 (hundred-and-tithing): village registration and self- management system + <strong>tax system in kind (e.g. rice if produced rice)</strong></p>

    <ul>
      <li>assumption that people stays in the land, and had loopholes = corruptions = less tax collected from wealthy people who try to evade</li>
    </ul>
  </li>
  <li>
    <p>more effective policy to <strong>collect tax all in “cash”</strong> = <mark>Single Whip Reform</mark> (1570)</p>

    <ul>
      <li>pushed forward by Grand Secretary Zhang (recall that grand secretary is very powerful in court)</li>
      <li>complete resurvey of all cultivated land (1580) = updated information = can ask <mark>all taxes to be paid in copper/silver</mark></li>
      <li>aims to simplify taxation and increase efficiency = very effective later
        <ul>
          <li>1568-73 Imperial Treasury’s annual deficit: 2-3 million taels of silver</li>
          <li>1582 The treasury has 4 million taels of silver surplus and grain reserve for a couple of years</li>
          <li>critical to <strong>help Ming dynasty to reach its peak</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Currency of the Hongwu Reign</strong></p>

<ul>
  <li>
    <p>paper money is issued by the government, but they are printed/produced without backup hence people lost faith in it:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101133338779.png" alt="image-20221101133338779" style="zoom:33%;" /></p>

    <ul>
      <li>
        <p>for tax: payment needs to be in copper coin/silver</p>
      </li>
      <li>
        <p>people lost faith in paper currency because <mark>Zhu Yuanzhang</mark> issued paper money <strong>without backup</strong> $\to$ can use silver to exchange paper money but when asked to <em>exchange from paper money to silver, government cannot issue</em></p>
      </li>
      <li>
        <p>regardless, government encouraged paper money = wanted to restrict digging silver. But they never succeeded in controlling its currency = <strong>ends up relying entirely on copper coin and silver ingot</strong></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101133404657.png" alt="image-20221101133404657" style="zoom: 25%;" /></p>

        <p>(note that silver ingots are trusted more as copper can regularly debase)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>thirst for silver ingot = encouraged for <mark>global trade</mark> (foreign countries using silver to trade for valuable Chinese goods)</p>

    <ul>
      <li>
        <p>lots of <strong>foreign private trade</strong> activities from European</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101133810017.png" alt="image-20221101133810017" style="zoom:33%;" /></p>

        <p>recall that:</p>

        <ul>
          <li>
            <p>Ming government did not sponsor maritime trade (<strong>opposite in Song’s altitude = took large revenue from maritime commerce</strong>)</p>
          </li>
          <li>
            <p>this is perhaps because a) gov charged light tax in commerce b) agrarian view c) private maritime trade tied with piracy = political problems. Hence very suspcious with private trade outside diplomatic reasons</p>
          </li>
        </ul>
      </li>
      <li>
        <p>trade with <mark>Portuguese</mark>: arrived and found a vigorous trade network in China</p>

        <ul>
          <li>
            <p>some maritime traders treat Chinese ships as competitors = kill them and take over their business</p>

            <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101134216818.png" alt="image-20221101134216818" style="zoom: 25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101134237483.png" alt="image-20221101134237483" style="zoom: 25%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

            <p>this caused some problem: where China official did not support trading with Japan due to piracy and suspicions, those Europeans don’t care hence those Chinese goods are illegally leaked to Japan</p>
          </li>
          <li>
            <p>but eventually, local Chinese officials sees trade benefits and defied government decisions = Portuguese conspired with local officials to <strong>establish a port in Macao</strong> (a backwater at the time) to trade with Canton and Nagasaki</p>

            <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101134506378.png" alt="image-20221101134506378" style="zoom: 25%;" /></p>

            <p>Macao = first and last European colony in China ruled by Portuguese</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>what goods did European liked from China? some trades included porcelain = Chinese artisans mass produce porcelain specifically for foreign trades:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101134612595.png" alt="image-20221101134612595" style="zoom: 25%;" /></p>

    <p>most of exported ones are not best quality, and best ones often reserved for China’s home market</p>
  </li>
</ul>

<p><strong>Trading with Spanish and Dutch</strong></p>

<ul>
  <li>
    <p>results of private maritime trades = period when porcelain appearing <strong>popular in countries such as Dutch</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101154259734.png" alt="image-20221101154259734" style="zoom: 25%;" /></p>
  </li>
  <li>
    <p>both Spanish and Dutch also had high trade interest with China</p>

    <ul>
      <li>early 1570s, the <mark>Spanish</mark> established a trade port in Manila (Philippines), still far from China.</li>
      <li>in 1623-62, the <mark>Dutch</mark> East India Company (VOC) occupies fort in northern <strong>Taiwan</strong>
        <ul>
          <li>note that at that time Taiwan was not under Ming’s control.</li>
          <li>Once colonization by Dutch took place, Chinese went and Taiwan become incorporated into the Qing</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Spanish</strong> found China was silver hungry = traded/provided <strong>a lot of silver</strong> as they had extraordinary level of silver from mines in its colonies</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101135157936.png" alt="image-20221101135157936" style="zoom:25%;" /></p>

    <p>Spanish ship silvers to China = established a stronghold in trading = on average more than 2,300 tons of silver arrived in China from abroad</p>

    <ul>
      <li>note that: <mark>since 1493 - 1898 the Americas were invaded and incorporated into the Spanish Empire</mark>, with the exception of Brazil, British America, and some small regions of South America and the Caribbean</li>
    </ul>
  </li>
  <li>
    <p>in 16th – 17th c., the trade imbalance with China was filled with silver from Spain’s New World colonies, and from Japanese mines</p>
  </li>
  <li>gold flow out, silver flow in = as silver seen more important in China</li>
  <li>
    <p>gives many bad consequences (as well as few good ones)</p>
  </li>
  <li>
    <p><strong>Consequence of the influx of silver</strong>:</p>

    <ul>
      <li>
        <p>depreciation of silver = inflation;  market speculation; .. all contributed to <strong>Ming’s collapse</strong> in the long run</p>
      </li>
      <li>
        <p>immediate effects = <mark>rise of merchant</mark> = wealth can surpass the gentry family = destabilize the social order</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101135859684.png" alt="image-20221101135859684" style="zoom:25%;" /></p>

        <p>e.g. the above is a book written for gentry/scholar class to instruct them how one <em>spend silver in a way to not look poor</em> = anxiety of Confucian gentleman facing the threat of status change from merchant class</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Consequence of foreign trade</strong>:</p>

    <ul>
      <li>
        <p>increase in <mark>population</mark></p>

        <ul>
          <li>
            <p>recall that Song population grew due to new tech for agriculture + new crop from Vietnam</p>
          </li>
          <li>
            <p>similarly, in Qing: <strong>new crop from Amiercan by spanish colonizers</strong>. Maize, Sweet potatoes, etc, grows in places unsuitable for millet and rice and have high yield:</p>

            <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101140025257.png" alt="image-20221101140025257" style="zoom:33%;" /></p>

            <p>hence also helped the life of poorer people</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Ming fascinated by <strong>Europeans’s ballistic technology</strong>, e.g. cannon</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101140323059.png" alt="image-20221101140323059" style="zoom:33%;" /></p>

        <p>many Chinese officials realized they are lagging behind in technology, and hence some advisers: should hire European governments to help defend northern enemies</p>
      </li>
      <li>
        <p>famous <strong>Jesuits</strong> in China:</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101140717697.png" alt="image-20221101140717697" style="zoom:25%;" /></p>

        <ul>
          <li>
            <p>impersonate as Confucian scholar to blend into Chinese society at first</p>
          </li>
          <li>
            <p>aimed to convert Ming gentry to <mark>Christianity</mark>, by reconciling Christian beliefs with Chinese beliefs</p>
          </li>
          <li>
            <p>managed to <strong>convert some but very small numbers to Christian</strong>, as many Chinese gentries are more interested in European tech than religion</p>
          </li>
          <li>
            <p>contribution to the world map = <strong>remembered as a person introducing western info + tech</strong> much more than as a preacher</p>

            <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101140916588.png" alt="image-20221101140916588" style="zoom:25%;" /></p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>A new cultural form: novel</strong></p>

<ul>
  <li>
    <p>recall that good economy usually results in prosperity of culture</p>
  </li>
  <li>
    <p>Ming poetry becomes less important hence less poets from Ming (and Qing), <strong>but more into (fictitious) novels: 3 out of the 4 classics of Chinese literature produced</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101141147687.png" alt="image-20221101141147687" style="zoom:33%;" /></p>

    <ul>
      <li>Journey to the west: Xuanzang’s trip to see Buddhist scripts during Tang, but a fantasied novel</li>
      <li>Three Kingdoms: is history</li>
      <li>The water margin: set in the northern Song period = how a group of 108 outlaws trying to rebel the government</li>
      <li>The Plum in the Golden Vase: 16th century work focusing on the corrupt noble in China = known for erotic reasons</li>
    </ul>
  </li>
  <li>
    <p>educated gentleman writes prose and fiction = those are more likely survive. The lack of novels in early time means novel might be there as well</p>

    <ul>
      <li>however, many novels are fictitious yet poems were real. Why are people suddenly interested in unreal things? Don’t know</li>
      <li>some say that it is due to Wang Yangming; others believe using easier language catered to a wider audience</li>
    </ul>
  </li>
</ul>

<p><strong>Changes in gender relations</strong></p>

<ul>
  <li>
    <p>overall, women facing <mark>increasingly intense subjugation</mark></p>

    <ul>
      <li><strong>female infanticide</strong>: wanted to keep small family size, hence kills girls rather than boys who can labor and continue family line</li>
      <li>revised inheritance and property laws = intended to help but reverse effect for women</li>
      <li>female seclusion = find themselves <strong>confined</strong> and even not allowed to visit temples a lot</li>
      <li>chastity cult = encouraged widow to be a widow. However, this was an ideal and in reality widow remarriage is common (as they lose properties from husband, due to Ming laws)</li>
      <li>foot-binding. started in Song, but in Ming becomes more extreme. Accepted by women as a way to ensure good marriage</li>
    </ul>
  </li>
  <li>
    <p>confined inside hence they are socializing within themselves.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221101142217363.png" alt="image-20221101142217363" style="zoom: 25%;" /></p>
  </li>
  <li>
    <p>but there are also some <mark>benefits</mark></p>

    <ul>
      <li><strong>rising level of female education</strong> + <strong>men sponsored female readers &amp; writers</strong>. Perhaps because men are exerting so much control that they think nothing can go wrong by allowing these</li>
      <li><strong>compassionate marriage</strong>
        <ul>
          <li>perhaps better education means men can more easily communicate with women</li>
          <li>worship of personal feeling and sentiments = influence of Wang Yangming</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="qing-imperialism">Qing Imperialism</h1>

<p><strong>Previously in Ming and the World</strong></p>

<ul>
  <li>Ming Dynasty: large-scale trends in population growth, <strong>commercialization (again since Song)</strong>, and <strong>global trade (private maritime trades)</strong>
    <ul>
      <li>much global influence from private maritime trade. Discouraged by government due to piracy + political reasons + their northern enemies</li>
    </ul>
  </li>
  <li><mark>Single-Whip tax reform</mark>: simplify taxation procedure, so that almost all are paid in cash (mostly <mark>silver</mark>)
    <ul>
      <li>so that tax evasion becomes harder = increased government revenue</li>
      <li>but <strong>increased demand of silver</strong></li>
    </ul>
  </li>
  <li>large demand of silver faciliated global trade
    <ul>
      <li>export Chinese manufactured goods such as porcelain, imported silver from abroad</li>
      <li>also funded the rise of Atlantic (e.g. Spanish) empires</li>
    </ul>
  </li>
  <li>along with silver, <strong>Jesuit commissionaires</strong> also came to China aiming to convert Chinese to Roman Catholicism
    <ul>
      <li>reconciling Christian with Confucian pratices,</li>
      <li>but Chinese are more interested in European science and technology rather than religion, hence not very effective</li>
      <li>introduction of <strong>western science into China</strong></li>
    </ul>
  </li>
  <li><strong>Novel as a new literary genre</strong>: vernacular language and a wide readership</li>
  <li><strong>Women more subjugated but increased education</strong>
    <ul>
      <li>also more women publisher</li>
    </ul>
  </li>
</ul>

<h2 id="manchu-conquest">Manchu Conquest</h2>

<p><strong>Decline of Ming</strong>: several factors (importance is unranked)</p>

<ul>
  <li>early period we had aristocratic emperor, but later also <strong>inattentive emperor</strong>
    <ul>
      <li>Emperor Wanli reigned for a long time but stopped attending to court affairs for 30 years</li>
      <li>ruling often needs regulation and interventions; promotion needed his approval; But he simply did not care</li>
    </ul>
  </li>
  <li>climate changes and <mark>natural disasters</mark>, such as <strong>famine</strong>, <strong>droughts</strong>, epidemics, <strong>cold spell,</strong> etc. in the 1630-1640s.
    <ul>
      <li>was also the coldest decade in Russian history = poor agriculture due to the cold as well = famine</li>
      <li>Beijing had frequent epidemic and grand canal become highways to spread</li>
    </ul>
  </li>
  <li><strong>collapse of fiscal system</strong> due to eventual shortage of silver
    <ul>
      <li>commoners do not have much silver to pay tax; govern cannot pay soldiers defending their borders</li>
      <li>80% of counties stopped forward taxation to central government</li>
      <li>Europeans became reluctant shipping silver to China as time goes on = also contributed to silver shortage</li>
    </ul>
  </li>
  <li>threats from <strong>Mongol and Jurchens</strong>
    <ul>
      <li>Ming also needed money to supply military = attempted to increase taxes, etc. all of which are non-popular approaches for the people</li>
    </ul>
  </li>
  <li>all of those combined gave <strong>revolts</strong>
    <ul>
      <li>the two most prominent groups led by former soldiers Li Zicheng (1606-1645) and Zhang Xianzhong (1606- 1647) eventually captured the capital Beijing</li>
    </ul>
  </li>
</ul>

<p><strong>Rise of Jurchens/Manchus</strong></p>

<ul>
  <li>recall that Jurchan were powerful <strong>during the Song</strong>: they established <mark>Jin金</mark> dynasty, helped Song to defeat Khitans but turned to sack Kaifeng later
    <ul>
      <li>Jurchans established the Jin dynasty but also become much sinicized in the past and their</li>
      <li>Jin dynasty 1127 - 1200s, when later Mongols sacked and took control of northern China = Yuan dynasty</li>
      <li>in 1600s, they see themselves as Later Jin hence a <strong>political legitimacy</strong> for restoration of power</li>
    </ul>
  </li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103132855563.png" alt="image-20221103132855563" style="zoom:25%;" /></p>

<ul>
  <li>brief overview: The Jurchens are chiefly known for producing the Jin (1115–1234) and Qing (1616/1636-1912) conquest dynasties on the Chinese territory. The latter dynasty, originally calling itself the Later Jin, was founded by a Jianzhou commander, <strong>Nurhaci (r. 1616–26), who unified most Jurchen tribes</strong>, incorporated their entire population into <strong>hereditary military regiments known as the Eight Banners</strong>, and patronized the creation of an alphabet for their language based on the Mongolian script.</li>
  <li>
    <p>they live in China’s north plain, had long engagement with Chinese in terms of trade, tribute, ally/enemy</p>
  </li>
  <li>Jurchans where <strong>half-sendantry</strong> = different from Mongols
    <ul>
      <li>had settlements, but also a lot of raids, enslaved captives, hunting and fishing, etc</li>
      <li>profited a lot from trade with Ming: Chinese liked <strong>ginseng</strong>. 25% silver went to Jurchen because of the ginseng trade = they now have money to fund military</li>
    </ul>
  </li>
  <li>rise of Jurchan as <strong>Nurhaci brought up a strong tribal confederacy</strong></li>
</ul>

<p><strong>Nurhaci: leader of Jurchen Confederacy</strong></p>

<ul>
  <li>
    <p>From the lineage of <strong>Aisin Gioro愛新覺羅</strong></p>
  </li>
  <li>
    <p>Ming’s divide and conquer strategy actually made his own tribe stronger</p>

    <ul>
      <li>first subjugated to Ming and used it to <strong>defeat other tribes</strong> to make his tribe strong</li>
      <li>in 1595 Ming awarded him the title of Dragon-Tier General</li>
      <li>eventually his tribe became the strongest, so <strong>Ming turned against him</strong>, and <mark>Nurhaci allied with Mongol tribes</mark></li>
    </ul>
  </li>
  <li>
    <p>in 1616 he declared himself the <mark>khan of the Great Jin</mark> (restoration of Jin dynasty as descendents)</p>

    <ul>
      <li>captial at Mukden, modern Shenyang</li>
      <li>adapt Mongolian alphabet to the Jurchen speech</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103133520903.png" alt="image-20221103133520903" style="zoom:33%;" /></p>

    <p>so Shenyang was the center of their political power</p>
  </li>
  <li>
    <p>another decisive step is the <strong>Eight Banner system</strong>: reconstituting social structure</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103133613612.png" alt="image-20221103133613612" style="zoom: 25%;" /></p>

    <ul>
      <li>
        <p>each banner is a permanent group also a <strong>fighting unit + unit of residence</strong></p>

        <ul>
          <li>included dependants of fighting mens includeing women children and servants</li>
        </ul>
      </li>
      <li>
        <p>hence <strong>shared identity and shared clothing within a group</strong></p>
      </li>
      <li>
        <p>over time, each banner becomes somewhat ethnic groups = close relationship within the group, but not purely one ethnic within a group</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103133752655.png" alt="image-20221103133752655" style="zoom:25%;" /></p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Hong Taiji’s creation of Manchu Qing</strong></p>

<ul>
  <li>succeeded Nurhaci, second Khan of later Jin</li>
  <li>declares himself emperor of the <mark>Great Qing</mark>
    <ul>
      <li>Qing because Qing = clear, hence water related &gt; Ming = fire (and moon)</li>
      <li>this new dynasty name is a clear announcement of his intention to conquer Ming</li>
    </ul>
  </li>
  <li>some measures he took to rule include
    <ul>
      <li>ordered translations of Chinese culture to Jurchen so he could study them</li>
      <li>moved towards Chinese-style burearcarcy centralization</li>
      <li>in 1636 he creates a new identity for his people called <mark>Manchu</mark></li>
    </ul>
  </li>
  <li>one key thing to realize is that <mark>Manchus are also not one ethnicity</mark> = rather amalgamation of steppe nomadic people + sedentary Chinese + etc. Therefore, Manchu as an “ethnicity” more like a <strong>political construct</strong></li>
</ul>

<p><strong>Manchu Identity</strong></p>

<ul>
  <li>
    <p>Manchu identity is <strong>not determined by birth</strong>, but by just being a member of the Banner</p>

    <ul>
      <li><strong>a tool for integration</strong>: shared identity overcame linguistic-ethnic differences of banner families that included Jurchen, Mongols, Korean, Chinese</li>
      <li><strong>a tool for segregation</strong>: emphasized unique claim of Manchus to rule China.</li>
    </ul>
  </li>
  <li>
    <p>They also emphasized identity markers such as hair styles and clothing, to be <strong>different from that of Han people</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103134331632.png" alt="image-20221103134331632" style="zoom:25%;" /></p>

    <p>so men hair style become political tool as well: shows who you are submissive to.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103134354376.png" alt="image-20221103134354376" style="zoom:25%;" /></p>

    <p>examples of Manchu clothing is more diverse. e.g. soldiers uses a lot of bows</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Soldiers</th>
          <th style="text-align: center">Qing Officials</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103134659635.png" alt="image-20221103134659635" style="zoom: 25%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103134730021.png" alt="image-20221103134730021" style="zoom:25%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Manchu also <em>tried</em> to forbid <mark>women</mark> to foot-binding</p>

    <ul>
      <li>but that ban didn’t work: foot-binding practices are already very entrenched in Han Chinese minds, and it was seen as attractiveness/object of sexual desire</li>
      <li>so most Manchu women don’t bind their feet, but Han people still continue the practice</li>
      <li>why no enforcing this change like with mens’ hairstyle? perhaps because men already submitted, no need to change women</li>
      <li>but the general trend in Ming and Qing can also be viewed as <strong>tightening controls of its people = also of women</strong>
        <ul>
          <li>Ming being active in domestic affairs with its Lijia policy, and using morality manuals (Sacred Edict of Six Maxims)</li>
          <li>prior to Ming, pre-martial sex was not proscribed. But in Ming and Qing there are a lot of much focus on chastity = <strong>interference</strong> from government</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Qing conquest of Korea and Japan</strong></p>

<ul>
  <li>wanted to at least prevent Korea to join Ming to fight against them, so basically <strong>conquered Korea</strong> (but left little cultural footprint)
    <ul>
      <li>initially, Korea see Jurchens as barbarians = many had Confucius thoughts</li>
      <li>Korea also wanted to help Ming because, before when Japan tried to invade Korea, Ming helped.</li>
      <li>but eventually Korea surrendered to Manchu due to their military strength and became <strong>vassal state of Qing</strong>
        <ul>
          <li>so no choice but to join conquest to Ming</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>sees themselves as <strong>repository Confucian hence named themselves soHwa</strong>
    <ul>
      <li>as in fact it is true: Ming is the <em>last dynasty ruled by Han Chinese</em></li>
    </ul>
  </li>
  <li>Qing conquest of Ming also shocked Japanese
    <ul>
      <li>the Japanese sees this as Ka-i Hental華夷變態 = transformation of China from civilized to barbarian</li>
      <li>(recall that during Tang, they see Chinese as a model for culture and civilization)</li>
      <li>so after this, Japanese also think that East Asian culture superiority is no longer in China, but in Japan</li>
    </ul>
  </li>
</ul>

<p><strong>Last Years of Ming empire</strong>: there player. Ming, the Rebels, Hong Taiji’s successor</p>

<ul>
  <li>Ming asked Manchu to help quell the rebel, but eventually they had a bigger plan and conquered the whole China</li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103135745712.png" alt="image-20221103135745712" style="zoom:25%;" /></p>

<ul>
  <li>
    <p>Qing took several yeas to conquer China proper, and expanded territories</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103135823958.png" alt="image-20221103135823958" style="zoom:25%;" /></p>
  </li>
  <li>
    <p>Qing had <strong>remarkably capable</strong> emperors = <mark>High Qing</mark>: <strong>Kangxi</strong> Emperor (r. 1654-1722) <strong>Yongzheng</strong> Emperor (r. 1722-1735) <strong>Qianlong</strong> Emperor (r. 1735-1796)</p>
    <ul>
      <li>set most of the boundaries of today’s China: Xingjiang, Tibet, etc</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Kangxi</strong></p>

  <ul>
    <li>The Kangxi Emperor is considered one of China’s greatest emperors.</li>
    <li>He suppressed the Revolt of the Three Feudatories, forced the Kingdom of Tungning in Taiwan and assorted Mongol rebels in the North and Northwest to submit to Qing rule, and blocked Tsarist Russia on the Amur River, retaining Outer Manchuria and Outer Northwest China.</li>
    <li>The Kangxi Emperor’s reign <strong>brought about long-term stability and relative wealth</strong> after years of war and chaos. He initiated the period known as the “Prosperous Era of Kangxi and Qianlong” or <strong>“High Qing”</strong>, which lasted for several generations after his death.</li>
  </ul>

  <p><strong>Yongzheng</strong></p>

  <ul>
    <li>A hard-working ruler, the Yongzheng Emperor’s main goal was to create an <mark>effective government at minimal expense</mark>. Like his father, the Kangxi Emperor, the Yongzheng Emperor used military force to preserve the dynasty’s position.</li>
    <li>Although Yongzheng’s reign was much shorter than that of both his father (the Kangxi Emperor) and his son (the Qianlong Emperor), the Yongzheng era was a <strong>period of peace and prosperity</strong>.</li>
    <li>Yongzheng Emperor cracked down on corruption and reformed the personnel and financial administration</li>
  </ul>

  <p><strong>Qianlong</strong>:</p>

  <ul>
    <li>As a capable and cultured ruler inheriting a thriving empire, during his long reign, the Qing Empire reached its most splendid and prosperous era, boasting a large population and economy.</li>
    <li>As a military leader, he led military campaigns <mark>expanding the dynastic territory to the largest extent by conquering and sometimes destroying Central Asian kingdoms</mark>.</li>
    <li>This turned around in his late years: the Qing empire began to decline with corruption and wastefulness in his court and a stagnating civil society.</li>
  </ul>
</blockquote>

<p><strong>Qing Conquest of Taiwan in 1683</strong></p>

<ul>
  <li>
    <p>recall that Dutch conquered in 1623 and established their colony in Taiwan</p>
  </li>
  <li>
    <p>brief history of Taiwan</p>

    <ul>
      <li>
        <p>Fall of Ming 1640-1650: people from Fujian started settling there, and <strong>Ming loyalists retreated</strong> to Taiwan</p>
      </li>
      <li>
        <p><strong>Zheng Chengong</strong> 1662: drove out the dutch and more=100k Chinese emigrated there</p>
      </li>
      <li>
        <p><mark>Qing defeated the Zheng regime 1683</mark>: but allow it to exist autonomously and made it a <strong>prefecture</strong> (district under the government of a prefect) of <strong>Fujian</strong></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103140256769.png" alt="image-20221103140256769" style="zoom: 33%;" /></p>
      </li>
      <li>
        <p>in 1870s, Taiwan became its own province as it is a key port for martime trade</p>
      </li>
      <li>
        <p>1894-1895 after Sino-Japanese war and WWII: <strong>became part of Japan</strong>. A lot of buildings today still have Japanese style</p>
      </li>
      <li>
        <p>at the end of 1945-1949: 孙中山 being the first leader of the Kuomintang (Nationalist Party of China), lost and followers immigrated to Taiwan.</p>

        <ul>
          <li>Thinking they can go back and recover, but obviously never happened. These are also considered as <em>Waisheng</em> and don’t speak Taiwanese (now the “ethnic” gap is much smaller)</li>
          <li>since many Waisheng are governments, so they are also kind of upperclass in Taiwan</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>therefore, Taiwanese very much takes over from <em>Mingnan</em> dialect from Fujian</p>
  </li>
  <li>
    <p>but again, Qing in general had loose control over newly conquered regions</p>
  </li>
</ul>

<p><strong>Qing conquest of Dzungar Mongols</strong></p>

<ul>
  <li>western Dzungar dmoinated the west euroasia in 1630</li>
  <li>long batter with Dzungar Mongol, and <strong>subdued Dzungar Mongol in 1696</strong></li>
  <li>but a lot of revolt by Dzungar Mongols = Qing launched genocide in 1750s
    <ul>
      <li>effectively <strong>ended the northen Mongol problem</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Qing conquest in Tibet</strong></p>

<ul>
  <li>Dzungar mongols tried to expand into Tibet = hence they invaded in 1717</li>
  <li>so Qing invaded Tibet and <mark>installed a pro-Qing Dalai Lama</mark>; Tibet became a protectorate</li>
  <li>but again little interference in Tibetan local governance: hence maintained Tibetan culture</li>
</ul>

<p><strong>Qing creation of Xinjiang</strong></p>

<ul>
  <li>
    <p>in the Han dynasty, some chinese troops stationed there, but not in Tang and Song as Chinese had lost control</p>
  </li>
  <li>
    <p>finally re-acquired by the Qing = again Qianlong’s troops basically <strong>massacred Dzungar troops and completed subjudation</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103140836249.png" alt="image-20221103140836249" style="zoom: 25%;" /></p>
  </li>
  <li>
    <p>after this conquest, in 1768 announced the formal annexation = <strong>Xingjing, meaning new dominion</strong></p>

    <ul>
      <li>but again, territory under control of Manchus, but <strong>local poeple also allowed to contine their practice</strong></li>
      <li>no restrictions as with Han chinesemen</li>
    </ul>
  </li>
  <li>
    <p>however, maintenance of far territory is also a <strong>financial burden</strong>, e.g. military spending. Therefore, perhaps most effective use of Xinjing in Qing is as a <strong>penal colony = 10% officials in Xingjinag as punitive banishment</strong></p>
  </li>
</ul>

<p><strong>Managing large territory and diverse people</strong></p>

<ul>
  <li>
    <p><mark>Qianlong</mark> assumed different persona to different audiences = portray him in many different costumes</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">as Manjusri</th>
          <th style="text-align: center">as Confucian</th>
          <th style="text-align: center">as European-style Warrior</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103141308857.png" alt="image-20221103141308857" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103141345240.png" alt="image-20221103141345240" style="zoom:25%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103141428175.png" alt="image-20221103141428175" style="zoom: 33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>e.g. as Manjusri = appeal to Laima in Tibet, or as Confucian scholar when speaking to Han Chinese</p>

    <ul>
      <li><strong>acknowledges culture differences</strong> and ethnic distinctions, also especially wanted to <strong>preserve his Manchu ethnicity</strong></li>
    </ul>
  </li>
  <li>
    <p>so in general, Qing are interested in conquering <em>new</em> lands but <strong>not governing those <em>new</em> land</strong></p>

    <ul>
      <li>global history: Russian expanded east-ward: time of imperial land grabbing = have as much land as possible = control over resources</li>
      <li>but remember those from Qing are not peaceful conquest = e.g. Xinjiang lots of people massacred due to dissent. So still insisting on their <strong>political dominance but not really in establishing culture statues</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Euroiserie in China</strong>: Yuanming Yuan built near a lake outside Beijing</p>

<ul>
  <li>
    <p>imperial palace began construction in 1707, added a number of <strong>western</strong> style building with the aid of <strong>Jesuit architects</strong></p>
  </li>
  <li>
    <p>unfortunately destroyed by British and French in 1860. Historical drawings shown below, and attempts of restoration in progress</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103141816162.png" alt="image-20221103141816162" style="zoom: 33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221103141923398.png" alt="image-20221103141923398" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
</ul>

<h2 id="the-qing-in-crisis-opium--western-imperialism">The Qing in Crisis: Opium &amp; Western Imperialism</h2>

<p><strong>Previously on the Qing Conquest</strong></p>

<ul>
  <li>Decline of Ming and rise of Manchu Qing
    <ul>
      <li>Ming facing adverse climate, peasant rebellion, etc</li>
      <li>Peasant rebellions give opportunity to Manchus from NE China, descendants of Jin金</li>
    </ul>
  </li>
  <li>Qing did <strong>really well</strong> and controlled China for almost 3 centuries</li>
  <li>Qing used the <strong>Banner System</strong> which constructed a “political” ethnicity
    <ul>
      <li>a banner group represents a unit of a military organization/ unit of residence</li>
      <li>joining to be a Bannerman = being a Manchu = an “invented” ethnic group</li>
    </ul>
  </li>
  <li>Manchu Qing emphasizes that Manchus were <mark>not just Sinicized, but regarded Qing as a multiethnic empire</mark>;
    <ul>
      <li>e.g. Qianlong with multiple faces, different policies, etc.</li>
    </ul>
  </li>
  <li><mark>High Qing:</mark> <strong>territory</strong> reached the greatest extent
    <ul>
      <li>18 century: unrivaled standard of living and invention flow more from east to west</li>
      <li>19 century: outmatched by western countries</li>
    </ul>
  </li>
</ul>

<p><strong>The Canton System</strong>: Qing’s international trade</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110132343392.png" alt="image-20221110132343392" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>Qing <strong>legalized private maritime commerce</strong>, though restricted to Canton</p>

    <ul>
      <li>there were many prosperous trade ports, but all abandoned by people and finally went with <strong>Canton = most dominate place in sino-western trade</strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110132453325.png" alt="image-20221110132453325" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>wanted to control western influence</p>

    <ul>
      <li>control with inflow of western foreigners: fear illegal infiltration of western missionaries</li>
      <li>therefore, in 1757: Canton the sole port open for Western trade</li>
      <li>1760 court imposed <strong>regulations on foreigners</strong> (e.g. their mobility) in order to keep them from disturbing Chinese
        <ul>
          <li>even when trading, they need to deal with designated Qing merchants (hong行)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>13 Hongs of Canton</strong>: 十三行</p>

<ul>
  <li>
    <p>Chinese merchants controlled business and <strong>monitored European trades</strong></p>

    <ul>
      <li>Hong = A hong 行 originally designates both a type of building and a type of Chinese merchant intermediary in Guangzhou (formerly known as Canton), Guangdong, China, in the 18–19th century, specifically during the Canton System period.</li>
      <li>foreigners can only spend a few weeks in the Hong, then return to Macao, which was still a Portuguese colony</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110132833326.png" alt="image-20221110132833326" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Wu Bingjian 伍秉鑑 (1769–1843), known as “Howqua”浩官: <mark>most powerful merchant at the time</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110133615919.png" alt="image-20221110133615919" style="zoom:33%;" /></p>

    <ul>
      <li>all Chinese home merchants belong to a guild: Cohong 公行 =  responsible for policing the trade with westerners</li>
      <li>granted right to monopolize import and export trade, but in turn <strong>guarantee to government all duties and proper behavior of western</strong></li>
      <li>he also involved in the opium trade = people in China today therefore have a fixed feeling towards him</li>
    </ul>
  </li>
  <li>
    <p>during 18th century, <strong>industrial revolution in Britain made it became leading commercial nation</strong> with colonies (e.g. in India)</p>

    <ul>
      <li>English trade with China monopolized by British East India Company = allowed by Queen Victoria</li>
      <li>but this powerful company is still restricted by Chinese Hong in Canton</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>The <strong>Industrial Revolution</strong> was the transition to <mark>new manufacturing processes in Great Britain, continental Europe, and the United States</mark>, that occurred during the period from around 1760 to about 1820–1840.</p>

  <ul>
    <li>This transition included going from <strong>hand production methods to machines</strong>, new chemical manufacturing and iron production processes, the increasing use of steam power and water power, the development of machine tools and the rise of the mechanized factory system.</li>
    <li>Output greatly increased, and a result was an <strong>unprecedented rise in population and in the rate of population growth</strong>.</li>
  </ul>
</blockquote>

<p><strong>Macartney Mission of 1793</strong>:</p>

<ul>
  <li>
    <p>British <strong>wanted to improve profits/trade footholds in China</strong>, e.g. get tea cheaper and establish footholds to store goods</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110134153766.png" alt="image-20221110134153766" style="zoom:33%;" /></p>

    <ul>
      <li>
        <p>hence wanted China to abandon tribute system = towards more in how European deal with each other</p>
      </li>
      <li>
        <p>a more direct trade. Free trade + <strong>Adam Smith = Wealth of Nation</strong> = justify British idea to ask China abandon Canton system</p>
      </li>
    </ul>
  </li>
  <li>
    <p>requests include</p>

    <ul>
      <li>
        <p>open commercial warehouse in Beijing</p>
      </li>
      <li>
        <p>sought extra territoriality:British nationals exempt from the Qing legal jurisdiction</p>
      </li>
    </ul>

    <p>i.e. wanted to revise the canton system to <strong>trade more freely in China</strong></p>
  </li>
  <li>
    <p>Therefore, missionaries sent to China brought some interesting European goods but Qianlong is not interested in</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110195847636.png" alt="image-20221110195847636" style="zoom:33%;" /></p>

    <p>notice that in this painting westerner <strong>tried to portray Chinese as despotic and un-understanding</strong>, because Qianlong (in his 80s) decisively <strong>refused all British request</strong> in a very condescending/arrogant tone:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110134503496.png" alt="image-20221110134503496" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Why did Qianlong reject those requests?</p>

    <ul>
      <li>fundamentally, those requests themselves are somewhat went overboard.</li>
      <li>perhaps Qianlong is not interested in western goods? Note that they <strong>did collected a lot of western gifts from other countries</strong>, so it is what presented was not new = China didn’t really need British manufactures
        <ul>
          <li>however, later on the major problem is his <mark>slight on western (military) tech</mark></li>
        </ul>
      </li>
      <li>perhaps because the missionaries did not kowtow? Recent research shows that obsession of the ritual does not come from Qianlong, but the missionaries</li>
    </ul>
  </li>
  <li>
    <p>Why was Qianlong so arrogant?</p>

    <ul>
      <li>
        <p>indeed, China was powerful as there is a huge trade imbalance: silk and porcelain, and tea became drink of choice in Britain</p>
      </li>
      <li>
        <p><strong>large amount of silver flow into China</strong>,</p>

        <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110135252789.png" alt="image-20221110135252789" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110135349779.png" alt="image-20221110135349779" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
      </li>
      <li>
        <p>so British had to have plans of <strong>coming up alternatives commodity to trade</strong>:</p>

        <ul>
          <li>cotton + silver from new world</li>
          <li>tried to re-sell tea to the US to get profit: <strong>Boston Tea Party</strong>. The target was the Tea Act of May 10, 1773, which allowed the British East India Company to sell tea from China in American colonies <strong>without paying taxes</strong> apart from those imposed by the Townshend Acts. Hence is unfair and Americans dumped/destroyed entire shipment of tea</li>
          <li>Finally, British turned to opium</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Opium trade with China</p>

    <ul>
      <li>opium was not new, but only used for medical purposes before</li>
      <li>17th century: smoking opium and tabacco caught on in China, just like how British took tea.</li>
      <li>The <strong>great demand of opium</strong> then solved the trade deficit with Britain</li>
      <li>19th century: net outflow of silver from China due to opium trade</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110135724678.png" alt="image-20221110135724678" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Opium Imports from India to China</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110140059672.png" alt="image-20221110140059672" style="zoom:33%;" /></p>

<ul>
  <li>opium became major product in China over time
    <ul>
      <li>EIC monopoly ended and private trade (e.g. Russel &amp; Company) begins, even more opium (like freedom of trade)</li>
      <li>e.g. lots of American gained most fortune from China trade</li>
    </ul>
  </li>
  <li>then obviously this caused a lot of <strong>problems</strong>
    <ul>
      <li><strong>2 million addicts in China</strong></li>
      <li>difficult for Chinese to enforce ban
        <ul>
          <li>attempted 1729 first opium ban; 1800, banned importation and domestic production</li>
          <li>however, this <strong>did not work</strong> (even open trading disappeared) because there are lots of European <strong>smuggling + corruption</strong> from local officials and <strong>underground networks</strong></li>
        </ul>
      </li>
      <li>since opium was illegal, Qing made no tax revenue from it</li>
      <li>net silver flow out = silver shortage soon caused problem in money supply
        <ul>
          <li>2 million taels deficit in 1820s and 9 million in early 1830s</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>The Opium War and its Aftermath</strong></p>

<ul>
  <li>opium war fundamentally changed the relationship between China and the Western
    <ul>
      <li>before, China shows <strong>dominance</strong> over western power, like how most civilization in centered in China and how Mongol/northern tribes conquered lots of territories, and establishing <strong>tributary/vassal states</strong></li>
      <li>now, difference in technology made <strong>China lag much behind western in power</strong></li>
    </ul>
  </li>
  <li>
    <p>Emeperor appointed <mark>Lin Zexu</mark> to destroy opium at Humen</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110140939345.png" alt="image-20221110140939345" style="zoom:33%;" /></p>

    <ul>
      <li>Charles Elliot (Chief Superintendent of British Trade in China) promised to <strong>compensate the merchants after confiscation, but did not happen</strong> after Lin confiscated them</li>
      <li>hence English commercial interest <strong>pushing war</strong> with China, so Lin’s confiscation of opium became a justification for war</li>
    </ul>
  </li>
  <li>
    <p>Qing navy was <mark>ill prepared against British navy forces</mark></p>

    <ul>
      <li>China had no significant government input in maritime tech after Zhenghe’s voyage</li>
      <li>in general, low tax imposed = low research fund for strong navy power</li>
      <li>ended up having fully iron warship from British v.s. wood ship in China</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110141548241.png" alt="image-20221110141548241" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>Treaty of Nanjing</strong>: defeat and humiliation</p>

<ul>
  <li>
    <p>China lost easily and is forced to <strong>sign unequal treaties</strong></p>
  </li>
  <li>
    <p>those treaties include:</p>

    <ul>
      <li>
        <p>21 million silver paid to England as punitive indemnity</p>
      </li>
      <li>
        <p>“opening” of treaty ports, such as Shanghai (later, “concessions”)</p>
      </li>
      <li>
        <p>gave England Hong Kong ‘in perpetuity’ (later, returned in 1997)</p>
      </li>
      <li>
        <p>pressured more opium trades</p>
      </li>
      <li>
        <p>low tariff set for the English</p>
      </li>
      <li>
        <p>in 1843: extra-territoriality: Qing laws did not apply to the English in Qing territory. (e.g., Englishmen could kill</p>

        <p>Chinese and not be subject to Qing law. )</p>
      </li>
    </ul>
  </li>
  <li>
    <p>all of those <mark>undermined Qing's autonomy</mark></p>

    <ul>
      <li>other treaties are also signed after Treaty of Nanjing</li>
      <li>China gradually had to rely on European, <strong>opium trade continued</strong>, even more unequal treaties</li>
      <li>Christian missionaries are allowed to preach</li>
    </ul>
  </li>
</ul>

<p><strong>“Century of Humiliation”: the PRC’s narrative of national history</strong></p>

<ul>
  <li>
    <p>this period refers to when China lost control of territories to Westerners, and starts with the Opium War (approximately 1839 - 1949)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221110142249123.png" alt="image-20221110142249123" style="zoom:33%;" /></p>

    <p>emphasize national rejuvenation by the community party</p>
  </li>
  <li>
    <p>lots of <strong>current policy is driven by the fear of China losing to western power</strong></p>

    <ul>
      <li>China’s modern-day policies are driven by this “never again” mentality.</li>
    </ul>
  </li>
</ul>

<h2 id="from-taiping-to-sino-japanese-war">From Taiping to Sino-Japanese War</h2>

<p><strong>Previously on the Qing Opium Crisis</strong></p>

<ul>
  <li><strong>Canton System</strong> (1757-1839/42): regulated Qing’s trade with the West through Chinese hong merchants; restrictive yet worked well before the abolition of the EIC’s monopoly in 1834</li>
  <li>
    <p>Mccartney Embassy (1793) is Britain’s attempt to revise the Canton System, it features two expansionist empires, bringing competing notions of world order; taken as a misinformed example of <strong>British flexibility vs. Chinese rigidity and xenophobia</strong></p>
  </li>
  <li>British E. India Company ships opium to Qing, in exchange for silver and goods; Qing court orders opium confiscated from British and destroyed, triggering Opium War (1839-42), <strong>Treaty of Nanjing</strong> is first of many <strong>‘unequal treaties’</strong></li>
</ul>

<p><strong>Domestic Problems under Qing</strong>: towards later rebellion</p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115131925018.png" alt="image-20221115131925018" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>recall that Qing at its best time (High Qing) had large territory and high standard of living</p>

    <ul>
      <li>around the turn 19 century, government failures and international relations caused trouble</li>
    </ul>
  </li>
  <li>
    <p>now, <strong>silver became scarce</strong> after 1800—especially due to silver outflows from China in order to purchase British EIC opium.</p>

    <ul>
      <li>silver scarcity meant more copper cash required to <strong>pay same taxes (in silver)</strong>.
        <ul>
          <li>in 1750: one tael of silver needed 780 copper cash</li>
          <li>in 1838: one tael of silver needed 1,650 copper cash.</li>
          <li>in effect, silver scarcity meant tax burden more than doubled</li>
        </ul>
      </li>
      <li>people can’t pay → taxes more burden on taxpayers → tax evasion → less governments revenue → weaker government</li>
    </ul>
  </li>
  <li>
    <p>source of internal unrest: <strong>limited resource and population pressure</strong></p>

    <ul>
      <li>recall end of Ming had climate change and wars, where starting from Qing 1640 increases</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115132218907.png" alt="image-20221115132218907" style="zoom:33%;" /></p>

    <ul>
      <li>however, as more population <strong>amount of arable land is still fixed</strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115132322946.png" alt="image-20221115132322946" style="zoom:33%;" /></p>

    <p>hence this lead to environment degradation and competition for resources</p>

    <ul>
      <li>unstable water ecosystems due to intense land reclamation</li>
      <li>deforestation = erosion, flooding, and desertification = 30,000 died in Kaifeng due to flooding. From 1645 to 1855, a flood almost every 1.89</li>
    </ul>
  </li>
  <li>
    <p><strong>talent glut</strong> = talent grows faster than the number of jobs available = students being jobless</p>

    <ul>
      <li>lower degree holders: 40k in 1400; 600k in 1700; over 1 million in 1800 (but gov only fixed 20k official posts)</li>
      <li>a social problem, hence <strong>causing those unemployed/underpaid people indignant</strong>, who later contributes to rebellion (combined with factors above)</li>
    </ul>
  </li>
  <li>
    <p>two of the largest (many) <strong>civil wars</strong> that occurred in Qing in 19th century</p>

    <ul>
      <li><strong>Taiping</strong> rebellion (1850-64)</li>
      <li><strong>Nian</strong> rebellion (1851-68)</li>
    </ul>
  </li>
</ul>

<p><strong>Outbreaks of Many Rebellions</strong></p>

<ul>
  <li>
    <p><strong>White lotus rebellion</strong> (1796-1804)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115133127394.png" alt="image-20221115133127394" style="zoom:25%;" /></p>

    <ul>
      <li>White Lotus Society: a millenarian Buddhist secret society that first appeared in the Yuan dynasty</li>
      <li>help poor people, women played leadership roles</li>
      <li>political movement of grass roots society in the area shown above, and organized them into rebel movement
        <ul>
          <li>again a <strong>result of economic strain, overpopulation, government corruption, etc</strong></li>
          <li>impoverished peasants feeling indignant</li>
        </ul>
      </li>
      <li>took 8 year to take them out and spent 5 years of government = further stress in government</li>
    </ul>
  </li>
  <li>
    <p><strong>Eight Trigram</strong>: another secret society</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115133335829.png" alt="image-20221115133335829" style="zoom:25%;" /></p>

    <ul>
      <li>in 1813, off-shoot of white lotus, again became a rebellion and many people died</li>
    </ul>
  </li>
  <li>
    <p><strong>Triads</strong></p>

    <ul>
      <li>impoverished, hence people formed fraternity group = anti-Manchu and pro-Han</li>
      <li>smuggling activity and armed quarrels = quite militant group of people</li>
    </ul>
  </li>
  <li>
    <p><strong>Taiping Rebellion</strong> (more detail next)</p>
  </li>
  <li>
    <p>other rebellions such as</p>

    <ul>
      <li><strong>Nian Rebellion</strong>
        <ul>
          <li>crushed by Generals <strong>Zuo Zongtang 左宗棠</strong> and Li Hongzhang 李鸿章</li>
          <li>chef invented that General Zuo’s Chicken 左宗棠鸡 is from the same Hunan province, has nothing to do with Zuo Zongtang as person</li>
        </ul>
      </li>
      <li><strong>Panthay Rebellion</strong>: ethnic tension between Muslim and Manchu</li>
      <li><strong>Ya’qub Beg Khante</strong>: turkey had Muslim establish autonomous regime, then suppressed by Zuo Zongtang</li>
    </ul>
  </li>
</ul>

<p><strong>Structural Difficulties of Managing Crisis</strong></p>

<ul>
  <li>country too large and too populous
    <ul>
      <li>many ethnic groups, and population <strong>outpaced administrative control</strong></li>
      <li>limitations of bureaucratic efficiency before telegraph and rail (e.g. wrong info from local)</li>
    </ul>
  </li>
  <li>competing national and <strong>local</strong> interests
    <ul>
      <li>e.g., local officials overreported rebel threats to siphon funds.</li>
    </ul>
  </li>
</ul>

<p><strong>Taiping Heavenly Kingdom</strong></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115134718029.png" alt="image-20221115134718029" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>greatest revolution the world has yet seen</p>

    <ul>
      <li>people are positive about the revolution as being confident in the portrayal</li>
      <li>involves both peasants and failed exam takers, being indignant of the stresses</li>
    </ul>
  </li>
  <li>
    <p>starts from the South: where may conflicts are from locals and <strong>Hakka</strong> (guest, ethnically Han who fled from north to South to evade from Xiongnu during the Han dynasty)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115134952213.png" alt="image-20221115134952213" style="zoom:33%;" /></p>

    <p>as newcomers, Hakkas are seen to compete resources and not welcomed by locals</p>

    <ul>
      <li>architecture themsleves with a hole is for defensive purpose = tension with the locals</li>
      <li>can be found in Guandong, Guangxi, and Fujian</li>
    </ul>
  </li>
  <li>
    <p><strong>Hong Xiuquan = a Hakka who organized the rebellion</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115170953866.png" alt="image-20221115170953866" style="zoom:33%;" /></p>

    <ul>
      <li>from a poor family,  failed examination four times.
        <ul>
          <li>after failed the 3 time = had dream vision who when he was the second son of God, and brother of Jesus.</li>
        </ul>
      </li>
      <li>then he
        <ul>
          <li>smashed shrines dedicated to <strong>Buddhist and Confucian</strong> worship.</li>
          <li>added <strong>anti-Manchu strain to his preaching</strong></li>
          <li>calling people to rise up and defeat the Manchu rulers</li>
        </ul>
      </li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115135542705.png" alt="image-20221115135542705" style="zoom:33%;" /></p>

    <p>eventually they expanded and included people outside of Hakka and went all the way to Nanjing, and made it a capital. There, he urges for</p>

    <ul>
      <li><mark>equalization</mark> of land holdings, <mark>equality</mark> of men and women</li>
      <li><mark>equal</mark> exam but test on stuff such as translation of bibles instead Confucian classics</li>
      <li><mark>Utopian</mark> view of the society</li>
    </ul>
  </li>
  <li>
    <p><strong>Western reactions to Taiping</strong>: Taiping armies known for their puritanical zeal</p>

    <ul>
      <li>western (both english and american) missionaries hoped Taiping will succeed
        <ul>
          <li>Hong Xiuquan claims to be related to Jesus = hope it will succeed and convert Chinese to Christian</li>
        </ul>
      </li>
      <li>European liked to because it is radial and revolutionary (devotion of equality)</li>
      <li>English government: was neutral and they wanted just trade. So they took advantage of it help Chinese to quell the rebellion and in turn <strong>take more privileges in trade</strong> (heavily dependent in trading in China and US)
        <ul>
          <li>note that US civil war is also during 1681 = less supply/trade from US; and similarly Taiping broke out in China</li>
          <li>English fear that their trade econ will collpase as they are major traders</li>
          <li>so they need to interfere somewhere = decided to intervene in China</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>but the largest credit of quelling Taiping still goes to <mark>Han Chinese officials</mark></p>

    <ul>
      <li>note those Han Chinese <mark>gentry</mark> (not people who failed, who usually join rebels) <mark>have not much anti-Manchu thougths</mark></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115140518034.png" alt="image-20221115140518034" style="zoom: 25%;" /></p>

    <ul>
      <li>he built an army from his home province = Hunan, hence Xiang Army</li>
      <li><strong>financed through local gentry</strong> instead of Manchu lead government</li>
      <li>indicates a <mark>shifting (military) power to locals</mark> from the center/government (e.g. rise of Han)</li>
    </ul>
  </li>
  <li>
    <p>the Taiping rebellion caused</p>

    <ul>
      <li>nation-wide: affected 16 out of 18 provinces</li>
      <li>unprecedented death toll: <strong>at least 20 million, up to 40 million</strong>. (by comparison, 620,000 died in the American Civil War)</li>
      <li>devastated the economically important lands south of Yangzi.</li>
      <li>so close to bring down the Qing dynasty, but didn’t.
        <ul>
          <li>Recall that the pattern of rebellion replacing dynasty was: warlords rise in power to quell the rebellion, but after they compete and winner takes a new Dynasty</li>
          <li>Now, the army still remains to be part of the imperial army. But after this local provincial army became so strong = replacing Manchu garrisons as major power = <mark>shift of balance of power from government to the locals</mark></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>this rebellion in total lasted 15 years, because Qing was also busy doing something else: fighting the Europeans</p>
  </li>
</ul>

<p><strong>The Second Opium War</strong> (1856-1860). Also called Arrow War</p>

<ul>
  <li>
    <p>right in the middle of Taiping Rebellion, <strong>China was also engaging with great Britain and France</strong> in addition to Taiping rebels</p>
  </li>
  <li>
    <p>called Arrow War as it is named by the Ship in suspicion of piracy, how ever it is layer registered with English.</p>

    <ul>
      <li>British use this incident as <strong>pretext to start war</strong>, to add more unequal terms to treaty</li>
      <li>at the same time, a French was to be executed, hence French joined as well</li>
    </ul>
  </li>
  <li>
    <p>ended with (second) defeat of Qing government to the English</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115141216430.png" alt="image-20221115141216430" style="zoom:33%;" /></p>

    <p>resulted in</p>

    <ul>
      <li>burn down of Summer Palance (built for Qianlong and his successors and had European style architectures)</li>
      <li>more unequal treaty signed (<strong>1860 Convention of Peking</strong>), more ports opened, and huge indemnity. Some Chinese being taken as slaves to the Americas (indentured)
        <ul>
          <li>Indentured servitude is a form of labor where an individual is under contract to work without a salary to repay an indenture or loan</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Self-Strengthening Movement</strong> 1861-95</p>

<ul>
  <li>
    <p>Qing fear that dynasty is going to collapse soon. Most urgently they wanted to <strong>reform the military and modernize with western tech</strong></p>
  </li>
  <li>
    <p>Li Hongzhang as an example:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115141726795.png" alt="image-20221115141726795" style="zoom:33%;" /></p>

    <ul>
      <li>equipped his army with <strong>imported weapons</strong> and developed <strong>modern</strong> provincial armies to defeat Taiping rebels.</li>
    </ul>
  </li>
  <li>
    <p>this promoted Qing to</p>

    <ul>
      <li>set up <strong>factories</strong> to build western style weapons and warships</li>
      <li><strong>education</strong> infrastructure to study western inventions/thoughts
        <ul>
          <li>e.g. schools of science and technology in the Fuzhou shipyard and Jiangnan arsenal.</li>
          <li>school of foreign languages.</li>
          <li>study abroad programs in America, England, and France</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>knowledge about the west gradually improved</p>

    <ul>
      <li>e.g. chinese kids sent to educational mission to the US, including going to Yale and Columbia</li>
      <li>China then also added structures such as railways</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115142103609.png" alt="image-20221115142103609" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>First Sino-Japanese War</strong> (1894-95)</p>

<ul>
  <li>
    <p>though the self-improvement movement is doing well, until Japan defeated China</p>

    <ul>
      <li>before, China thought Japan is a little brother</li>
      <li>now, there is this humiliating defeat</li>
    </ul>
  </li>
  <li>
    <p>Meiji emperor during the time: <strong>conflicted with Qing influence of Korea, and engaged into war</strong></p>

    <ul>
      <li>Korea had long been China’s most important client state, but its strategic location opposite the Japanese islands and its natural resources of coal and iron attracted Japan’s interest.</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115142336795.png" alt="image-20221115142336795" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>war broke out mostly at sea, and Japan had China sign a treaty after China’s defeat</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221115142436096.png" alt="image-20221115142436096" style="zoom:33%;" /></p>

    <ul>
      <li>term includes ceding Taiwan to Japan, and recognition of Korean Independence (short-lived, for it was colonized by Japan in 1910)</li>
    </ul>
  </li>
  <li>
    <p>Qing realized that its nation is weak, but why even after self-improvement movement?</p>

    <ul>
      <li>some people see self-improvement movement as useless</li>
      <li>others attribute to problems such as corruption + lack of training and organization/literacy of Chinese troops despite advance in technology</li>
    </ul>

    <p>regardless, the failure of the Qing in its late decades also contribute</p>
  </li>
</ul>

<h1 id="from-empire-to-republic">From Empire to Republic</h1>

<p><strong>Previously on the late Qing crisis</strong></p>

<ul>
  <li>
    <p>Just prior to the start of the Opium War, silver shortage, ecological stress, have pushed <strong>Qing China into general economic crisis</strong>; peasant unrest and tax revolts proliferate</p>
  </li>
  <li>Taiping Rebellion (1850-64): failed scholars espousing heterodox Christianity (Hong Xiuquan)
    <ul>
      <li>had motivation for equality, but also the most destructive rebellion</li>
      <li>long lasting rebellions = resulting in Qing dynasty power is severely undermined</li>
    </ul>
  </li>
  <li>Han <strong>provincial gentry</strong> come to aid of the dynasty, raised local armies to defeat Taipings
    <ul>
      <li>also came to initiate <strong>self-strengthening movement to learn western technologies</strong></li>
      <li>e.g. Li Hongzhang, builds western arsenals, weapons, and translate western texts.</li>
    </ul>
  </li>
  <li><strong>Arrow War</strong> (1856-60) at the same time, also tension with western especially the English and French
    <ul>
      <li>Qing lost again to the Anglo-French force, and have <strong>summer palace burnt and more unequal treaties signed</strong></li>
    </ul>
  </li>
  <li><strong>Sino-Japanese War</strong> (1894-1895): Chinese defeat again, still due to technology diff or lack of military organization/coordination? Don’t know.
    <ul>
      <li>defeat has deep consequence = realizes how astonishing weak the empire was = Chinese <strong>loss of faith in Manchus</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Hundred Day’s Reform</strong>: and later a serious of rebellions/reformed until 1911 <mark>Xinhai Rebellion 辛亥革命</mark> ended Qing in 1912</p>

<ul>
  <li>
    <p>when China lost Sino-Japan War, western realized Chinese weakness and were competing for territory.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117132336654.png" alt="image-20221117132336654" style="zoom: 25%;" /></p>

    <p>previously they were mostly occupying parts of South-East Asia. But seeing how easily China is defeated by Japan, <strong>westerns are also lurking and China is “cut up”</strong> during 1902-1903:</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117132534877.png" alt="image-20221117132534877" style="zoom: 25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117132748517.png" alt="image-20221117132748517" style="zoom:25%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
  <li>
    <p>How to save China? First attempt by <strong>Hundred Day’s Reform</strong> 戊戌(Wu Xu)變法</p>

    <ul>
      <li>was a failed 103-day national, <strong>cultural, political, and educational reform movement</strong> that occurred from 11 June to 22 September 1898 during the late Qing dynasty</li>
      <li>they heard the humiliating signing of Sino-Japanese defeat treaty, and became indignant</li>
      <li>in 1898, they launched a reform funded by Guangxu Emperor (but <mark>Cixi</mark> is power behind the throne = most powerful since Wudi)
        <ul>
          <li>Cixi’s aim is to maintain her role and power, rather than help people. e.g. reform policies approved usually for self-serving purposes</li>
          <li>but in 1898 she allowed Guangxu to rule on his own, and K and L are asked to help with the reform</li>
        </ul>
      </li>
    </ul>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117133001371.png" alt="image-20221117133001371" style="zoom: 25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117133109537.png" alt="image-20221117133109537" style="zoom:25%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <ul>
      <li>she then was afraid that Guangxu’s reform would undermine she power and Manchu, so locked up Guangxu and the reform proponents fled to Japan = hence only 103 days of reform</li>
      <li>Cixi = <mark>ended last chance of China to change peacefully</mark></li>
    </ul>
  </li>
</ul>

<p><strong>The Boxer Uprising</strong> (1900): purge foreigners and Christians. It was an anti-foreign, anti-colonial, and anti-Christian uprising in China between 1899 and 1901, towards the end of the Qing dynasty</p>

<ul>
  <li>
    <p>Chinese blamed failing/natural disasters to be caused by foreigners: e.g. wrongly influenced Chinese culture and practices</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117133822915.png" alt="image-20221117133822915" style="zoom:25%;" /></p>

    <p>note that mostly peasants, and called “Boxer” because they mostly fight with martial arts</p>
  </li>
  <li>
    <p>emerged also in 1898 after 100 days reform, and hates foreigners and converts (to follow foreign religions such as Christianity)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117134110487.png" alt="image-20221117134110487" style="zoom:25%;" /></p>

    <p>as you see it is technically “not a religion problem”, but the <strong>socio-economic problem</strong> that whoever converted to Christian had this unfair protection and foreign backing.</p>
  </li>
  <li>
    <p>The events came to a head in June 1900 when Boxer fighters, convinced they were invulnerable to foreign weapons, <strong>converged on Beijing with the slogan “Support the Qing government and exterminate the foreigners.”</strong></p>
  </li>
  <li>
    <p>To counter this rebellion against foreigners, an <mark>Eight Nation Alliance</mark> of American, Austro-Hungarian, British, French, German, Italian, Japanese and Russian troops moved into China to lift the siege, and afterwards <strong>took over the palace in Beijing</strong></p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117134700506.png" alt="image-20221117134700506" style="zoom:25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117134737638.png" alt="image-20221117134737638" style="zoom:25%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <p>and again Chinese’ loss lead to the Boxer Protocol in 1901 = total indemnity of 450 million</p>
  </li>
  <li>
    <p><em>again</em> spurred national debate of how China can still move forward</p>
  </li>
</ul>

<p><strong>Third and Last Attempt to Save China</strong>: 1911-1912 辛亥革命</p>

<ul>
  <li>
    <p>so far we see efforts starting from self-strengthening, 100 days reform, boxer uprising, all failed</p>
  </li>
  <li>
    <p><strong>people needed <em>will</em> reformation</strong> = Zou Rong, an anti-Manchu nationalist and published the <em>Revolutionary Army</em> journal</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117135352207.png" alt="image-20221117135352207" style="zoom: 33%;" /></p>

    <ul>
      <li>convinced that wipe out Manchus is necessary, and called out in the <em>Revolutionary Army</em> journal for comrades</li>
      <li>advocated to establish the Chinese republic</li>
    </ul>
  </li>
  <li>
    <p><strong>Sun Yat-sen</strong>: the most prominent anti-Manchu revolutionary and a founding member of anti-Machu</p>

    <ul>
      <li>believes the best way to overthrow is to <strong><em>ally with the secrete society</em></strong> (e.g. the triad, etc)</li>
      <li>he spent most of the time traveling to many foreign grounds = to gain financial support to fund uprisings in China</li>
      <li>in 1905-12 he formed the Revolutionary Alliance = people with many different ideals such as socialist and republican, but all allied to <mark>overthrow the Manchu rulers and revive China</mark>. Later this alliance is transformed into <strong>Guomingdang (KMT)</strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117135747597.png" alt="image-20221117135747597" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>his <mark>three main principles</mark> are</p>

    <ul>
      <li><strong>nationalism</strong> 民族主义 = members of the country to pay highest loyalty to the nation, rather than to family, etc.</li>
      <li><strong>democracy</strong> 民权主义 = to him it means western constitutional government</li>
      <li><strong>livelihood of the people</strong> 民生主义 = social welfare such as food, housing, transportation, and etc.</li>
    </ul>
  </li>
  <li>
    <p>revolutionary broke out in 1911 (Xinhai Revolution), many provinces declared independence, and <mark>ended Qing in 1912</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117140318311.png" alt="image-20221117140318311" style="zoom:25%;" /></p>

    <ul>
      <li>Qing gave up in only 4 month</li>
      <li>“founding father” at that time was at Colorado, so some people say this title of “founding father” is overestimated</li>
      <li>On 1 January 1912, the National Assembly declared the establishment of the <strong>Republic of China,</strong> with Sun Yat-sen, leader of the Tongmenghui (United League), as President of the Republic.</li>
    </ul>
  </li>
  <li>
    <p>But Sun quickly gave away the power to <mark>Yuan Shikai</mark> = no longer the Chinese coin of hole in the middle = western style coin</p>

    <ul>
      <li>because there was this promise that: Sun would resign in favor of Yuan Shikai, who would become President of the new national government, <em>if Yuan could secure the abdication of the Qing emperor</em></li>
      <li>Even Sun had gather fund, had ideals, but he didn’t have much military power. Hence Yuan Shikai, who was appointed as the leader of the powerful Beiyang Army by Qing</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117140804031.png" alt="image-20221117140804031" style="zoom: 25%;" /></p>
  </li>
  <li>
    <p>however, Yuan Shikai died early before he could consolidate a legitimate central government before his death in 1916, led to decades of political division and <mark>warlordism</mark>, including an attempt at imperial restoration = China entered <mark>again the unrestful warlord period (1916 - 1927)</mark></p>
  </li>
</ul>

<p><strong>Change of Beliefs and Practices in China</strong> during 1915-1926</p>

<ul>
  <li>
    <p>a new periodical serving as a source of new ideology and heavily influenced the New Culture Movement and May 4th protests later</p>

    <ul>
      <li><strong>The May Fourth Movement</strong> was a Chinese anti-imperialist, cultural, and political movement which grew out of student protests in Beijing on May 4, 1919. Students came to protest the Chinese government’s weak response to the Treaty of Versailles decision to allow Japan to retain territories in Shandong that had been surrendered to Germany after the Siege of Tsingtao in 1914.</li>
      <li>The New Culture Movement (Chinese: 新文化運動) was a movement in China in the 1910s and 1920s that <strong>criticized classical Chinese ideas</strong> and promoted a new Chinese culture based upon progressive, <mark>modern and western ideals like democracy and science</mark>.</li>
    </ul>
  </li>
  <li>
    <p>the new periodical: <em>New Youth</em> was a Chinese literary magazine founded by <mark>Chen Duxiu</mark> and published between 1915 and 1926</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117141302470.png" alt="image-20221117141302470" style="zoom:25%;" /></p>

    <ul>
      <li>
        <p>founded by Chen Duxiu, advocate for individual freedom, and some major ideologies including rejecting Confucianism and <strong>crave the energy of youth</strong> (hence challenge the Confucius idea of young obey elders), promoting the young to think by themselves</p>
      </li>
      <li>
        <p>advocated serious ideas in vernacular language v.s. in the past those serious writings tend to use classical styles</p>
      </li>
      <li>
        <p>also had works from both <mark>Lu Xun</mark> and <mark>Mao Zedong</mark></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117171901925.png" alt="image-20221117171901925" style="zoom:25%;" /></p>

        <p>note that even though Japan is disliked due to its invasion, <strong>Japan was still the first choice of over sea study</strong> because it is a) close b) had modernized tech, and c) similar language</p>
      </li>
      <li>
        <p>the journal also focused the <mark>adoption of Western ideals</mark> of <strong>“Mr. Science”</strong> (賽先生) and <strong>“Mr. Democracy”</strong> (德先生) in place of “Mr. Confucius” in order to strengthen the new nation (though science was better received)</p>
      </li>
    </ul>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117141125330.png" alt="image-20221117141125330" style="zoom: 33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117141129267.png" alt="image-20221117141129267" style="zoom: 33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
  <li>
    <p>during this period, many male also came out to speak for women’s rights (e.g. Lu Xun, a male and He Zhen, a female)</p>
  </li>
</ul>

<p><strong>May 4th Protests</strong>, 1919</p>

<ul>
  <li>
    <p>recall that China joined allies, so when German defeated territories in China should be handed back to China, but the result was disappointing that German concessions will be handed over to Japan due to the Treaty of Versailles decision</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117141804283.png" alt="image-20221117141804283" style="zoom:25%;" /></p>
  </li>
  <li>
    <p>students were very angry = government arrested student leaders = general consensus is reached among the students such as <strong>nationalism, patriotism, science, and democracy</strong>, and enemies are imperialism, warlordism, etc.</p>
  </li>
</ul>

<p><strong>Founding Chinese Communist Party (CCP)</strong> in 1921</p>

<ul>
  <li>
    <p>at the same time WWI finished, where <mark>Marxism and Leninism is successful</mark> as seen in soviet union</p>

    <ul>
      <li>
        <p>also had anti-western, so liked soviet union a lot</p>
      </li>
      <li>
        <p>Marxism and Leninism shows capable of revolution = exactly what China need</p>
      </li>
    </ul>
  </li>
  <li>
    <p>again founded by Li Daozhao and Chen Duxiu, for Maxism (Mao Zedong also influenced by Li Daozhao)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221117142227411.png" alt="image-20221117142227411" style="zoom:25%;" /></p>
  </li>
  <li>
    <p>as we know later, under the leadership of Mao Zedong, the <strong>CCP emerged victorious in the Chinese Civil War against the Kuomintang</strong>, and in <mark>1949 October 1st Mao proclaimed the establishment of the People's Republic of China</mark>.</p>
  </li>
</ul>

<hr />

<p><strong>Into the Modern China</strong>: a short review and preview of major events until 1949. In general, this is the period with <strong>much more foreign influence</strong> compared to past China/Dynasty changes. Another difference is that before: rebellion = restoration of past glory. But now modern = completely new motivation/ideology</p>

<ul>
  <li>1841 - first opium war</li>
  <li>1860s - second opium war: imperial incursions and increasing christian missionaries, English, etc in China
    <ul>
      <li>christian autonomy in local society here is central to the cause of the later boxer up-rising</li>
    </ul>
  </li>
  <li>1861-95 self-strengthening movement</li>
  <li>1895 - Sino-Japanese war: watershed moment that losing to Japan (long seen as vassal state) made Chinese realizes its weakness + anti-Manchu</li>
  <li>1898 - Hundred Days reform</li>
  <li>1900 - Boxer rebellion. Can think of as “thugs” trying to bring back Chinese ruling. Quelled by the eight-nation alliance</li>
  <li>1911 - <mark>Xinhai Revolution</mark>. Starting in Wushang - we’ve rebelled, who will join us - lots of provinces joined quickly
    <ul>
      <li>in Feb 1912 Qing abdicated the throne, and <strong>Republic of China found 1912 Jan 1</strong> (recognized in Taiwan), and Sun Yat-sen was the head</li>
      <li>however, Sun didn’t have military power but the ideals, hence collabed with Yuan Shikai = will be the new president if his military can help</li>
    </ul>
  </li>
  <li>1916 - 1927 warlord period of decentrailized china = time of disunity
    <ul>
      <li>Yuan Shikai died in 1916 - very short reign before the government is coalesced</li>
    </ul>
  </li>
  <li>1921-24: KMT allied with CCP to march north and subdue the warlords and reunified the coountry
    <ul>
      <li>In 1921, Chen Duxiu and Li Dazhao led the founding of the CCP with the help of the Far Eastern Bureau of the Communist Party of the Soviet Union and Far Eastern Secretariat of the Communist International.</li>
      <li>For the first six years of its history, the CCP aligned itself with the Kuomintang (KMT) as the organized left-wing of the larger nationalist movement.</li>
    </ul>
  </li>
  <li>1919 <strong>May 4th movement</strong>: territory not given back to China - highly literary movement of intellectuals such as Lu Xun and Cheng Duxiu trying to <strong>incorporate western ideas into China governing</strong></li>
  <li>1925: Sun Yat-sen died, who is a key figure making the alliance of CCP and KMT together, things goes wrong between CCP and KMT
    <ul>
      <li>General Chiang Kai-shek, who became the Chairman of the Kuomintang after Sun’s death and subsequent power struggle in 1925, began the Northern Expedition in 1926 to overthrow the Beiyang government.</li>
      <li>In 1927, Chiang moved the nationalist government to Nanking and <strong>purged the CCP, beginning with the <mark>Shanghai massacre</mark></strong>. The latter event forced the CCP and KMT’s left-wing into armed rebellion, marking the beginning of the <mark>Chinese Civil War</mark></li>
      <li>basically KMT turned against CCP. Bear in mind that former had strongholds in cities, whereas latter whose ideal is communist only yet had supports by peasants in countrysides</li>
    </ul>
  </li>
  <li>1927 - 1949: a lot of upheaval and disunity
    <ul>
      <li>CCP fighting with KMT, and Japan is also attacking</li>
      <li>The <mark>Second Sino-Japanese War (1937–1945, WWII)</mark> or War of Resistance (Chinese term) was a military conflict that was primarily waged between the Republic of China and the Empire of Japan.</li>
    </ul>
  </li>
  <li>1949 - <strong>CCP won</strong> and start of People Republic of China
    <ul>
      <li>The Communists gained control of mainland China and established the People’s Republic of China in 1949, forcing the leadership of the Republic of China to retreat to the island of Taiwan.</li>
    </ul>
  </li>
</ul>

<h2 id="the-nanjing-decade-1927---1937">The Nanjing Decade (1927 - 1937)</h2>

<blockquote>
  <p>Some key terms in this period</p>

  <ul>
    <li><strong>Fascism</strong> is a authoritarian and ultra-nationalist political ideology, characterized by a dictatorial leader, centralized autocracy, militarism, forcible suppression of opposition, belief in a natural social hierarchy, subordination of  individual interest for the perceived good of the nation and race, and strong regimentation of society and the economy</li>
    <li><strong>Imperialism</strong> is the state policy, practice, or advocacy of extending power and dominion, especially by direct territorial acquisition or by gaining political and economic control of other areas, often through employing hard power, but also soft power.</li>
    <li><strong>Warlord Era</strong>
      <ul>
        <li>The <strong>Warlord Era</strong> was a period in the history of the <strong>Republic of China</strong> (not PRC) when control of the <strong>country was divided among former military cliques of the Beiyang Army</strong> and other regional factions from 1916 to 1928.</li>
        <li>In historiography, the Warlord Era began in 1916 upon the <strong>death of Yuan Shikai,</strong> the de facto dictator of China after the Xinhai Revolution overthrew the Qing dynasty and established the Republic of China in 1912. <strong>Yuan’s sudden death created a power vacuum</strong> that spread across the country hence the warlord area.</li>
      </ul>
    </li>
    <li>The <strong>Nanjing Decade</strong>:
      <ul>
        <li>The Nanjing decade (also 南京十年; 黃金十年) is an informal name for the decade from 1927 (or 1928) to 1937 in the People’s Republic of China. It began when Nationalist, or even “Confucian Fascist” Generalissimo <strong>Chiang Kai-shek took Nanjing</strong> from warlord Sun Chuanfang halfway through the Northern Expedition in 1927. Chiang declared it <strong>to be the national capital</strong> despite the existence of a left-wing Nationalist government in Wuhan.</li>
        <li>The Nanjing decade was marked by both progress and frustration.
          <ul>
            <li>The period was far more stable than the preceding Warlord Era. There was <strong>enough stability</strong> to allow economic growth and the start of ambitious government projects. Entrepreneurs, educators, lawyers, doctors, and other professionals were more free to create modern institutions than at any earlier time.</li>
            <li>However, the Nationalist government also suppressed dissent, corruption and nepotism were rampant and <strong>revolts</strong> broke out in several provinces; internal conflicts also perpetuated within the government. The <mark>Nationalists</mark> were never able to fully pacify the <mark>Chinese Communist Party</mark>, and struggled to address the widespread unrest and protests over their failure to check Japanese aggression</li>
          </ul>
        </li>
        <li>The decade ended with the outbreak of the <strong>Second Sino-Japanese War</strong> in 1937 and the retreat of the Nationalist government to Wuhan.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>There will be a lot on student protests, resistance, etc.</p>

<ul>
  <li>Racism in Shanghai, Chinese tormented by powerful imperial forces.</li>
  <li>On going struggle between <strong>KMT</strong>, <strong>CCP</strong>, and the encroaching <strong>Japanese</strong> Imperialism</li>
</ul>

<p><strong>The Warlord Era</strong></p>

<ul>
  <li>
    <p>In 1915 <mark>Yuan Shicai with his Beiyang army</mark> has declared himself a new emperor, with the intention of finding new dynasty = Republic of China (the one with Sun Yatsen). But he died soon in 1916 so China is divided into regions ruled by separate generals and hence a <strong>warlord era</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129132107080.png" alt="image-20221129132107080" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>KMT revival with Sun Yat-sen’s return in 1917, and Communist continued their activities</p>

    <ul>
      <li>intense ideology struggle on both sides, nationalist v.s. communist</li>
      <li>on KMT, some were believing in fascist, but also some sympathizes with communist. Same on CCP’s side</li>
    </ul>
  </li>
  <li>
    <p>but both KMT and CCP agreed on bourgeosis revolution (as with Marxism–Leninism in Soviet), hence they cooperate to create <mark>United Front</mark></p>

    <ul>
      <li>
        <p>launch northern expedition against warlords in the north</p>
      </li>
      <li>
        <p>before the beginning of expedition, some tension: nationist turn on CCP because of its popularity</p>
      </li>
      <li>
        <p>e.g. May 30, KMT police opened fire, leading to strikes in many cities</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129132539074.png" alt="image-20221129132539074" style="zoom:50%;" /></p>

        <p>which made CCP grow even more in power/popularity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>In general, many KMT encouraged CCP to cooperate, while some right-wing from KMT do not trust CCP = cannot co-exist</p>

    <ul>
      <li>
        <p>hence KMT into two major wings: <strong>left wing sides with CCP, while right wing believes in militarism</strong></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129222115881.png" alt="image-20221129222115881" style="zoom:33%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>then in 1927 = White Terror = surprise attacks KMT arrested hundreds of CCP people and executed them = end of KMT support from people</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129133109511.png" alt="image-20221129133109511" style="zoom:50%;" /></p>

    <ul>
      <li>many of these purges were executed by the more extreme factions in KMT</li>
    </ul>
  </li>
</ul>

<p><strong>Shift towards an Agrarian Communism</strong></p>

<ul>
  <li>
    <p>believes that factory workers would be the ones for Communist revolution in China</p>

    <ul>
      <li>Mao’s emphasis of investigation = historical materialism from Marxist</li>
      <li>Firmly situated <mark>peasants as revolutionary subjects</mark> - and moved Chinese communism towards an agrarian focus.</li>
      <li>Saw ritualistic and <mark>violent struggle against class enemies as critical to revolution</mark>. This is a key difference from KMT socialists, who believed in “cooperatives to support “people’s livelihood”</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129133412885.png" alt="image-20221129133412885" style="zoom:50%;" /></p>
  </li>
</ul>

<p><strong>End of the First United Front</strong></p>

<ul>
  <li>
    <p>CCP and KMT civil war, and CCP establish bases/soviets between 1927 to 1933 (the main one being Mao’s base)</p>
  </li>
  <li>
    <p>worried by CCP’s growing support even in urban areas, Chiang Kai-shek <strong>launched four massive military campaigns</strong> to encircle and annihilate the Jiangxi Soviet, all of which were repulsed by the communists with their <strong>guerrilla tactics</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129133928967.png" alt="image-20221129133928967" style="zoom:33%;" /></p>

    <ul>
      <li>however eventually CCP had to retreat, hence this long march shown above = recorded as their heroic struggle</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Diff of KMT v.s. CCP in Economy</strong></p>

<ul>
  <li>China had econ problem such as famine before, and both parties are concerned about imperialism that Chinese cannot survive. But they differ in <strong>how to solve this problem</strong></li>
  <li>nationalist believe
    <ul>
      <li>accumulate <strong>national</strong> wealth, aim to annihilate dependencies on others hence social strength/<strong>self-sufficiency</strong></li>
      <li>econ independennce = autarky = <strong>national prosperity is precondition for happiness of people</strong>
        <ul>
          <li>wanted export lead economy. but this did not work with the great depression in 1932 (Wall street crush), so silk and tea lost much markets, and Japan’s influence made China grow (China exporting raw materials to Japan while Japan grows in technology)</li>
        </ul>
      </li>
      <li>solve poverty by <strong>increasing production</strong> = need tech advancement = gives land questions
        <ul>
          <li>Left-wing (Wang Jingwei and others) wanted to build up land cooperatives</li>
          <li>Right-wing (Chiang Kai-Shek) more concerned with military control of villages</li>
        </ul>
      </li>
      <li>but there are <strong>conflicts with private industry,</strong> as they needs its own profit but nationalist needs national wealth</li>
      <li>does not care much about rural, but more central power</li>
    </ul>
  </li>
  <li>CCP focused on <strong>rural organization</strong>, land reform, and <strong>class struggle</strong>
    <ul>
      <li>global capitalism is the problem, and being semi-colonized = both landlords needs to be overthrown and wealth redistributed</li>
      <li>land reform program, redistribute to peasants</li>
      <li>focus a lot on <strong>women emancipation</strong> = e.g. local women can diverse their husbands, can own land, etc</li>
      <li>give peasants opportunity to read and write</li>
    </ul>
  </li>
</ul>

<p><strong>Diff in Military mobilization</strong></p>

<ul>
  <li>Mao’s Red Army: <strong>Guerrilla warfare</strong>
    <ul>
      <li>draw enemy into deep CCP territory such as mountains, so KMT becomes naturally split up due to landscapes</li>
      <li>red army are <strong>mostly consists of volunteers</strong>; and hold respect to civilians
        <ul>
          <li>Eight points of attention: do not hit or swear others, be honest about buying and selling, etc</li>
          <li>helped increase popularity = a lot of support from peasants + China had a large size of peasants</li>
        </ul>
      </li>
      <li>cooperate between army and people</li>
    </ul>
  </li>
  <li>Nationalist
    <ul>
      <li>advance altogether in one column, traditional approach</li>
      <li>more than half are <strong>soldiers, not civilians</strong> = Chaang Kai shek prioritized military discipline</li>
    </ul>
  </li>
</ul>

<p><strong>Factions within KMT</strong></p>

<ul>
  <li>
    <p>two big groups controlling the executive functions, CC Clique and Blue Shirt</p>
  </li>
  <li>
    <p>CC Clique v.s. Blue Shirts</p>

    <ul>
      <li>
        <p>CCP Clique return for Chinese past and vision for future. LHS has stuff from the past such as great wall, on the RHS tech advanced visions</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129140336691.png" alt="image-20221129140336691" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>Blue Shirt: emphasize of strngth and strong young people</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129140433293.png" alt="image-20221129140433293" style="zoom:33%;" /></p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>New Life Movement</strong> (nationalist)</p>

<ul>
  <li>
    <p>draw on four virtues, advocate that these virtues penetrate peoples life</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129140624061.png" alt="image-20221129140624061" style="zoom:33%;" /></p>

    <p>being productive at all times</p>
  </li>
  <li>
    <p>then Chiang Kai-shek’s Speech incorporates ideas of <strong>convergence of militarism and being productive in life</strong></p>

    <ul>
      <li>i.e. your individual behavior can hurt the wealth of the entire nation = you should stick to your social role. If everyone perform their social role, then not chaos and uprising associated with communist</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129140939040.png" alt="image-20221129140939040" style="zoom:50%;" /></p>

    <ul>
      <li>but still gained a lot critique from Confucianism:
        <ul>
          <li>New Life Movement was an attempt to <strong><em>suppress individual freedom</em></strong> and to impose a rigid and authoritarian ideology on the people = the movement’s emphasis on obedience and conformity was at odds with the principles of Confucianism, which emphasize personal responsibility and self-cultivation.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Left-Wing Cultural Movement</strong></p>

<ul>
  <li>
    <p>Marxist texts, concerned with every day life, and experimented with new literary forms</p>

    <ul>
      <li>aim to make literature appeal with the masses = new form of short novels</li>
    </ul>
  </li>
  <li>
    <p>two famous person her is <strong>Song Dingling and Mao Dun</strong></p>
  </li>
  <li>
    <p>but then left wing faced cracked down by 5 prominent being executed</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129141427721.png" alt="image-20221129141427721" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>overall had rich cultural production, both soft v.s. hard films as tools for politic education</p>
  </li>
  <li>
    <p>KMT supported tabloits and defamed Cai Chusheng (a famous women actor at that time) and her movie = committed suicide</p>

    <ul>
      <li>impact on later communist cultural production</li>
      <li>Mao: importance of appealing to the masses</li>
    </ul>
  </li>
</ul>

<p><strong>Japanese Imperialism</strong>: belief that Japan can relieve the influence from western by conquering Asian countries and ruling them</p>

<ul>
  <li>
    <p>Japan controlled a lot of due to their control of the Manchurian Railway</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129142010627.png" alt="image-20221129142010627" style="zoom: 33%;" /></p>

    <p>basically the 东北region</p>
  </li>
  <li>
    <p>Battle ensued when plain clothes Chinese</p>

    <ul>
      <li>eventually Japan won the right to police Shanghai</li>
      <li><strong>Japanese established Manchukuo within that Chinese region, which enraged CCP and KMT</strong></li>
    </ul>
  </li>
  <li>
    <p>then goes to the <strong>Second Sino-Japanese war</strong></p>

    <ul>
      <li>and <strong>Paul Robeson</strong> and our national anthem</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221129225545886.png" alt="image-20221129225545886" style="zoom:25%;" /></p>
  </li>
</ul>

<h2 id="wwii-and-second-sino-japanese-war-1937-1945">WWII and Second Sino-Japanese War (1937-1945)</h2>

<p><strong>Previously on the Republican Era</strong></p>

<ul>
  <li>1911-16 The first <strong>Republic</strong>
    <ul>
      <li>first Sino-Japanese war and Boxer protocol are national traumas</li>
      <li>1911 revolution: replace Qing and new government, and <strong>Sun Yat-Sen</strong> established the first Republic of China</li>
      <li>Sun soon replaced by Yuan Shicai, who had strong military. However Yuan died early in 1916, leading to warlord era</li>
    </ul>
  </li>
  <li>1916-27 The <strong>Warlord Era</strong>
    <ul>
      <li>Centered around Beijing, the <strong>New Culture Movement</strong> sees a new generation of intellectuals question the Confucian basis of social, moral, political traditions. Believe that those are the reason for Chinese’fall, and <strong>emphasized on Mr. Science and Mr. Democracy</strong></li>
      <li>May 4th Movement (1919) politicizes intellectuals (anti-imperialist movement)</li>
      <li><strong>CCP founded</strong> 1921 in Shanghai.</li>
    </ul>
  </li>
  <li>1927-37 The <strong>Nanjing decade</strong>
    <ul>
      <li>Chiang Kai-Shek (CKS) succeeded Sun and became KMT leader, and with (appear) allied with CCP to go on northern expedition to <strong>re-unite China</strong> from the warlords = <strong>First United Front</strong></li>
      <li>CKS with his strong military belief never trusted CCP, hence <strong>white terror</strong> in 1927, and CCP flee
        <ul>
          <li>retreated to Jiangxi Soviet. Stop of Soviet Union’s support to KMT, and <strong>Mao start to emerge as the leader in CCP</strong></li>
          <li>then KMT then tried campaigns to get rid of Communist in Jiangxi, and in the 4th occasion, CCP fled despite their encirclement and went on a <strong>long march</strong> (34-35) all the way to Yan’an, Shaanxi</li>
          <li>only a bunch of true faithful people in CCP left (a lot key figures including Mao, Zhou Enlai, Xi’s father, etc). But they collected lots of supports along the way</li>
          <li>CCP regroups, builds <strong>peasant support through land reform</strong>, uses mass movements to accomplish objectives</li>
        </ul>
      </li>
      <li>hence overall a unity after the chase, and KMT + CKS become the major government in China. CKS’s view is highly militaristic and productive, that through discipline in a top-down approach we can make ourselves stronger
        <ul>
          <li><strong>New Life Movement</strong> to impose discipline and morality on China’s society. Inspiration drawn from the Italian fascist and German Nazi movements</li>
          <li>also many cultural influence during the time: <strong>leftist intellectuals making films</strong>, etc.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Some Major Events</strong> in this period</p>

  <ul>
    <li>
      <p>In 1931, the <a href="https://en.wikipedia.org/wiki/Mukden_Incident">Mukden Incident</a> helped spark the Japanese invasion of Manchuria. The Chinese were defeated and Japan created a new puppet state, <mark>Manchukuo</mark>; many historians cite as the beginning of the war</p>
    </li>
    <li>The <strong>beginning of the war</strong> is conventionally dated to the <mark>Marco Polo Bridge Incident 卢沟桥事变</mark> on <strong>7 July 1937</strong>
      <ul>
        <li>Since the Japanese invasion of Manchuria (东北region) in 1931, there had been many small incidents along the rail line connecting Beijing and Tianjing.</li>
        <li>On this occasion, a Japanese soldier was temporarily absent from his unit, and the Japanese commander <strong>demanded the right to search the town for him</strong>. When this was refused, other units on both sides were put on alert; with tension rising, the <strong>Chinese Army fired on the Japanese Army, which further escalated the situation</strong>, even though the missing Japanese soldier had returned to his lines.</li>
      </ul>
    </li>
    <li>following this incident, Japanese scored major victories, capturing Beijing, Shanghai and the Chinese capital of Nanjing in 1937, which resulted in the <mark>Rape/Massacre of Nanjing</mark> (南京大屠杀) beginning on <strong>December 13, 1937</strong>
      <ul>
        <li>As the Japanese approached, the Chinese army <strong>withdrew the bulk of its forces since Nanjing was not a defensible positio</strong>n. The civilian government of Nanjing fled</li>
        <li>The perpetrators also committed other war crimes such as <strong>mass rape, looting, and arson</strong>. The massacre was one of the worst atrocities committed during WWII</li>
        <li>Due to multiple factors, death toll estimates vary from 40,000 to over 300,000, with rape cases ranging from 20,000 to over 80,000 cases</li>
      </ul>
    </li>
    <li>Following the <strong><mark>Sino-Soviet Treaty of 1937</mark>, strong material support</strong> helped the Nationalist Army of China and the Chinese Air Force continue to exert strong resistance against the Japanese offensive. By 1939, after Chinese victories in Changsha and Guangxi, and with Japan’s lines of communications stretched deep into the Chinese interior, the <strong>war reached a stalemate</strong>.</li>
    <li>in August 1940,  the <mark>United States supported China</mark> through a series of increasing boycotts against Japan, culminating with cutting off steel and petrol exports into Japan by June 1941. Additionally, <strong>American mercenaries such as the Flying Tigers</strong> provided extra support to China directly.</li>
    <li>In December 1941, Japan launched a surprise attack on <mark>Pearl Harbor</mark>, and declared <mark>war on the United States</mark>. The United States declared war in turn and increased its flow of aid to China</li>
  </ul>
</blockquote>

<p><strong>CCP v.s. KMT:</strong> grassroots v.s, organizations</p>

<ul>
  <li>
    <p>some degree of fluidity/flexibility within the party, but overall differ in how to restructure/rule</p>
  </li>
  <li>CCP: socialist, <strong>redistribution of land and peasants</strong> can now on lands
    <ul>
      <li>appeal to peasants and grassroots</li>
      <li>start revolution by helping the peasant, and note that China at that time had a lot of peasants/is rural</li>
    </ul>
  </li>
  <li>KMT=nationalist=Republic of China: in a more extreme way, a bit fascist and military society.
    <ul>
      <li>Hence new cultural movement, refining and strenghening China <strong>in a militaristic way;</strong> also a lot of focus on productivity and utilitarianism</li>
    </ul>
  </li>
  <li>both had some influence from Soviet Union, e.g. Marxist-Leninism
    <ul>
      <li><em>The Communist International</em> (<em><mark>Comintern</mark></em>), was a <em>Soviet</em>-controlled international organization. They sent advisers and aids to help with revolutions</li>
    </ul>
  </li>
</ul>

<p><strong>Japanese Imperialism</strong></p>

<ul>
  <li>
    <p>South Korea asked Japan to ban the <em>rising sun flag</em> during Olympics, which is also the war flag of imperial Japan (which occupied Korea and part of China)</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201132603199.png" alt="image-20221201132603199" style="zoom: 25%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201132549883.png" alt="image-20221201132549883" style="zoom: 25%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
  <li>
    <p>recall that before, Qing has lost Korea territory and Taiwan to Japan; Japan became competitor with Russia for influence in China in the north</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201132906262.png" alt="image-20221201132906262" style="zoom:25%;" /></p>

    <ul>
      <li>surprisingly, they even defeated Russia in the northern territories in China = 1904-5 the <strong>Russo-Japanese</strong> War</li>
      <li>in 1931 the Mukden incidence and <mark>established Manchuko</mark>满洲国;
        <ul>
          <li>Japan really took this territory as its colony; made a lot of investments; therefore it could be the most prosperous area in China at that time</li>
        </ul>
      </li>
      <li>1937-1945; Japan tried to encroach into deeper China, and hence second Sino-Japanese war</li>
    </ul>
  </li>
</ul>

<p><strong>Marco Polo Bridge Incident</strong>: July 7, 1937</p>

<ul>
  <li>
    <p>CKS wanted to first unify Chinese = open non-resistance policy = became very non-popular. He is then kidnapped and forced to ally with CCP to fight against Japan</p>
  </li>
  <li>
    <p>hence <strong>high tension</strong> between Japanese and Chinese with this minor Marco Polo Bridge Incident = triggered second Sino-Japanese war</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201133343977.png" alt="image-20221201133343977" style="zoom:33%;" /></p>

    <ul>
      <li>after that Japan attacked Beijing, Tianjing, etc.</li>
    </ul>
  </li>
</ul>

<p><strong>First Phase of Sino-Japanese War</strong>: Battles</p>

<ul>
  <li>
    <p>Shanghai is an main source of Nationalist revenue + strategic position = proximity to Nanjing;</p>
  </li>
  <li>
    <p>Shanghai had significant western living in there; but their policy is to not interfere at the moment</p>
  </li>
  <li>
    <p>However nationalist eventually <strong>lost</strong>, <strong>Battle of Shanghai</strong> (1937)</p>

    <ul>
      <li>first and fiecest of the 22 major enegagements, lasted three month before Chinese failed</li>
      <li>the bloody Saturday + anti-Japanese feelings</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201133759519.png" alt="image-20221201133759519" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>big defeat of China, CKS lost over 30% of elite German-trained divisions troops (later on contributed to CCP’s victory during civil war)</p>
  </li>
  <li>
    <p>however, at least <strong>pushed some unification of China</strong> (some warlords) to face the Japan troops</p>
  </li>
</ul>

<p><strong>First Phase of Sino-Japanese War</strong>: war crimes</p>

<ul>
  <li>
    <p><strong>Nanjing Massacre</strong> and Rape of Nanjing. Japan failed to conquered Shanghai quickly= angered Japanese, hence when conquering Nanjing this happened</p>

    <ul>
      <li>Japanese troops went on rampage for six weeks</li>
      <li>20,000 - 80,000 women and girls of all ages were raped, many of whom were mutilated or killed in the process.</li>
      <li>200,000 - 300,000 civilians and POWs were killed.</li>
      <li>a woman writer recorded this, who turned the school into a sanctuary, and her diaries records those war and atrocities</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201134307167.png" alt="image-20221201134307167" style="zoom:50%;" /></p>

    <p>up until now, they still had not apologized for this, compared against Germany = Japanese acquired reputation for cruelty</p>
  </li>
  <li>
    <p>“<strong>Comfort Women</strong>”, i.e. sex slaves; ensure an isolated group of women to satisfy solider’s sexual eneds</p>

    <ul>
      <li>an estimate of 200,000 young women and girls were forced into this service; mostly Korea, but also Chinese, Japanese, etc</li>
      <li>one women assigned for 70-80 soldiers before battle</li>
      <li>again, Japanese arguments were that they were paid prostitutes; but many are not/are abducted</li>
    </ul>
  </li>
  <li>
    <p><strong>Human Experiments and use of Biological Weapons</strong>: using diseases as weapon</p>

    <ul>
      <li>
        <p>like Nazi’s, Japanese conducted experiments with human = death camp = infected healthy subjects with various diseases and observe them; operating them without anesthesia</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201134930024.png" alt="image-20221201134930024" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>many biological weapon researchers escaped punishment and went to continue research as other countries feel worried if they are behind; questions of medical ethics = had no ethical standards setup yet</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>First Phase of Sino-Japanese War</strong>: Man-made Yellow River Flood</p>

<ul>
  <li>
    <p>nationalist government made a desperate decision to breach the dam = <strong>cause the flood to stop advancement of Japanese troops</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201135255770.png" alt="image-20221201135255770" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>flood inundated a lot of land and killed over 800,000 people in those provinces (unexpected by the government) = sacrificing million of lives to stop Japanese = though only stopped them for five month, and later continued to capture Wuhan</p>
  </li>
</ul>

<p><strong>Second Phase: Stalemate</strong> 1939-41</p>

<ul>
  <li>
    <p>blue is territory occupied by Japan, and nationalist government moved to interior</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201135451696.png" alt="image-20221201135451696" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>three parties, Though CCP and KMT are both fighting Japanese, they are still not “allies”</p>
  </li>
  <li>
    <p><strong>Japan’s New Order in Asia</strong>: Japan’s belief that they can rebuild Asia and replace the western colonizers: propaganda that other countries should provide resources to Japan</p>
  </li>
</ul>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201135737880.png" alt="image-20221201135737880" style="zoom:33%;" /></p>

<ul>
  <li>puppet regime in Nanjing (1940-45), Wang Jingwei (top in KMT) collaborated with Japan during war time, seen as a national traitor</li>
</ul>

<p><strong>Third Phase: Global War</strong></p>

<ul>
  <li>
    <p>China alliance with United States and Britain = received military support and supplies</p>

    <ul>
      <li>e.g. US army transportation in China, with supplies and soldiers such as Flying Tigers</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201140212011.png" alt="image-20221201140212011" style="zoom:33%;" /></p>

    <ul>
      <li>some sense that US rescured Chinese, but China still had important contribution to the WWII. One is that it held huge numbers of Japanese troops on its territory. Second is that China it technically the first to engage in WWII in 1937 (or even 1931), while Europeans start in 1939</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201140543710.png" alt="image-20221201140543710" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Japan retaliated with <strong>Attack on Pearl Harbor (1941)</strong></p>
  </li>
  <li>
    <p>in the end US dropped two atomic bomb and <strong>Japan surrendered</strong></p>
  </li>
</ul>

<p><strong>Impact of the War on China</strong></p>

<ul>
  <li>
    <p>devastation of many parts in China; <strong>heightened since of national identity</strong>; a <strong>weakened nationalist government</strong>, and CKS lost lots of his troops</p>

    <ul>
      <li>therefore, it set the stage for Communist success later.</li>
      <li>it can be seen that CKS fight against Japan diverted his troop and effort from eliminating CCP, and during those time CCP enjoyed more support in rural China = had more time to develop and became stronger</li>
    </ul>
  </li>
  <li>
    <p>China-Japan relation today: a bit of awkward handshake between two presidents</p>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201141620456.png" alt="image-20221201141620456" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201141629826.png" alt="image-20221201141629826" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
</ul>

<p><strong>Chinese Civil War</strong>: CCP v.s. KMT final showdown after the WWII. 1945 - 1949</p>

<ul>
  <li>
    <p>final showdown; US reduced their support in CKS’s government due to some corruption in government. Mao didn’t get full support from Stalin either</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201141848649.png" alt="image-20221201141848649" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>but we know the final result is that Mao won, established <mark>People's Republic of China</mark></p>
  </li>
  <li>
    <p>after the defeat, <mark>CKH fleet to Taiwan</mark>, with 1.2 million nationalist refugees there (and always wanted to retake the mainland)</p>

    <ul>
      <li>but in 1950, US sends the Seventh Fleet to the Taiwan Strait = <strong>existence of US power prevented each side from attacking</strong></li>
      <li>but still tension and lots of propaganda on-going</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221201142157579.png" alt="image-20221201142157579" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>yet overall, both party <strong>recognize Sun Yat-sen as father of modern China</strong> (idea of a unified China during warlord era, and founder of Republic of China). However, while Mainland China supported Mao, Taiwan supports CKS</p>
  </li>
</ul>

<h1 id="maos-china-1920s---1976">Mao’s China (1920s - 1976)</h1>

<p><strong>Previously on the war years</strong></p>

<ul>
  <li>1931 <strong>Mukden Incident</strong>  (many historians view this as the beginning of WWII for China) touches off a well-planned invasion of Manchuria; starts the 15 years’ China- Japan war (1931-45). Japan establishes a puppet state of <strong>Manchukuo</strong> in northeast China.</li>
  <li>1937-1945 <strong>Second Sino-Japanese War</strong> (another view of beginning of WWII). Tentative <strong>second united frontier</strong>.
    <ul>
      <li>Chinese forces suffer heavy losses, trade space for time.</li>
      <li>Japanese overrun E. China, GMD fights war of attrition, CCP preserves strength in rural areas</li>
      <li>CCP have time to win popular support, partly due to Japan’s invasion that KMT cannot focus on fighting CCP</li>
    </ul>
  </li>
  <li>After <strong>Pearl Harbor</strong>, China formally allies with U.S. and Britain;
    <ul>
      <li>Japanese war crimes (e.g., the <strong>Rape of Nanjing</strong>, <strong>Comfort Women</strong>, Unit 731) remain inadequately acknowledged by the Japanese govt and victims not compensated for.</li>
      <li>Wartime collaboration exists yet remains underexamined in scholarship.</li>
    </ul>
  </li>
  <li>1945-1949 <strong>Chinese Civil War</strong>
    <ul>
      <li>KMD retreat to Taiwan, but both claim to be the legitimate government</li>
    </ul>
  </li>
</ul>

<p><strong>Why did China become Communist?</strong></p>

<ul>
  <li>
    <p>Explanation 1: just because it was the <strong>alternative to KMT</strong></p>

    <ul>
      <li>
        <p>how did nationalist fail? KMT had wide spread corruption, inflation, poverty, declining purchasing power (huge inflation), warlordism</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206132234171.png" alt="image-20221206132234171" style="zoom:50%;" /></p>
      </li>
      <li>
        <p>CCP just happened to be there as an alternative: if KMT more successful, China might not have CCP winning = no communist</p>
      </li>
      <li>
        <p>Japan is also a contributor, that holding KMT off so CCP can have time to develop</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Explanation 2: CCP itself aimed to gain <strong>popularity, mobilizing peasantry, anti-Japan</strong> idea resonated well with the large peasant-based population at that time</p>
  </li>
</ul>

<p><strong>Mao Zedong</strong> 1893-1976</p>

<ul>
  <li>
    <p>in his early years</p>

    <ul>
      <li>
        <p>born to a rich peasant = wealthy farmer, education in Confucian classics; traveled to work in Peking University Library</p>
      </li>
      <li>
        <p>attended CCP first meeting as the delegate of Hunan, working under <strong>Li Dazhao</strong></p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206133027857.png" alt="image-20221206133027857" style="zoom:33%;" /></p>
      </li>
      <li>
        <p>Mao concentrated on <strong>rural work</strong> and had this report on an <strong>Investigation of the Hunan Peasant Movement</strong> = before CCP believe in urban workers, but he advocated to mobilizing peasantry and violence as a necessary mean for revolt</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Shanghai Massacre 1927, communist hiding and fleeing from KMT = Chen Duxiu blamed as the mistake. Then Mao lead them into <strong>Jiangxi Province</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206133303458.png" alt="image-20221206133303458" style="zoom:33%;" /></p>

    <p>for the first time, CCP is governing some area: <strong>organized land reform</strong> (redistribute land) and other reforms = quite successful in Jiangxi</p>
  </li>
  <li>
    <p><strong>Long March</strong> in search of new base in Yan’an in Shaanxi province, after KMT attack again. Only 10% made it the whole way. Those people are seen as selected/destined for this mission = <strong>founding myths of PRC</strong></p>

    <ul>
      <li>Yan’an is like Qin’s captial which is <strong>easy to defend,</strong> and easy to send troops</li>
      <li>but living condition was quite tough: Mao lives in a cave home. But CCP are very friendly to peasants</li>
      <li>important strategy of masses to the masses:
        <ul>
          <li>“all correct leadership is necessarily “<mark>from the masses, to the masses</mark>”. This means: take the ideas of the masses (scattered and unsystematic ideas) and concentrate them (through study turn them into concentrated and systematic ideas), then go to the masses and propagate and explain these ideas until the masses embrace them as their own”</li>
          <li>Mao believes <strong>peasants are the vanguard = true masses</strong> for this revolution v.s. Marxist called peasants a sack of potatoes</li>
        </ul>
      </li>
    </ul>

    <p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206133748438.png" alt="image-20221206133748438" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206134147004.png" alt="image-20221206134147004" style="zoom: 50%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
  <li>
    <p>Techniques in this mass movement</p>

    <ul>
      <li><strong>study groups</strong> = small group meetings like “book club” = express and criticize opinions on <strong>socialists texts</strong></li>
      <li><strong>struggle sessions</strong> = performative displays of revolutions = 民众诉苦
        <ul>
          <li>encourage the peasants to come out and speak publicly accusing landlord of this crimes = directly confronting</li>
          <li>this act of humiliation for landlord could be more important than the land transfer = <strong>symbolize transfer of power from rich to poor</strong></li>
        </ul>
      </li>
      <li>everyone in the group has to participate in the above activities</li>
      <li>really emphasize <strong>small groups + support from locals = decentralized</strong> idea instead of purely top-down structure to rely on center
        <ul>
          <li>e.g. land reform = abolishment of landlords and returning lands to peasants aims to support locals</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>sees himself as the interpreter of Marxism-Leninism in Chinese context, and a master of guerrilla warfare</p>
  </li>
  <li>
    <p><strong>in 1949, Mao’s proclamation of PRC</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206135101458.png" alt="image-20221206135101458" style="zoom:33%;" /></p>

    <p>this painting used more as a political tool, later on Deng Xiaoping is added, even though he is not even there</p>
  </li>
  <li>
    <p>PRC’s political organization: <strong>party supervising government</strong> = so really it is a one party country (a feeling of authocracy)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206135407607.png" alt="image-20221206135407607" style="zoom: 33%;" /></p>
  </li>
</ul>

<p><strong>Land Reform Continued</strong> after PRC founded</p>

<ul>
  <li>
    <p><strong>First phase</strong> of land reform</p>

    <ul>
      <li>before it is mostly north in Mao’s region, now also includes south of China</li>
      <li>
        <p>continues struggle sessions etc; redistribution of land = broke up the older elites = CCP to introduce modern modes of agriculture</p>
      </li>
      <li>also used as a reward for peasants for supporting CCP</li>
      <li>problem: <strong>small land holdings is not a productive way</strong> for high agriculture output</li>
    </ul>
  </li>
  <li>
    <p><strong>Second phase</strong>: farmers join together = <strong>collectivization of agriculture</strong></p>

    <ul>
      <li>households to help each other and <em>forced</em> into cooperation = collectivization</li>
      <li>hope to further increase agricultural output = further support modern inditurialization</li>
      <li>but many felt <strong>discontent</strong> because again lost ownership of it = shared responsibility</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206140000494.png" alt="image-20221206140000494" style="zoom:33%;" /></p>
  </li>
</ul>

<p><strong>America and Korea</strong>: “War to Resist America and Aid Korea” (1950–53) 抗美援朝</p>

<ul>
  <li>recall that US is against Soviet Union = communist. However, they know they cannot exterminate communists.
    <ul>
      <li>in order to use Japan as a base to <strong>contain communism in Asia</strong></li>
      <li>communist ideology is that <strong>every one should be communist</strong> = China joins then it is helpful = also for building alliance for Soviet</li>
    </ul>
  </li>
  <li>Therefore, America now is against China/refuse to help Korea against America
    <ul>
      <li>decided to help North Korea as if it is lost, America would attack China</li>
    </ul>
  </li>
  <li>Consequences of the war include
    <ul>
      <li>China more close to Soviet for econ assists</li>
      <li>confirming US imperialist ambition in East Asia, as US intervention in Korea</li>
      <li>suffered high death tool and called US is a paper tiger</li>
      <li>difficult to get Taiwan <em>fully</em> back in control = with <strong>US-China relationship on Taiwan</strong> a problem until today</li>
      <li>also emerges campaigns in China against foreigners</li>
      <li>Mao lost the elder of his two sons; and the other son has mental trouble = lost his successor</li>
    </ul>
  </li>
</ul>

<p><strong>Five Year Plans</strong></p>

<ul>
  <li>
    <p>PRC’s <strong>First Five-year Plan</strong>: won the Korean War, and land reform aimed to help industrialization as a model from soviet = <strong>invest in heavy industry</strong>.</p>

    <ul>
      <li>indistural output went up by 25%, significant number</li>
      <li>quite successful, Mao very confident and listened to intellectuals outside CCP to speak up = <strong>Hundred Flowers Campaign</strong> = expecting only mild criticism
        <ul>
          <li>turns out this campaign brought <em>heavy</em> criticism = accused of monopoly of power</li>
          <li>then Mao retaliate = <strong>anti-rightist campaign</strong> to force those labeled as rightest forced for re-education</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>second five year plan: the <strong>Great Leap Forward</strong> (1958-1960). Sense of urgency to catch up with other developed country in the west</p>

    <ul>
      <li>e.g. surpass the UK and catch up with the US in <strong>steelmaking</strong></li>
      <li>a feature of this leap forward = people’s commune
        <ul>
          <li>5k households within a commune = so that if women have to go back and cook, takes time away</li>
          <li>aim is to <strong>scale</strong> it by cooking for all = saves time = <strong>more efficient</strong> and productive</li>
        </ul>
      </li>
      <li>but this could give problem that peasants could eat 6 month of rice in 20 days = stock is shared in commune but not in own home = later <strong>famine disaster</strong></li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206141317264.png" alt="image-20221206141317264" style="zoom:33%;" /></p>

    <ul>
      <li>
        <p>everyone setup their <strong>backyard steel production</strong> = again scaling;</p>
      </li>
      <li>
        <p>but overall this divert attention to agriculture = reduce in agricultural output = great famine later</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206141718230.png" alt="image-20221206141718230" style="zoom:33%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>but peasants were very enthusiastic about it. Baby named after those concepts:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206141910439.png" alt="image-20221206141910439" style="zoom:33%;" /></p>

    <p>however, we know now it is a <mark>economic disaster</mark> contributing to the great famine</p>
  </li>
</ul>

<p><strong>Great Famine</strong> 1959-61</p>

<ul>
  <li>
    <p>caused by the weather and drought, but also the <mark>Great Leap Forward fiasco</mark>: 30 million <strong>starved to death</strong></p>
  </li>
  <li>
    <p>historians believe that the problem is not absolute lack of food but the systemic <strong>flaws or decisions that prevent food getting to the people</strong></p>
  </li>
  <li>
    <p>anyway criticism within the party of the Great Leap Forward = Mao took responsibility and <strong>resigned in 1959</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221206142246061.png" alt="image-20221206142246061" style="zoom:33%;" /></p>

    <p><strong>more pragmatic people Liu Shaoqi and Deng Xiaoping took power</strong> = more into Marxist materialistic base than the idealized vision</p>
  </li>
  <li>
    <p>new leaders = <strong>economic recovery</strong> in 1966</p>
  </li>
</ul>

<h2 id="economic-reforms-and-political-authoritatianism">Economic Reforms and Political Authoritatianism</h2>

<p><strong>Previously on Mao’s China</strong></p>

<ul>
  <li>People’s Republic founded 1949, CCP pursues Soviet-style socialist construction, emphasis on heavy industry, <strong>collectivization</strong>, reorganizing grassroots society to unlock labor power; Mao promotes rapid development $\to$ Great Leap Forward 1958</li>
  <li><strong>Great Leap</strong> is an economic disaster; in CCP leadership reshuffle, <mark>pragmatists Liu Shaoqi and Deng Xiaoping rise</mark>; Mao’s desire to regain influence factors in launch of the <strong>Cultural Revolution</strong> 1966-69 factional warfare, civil unrest, paralyze nation
    <ul>
      <li>his goal can be said partly to regain his power, but also as a socialist “ideologist”, he believes in social egalitarianism and sees Deng’s policy on economical and political reform <strong>astray from the correct path</strong> = cultural revolution</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Some key terms:</p>

  <ul>
    <li>The term “<strong>bourgeois</strong>” has a specific meaning in Marxist theory. It refers to the middle class, which is seen as the class that controls the means of production in capitalist societies. In a Marxist framework, the bourgeoisie are seen as the <strong>enemy of the proletariat</strong>, or working class, because they exploit the labor of the proletariat for their own benefit.</li>
  </ul>
</blockquote>

<p><strong>The Great Proletariat Cultural Revolution</strong> 1966-1969/76</p>

<ul>
  <li>
    <p>Mao believes that:</p>

    <ul>
      <li>the method to solve all problems is by <strong>continuous revolution</strong>. New contradictions/problems will always come up, and class struggles should be used as a tool to solve those new problems</li>
      <li>believes in social <strong>egalitarianism</strong> and no elitism, that policies should be made by the people for the people</li>
      <li>In Mao’s view, there are “<strong>bourgeois</strong>” elements within the Chinese Communist Party, i.e. those who had <strong>abandoned</strong> the party’s revolutionary ideology and had <mark>embraced more moderate, reformist policies</mark>. These individuals were seen as a threat to the party’s revolutionary principles</li>
      <li>Therefore, party needed to be purged of what he saw as “bourgeois” elements, and he <mark>launched the Cultural Revolution</mark> to achieve this goal.</li>
      <li>another perhaps popular view is in his attempt to <strong>regain power/influence</strong> in the party</li>
    </ul>
  </li>
  <li>
    <p>1966 Mao wrote a poster “<strong>bombard the headquarters</strong>” = making a war on the party that he established</p>

    <ul>
      <li>this Cultural Revolution first lasted until 1969</li>
      <li>again, he utilize his famous strategy of mass movement = <strong>red guards</strong> (young women students)
        <ul>
          <li>shows that even not in party, he still have <strong>wide-spread support in people</strong></li>
          <li>combined with his earlier mistakes, it can be said he is a person better at <strong><em>moving</em> the masses</strong> to, e.g. revolt, than to govern</li>
        </ul>
      </li>
      <li>the goal is to remove those bourgeois elements from the party</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208132106538.png" alt="image-20221208132106538" style="zoom:33%;" /></p>

    <p>notice that they are holding “<mark>Mao's little red book</mark>” = Quotations from Chairman Mao. This book is seen as <strong>bible for cultural revolution</strong>.</p>

    <p>Other forms of “struggle” within this revolution include:</p>

    <ul>
      <li>
        <p>“criticize and struggle” publicly criticize/beat teachers</p>

        <ul>
          <li>recall that during the anti-Rightest movement (after 100-flowers), many intellectuals were condemned = had a bad reputation</li>
          <li>A lot of victims committed suicide to avoid humiliation</li>
          <li><strong>Deng went to exile</strong>, and President Xi’s father was also denounced in this period as well</li>
          <li>an perhaps “unexpected” chaos as Mao did not specify “who to criticize against”, thereby causing <mark>wide-spread chaos</mark></li>
        </ul>
      </li>
      <li>
        <p>rebels also followed Mao’s call to “Smash the Four Olds”: old thoughts; old culture; old customs; old habits. For exapmle Confucianism</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208133146032.png" alt="image-20221208133146032" style="zoom: 33%;" /></p>
      </li>
      <li>
        <p>the idea of being permitted to defy superior, <strong>rising up again established authority</strong> is a theme in this period</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Deng’s Solution: redirect Young people’s energy is to drive them <mark>to countryside</mark> = 20mil displaced to country side. <strong>Propaganda</strong> below seeming to convince them go there</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Propaganda</th>
          <th style="text-align: center">President Xi</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208133344893.png" alt="image-20221208133344893" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208133652739.png" alt="image-20221208133652739" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>ended finally when Mao passed away</p>
  </li>
</ul>

<p><strong>Deng Xiaoping’s Period</strong>. Whoever is the leader of the party = the man that exert <strong>most impact</strong>. However, the paths taken by post-Mao China were NOT solely dictated by CCP leaders such as Deng</p>

<ul>
  <li>
    <p>at the end of Mao, already some significant changes: <strong>United States President Nixon’s China visit</strong></p>

    <ul>
      <li>In 1972, President Richard Nixon made a historic visit to China, where he met with Chinese leader Mao Zedong.</li>
      <li>This visit marked the beginning of a <mark>thaw in relations between the two countries</mark>, and it paved the way for the normalization of diplomatic relations between them. Prior to his visit, the United States and China had been on opposite sides of the Cold War as China side with Soviet</li>
      <li>One of the main reasons for the <mark>deterioration of relations between China and the Soviet Union</mark> was ideological. The two countries were both communist states, but they had different interpretations of Marxism-Leninism, the ideology that guided their governments.
        <ul>
          <li>The Soviet Union, under the leadership of Leonid Brezhnev, embraced a more moderate and pragmatic approach to communism</li>
          <li>while China, under the leadership of Mao Zedong, took a <strong>more radical and revolutionary</strong> stance. This led to tensions between the two countries, as they competed for leadership of the international communist movement.</li>
        </ul>
      </li>
      <li>therefore, Mao begins to see Soviet as the biggest threat and now <strong>tactical cooperation with US might be worthwhile</strong></li>
    </ul>
  </li>
  <li>
    <p>after Mao’s death, three groups competing for power after Mao:</p>

    <ul>
      <li>
        <p><strong>the Gang of Four</strong>, based in Shanghai and more like a clique controlled by Chairman Mao, lead by his wife Jiang Qing</p>

        <ul>
          <li>but after Mao died, they were purged</li>
          <li>Jiang escaped by viewing herself as Mao’s scapegoat: “I’m just a dog of Chairman Mao, and I bite whoever he wants me to bite!”</li>
        </ul>
      </li>
      <li>
        <p><strong>Hua Guofeng</strong> who is a young official gaining Mao’s trust</p>

        <ul>
          <li>“Two whatevers”: We will resolutely uphold whatever policy decisions Chairman Mao made, and unswervingly follow whatever instructions Chairman Mao gave</li>
          <li>but quickly forced out of power by Deng</li>
        </ul>
      </li>
      <li>
        <p><strong>Deng Xiaoping</strong>’s group, who eventually succeeded as he was a core member and had wide spread connections in the party</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208135041294.png" alt="image-20221208135041294" style="zoom:33%;" /></p>

        <ul>
          <li>once said: “Should China one day become a superpower that bullies, invades, and exploits people everywhere, then the world should expose it, oppose it, and work together with the Chinese people to overthrow it.” = was <strong>advocating peaceful existence between countries with different ideologies</strong> (unlike Soviet)</li>
          <li>connections within the party:
            <ul>
              <li><strong>Long March</strong> veteran</li>
              <li>he never turned on his colleagues despite being purged twice during the Cultural Revolution
                <ul>
                  <li>hence they felt grateful</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>he then successfully <strong>drove out Mao’s chosen successor</strong> (Hua) to reform
            <ul>
              <li>reform direction: Deng distanced the CCP from the Cultural Revolution, from Mao, and from the Soviet economic model</li>
              <li>Deng installed reformists in positions of power.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>In 1981 “Resolution”: on failures of CCP and <strong>enabled the party to distant itself with Mao</strong> = <mark>start new goals</mark></p>

    <ul>
      <li>some denunciation on Mao = cultural revolution was an error</li>
      <li>but still recognize Mao had vital contributions (especially early years), still respected and beloved
        <ul>
          <li>after this little red book disappeared/decayed, etc.</li>
        </ul>
      </li>
      <li>but still not really solving any fundemental change, e.g. structural change, that could prevent this from happening again</li>
    </ul>
  </li>
  <li>
    <p>Deng’s goal for China was to</p>

    <ul>
      <li><mark>modernize/reform the economy</mark> and improve living standards, while still maintaining Communist Party rule</li>
      <li>argues that poverty $\neq$ socialism, and <mark>poverty is problem</mark> = very different from previous</li>
      <li>hence a somewhat <mark>capitalist</mark> approach</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208135840708.png" alt="image-20221208135840708" style="zoom:23%;" /></p>
  </li>
</ul>

<p><strong>Rural Reform under Deng</strong>: Household responsibility System</p>

<ul>
  <li>
    <p>recall that the Great Leap Forward was an attempt but a failed one</p>
  </li>
  <li>
    <p>reform not a top-down decision, but 18 farmers came together to signed. The idea is that land is shared but you <strong>get to keep what you grew</strong> (in addition to what you should grow/submit to government)</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208140056290.png" alt="image-20221208140056290" style="zoom:33%;" /></p>

    <ul>
      <li>After this treaty, every farmer in this village competed to produce more as compared to the Great Leap Forward</li>
      <li>called the first capitalist village</li>
    </ul>
  </li>
</ul>

<p><strong>Urban Reform</strong>: “smash the iron rice bowl”</p>

<ul>
  <li>
    <p><mark>under Mao, there were no private enterprises</mark>, all are controlled under the state.</p>

    <ul>
      <li>Workers belonged to their “work unit” (after graduation) and had lifetime job security.</li>
    </ul>
  </li>
  <li>
    <p>Deng’s reform: <mark>privatize many state-owned enterprise</mark>s, so for workers in inefficient enterprises <strong>gets laid off</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208140509562.png" alt="image-20221208140509562" style="zoom:33%;" /></p>

    <ul>
      <li>though lead to economic development, but income <strong>inequality comes up</strong> as an inevitable problem</li>
    </ul>
  </li>
  <li>Opening to the world: <mark>Special Economic Zones</mark> (SEZ)
    <ul>
      <li>The SEZs were designed to be areas where <strong>foreign investment and economic development</strong> were encouraged, in order to stimulate <strong>economic growth and create jobs</strong>.</li>
      <li>also lead to China’s later entry into WTO</li>
      <li>Shenzhen would be an example of successful ecnomoic zone</li>
    </ul>
  </li>
  <li>by introducing this reform, it moved away from socialism towards <mark>capitalism</mark>: “<strong>Socialism with Chinese characteristics</strong>”</li>
</ul>

<p><strong>One Child Policy</strong> (1980-2011): more people due to improvemened of public health</p>

<ul>
  <li>
    <p>rationale: China had already been burdened to lift 1 billion people out from poverty, which was Deng’s goal. Hence in 1980s this comes up</p>

    <ul>
      <li>of course had propaganda</li>
    </ul>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208141920760.png" alt="image-20221208141920760" style="zoom:33%;" /></p>

    <ul>
      <li>but had a lot of problem/criticism e.g. female fetus the victum</li>
      <li>note that it is only targeted at Han, so not to exterminate ethnic minorities</li>
    </ul>
  </li>
</ul>

<p><strong>Political Authoritarianism</strong>: Deng and leaders seem themselve as authoritarian</p>

<p>| <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208142201727.png" alt="image-20221208142201727" style="zoom:33%;" /> | <img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208142406020.png" alt="image-20221208142406020" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

<ul>
  <li>
    <p>“Economic reform does not to imply <strong>political liberalization</strong>”</p>
  </li>
  <li>
    <p>Tian’anmen Protest in May 1989, <mark>also refered to as May 4-th Movement</mark>. Tension/demends include</p>

    <ul>
      <li>Freedom of the press: The protesters called for <strong>greater freedom of the press</strong>, so that journalists could report on political and social issues without fear of censorship or reprisal.</li>
      <li>Freedom of speech: The protesters demanded the right to <strong>express their views</strong> and opinions without fear of retribution.</li>
      <li><mark>An end to corruption</mark>: Many of the protesters were concerned about corruption in the Chinese government
        <ul>
          <li>An example is that children of elites can get into Tsinghua and Peking University easily</li>
          <li>party official has the priviledge being more wealthy (result of captialism)</li>
        </ul>
      </li>
      <li>Political reform: Some of the protesters called for greater political reform, including the introduction of <strong>democratic</strong> institutions and the expansion of civil liberties.</li>
      <li>fundementally it is caused by wide-spread discontent, result from factor such as
        <ul>
          <li>more <strong>lay-off</strong></li>
          <li>before <strong>controlled rice price</strong>, but now it shoots up (as market is opened) and gives resource tension and inflation</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>lasted for 2 month, the party become <strong>split between liberal and conservative factions</strong></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20221208212944976.png" alt="image-20221208212944976" style="zoom:33%;" /></p>

    <ul>
      <li><mark>liberal</mark> = more moderate, we should talk to students, listen to their complaints and reform
        <ul>
          <li>reformist such as Zhao Ziyang argued that the economy should be opened up to market forces, and that political reforms, such as greater democracy and respect for human rights, were necessary in order to modernize China.</li>
        </ul>
      </li>
      <li><mark>conservative</mark> = repress those news to prevent revolution, very sensitive to revolution (hence also sent troops)
        <ul>
          <li>Deng Xiaoping and his supporters believed that the economy should continue to be planned and <strong>controlled by the state</strong></li>
          <li>but they also believed that <strong>market forces</strong> should play a greater role in the economy.</li>
          <li>in the end, believes that the party should have leading power in decision making</li>
        </ul>
      </li>
      <li>Both believed in the importance of market forces in the economy, but differed <strong>on the extent</strong> to which market forces should be allowed to play a role</li>
    </ul>
  </li>
</ul>

<h1 id="final-exam">Final Exam</h1>

<p>Structure</p>

<ul>
  <li>8 ID, pick 5</li>
  <li>6 pick 2 primary sources: what is says, how does it say it, etc
    <ul>
      <li>who, what, when, why, how = analytical</li>
      <li>significance</li>
      <li>contextualize what source I am talking about</li>
    </ul>
  </li>
  <li>a decent essay</li>
</ul>

<p>Overall idea:</p>

<ul>
  <li>Significance of each event
    <ul>
      <li>e.g. foot-binding significance = commercialization of women + the entire economy</li>
    </ul>
  </li>
</ul>

<p>Key terms</p>

<ul>
  <li><strong>Foot-binding</strong>:
    <ul>
      <li>in Song, foot-binding started among low-class women who were performers/entertainers = so poor had to sell themselves; commercialization of women also during the great movement from north to south = security of women at most agile point = all people are unsure about their worth as now you are going south; women competing in courtesans desired foot-binding = mothers had to also get their daughter into the mold though it is a hard choice</li>
      <li>in Qing, tried to ban foot-binding but didn’t work = ethnic Hans couldn’t because foot-binding became a social status = being foot-bound you don’t need to work in the fields/households , and had servants whose feet were not bound = somewhat an elite status; with the fall of Qing that practice is now gone</li>
    </ul>
  </li>
  <li><strong>Canton system</strong>: Hongs; international trade;</li>
  <li><strong>Qianlong Emperor</strong>: golden age in Qing; Qing really started to expand territory, e.g. Xinjiang under Qianlong; supported some aesthetic engagement; foreign trade</li>
  <li><strong>Neo-Confucianism</strong>: HuangChao rebellion wiped aristocrats with power; Zhu Xi</li>
  <li><strong>Yam System</strong>: serious of outposts so that horse can gallop = allows for speedy transfer of information = how quickly info can propagate
    <ul>
      <li>The postal system, known as the Yam system, was a sort of medieval pony express with stations positioned at intervals of 20-30 miles. At each station, an “arrow messenger” would mount a fresh horse and ride to the next station at a full gallop.</li>
    </ul>
  </li>
  <li><strong>Kurultai</strong> group elect a new leader from a group; elder male chosen as “Kurultai” = next Khan; an election process
    <ul>
      <li>a kuriltai (also spelled <em>kurultai</em>; “general assembly”) of Mongol nobles was convoked in order to elect the new great khan</li>
    </ul>
  </li>
  <li><strong>Chinngis Khan</strong>; <strong>Kublai Khan</strong>
    <ul>
      <li><em>Temüjin</em> (original name) crowned as the kurultai to become Chinngis Khan; unified his steppe tribe; made a lot of invasions = feared ; disable steppe system and created autocracy in the steppe empire</li>
      <li>Kublai: a switch of inward facing history of creation of empire; most people consider the founder of Yuan; invaded southern Song; sinification when ruling; differ with Chinngis as how you rule = cannot cut and paste nomadic strategy to thrive; start of <em>governance</em> = focused more on social and economic issues rather than military conquest</li>
      <li>influence of nomadic culture on China</li>
    </ul>
  </li>
  <li><strong>Columbian Exchange</strong>
    <ul>
      <li>got plants, tomatoes, etc = nutritional = well suited for growth in China; social implications = growth of population</li>
      <li>also diseases</li>
      <li>deficit of silver; Zhang Juzhen = chief secretary; credited to transit from agrarian to; encourage people not to follow the Confucian; policy really benefited the main economy = increasing demand of silver ; as a side effect boosted power of merchants =
        <ul>
          <li>1679 Tokgawa shut down european and halted silver import but still came in = no tap silver inflow</li>
          <li>made paying taxes in this form impossible = people horde their silver = copper to silver went into decline</li>
          <li>Colombian exchange also had the inflow of silver to China=the increased supply of silver led to a <em>devaluation</em> of the metal, which had previously been a scarce and valuable resource.</li>
        </ul>
      </li>
      <li>meaningful engagement with US</li>
    </ul>
  </li>
  <li><strong>1911 revolution</strong> ended Qing; Xinhai Rebellion
    <ul>
      <li>Sun Yat-sen needed some military backing and made agreement with Yuan</li>
      <li>warlord period after Yuan died in 1916-1927</li>
    </ul>
  </li>
  <li><strong>New Cultural Movement v.s. May 4th Movement</strong>
    <ul>
      <li>May 4th 1919: treaty of Versailles with Shandong transferred to Japan angered students = shift focus in China towards soviet unions instead of western power = felt like westerns have abandoned them even though they are all ally in WWI</li>
      <li>but May 4th Movement $\neq$ this event only, but more the idea of <em>shift away from traditional Confucian education</em> to a more modern science and democracy + vernacular literature (e.g. Lu Xun)</li>
      <li>New Culture movement =&gt; New Youth periodical</li>
      <li>what does it has to do with 1911: 1911 being political changes, while these are more literary/and cultural changes to China</li>
    </ul>
  </li>
</ul>

<p>Long Essay</p>

<ol>
  <li>a</li>
  <li>a</li>
  <li>opium war and boxer rebellion</li>
  <li>a</li>
  <li>a</li>
  <li>e.g. May 4-th; what they advocated for = departure from confucian edu values; how does society change but still persisted from the traditional path</li>
</ol>

<h1 id="notes-on-the-open-empire-book">Notes on The Open Empire Book</h1>

<p>This section contains reading notes for the various related chapters of the book “The Open Empire: A History of China to 1800”</p>

<h2 id="ch1-beginnigns-of-the-written-record">Ch.1 Beginnigns of the Written Record</h2>

<p>Written records were mostly for written on <strong>tortoise-shell bottoms</strong>, or called <strong>plastrons</strong> (甲骨文), dating to the reign of King Wu Ding(武丁王) for the <mark>Shang Dynasty</mark>.</p>

<ul>
  <li>
    <p>texts were transcribed onto either plastrons or sometimes scapula of cattles. Specically, this is done by 1) preparing the bones 2) heting it up 3) the bone cracks 4) interpret the message from cracks and write the questions/answers onto it</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220911153533594.png" alt="image-20220911153533594" style="zoom: 67%;" /></p>

    <p>how was this discovered? This is first discovered during 1899 Beijing, during Malaria, when some people give out “dragon bones” (scapula of cattles and turtle shells dug up in large quantities from Anyang) to be grinded into powder for medicine. Then, some scholar found scripts on those dragon bones, and hence lead to realize those are actual texts.</p>
  </li>
  <li>
    <p>For King Wu Ding, <strong>Anyang</strong> (安阳，河南)was the capital</p>
  </li>
  <li>
    <p>finally, <mark>Shang</mark> is succeeded by <mark>Zhou</mark> by the concept of <strong>Mandata of Heaven</strong> (天意)</p>
  </li>
</ul>

<p><em>How Chinese Characters Work</em>:</p>

<ul>
  <li>most well-known examples of chinese characters resemble the concept they depict</li>
  <li>vast majority of cahracters contain a radial (偏旁) to convey the general topic of the word, and a phonetic element to indicate the sound. For instance, 象 means elephant or image, but if added a radical 像, it means image.</li>
  <li>some people also say that chineses are inefficient, indicated by lower literarcy rate in China v.s. US. However, in places such as Hong Kong, Singapore, and Taiwan, literarcy rates are higher.</li>
  <li>Finally, the advent of 拼音 is largely ue to systems to trasnscribe sounds of Chinese into alphabet to be studied by the Western.</li>
</ul>

<p><em>The Advantages of Chinese Script</em></p>

<ul>
  <li>Only few scribes can read or write at that time</li>
  <li>since written chinese characters are decoupled from pronounciation, you can keep your spoken language but just <strong>assign Chinese characters to preexisting words in your language</strong>. This gives rise to the diverse dialects but still a uniform written system.</li>
</ul>

<p><em>The Content of Oracle Bones</em></p>

<ul>
  <li>
    <p>Ritual related questions such as: What should be sacrifiesed? In what quantity?</p>
  </li>
  <li>
    <p>Political questions such as: Sohuld the king send this troops to a certain kingdom?</p>
  </li>
  <li>
    <p>the layout of a typical script on oracle bone consist of:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220911155732457.png" alt="image-20220911155732457" /></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220911155718282.png" alt="image-20220911155718282" /></p>

    <p>where the <strong>preface</strong> explains which priest presided and which date, <strong>charges</strong> states the topic, <strong>prognostication</strong> gives the interpretation of the cracks (i.e. predictions) and <strong>verification</strong> is the actual outcome</p>
  </li>
  <li>
    <p>do not conclude from the above that Shang preferred sons to daughters, because there are cases when “good” births were of daughters.</p>
  </li>
  <li>
    <p>Found a lot of oracle bones, indicating that <strong>divination occured daily</strong></p>
  </li>
  <li>
    <p>Finally, scripts on plastrons are also used for <strong>teaching</strong>, where we found teacher scripts with cracks along with student’s messy and error-containing scripts on plastrons without cracks. This indicate that plastrons are plentiful during the time and students do not need to write on other materials first (if they did practice, then their characters would be better formed!)</p>
  </li>
</ul>

<p><em>Discoveries at Anyang</em></p>

<ul>
  <li>Lady Hao, one of sixty-four consorts of the Shang King Wu Ding, has a tomb contained 3 ivory carvings, close to 500 bone hairpins, over 500 jdaes and nearly 7000 cowry shells (used as money)</li>
  <li>this is one of the only complete tomb excavated, withot the disturbance of gold diggers/theives</li>
</ul>

<p><em>Art of Making Bronzes</em></p>

<ul>
  <li>
    <p>A large of amount of Bronze vessels found at Anyang and Erlitou, suggesting that it is a society with significant resource to produce those at such a large quantity.</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220911160601075.png" alt="image-20220911160601075" /></p>
  </li>
</ul>

<p><em>The Antecedens of Modern Chinese Cuisine at Anyang</em></p>

<ul>
  <li>people at Anyang produced many different type of bronze veseels for cooking meat, holding cooked grain, etc.</li>
  <li>most cooked dishes were meat stew</li>
  <li>resistents of Anyang believed their <strong>ancestors retrained their desire for food</strong>, even when dead</li>
</ul>

<p><em>The Pyramid of Shang Society</em></p>

<ul>
  <li>This <strong>power pyramid</strong> can be found in the tomb: underneath the nature gods stood the ancestors of the royal family. The long-dead outranked the more recently dead, who in turn out ranked Shang king.</li>
  <li>Shang believed one could communicate more easily with those in nearby tires, and offerd a lot to recent dead.</li>
  <li>Member of the royal linage and women from the families who could marry with them constituted the aristocracy of the Shang.</li>
  <li>Shang believed the dead king would occupy this tomb after his death, and because he neded the service of all those who worked for him while alive, <strong>they had to perish when he died</strong>.</li>
</ul>

<p><em>The Nature of the Shang Polity</em></p>

<ul>
  <li>
    <p>His men rounded up livestock from the residents of places the royal entourage visited, but his officials <strong>did not collect taxes</strong> on a regular basis from a set territory</p>
  </li>
  <li>
    <p>therefore, in what sense did Shang constitute a state? It is perhaps more like a soft state, whose stability hinged on the king’s frequent visits to his subordinates and their domains, <strong>whose bureaucracy remained divine</strong> rather than human.</p>
  </li>
</ul>

<p><em>The Illiterate Contemporaries of the Shang in Sichuan</em></p>

<ul>
  <li>the discovery of the Sanxingdui 三星堆 in Sichuan illustrates the <strong>cultural variety of Bronze Age China during Shang</strong></li>
  <li>artifacts in Sichuan different dramatically from anything at Anyang</li>
</ul>

<p><em>Mandate of Heaven</em>: from <mark>Shang</mark> to <mark>Zhou</mark></p>

<ul>
  <li>
    <p>According to Sima Qian (most critics argue this might not be accurate), the last Shang king liked the company of owman, drnak too much, enjoyed depraved songs, etc</p>
  </li>
  <li>
    <p>Many officials left Shang to serve Zhou, and when Zhou’s advisor urged him to invade Shang, he says “You don’t know the <strong>Mandate of Heaven</strong> yet”</p>
  </li>
  <li>
    <p>Then the last Shang king killed an offical who dared to criticize him, by cutting his chest open wile alive, <strong>Zhou king launched the invasion and won</strong>.</p>
  </li>
  <li>
    <p>from then on, people start to believe that:</p>

    <ul>
      <li>if emperor could not govern and is soon revolted, the Heaven has retreated support and revolter would have the new Mandate of Heaven</li>
      <li>but if the revolution failed, then it is said the <strong>Mandate of Heaven is retained</strong></li>
    </ul>

    <p>as a result, Mandate of Heaven is practically a 马后炮</p>
  </li>
</ul>

<p><em>Zhou Conquest of the Shang</em>:</p>

<ul>
  <li>once <mark>Zhou</mark> had defeated the <mark>Shang</mark>, they established their capital along the Wei River (渭河).</li>
  <li>Until 771 B.C.E, when Zhou abandoned their capital in the Wei River valley (near 西安), historian then called the period <mark>before this Western Zhou</mark>, and the period after such a move <mark>Eastern Zhou</mark></li>
</ul>

<p><em>Divination in the Zhou: The Book of Changes</em></p>

<ul>
  <li>Like the Shang, the Zhou also divined with help of bones and shells, but also used a <strong>leafy herb with white flower called yarrow</strong>, where diviners manipulated the <strong>stalks of yarrow in group of six and decided fate based on hexagrams.</strong></li>
  <li>From 诗经, there are mentions that commoner men can also perform divination with both shells and yarrow stalks, such as for marriage</li>
</ul>

<p><em>The Book of Songs</em> 诗经</p>

<ul>
  <li>contains the first songs from anceint China, and depicts vividly the elite and the <strong>lives of commoners</strong></li>
  <li>these recorded songs are culled from 3,000 to 306 by <strong>Confucius</strong> (c.a. 551-479 B.C.E)</li>
  <li>the ealiest songs record ritual chants, but later ones include contents about battles and eventually love</li>
</ul>

<p><em>The Agricultural Year</em></p>

<ul>
  <li>
    <p>A late song details many economic exchanges between the lord of estate and his dependents</p>

    <ul>
      <li>the lord’s obligation is to <strong>provide people with clothing</strong></li>
      <li>commoner women’s obligation is to <strong>provide silk and thread to the lord</strong></li>
      <li>commoner men’s obligation is to provide <strong>animal skins</strong></li>
    </ul>
  </li>
  <li>
    <p>an example would be</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220914152436952.png" alt="image-20220914152436952" style="zoom: 50%;" /></p>

    <p>which gives a sense of life in <mark>Western Zhou</mark>:</p>

    <ul>
      <li>people provided their lord with <strong>a share of crop, hunt, and textile made</strong></li>
      <li>the lord provide them with <strong>clothings</strong> and <strong>invite them to feasts</strong></li>
    </ul>
  </li>
  <li>
    <p>but there are also other songs when people are <strong>dissatisfied</strong> with their lord, in which case they may threaten to leave the lord</p>
  </li>
</ul>

<p><em>Bronzes of Western Zhou</em></p>

<ul>
  <li>besides Book of Songs, <strong>inscriptions on bronzes also provided an outline of government and economy of Zhou</strong></li>
  <li>contents include:
    <ul>
      <li>how <mark>Zhou kings</mark> also worshiped their ancestors, like <mark>Shang</mark></li>
      <li>many rituals are conducted by inscribing on a <strong>bronze bell</strong>, so that when <strong>struck</strong>, the living hoped messages would be conveyed to the dead and perhaps wishes granted</li>
    </ul>
  </li>
</ul>

<p><em>A Zhou Warrior and the Reward for his Service</em></p>

<ul>
  <li>one text described a battle when Xianyun people raided Zhou garrison, who were defeated by the contribution of the Zohu warrior Duo You.</li>
  <li>king then <strong>award him lands</strong>, with people in that land obligated to serve the lord
    <ul>
      <li>since men in the lower rank can rise, it can be seen as a <strong>form of early bureaucracy</strong></li>
    </ul>
  </li>
  <li>hence, Western Zhou can be seen as “<strong>the aggregation of thousands of pieces of land woemn together by the political power of the state</strong>”</li>
</ul>

<p><em>Ancestor Worship</em></p>

<ul>
  <li>the Book of Songs make it possible to reconstruct the <strong>ritual context</strong> (i.e. exactly what happened) of the Western Zhou bronzes</li>
  <li>basically in one song, what happened is:
    <ul>
      <li>a young boy in the clan designated as the impersonator for the spirit to enter his body</li>
      <li>chief of the lineage conducts the ritual</li>
      <li>an officiating invoker serves as the go-between connecting the impersonator and the pious descendant.</li>
      <li>then, living family members requets good fortune and long life from the dead</li>
      <li>officiating invoker then consults an oracle</li>
      <li>and the “spirit” replies</li>
    </ul>
  </li>
</ul>

<p><em>The Zhuangbei Hoard: Ritual Revolution?</em></p>

<p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220915203850551.png" alt="image-20220915203850551" style="zoom:40%;" /></p>

<ul>
  <li>
    <p>in 771 B.C.E, Zhou is invaded and abandoned their captial near Xi’an and <strong>fled east</strong>. Then the Wei family, fore fled, buried seventy-five vesssels and <strong>twenty-eigth bells</strong> in a bit (maybe hoping to recover it when they are back)</p>
  </li>
  <li>
    <p>this pit is now found, and it is realized that objects can be categorized distinctly into <mark>two types</mark>, one ealier dating to 1050 B.C.E and one later to 880 B.C.E</p>

    <ul>
      <li>the ealier vessels were having exaggerated, delicate patterns, such as animals and tao-tie</li>
      <li>the later ones become flatter and stylized (<strong>simpler</strong>), and the tao-tie motif beame a little bit abstract</li>
    </ul>

    <p>therefore, it is hypothesized (largely agreed) that a <mark>ritual revolution</mark> occured c.a. 850 B.C.E, and she explains by:</p>

    <ul>
      <li>ealier ones belived in shamnic travel to he world of spirits, and have no limits on how many vessels a certain person can have</li>
      <li>later ones perhaps have a decline in the belief inanimal spirits, and is <strong>only permitted to have a certain number of vessels</strong></li>
    </ul>
  </li>
</ul>

<p><em>The continuity of Shang and Western Zhou</em></p>

<ul>
  <li>both shared
    <ul>
      <li>belief in oracle bones and divinations (Zhou additionally used yarrow stalks)</li>
      <li>both are soft states, with incipient初始的 form of tax and uncertain borders. Gifts from subjects to rulers are the major form of income</li>
    </ul>
  </li>
  <li>they differ
    <ul>
      <li>Zhou also have some written records of the commoners</li>
      <li>Zhou is beginning to have bureaucracy system</li>
      <li>Book of Songs described a gender split in Zhou: men could farm and hunt, but women needs to gather mulberry leaves, spun thread, dye textiles and making clothing. Also inscriptions on Zhou bells chime the prayers of the living for their sons and grandsons, <strong>not for daughters and granddaugthers</strong>.</li>
    </ul>
  </li>
</ul>

<h2 id="ch2-the-age-of-the-warrior-and-the-thinker">Ch.2 The Age of the Warrior and The Thinker</h2>

<blockquote>
  <p><strong>Here we are featuring:</strong></p>

  <ul>
    <li>Spring and Autumn Period 春秋, and The Warring State Period 战国, both of which is during the <mark>Eastern Zhou</mark></li>
    <li>Chong’er (重耳=晋文公) from the Jin family.</li>
    <li>The period of Confucious 孔子 (died near the start of 战国), Zhuang Zi 庄子, Mencius 孟子, and Xun Zi 荀子</li>
  </ul>
</blockquote>

<p><em>Start of Easter Zhou in. 771 B.C.E</em></p>

<ul>
  <li>
    <p>in 771 B.C.E, two dependent states of Zhou made alliance with some tribal people to overthrow the Zhou King.</p>
  </li>
  <li>
    <p>The defeated <mark>Zhou</mark> shifted their captial to the east <strong>near the modern city of Luoyang</strong>, and became more like a <strong>symbolic head</strong> as the competing regional leaders at far superior armies</p>
  </li>
  <li>
    <p>There were some <strong>more than 100 polities at that time</strong>, and non had siginificant power to conquer all its neighbors. Besides them, there are also groups of “less-civilized” people who do not have their own written records</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917124507405.png" alt="image-20220917124507405" style="zoom:33%;" /></p>

    <p>the four major “non-chinese” groups are therefore the Rong, Di, Man, and Yi, whom although is less civilized, but many chinese had contact with them to learn <strong>battling skills</strong> and cavalry (see later)</p>
  </li>
  <li>
    <p>In the end, Easter Zhou is one with 6 centuries of warefare (770 to 221 B.C.E), and is usually considered into two halves:</p>
    <ul>
      <li><mark>春秋</mark>: named after the book <em>The Spring and Autumn Annals</em>, but actually the book that stores most of our knowledge from the period is <em>The Commentary of Mr.Zuo</em> (左传)</li>
      <li><mark>战国</mark> (the Warring States Period): when the Zhou has became so weak that everybody is fighting between themselves to give birth to the next dynasty</li>
    </ul>
  </li>
</ul>

<p><em>The Commentary of Mr.Zuo and the Society it Describes</em></p>

<ul>
  <li>
    <p>keep in mind that this is written 2 centuries after this period</p>
  </li>
  <li>More than 100 polities, described over 500 battles, and more than 100 civil wars within polities as well.</li>
  <li>At the beginning of 春秋, a hierarchy of <strong>birth deciding which linage a man belonged=social status</strong></li>
  <li>but even at this difficult time, there are several people who fought their own way to be recognized as the “<mark>lord protector</mark>”
    <ul>
      <li>Duke Huan of Qi</li>
      <li>Duke Wen of Jin 晋文公, also called Double Ears (Chong’er)</li>
    </ul>
  </li>
</ul>

<p><em>The Struggle of Double Ears to Win His Realm</em> (Jin)</p>

<ul>
  <li>the Rong wife (of the current ruler) who bore Double Ears neber necame a favorite (e.g. because Rong is non-chinese)</li>
  <li>Wome at the time was also having low status:
    <ul>
      <li>because marriage = building alliance, usually <mark>bride + female relatives are sent so that a son can be born</mark></li>
      <li>therefore, all mother can do it so <mark>angle for the success of her son</mark>: 宫斗
        <ul>
          <li>sent her son to the territory of the Di people</li>
          <li>Lady Li (mother of Double Ear) created the impression that the designated heir (for crown) sought to poison his father. But to accuse Lady Li means to offtend his father, the heir decided to kill himself.</li>
          <li>Awaited the time when their father died, and thought this is the time</li>
          <li><strong>Duke’s ministers could also took</strong> over the throne by killing her sons</li>
          <li>So Double Ears were travelling (e.g. making friends with Chu) before returning to take over the Jin polity with help. To obtain the help, he made a promise to the Chu King that if they meet on battle, he <strong>will withdraw his force a distance of three day’s march</strong></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>晋文公finally took over, and he:
    <ul>
      <li>enlarged his army to three times the size</li>
      <li>met the Chu force, did the retreat as promised, but Chu still engaged. Double Ear won by <mark>tricking</mark> them tht they are retreating</li>
      <li>since he has shown to be a powerful ruler, he is finally appointed by the <mark>Zhou King</mark> to be a <strong>lord protector</strong> and awarded him the new rank.</li>
    </ul>
  </li>
</ul>

<p><em>Changes in the Art of War after Death of Double Ears</em></p>

<ul>
  <li>Double Ears (636-628 B.C.E) marked the begining which <mark>military strength and innovative strategy become important</mark>
    <ul>
      <li>people were trying to have many <strong>chariots</strong>, with about 10-30 carrying the chariot with 3 skilled warriors on it</li>
      <li>but since chariots are costly and ineffective unless on flat plains, they lost to <strong>infantry</strong> 步兵, which also marked an era of political shift since it means <strong>farmers could also revolt</strong></li>
      <li>Di people took advantage of infantry by having a lot of them, and even <mark>erased the social distinction</mark> that divided those who can be ON the chariot and those who ran along. In warefare with infantry, <mark>anyone can rise up the ranks</mark> to become a general, and social flux is encouraged.</li>
      <li>the above is adopted by other rulers later as well</li>
    </ul>
  </li>
  <li>Following a need for army is a rise in tax, literarcy rate (e.g. people can rise up to be the King’s advisor), and agricultural productivity
    <ul>
      <li>started the use of <strong>iron tools</strong> instead of wooden ones, possibly introduced by the Europoid.</li>
      <li>the rise of infantry and army size means <strong>experts generals are needed</strong> to take lead instead of royal lineages. They believed that the <mark>art of war can be taught</mark>, and gives rises to books/teachings.</li>
    </ul>
  </li>
  <li>The first book on strategems is <strong>The Art of War (孙子兵法)</strong>
    <ul>
      <li>stressed that the entire amry had to be trained to follow the orders unquestionably</li>
      <li>“the highest skill is to bring the enermy’s army to <mark>submit without combat</mark>”</li>
      <li>students should seek to <strong>identify weakness in their army</strong> and find the <strong>right time to attack</strong></li>
    </ul>
  </li>
  <li>But another new change soon emerged is the introduction of <strong>calvary</strong> by Zhao King (learnt from their neighbor)</li>
</ul>

<p><em>The Demand of Experts</em></p>

<ul>
  <li>
    <p>rulers during the 战国 <strong>hire advisors on the basis of their skills</strong>. A famous example is Feng Xuan:</p>

    <ul>
      <li>
        <p>the intermediary introduced Feng Xuan to the lord Mengchang:</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917132734568.png" alt="image-20220917132734568" style="zoom:50%;" /></p>
      </li>
      <li>
        <p>then, during a time when Mengchang wants to collect tax and Feng Xuan volunteered, he did not bring the taxes back but “forgive” all of them. The Lord is confused and asked why:</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917132900022.png" alt="image-20220917132900022" style="zoom:50%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>such sharp observation and fast decision is referred to as <mark>acumen</mark>, being hungered for during 战国</p>
  </li>
</ul>

<p><em>The World of Confucius</em></p>

<ul>
  <li>born to the Kong family in Qufu 曲阜</li>
  <li>China’s first <strong>private thinker</strong> (i.e. not served as ministers or generals)</li>
  <li>The only source for understanding his teaching is The Analects (论语), which is a collection of conversations/teachings compiled by later students
    <ul>
      <li>again this is compiled much later after he died</li>
      <li>conversations are first orally tranmitted, and then <mark>written down on bamboo (first source!)</mark>
        <ul>
          <li>scripts on bamboo are written from right to left, top to bottom</li>
        </ul>
      </li>
      <li>the received text of 论语 is compiled by having different disciples writting different sections, hence there are <strong>sharp differences among how they perceive Kongzi and his teachings</strong></li>
    </ul>
  </li>
</ul>

<p><em>The Teaching of Confucius as in the Analects</em></p>

<ul>
  <li>
    <p>he likes to converse with peoplesx</p>
  </li>
  <li>
    <p>claims that he will 述而不作, but did innovate by</p>

    <ul>
      <li>how to behave virtuously and how to govern (ealier believed this skill is born with rulers)</li>
      <li>the concept of ritual <mark>礼 is an important quality,</mark> and if men employ this, society could be reformed</li>
      <li>other <mark>essential qualities include humanness 仁</mark></li>
      <li>during their parent’s lifetimes, children should obey their wishes and after their death, children should continue for three years to conduct family affairs just as their parents had.
        <ul>
          <li>however, Confucious <mark>did not teach anything about spirit and supernatural</mark>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Confucious seems to have <mark>only taught sons</mark>. Why?</p>

    <ul>
      <li>one possible explaination is that men’s interest was to have a larger family unit, and since only men can worship their ancestors, only sons cold carry on the family line. Whereas women are interested in a uterine family 同母异父 consisting themselves and their own children, there is a conflict.</li>
    </ul>
  </li>
  <li>
    <p>Confucious believed that one can influence the wolrd by behaving as a gentleman at home, and that with benevolence (仁) all chaos will be checked</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917135156728.png" alt="image-20220917135156728" style="zoom:50%;" /></p>

    <p>but later people did find it difficult to rule without force, and indeed Confucious was powerless when his city was attacked</p>
  </li>
  <li>
    <p>In 479 B.C.E. he died</p>
  </li>
</ul>

<p><em>Ritual as shown on the Vessels of 战国</em></p>

<ul>
  <li>
    <p>many vessels from Confucious time survived until today, but it is found that they are <strong>very different from the Western Zhou</strong>.</p>

    <ul>
      <li>
        <p>no more animals, but human figures engaging in activities</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917135515555.png" alt="image-20220917135515555" style="zoom: 25%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>In Shang and Zhou, kings had retrained a monopoly over bronze vessels but by the time of Confucious, <mark>lower-ranking lords has taken over production and begin to record their own victories</mark></p>

    <ul>
      <li>to them, <mark>ritual activities (a them in the Analects) is viewed differently</mark> as attesting the effectiveness of power and violence, while Confucious wanted the rituals as a forum of the individual’s development of benevolence.</li>
    </ul>
  </li>
</ul>

<p><em>After Confucious</em></p>

<ul>
  <li>since the Analects is collectively edited by students, separate sections contain different views on the teachings, and hence there were a lot of disagreement = Confucians broke up into eight rival schools.</li>
  <li>the century after Confucious death then giave brith to some of China’s liveliest and most intersting thinkers</li>
</ul>

<p><em>Mozi’s Criticism</em> (50 years after Confucious)</p>

<ul>
  <li>reformulated his teachings in response to Confucious</li>
  <li>At this period 5th century B.C.E, the fighting during the 战国 has dwindled to <strong>four major contenders - Qi, Jin, Chu, and Yue</strong>.</li>
  <li>Mozi accepted certain basic Confucian prescriptions, but felt that:
    <ul>
      <li>bonds with acquaintances are overemphasized, e.g. thief steal from other families to benefit their own. Therefore, he proposes <mark>兼爱, which is that each individual had an obligation towards all other people in the society</mark></li>
      <li>ridiculed that Confucious advocate for elaborate funerals but gives no discussion of spirits and afterlife. Hence he proposes measures for <strong>controlling the cost of funerals</strong>.</li>
      <li>but he agrees with Confucious that gentleman had to remain engaged in the society for reformation</li>
    </ul>
  </li>
</ul>

<p><em>The Way and Integrity Classic</em> 道德经</p>

<ul>
  <li>
    <p>written by 老子, and disciples of this school are called Daoist</p>
  </li>
  <li>
    <p>emphasizes the quality of Wuwei <mark>无为, which should be interpreted as non-interfence:</mark></p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917141028913.png" alt="image-20220917141028913" style="zoom:50%;" /></p>
  </li>
</ul>

<p><em>Zhuangzi: The use of Paradox and Humor</em></p>

<ul>
  <li>
    <p>while both 道德经 and 庄子’s teachings often addresses the ruler of the state, Zhuangzi focuses on the individual and presents a series of anecdotes</p>
  </li>
  <li>
    <p>he believes that people should <mark>abandon preconceived notions</mark></p>

    <ul>
      <li>
        <p>death should not be mourned</p>
      </li>
      <li>
        <p>advantage of the disadvantaged</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917141422248.png" alt="image-20220917141422248" style="zoom:50%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Zhuangzi also emphasizes on 无为, letting things run their natural course, but does not seem to condemn deforestation</p>
  </li>
</ul>

<p><em>Zhuangzi and Mencius</em> 孟子</p>

<ul>
  <li>
    <p>Mencius is a contemporary of Zhuangzi</p>
  </li>
  <li>
    <p>Mencius believed that <strong>men were born good, but the environment transformed /corrupted them</strong></p>

    <ul>
      <li>
        <p>an example would be deforestation, fked by the environment</p>

        <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917141929779.png" alt="image-20220917141929779" style="zoom:50%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Mendius, since believing in men, felt the people could be trusted to select a new rules, and the <mark>concept of Mandate of Heaven is to mean that a ruler could rule only with support of his subjects</mark></p>
  </li>
</ul>

<p><em>Mencius and Xunzi: the Continuing Confucian Debate</em></p>

<ul>
  <li>Xunzi 荀子 is in contrary with Mencius, believing that <strong>men were born evil but can improve via education</strong>.</li>
  <li>like Confuscious, they focused on question of how to govern, but they were <mark>hoping more of a unification rather than restoration</mark> (as the Zhou dynasty has became so weak)</li>
  <li>at the time of Xunzi, there were only three major power vieing: Qi, Chu, and Qin
    <ul>
      <li>Xunzi visited Qin just before their conquest, and saw that “They employ them (the people) harshly, terrorize them with authority, embitter them with hardship, coax them with r4ewards, and cow them with punishments”</li>
      <li>hence <mark>predicted that Qin would not last long</mark></li>
    </ul>
  </li>
</ul>

<p><em>Money Economy of the Warring States Period</em></p>

<ul>
  <li>
    <p>In Shang and early Zhou, rulers used cowrie shells and cloth are currency, and their subjects exchanged goods</p>
  </li>
  <li>
    <p>In 战国, the <mark>merchant class grew as trade across regions increased</mark>, and that the economy were well-developed for trading:</p>

    <p><img src="/lectures/images/2022-12-19-ASCE1359_Intro_to_China/image-20220917143052255.png" alt="image-20220917143052255" style="zoom:50%;" /></p>

    <p>because of the increase in trading, it leads to an <mark>increase in specialization</mark></p>
  </li>
</ul>

<p><em>The Chaotic Pace of Change</em></p>

<ul>
  <li>
    <p>constant fighting throught the period = large resource consumed = rulers taxing individuals</p>
  </li>
  <li>
    <p>a bunch of changes:</p>

    <ul>
      <li>regional rulers displacing Zhou kings</li>
      <li>chariot warriors displaced by infantry</li>
    </ul>

    <p><mark>a great degree of social flux</mark> also gave anciety for higher class, but also <mark>promoted literarcy</mark></p>
  </li>
  <li>
    <p>chaotic period also gave rise to all the different thinkers starting from Confuscious</p>
  </li>
  <li>
    <p>the <strong>Legalist teachings</strong> 法家 in Qin was so repelled, by Xunzi, but they proposed a radically new way of organizing the state and its subject that allows its adherets to <mark>unite the realm for the first time</mark></p>
  </li>
</ul>

<h2 id="ch3-the-creation-of-empire">Ch.3 The Creation of Empire</h2>

<p><em>From the Warring States to Qin</em></p>

<ul>
  <li>Qin defeated the other riavals not because of any new technology, but because they found a new way to organize their state: Legalist ministers who advocated the <strong>abolition of all privilegeds of nobility</strong>
    <ul>
      <li>Results in very efficient system in Qin (in contrast to the modern view of bureaucracy)</li>
    </ul>
  </li>
  <li>After the unification of China, scholars in <mark>Qin</mark> and <mark>Han</mark> begin to face the question: how should they <strong>govern</strong> talented officials? What role should the state play in the economy?</li>
  <li>during the Warring state, it was mostly priviledged aristocracy and the laboring masses. But after 221 B.C.E. it became mostly scholars, peasants, artisans, and merchants (merchants last because pesants and artisans produced something)
    <ul>
      <li>technically we still have slaves, doctors and religious specialists in addition</li>
    </ul>
  </li>
  <li>Qin ended within 14 years of ruling, due to problems in the court and people unsatified=rebellion</li>
</ul>

<p><em>Brief history of Han</em></p>

<ul>
  <li>founded by a strong leader and succeeed by strong woman, Han enjoyed 200 years of competent ruling</li>
  <li>Interloper Wang Mang, tired of being regent (to young emperor), formed a <mark>new dynasty called Xin</mark></li>
  <li>within 15 years, other families of Han regained power and weakened the dynasty</li>
  <li>destruction of their capital at Changan forced them to move to Luoyang</li>
  <li>beginning of increased tax, and emperesses wrest control from weak emperors</li>
  <li>rise of enunuchs and Confucian students began protests, as well as many following Daoism</li>
  <li>lots of protest and rebellion, a disaster ended the dynasty</li>
</ul>

<p><em>The Legalist State</em></p>

<ul>
  <li>韩非子, who stuttered during an ear where eloquence is prized, had to record his idea on pen and paper</li>
  <li>believed that ruler should be detached from everyday business, and apply <strong>unbending standard to judge officals and people</strong></li>
  <li>believed in a <strong>law that treated everyone equally</strong>, and that only the <strong>systematic application of law</strong> can control people, whose nature are evil (Xunzi)</li>
</ul>

<p><em>The Architecture of Qin’s Success: Shan Yang’s Reforms</em></p>

<ul>
  <li>Qin wasn’t strong enough to take over all rivals until 商鞅’s term in office</li>
  <li>Shangyang <strong>disdain the past</strong>, hence <mark>rejected Confucious values</mark>, especially ritual, and sees it as pointless</li>
  <li>Shangyang greatly improved the fiscal of the Qin:
    <ul>
      <li>registration of households, and let people <strong>supervise each other</strong></li>
      <li>if criminal activity is not reported, chop heads</li>
      <li>once a man reached 16-17, <mark>obliged to serve military</mark> and pay <mark>taxes</mark></li>
      <li>establishing the concept of <mark>ownership of land</mark></li>
      <li><mark>achievements in army gives promotion in rank</mark> proportional to the achievement</li>
      <li>the entire population of Qin is divided into <mark>ranks</mark> = permitted clothing, land, slaves, households.</li>
      <li>people (pesants/artisans) who contributed much grain/cloth to the state is free from tax</li>
      <li>proposed rulers of small districts <mark>县, who is in charge of organizing army</mark>, carry out public works, collect taxes…</li>
      <li><strong>standardized units</strong> such as weight and length</li>
    </ul>
  </li>
</ul>

<p><em>China’s First Emperor</em></p>

<ul>
  <li>
    <p>given a strong fiscal basis, King Zheng (later called <strong>秦始皇帝</strong>) decided to attack all the other rivals at the time</p>
  </li>
  <li>
    <p>after destorying the other six kingdoms, King Zheng:</p>

    <ul>
      <li>forced the royals from the six kingdoms to live near the capital (for monitoring)</li>
      <li>emphasized on <strong>farming</strong> as the mainstay of the economy</li>
      <li><strong>standardized coins</strong> to be a circle with holes (so that they can be stringed together)</li>
      <li>divided territories into <strong>commanderies</strong>, which is further divided into <strong>counties</strong>.
        <ul>
          <li>Commanderies are like little central government:</li>
          <li>they need to perform three tasks: civl matters, e.g. taxation, military affairs, and <mark>supervision of governmental officials</mark></li>
        </ul>
      </li>
      <li>the top officials in the central government include
        <ul>
          <li>chancellor, who heade the bureaucracy</li>
          <li>imperial secratry, who drafted king’s orders</li>
          <li>grand commandant, in charge of military</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Qin has also done some <strong>significant projects</strong></p>

    <ul>
      <li>built <strong>networks of roads</strong> over 6,800 km long</li>
      <li><mark>dug irrigation cannals</mark>, and some three hundred thousand men built extensive walls of pounded earch along northern borders</li>
      <li><mark>fearful of death</mark>, he tried to obtain exilir for immortality but failed. Instead, he let seven hundered thousand men working for his <mark>tomb</mark>, which was intested to replicate the universe as his permanet resting home in the <mark>afterlife</mark>
        <ul>
          <li>for fear that workers may disclose the position of the tomb, all artisans and laborers who had worked there <mark>were imprisoned in the tomb</mark> while alive</li>
          <li>gives rise to the <mark>Terra-Cotta army</mark> 秦陵兵马俑, who were thought to be troops that the emperor can fight with in the underworld</li>
          <li>however, his tomb today was never excavated/found</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>When Qin founder died in 210 B.C.E, he named his more popular first son to succeed, but with interloped by a powerful official named Zhao Gao</p>

    <ul>
      <li>took over as a chancellor</li>
    </ul>
  </li>
  <li>
    <p>but the greatest opposition is the <strong>rebellion of the people</strong>. This happened because there were laborers who are delayed by the rain, and that:</p>

    <ul>
      <li>since they cannot complete their work, they “will die” (according to Han historians)</li>
      <li>if they revolt, they die as well (according to Han historians)</li>
    </ul>

    <p>therefore, maybe they should risk for their own kingdom.</p>
  </li>
</ul>

<p><em>Reasons for Doubting the Historical Record</em></p>

<ul>
  <li>
    <p>Win ordered a <mark>large-scale book burning</mark> in 213 B.C.E destroying all dissenting point of view</p>

    <ul>
      <li>except <em>The Book of Song</em> 诗经 and <em>The Book of Documents</em> 书经 were banned, and only agricultral and divination books permitted</li>
      <li>for books such as 诗经, most content are anyway transmitted from teacher to student, hence book burning had no effect</li>
    </ul>
  </li>
  <li>
    <p>贾谊Jia Yi’s Confucian view point, which emphasized humanity and rightenousness, provided Han dynasty the perfect justification to overthrow Qin</p>
  </li>
  <li>
    <p>but it was found later in a tomb in 睡虎地 Shuihudi, where a Qin clerk lived a life as a hearing for crimal suit and took <mark>several legal writings with him in the tomb</mark></p>

    <ul>
      <li>described a detailed judification system where poeple would often be <mark>beaten as a punishment instead of killed</mark></li>
      <li>women also had work outside the typical sewing: some were working in <mark>lacquer production with signed vessels</mark></li>
    </ul>

    <p>however, it is <em>prescriptive</em> of what sohuld happen rather than <em>descriptive</em> of what actually happened, so it should also be taken with a grain of salt</p>
  </li>
</ul>

<p><em>The Founding of Han</em></p>

<ul>
  <li>peace during 秦始皇帝 but as soon as he died and much succession problem happened at palace, many of the former regional states broke away</li>
  <li>bona-fide peasants 刘邦 and low-ranking official <strong>rebelled and started a new dynasty</strong>
    <ul>
      <li>Liu Bang is very social but crude, hence have a lot of drinking friends at inn</li>
      <li>when he won the battle against Qin, he pleged to abolish all other Qin’s law but only keep a) those who kills others shall die b) he who harms others or steals should be punished</li>
      <li>but in the end it is found that the <mark>Han dynasty took many Qin laws verbatim</mark></li>
    </ul>
  </li>
  <li>one major depature from Qin is Han’s policy on placing nobility all near captical. Instead, Han created new nobilities to his brothers and relatives and <mark>dispersed them</mark>:
    <ul>
      <li>gave them titles of local Kings</li>
      <li>2/3 of the territory reined by sons and relatives, 1/3 by Liu Bang himself</li>
      <li>but the core of Han is still <mark>西安</mark></li>
    </ul>
  </li>
  <li>but the first years of Liu Bang’s reign was difficult, as it has to <strong>suppress rebellions within China and defned against the powerful 匈奴</strong> in the north
    <ul>
      <li>after Liu Bang’s death, a new ruler in 209 B.C.E. lost to Xiongnu and signed a <mark>humiliating peace treaty</mark> requiring China to present gifts of textiles, food, and wine, wile Xiongnu not to attack China</li>
    </ul>
  </li>
</ul>

<p><em>The Reign of Dowager Empress Lv</em></p>

<ul>
  <li>Empress Lv 吕后 ruled for an extended period of time by placing infants on throne</li>
  <li>as an empress, suffered pressure even from Xiongnu (asking she to marry over)</li>
</ul>

<p><em>The World of the Regional Rulers: 马王堆 finds</em></p>

<ul>
  <li>
    <p>found <mark>Han's tomb in 马王堆汉墓</mark>, with three of the four tombs almost intact</p>
  </li>
  <li>
    <p>most important find is <mark>Lday Dai's tomb</mark></p>

    <ul>
      <li>
        <p>Lady Dai’s skin fully preserved</p>
      </li>
      <li>
        <p>has a lot of cloths and wine vessels in the tomb</p>
      </li>
      <li>
        <p>found much evidence of Han cuisine, e.g. meat stew and rice</p>
      </li>
      <li>
        <p>her <mark>view of the afterlife</mark> characterized by the T-shaped banner which contained three sections:</p>

        <ul>
          <li>top: the immortal Lady Dai (spirit)</li>
          <li>middle: Lady Dai existence in the underworld</li>
          <li>bottom: Lady Dai in tomb (soul)</li>
        </ul>

        <p>not a sequential event, but silmultanoues</p>
      </li>
      <li>
        <p>migth not be literate due to no books in tomb</p>
      </li>
    </ul>
  </li>
  <li>
    <p>other tombs are his sons, where a vast majority of books is fonud = reassess the <strong>intellectual world of Han</strong></p>
  </li>
</ul>

<p><em>The Han Dynasty Under Emperor Wu</em> 汉武帝</p>

<ul>
  <li>Emperor Wu elimited checks on his power, and then <mark>established a Confucian academy</mark>, which is deeply affected by his advisor 董仲舒</li>
  <li>Emperor Wu started the academy by
    <ul>
      <li>first appointing five teachers who specialized in five books: <mark>诗经, 尚书/书经, 周易/易经, 尚书 春秋</mark></li>
      <li>enrolling thousands of students, who <strong>upon graduation can go into the government</strong></li>
    </ul>
  </li>
  <li>Therefore, the procedure of hiring new officials become a) be <mark>recommended</mark> and b) take the <mark>examinations</mark></li>
  <li>Emperor Wu also tried to battle against Xiongnu due to the unfair treaty, but neither achieved a decisive victory
    <ul>
      <li>during some of the battles, Li Ling founght hard but lost. He failed to suicide and became a slave</li>
      <li>司马迁wanted to defend Li Ling, who he knows as a childhood friend, but got himself castrated instead of put to death because he thinks its even worse <strong>if his literary work 史记 cannot be left to posterity</strong></li>
      <li>史记, unlike later books written by historians <em>funded by the government</em>, is <mark>self-funded and hence a more trust-worthy source</mark></li>
    </ul>
  </li>
</ul>

<p><em>The Creation of Autocracy</em></p>

<ul>
  <li>Emperor Wu asserted control during his time by dismissed or sentenced to death some five chancellors, and became unchecked in power</li>
  <li>he also promoted 霍光 to serve as regent after his death, but unfortunately he never gave up power and just <mark>placed child emperor over another</mark>
    <ul>
      <li>this cycle of regent people/empresses/enunuchs <mark>named themselves are regent and gained control</mark> is common for later era</li>
    </ul>
  </li>
</ul>

<p><em>Economic Problems in the Han</em></p>

<ul>
  <li>
    <p>Since almost every measure Emperor Wu imposed, e.g. sending troops to Xiongnu and establishing academy required funds, some mechanism is needed</p>
  </li>
  <li>
    <p>solution: <strong>government monopolies</strong></p>

    <ul>
      <li>salt and iron, where the latter is immensely useful for making tools</li>
      <li>later also assumed monopoly of copper, bronze, and wines</li>
    </ul>
  </li>
  <li>
    <p>but long years to fightings = lots of taxings = people suffering. Hence after 汉武帝 death investigation are sent out</p>

    <ul>
      <li>scholars were against tradings and monopolies, while the government officials successfully defended it</li>
    </ul>
  </li>
  <li>
    <p>In an essay by Wang Bao, were a slave’s life for a widow is mentioned, the econ for that time can be inferred:</p>

    <ul>
      <li>there is a widespread network of small and wide markets</li>
      <li>slaves are expected to travel from places to places to buy stuff</li>
      <li>although a money economy existed, mostly they are used to <strong>trade for luxuries</strong></li>
    </ul>

    <p>additionally, it meant the society can also be split into two types:</p>

    <ul>
      <li><mark>estate ownders</mark>: collected large amount of wealth and became <strong>increasinly powerful and influential</strong></li>
      <li><mark>slaves</mark>: e.g. lost from war or seriously in debt</li>
    </ul>
  </li>
</ul>

<p><em>Wang Mang 王莽 Regency</em>:</p>

<ul>
  <li>
    <p>Wang Mang wanted to <mark>suppress the power of large estate owners</mark>, and founded the Xin dynasty</p>
  </li>
  <li>
    <p>Wang Mang’s approach included limiting the maximum land you can have, and cessing some land from owner to poors</p>
  </li>
  <li>
    <p>However, his reign came to an end due to <strong>flooding</strong>, which caused</p>

    <ul>
      <li>unsatisfactory landowners due to his policy</li>
      <li>angry peasants</li>
    </ul>

    <p>hence he is killed and taken over</p>
  </li>
</ul>

<p><em>The Restoration of the later Han</em></p>

<ul>
  <li>The later Han with captial in Luoyang is sometimes called <mark>Eastern Han</mark> as it is east of Chang’an</li>
  <li>The captical has walls surrounded imperial palace, inns, canals, markest, and schools, and had a <strong>huge amount of residents, students, and officials</strong></li>
  <li>since it is just restored, it still relies on large land owners and hence they have some power over the recommendation system and put sons into government</li>
  <li>the <mark>rise of eunuchs</mark> in the palance</li>
</ul>

<p><em>Wang Chong’s Skeptism and 班’s Family</em></p>

<ul>
  <li><strong>argued that many early arguments are made <em>without</em> logic</strong>. For instance, people performing ceremonies to the spirit of earth is useless/unheard by the earth, just as people won’t head anything about the lice on their skin.</li>
  <li>Ban’s family is full of literate sons and daughters
    <ul>
      <li>Ban Gu, son of Ban Biao, resolved to complete the history of China written by his father was thrown into jail accused of distorting history</li>
      <li>Ban Zhao 班昭, the daughter of 班彪, is one of the most famous women writers
        <ul>
          <li><em>Lessons for Women</em>: unequal treatment of women and how she thinks married women should behave</li>
          <li>later on <mark>sponsered by the court</mark> to finish writing the history</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Ban Zhao’s view of women:
    <ul>
      <li>four qualities women should have: “womenly virtue, womenly words, womanly bearing, and womenly work”</li>
      <li><mark>wife should not question her husband</mark>, but husband should not beat wife either</li>
      <li><mark>plead for equal education</mark>, as usually sons are educated by daughters not
        <ul>
          <li>also inferred difficulty of women: families abandon their daughter due to <strong>marriage costs</strong></li>
          <li>but powerful families prefer having a <mark>daughter married to emperor</mark> so that they can <mark>seize power</mark></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Ban Biao was also served as the advisor to Emperor Deng</li>
</ul>

<p><em>Refusal to Serve in Government</em></p>

<ul>
  <li>but as the successor Emperor Huan could not bear the power of in-laws, used <mark>eunuchs to launch a coup and unseated the Liang family</mark>, which is the family of his wife</li>
  <li>as Emperor Huan’s needed eunuchs to succeed, he became very <strong>unpopular</strong> and people are rejecting his invite to serve. For example, one person who rejected explained:
    <ul>
      <li>holding office meant having to censure the emperor, and since critism will be punished, this is not good</li>
      <li>if not censuring, then his <strong>own “purity” as scholar is damaged</strong></li>
    </ul>
  </li>
</ul>

<p><em>The Rise of the Organized Daoist Church</em></p>

<ul>
  <li>Instead of the righteous scholars, Emperor Huang tried to tap from the new Daoist</li>
  <li>Daoist belief at that time went under a major transformation
    <ul>
      <li>believe that <mark>老子 is a deity and can come to give advices</mark></li>
      <li>split into two fractions, 五斗米道 and 黄巾</li>
    </ul>
  </li>
  <li>the 五斗米道 Five Peck of Daoist movement believed that
    <ul>
      <li>instead of teaching selected adepts techniques to obtain immortality, teach a larger community</li>
      <li>the leader 张道陵 <mark>promised good health in exchange to peoples belief/rice offerings to Daoist deity</mark></li>
      <li>the idea of <mark>linking illness with bad deeds</mark> initiates here, and that people believe confessions is the cure</li>
    </ul>
  </li>
  <li>the yellow turban planned for a rebellion 黄巾之乱, but
    <ul>
      <li>leaders quickly executed by military, under the lead of 曹操</li>
      <li>people quicly dissovled</li>
    </ul>
  </li>
  <li>foreshadows the introduction of buddism and changes of the religious lanscape later</li>
</ul>

<p><em>The Legacy of the Han</em></p>

<ul>
  <li>much remains uncertain about the fall of Han, but popular theory suggests that the empire experienced a <mark>massive shortfall of funds</mark></li>
  <li>However, both Qin and Han left an <strong>impression of unified China in peoples minds</strong></li>
  <li>during the 400 years of reign of Qin and Han
    <ul>
      <li>Chineses society assume the contours it would take today, and much market economy of the Han and government’s role in regulating also persisted until today</li>
      <li>women still has subordinate roles, but when opportunies arrive people such as Ban Zhao can break out of their traditional roles</li>
      <li>the only siginficant change, which comes later, is spiritual: introduction of <strong>Buddism</strong></li>
    </ul>
  </li>
</ul>

<h2 id="ch4-chinas-religious-landscape">Ch.4 China’s Religious Landscape</h2>

<p><strong>Overview</strong>:</p>

<ul>
  <li>Buddhism, started as a foreign religion, won more followers than Daoism for the year 200-600</li>
  <li>Buddhism included life of lay people who live at home with their faimlies or monk who joined monasteries</li>
  <li>
    <p>many reason underlay Buddihist’s success: most important is the ability to <strong>absorb elements of preexisting religions</strong>, e.g. recruited many local deities as guardian. Therefore, you can worship both Daoism and Buddhism at the same time</p>
  </li>
  <li>long-lasting political instability marked the centuries of China’s conversion to Buddhism: for three centureies after the fall of Han, a period often called <mark>Six Dynasties (220 - 589)</mark>, no regime successed in conquering more than half of China’s territory (until <mark>Sui</mark> dynasty that comes afterward)</li>
</ul>

<p><strong>The First Buddhist in China</strong></p>

<ul>
  <li>Buddhism is associated with <strong>trade</strong>, since the Buddha (ca 500 B.C.E) liked merchants show provided crucial financial support</li>
  <li>In 148 C.E. the famous missionary An Shigao from Parthia arrived in Luoyang, and began work on <strong>translating Sanskrit texts into Chinese</strong> (Sanskrit text is the text of Buddhist teaching)</li>
</ul>

<p><strong>The Successors to the Han Dynasty</strong></p>

<ul>
  <li>succeeded by the <mark>Three Kingdom period</mark> (220 - 280), in which Cao Cao’s <strong>nine grade system</strong> had a long lasting impact
    <ul>
      <li>in principle, a local official, the impartial judge, was supposed ot assign each candidate a rank from one to nine in the office</li>
      <li>in reality, the pattern established during Han prevailed, recommendations were influential, hence sons of powerful families have advantage</li>
    </ul>
  </li>
  <li>
    <p>when Cao Cao’s heirs lost power, the Sima family founded the <mark>Jin Dynasty</mark>, who ruled northen China for barely half a century util broken up in the War of Eight Princes (290 - 306), so northern part becomes separated again with Jin retrained its control at Luoyang</p>
  </li>
  <li>In 311, the <strong>Xiongnu conquered the former capital Luoyang</strong></li>
</ul>

<p><strong>Appeal of Buddhism to Northern Rulers</strong></p>

<ul>
  <li>Shi Le, the ruler who conquered Luoyang, was once introduced to the Buddhist missinoary <strong>Fotudeng</strong>, who performed some miracle tricks and impressed him</li>
  <li>he then granted the Buddhists the right to build <strong>monasteries</strong></li>
  <li>whether if Fotudeng can do magic tricks is tangantial here, the important thing is:
    <ul>
      <li>the news of this spreaded Buddhism</li>
      <li>Buddhism offered <mark>non-Chinese rulers an alternative to Confucianism,</mark> which is very Chinese</li>
      <li>also, for <mark>women</mark>, this offered a <strong>new alternative than family life</strong>: joining a nunnery. Justified by <mark>transfer of merit</mark> (e.g. to family)</li>
    </ul>
  </li>
</ul>

<p><strong>The Difficulties of Translating Buddhist Concept</strong></p>

<ul>
  <li>Chinese belongs to Sino-Tibetan language, while Sanskrit which has an alphabet, belongs to Indo-European</li>
  <li>also many different cultural assumptions: <strong>Indians talked about sex much more freely</strong> than Chinese, hence cautious translators dropped any mentions of kissing and hugging</li>
  <li>difficult to translate new terms/concept not in Chinese:
    <ul>
      <li><strong>early translators decided to use Daoism concepts</strong>: Nirvana $\to$ wuwei, which is far from accurate</li>
      <li>also difficult to translate <mark>karma</mark>: one’s next life could be reborn into an animal or person depending on behaviors; Daoism viewed a constant afterlife</li>
    </ul>
  </li>
  <li>then in 344 - 413, Kumarajiva, a monk from Central Asia or from India, is sponserd by private individuas to come to Chang’an to perform <strong>better translations</strong></li>
</ul>

<p><strong>Buddhism in Central Asia</strong></p>

<ul>
  <li>Kumarajiva is trained by a monk in Kucha, which is now part of China’s Xinjiang, and many people there speak <strong>Turkic language Uighur</strong> instead of Chinese</li>
  <li>he studied Buddhist texts in Sanskirt, spoke Tokharian, and learned some Chinese from merchants who came to Kucha</li>
  <li>he was later kiddnapped to Gansu for 17 years, where he mastered Chinese, and he started to believe in the <strong>Greater Vehicle</strong>
    <ul>
      <li>the <mark>Lesser Vehicle</mark> (Hinayana) held Nirvana possible only for the few who joined the Buddhist clergy</li>
      <li>the <mark>Greater Vehicle</mark> (Mahayana) offered salvation even to laity</li>
    </ul>
  </li>
  <li>starting in 401, skilled translators and knowledgable monks also joined the translation project</li>
  <li>even though Buddhism has value for <strong>celibacy</strong>, there are many exceptions (e.g. Kumarajiva) where the practioner had family</li>
  <li>Bodhisattva named Avalokitesvara = nowadays <mark>观音</mark></li>
</ul>

<p><strong>Contact between India and China</strong></p>

<ul>
  <li>pilgrams going to India as early as 260 on the overland trade route linking India and China</li>
  <li>no single <mark>Silk Road</mark> strecthed all the way yfrom Rome to CHina, but most frequented routes <strong>led to India</strong>
    <ul>
      <li><strong>Chinese silk was very lucrative</strong> as only Chinese knew how to process silk worms</li>
      <li>much silk also went to European conutries such as Rome, but a lot might be made in Byzantium</li>
      <li>then silk is also used as gifts to monasteries</li>
      <li>Chinese imported a lot of glass, 琉璃 v.s. 玻璃</li>
    </ul>
  </li>
  <li>In fact, the silk road did not have a single merchant travelling on a camel to Rome, but more often:
    <ul>
      <li><strong>only few were long distance merchants</strong></li>
      <li>most stayed on circuit close to home</li>
      <li>other silk road travelelrs include missionaries, refugees, and envoys for Kings, etc.</li>
    </ul>
  </li>
  <li>when a wave of conquests (Persian Empire) cut off the trade routes between China and Inda, the great age of SIlk Road came to an end, having lasted for over 1000 years</li>
</ul>

<p><strong>The Northern Wei Dynasty</strong></p>

<ul>
  <li>during early contact with India and Central Asiam the only government in north China to retrain power for over a century was establiedh by a non-Chinese people, the <mark>Tabgach 拓跋</mark>, or also reffered ot as <mark>鲜卑</mark> (name of the tribe)</li>
  <li>Xianbei people speaks <strong>both Turic and Mongolian laguages</strong>, but had no written langue of their own</li>
  <li>Xianbei people had:
    <ul>
      <li>a system of tanistry: more powerful person in duel is the leader</li>
      <li>women knoew how to tend to hers and hunt because men left for war. Hence <mark>women enjoyed more power</mark> than most other societies</li>
      <li>no stable capital, as it is defined where the ruler set camp</li>
    </ul>
  </li>
  <li>slowly shifted base from Yin Shan mountain in Inner Monolia to northern Shanxi, and staretd taxing the Chinese peasantry and <strong>adopted a Chinese law code</strong></li>
  <li>by the middle of fifth century, Northern Wei has defated most of their rivals to <strong>take control of north China</strong></li>
</ul>

<p><strong>Dowager Empress Feng and the Equal-Field System</strong></p>

<ul>
  <li>established a new captial in Pingcheng, and some of Tabgach are drawn to Chinese ways</li>
  <li>Dowager Empress Feng had a chance to enter the system and <strong>systematically appoint Chinese to office</strong> to lessen Tabgach influence</li>
  <li>Empress Feng seriouly wanted to transform Tabgach to Chinese:
    <ul>
      <li>wanted to tie the Chinese <strong>farming</strong> population to the land, as otherwise it is mostly bureaucrats and atisans who did not grow their own food</li>
      <li>this is also useful as it could serve as a source of income by taxing</li>
    </ul>
  </li>
  <li>the policy to realized the above is the <strong>Equal-Field system</strong>:
    <ul>
      <li>apointed land to all those of tax paying age
        <ul>
          <li>since this includes slaves, this means powerful landowning families could retain control over large areas of land</li>
        </ul>
      </li>
      <li>gives two types of lands, <strong>farmland</strong> for growing crops, which is short term investment since lands are often quickly redistributed when farmer stopped working</li>
      <li>and <strong>mulberry tree</strong> for silkworms, which is a long term investment as it takes a long time for the tree to mature</li>
    </ul>
  </li>
  <li>all of which is to tie people to the land, but did not take effect immediately
    <ul>
      <li>the system <strong>lasted into the eight century</strong>, as it helped tied people to land and increased revenue</li>
    </ul>
  </li>
</ul>

<p><strong>Adoption of Chinese Ways</strong></p>

<ul>
  <li>in 493, Emperor Xiaowen 孝文 (北魏孝文帝) implemented a series of measures designed to make his empire Chinese</li>
  <li>his measures include:
    <ul>
      <li>establish a new captial in Luoyang</li>
      <li>dropped their own family name of Tabgach and had surname of <strong>Yuan</strong></li>
      <li>also appointed the <strong>nine-grade system</strong></li>
      <li>advocated conversion to Daosim instead of Buddhism (may wanted to weaken Buddhist establishment for more power)</li>
    </ul>
  </li>
  <li>but he died young in 499, and the dynasty entered three decades of discord, where <mark>Northern Wei came to a miserable end in 534</mark></li>
</ul>

<p><strong>Growth of Buddhism under the Northern Wei</strong></p>

<ul>
  <li>even during those difficult times, Buddhism received wide support from different factions</li>
  <li>examples of its populance include:
    <ul>
      <li>“<strong>there is no place that does not have a Buddhist sanctuary</strong>”, by a prince in 518</li>
      <li>had large buddhist monarsteries up to 300 meters high, and <strong>built over 839 royal monasteries</strong>, and about thirty thousand places of worship</li>
    </ul>
  </li>
  <li>In 528, Erzhu tribes defeated Empress Ling, conquered Luoyang, named a new emperor, and Luoyang was laid waste in 534 from fire</li>
</ul>

<p><strong>Persistance of Tabgah Identity</strong></p>

<ul>
  <li>
    <p>even after the fall of Northern Wei Dynasty, Tabgach did not lose their identity since the warlords could not agree on a new emperor. Hence they went back to stay in north Shanxi</p>
  </li>
  <li>
    <p>two generations after Emperor 孝文, Tabgach people <strong>still retained their language and identity</strong></p>
  </li>
  <li>
    <p>however, the <mark>impact of their steppe culture on north Chinese is long-lasting</mark></p>

    <ul>
      <li>
        <p>Yan Zhitui recorde thah people in the north and south (of yellow river) behaved very differently:</p>

        <ul>
          <li>in the south husband can be extravagent even if wife and children suffer</li>
          <li>in the north, it is <strong>usually wife who runs the household, and sometimes husband has to put up with wife’s insult</strong></li>
        </ul>

        <p>where the latter seems related to Tabgach’s culture: women had more influence</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>The Move South</strong></p>

<ul>
  <li>before, the south is often viewed as <strong>uncivilized</strong>, and the wet climate (<mark>perfect for rice</mark>) is thought of as a <strong>home for illness</strong></li>
  <li>however, the fall of Luoyang prompted migration, as well as the dangers of battles</li>
  <li>consequently, many northern Chinese aristocrats moved to <mark>Nanjing</mark></li>
  <li>in south China, Daoist teachers were most active, but there were some buddhist influences as well. But there were somem dramatic changes:
    <ul>
      <li>for buddhist, the teachings of the <strong>Greater Vehicle largely superseded those of Lesser Vehicle</strong></li>
      <li>Daoist visionaries divided into <strong>Lingbao 灵宝 and Mao Shan 茅山</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Religious Life in South China</strong></p>

<ul>
  <li>the Liang emperor was interested in Daoism, studied it in Mao Shan Montain茅山, hence new Daoist sect took the name Mao Shan</li>
  <li>the Maoshan adherents continued the practices of Five Peck Daoist, but <strong>distance themselves from practice that had given the Doaist a bad name</strong>, specifically certian sexual rites</li>
  <li>had a system of seven levels housing both divine immortals as well as the spirits of the dead
    <ul>
      <li>slightly optimistic than Chinese view of afterlife were dead would go through a series of courts</li>
      <li>believed that you could <strong>lodge suits in the underground agasint people who had wronged them</strong></li>
      <li>however, you can rise a level if you had <strong>revealations from certified teachers</strong>, which in reality is a “secretive service” and costs a lot</li>
    </ul>
  </li>
</ul>

<p><strong>Religious Policies of Emperor Wu of the Liang</strong></p>

<ul>
  <li>he stued Daosit, but then in 504 urged his family and officails to <strong>give up Doaism for Buddhist teaching</strong></li>
</ul>

<p><strong>Critics of the Buddhist Establishments</strong></p>

<ul>
  <li>by the middle of 6th century, there are approx two million monks</li>
  <li>those clery <strong>did not work or serve in military</strong>, hence seen as a drain on state resource</li>
  <li>this caused <mark>ire in the emperor of Zhou dynasty</mark> (557 - 581) who succeeded the Qi, and ordered all Buddhist monks to return to lay life and <mark>all Buddhist texts and statues destoryed</mark>
    <ul>
      <li>even inside buddhism, there are criticism as they shelted criminals and engaged in loans</li>
      <li>however, those records are mostly about the wealthy clergy, so we don’t know the common clergy’s behavior</li>
    </ul>
  </li>
  <li>however, a <mark>short supression in 574 cuold do little to shake the hold</mark> of either Buddhist or Daoist, both of which had history over 4 centuries</li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Intro to East Asia: China]]></summary></entry><entry><title type="html">ELEN6885 Reinforcement Learning part1</title><link href="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning.html/" rel="alternate" type="text/html" title="ELEN6885 Reinforcement Learning part1" /><published>2022-12-15T00:00:00+00:00</published><updated>2022-12-15T00:00:00+00:00</updated><id>/lectures/2022@columbia/ELEN6885_Reinforcement_Learning</id><content type="html" xml:base="/lectures/2022@columbia/ELEN6885_Reinforcement_Learning.html/"><![CDATA[<p>Reinforcement Learning</p>

<hr />

<p>The office hours will take place by default in EE dept. student lounge, <strong>13th FL, Mudd building</strong>. TA’s may decide to host their office hours remotely. If that is the case they will post an announcement here with a link.</p>

<table>
  <thead>
    <tr>
      <th>TA</th>
      <th>Day</th>
      <th>Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Trevor Gordon</td>
      <td>Monday</td>
      <td>1:30 PM - 3:30 PM</td>
    </tr>
    <tr>
      <td>Xin Huang</td>
      <td>Tuesday</td>
      <td>9:30 am - 11:30 am</td>
    </tr>
    <tr>
      <td>Jinxuan Tang</td>
      <td>Wednesday</td>
      <td>3:30 pm - 5:30 pm</td>
    </tr>
    <tr>
      <td>Yukai Song</td>
      <td>Thursday</td>
      <td>9:30 am - 11:30 am</td>
    </tr>
    <tr>
      <td>Gokul Srinivasan</td>
      <td>Friday</td>
      <td>3:30 PM - 5:30 PM</td>
    </tr>
  </tbody>
</table>

<h1 id="introduction-to-rl">Introduction to RL</h1>

<p><strong>Instructors</strong>:</p>

<ul>
  <li>Chong Li (first half of the semester)</li>
  <li>Chonggang Wang (second half)</li>
</ul>

<p><strong>Exam</strong>:</p>

<ul>
  <li>bi-weekly assigments 50% (first assginment out after week 2)</li>
  <li>midterm exam 20%</li>
  <li>final exam 30%</li>
</ul>

<p><strong>Key aspects of RL</strong> (as compared to other algorithms):</p>

<ul>
  <li>no supervisor, only reward signal (and/or environment to interact with)</li>
  <li>feedback/reward could be delayed</li>
  <li>time matters, as it is a sequenitial decision making</li>
</ul>

<p>The general process of interaction looks like:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170944706.png" alt="image-20220915170944706" style="zoom: 50%;" /></p>

<p>where essentially, the agent learns</p>

<ul>
  <li>
    <p>a <strong>policy</strong> (e.g. the decision making agent)</p>

\[s \in S \to \pi(a|s),a\in A(s)\]

    <p>since the action space could be dependent on the state. Note that a policy could be <strong>stochastic</strong> or <strong>determinstic</strong></p>
  </li>
  <li>
    <p>a <strong>reward</strong> indicates the <mark>desirability</mark> of that state. Reward is immediate or sometimes delayed (but as long as you have a reward for the entire episode, it is fine)</p>
  </li>
  <li>
    <p>a <strong>return</strong> is a cumulative sequence of received rewards after a given time step</p>

\[G_t = R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^T \gamma^k R_{t+1+k}\]

    <p>where usually $\gamma &lt; 1$ if we have infinite episodes $T \to \infty$</p>
  </li>
  <li>
    <p>a <strong>value function</strong> (which depends on the policy)</p>

\[V_\pi(s) = \mathbb{E}_\pi [G_t|S_t=s] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+1+k} |S_t=s \right]\]

    <p>given that I am currently at $s$, what is the <em>expected return if I follow the policy $\pi$</em>.</p>
  </li>
  <li>
    <p>a <strong>action-value function</strong></p>

\[Q_\pi(s,a) =  \mathbb{E}_\pi [G_t|S_t=s, A_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+1+k} |S_t=s, A_t=a\right]\]

    <p>given that I am currently as $s$, what is the <em>expected return if I take $a$ as next step and then follow policy $\pi$</em>. Note that if we take a “marginal” on $a$, we can get the $V_\pi$ from the $Q_\pi$:</p>

\[V_\pi(s) = \sum_{a \in A(s)} Q_\pi(s,a) \pi(a|s)\]

    <p>which can be easily seen from their definitions. The reverse can be derived as well:</p>

\[Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')\]
  </li>
  <li>
    <p>a <strong>world model</strong> (optional, so that if you know this, planning becomes much easier)</p>

\[\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1} = s' | S_t=s , A_t=a]\]

    <p>which is the transition probability</p>

\[\mathcal{R}_s^a = \mathbb{E}[R_{t+1} | S_t=s,A_t=a]\]

    <p>which is the reward model. Note that it is expected when what happens next is stochastic.</p>
  </li>
</ul>

<blockquote>
  <p>The agent’s goal is to find a <strong>policy</strong> to <strong>maximize the total amount of reward</strong> it receivers over the long run.</p>
</blockquote>

<hr />

<p><em>Examples</em></p>

<ul>
  <li>
    <p><strong>Maze</strong></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Maze Map</th>
          <th style="text-align: center">Reward</th>
          <th style="text-align: center">$V^{\pi^*}(s)$</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170101082.png" alt="image-20220915170101082" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170120864.png" alt="image-20220915170120864" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170136366.png" alt="image-20220915170136366" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>note that</p>

    <ul>
      <li>
        <p>each step has a $-1$ reward (we designed, so that the agent finds the shortest path = max reward), and states is the agent’s location</p>
      </li>
      <li>
        <p>these values are computed given the optimal policy</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Student Markov Chain</strong>: consider some given stochastic <em>policy</em>:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Student Markov Chain</th>
          <th style="text-align: center">+Reward</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170318596.png" alt="image-20220915170318596" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170339032.png" alt="image-20220915170339032" style="zoom: 67%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>we can sample episodes for student markov chain starting from $S_1=C_1$ and obtain episodes such as:</p>

    <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170437631.png" alt="image-20220915170437631" style="zoom:33%;" /></p>

    <p>Of course, for each episode, you can have some reward given some reward model:</p>

    <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170506093.png" alt="image-20220915170506093" style="zoom:50%;" /></p>

    <p>where essentially we are just computing the expected return $G_t$. But since you can have as much episodes as you want, we can consider:</p>

\[V(S=C_1)=\mathbb{E}[G_t | S_t=C_1] = \sum_{i=1}^\infty P(\tau_i)\cdot V(\tau_i)\]

    <p>where probability of a trajectory $P(\tau_i)$ we can get since we know the transition model. However, note that this is impossible to compute since we can have infinite episodes (so we need other tricks, e.g. Bellman Equations &amp; Dynamic Programming)</p>
  </li>
</ul>

<hr />

<p>This gives rise to the definitions:</p>

<blockquote>
  <p><strong>Markov Decision Process</strong> is a tuple $\lang S,A,\mathcal{P},\mathcal{R},\gamma \rang$</p>

  <ul>
    <li>
      <p>$\mathcal{S}$ is a finite set of states</p>
    </li>
    <li>
      <p>$\mathcal{A}$ is a finite set of actions</p>
    </li>
    <li>
      <p>$\mathcal{P}$ is a state trainsition probability matrix:</p>

\[\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]\]
    </li>
    <li>
      <p>$\mathcal{R}$ is a reward function, so that $\mathcal{R}<em>s^a = \mathbb{E}[R</em>{t+1}\vert S_t=s, A_t=a]$ for cases when reward is not deterministic</p>
    </li>
    <li>
      <p>$\gamma$ being the discount factor $\gamma \in [0,1]$.</p>
    </li>
  </ul>
</blockquote>

<p>so that if the problem can be formulated into a MDP, then we can apply RL algorithms.</p>

<p>And finally, after we find a policy using RL algorithm/or even train a policy using RL, we need to have access to an <strong>environment</strong>: in this course we will use <mark>OpenAI Gym</mark>. (simulation are used usually because human-invovled real environment would be too costly)</p>

<h2 id="rl-history">RL History</h2>

<p>There are two threads of development</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220915170902678.png" alt="image-20220915170902678" style="zoom:50%;" /></p>

<p>So that essentially:</p>

<ul>
  <li>control theory includes the Bellman Optimality Equations, and essentially solves the problem of how to plan, <strong>given a known model</strong></li>
  <li>the trial and error structure gives rise to the structure of how to train an agent to learn how to plan <strong>when we don’t know the model</strong></li>
</ul>

<h1 id="bandit-problem-and-mdp">Bandit Problem and MDP</h1>

<p>Fundemental mathematical stuff behind RL algorithms.</p>

<h2 id="bandit-problem">Bandit Problem</h2>

<p>One of the first problem that people look at. The problem is simple, but the solution shows principles in RL. The classical is the n-armed bandit problem</p>

<blockquote>
  <p><strong>N-armed Bandit Problem</strong>: suppose you are facing two machines, where both are Gaussian and</p>

  <ul>
    <li>reward from the first gives $\mu=5$, with variance 1</li>
    <li>reward from the second gives $\mu=7$, with variance $1$</li>
  </ul>

  <p>The obviously we will use the second machine. But in reality, the <strong>above information is not available</strong>. So what do you do? Your objective is to <mark>maximize the expected total reward over some time period</mark>.</p>
</blockquote>

<p><strong>Intuitively</strong>, the idea should be:</p>

<ol>
  <li>spend some initial money to explore and estimate the epectation using average reward</li>
  <li>then play towards the better machine</li>
</ol>

<p>With this, we consider $Q_t(a)$ value being the estimated value/average reward so far of action $a$ at the $t$-th play:</p>

\[Q_t(a) = \frac{R_1 +R_2 + ... + R_{K_a}}{K_a}\]

<p>for $K_a$ is the number of times action $a$ was chosen and $R_1,…,R_{K_a}$ being the reward associated with action $a$.</p>

<p>Once you estimated $Q_t(a)$, we can consider:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916082722457.png" alt="image-20220916082722457" style="zoom: 67%;" /></p>

<p>where we can test:</p>

<ul>
  <li>
    <p><strong>greedy</strong> policy (or $\epsilon=0$), choosing the machine with current best $Q_t(a)$</p>
  </li>
  <li><strong>$\epsilon$-greedy</strong> means you choose a random machine with probability $\epsilon/N$ even though you know the current best. E.g. if you have, at step $t=10$, achieved $Q_{10}(a_1)=4.5,Q_{10}(a_2)=4$, then with $\epsilon=0.2$:
    <ul>
      <li>choose $a_1$ with probability $0.8+0.1=0.9$</li>
      <li>choose $a_2$ with probability $0.1$</li>
    </ul>
  </li>
  <li>why is $\epsilon=0$ not good? Once you have made a unlucky estimation of the average, you may be stuck at a suboptimal solution. Especially in the early stage. But with $\epsilon$-greedy, I always have chance to jump to the correct arm.
    <ul>
      <li>note that of course, there are cases where greedy policy works. But more often $\epsilon$-greedy gives more consistent performance.</li>
    </ul>
  </li>
  <li>but what is the optimal solution?</li>
</ul>

<blockquote>
  <p><strong>Exploitation vs. exploration dilemma</strong>: Should you <mark>exploit</mark> the information you’ve learned or <mark>explore</mark> new options in the hope of greater payoff?</p>

  <ul>
    <li>this is what makes most RL problem hard to find the optimal</li>
  </ul>
</blockquote>

<p>An alternative to $\epsilon$-greedy is the <strong>Softmax policy</strong> (to balance exploration and exploitation)</p>

<blockquote>
  <p><strong>Softmax Policy</strong>: essentially use softmax of the Q values to give probability</p>

\[\mathrm{Softmax}(Q(a)) = \frac{\exp(Q_t(a)/\tau)}{\sum_{a \in A} \exp(Q_t(a)/\tau)}\]

  <p>note that $\tau$ has a critical effect:</p>

  <ul>
    <li>if $\tau \to \infty$, the distribution will become almost uniform</li>
    <li>if $\tau \to 0$, then it becomes greedy action selection</li>
  </ul>
</blockquote>

<p>But which one is better, softmax or $\epsilon$-greedy? In practice it depends on applications.</p>

<hr />

<p><strong>Notes on Algorithmic Implementation</strong></p>

<ul>
  <li>
    <p>do you need to store all rewards to estimate average reward $Q_k$ for each action? No, you can use a <strong>moving average</strong></p>

\[\begin{align}
Q_{k+1}
&amp;= \frac{1}{k} \sum_{t=1}^k R_t\\
&amp;= \frac{1}{k} \left( R_t + \sum_{i=1}^{k-1} R_i \right) \\
&amp;= \frac{1}{k} \left( R_t + kQ_k - Q_k \right) \\
&amp;= Q_k + \frac{1}{k}[R_k - Q_k]
\end{align}\]

    <p>where $Q_k$ is for the computed average reward from the previous step, and <strong>$R_k$ is the new info</strong>. Note that <mark>this form is very commonly seen in RL</mark>:</p>

\[\text{New Estimate} \gets \text{Old Estimate} + \alpha \underbrace{(\text{Target - Old Estimate})}_{\text{innovation term}}\]

    <p>where the innovation term would give you some idea of the new information. The step size is there because we know the innovation term will oscillate since $\mathrm{Target}$ might contain inaccuraries</p>
  </li>
  <li>
    <p>what is a <strong>good step size</strong> to choose? We want your estimate, e.g. $Q$-value to converge.</p>

    <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916084728218.png" alt="image-20220916084728218" style="zoom:50%;" /></p>

    <p>an example for this to work is $\alpha=1/k$. Essentially, the intuition behind this is that it will take past information a bit more, but you should <em>always have some non-trivial weight on the new information</em>.</p>
  </li>
  <li>
    <p>What if this is a <strong>non-stationary</strong> problem? i.e. the distribution for the n-arm bandit machine is <em>varying over time</em>, e.g. mean changes over time. In this case, the convergence doesn’t mean anything since the “correct” mean is moving. In this case, we can choose $\alpha$ being a constant.</p>
  </li>
</ul>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>MDP formally describe an environment for reinforcement learning, where the <mark>environment is fully observable</mark>. First</p>

<blockquote>
  <p><strong>Markov Property</strong>: the future is independent of the past given the present. i.e. I only care about present, past gives no information</p>

\[P(S_{t+1}|S_t) = P(S_{t+1}|S_1,...,S_t)\]

  <p>may or may not be a good assumption in some problems. However, you can always make this a good assumption, e.g. <mark>if you choose $S_t = \text{everything up to time t}$, then this trivially holds</mark>.</p>
</blockquote>

<p>A recap of the definitions</p>

<blockquote>
  <p><strong>Markov Process</strong> is a tuple $\lang S,\mathcal{P}\rang$</p>

  <ul>
    <li>
      <p>$\mathcal{S}$ is a finite set of states</p>
    </li>
    <li>
      <p>$\mathcal{P}$ is a state trainsition probability matrix:</p>

\[\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]\]
    </li>
  </ul>

</blockquote>

<p>An example of the transition matrix is</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916085816449.png" alt="image-20220916085816449" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>Markov Reward Process</strong> is a tuple $\lang S,\mathcal{P},\mathcal{R},\gamma \rang$</p>

  <ul>
    <li>
      <p>$\mathcal{S}$ is a finite set of states</p>
    </li>
    <li>
      <p>$\mathcal{P}$ is a state trainsition probability matrix:</p>

\[\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1}=s'|S_t=s]\]
    </li>
    <li>
      <p>$\mathcal{R}$ is a reward function, so that $\mathcal{R}<em>s = \mathbb{E}[R</em>{t+1}\vert S_t=s]$ for cases when reward is not deterministic</p>
    </li>
    <li>
      <p>$\gamma$ being the discount factor $\gamma \in [0,1]$. Useful for infinite episodes</p>
    </li>
  </ul>
</blockquote>

<p><em>For example:</em></p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916090117592.png" alt="image-20220916090117592" style="zoom: 50%;" /></p>

<p>Then from this, we can also define</p>

<ul>
  <li>
    <p><strong>return</strong>: think of the $R_{t+1}$ as going from a MDP to a MRP. If you start at Class 1, then you should also include $R=-2$ in your computation, just as in MDP, the reward</p>

\[G_t = R_{t+1} + \gamma R_{t+2} += ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}\]
  </li>
  <li>
    <p>the choice of $\gamma$ affects the bahavior/goal of your algorithm</p>

    <ul>
      <li>$\gamma=0$ leads to myopic evaluation</li>
      <li>$\gamma=1$ leads to far-sighted evaluation</li>
    </ul>
  </li>
</ul>

<p>Additionally, we can also define <strong>state-value function</strong></p>

<blockquote>
  <p><strong>State Value Function</strong> (for MRP): the expected return starting from state $s$</p>

\[V(s) = \mathbb{E}[G_t|S_t=s]\]

</blockquote>

<p>which we have discussed before, and to compute this intuitively would be:</p>

<ul>
  <li>start from a state $s$</li>
  <li>compute expected reward for all possible trajectory from this state</li>
  <li>weight it by probability of each trajectory</li>
</ul>

<p>Which would give this</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916090938392.png" alt="image-20220916090938392" style="zoom:50%;" /></p>

<blockquote>
  <p>Is there an easier way than sample, weight, and sum? The <strong>Bellman Equation</strong>.</p>
</blockquote>

<p>We can decompose the value function definition, and see what we get</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916091139515.png" alt="image-20220916091139515" style="zoom:50%;" /></p>

<p>where only the last step is tricky:</p>

<ul>
  <li>
    <p>what is $\mathbb{E}[\gamma G_{t+1}\vert S_t=s]$? By definition of expected value:</p>

\[\begin{align*}
\mathbb{E}[\gamma G_{t+1}|S_t=s]
&amp;= \gamma \sum_g G_{t+1} P(G_{t+1}|S_t=s)\\
&amp;= \gamma \sum_g G_{t+1} \sum_{s'} P(G_{t+1}, S_{t+1}=s'|S_t=s)\\
&amp;= \gamma \sum_g G_{t+1} \sum_{s'} P(G_{t+1}|S_{t+1}=s', S_t=s)P(S_{t+1}=s'|S_t=s)\\
&amp;= \gamma  \sum_{s'} \left( \sum_g G_{t+1}  P(G_{t+1}|S_{t+1}=s') \right) P(S_{t+1}=s'|S_t=s)\\
&amp;= \gamma \sum_{s'\in S}V(s') P(S_{t+1}=s'|S_t=s)\\
&amp;= \mathbb{E}[\gamma V(s') | S_t=s]
\end{align*}\]

    <p>where:</p>

    <ul>
      <li>the aim for the second equality is because we know the final result has $v(S_{t+1})$, so we need to introduce a next state $s’$.</li>
      <li>the third equality comes from applying chain rule for joint probability</li>
      <li>the fourth equality comes from the Markov Assuption, where $G_{t+1}$ does not depend on the past, only present. And that we are taking $G_{t+1}$ since it does not depend on the future state $s’$</li>
    </ul>
  </li>
  <li>
    <p>now, given the above, we essentially realize for an ealier state:</p>

\[\begin{align*}
  V(s) 
  &amp;= \mathbb{E}[R_{t+1}|S_t=s] + \gamma \mathbb{E}[V(S_{t+1})|S_t=s] \\
  &amp;= \mathcal{R}_s + \gamma \sum\limits_{s' \in S} \mathcal{P}_{ss'} V(s')
\end{align*}\]

    <p>for $\mathcal{R}$ is the expected reward in case reward is stochastic. And in matrix form:</p>

\[V = \mathcal{R} + \gamma \mathcal{P}V\]

    <p>for instance:</p>

    <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916092443352.png" alt="image-20220916092443352" style="zoom:33%;" /></p>

    <p>so given this, <strong>can we solve for $V$</strong> (which is our final goal)? Simply solve it if given $P,R$:</p>

\[\begin{align*}
V 
&amp;= R+\gamma P V\\
V &amp;=(I - \gamma \mathcal{P})^{-1} R
\end{align*}\]

    <p>An example using this to compute $V$ from the closed form solution:</p>

    <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916092636709.png" alt="image-20220916092636709" style="zoom: 50%;" /></p>

    <p>but is this all? We have real life concerns:</p>

    <ul>
      <li>The complexity of <em>this approach</em> is $O(n^3)$ due to the inverse, for $n$ being the number of states</li>
      <li>Most often in reality you are <mark>not given $P$</mark>, i,e, you don’t know it</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Two important results from the previous discussion for MRP:</p>
  <ul>
    <li>
      <p>Bellman’s equation for $V$:</p>

\[V = \mathcal{R} + \gamma \mathcal{P}V\]
    </li>
    <li>
      <p>and we can solve for $V$ easily in closed form:</p>

\[V =(I - \gamma \mathcal{P})^{-1} R\]
    </li>
  </ul>

</blockquote>

<p>However</p>

<blockquote>
  <p><strong>For large MDPs, or unknown transition models:</strong> we have the following algorithms to be our savior</p>

  <ul>
    <li>Dynamic programmming (known transition, but more efficient)</li>
    <li>Monte-Carlo evaluation (sampling, unknown transition)</li>
    <li>Tempora-Difference learning (sampling, unknown transition)</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Professor Li:</strong> “Why you need algorithm if you have the closed form solution? All algorithms are there to exchange for <em>compute</em>, or more oftenly, <em>approximate</em> a solution”.</p>
</blockquote>

<p>Finally, we introduce our main problem:</p>

<blockquote>
  <p><strong>Markov Decision Process</strong> is a tuple $\lang S,A,\mathcal{P},\mathcal{R},\gamma \rang$</p>

  <ul>
    <li>
      <p>$\mathcal{S}$ is a finite set of states</p>
    </li>
    <li>
      <p><mark>$\mathcal{A}$ is a finite set of actions</mark></p>
    </li>
    <li>
      <p>$\mathcal{P}$ is a state trainsition probability matrix:</p>

\[\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1}=s'|S_t=s,\textcolor{red}{A_t=a}]\]
    </li>
    <li>
      <p>$\mathcal{R}$ is a reward function, so that $\mathcal{R}<em>s^a = \mathbb{E}[R</em>{t+1}\vert S_t=s, A_t=a]$ for cases when reward is not deterministic</p>
    </li>
    <li>
      <p>$\gamma$ being the discount factor $\gamma \in [0,1]$.</p>
    </li>
  </ul>
</blockquote>

<p>and remember that, most of the time, we do no have access to $P$.</p>

<p>Then, <mark>when given a policy $\pi(a\vert s)$, we are back to a MRP</mark>: it becomes a process of $\left\langle \mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma \right\rangle$, where:</p>
<ul>
  <li>
    <p>the transition matrix’s action is removed by:</p>

\[\mathcal{P}^{\pi}_{ss'} = \sum_a \pi(a|s) \mathcal{P}_{ss'}^a\]

    <p>i.e. if I take an action $a$ at state $s$, what the the trainsition probability to $s’$? Then I take the average, to get the averae transition probability from $s\to s’$ for the policy</p>
  </li>
  <li>
    <p>the reward function’s action is removed by:</p>

\[\mathcal{R}^{\pi}_s = \sum_a \pi(a|s) \mathcal{R}_s^a\]

    <p>for $\mathcal{P}_{ss’}^{a}, R_s^{a}$ are given as part of the MDP.</p>
  </li>
  <li>
    <p>when given a policy, your MDP evaluations become MRP evaluations, since now $P^\pi_{s,s’}$ is your transition matrix independent of action, and you can compute them in a closed form using our previous result.</p>
  </li>
</ul>

<blockquote>
  <p>But now, <mark>value function</mark> will depend on the <mark>policy I use</mark>, as compared to the previous cases:</p>

\[V_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]\]

  <p>and <mark>so is the action-value function</mark></p>
</blockquote>

<p><em>For instance</em>: consider a given policy $\pi(a\vert s)=0.5$ uniformly, with $\gamma=1$. Then you should get</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916094059606.png" alt="image-20220916094059606" style="zoom: 50%;" /></p>

<p>using essentially the MRP results we got, and reducing MDP given a policy to MRP.</p>

<p>But now, the <mark>Bellman equations depend on the policy</mark>, and we have the following:</p>

<blockquote>
  <p><strong>Bellman Expectation Equation</strong>: notice the connection between value function and action-value function</p>

  <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916113343.png" style="zoom:30%;" /></p>

  <p>giving the equations for:</p>

  <ul>
    <li>Value function $V_\pi(s)$ being averaging over next value functions</li>
    <li>Action-value function $Q_\pi(s,a)$ being averaging over next action-value functions after taking action $a$ at state $s$
Hence we get:</li>
  </ul>

\[V_\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma V_\pi(S_{t+1})|S_t=s]\]

\[Q_\pi(s,a) = \mathbb{E}_\pi [R_{t+1} + \gamma Q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]\]

  <p>or we can express them as:</p>

\[V_\pi(s) = \sum_a \pi(a|s) Q_\pi(s,a)\]

\[Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s')\]

  <p>then using those relationships, I can express <mark>$V$ in terms of $V$</mark>, and <mark>$Q$ in terms of $Q$</mark>, which gives the Bellman Expectation Equation:</p>

\[V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s') \right)\]

\[Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \sum_{a'} \pi(a'|s') Q_\pi(s',a')\]

  <p>which is basically the <mark>same as the equation we had with $\mathbb{E}_\pi$</mark>. Note that</p>

  <ul>
    <li>when transition is deterministic given an action, then $\mathcal{P}<em>{ss’}^a=1$ for the destined state $(s,a) \to s’$, and $\mathcal{P}</em>{ss’}^a=0$ otherwise.</li>
    <li>similarly, the expected reward $\mathcal{R}_s^a$ is used when reward is non-deterministic given a $s,a$,</li>
  </ul>
</blockquote>

<p>And with this, we again have the closed form solution as implied by the above as we reduced MDP to MRP once we have a policy:</p>

<blockquote>
  <p><strong>Closed Form Solution for $V^\pi$</strong> in MDP: the Bellman’s expectation equation can essentially be translated to the MRP version:</p>

\[V_\pi = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} V_\pi\]

  <p>and the closed form solution is:</p>

\[V_\pi = (\mathbb{I} - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}\]

  <p>for $\mathcal{P}^\pi, \mathcal{R}^\pi$ being the transition and reward matrix reduced to MRP implied by the policy $\pi$.</p>
</blockquote>

<h2 id="optimal-value-functions-and-policies">Optimal Value Functions and Policies</h2>

<p>Now, we understand the basics of MDP, and how we are essentially solving it by reducing to MRP, we come back to the central problem of control: <mark>how to find the *optimal* policy and/or the optimal value function?</mark></p>

<p>First, we need to define what they are:</p>
<blockquote>
  <p><strong>Optimal Value Function</strong>: the value function that maximizes the expected return for any given policy $\pi$:</p>

\[V_*(s) = \max_\pi V_\pi(s)\]

  <p>and the optimal action-value function is the same:</p>

\[Q_*(s,a) = \max_\pi Q_\pi(s,a)\]

</blockquote>

<p>But what about optimal policy, which if we recall are distributions $\pi(a\vert s)$? We can define it as:</p>

<blockquote>
  <p><strong>Optimal Policy</strong>: we can define a partial ordering</p>

\[\pi &gt; \pi' \iff V_\pi(s) \geq V_{\pi'}(s),\quad \forall s\]

  <p>and note that, the two most important attribute for a problem</p>
  <ul>
    <li>(existence) solution always exists. For any MDP, there eixts an optimal policy (e.g. see example below)</li>
    <li>(uniqueness) the optimal policy <em>not</em> unique, but all optimal policies achieve the same value function/action-value functin.</li>
  </ul>
</blockquote>

<p>For instance, we can give an optimal policy better than all other policies</p>

\[\pi_*(a|s) = \begin{cases}
1 &amp; \text{if } a=\arg\max_{a\in A} Q_*(s,a)\\
0 &amp; \text{otherwise}
\end{cases}\]

<p>which is optimal, meaning there always exist an optimal policy. But in reality, we <mark>don't know $Q_*$ yet</mark>. So our task remains how we can find such a policy, now that we know it exists.</p>

<blockquote>
  <p><strong>Bellman Optimality Equation</strong>: we can again draw the graph, and derive the optimality equations from $V_<em>,Q_</em>$, which gives us some idea how we can find it</p>

  <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220916113343.png" style="zoom:30%;" /></p>

  <p>giving the equations for intuitively as:</p>

\[V_*(s) = \max_a Q_*(s,a)\]

\[Q_* (s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_*(s')\]

  <p>and we can combine them to obtain a form only including itself:</p>

\[V_*(s) = \max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_*(s') \right)\]

\[Q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \max_{a'} Q_*(s',a')\]

  <p><mark>this is essentially the basis of all RL algorithms and what they aim to solve</mark></p>
</blockquote>

<p>But now there is a $\max$, and we <mark>can no longer have a closed form solution</mark> as the equations become nonlinear. Therefore, we need to use iterative solution algorithms to <mark>approximate them</mark> (these two equations are the fundementals of the whole RL algorithm)</p>
<ul>
  <li>Value Iteration</li>
  <li>Policy Iteration</li>
  <li>Q-learning</li>
  <li>SARSA</li>
  <li>etc.</li>
</ul>

<h2 id="extended-mdp">Extended MDP</h2>

<p>Sometimes, this is not MDP by nature, but we can do some modification and convert them to MDP</p>

<p><strong>Partially Obseravable MDP</strong>: for instance, the robot can only observe neighboring grids, but not the global information</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923081511421.png" alt="image-20220923081511421" style="zoom:33%;" /></p>

<p>therefore, technically:</p>

<ul>
  <li>I do not know $s$, but I know something related.</li>
  <li>Solved by introducing <mark>belief MDP</mark> (see wiki for more details)</li>
</ul>

<blockquote>
  <p><strong>Belief MDP</strong></p>

  <p>ideally, we want $\pi(a\vert s)$ given a state. All I have is a history $h=(A_0,O_1,…,A_{t-1},O_t,R_t)$</p>

  <p>therefore, we can consider a <strong>belief state</strong> (instead of state):</p>

\[b(h) = (\mathcal{P} (S_t=s^1 | H_t = h),\mathcal{P} (S_t=s^2 | H_t = h), ...,\mathcal{P} (S_t=s^n | H_t = h) )\]

  <p>which is a probablity over all possible states.</p>

  <ul>
    <li>note that we <mark>do not have to encode a belief state like this</mark>. In the case of a continous distirbutoin, this can be a gaussian distribution, for instance.</li>
  </ul>

  <p>then your policy becomes:</p>

\[\pi(a| b(h))\]

  <p>notice that belife states satisfy Markov propety</p>
</blockquote>

<hr />

<p><strong>Continous State MDP</strong>: what if your state is continuous</p>

<ul>
  <li>discretization on the continous-state.
    <ul>
      <li>but this becomes a trade-off between granuality = accurarcy v.s.</li>
    </ul>
  </li>
  <li>use a value function approximation to approximate value directly</li>
</ul>

<h1 id="model-based-rl">Model-based RL</h1>

<p>Keep in mind that all approaches is still tightly related to the bellman optimality equations</p>

<blockquote>
  <p><strong>Notation</strong>: note that we <em>try</em> to capitial letter such as $V$ to represent the true/random variable, and $v$ represent <em>our</em> estimate/realization.</p>
</blockquote>

<h2 id="introduction-of-dynamic-programming">Introduction of Dynamic Programming</h2>

<blockquote>
  <p>DP refers to a <strong>collection of algorithms</strong> that can be used to compute optimal policies given a <mark>perfect model</mark> of the <strong>environment</strong> as a MDP</p>

  <ul>
    <li>because at each iteration, we need some form of $\mathcal{P}<em>{ss’}^a V_k(s’)$ to update $V</em>{k+1}$</li>
    <li>for now, we assume a finite MDP, but DP ideas can easily be extended to cnotonus states or POMDP</li>
  </ul>
</blockquote>

<p>However, Classical DP algorithms are of limited utility in RL. Why?</p>

<ul>
  <li>Need perfect model</li>
  <li>Great computational expense</li>
</ul>

<p>But regardless, if those can be overcome, we would like to perform two tasks in general:</p>

<ul>
  <li><mark>prediction</mark>: given a policy $\pi$ and the MDP, evaluate how good it is by outputting its <strong>value funciton $V_\pi$</strong></li>
  <li><mark>control</mark>: given a MDP, find the <strong>optimal policy $\pi_*$</strong></li>
</ul>

<h2 id="prediction-policy-evaluation">Prediction: Policy Evaluation</h2>

<blockquote>
  <p><strong>Aim</strong>: given any policy $\pi$, we want to evaluate its value function in a MDP.</p>
</blockquote>

<p>The key idea is the Bellman equation: assuming we know everything on the RHS, then that should give me the correct value on LHS:</p>

\[V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_\pi(s') \right)\]

<p>using this idea of dependency=constraint, we can <strong>iteratively estimate $V_\pi(s)$</strong> by:</p>

\[V_{k+1}(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k}(s') \right)\]

<p>which is called a <strong>synchronous backup</strong> since it uses $V_k(s)$ even if we have <em>for some states a better estimate $V_{k+1}(s)$</em></p>

<blockquote>
  <p><strong>Synchronus Policy Evaluation</strong>:</p>

  <p><code class="language-plaintext highlighter-rouge">Repeat</code> until convergence:</p>

  <ol>
    <li>
      <p>store $V_k(s)$</p>
    </li>
    <li>
      <p>for each state $s \in S$:</p>

      <ol>
        <li>
          <p>estimaet $V_{k+1}(s)$ by</p>

\[V_{k+1}(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k}(s') \right)\]
        </li>
      </ol>
    </li>
    <li>
      <p>use the latest update $V_{k} \gets V_{k+1}$</p>
    </li>
  </ol>
</blockquote>

<p>But do we <em>always</em> needs such an interation, as we saw in some examples in HW1?</p>

<blockquote>
  <p><strong>Note</strong> that in homeworks, we had this <mark>bottom up approach</mark> where we found the correct value function <strong>in one step</strong>. For instance, we can compute the following value function of a policy:</p>

  <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923111844922.png" alt="image-20220923111844922" style="zoom:33%;" /></p>

  <p>then since $V(S=s_{t+2})=0$ for any state, we can compute $V(S=s_{t+1})$ <mark>without any error</mark> using the backup equation</p>

\[V_\pi(s) = \sum_a \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{\pi}(s') \right)\]

  <p>and all can be done over one iteration.</p>
</blockquote>

<p>But in reality, <mark>often your state space is not a tree</mark>, i.e. to get to $S=s_{14}$ in the example below, there will be <mark>loops</mark> in your tree (i.e. you can get to $s_{14}$ from $s_{13}$, and to get to $s_{13}$ you can also start from $s_{14}$). Therefore, since then you <mark>cannot estimate $V(S=s_{14})$</mark> correctly, you will need a loop for convergence.</p>

<hr />

<p><em>For Example</em>: Small Grid World</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923083659494.png" alt="image-20220923083659494" style="zoom:50%;" /></p>

<p>where we have:</p>

<ul>
  <li>reward of $-1$ for all non-terminal states, and there is one terminal state marked in grey</li>
  <li>
    <p>undisconuted MDP with $\gamma =1$</p>
  </li>
  <li>actions leading out of the state gives no changes (i.e. go left of $s_8$ is still $s_8$)</li>
</ul>

<p><strong>Question</strong>: if my policy is random $\pi(a\vert s)=0.25,\forall a$, what is the value of it/how good is this?</p>

<p>Since we also have transition model, we can use the DP algorithm. We can initialize $V_0(s)=0$:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923083836193.png" alt="image-20220923083836193" style="zoom:33%;" /></p>

<p>for intance, to compute $V_2(s=1)$ would be:</p>

\[\begin{align*}
V_{k=2}(s_1) 
&amp;= \sum_a 0.25 \cdot \left( -1 + \gamma \sum_{s'} \mathcal{P}_{ss'}^a V_{k=1}(s') \right) \\
&amp;= 0.25 (-1 + 0) + 0.25 (-1 + -1) + 0.25 (-1 + -1) + 0.25 (-1 + -1) \\
&amp;= -1.75
\end{align*}\]

<p>where all the $V_1(s)$ comes from our estimate from $v_1$ at the previous timestep.</p>

<p><strong>Additional Question</strong>: what would be my control?</p>

<p>(This is just a lucky case that we got optimal policy in one greedy step.) We know that the value function is for a random policy, but can we get a better policy from it? Notice that by simply using the <strong>greedy policy of $\pi$</strong>:</p>

<p>| <img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923084435565.png" alt="image-20220923084435565" style="zoom:50%;" /> | <img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923084439721.png" alt="image-20220923084439721" style="zoom:50%;" /> |
| ———————————————————— | ———————————————————— |</p>

<p>notice that this is essentially performing <strong>exactly one iteration of policy improvement</strong>, but we luckily get the optimal policy just by doing policy improvement once</p>

<h2 id="control-policy-improvement-and-policy-interation">Control: Policy Improvement and Policy Interation</h2>

<p>Now I have evaluated a policy, I would like to improve the policy and perhaps <mark>find the best policy</mark></p>

<blockquote>
  <p><strong>Policy Improvement</strong>: generate a policy $\pi’$ that is better than $\pi$</p>

  <p><em>one way</em> to guarantee improvement is to apply a greedy policy</p>

\[\pi' = \mathrm{greedy}(V_\pi)\]

  <p>and we can later show that indeed improves the policy and is useful.</p>
</blockquote>

<blockquote>
  <p><strong>Policy Iteration</strong>: since we can get a better policy from old value, then we can iterate:</p>

  <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923084722197.png" alt="image-20220923084722197" style="zoom:33%;" /></p>

  <p><code class="language-plaintext highlighter-rouge">Repeat</code> until no improvement/convergence</p>

  <ol>
    <li>(<strong>Policy Evaluation</strong>) evaluate current policy $\pi$</li>
    <li>(<strong>Policy Improvement</strong>) improve the policy (e.g. by acting greedily) by generating $\pi’ \ge \pi$</li>
  </ol>
</blockquote>

<p>This gives the <strong>policy iteration algorithm</strong> in more details:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923093222983.png" alt="image-20220923093222983" style="zoom:50%;" /></p>

<p>where essentially</p>

<ul>
  <li>
    <p>during policy evaluation, we are taking $\Delta$ to be the largest error we encountered for each state</p>
  </li>
  <li>
    <p>we are selecting the greedy policy by selecting the best action given the value function.</p>
  </li>
</ul>

<hr />

<p><em>Proof</em>: We want to prove that greedy policy always improve the policy, hence we have a <mark>monotonic improvement</mark></p>

<p>Consider some existing deterministic policy $\pi(s)$, and we have a greedy policy version $\pi’(s)$:</p>

\[\pi'(s) = \mathrm{greedy}(\pi(s)) = \arg\max_{a \in A}q_\pi(s,a)\]

<p>Then by definition, the <strong>action-value must have obviously improved the policy by one-step</strong></p>

\[q_\pi(s,\pi'(s)) = \max_{a \in A} q_\pi(s,a) \ge q_\pi(s,\pi(s)) = v_\pi(s)\]

<p>where the last step is due to the fact that we are using a <strong>determistic policy</strong>, so the state-value function will be the same as value function since you <strong>always only choose one action per state</strong>.</p>

<p>Finally, we just need to show that $V_{\pi’}(s) \ge V_\pi(s)$ by connecting the above</p>

\[\begin{align*}
v_{\pi}(s)
&amp;\le q_\pi(s,\pi'(s)) \\
&amp;= \mathbb{E}_{(a,r,s'|s) \sim \pi'}[ \mathcal{R}_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s ] \\
&amp;\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s ] \\
&amp;\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma R_{t+2} + \gamma^{2} q_\pi(S_{t+2}, \pi'(S_{t+2})) | S_t=s ] \\
&amp;\le \mathbb{E}_{\pi'}[ \mathcal{R}_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... | S_t=s ] \\
&amp;= v_{\pi'}(s)
\end{align*}\]

<p>where:</p>
<ul>
  <li>the second equality is because we have are sampling the immediate next step from $\pi’$, but the rest is following $\pi$.</li>
  <li>third inequality relies on the fact that we are a deterministic policy and $q_\pi(s,\pi’(s)) \ge v_\pi(s)$ by construction</li>
</ul>

<p>Notice that since the improvement is monotonic, then we have everything becomes an equality at convergence:</p>

\[q_\pi(s,\pi'(s)) = \max_{a \in A} q_\pi(s,a) = q_\pi(s,\pi(s)) = v_\pi(s)\]

<p>meaning that the Bellman optimality equation is satisfied</p>

\[v_\pi(s) = \max_{a \in A}q_\pi(s,a)\]

<p>which means the $\pi$ we get at the <strong>end of convergence</strong> is the optimal policy</p>

<hr />

<blockquote>
  <p><strong>Generalized Policy Iteration</strong>: the general recipe include:</p>
  <ol>
    <li>any policy evaluation algorithm</li>
    <li>any (proven) policy improvement</li>
    <li>repeat until convergence to the optimal policy</li>
  </ol>
</blockquote>

<p>But whatever new algorthim you come up with, we want to measure/compare (note that the converged value function is optimal, hence the same):</p>

<ul>
  <li><strong>convergence speed</strong>: how fast the algorithm can converge</li>
  <li><strong>convergence performance</strong>: same convergence speed, but <em>variance</em> during training could be big/small</li>
</ul>

<h2 id="control-value-iteration">Control: Value Iteration</h2>

<blockquote>
  <p><strong>Aim</strong>: We want to <mark>improve the speed of the policy iteration algorithm</mark> because <strong>each of its iterations involves policy evaluation</strong>, which itself is an iterative computation through the state set.</p>
</blockquote>

<p>Intuition, we notice that at $k=3$ in the example before, just one greedy policy we already get the optimal policy <mark>even if the value function has not converged</mark></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923084435565.png" alt="image-20220923084435565" /></th>
      <th style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923084439721.png" alt="image-20220923084439721" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Therefore, maybe we can directly improve before waiting for converged value function</td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<p><strong>Value Iteration</strong>:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923094259901.png" alt="image-20220923094259901" style="zoom: 50%;" /></p>

<p>where essentially</p>

<ul>
  <li>
    <p>I computed my $v(s’)$ for one round, and took the max (greedy policy) directly afterwards</p>
  </li>
  <li>notice that this is also the Bellman Optimality Equation</li>
  <li>it is also proven that this <strong>converges to the optimal value</strong></li>
</ul>

<blockquote>
  <p><strong>Value Iteration is used much more often</strong> than policy iteration due to its faster compute</p>
</blockquote>

<h2 id="summary-synchronous-dp">Summary: Synchronous DP</h2>

<p>Note that all the above is synchronous DP, because at each step $k+1$ we are using the value function strictly from step $k$.</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923094516742.png" alt="image-20220923094516742" style="zoom:40%;" /></p>

<ul>
  <li><strong>major dropback:</strong> even for value iteration, it needs to <strong>iterate through every state at each iteration</strong> (which could be intractable for large state space)</li>
  <li>Asynchronous DP: maybe we can selectively update, or update more in real time</li>
</ul>

<h2 id="asynchronous-dp">Asynchronous DP</h2>

<p>Before, we compute the new value for all states at $t=k+1$ strictly using $t=k$ values, meaning for each improvement we need to wait until all states are updated.</p>

<blockquote>
  <p><strong>Aim</strong>: We want to see improvement a bit faster, instead of waiting for the entire sweep of states to complete. We want our algorithm to not get locked into any hopelessly long sweep before it can make progress.</p>
  <ul>
    <li>but note that this does not necessarily mean less compute</li>
  </ul>
</blockquote>

<p>Three simple ideas in Asynchronous DP:</p>
<ul>
  <li>in-place DP</li>
  <li>Prioritized Sweeping</li>
  <li>Real-time Dynamic DP</li>
</ul>

<h3 id="in-place-dp">In-place DP</h3>

<blockquote>
  <p><strong>Intuition</strong>: since we already have a new value for some states we finished computing at $t=k+1$. we use the <strong>new value directly</strong> before completing the entire estimate over all states</p>
</blockquote>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220923094905916.png" alt="image-20220923094905916" style="zoom:40%;" /></p>

<p>note that already here, we made an improvement <strong>for memory</strong>. But what about state space complexity?</p>

<h3 id="prioritized-sweeping">Prioritized Sweeping</h3>

<blockquote>
  <p><strong>Intuition</strong>: instead of updating each step in a fixed order, we can consider a better order to update the states so that we can perhaps converge faster. For instance, we can use the size of the Bellman error as a guidance.</p>
</blockquote>

<p>For states with large Bellman error, we might want to update them first</p>

\[\text{Bellman Error}(s) = \left|  \max_{a\in A} \left( \mathcal{R}_s^{a} + \gamma \sum\limits_{s'\in S} \mathcal{P}_{ss'}^{a} v(s') \right) - v(s)\right|\]

<p>which is basically comparing the difference between a target the current estimate. But this means we need:</p>
<ul>
  <li>first compute this error for every state, then choose which one has the largest error to update</li>
  <li>next, we need to <strong>update the error table</strong>, which can be done <mark>only on the affected states $s'$</mark> (the error is only a functoin of current state and next state)</li>
  <li>no guaranteed convergence to the optimal policy</li>
</ul>

<h3 id="real-time-dp">Real-Time DP</h3>

<blockquote>
  <p><strong>Intuition</strong>: I only update the value function $V(s)$ on states that the agent <strong>has seen</strong>, i.e. is relevant for the agent</p>
</blockquote>

<ol>
  <li>agent has a current policy $\pi$</li>
  <li>agent performs some action and observe $S_t, A_t, R_{t+1}$, and is at $S_{t+1}$</li>
  <li>
    <p>backup the value function $V(S_t)$ using the new observation</p>

\[v(S_t) \gets \max_{a\in A} \left( \mathcal{R}_{\textcolor{red}{S_t}}^{a} + \gamma \sum\limits_{s'\in S} \mathcal{P}_{\textcolor{red}{S_t}s'}^{a} v(\textcolor{red}{s'}) \right)\]

    <p>notice that this is off-policy since the value now is not about the behavior policy</p>
  </li>
  <li>update the policy $\pi$ using the new value function and repeat</li>
</ol>

<p>Note that, again, there is <strong>no guaranteed convergence to the optimal policy</strong></p>

<h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2>

<p>Policy iteration consists of two simultaneous, interacting processes:</p>

<ol>
  <li>one making the value function consistent with the current policy (<strong>policy evaluation</strong>)</li>
  <li>making the policy greedy with respect to the current value function (<strong>policy improvement</strong>).</li>
</ol>

<p>Up to now we have seen:</p>

<ul>
  <li>In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary (e.g. value iteration).</li>
  <li>In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement.</li>
  <li>In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other.</li>
</ul>

<blockquote>
  <p><strong>Genearalized Policy Iteration</strong>: As long as both processes (policy evaluation and policy improvement) continue to update all states, the ultimate result is typically the same—convergence to the optimal value function and an optimal policy.</p>
</blockquote>

<p>Intuitively, the evaluation and improvement processes in GPI can be viewed as both competing and cooperating.</p>

<ul>
  <li>They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically <strong>makes the value function incorrect</strong> for the changed policy, and making the value function consistent with the policy typically causes that <strong>policy no longer to be greedy</strong> (i.e. has a better policy given this new value function)</li>
  <li>In the long run, however, these two processes interact to <strong>find a single joint solution</strong>: the optimal value function and an optimal policy</li>
</ul>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007144344783.png" alt="image-20221007144344783" style="zoom: 33%;" /></p>

<blockquote>
  <p>Therefore, why GPI holds, i.e. usually converge to optimal value/policy, can be intuitively explained as:</p>

  <ul>
    <li>The value function <strong>stabilizes only when it is consistent with the current policy</strong>,</li>
    <li>The policy <strong>stabilizes only when it is greedy with respect to the current value function</strong>.</li>
  </ul>

  <p>Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the <mark>Bellman optimality equation holds</mark>, and thus that the policy and the value function are optimal.</p>
</blockquote>

<p>This overall idea of GPI is used in <mark>almost any RL algorithms</mark></p>

<h1 id="model-free-rl">Model-free RL</h1>

<p>Recall that Model-based RL requires knowing the transition function and the reward function. More importantly in a <strong>Model-Based problem</strong>:</p>

<ul>
  <li>we <em>could</em> get a closed form solution if the state space is small in a model-based MDP (using the Bellman’s equation)</li>
  <li>iterative algorithms, that uses <strong>Bellman expectation</strong> equation, are used for estimating policy value</li>
  <li>control algorithm (i.e. value iteration), that uses <strong>Bellman optimality</strong> equation, are used to find optimal value/policy</li>
</ul>

<p>But more often in reality we <mark>don't know the model</mark>. So how do we solve the control problem?</p>

<blockquote>
  <p><strong>Aim</strong>: we can use sampling to <mark>estimate</mark> the missing transition/reward models. The key question is then <mark>how do we take samples</mark> to make our model more efficient in estimating the value functions?</p>

  <ul>
    <li>MC Methods</li>
    <li>TD Methods</li>
  </ul>
</blockquote>

<h2 id="monte-carlo-learning">Monte Carlo Learning</h2>

<blockquote>
  <p><strong>MC Sampling</strong>: learns from <strong>complete</strong> episodes (hence finite episodes)</p>

  <ul>
    <li>
      <p>hence can be only used with episodic MDP</p>
    </li>
    <li>
      <p>the main idea is to consider value function of a state = mean return from it</p>

\[V(s) = \mathbb{E}[G(s)] \approx \frac{1}{n}\sum_{i=1}^n G(s)\]
    </li>
  </ul>

</blockquote>

<p>Specifically, we consider the goal of learning $V_\pi$ from episodes of a policy $\pi$</p>

\[(S_1, A_1, R_1, ..., S_k) \sim \pi\]

<p>is <strong>one episode from $\pi$</strong>. We ca sample many episodes, and for each episode:</p>

<ol>
  <li>
    <p>compute the total discounted reward for the future for <em>each state</em>:</p>

\[G_t^i = R_{t+1}^i + \gamma R_{t+2}^i + ... + \gamma ^{T-1} R_T^i\]

    <p>for $i$-th episode</p>
  </li>
  <li>
    <p>estimate the value function using the law of large numbers:</p>

\[v_\pi(s) = \mathbb{E}_\pi[G_t | S_t=s] = \lim_{n \to \infty}  \frac{1}{n}\sum_{i=1}^n G^i_t(s)\]

    <p>for $G_t^i(s)$ means the discounted reward starting from state $s_t$ at the $i$-th episode</p>
  </li>
</ol>

<blockquote>
  <p>While this is the basic idea, there are many <strong>modifications</strong> of this:</p>

  <ul>
    <li>every visit v.s. first visit MC estimate to have different convergence speed - bias tradeoff</li>
    <li>collecting all episodes and computing together is <em>computationally expensive</em>, hence there are iterative version of this</li>
  </ul>
</blockquote>

<h3 id="firstevery-visit-mc-policy-evaluation">First/Every Visit MC Policy Evaluation</h3>

<p>Consider the following two epsiodes we have sampled following policy $\pi$:</p>

\[(S_1, S_2, S_1, S_2, S_4, S_5) \sim \pi \\
(S_1, S_2, S_2, S_4, S_5, S_3) \sim \pi\]

<p>with some reward for each state, but ignored here. Suppose I want to estimate $V(s_1)$ from the two samples</p>

<blockquote>
  <p><strong>First Visit MC Policy Evaluation</strong>: to evaluate $V_\pi(s)$ for a state $s$</p>
  <ol>
    <li><em>only</em> for the first time step $t$ that the state is visited in an episode</li>
    <li>count that as a sample for state $s$ and increment counter $N(s) \gets N(s) + 1$</li>
    <li>increment total return for that state $S(s) \gets S(s) + G_t$</li>
  </ol>

  <p>Then the value estimate is:</p>

\[v_\pi(s) = \frac{S(s)}{N(s)}\]

  <p>and by law of large numbers, $N(s) \to \infty$ we have $v_\pi(s) \to V_\pi(s)$. Hence this is a <mark>consistent</mark> estimator.</p>
</blockquote>

<p>Therefore, using first visit MC policy evaluation in the previous example, we would have:</p>

\[v_\pi(s_1) = \frac{G^{1}_1(s_1) + G^{2}_1(s_1)}{2}\]

<p>where $G^{i}_t(s)$ is the t-th timestep in the i-th episode.</p>

<blockquote>
  <p><strong>Every Visit MC Policy Evaluation</strong>: to evaluate $V_\pi(s)$ for a state $s$</p>
  <ol>
    <li>for the <em>every</em> time step $t$ that the state is visited in an episode</li>
    <li>count that as a sample for state $s$ and increment counter $N(s) \gets N(s) + 1$</li>
    <li>increment total return for that state $S(s) \gets S(s) + G_t$</li>
  </ol>

  <p>Then the value estimate is:</p>

\[V(s) = \frac{S(s)}{N(s)}\]

  <p>and this again, by law of large numbers, this is a <mark>consistent</mark> estimator.</p>
</blockquote>

<p>Using every visit MC policy evaluation in the previous example, we would have:</p>

\[V(s_2) = \frac{G^{1}_2(s_2) + G^1_4(s_2) + G^{1}_2(s_2) + G^{1}_2(s_2)}{4}\]

<p>Essentially we treat every sample of even the same episode separately</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Property</th>
      <th style="text-align: center">First Visit</th>
      <th style="text-align: center">Every Visit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Consistent</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td style="text-align: center">Biased</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td style="text-align: center">Convergence Speed</td>
      <td style="text-align: center">Slow</td>
      <td style="text-align: center">Fast</td>
    </tr>
  </tbody>
</table>

<p>where:</p>
<ul>
  <li><em>usually</em> every-visit has a faster convergence speed (unless in cases when state is rare in both cases, could have similar speed)</li>
  <li>recall that for law of large number, we required independence of samples. However, samples from every visit are <mark>correlated</mark>. Therefore, this could be <mark>biased</mark> even though it could converge due to large number of samples.</li>
</ul>

<blockquote>
  <p><em>Recall</em> that Bias and different from Consistent:</p>
  <ul>
    <li>Consistent estimator: as the sample size increases, the estimates (produced by the estimator) “converge” to the true value</li>
    <li>Unbiased estimator: if I feed in different sample sets, the average of the estimates should be the true value</li>
  </ul>
</blockquote>

<p>Last but not least, implementation wise since we are computing a mean, we could use <strong>recursive mean computations</strong>:</p>

\[\begin{align*}
  \mu_k
  &amp;= \frac{1}{k} \sum_{i=1}^k x_i \\
  &amp;= \frac{1}{k} \left( \sum_{i=1}^{k-1} x_i + x_k \right) \\
  &amp;= \frac{1}{k} \left( (k-1) \mu_{k-1} + x_k \right)\\
  &amp;= u_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})
  \end{align*}\]

<p>in our case, $\mu$ would be the mean hence estimation of value $V_\pi$. This gives use the usually programmatic <strong>recursive MC updates</strong>, where we perform updates on the fly when doing sampling:</p>

\[\begin{align*}
  N(S_t) &amp;\gets N(S_t) + 1\\
  V(S_t) &amp;\gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))
\end{align*}\]

<p>whenever an update is needed. This gives a brief outline of the pseudo-code for both algorithm:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">First-Visit</th>
      <th style="text-align: center">Every-Visit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930175001.png" style="zoom:100%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930175019.png" style="zoom:100%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="mc-for-non-stationary-state">MC for Non-Stationary State</h3>

<p>However, if you have a non-stationary problem (e.g. mean of a distribution is time-dependent), then we want to have a dynamic estimator and we don’t want convergence.</p>

<p>Hence we just want to use <mark>new information directly</mark>, without doing the averaging:</p>

\[V(s_t) \gets V(s_t) + \alpha (G_t - V(s_t))\]

<p>which you can just convert to the equation of moving average</p>

\[V(s_t) \gets (1-\alpha) V(s_t) + \alpha G_t\]

<p>for a <strong>constant step size $\alpha$</strong>. We can intuitively see the difference as the previous form of $V(S_t) \gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$ since now this is a constant step size:</p>

\[\begin{align*}
V_{t+1} 
&amp;= V(s_t) + \alpha (G_t - V(s_t)) \\
&amp;= \alpha G_t + (1-\alpha) V_t \\
&amp;= \alpha G_t + (1-\alpha) [ \alpha G_{t-1} + (1-\alpha)V_{t-1}]\\
&amp;= \alpha G_t + (1-\alpha) \alpha G_{t-1} + (1-\alpha)^2 V_{t-1}\\
&amp;= ...
\end{align*}\]

<p>so eventually you reach $G_1$, hence we get:</p>

\[v_{t+1}(s) = (1-\alpha)^t v_1(s) + \sum_{i=1}^t \alpha (1-\alpha)^{t-i}G_i\]

<p>meaning that for large $t$:</p>

<ul>
  <li>the first term will become extremely small, meaning that the <strong>old information would matter very slightly</strong> (whereas in $V(S_t) \gets V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$ every sample has equal contribution)</li>
  <li>usually we want to put <strong>more weight on new information</strong>, as this is non-stationary.</li>
  <li>but we usually use the other formula $V(s_t) \gets V(s_t) + \alpha (G_t - V(s_t))$ for update as that is more computationally efficient</li>
</ul>

<p>But in principle, all the MC backups essentially does:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930085105526.png" alt="image-20220930085105526" style="zoom:33%;" /></p>

<p>which is like a <strong>DFS</strong> when sampling. This is later constrasted with TD approaches.</p>

<h2 id="monte-carlo-control">Monte Carlo Control</h2>

<p>First, we revisit our previous approach of generalized policy iteration</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930085400935.png" alt="image-20220930085400935" style="zoom: 50%;" /></p>

<p>where we used greedy policy for improvement and is proven to work.</p>

<p>Therefore, a simple idea of model-free policy improvements</p>

<ul>
  <li>use MC policy evaluation</li>
  <li>use greedy policy improvement?</li>
</ul>

<p>We realize that using greedy-policy here have two concerns:</p>
<ol>
  <li>
    <p>if we estimateed $v_\pi$, then since greedy needs:</p>

\[\pi'(s) = \arg\max_{a \in A} Q(s,a)\]

    <p>but <em>if we only have $V_\pi$</em>, then we need to compute:</p>

\[\pi'(s) = \arg\max_{a \in A} \mathcal{R}_s^{a}+ \mathcal{P}_{ss'}^{a}V'_\pi\]

    <p>but we don’t know $\mathcal{P}$</p>
  </li>
  <li>
    <p>could get me stuck in some suboptimal action due to exploitation</p>
  </li>
</ol>

<blockquote>
  <p>To solve the first problem, it is easy: we can instead use <mark>MC methods to directly estimate $Q(s,a)$</mark></p>
</blockquote>

<p>But then second problem is a little more complicated:</p>

<blockquote>
  <p>We can use <strong>$\epsilon$-greedy policy</strong> instead of greedy policy to allow for <mark>exploration</mark> since we are estimating the world (hence do no have complete information as in the Model-based case)</p>

\[\pi(a|s) = \begin{cases}
  (\epsilon / m) + 1 - \epsilon &amp; \text{if } a = \arg\max_{a \in A} Q(s,a) \\
  \epsilon / m &amp; \text{otherwise}
\end{cases}\]

  <p>for $m = \vert A\vert$. But does this <strong>guarantee the improvement of the policy</strong>? This is proven to be true!</p>
</blockquote>

<p><em>Proof</em>: for all states, any Ɛ-greedy policy 𝜋, the Ɛ-greedy policy 𝜋’ with respect to is an improvement, that is,</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930090408152.png" alt="image-20220930090408152" style="zoom:60%;" /></p>

<p>where a key step is shown above. There we wanted to show that $q_\pi(s,\pi’(s))$ is better than the old $v_\pi(s)$</p>

<ul>
  <li>the second equality comes from the fact that best action has an additional $(1-\epsilon)$ probability</li>
  <li>the third equality comes from the fact that $\sum_x \alpha(x)f(x) \le \max_x f(x)$ for $\sum \alpha(x) = 1$. In other words, weighted average is always less than the maximum value.</li>
  <li>the fourth equality comes from expanding the second term and cancelling the first term, with only $\sum \pi(a\vert s) q_\pi(s,a)$ left</li>
</ul>

<p>Then finally the last step to show $v_{\pi’}(s) \ge v_\pi(s)$ which we ignored basically comes from expanding the definition of value function, and substituting in our result above.</p>

<p>Therefore, the final MC policy iteration becomes</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930091516025.png" alt="image-20220930091516025" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>Note</strong> that different from the model-based case, we cannot perform value-iteration here to simplify the computation. This is because we don’t have the transition model $\mathcal{P}$ as required in the Bellman optimality equation:</p>

\[Q_*(s,a) = \mathcal{R}_s^a + \sum_{s'} \mathcal{P}_{ss'}^a \max_{a'} Q_*(s',a')\]

</blockquote>

<h3 id="greedy-in-the-limit-with-infinite-exploration">Greedy in the Limit with Infinite Exploration</h3>

<p>Recall that we know, for every MDP problem, there exists an optimal deterministic policy by taking the greedy policy of an optimal Q-function. But if we are doing $\epsilon$-greedy, then we are not guaranteed to converge to the optimal policy.</p>

<blockquote>
  <p><strong>GLIE</strong>: A learning policy is called <strong>GLIE</strong> (Greedy in the Limit with Infinite Exploration) if it satisfies the following two properties:</p>
  <ol>
    <li>If a state is visited infinitely often, then <strong>each action in that state is chosen infinitely often</strong> (with probability 1):</li>
  </ol>

\[\lim_{k \to \infty} N_k(s,a) = \infty\]

  <ol>
    <li>In the limit (as $t \to \infty$), the learning policy is <strong>greedy with respect to the learned Q-function</strong> (with probability 1):</li>
  </ol>

\[\lim_{k \to \infty} \pi_k(a|s) = 1(a = \arg\max_{a' \in A} Q_k(s,a'))\]

</blockquote>

<p>Therefore, taking into account our policy improvement with $\epsilon$-greedy, we can <mark>still achieve optimal policy by satisfying the GLIE condition</mark> with:</p>

\[\epsilon_{k}  = \frac{1}{k}\]

<p>for the $k$ th iteration we have updated the policy.</p>

<p>Therefore, we get essentially a change for policy improvement step:</p>

\[\text{Policy Improvement:}\quad \epsilon \gets \frac{1}{k}, \quad \pi'(s) \gets \epsilon\text{-greedy}(Q)\]

<h2 id="off-policy-evaluation">Off-Policy Evaluation</h2>

<p>So far, all we have learned is on-policy, i.e. the <strong>behavior policy</strong> used to collect samples is the same as the policy you are estimating $V_\pi$</p>

<blockquote>
  <p><strong>Off-policy evaluation</strong>: means the <strong>behavior policy $\mu$</strong> you used to collect data is <strong>not</strong> the same policy you want to estimate $V_\pi$. So you want to learn policy $\pi$ from $\mu$.</p>
  <ul>
    <li>for here, we can suppose both $\mu$ and $\pi$ policy are known</li>
    <li>essentially you may want to evaluate your $V_\pi$ or $Q(S_t,A_t)$ but only <strong>using data from $\mu$</strong>.</li>
  </ul>
</blockquote>

<p>How do we do that? The basic methods here is importance sampling.</p>

<h3 id="importance-sampling">Importance Sampling</h3>

<blockquote>
  <p><strong>Importance Sampling</strong>: goal is to estimate the expected value of a function $f(x)$ (i.e. $G(s)$) under some probability distribution $p(x)$, without sampling from the distribution $p(x)$ but <strong>using $f(x)$ sampled from $q(x)$</strong>:</p>

\[\mathbb{E}_{x \sim p(x)}[f(x)] =?\]

  <p>with data sampled from $x \sim q(x)$. It turns out we can easily show that, under few assumptions:</p>

\[\mathbb{E}_{x \sim p(x)}[f(x)] = \mathbb{E}_{x \sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]\]

</blockquote>

<p>Notice that the <mark>stationary distribution of states</mark> will be different, i.e. we will be considering</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20220930093651232.png" alt="image-20220930093651232" style="zoom:33%;" /></p>

<p>So, if we have samples generated from $x \sim q$, and since in our case we consider $f(X) = G_t(s)$, then basically need to consider, for each trajectory $\tau_j$ and its discounted reward $G^{j}$:</p>

\[Q_{\tau \sim \pi}(s,a) = \mathbb{E}_{\tau \sim \mu(x)}\left[\frac{P_\pi(\tau)}{P_\mu(\tau)}G^{j}\right]\]

<p>But what is this ratio of probability?</p>

\[P_\pi(\tau) = P_\pi(S_t, A_t, ..., S_T, A_T)\\
P_\mu(\tau) = P_\mu(S_t, A_t, ..., S_T, A_T)\]

<p>we can essentially use chain rule to represent this as quantities we know:</p>

\[\begin{align*}
P_\pi(\tau) 
&amp;= P_\pi(S_t, A_t, ..., S_T, A_T)\\
&amp;= \pi(A_T | S_t, S_{T-1},A_{T-1}, ..., S_t,A_t) P_\pi(S_t, A_t, S_{t+1},A_{t+1},...S_T, A_T)\\
&amp;= \pi(A_T | S_T) P_\pi(S_T | S_{T-1},A_{T-1},...S_t, A_t)P_\pi(S_{T-1},A_{T-1},...S_t, A_t)\\
&amp;= \underbrace{\pi(A_T | S_T)}_{\mathrm{policy} } \underbrace{P_\pi(S_T | S_{T-1}, A_{t-1})}_{\text{transition model} } P_\pi(S_t, A_t, S_{t+1},A_{t+1},...S_{T-1},A_{T-1})\\
&amp;= ...\\
&amp;= p(S_{1}) \prod_{i=1}^{T}\underbrace{\pi(A_{i}|S_{i})}_{\mathrm{policy}} \underbrace{p(R_{i}|S_{i},A_{i})}_{\text{reward model}} \underbrace{p(S_{i+1}|S_{i},A_{i})}_{\text{transition model}}
\end{align*}\]

<p>where</p>

<ul>
  <li>the third and fourth equality comes from the fact that we are MDP, hence future <strong>only depends on current</strong></li>
  <li>
    <p>finally, the transition probability and reward probability will be cancelled when we do the ratio:</p>

\[\frac{P_\pi(\tau)}{P_\mu(\tau)} = \frac{p(S_{1}) \prod_{i=1}^{T}\pi(A_{i}|S_{i}) p(R_{i}|S_{i},A_{i}) p(S_{i+1}|S_{i},A_{i})}{p(S_{1}) \prod_{i=1}^{T}\mu(A_{i}|S_{i}) p(R_{i}|S_{i},A_{i}) p(S_{i+1}|S_{i},A_{i})} = \prod_{i=1}^{T}\frac{\pi(A_{i}|S_{i})}{\mu(A_{i}|S_{i})} \equiv w_{\mathrm{IS}}\]

    <p>is called the <mark>importance sampling weight</mark> for each trajectory</p>
  </li>
  <li>can estimate <em>any other policy</em>, but given that we have the same coverage. unseen action in policy $\mu$, it has prob zero</li>
</ul>

<blockquote>
  <p><strong>Ordinary IS</strong>: the above essentially gives the ordinary IS algorithm:</p>

\[\begin{align*}
  V_{\pi}(s)
  &amp; \approx \frac{1}{n} \sum\limits_{j=1}^{n} \frac{p_\pi(T_j|s)}{p_\mu(T_j|s)} G(T_j)\\
  &amp;= \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{T} \frac{\pi(a_{j,i}|s_{j,i})}{\mu(a_{j,i}|s_{j,i})} \right) G(T_j)\\
  &amp;= \frac{1}{n} \sum\limits_{j=1}^{n} w_{\mathrm{IS}} \cdot  G(T_j)
\end{align*}\]

  <p>for practically, we are just <strong>swapping out $G(T_j)$ with $w_{\mathrm{IS}} \cdot  G(T_j)$ in our previous on-policy algorithms</strong>.</p>
</blockquote>

<p>Now, what are the assumptions used to make IS work? You might notice the term $\frac{\pi(a_{j,i}\vert s_{j,i})}{\mu(a_{j,i}\vert s_{j,i})}$ could have gone badly, and it is exactly the case</p>

<blockquote>
  <p><strong>Importance Sampling Assumptions</strong>: since we are reweighing samples from $\mu$, if we have distributions that are non-overlapping, then this will obviously not work.</p>

  <ul>
    <li>in particular, if we have any single case that $\mu(a\vert s)=0$ but $\pi(a\vert s)&gt;0$, then this will not work.</li>
    <li>therefore, for this to work, we want to have a large <mark>coverage</mark>: so that for $\forall a,s$ such that $\pi(a\vert s)&gt;0$, you want $\mu(a\vert s)&gt;0$.</li>
  </ul>
</blockquote>

<p>Intuitively, this means that if $\pi$ is not too far off from $\mu$, then the importance sampling would work reasonably.</p>

<p>In practice, there are still some problems with this, and hence some variants include:</p>

<ul>
  <li><strong>Discounting-aware Importance Sampling</strong>: suppose you have $\gamma =0$ and you have for long episodes. Then techinically since $\gamma=0$, your return is determined since $S_0,A_0,R_0$ and multiplying the weight by $w_{\mathrm{IS}}$ contributes significantly to <mark>variance</mark></li>
  <li><strong>Per-decision Importance Sampling</strong>: which applies a weight to each single reward $R_T$ within the $G$.</li>
</ul>

<h2 id="td-learning">TD Learning</h2>

<p>In practice, the MC is used much less because:</p>

<ul>
  <li>we need to <strong>wait</strong> until the agent reaches the <strong>terminal state</strong> to compute $G_t$, which is our target for update equation
    <ul>
      <li>e.g. if we play go game, it needs to wait until the end of the game</li>
    </ul>
  </li>
  <li>hence also requires finite episode settings</li>
  <li><strong>large variance</strong>, because the <strong>episode can be long</strong></li>
</ul>

<blockquote>
  <p><strong>Aim</strong>: TD learning tries to resolve that issue by “estimating the future” instead of waiting and using the true future. So the questions become: is there some useful information we can do without this wait?</p>

  <ul>
    <li>
      <p>learns from incomptele episodes, hence bootstrapping</p>
    </li>
    <li>
      <p>but because you are updating from your estimate based on your estimate, <strong>there will be bias</strong> (MC has no bias)</p>
    </li>
    <li>
      <p>For any fixed policy $\pi$, TD(0) has been proved to converge to $v_\pi$, in the mean for a constant step-size parameter if it is sufficiently small, and with probability $1$ if the step-size prameter decreases according to the usual stochastic approximation condition:</p>

\[\sum_{n=1}^\infty \alpha_n(t) = \infty,\quad \text{and}\quad \sum_{n=1}^\infty \alpha^2_n(t) &lt; \infty\]

      <p>The first condition is required to guarantee that the steps are large enough to eventually <mark>overcome any initial conditions or random fluctuations</mark>. The second condition guarantees that <strong>eventually the steps become small enough to assure convergence</strong>.</p>
    </li>
  </ul>
</blockquote>

<p>Recall that MC updates in general looks like</p>

\[V(S_t) \gets V(S_t) + \alpha (\underbrace{G_t}_{\text{MC target}} - V(S_t))\]

<p>which is the actual return, but in TD(0) we can update using an <strong>estimated return</strong></p>

\[V(S_t) \gets V(S_t) + \alpha (\underbrace{(R_{t+1} + \gamma V(S_{t+1})}_{\text{TD(0) target}}) - V(S_t))so that :\]

<ul>
  <li>
    <p>essentially I am utilizing the fact that:</p>

\[G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ...= R_{t+1} + \gamma V(S_{t+1})\]

    <p>hence since we can approximate $V(S_{t+1}) \approx v(s_{t+1})$, we get our TD(0) update</p>
  </li>
  <li>intuitively, this TD(0) means that I wanted to make update on $S_t$ and I do this by:
    <ul>
      <li>taking an action $A_t$ and get to $S_{t+1}$</li>
      <li>then using my trajectory $S_{t}, R_{t+1}, S_{t+1}$ to update by estimating the expected return I would get in the future</li>
    </ul>
  </li>
  <li>so this means that like MC, I will still be estimating with acting and sampling experience from real environment</li>
</ul>

<p>But since TD$(0)$ is only moving/looking one step forward at $V(S_{t+1})$, you can easily also look many steps:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007111034003.png" alt="image-20221007111034003" style="zoom: 25%;" /></p>

<blockquote>
  <p><strong>General TD$(n)$</strong> updates: since we can also have</p>

\[G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ...= R_{t+1} + \gamma R_{t+2}+\gamma^2 V(S_{t+2})\]

  <p>this becomes a TD(1) since now we are looking even one more step ahead, hence suing</p>

\[V(S_t) \gets V(S_t) + \alpha (\underbrace{(R_{t+1}+\gamma R_{t+2} + \gamma^2 V(S_{t+1})}_{\text{TD(1) target}}) - V(S_t))\]

  <p>and this can go to TD$(n)$ easily.</p>
</blockquote>

<p>Visually, we can compare the three algorithms discussed so far:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MC Update</th>
      <th style="text-align: center">TD Update</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007082930340.png" alt="image-20221007082930340" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007082958257.png" alt="image-20221007082958257" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>And just to be complete, for DP method we know all transition probabilities, henc we can take into account all possibilities</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007083124404.png" alt="image-20221007083124404" style="zoom:33%;" /></p>

<p>In this sense,  TD(0) can be seen as <strong>taking samples over this DP backup</strong>, which also “bootstraps”</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Needs Model</th>
      <th style="text-align: center">Sampling</th>
      <th style="text-align: center">Bootstrapping</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">DP</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td style="text-align: center">MC</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">No</td>
    </tr>
    <tr>
      <td style="text-align: center">TD</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
  </tbody>
</table>

<p>Finally, algorithmically for TD(n):</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007111217374.png" alt="image-20221007111217374" style="zoom: 33%;" /></p>

<h2 id="mc-vs-td-methods">MC v.s. TD Methods</h2>

<p>Here we compare some features of using MC v.s. TD to perform policy evaluation:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">MC</th>
      <th style="text-align: center">TD(0)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Learn before/without knowing final outcome</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td>Target for update</td>
      <td style="text-align: center">$G_t$ which is a real return</td>
      <td style="text-align: center">estimate $R_{t+1} + \gamma v(S_{t+1})$ can be biased</td>
    </tr>
    <tr>
      <td>Variance</td>
      <td style="text-align: center">High, because single branch + long episode means uncertainy builds up quickly</td>
      <td style="text-align: center">Low, because now depend on one random action, transition, and reward</td>
    </tr>
    <tr>
      <td>Bias</td>
      <td style="text-align: center">no bias</td>
      <td style="text-align: center">not proven, has shown bias upperbound</td>
    </tr>
    <tr>
      <td>Sentitive to Initial Value</td>
      <td style="text-align: center">no</td>
      <td style="text-align: center">yes, can affect convergence speed</td>
    </tr>
    <tr>
      <td>Convergence Properties</td>
      <td style="text-align: center">Converges</td>
      <td style="text-align: center">Converges</td>
    </tr>
    <tr>
      <td>Convergence Speed</td>
      <td style="text-align: center">Slow</td>
      <td style="text-align: center">Fast</td>
    </tr>
    <tr>
      <td>used with Function Approximation</td>
      <td style="text-align: center">Little problem</td>
      <td style="text-align: center">often has problem converging</td>
    </tr>
    <tr>
      <td>Storing Tabular Data</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
  </tbody>
</table>

<p>The still remaining concern but both still requires some kind of table to store value for each state, and/or action. For example, we update $Q$ estimate by storing the values of $(s,a)$ in a <mark>tabular representation</mark>: <mark>finite</mark> number of state-action pair.</p>

<p>However, as you can imagine many real world problems have <strong>enormous state and/or action space</strong> so that we cannot really tabulate all possible values. So we need to somehow <mark>generalize</mark> to those unknown state-actions.</p>

<blockquote>
  <p><strong>Aim</strong>: even if we encounter state-action pairs <em>not</em> met before, we want to make <em>good decisions</em> by past experience.</p>
</blockquote>

<blockquote>
  <p><strong>Value Function Approximation</strong>: represent a (state-action/state) value function <strong>with a parametrized function</strong> instead of a table, so that even if we met an inexperienced state/state-action, we can get some values. (which will be discussed in the next section)</p>
</blockquote>

<hr />

<p><em>For Example</em>: using TD for policy Evaluation</p>

<p>Consider the following MDP, with reward $0$ except for one state:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007084804460.png" alt="image-20221007084804460" style="zoom: 50%;" /></p>

<p>So that essentially there are:</p>

<ul>
  <li>two terminate states, so your optimal policy is to just move right.</li>
  <li>And suppose my current policy $\pi$ is random left or right at equal probability, we want to evaluate this policy $\pi$ .</li>
</ul>

<p>Let us use TD(0) learning and have initial values $V(S)=0.5$ for non-terminal states and $V(S_T)=0$ for those temrinal states.</p>

<ul>
  <li>we take $\alpha=0.1$, $\gamma=1$</li>
  <li>then we update the estimate <strong>per time-step</strong> following TD(0) algorithm</li>
</ul>

<p>With which we arrive at:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007134742879.png" alt="image-20221007134742879" style="zoom: 50%;" /></p>

<p>so after 100 episodes seen, we see it is going to converge to the true value.</p>

<p>Interestingly, from here we can ask: what is the <strong>episode encountered</strong> (see the line annotated by number 1) for the so that TD(0) only end up updated $V(A)\gets 0.45$?</p>

<ul>
  <li>
    <p>realize that if you calculate anything from $V(S)\to S’ \neq S_T$ you will get no change in value, still $0.5$. For instance:</p>

\[V_A \gets V_A + \gamma (R + V_B - V_A) = 0.5 + 1 \cdot (0) = 0.5\]
  </li>
  <li>
    <p>but things change only if you ended up at $V_T$ on the left, so that you get</p>

\[V_A \gets V_A + \gamma (R + V_T - V_A) = 0.5 + 0.1 \cdot (0 + 0 -0.5) = 0.45\]
  </li>
</ul>

<p>Therefore the episode just needs to hit left end state.</p>

<p>In this example, we can also compare the MC and TD variance</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007090134530.png" alt="image-20221007090134530" style="zoom: 50%;" /></p>

<p>and note that MC achieves least MSE, while TD is performing MLE.</p>

<h2 id="td-control">TD Control</h2>

<p>In the end all people care is to get the optimal policy, rarely just to evaluate. Hence we need to use the evaluation/learning and do control</p>

<blockquote>
  <p><strong>Intuitively</strong>: we can just apply TD to the $Q(S,A)$, and use $\epsilon$-greedy policy improvement as we have proven to work with MC methods</p>

  <ul>
    <li>as long as we have some form of alternating policy evaluation and policy improvement, following <a href="#Generalized Policy Iteration">Generalized Policy Iteration</a> mos algorithms typically converge to the optimal policy</li>
  </ul>
</blockquote>

<h3 id="sarsa">SARSA</h3>

<blockquote>
  <p><strong>SARSA</strong>: Essentially TD(0) learning for one step followed immediately by $\epsilon$-greedy policy improvement.</p>

  <ul>
    <li>
      <p>intuitively, it follows the <a href="#Generalized Policy Iteration">Generalized Policy Iteration</a> style</p>
    </li>
    <li>
      <p>Sarsa converges with probability 1 to an optimal policy and action-value function, under the usual conditions on the step sizes $\sum \alpha^2 &lt; \infty$, as long as all state–action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (GLIE, which can be arranged, for example, with $\epsilon$-greedy policies by setting $\epsilon = 1/t$).</p>
    </li>
  </ul>
</blockquote>

<p>Given a single move/step $(S,A,R,S’,A’)$ we can perform an update using TD(0) equation:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007091739133.png" alt="image-20221007091739133" style="zoom: 33%;" /></p>

<p>Then, instead of waiting it to cnverge we can directly perform update, hence the algorithm becomes</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007091933118.png" alt="image-20221007091933118" style="zoom:50%;" /></p>

<p>where notice that essentially I improve my policy by picking better $A$ using $\epsilon$-greedy, and then ask my model to update accordingly.</p>

<ul>
  <li>
    <p>this is <strong>on-policy control</strong> (i.e. behavior policy $\pi_b$ is the same as evaluation policy $\pi_e$)</p>
  </li>
  <li>on-policy as you already chosen your action while doing policy improvement</li>
  <li>and recall that for convergence to the optimal greedy policy, you will <strong>need $\epsilon$ to decrease (i.e. the GLIE)</strong></li>
</ul>

<h3 id="q-learning">Q-Learning</h3>

<blockquote>
  <p><strong>Q-Learning</strong>: Most important and widely used today. It is motivated by having the <strong>learned</strong> action-value function, $Q$, from TD updates to <strong>directly approximates $Q_*$</strong>, the optimal action-value function, independent of the policy being followed.</p>

  <ul>
    <li>dramatically simplifies the analysis of the algorithm and enabled early convergence proofs (is proven to converge)</li>
    <li>All that is required for correct convergence is that all pairs continue to be updated. Under this assumption and a variant of the usual stochastic approximation conditions (i.e. $\sum \alpha^2 &lt; \infty$) on the sequence of step-size parameters, <strong>$Q$ has been shown to converge with probability 1 to $Q_*$.</strong></li>
  </ul>
</blockquote>

<p>Now, the algorithm is that you are not choosing $A’$ from the current policy $\pi$, but update using greedy policy $\mathrm{greedy}(\pi)$</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007092305741.png" alt="image-20221007092305741" style="zoom: 50%;" /></p>

<p>hence this is off-policy</p>

<ul>
  <li>your update is to estimate a greedy policy (i.e. optimal policy). This could have you, for example picking leftest action</li>
  <li>but when I sample, I use $\epsilon$-greedy policy, meaning that I might have ended up in a differnt state. e.g. by picking the rightest action</li>
  <li>but it is proven that it still converges to the optimal policy quickly</li>
</ul>

<p>Algorithm:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007145704583.png" alt="image-20221007145704583" style="zoom:33%;" /></p>

<p>where here I improve my policy by picking better $A$ using $\epsilon$-greedy, but then ask my model to update the greedy policy.</p>

<ul>
  <li>
    <p>This is widely used as there are early theoretical proofs to show it always converge to the optimal policy. But note that</p>
  </li>
  <li>but SARSA + greedy is still different than Q-Learning</li>
  <li>so under the usual stochastic approxmation conditions with step size, and using GLIE, i.e. $\epsilon$ decreasing, <strong>both SARSA and Q-Learning will converge to the optimal value/policy</strong>. So what is the different between using them?</li>
</ul>

<hr />

<p><em>For Example: Cliff Walking</em></p>

<p>Consider the following cliff walking problem, where we want to use the SARSA and Q-learning to find the <mark>optimal policy given fixed $\epsilon=0.1$.</mark> Then eventually you will see the following performance</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007093725043.png" alt="image-20221007093725043" style="zoom:50%;" /></p>

<p>however, here we notice that Q-learning is doing worse. This does <strong>not</strong> contradict our previous conclusions because:</p>

<ul>
  <li>Q-learning aims to directly learn the optimal policy <strong>without taking into account the behavioral policy</strong> (i.e. $\epsilon=0.1$). Hence it learns the optimal path but occasionally falls off the cliff due to $\epsilon=0.1$</li>
  <li>
    <p>SARSA aims to learn the policy <strong>taking into account the behavioral policy</strong> and hence picks the safer path</p>
  </li>
  <li>but at the end of day, if we decrease $\epsilon \to 0$, then both converge to the same optimal path</li>
</ul>

<p>But why would you want to have $\epsilon$-greedy policy used as behavioral policy when deployed?</p>

<ul>
  <li>in cases such as automonomous driving, simulation environment might be too optimistic and you may want your model to be more <strong>robust to random errors</strong>, as shown in the cliff case</li>
  <li>so in that case SARSA could be prefered, which might not give you the optimal $Q$ values hence more robust</li>
</ul>

<h2 id="dp-vs-td-methods">DP v.s. TD Methods</h2>

<p>We can compare the backup diagrams and realize that our update forms corresponds a lot to <strong>DP=Bellman Equations while TD=sampling Bellman Equations</strong></p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007094456708.png" alt="image-20221007094456708" style="zoom: 50%;" /></p>

<p>This becomes much clearer if we compare the update equation used:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221007094708566.png" alt="image-20221007094708566" style="zoom:50%;" /></p>

<p>where $x \xleftarrow{\alpha} y$ means the update $x \gets x+ \alpha (y-x)$</p>

<ul>
  <li>so that Q-learning is <strong>sampling Bellman Optimality from DP</strong></li>
  <li>SARSA is like <strong>sampling from Bellman Expectation from DP</strong></li>
</ul>

<h2 id="tdlambda-and-eligibility-traces">TD($\lambda$) and Eligibility Traces</h2>

<p>We want to expand the basic TD(0) idea which can perform per step update (no wait for $G_t$) and MC which involves deep updates. Specifically, you will see how those relate to the <strong>idea of TD($\lambda$)</strong>, and eventually see a <strong>unified view of RL algorithmic solutions</strong>.</p>

<p>At the end of this section, you should</p>

<ul>
  <li>
    <p>understand what TD$(\lambda)$ and eligibility traces are</p>
  </li>
  <li>
    <p>have a picture of how existing algorithms are came up/organized</p>
  </li>
</ul>

<p>Then we can go into implementations with Deep NN in the next section.</p>

<h3 id="tdlambda">TD($\lambda$)</h3>

<p>Recall that TD(0) considers the update of <strong>one-step look ahead</strong></p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221015013800029.png" alt="image-20221015013800029" style="zoom:33%;" /></p>

<p>which basically aims to approximate the target $G_t = R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ …= R_{t+1} + \gamma V(S_{t+1})$. We mentioned that we can easily extend this idea to <strong>$n$-step look ahead</strong>, which can be even generalized to the MC which gets to the terminate state:</p>

\[\begin{align*}
G_t^{(n)} 
&amp;=R_{t+1} + \gamma R_{t+2}+ \gamma ^2 R_{t+3}+ ... \\
&amp;= R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \underbrace{\gamma^nV(S_{t+n})}_{\text{n-th step}}
\end{align*}\]

<p>Hence visually</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Visual</th>
      <th style="text-align: center">TD(n) to MC Update Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014081856031.png" alt="image-20221014081856031" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014081936837.png" alt="image-20221014081936837" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Intuition</strong>: We can perhaps combine both MC and TD, and use <strong>all the information in the sample</strong>. So that instead of a single $n$-step return, we can <strong>average/combine all the $n$-step return</strong>.</p>
</blockquote>

<p>For instance, consider you have a 2 and 4 step return computed, we can consider</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Data We Have</th>
      <th style="text-align: center">Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014082300419.png" alt="image-20221014082300419" style="zoom: 33%;" /></td>
      <td style="text-align: center">$\frac{1}{2} G^2 + \frac{1}{2} G^4 = \frac{1}{2}\sum G^{(n)}$</td>
    </tr>
  </tbody>
</table>

<p>So the question is what is a good way to combine those information?</p>

<blockquote>
  <p><strong>$\lambda$-Return:</strong> compute $G_t^\lambda$ which combines all $n$-step return $G_t^{n}$ <mark>using weight $(1-\lambda )\lambda^{n-1}$</mark>, which decays each future by $\lambda$ while summing up to $1$. This means that given all returns:</p>

\[G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}, \quad G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{N}V(S_{t+N})\]

  <p>and hence your update equation becomes</p>

\[V(S_t) \gets V(S_t) + \alpha (G_t^\lambda - V(S_t))\]

  <p>however, since this means you are adding both long episodes (MC) and short ones (TD), you will get</p>

  <ul>
    <li><mark>both high bias and big variance</mark> (from MC and TD)</li>
    <li>you need a <mark>delay as it waits until terminal state</mark> (based on definitino $n=T$ here is the terminal state)</li>
    <li>using <mark>bootstrapping</mark> because we calculate $G_t^{(n)}$ using TD method, hence <mark>sensitive to initialization</mark></li>
  </ul>

  <p>so even in TD(0) we can get rid of a lot of those problems. Meaning that TD($\lambda$) is not really used in practice. But this did provided the theoretical foundation for the eligibility trace, which is very useful.</p>
</blockquote>

<p>Visually, the weights look like</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Using All Information</th>
      <th style="text-align: center">Weights on Early $G_t^{(n)}$ decays = less important</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014082434105.png" alt="image-20221014082434105" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014083138482.png" alt="image-20221014083138482" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><mark>Note</mark> that in the forward-view, the <strong>weights need to sum up to $1$</strong>. This means in case if you have a finite horizon, as shown in the figure above you need to <strong>adjust your last weight</strong> so that your weights sum up to $1$.</p>

\[G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t}^{(n)}+\underbrace{\lambda ^{T-t-1}G_t}_{\text{last return}}\]

  <ul>
    <li>for instance, if you have three $G_t^{(n)}$, and $\lambda = 0.5$, then the weights will be $0.5, 0.5^2, 1-(0.5+0.5^2)$</li>
  </ul>
</blockquote>

<p>So how do we <strong>improve this theoretical idea</strong>. Consider viewing TD($\lambda$) as a <mark>forward view</mark> as we are looking into the future</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014083247509.png" alt="image-20221014083247509" style="zoom: 50%;" /></p>

<blockquote>
  <p>This forward view provides theory, the <mark>backward view</mark> provides mechanism/practicality</p>
</blockquote>

<p>I would like to fix the following problems in this algorithm:</p>

<ul>
  <li>get a sample and can update like TD(0) per time step (instead of waiting)</li>
  <li>combines information like TD($\lambda$) but hopefully avoid those bias and variance</li>
</ul>

<h3 id="eligibility-trace">Eligibility Trace</h3>

<blockquote>
  <p><strong>Heuristics</strong>: what are the useful features of the forward view TD($\lambda$) update?</p>

  <ul>
    <li><mark>frequency heuristics</mark>: most visited states should be more important (=need to have some kind of <em>memory</em> to know that it was visited in the past)
      <ul>
        <li>achieved by constantly adding the history and yet decaying like TD($\lambda$). i.e. add past eligibility but scaled: $\lambda E_{t-1}$</li>
      </ul>
    </li>
    <li><mark>recent heuristic</mark>: most recent states should be also important
      <ul>
        <li>add a weight of $1$ (the indicator function)</li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>So the idea technically comes form TD($\lambda$), which actually does</p>

<ul>
  <li>put more weight on the most recent state it has the highest weight</li>
  <li>more weight on most visited states as they will be added more than once in the $\sum$</li>
</ul>

<blockquote>
  <p><strong>Eligibility Trace</strong>: essentially how do we <mark>design a weighting</mark> that achieves the above but also allow us to do per-step update?</p>

  <p>Initialize the weight $E_0(s)=0$ for each state, and then consider</p>

\[E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)\]

  <p>for $\mathbb{1}$ is the indicator function, and it records the <strong>“importance” for each state</strong> when the value function is updated (see later). But why this form? It turns out that this specific form allows for an <mark>(offline) equivalence</mark> of eligibility trace to TD($\lambda$) forward view.</p>
</blockquote>

<p>For example, consider you are rolling out a single episode, and a state that is being visited 4 times in a row, not seen for a while, then seen twice, then seen once. Then, the eligibility trace of it looks like:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014084104772.png" alt="image-20221014084104772" style="zoom:50%;" /></p>

<p>therefore, it satisfied what I needed:</p>

<ul>
  <li>if a state is visited more <strong>often</strong>, it gets a higher eligibility trace (like in the beginning)</li>
  <li>if a state is visited more <strong>recently</strong>, it automatically have a weight of $+1$</li>
</ul>

<blockquote>
  <p><strong>Backward View</strong> of TD($\lambda$): now we can use eligibility trace for value update.</p>

  <p>Consider the TD error $\delta_t$ we used to update when doing TD(0):</p>

\[\delta_t = \underbrace{R_{t+1} + \gamma V(S_{t+1})}_{\text{TD(0) target}} - V(S_t)\]

  <p>which enabled us to do per time-step update. Then, the idea is we <mark>weigh</mark> the per-time step error, such that <mark>if we accumulate the updates until the end of the episode</mark>, it will result in the <mark>same update as the forward TD($\lambda$)</mark>. This is also called the <strong>offline equivalence</strong> (see next section), but with this constraint we end up with the weight called <mark>eligibility trace $E_t(s)$</mark> which we showed before:</p>

\[E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)\]

  <p>and hence the update rule:</p>

\[V(s)\gets V(s) + \alpha E_t(s) \delta_t\]

  <p>so that this is given to each state, e.g. if we have 10 states, we get 10 eligibility traces. Intuitively, since $E_t(s)$ depended on history, it measures how much this state matters <strong>by looking in the past</strong>: to see if it is seen frequently, recently.</p>
</blockquote>

<p>Visually, the backward view means we are using this loss $\delta_t$ using information in the past:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014084613356.png" alt="image-20221014084613356" style="zoom:33%;" /></p>

<p>additionally</p>

<ul>
  <li>
    <p>if $\lambda = 0$, then $E_t(s) =\mathbb{1}(S_t=s)$  and then eligibility trace becomes TD(0) update since we are then doing:</p>

\[V(s)\gets V(s) + \alpha \delta_t(s)\cdot \mathbb{1}(S_t=s)\]

    <p>meaning we only update the ones we just saw</p>
  </li>
</ul>

<p>So this backward update now has the advantage of</p>

<ul>
  <li>
    <p>including in the <em>idea</em> of forward TD($\lambda$) to use <strong>all the information we have</strong></p>
  </li>
  <li>
    <p>this is implementable as we don’t need to look n-step beyond (but behind). Meaning we can perform <strong>update per time step</strong></p>
  </li>
</ul>

<p>Finally, we can show the algorithm of using Eligibility trace:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014085742951.png" alt="image-20221014085742951" style="zoom: 67%;" /></p>

<p>where basically the error is TD(0) error, but we are <strong>updating all states using eligibility trace per time step</strong></p>

<ul>
  <li>this means we need to have two tables, one for eligibility per state and anther store the value per state</li>
  <li>since now it is online (update per step), this backward is <strong>not</strong> equivalent to the forward view. But if we pick a step size sufficiently small, it is almost equivalent</li>
  <li>practically, this <strong>online TD($\lambda$) updates</strong> is what we use and works</li>
</ul>

<blockquote>
  <p><mark>Note</mark> that</p>

  <ul>
    <li>
      <p>we are updating <strong>all states per time step</strong>, so even if you have just seen a state $S_t=B$, and $S={A,B}$, you still need to update <strong>both $S=A,S=B$</strong> in the loop when updating $V(s)$. A hint is to notice that $\delta$ is not a function of state in the above algorithm.</p>
    </li>
    <li>
      <p>when used with value function approximation, the <strong>eligibility trace generalizes</strong> to</p>

\[E_t(s) = \gamma \lambda E_{t-1}(s) + \nabla \hat{V}(S,w)\]

      <p>and algorithm looks like</p>

      <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221205221054594.png" alt="image-20221205221054594" style="zoom: 25%;" /></p>
    </li>
  </ul>
</blockquote>

<h3 id="offline-equivalent-of-forward-and-backward-views">Offline Equivalent of Forward and Backward Views</h3>

<p>Finally, here we answer the question why did people decide to use specifically the weight in the form of $E_t(s)$</p>

\[E_t(s) = \gamma \lambda E_{t-1}(s) + \mathbb{1}(S_t=s)\]

<blockquote>
  <p><strong>Offline Equivalence</strong></p>

  <p>Consider having your eligibility trace being updated/accumulated all the time, but <mark>you only update $V$ at the end</mark>. Recall that for the forward view, we also only update at the end. Now, you realize that</p>

  <p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014085128664.png" alt="image-20221014085128664" style="zoom:33%;" /></p>

  <p>so if we were offline only learning the eligibility traces, your final updated $V$ will be the same as the forward view</p>

  <ul>
    <li>so in offline mode you are essentially <strong>summing up all the errors of each state over all time</strong> but without updating $V$</li>
    <li>so then the proof is to show that the this accumulated error is the same in forward view! But this proof is long so skipped</li>
  </ul>
</blockquote>

<h2 id="tdlambda-control">TD($\lambda$) Control</h2>

<p>In the previous section we are essentially <strong>learning the value function</strong>, now we care about using it to <strong>find the optimal policy</strong>. The basic idea is again simple: we can apply this learning style to learn an optimal $Q_*(s,a)$.</p>

<p>Recall that to derive the backward eligibility trace we need to</p>

<ol>
  <li>derive the <strong>forward</strong> algorithm by considering the $n$ step look-ahead version of the TD error</li>
  <li>use the eligibility trace in <strong>backward</strong> algorithm</li>
  <li>show their <strong>offline equivalence</strong></li>
</ol>

<h3 id="sarsalambda">SARSA($\lambda$)</h3>

<p>First, we consider the forward TD($\lambda$). Consider the <mark>$n$-step look ahead for SARSA's target</mark></p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221015155935901.png" alt="image-20221015155935901" style="zoom:33%;" /></p>

<p>So that the $n$-step look ahead return is</p>

\[q_t^{n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1} R_{t+n} + \gamma^n Q(S_{t+n})\]

<p>Hence the forward view considers weighting those returns:</p>

\[q_t^{\lambda} = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} q_t^{(n)}\]

<p>So the forward view of SARSA becomes:</p>

\[Q(S_t, A_t) \gets Q(S_t,A_t) + \alpha (\underbrace{q_t^{\lambda}}_{\text{TD($\lambda$) target}} - Q(S_t,A_t))\]

<p>and notice that using $\lambda=0$ we get back SARSA (which uses TD(0) target)</p>

<p>Then from it, we can <strong>find the backward view</strong> by building this <mark>eligibility trace</mark> knowing that</p>

\[\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\]

<p>so then intuitively we can say that the eligibility trace is, with $E_0(s,a)=0$:</p>

\[E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \mathbb{1}(S_t=s, A_t=a)\]

<p>so that our update rule is:</p>

\[Q(s,a) \gets Q(s,a) + \alpha E_t(s,a) \delta_t\]

<p>essentialy:</p>

<ul>
  <li>target is stil SARSA target, but error is weighted by eligibility trace</li>
  <li>so again two tables, one for Q and one for eligibility trace</li>
  <li>and we skip the proof that this gives the same result as the forward view of SARSA if offline</li>
</ul>

<p>Therefore, we get this SARSA($\lambda$) algorithm</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014091605986.png" alt="image-20221014091605986" style="zoom:50%;" /></p>

<p>We can visualize the differences:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014091753630.png" alt="image-20221014091753630" style="zoom: 50%;" /></p>

<p>so that</p>
<ul>
  <li>if SARSA, then we only updated using only one “previous” step</li>
  <li>if using 10-step SARSA, my last step will take into account my previous 10 steps equally</li>
  <li>if using SARSA($\lambda$) with $\lambda=0.9$, then we update using all history weighted by eligibilty trace</li>
</ul>

<h3 id="qlambda-learning">Q($\lambda$)-Learning</h3>

<p>There are many ways to do this, but you need to have some proof that it works. So here we use Watkins’s version. First recall that</p>

<ul>
  <li>Q-learning aims to do Bellman optimality directly each step</li>
  <li>it is off-policy, and like SARSA, the update is per step (no need to wait)</li>
  <li>both Q-learning and SARSA is proven to give you the optimal policy (under mild conditions)</li>
</ul>

<p>Now, consider Q($\lambda$).  We first consider <mark>$n$-step look ahead of the target</mark> (different from SARSA’s target)</p>

\[\begin{cases}
\text{1-step}, &amp; R_{t+1} + \gamma \max_a Q(S_{t+1}, a) \\
\vdots \\
\text{$n$-step}, &amp; R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1} R_{t+n} + \gamma^n \max_a Q(S_{t+n}, a)
\end{cases}\]

<p>But then, you have a problem that since Q-learning is off-policy, what does the forward view look like? In Watkin’s $Q(\lambda)$, we considers two terminate cases:</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014092301366.png" alt="image-20221014092301366" style="zoom:50%;" /></p>

<p>so that your last look-ahead either is</p>
<ul>
  <li>the actual terminate state</li>
  <li>your behavior policy picked an action that is non-greedy</li>
</ul>

<p>So that your backward view becomes</p>

\[\delta_{t} = R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\]

<p>with eligibility trace</p>

\[E_t(s,a) = \begin{cases}
1 + \gamma \lambda E_{t-1}(s,a) &amp; \text{if }s=s_t, \text{ and } a\sim \epsilon\text{-greedy}(Q(s,a)) = \arg\max_{a} Q(s,a) \\
0, &amp; \text{if }a \neq \arg\max_{a} Q(s,a)\\
\gamma \lambda E_{t-1}(s,a) &amp; \text{otherwise, i.e. not at } s_t
\end{cases}\]

<p>with the upadte rule being</p>

\[Q(s,a) \gets Q(s,a) + \alpha E_t(s,a) \delta_t\]

<p>Hence the algorithm looks like</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014092949925.png" alt="image-20221014092949925" style="zoom:50%;" /></p>

<p>so basically</p>
<ul>
  <li>if action I took w.r.t. to my current $\epsilon\text{-greedy}(Q)$ is the greedy action, then we do the normal eligibility trace update</li>
  <li>if not, then reset all to zero, because we reached a “dummy terminal state” which we defined by next move being not greedy action (by this definition, we can still have equivalence of backward and forward view)</li>
  <li>so again, my error is the Belmman Optimality Error using the $A^{*}$ from the greedy policy, but my behavior policy is to do $A’$ which is $\epsilon$-greedy. Hence this is off-policy as is Q-learning.</li>
</ul>

<blockquote>
  <p>Recall that all algorithms we see here <strong>needs a tabular value</strong>. Hence for large state space/state-action space, those algorithms are slow due to this loop for all state/state-action.</p>

  <p>This problem is solved by using <strong>function approximation</strong>: the only parameters are the parmeters of the functions which can spit out those values, which we will see in the next section.</p>
</blockquote>

<h2 id="unified-view-of-rl-solutions">Unified View of RL Solutions</h2>

<p>All the basics of RL algorithms can be put in one picture, and we can understand their differences/similarities</p>

<p><img src="/lectures/images/2022-12-15-ELEN6885_Reinforcement_Learning/image-20221014094057829.png" alt="image-20221014094057829" style="zoom:50%;" /></p>

<p>where</p>

<ul>
  <li>shallow means update only looks at 1 step, and deep is like MC (<strong>depth</strong>). This depth can be changed depending on n-step return and hence the $\lambda$ parameter weighting <strong>how many future</strong> you want to use.</li>
  <li>full backup means you look at all possible neighbors (<strong>width</strong>)</li>
  <li>in practice, all algorithms such as Q-learning and SARSA are based on sample backups, as the updates are faster and more efficient</li>
  <li>MCTS with truncation and approximations would be similar to the exhaustive search on top right</li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Reinforcement Learning]]></summary></entry><entry><title type="html">COMS4995 Deep Learning part1</title><link href="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/" rel="alternate" type="text/html" title="COMS4995 Deep Learning part1" /><published>2022-12-08T00:00:00+00:00</published><updated>2022-12-08T00:00:00+00:00</updated><id>/lectures/2022@columbia/COMS4995_Deep_Learning_part1</id><content type="html" xml:base="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/"><![CDATA[<h1 id="logistics-and-introduction">Logistics and Introduction</h1>

<p>Office hours</p>

<ul>
  <li>Lecturer, Iddo Drori (idrori@cs.columbia.edu), Tuesday 2:30pm, Zoom (Links to an external site.)</li>
  <li>CA, Anusha Misra, Wednesday 3:30-4:30pm, Zoom (Links to an external site.)</li>
  <li>CA, Vaibhav Goyal, Friday 3-4pm, Zoom (Links to an external site.)</li>
  <li>CA, Chaewon Park (cp3227@columbia.edu), Thursday 3:30-4:30PM, Zoom (Links to an external site.)</li>
  <li>CA, Vibhas Naik (vn2302@columbia.edu), Monday 11AM-12PM, Zoom</li>
</ul>

<p>Grades:</p>

<ul>
  <li>9 Exercises (30%, 3% each, <strong>individual</strong>, quizzes on Canvas)
    <ul>
      <li>quizzes will timed in some of the live lectures. So attend lectures!</li>
    </ul>
  </li>
  <li>Competition (30%, in pairs)</li>
  <li>Projects (40%, in teams of 3 students)</li>
</ul>

<p><strong>Projects Timeline</strong></p>

<ul>
  <li><em>Feb 18</em>: Form Teams and Signup</li>
  <li><em>Feb 25</em>: Select project</li>
  <li><em>Mar 10-11</em>: Project Kick-off and proposal meetings</li>
  <li><em>Mar 31 - Apr 1</em>: Milestone Meetings</li>
  <li><em>Apr 21-11</em>: Final Project Meetings</li>
  <li><em>Apr 28</em>: Project poster session</li>
</ul>

<h2 id="human-brain-deep-learning">Human Brain Deep Learning</h2>

<p>In comparison of sizes of number of neurons:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133414667.png" alt="image-20220118133414667" style="zoom: 67%;" /></p>

<p>where the left hand size are the deep learning models and the right hand size the human brain</p>

<ul>
  <li>however, it is also said that humans are “generalized”, where machines are “specialized”</li>
</ul>

<p>What happens</p>

<p>| <img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133924987.png" alt="image-20220118133924987" /> | <img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133935376.png" alt="image-20220118133935376" /> |
| ———————————————————— | ———————————————————— |</p>

<ul>
  <li>Type 2 process, your pupil will dilate (slow)</li>
</ul>

<p>Then, in AlphaGo, as well as other models, essentially it is doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134044859.png" alt="image-20220118134044859" style="zoom:50%;" /></p>

<p>where the</p>

<ul>
  <li>neural networks DNN are doing Type 1 processes</li>
  <li>tree search doing Type 2. (e.g. Monte Carlo Tree Search)</li>
</ul>

<h2 id="intro-transformers">Intro: Transformers</h2>

<p><a href="https://arxiv.org/abs/1706.03762">The paper</a> ‘Attention Is All You Need’ describes transformers and what is called a <strong>sequence-to-sequence architecture</strong>. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the  sequence of words in a sentence, into another sequence. (Well, this  might not surprise you considering the name.)</p>

<p>Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A brief sketch of how it works would be:</p>

<ol>
  <li>Input (e.g. in English) passes through an encoder</li>
  <li>Encoder takes the input sequence and maps it into a higher dimensional space (imagine translating it to some imaginary language $A$)</li>
  <li>Decoder takes in the sequence in the imaginary language $A$ and turns it into an output sequence (e.g. French)</li>
</ol>

<p>Initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.</p>

<ul>
  <li>A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.</li>
</ul>

<p>More details:</p>

<ul>
  <li>
    <p>https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</p>
  </li>
  <li>
    <p>https://towardsdatascience.com/transformers-141e32e69591</p>
  </li>
</ul>

<h3 id="example-application">Example Application</h3>

<p>Consider the task of trying to use AI to solve math problems (e.g. written in English)</p>

<p>It turns out that using language models, it doesn’t work if you want to do those math questions. However, if you turn math questions into programs, then it worked!</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134738566.png" alt="image-20220118134738566" style="zoom: 33%;" /></p>

<p>Then, some example output of this using Codex would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134913140.png" alt="image-20220118134913140" /></p>

<p>notice that questions will need to be able to <strong>rephrased/transformed</strong> so that it is clearly a <strong>programming task</strong>, before putting into learning models.</p>

<ul>
  <li>this also means a full automation would be difficult</li>
</ul>

<h2 id="dl-timeline">DL Timeline</h2>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118135630742.png" alt="image-20220118135630742" /></p>

<h2 id="supervised-deep-learning-example">Supervised Deep Learning Example</h2>

<p>Consider the following data, and we want to use a linear model to separate the data</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118135900287.png" alt="image-20220118135900287" style="zoom:50%;" /></p>

<p>notice that by default, a linear transformer does not work. Hence we need to consider $x_1x_2$ as a feature</p>

<ul>
  <li>the idea is that we may want to consider <mark>feature extraction/processing</mark> before putting them into the network</li>
</ul>

<p>However, what if we are given the following data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118140203431.png" alt="image-20220118140203431" /></p>

<p>where there doesn’t seem to be a clear/easy solution if we stick with a linear classifier even with some single layer feature transformation. As a result, in this case you will have to use a neural network:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118140229608.png" alt="image-20220118140229608" /></p>

<p>the upshot is that we really need to consider <strong>extracting features and use linear classifiers</strong> before using deep neural network, which is necessary only in some cases like the one above.</p>

<h2 id="representations-sharing-weights">Representations Sharing Weights</h2>

<p>Some nowadays popular NN architectures are:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118145207752.png" alt="image-20220118145207752" /></p>

<p>where notice that one common feature that made them successful is to <strong>share weights $W$ between layers/neurons</strong>:</p>

<ul>
  <li>
    <p>CNN share $W$ through space</p>
  </li>
  <li>
    <p>RNN share $W$ through time</p>
  </li>
  <li>
    <p>GNN share $W$ across neighborhoods</p>
  </li>
</ul>

<h1 id="forward-and-back-propagation">Forward and Back Propagation</h1>

<p>The basics of all the NN is a Neuron/Perceptron</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120131919835.png" alt="image-20220120131919835" style="zoom:50%;" /></p>

<p>where:</p>

<ol>
  <li>input is a single vector</li>
  <li>the $\Sigma$ represents we are summing the components of $\vec{x}$ ($w_0$ is a scalar representing bias)</li>
  <li>then the scalar is passed into an activation function $f$, often non-linear</li>
</ol>

<p>Now, remember that our aim is to <mark>minimize loss</mark> with a given training label:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132007260.png" alt="image-20220120132007260" style="zoom:50%;" /></p>

<p>where $\mathcal{L}$ is a loss function of our choice:</p>

<ul>
  <li>this is summed over all the training $m$ samples</li>
  <li>$w_0$ will represent the bias, often/later absorbed into $W$</li>
  <li>our objective is to minimize this $J$</li>
</ul>

<blockquote>
  <p><em>Note</em></p>

  <p>Remember that for any Learning task</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120203244526.png" alt="image-20220120203244526" style="zoom:80%;" /></p>

  <p>Before the step of feature extraction and selection, you may want to pay careful attention on how to do <strong>feature extraction/selection</strong> to improve your model.</p>
</blockquote>

<h2 id="neural-networks">Neural Networks</h2>

<p>The simplest component is a Neuron/Perceptron, whose mathematical model was basically finding:</p>

\[g(\vec{x}) = w^T\vec{x}+w_0 = 0\]

<p>and doing the following for classification:</p>

\[f(x):= 
\begin{cases}
+1 &amp; \text{if } g(x) \ge 0\\
-1 &amp; \text{if } g(x) &lt; 0
\end{cases} \quad = \text{sign}(\vec{w}^T\vec{x}+w_0)\]

<p>However, since the output is $\text{sign}$ function which is not differentiable, in Neural Network we will use $\sigma$ sigmoid instead. This also means that, instead of using Perceptron Algorithm to learn, we can use <strong>backpropagation</strong> (basically a taking derivatives using chain rule backwards).</p>

<p>That said, a <mark>Neural Network</mark> basically involves connecting a number of <strong>neurons</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205520527.png" alt="image-20220120205520527" style="zoom: 33%;" /></p>

<p>where since at $\Sigma$ we are just doing $\vec{w}^T \vec{x}$, which is a linear combination, we used the $\Sigma$ symbol.</p>

<p>Then, combining those neurons in a <strong>fully connected network</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132436067.png" alt="image-20220120132436067" style="zoom:67%;" /></p>

<p>Conceptually, since each neuron/layer $l$ basically does two things, from the book:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204213314.png" alt="image-20220120204213314" /></p>

<p>Hence each layer can be expressed as:</p>

\[z^l = W^l \vec{a}^{l-1},\quad \vec{a}^l = f(z^l)\]

<p>where we assumed a single data point input (i.e. a vector instead of a matrix), but note that:</p>

<ul>
  <li>
    <p>$Z^l$ is the linear transformation, $a^l$ is the $\sigma$ applied to each element if activation is $\sigma$.</p>

    <p>Therefore, since we are just doing a bunch of linear transformation (stretchy) and passing to a sigmoid (squash):</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Start</th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204442993.png" alt="image-20220120204442993" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204456452.png" alt="image-20220120204456452" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204624982.png" alt="image-20220120204624982" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205207727.png" alt="image-20220120205207727" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205247722.png" alt="image-20220120205247722" /></td>
        </tr>
      </tbody>
    </table>

    <p>where:</p>

    <ul>
      <li>at the start, the red will be labeled as $+1$, and blue $-1$ in a space perpendicular to the graph</li>
      <li>the last one looks separable.</li>
    </ul>
  </li>
  <li>
    <p>This can be easily gerealized when you have $n$ data points, so that:</p>

\[Z^l = (W^{l})^T A^{l-1},\quad A^l = f(Z^l)\]

    <p>basically you have now <strong>matrices</strong> as input and output, weights are still the same. To be more precise, a layer then looks like</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120210731448.png" alt="image-20220120210731448" style="zoom:50%;" /></p>

    <p>so that you are doing $z^l = (W^{l})^T a^{l-1}+W_0$, if again, we have a single data point input of $m$ dimension. Yet often we would have absorbed the $W_0$ by lifting, so that we have:</p>

\[\vec{a} = \begin{bmatrix}
a_1\\
\vdots\\
a_m\\
1
\end{bmatrix} \in \mathbb{R}^{m+1}, \quad W^T \leftarrow [W^T, W_0] \in \mathbb{R}^{n \times (m+1)}\]
  </li>
</ul>

<p>In general, if there are $n_l$ neurons in the $l$-th layer, then at the $l$-th layer:</p>

\[A^{l-1} = \text{input} \in \mathbb{R}^{(n_{l-1}+1)) \times m},\quad  (W^l)^T \in \mathbb{R}^{n_l \times (n_{l-1} + 1)}\]

<p>where $n_0 = m$ is the dimension of the data, and we absorbed in the bias.</p>

<ul>
  <li>a quick check is to make sure that $Z^l = (W^l)^T A^{l-1}$ works</li>
  <li>a visualization is that data points are now aligned <strong>vertically</strong> in the matrix</li>
</ul>

<h3 id="neuron-layer">Neuron Layer</h3>

<p>To make it easier to digest <strong>mathematically</strong>, we can think of <mark>each layer as a single operation</mark>. Graphically, we convert:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132436067.png" alt="image-20220120132436067" style="zoom: 50%;" /></p>

<p>To this, simply three “neurons” (excluding the input):</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120212259363.png" alt="image-20220120212259363" style="zoom: 33%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>$F(x) = f(W^Tx)$ is a shorthand for notating the entire linear and nonlinear operation. This would be <em>nonlinear</em>.</p>
  </li>
  <li>
    <p>therefore, each layer $l$ basically just does:</p>

\[F^l(A^{l-1}) = A^l\]

    <p>outputting $A^l$. Hence we can do a “Markov chain” view of the NN as just doing:</p>

\[F(x) = F^3 (F^2 (F^1(x)))\]
  </li>
  <li>
    <p>if $f$ are identities, then the output is just a linear transformation since $F$ would be <em>linear</em>.</p>
  </li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the following NN. For brevity, I only cover the first layer:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120212538132.png" alt="image-20220120212538132" /></p>

<p>First we do the pre-activation $z^1$ for the input of a single data point of dimension $3$:</p>

\[z^1 = (W^1)^T a^{0} = \begin{bmatrix}
w_{11} &amp; w_{21} &amp; w_{31}\\
w_{12} &amp; w_{22} &amp; w_{32} \\
\end{bmatrix}\begin{bmatrix}
x_{1}\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
z_{1}^1\\
z_2^1\\
\end{bmatrix}\]

<p>where bias would be ignored for now (otherwise there will be one more column of $[b_1, b_2^T$ for $(W^1)^T$ and a row of $1$ for $a^0$. Then the activation does:</p>

\[a^1 =  \begin{bmatrix}
f(z_{1}^1)\\
f(z_{2}^1)
\end{bmatrix}= \begin{bmatrix}
a_1^1\\
a_2^1
\end{bmatrix}\]

<p>which will be input of the second layer. Eventually:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120134137821.png" alt="image-20220120134137821" style="zoom: 50%;" /></p>

<p>the last part does not have an activation since we are doing a regression instead of classification.</p>

<h3 id="activation-functions">Activation Functions</h3>

<p>The activation function always does a mapping from $f: \mathbb{R} \to \mathbb{R}$. Hence they are applied element-wise.</p>

<p>Common examples of activation functions include:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120213419561.png" alt="image-20220120213419561" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>a difference between Swish and ReLU is that Switch has a defined derivative at $z=0$</p>
  </li>
  <li>
    <p>In the case of ReLU, consider an input space of dimension $2$. Now fold it along an axis, returning two separately smooth planes intersecting at a “fold”. At this point, one of them is flat, and the other is angled at $45$ degrees. Since the input dimension is $2$, we will be folding again at the y-axis.</p>

    <ul>
      <li>By repeating this to create many ‘folds’, we can use the ReLU function to generate an “n-fold hyperplane” from the smooth 2D input plane.</li>
    </ul>

    <p>Therefore, combined with linear transformation, the folds will be slanted:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120220614045.png" alt="image-20220120220614045" style="zoom:67%;" /></p>

    <p>in the end we just have a sharded space from many folds.</p>
  </li>
  <li>
    <p>Swish: notice that $\lim_{\beta \to \infty}$ Swish becomes ReLU</p>
  </li>
</ul>

<p>Their derivatives are important to know since we will use them when taking derivatives during backpropagation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132903520.png" alt="image-20220120132903520" style="zoom:50%;" /></p>

<p>where notice that</p>

<ul>
  <li>Sigmoid: $\sigma’$ depends on $\sigma$ means we would have <mark>already computed this</mark> in the forward pass. This will save computational effort!</li>
  <li>ReLU: the simplest derivative among all. Saves computational effort.</li>
</ul>

<p>Last but not least, for multiclass classification, often the <strong>last layer</strong> uses SoftMax, which maps $\mathbb{R}^d \to \mathbb{R}^d$. This is often used <strong>only for the last layer if we are doing multiclass</strong>:</p>

\[g^L(z^L)_i = \frac{e^{z_i^L}}{\sum_{j=1}^d e^{z_{j}^L}}\]

<p>where:</p>

<ul>
  <li>
    <p>$z_i^L$ basically is the pre-activatoin on the last layer $l$</p>
  </li>
  <li>
    <p>this is a generalizatoin of logistic regression because $\sum_i^d g(z)_i = 1$, i.e. elements sum up to 1.</p>
  </li>
  <li>
    <p>also pretty easy to implemnet:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>yet since they add up to 1, they can also be seen as a generalization of logistic regressoin.</p>

<h3 id="loss-functions">Loss Functions</h3>

<p>Now we have several choice of $f$, our final objective of minimize is:</p>

\[\frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)\]

<p>for $\hat{y}^i$ is basically the $F^3 (F^2 (F^1(\vec{x}^i)))$, for example. Therefore, a couple of different choices for $\mathcal{L}$ function:</p>

<ul>
  <li>
    <p>Mean Squared Error</p>

\[\mathcal{L}(y^i, \hat{y}^i) = (y^i - \hat{y}^i)^2\]
  </li>
  <li>
    <p>Other power $p$ error:</p>

\[\mathcal{L}(y^i, \hat{y}^i) = |y^i - \hat{y}^i|^p\]

    <p>Graphically:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120214540633.png" alt="image-20220120214540633" style="zoom: 80%;" /></p>
  </li>
  <li>
    <p>Logistic Regression Loss (Cross-Entropy Loss)</p>

\[\mathcal{L}(y^i, \hat{y}^i) = -y^i \log(\hat{y}^i) - (1-y^i) \log (1-\hat{y}^i)\]

    <p>If this loss is used, the objective $J$ is convex in $W$.</p>
  </li>
  <li>
    <p>There is also a Softmax version/multiclass version of the logistic loss. Checkout the book for more info.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>In general, the loss function is <strong>not</strong> convex with respect to $W$, therefore solving:</p>

\[\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)=\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W))\]

  <p>does not guarantee a global minimum. We therefore <strong>use gradient descent</strong> to find a <strong>local minimum</strong>.</p>
</blockquote>

<h3 id="regularization">Regularization</h3>

<p>What happens if we add regularization, such that we consider:</p>

\[\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W)) + R(W)\]

<p>In general, this will more or less force values inside $W$ to be small.</p>

<p>Now, recall that $W$ basically does the linear part/composes the pre-activation before putting into $f$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120215157181.png" alt="image-20220120215157181" style="zoom:50%;" /></p>

<p>So we if have $f$ being a function such as sigmoid, it means that pre-activation would be most at the blue part:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120215433180.png" alt="image-20220120215433180" style="zoom: 80%;" /></p>

<p>where notice that:</p>

<ul>
  <li>adding regularization will likely shrink the $W$, which means that $Z^l$ would be small. Hence, $A^l=f^l(Z^l)$  would likely be <mark>linear</mark> (the green part)!</li>
  <li>This means our activation being less complex, i.e. $f$ becomes almost a <strong>linear</strong> operation. Intuitively, the more we regularize, the less complicated our model</li>
</ul>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120134756312.png" alt="image-20220120134756312" style="zoom: 80%;" /></p>

<p>where if we use the previous example:</p>

<ul>
  <li>
    <p>e.g. $L=3$ since we have 3 layers</p>
  </li>
  <li>
    <p>$T$ is the number of iterations we want to perform (e.g. number of epochs if we are doing over the entire batch)</p>
  </li>
  <li>
    <p>the <mark>initialization must be randomized</mark>, so that the updates for new $W$ will not be identical/same (when doing back prop)</p>

    <ul>
      <li>
        <p>If we set all the weights to be the same, then all the the neurons in the same layer <strong>performs the same calculation</strong>, there by making the whole deep net useless. If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron.</p>

        <p>E.g.</p>

\[z^1 = (W^1)^T a^{0} = \begin{bmatrix}
w_{11} &amp; w_{21} &amp; w_{31}\\
w_{12} &amp; w_{22} &amp; w_{32} \\
\end{bmatrix}\begin{bmatrix}
x_{1}\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
z_{1}^1\\
z_2^1\\
\end{bmatrix}\]

        <p>will have $z_1^1 = z_2^1$ and updates within the same layer will be identical.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>we are picking only one sample from the set because we will be doing <strong>stochastic gradient descent</strong> with the backprop algorithm. This is commonly used when datasets are large so we don’t want to do the entire dataset per step.</p>
  </li>
</ul>

<h2 id="back-propagation">Back Propagation</h2>

<p>The aim of this algorithm is, for each $W^l$ at layer $l$ (assumed bias is absorbed):</p>

\[W^l := W^l - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}\]

<p>basically doing a gradient descent to minimize our loss:</p>

<ul>
  <li>$\alpha$ would be the learning step size, which we can tune as a parameter</li>
</ul>

<p>Now, to compute the derivative, instead of <strong>doing it in a forward pass</strong> so that we need to do:</p>

<ol>
  <li>compute $\partial \mathcal{L}/\partial \mathcal{W^1}$</li>
  <li>then compute $\partial \mathcal{L}/\partial \mathcal{W^2}$</li>
  <li>then compute $\partial \mathcal{L}/\partial \mathcal{W^3}$</li>
</ol>

<p>if we have 3 layers. However, it turns out we can do <mark>achieve all the calculations</mark> if we do it in <mark>a backward pass</mark>:</p>

<ol>
  <li>
    <p>Notice that:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]

    <p>since $Z^l = (W^l)^TA^{l-1}$. notice that $A^{l-1}$ is already computed in the forward pass. Graphically, if $l=2$, we are here:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120225335839.png" alt="image-20220120225335839" /></p>

    <p>note that we basically are doing derivative of a scalar w.r.t. a vector, so <mark>Jacobians</mark> would be the brute force way:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120231135345.png" alt="image-20220120231135345" /></p>

    <p>yet the point is that we can save much effort using chain rule.</p>

    <ul>
      <li>if you have a $f:\mathbb{R}^n \to \mathbb{R}^m$, then the Jacobian will be dimension $\mathbb{R}^{m \times n}$. You can image each row doing $[df_i/dx_1,…,df_i/dx_n]$.</li>
    </ul>
  </li>
  <li>
    <p>Then, we need:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
= \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}\]

    <p>since we know $A^l=f(Z^l)$. If we have sigmoid, then we know $\sigma’(x) = \sigma(x)\cdot(1- \sigma(x))$.</p>

    <ul>
      <li>
        <p>Hence each element such as</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l_{11}}} 
= \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l_{11}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{\sigma(Z^l_{11})}}{\partial \mathcal{Z^l_{11}}}
=\sigma(Z_{11}^l)\cdot(1-\sigma(Z_{11}^l))\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
=A_{11}^l\cdot(1-A_{11}^l)\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

        <p>so then computing for the derivative for the entire matrix, means just doing the above for each element</p>
      </li>
      <li>
        <p>so the <mark>one thing we actually have to compute</mark> would be:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

        <p>though we know $\partial Z^l / \partial A^{l-1}=W^l$, we don’t know the loss function. However, it turns out we only needed to compute this for <mark>one (the last) layer</mark> (see next step)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Lastly, the real shortcut in back propagation is that:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
=W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

    <p>hence, the <mark>other thing we have to compute is</mark>:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

    <p>and that from this formula, knowing $\partial L / \partial Z^{l}$ means we can compute $\partial L / \partial A^{l-1}$ of the <strong>previous layer</strong>!</p>
  </li>
</ol>

<blockquote>
  <p><strong>In summary</strong></p>

  <p>To compute</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]

  <p>we needed to know</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}\]

  <ul>
    <li>notice that this computation is <strong>local</strong>: it only involves stuff from layer $l$</li>
  </ul>

  <p>which requires</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

  <p>which can be done in an efficient way once we are done with a layer:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
=W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

</blockquote>

<p>Therefore, the back propagation algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232037290.png" alt="image-20220120232037290" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>we have an “extra” step here because this assumes that the <em>biases are not absorbed</em></p>
  </li>
  <li>the three main steps are outlined in the summary mentioned above. Due to dependency, they are done in reverse order</li>
  <li>the purple line shows the efficiency, that $\partial L / \partial A^{l}$ can be  computed from previous result with layer $l+1$. The only one what requires computation is the first iteration/last layer.</li>
  <li>this is basically the entire model! i.e. back propagation does the <strong>training/update</strong> of the parameters</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>In the first iteration, if we don’t care about losses yet, we need to figure out the second step on $\partial L / \partial Z^{l}$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232507028.png" alt="image-20220120232507028" style="zoom: 67%;" /></p>

<p>which we are doing element-wise for clarity. This can be easily generalized and placed into a vector:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232742923.png" alt="image-20220120232742923" style="zoom:67%;" /></p>

<ul>
  <li>
    <p>then we can compute easily:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]
  </li>
</ul>

<p>In the second iteration, we notice that we can reuse the results above:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232935679.png" alt="image-20220120232935679" style="zoom:67%;" /></p>

<ul>
  <li>which then can compute the $$</li>
</ul>

<p>Lastly:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120233035309.png" alt="image-20220120233035309" style="zoom:67%;" /></p>

<p>which again reused the result from its higher up layer.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>If we take derivatives in the forward pass, we get only derivative of one variable. If we do it in the backward pass, we do it once and get <mark>all the derivatives</mark> with little effort</p>

  <p>A big advantage of back propagation is also utilizing the fact that we are doing <strong>local</strong> computation (and passing on the result)</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120140038638.png" alt="image-20220120140038638" style="zoom:50%;" /></p>

  <p>which means backpropagation is a special case of <strong>differential programming</strong>, which can be optimized.</p>
</blockquote>

<h2 id="weight-initialization">Weight Initialization</h2>

<p>Basically a uniform distribution for weights and zeros for bias</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127141900682.png" style="zoom:50%;" /></p>

<p>Program</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="c1"># glorot init
</span>    <span class="n">epa</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.01</span>
</code></pre></div></div>

<h2 id="problems-with-nn">Problems with NN</h2>

<p>Recall that our objective is to minimize:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120233655595.png" alt="image-20220120233655595" style="zoom: 50%;" /></p>

<p>This implies the following problem:</p>

<ol>
  <li>What if $n$ is large? Since our loss sums over all $n$ data points, it would take a long time to compute
    <ul>
      <li><strong>Solution</strong>: Stochastic Gradient Descent</li>
    </ul>
  </li>
  <li>Computing derivatives/doing gradient descent of large network takes time
    <ul>
      <li><strong>Solution</strong>: Backpropagation</li>
    </ul>
  </li>
  <li>For each time step/update, the gradient would be <em>perpendicular</em> to the previous one, forming a slow zig-zag pattern (slow to converge).
    <ul>
      <li><strong>Solution</strong>: Adaptive gradient Descent</li>
    </ul>
  </li>
</ol>

<h2 id="example-implementation">Example Implementation</h2>

<p>Consider the implementing the following architecture</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/neural_network_practice.png" style="zoom: 23%;" /></p>

<p>Then we would have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Implement the forward pass
</span><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W1'</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>  <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>

    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W2'</span><span class="p">].</span><span class="n">T</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>    
    <span class="c1"># Z1 -&gt; output of the hidden layer before applying activation
</span>    <span class="c1"># H -&gt; output of the  hidden layer after applying activation
</span>    <span class="c1"># Z2 -&gt; output of the final layer before applying activation
</span>    <span class="c1"># Y -&gt; output of the final layer after applying activation
</span>    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Z1</span>
</code></pre></div></div>

<p>And backward:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implement the backward pass
# Y_T are the ground truth labels
</span><span class="k">def</span> <span class="nf">back_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_T</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">N_points</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># forward propagation
</span>    <span class="n">Y</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Z1</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N_points</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_T</span><span class="p">))</span>
    
    <span class="c1"># back propagation
</span>    <span class="n">dLdY</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">N_points</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_T</span><span class="p">)</span>
    <span class="c1"># dLdZ2 = dLdA2 * dA2dZ2 = dLdA2 * sig(Z2)*[1-sig(1-Z2)] # broadcast multiply
</span>    <span class="n">dLdZ2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dLdY</span><span class="p">,</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">))))</span> 
    <span class="c1"># dLW2 = dLdA2 * dA2dZ2 * dZ2dW2 = dLdZ2 * dZ2dW2 = dLdZ2 * A1 # matrix multiply
</span>    <span class="n">dLdW2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dLdZ2</span><span class="p">)</span> 

     <span class="c1"># dLb2 = dLdA2 * dA2dZ2 * dZ2db2 = dLdZ2 * dZ2db2 = dLdZ2 * 1
</span>    <span class="n">dLdb2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ2</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_points</span><span class="p">))</span>
    
    <span class="c1"># dLdA1 = dLdA2 * dA2dZ2 * dZ2dA1 = dLdZ2 * dZ2dA1 = dLdZ2 * W2
</span>    <span class="n">dLdA1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ2</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span> 
    <span class="c1"># dLdZ1 = dLdA1 * dA1dZ1 = dLdA1 * sig(A1) * [1-sig(A1)] # broadcast multiply
</span>    <span class="n">dLdZ1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dLdA1</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">H</span><span class="p">)))</span>
     <span class="c1"># dLW1 = dLdA1 * dA2dZ1 * dZ2dW1 = dLdZ1 * dZ2dW1 = dLdZ1 * A0 # matrix multiply
</span>    <span class="n">dLdW1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dLdZ1</span><span class="p">)</span> 

    <span class="n">dLdb1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_points</span><span class="p">))</span>
    
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'W1'</span><span class="p">:</span> <span class="n">dLdW1</span><span class="p">,</span>
        <span class="s">'b1'</span><span class="p">:</span> <span class="n">dLdb1</span><span class="p">,</span>
        <span class="s">'W2'</span><span class="p">:</span> <span class="n">dLdW2</span><span class="p">,</span>
        <span class="s">'b2'</span><span class="p">:</span> <span class="n">dLdb2</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">L</span>
</code></pre></div></div>

<h1 id="optimization">Optimization</h1>

<p>In practice, it is important to have the some understanding of the optimizer we will use (e.g. how to gradient descent), because they perform differently in different scenarios.</p>

<p>The ones we use in practice the <mark>different algorithms to find minima</mark> can be separated into the following three classes:</p>

<ul>
  <li>First order methods.
    <ul>
      <li>Gradient Descent (Stochastic, Mini-Batch, Adaptive)</li>
      <li>Momentum Related (Adagrad, Adam, Hypergradient Descent)</li>
    </ul>
  </li>
  <li>Second order methods
    <ul>
      <li>Newton’s Method (<em>generally faster than gradient descent</em> because it uses Hessian/is higher order)</li>
      <li>Quasi Newton’s Method (SR1 update, DFP, BFGS)</li>
    </ul>
  </li>
  <li>Evolution Strategies
    <ul>
      <li>Cross-Entropy Method (uses cluster of initial points as initial conditions, then descent as a group)</li>
      <li>Distributed Evolution Strategies, Neural Evolution Strategies</li>
      <li>Covariance Matrix Adaptation</li>
    </ul>
  </li>
</ul>

<h2 id="overview">Overview</h2>

<p>Again, our goal of learning this is to understand, given an optimization problem of finding best $\theta^*$:</p>

\[\theta^* = \arg\min_\theta J(\theta)\]

<p>where $J$ would be the total loss we are dealing with.</p>

<ul>
  <li>
    <p>an example for $J$ would be</p>

\[J(\theta)  = \left( \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^{(i)}, h(x^{(i)}; \theta)) \right) + \lambda R(\theta)\]

    <p>where we included a regularization term $R(\theta)$ here as well.</p>
  </li>
</ul>

<p>First, let us recall that the basic gradient descent algorithm generally looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125132905495.png" alt="image-20220125132905495" /></p>

<p>where $\eta$ would be tunable:</p>

<ul>
  <li>its aim is to obvious find the $\theta^*$, but it might get <strong>stuck at local minima</strong>
    <ul>
      <li>in that case, we need to add some noise, or use stochastic gradient descent</li>
    </ul>
  </li>
  <li>for large NN, finding $dJ/dW$ takes effort.
    <ul>
      <li>this is optimized with back-propagation algorithm</li>
    </ul>
  </li>
  <li>but obviously this is not the only way to do it, as you shall see soon</li>
</ul>

<blockquote>
  <p><strong>Convex Optimization</strong></p>

  <p>It would be so little pain if $J$ is convex w.r.t. $W$, so that any <strong>local minimum</strong> is also a <strong>global minimum</strong>.</p>

  <ul>
    <li>but often in NN, $J$ is not a convex function of $W$, so we do have the problem of stopping at local minimas.</li>
  </ul>

  <p>For a problem to be a convex optimization (with constraints):</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20211014155124853.png" alt="image-20211014155124853" /></p>

  <p>this problem is a convex optimization <mark>IFF both holds</mark>:</p>

  <ul>
    <li>the <strong>feasible</strong> region of output (due to the constraint) is a <strong>convex set</strong></li>
    <li>the <strong>objective</strong> function $f(\vec{x})$ is a <strong>convex function</strong></li>
  </ul>

</blockquote>

<p>But in reality, this is what are we are facing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133004782.png" alt="image-20220125133004782" style="zoom: 50%;" /></p>

<p>where recall that:</p>

<ul>
  <li>Linear programs: objective function is linear (affine), and constraints are also linear (affine)
    <ul>
      <li>so that the feasible region is a convex set (because the feasible region is always a polygon = convex set)</li>
    </ul>
  </li>
  <li>Quadratic program: objective function is quadratic, and constraints are linear (affine)
    <ul>
      <li>if constraints are quadratic, then the feasible region might not be a convex set.</li>
    </ul>
  </li>
  <li>Conic Program: where constraints are a conic shaped region</li>
  <li>Other common solvers include: <code class="language-plaintext highlighter-rouge">CVX</code>, <code class="language-plaintext highlighter-rouge">SeDuMi</code>, <code class="language-plaintext highlighter-rouge">C-SALSA</code>,</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133237058.png" alt="image-20220125133237058" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>LHS shows starting with different initial points yields different result, i.e. some ended up at local minimas</li>
  <li>RHS shows starting from different initial points yields the same result, i.e. global minimum.</li>
</ul>

<h2 id="derivative-and-gradient">Derivative and Gradient</h2>

<p>Most of the cases we will be dealing with $f(\vec{x})$ where $\vec{x}$ is multi-dimensional. For instance $L(W)$ with loss being dependent on weights. Then, an obvious usage of this would be in gradient descent:</p>

\[w_{t+1} = w_t - \alpha_t \nabla f(w_t)\]

<p>for us using $\alpha_t$ because it can be changing (e.g. in adaptive methods)</p>

<blockquote>
  <p><em>Recall</em></p>

\[\nabla f(\vec{x}) = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} , ..., \frac{\partial f}{\partial x_n}  )\]

  <p>being a $n$ dimensional vector:</p>

  <ul>
    <li>imagine graphing $f(\vec{x})$ in a $\mathbb{R}^{n+1}$ since $\vec{x}\in \mathbb{R}^n$</li>
    <li>then $\nabla f$ points at direction of steepest ascent</li>
  </ul>
</blockquote>

<p>Another useful quantity would be the second derivative:</p>

<blockquote>
  <p><strong>Hessian</strong></p>

  <p>Again, for a scalar function with vector input:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133408736.png" alt="image-20220125133408736" style="zoom: 33%;" /></p>

  <p>which is useful because:</p>

  <ul>
    <li>$\vec{x}^*$ is a local minimum if $H\equiv \nabla^2f$ is <strong>positive semi-definite</strong>
      <ul>
        <li>in the case of $x\in \mathbb{R}$, we know that $f’‘(x) \ge 0$ means minima. In the case of vector input space, you have $n$-directions to look at. If each direction satisfies $Hx = ax$ for $a \ge 0$, then obviously it is “concave up”, and that $Hx = ax$ for all $x$ means $H$ contains only non-negative eigenvalues -&gt; positive semidefinite</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Sometimes, we may want to use <strong>numerical calculations of derivatives</strong> to make sure our formula put in practice is correct:</p>

\[f'(x) \approx \frac{f(x+\epsilon) - f(x - \epsilon)}{2 \epsilon}\]

<p>for small $\epsilon$.</p>

<ul>
  <li>
    <p>notice we are all using $x$ as the input variable. It might be useful in context if we think of $x \to \vec{w}$ being the weights that we need to optimize on.</p>
  </li>
  <li>
    <p>an example program would be</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133621511.png" alt="image-20220125133621511" /></p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>To compute gradient using:</p>

  <ul>
    <li>analytic equation: exact, fast
      <ul>
        <li>e.g. for NN, we derive the derivatives and used the formula</li>
      </ul>
    </li>
    <li>numerical equation: slow, contains error
      <ul>
        <li>useful for debugging, .e.g if backprop is implemented correctly</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="first-order-methods">First Order Methods</h2>

<p>Now, we talk about first order methods: <strong>using only first order derivative</strong> $g_t \equiv \nabla f(x_t)$ for weight (remember we generalized weights $w \to x$ any input) at iteration $t$.</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>The easiest and direct use of $\nabla f(x_t)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133718470.png" alt="image-20220125133718470" style="zoom: 33%;" /></p>

<p>where note that:</p>

<ul>
  <li>
    <p>you could add an early-stopping criteria at the end</p>
  </li>
  <li>
    <p>the tunable learning rate $\alpha_t$ is critical:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Too Big</th>
          <th style="text-align: center">Too Small</th>
          <th>Just Right</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133933698.png" alt="image-20220125133933698" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133949095.png" alt="image-20220125133949095" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134009975.png" alt="image-20220125134009975" /></td>
        </tr>
      </tbody>
    </table>

    <p>but as you might have guessed, step size $\alpha_t$ could be updated automatically in some other methods</p>
  </li>
  <li>
    <p>However, at saddle points, it may cause the <strong>update</strong> be too small. Hence often a noise term will be added</p>

\[x_{t+1} = x_t - \alpha_tg_t + \epsilon_t\]

    <p>for $\epsilon_t \sim N(0, \sigma)$ hoping that it goes out of local minima/saddle points. (related: <a href="#Vanishing/Exploding Gradient">Vanishing/Exploding Gradient</a>)</p>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider logistic loss:</p>

\[J(x;\theta) = -\frac{1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]\]

<p>Then, compute the gradient:</p>

\[\frac{\partial J(\theta)}{\partial \theta_j}  = 
\frac{\partial}{\partial \theta_j} \,\frac{-1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]\]

<p>Carefully computing the derivative yields:</p>

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m\left[h_\theta\left(x^{(i)}\right)-y^{(i)}\right]\,x_j^{(i)}\]

<p>Then you can update $\theta_j$ using this.</p>

<hr />

<p><strong>Problems with Gradient Descent</strong></p>

<ol>
  <li>
    <p>The above takes an entire training set for computing the loss. Takes time.</p>

    <ul>
      <li>use mini-batch or stochastic</li>
    </ul>
  </li>
  <li>
    <p>Computing derivative w.r.t weights $\theta$ takes effort if $\theta$ is high dimensional</p>

    <ul>
      <li>use backpropagation</li>
    </ul>
  </li>
  <li>
    <p>What step-size should we use? We may overshoot if too large of a stepsize.</p>

    <ul>
      <li>adaptive learning rate</li>
    </ul>
  </li>
  <li>
    <p>Gradient descent typically spend too much time in regions that is relatively flat as gradient is small</p>

    <ul>
      <li>
        <p>e.g.</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Normal Region</th>
              <th style="text-align: center">Flat Region</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134255982.png" alt="image-20220125134255982" style="zoom:50%;" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134221684.png" alt="image-20220125134221684" style="zoom:50%;" /></td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Use <a href="#Adaptive Gradient Descent">Adaptive Gradient Descent</a></p>
      </li>
    </ul>
  </li>
</ol>

<h4 id="adaptive-step-size">Adaptive Step Size</h4>

<p>There are certain options we can choose from:</p>

<ul>
  <li>
    <p><strong>decrease learning rate as training progresses</strong> (learn less in the future -&gt; prevent overfitting)</p>

    <ul>
      <li>
        <p>this can be done using either a decay factor that gets smaller over time:</p>

\[\alpha_t = \alpha_0 \frac{1}{1+t\beta}\]

        <p>for $\beta$ being small</p>
      </li>
      <li>
        <p>simply exponential decay:</p>

\[\alpha_t = \alpha_0 \gamma^t\]

        <p>for some $\gamma &lt; 1$ but close to $1$, or</p>

\[\alpha_t = \alpha_0 \exp(-\beta t)\]
      </li>
    </ul>
  </li>
  <li>
    <p><strong>line searches</strong>: given some $\min_x f(x)$, and suppose we are currently at $x_t$ being our <strong>current best guess</strong>. We know the current gradient is $g_t = \nabla f(x)\vert _{x_t} = \nabla f(x_t)$. We consider some step size $\alpha_t$ we <strong>might take</strong>:</p>

\[\phi(\alpha_t) \equiv f(x_t + \alpha_t g_t)\]

    <p>and we want to <strong>approximately minimize $\phi(\alpha_t)$</strong> to output a $\alpha_t$ to use. Basically we are <mark>sliding along the tangent line of the current point and see how far we should slide</mark></p>

    <ul>
      <li>
        <p><strong>backtrack line search</strong></p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125222301156.png" alt="image-20220125222301156" /></p>

        <p>where $t^*$ is our desired $\alpha_t$</p>

        <p>Graphically:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125225435921.png" alt="image-20220125225435921" style="zoom: 67%;" /></p>
      </li>
      <li>
        <p><strong>exact line search</strong></p>

        <p>Solve the following exactly:</p>

\[\min_{\alpha_t} f(x_t + \alpha_t \hat{g}_t)\]

        <p>where $\hat{g}_t = g_t  /\vert g_t\vert$. So this means we want:</p>

\[\nabla f(x_t + \alpha_t \hat{g}_t) \cdot \hat{g}_t = 0\]

        <p>and we need to solve this for $\alpha_t$</p>

        <ul>
          <li>
            <p>one property of this result is that, since we know:</p>

\[\quad \hat{g}_{t+1} = \nabla f(x_t + \alpha_t \hat{g}_t)\]

            <p>So we see that</p>

\[\hat{g}_{t+1} \cdot \hat{g}_t = 0\]

            <p>meaning <strong>consecutive runs gives perpendicular gradient direction</strong>. This makes sense since we are taking the <em>optimal step size</em>, i.e. we have walked the farthest along that direction.</p>
          </li>
          <li>
            <p>Finding $\alpha_t$ is computationally expensive as we need to solve for it, so it is rarely used</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>adaptive line search</strong>: skipped</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h4>

<p>A common technique within gradient descent is to split your dataset into $n$ sets of size $k$, and train each set as <strong>one step for updating the gradient</strong>.</p>

<ul>
  <li>so that we don’t spend time computing the loss function on the entire data</li>
  <li>if you want, you can also parallelize this computation</li>
</ul>

<p>This is useful because, for a sample of size $k$, the <strong>sample mean</strong> follows the central limit theorem:</p>

\[\hat{\mu} \sim N(\mu, \sigma^2/n)\]

<p>which can be easily seen because:</p>

<ul>
  <li>
    <p>$\text{Var}[X_i] = \sigma^2$, and using linearity:</p>

\[\text{Var}\left[\frac{1}{n}\sum X_i\right] = \frac{1}{n^2} \text{Var}\left[\sum X_i\right] = \frac{1}{n^2} \sum\text{Var}\left[ X_i\right] = \sigma^2 / n\]
  </li>
  <li>
    <p>This is good because **standard deviation of $\hat{\mu} \propto 1/\sqrt{n}$ **</p>

    <p>So if using 100 samples vs 10000 samples means:</p>

    <ul>
      <li>faster computation for factor of 100</li>
      <li>but only more error of factor of 10</li>
    </ul>
  </li>
</ul>

<h4 id="stochastic-gradient-descent">Stochastic Gradient Descent</h4>

<p>Basically equivalent of Mini-batch of size $1$</p>

<ul>
  <li>i.e. each update involves <strong>taking 1 random sample</strong></li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125225520623.png" alt="image-20220125225520623" style="zoom: 67%;" /></p>

<p>Some common nomenclatures:</p>

<ul>
  <li><strong>Epoch</strong>: a pass through the entire data</li>
</ul>

<p>Some <strong>properties</strong>:</p>

<ul>
  <li>unbiased estimators of the true gradient</li>
  <li>
    <p>early steps often converge fast towards the minimum</p>
  </li>
  <li>very noisy -&gt; but increases the chance of getting a global minima
    <ul>
      <li>e.g. at saddle points, it may cause the step be too small. But this is <strong>already noisy</strong>, so no problem.</li>
    </ul>
  </li>
</ul>

<h3 id="adaptive-gradient-descent">Adaptive Gradient Descent</h3>

<p>Either we use normal gradient descent, or gradient descent with optimized steps, we faced the problem of taking too long to converge in <strong>flat regions</strong>.</p>

<p>An overview would be that it uses <strong>gradients from previous steps</strong> to compute current gradient.</p>

<ul>
  <li>want to achieve faster convergence by move faster in dimension with low curvature, and slower in dimension with oscillations</li>
  <li>
    <p>the more official documentation:  <strong>AdaGrad</strong> for short, is an <strong>extension of the gradient</strong> descent optimization algorithm that allows the step size in each dimension used by the optimization algorithm to be automatically adapted <mark>based on the gradients seen for the variable</mark></p>
  </li>
  <li>however, some critics of this would say that it yields different result with gradient descent</li>
</ul>

<p>Examples with adaptive gradients include:</p>

<ul>
  <li>Momentum</li>
  <li>AdaGrad</li>
  <li>Adam</li>
</ul>

<h4 id="momentum">Momentum</h4>

<p>The basic idea is that the momentum vector <strong>accumulates gradients from previous iterations</strong> for computing the current gradient.</p>

<p><strong>Arithmetically weighted moving average</strong></p>

\[a_t = \frac{na_t +  (n-1)a_{t-1}) + ... + a_{t-n+1}}{n+(n-1)+ ... + 1}\]

<p>for basically imagining $a_t \to g_t$ is the gradient</p>

<ul>
  <li>$n$ is the <strong>weight</strong> which we can specify</li>
  <li>basically this is in a <mark>weighted moving average</mark> the weights decrease <mark>arithmetically</mark>, normalized by the sum of weights</li>
</ul>

<p>In the end, we see <strong>accumulation of gradients</strong> because:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232040667.png" alt="image-20220125232040667" style="zoom: 80%;" /></p>

<p>where notice that:</p>

<ul>
  <li>$s_t = \sum_{i=t-n+1}^ta_i$ is the accumulation of past gradients, since $a_t \to g_t$</li>
</ul>

<hr />

<p>Alternatively, there is also an expoentially weighted version</p>

<p><strong>Exponentially Weighted Moving Average</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232352096.png" alt="image-20220125232352096" style="zoom: 80%;" /></p>

<p>where here:</p>

<ul>
  <li>
    <p>again basically $a_t \to g_t$</p>
  </li>
  <li>
    <p>the parameter is actually $(1-\alpha) = \beta$ for convenience, and we want $\beta \in [0,1)$</p>
  </li>
  <li>
    <p>in an algorithm:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" /></p>

    <p>where basically $x$ would be our weights.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>A first problem with momentum is that the step sizes may not decrease once we have reached close to the minimum that may cause oscillations, which can be remedied by using Nesterov momentum (Dozat 2016) that replaces the gradient with the gradient after computing momentum (Dozat 2016):</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232704982.png" alt="image-20220125232704982" style="zoom:80%;" /></p>
</blockquote>

<p>Graphically</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125233001622.png" alt="image-20220125233001622" /></p>

<h4 id="adagrad">AdaGrad</h4>

<p>This deals with the case that we <strong>didn’t talk about what to do with $\alpha_t$</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" /></p>

<p>So one variation, AdaGrad, <mark>adapts the learning rate</mark> to the parameters, i.e. $\alpha_t$ is different for <strong>each $\theta_i$</strong>/parameter:</p>

<ul>
  <li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features</li>
  <li>larger updates (i.e. high learning rates) for parameters associated with infrequent features</li>
</ul>

<p>For this reason, it is well-suited for dealing with sparse data, and suitable for SGD.</p>

<p>Instead of using $\alpha_t$ for all parameters at current time, use</p>

\[\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha_t}{\sqrt{s_{t,i} + \epsilon}} g_{t,i}\]

<p>and that $s_{t,i}$ is a weighted sum of gradients of $\theta_i$ up to time $t$:</p>

\[s_{t,i} = \beta s_{t-1,i} + g_{t,i}^2\]

<p>for $g_{t,i}$ is the gradient for the $\theta_i$.</p>

<p><strong>Problem</strong></p>

<p>This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is <strong>no longer able to acquire additional knowledge</strong>:</p>

\[\lim_{s_{t,i} \to \infty} \frac{1}{\sqrt{s_{t,i} + \epsilon}} = 0\]

<p>This is then solved by:</p>

<ul>
  <li>
    <p><strong>Adadelta</strong></p>

    <p>An exponential decaying average of square updates without a learning rate, replacing</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234218229.png" alt="image-20220125234218229" style="zoom:80%;" /></p>
  </li>
  <li>
    <p><strong>RMSProp</strong></p>

    <p>Adagrad using a weighted moving average, replacing:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234149326.png" alt="image-20220125234149326" style="zoom: 80%;" /></p>
  </li>
</ul>

<h4 id="adam">Adam</h4>

<p>Adaptive moment estimation, or Adam (Kingma &amp; Ba 2014), combines the best of both momentum updates and Adagrad-based methods as shown in Algorithm 6.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234407385.png" alt="image-20220125234407385" style="zoom:67%;" /></p>

<p>where basically:</p>

<ul>
  <li>uses momentum in the red part</li>
  <li>uses AdaGrad like adaptive learning rate on the yellow part</li>
  <li>since it combined two models, we have <strong>two parameters to specify</strong>. Typically $\beta_1 = 0.9, \beta_2 = 0.99$</li>
</ul>

<p>Several improvements upon Adam include:</p>

<ul>
  <li>
    <p><strong>NAdam</strong> (Dozat 2016) is Adam with Nesterov momentum</p>
  </li>
  <li>
    <p><strong>Yogi</strong> (Zaheer, Reddi, Sachan, Kale &amp; Kumar 2018) is Adam with an improvement to the second momentum term which is re-written as:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234659018.png" alt="image-20220125234659018" style="zoom:80%;" /></p>
  </li>
  <li>
    <p><strong>AMSGrad</strong> (Reddi, Kale &amp; Kumar 2018) is Adam with the following improvement</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234724476.png" alt="image-20220125234724476" style="zoom:80%;" /></p>
  </li>
</ul>

<h3 id="hyper-gradient-descent">Hyper-gradient Descent</h3>

<p>Hypergradient descent (Baydin, Cornish, Rubio, Schmidt &amp; Wood 2018) performs <mark>gradient descent on the learning rate</mark> within gradient descent.</p>

<ul>
  <li>may be applied to any adaptive stochastic gradient descent method</li>
</ul>

<p>The basic idea is to consider $\partial f(x_t)/ \partial \alpha$, for $x \to w$</p>

\[\frac{\partial f(w_t)}{\partial \alpha} = \frac{\partial f(w_t)}{\partial w_t} \frac{\partial w_t}{\partial \alpha}\]

<p>we know that $w_t =  w_{t-1} - \alpha g_{t-1}$:</p>

\[\frac{\partial f(w_t)}{\partial \alpha} = g_t \cdot \frac{\partial }{\partial \alpha} ( w_{t-1} - \alpha g_{t-1})\]

<p>where:</p>

<ul>
  <li>The schedule may lower the learning rate when the network gets stuck in a local minimum, and increase the learning rate when the network is progressing well.</li>
</ul>

<p>Algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234826019.png" alt="image-20220125234826019" style="zoom:80%;" /></p>

<h3 id="vanishingexploding-gradient">Vanishing/Exploding Gradient</h3>

<p>These problem is encountered when training artificial <strong>neural networks</strong> with <mark>gradient-based learning methods</mark> and <mark>backpropagation</mark>. In such methods:</p>

<ul>
  <li>
    <p><strong>vanishing gradient:</strong> during each iteration of training each of the neural network’s weights receives an update proportional to the partial derivative of the error function with respect to the current weight.</p>

\[W^l := W^l - \alpha \frac{\partial L}{\partial W^l}\]

    <p>The problem is that in some cases, the <strong>gradient will be vanishingly small</strong>, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.</p>
  </li>
  <li>
    <p>When <strong>activation functions</strong> are used whose derivatives can take on larger values, one risks encountering the related <strong>exploding gradient problem</strong></p>
  </li>
</ul>

<p>One example of the problem cause for vanishing gradient</p>

<ul>
  <li>traditional <strong>activation</strong> functions such as the hyperbolic tangent function have gradients in the range $(0,1]$, is very small</li>
  <li>Since <strong>backpropagation</strong> computes gradients by the chain rule. This has the effect of <mark>multiplying $n$ of these small numbers</mark> to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially</li>
</ul>

<h2 id="second-order-methods">Second Order Methods</h2>

<p>First order methods are easier to implement and understand, but they are <strong>less efficient than second order methods</strong>.</p>

<ul>
  <li>Second order methods use the first and <mark>second derivatives</mark> of a univariate function or the gradient and Hessian of a multivariate
function to compute the step direction</li>
  <li>Second order methods approximate the objective function using a <strong>quadratic</strong> which results in faster convergence
    <ul>
      <li>imagine basically 2nd order methods -&gt; parabola -&gt; go down a bowl with a bowl (2nd order); as compared to with a ruler (1st order method)</li>
    </ul>
  </li>
  <li>but a problem that they need to overcome is how to deal with computing/storing <strong>Hessian</strong> matrix, which could be large</li>
</ul>

<h3 id="newtons-method">Newton’s Method</h3>

<p>Basically we know that we can find the <strong>root of an equation</strong> using newton’s method:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125235632561.png" alt="image-20220125235632561" style="zoom: 67%;" /></p>

<p>so we basically guessed $x_{t+1}$ to be the root by <strong>fitting a line</strong>:</p>

\[x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}\]

<p>which basically does:</p>

<ul>
  <li>the number of steps to move being $\Delta x = f(x_t)/{f’(x_t)}$</li>
</ul>

<p>Then, since our goal is to solve (in 1-D case):</p>

\[\min f(x) \to f'(x) = 0\]

<p>So basically we consider finding root for $f’(x)$:</p>

\[x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}\]

<p>This results in</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125235919309.png" alt="image-20220125235919309" style="zoom: 67%;" /></p>

<p>where the blue line is the “imagined function” using Newton’s Method</p>

<ul>
  <li>notice it is a <strong>quadratic</strong></li>
  <li>therefore, it goes down the “bowl” <strong>faster than first order methods</strong> as mentioned before</li>
</ul>

<hr />

<p>The same formula can be derived using Taylor’s methods as well</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000201474.png" alt="image-20220126000201474" style="zoom: 67%;" /></p>

<p>However, this is <strong>useful</strong> because it guides on how to deal with <mark>vector input functions $f(\vec{x})$</mark> which we need to deal with:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000337166.png" alt="image-20220126000337166" style="zoom: 67%;" /></p>

<p>where the last step is basically our new update rule.</p>

<ul>
  <li>
    <p>note that this $H^{-1}$ basically <strong>takes place of the $\alpha_t$</strong> we had in first order methods</p>
  </li>
  <li>
    <p>therefore the algorithm is:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000628969.png" alt="image-20220126000628969" style="zoom:67%;" /></p>
  </li>
</ul>

<p>However, some problem resides:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000538635.png" alt="image-20220126000538635" style="zoom:80%;" /></p>

<p>notice that computing and inverting Hessian takes lots of computation, and storing Hessian takes space!</p>

<h3 id="quasi-newton-methods">Quasi-Newton Methods</h3>

<p>Quasi-Newton methods, which provide an iterative <strong>approximation</strong> to the inverse Hessian $H^{-1}$, so that it may:</p>

<ul>
  <li>avoid computing the second derivatives</li>
  <li>avoid inverting the Hessian</li>
  <li>may also avoid storing the Hessian matrix.</li>
</ul>

<p>The idea is to start thinking exactly what we need to approximate. Our goal is anyway the <strong>iterative update</strong>:</p>

\[x^+ = x - B^{-1}\nabla f(x)\]

<p>so our goal is to <mark>approximate $B^{-1} \approx \nabla^2 f$</mark>. The task is therefore find some <strong>conditions to calculate $B$</strong></p>

<p>By definition of second derivative, we know that:</p>

\[\nabla f(x^k+s^k) - \nabla f(x^k) \approx B^k s^k\]

<p>where:</p>

<ul>
  <li>$s^k$ is the <strong>step size</strong> at iteration $k$</li>
  <li>$B^k$ is our approximation of Hessian/second derivative at step $k$</li>
</ul>

<p>Now, since it will be an approximation, we want to impose some constraints to make the approximation good:</p>

<ol>
  <li>
    <p>Second equation <strong>for next $B$</strong> should hold eaxctly:</p>

\[\nabla f(x^{k+1}) - \nabla f(x^k) =  B^{k+1}s^k\]

    <p>where $x^{k+1} = x^k+s^k$. This will be then represented as:</p>

\[B^{k+1}s^k = y^k\]

    <p>for $\nabla f(x^{k+1}) - \nabla f(x^k)  \equiv y^k$, or even more simply:</p>

\[B^{+}s = y\]
  </li>
  <li>
    <p>We also want the following <strong>desirable properties</strong></p>

    <ul>
      <li>$B^+$ is symmetric, as Hessians are symmetric</li>
      <li>$B^+$ should be close to $B$, which is the previous approximation</li>
      <li>$B,B^+$ being positive definite</li>
    </ul>
  </li>
</ol>

<p>Now, we explore some <strong>approximations for $B^+$</strong> that attempts to satisfy the above constraint.</p>

<h4 id="sr1-update">SR1 Update</h4>

<p>This is the simpliest update procedure, such that $B^+$ can be close to $B$, and it will be symmetric:</p>

\[B^+ = B + a u u^T\]

<p>for some $a,u$ we will solve soon. Notice that if we let this be our <strong>update rule for $B$</strong> (first iteration just initialize $B=I$), and we have the <mark>enforcement that secant equation should hold</mark>:</p>

\[y=B^+s = Bs + a u u^Ts=Bs + (au^Ts)u\]

<p>for $s, u, y$ all being vectors. Notice that this means:</p>

\[y - Bs = (au^Ts)u\]

<p>where:</p>

<ul>
  <li>both sides of the equation are <strong>vectors</strong>! This means that $u$ is a scalar multiple of $y-Bs$.</li>
</ul>

<p>So we can <mark>solve for $u,a$</mark>, and obtain the solution and <strong>plug back into our update rule for $B^+$</strong>:</p>

\[B^+ = B + \frac{(y-Bs)(y-Bs)^T}{(y-Bs)^Ts}\]

<p>where at iteration $k$, we already know $B\equiv B^k, s \equiv s^k$ and $y$, so we can compute $B^+$ at iteration $k$.</p>

<hr />

<p><em>Just to be clear</em>, using the above formula our <strong>descent algorithm</strong> would be:</p>

<p><strong>At iteration $k$</strong></p>

<ol>
  <li>compute $(B^{k})^{-1} \nabla f(x^k)$</li>
  <li>do the descent $x^{k+1} = x^k - \alpha_k (B^{k})^{-1} \nabla f(x^k)$ for some tunable parameter $\alpha_k$</li>
  <li>prepare $B^{k+1}$ using the above formula.</li>
</ol>

<hr />

<p>Now, while this technically <strong>computes the approximation</strong>, we can make the algorithm even better by <strong>directly computing $H = B^{-1}$</strong> and its updates using the above formula for $B^+$.</p>

<p>Using the following theorem:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220130161458549.png" alt="image-20220130161458549" /></p>

<p>We can show that $(B^+)^{-1}$ <strong>can be computed directly</strong></p>

\[(B^+)^{-1} = H^+ = H + \frac{(s-Hy)(s-Hy)^T}{(s-Hy)^Ty}\]

<p>then we just use $H$ and $H^+$ all the time instead of $B,B^+$ in the above algorithm.</p>

<h4 id="other-approximations">Other Approximations</h4>

<p>The <strong>David-Fletcher-Powell (DFP)</strong> correction is defined by</p>

\[H^+ = H+ \frac{ss^T}{y^Ts} - \frac{(Hy)(Hy)^T}{y^T(Hy)}\]

<p>The <strong>Broyden-Fletcher-Goldfarb-Shannon</strong> (BFGS) is defined by:</p>

\[H^+ = H+ \frac{2(Hy)s^T}{y^T(Hy)} - \left( 1 + \frac{y^T s^T}{y^T(Hy)} \right)\frac{(Hy)(Hy)^T}{y^T(Hy)}\]

<p>In summary, they all attempt to approximate the real Hessian, which is expensive in computation.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>These methods are similar to each other in that they all begin by initializing the inverse Hessian to the identity matrix and then iteratively update the inverse Hessian. These three update rules differ from each other in that their <strong>convergence properties improve upon one another</strong>.</p>
</blockquote>

<h2 id="evolution-strategies">Evolution Strategies</h2>

<p>In contrast to gradient descent methods which advance a single point towards a local minimum, evolution strategies update
a probability distribution, from which multiple points are sampled, lending itself to a highly efficient distributed computation</p>

<blockquote>
  <p><strong>Useful resource</strong></p>

  <ul>
    <li>https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#simple-gaussian-evolution-strategies</li>
  </ul>
</blockquote>

<p>Intuition: Instead of updating a single initial point and go downhill, use <strong>a distribution of points</strong> to go downhill</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125140715802.png" alt="image-20220125140715802" /></p>

<h3 id="simple-gaussian-evolution-strategies">Simple Gaussian Evolution Strategies</h3>

<p>at each iteration, we sample from distribution and update that distribution</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126002014142.png" alt="image-20220126002014142" /></p>

<h3 id="covariance-matrix-adaptation">Covariance Matrix Adaptation</h3>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/CMA-ES-algorithm.png" alt="CMA-ES Algorithm" style="zoom: 33%;" /></p>

<p>Example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126002131825.png" alt="image-20220126002131825" style="zoom: 67%;" /></p>

<h3 id="natural-evolution-strategies">Natural Evolution Strategies</h3>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/NES-algorithm.png" alt="NES" style="zoom: 33%;" /></p>

<h1 id="regularization-1">Regularization</h1>

<p>Regularization is a technique that helps prevent over-fitting by <strong>penalizing the complexity</strong> of the network. Often, we want our model to achieve <strong>low bias and low variance</strong> for <mark>test set</mark>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131208903.png" alt="image-20220127131208903" style="zoom: 50%;" /></p>

<p>where</p>

<ul>
  <li>our final aim is to have the model generalize to <strong>unseen data</strong>.</li>
</ul>

<blockquote>
  <p>Why <strong>overfitting</strong> happens? In general it is because your <mark>training dataset is not representative of all the trends in the population</mark>, so that you could fit too much to the training data and <strong>miss the real “trends”</strong> in the population.</p>

  <ul>
    <li>hence, it cannot generalize to test sets well</li>
  </ul>
</blockquote>

<hr />

<p><em>Recall that</em></p>

<p>Bias and variance are basically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127164739968.png" alt="image-20220127164739968" style="zoom:50%;" /></p>

<p>and intuitively:</p>

<ul>
  <li><strong>bias</strong>: error introduced by approximating a complicated true model by a simpler model</li>
  <li><strong>variance</strong>: amount by which our approximation/model would change for different training sets</li>
</ul>

<p>e.g. an unbiased estimator would have:</p>

\[\mathbb{E}_{\vec{x}\sim \mathcal{D}}[\hat{\theta}(\vec{x})] = \lang \hat{\theta}(\vec{x}) \rang = \theta\]

<p>which is different from consistency:</p>

\[\lim_{n \to \infty} \hat{\theta}_n(\vec{x}) = \theta\]

<p>In reality, NN does better than traditional ML models such as SVM by being more complicated:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131541371.png" alt="image-20220127131541371" style="zoom: 67%;" /></p>

<hr />

<p>Main <strong>regularization techniques</strong> here include:</p>

<ol>
  <li>add a penalty term of the weights $W$ directly to the loss function</li>
  <li>use dropouts, which randomly “disables” some neuron during training</li>
  <li>augment the data</li>
</ol>

<h2 id="generalization">Generalization</h2>

<p>Training data is a sample from a population and we would like our neural network model to <mark>generalize well to unseen test data</mark> drawn from the same population.</p>

<p>Specifically, the <strong>definition of generalization error</strong> would be the difference between the <strong>empirical loss</strong> and <strong>expected loss</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127171220853.png" alt="image-20220127171220853" style="zoom: 67%;" /></p>

<p>where in Machine Learning, you would have seen something like this:</p>

\[\mathrm{err}(f) := \mathbb{P}_{(x,y)\sim \mathcal{D}}[f(x) \neq y]\]

<p>here:</p>

<ul>
  <li>
    <p>$f$ is a given model to test, and $\text{err}(f)$ is the generalization error.</p>
  </li>
  <li>
    <p>so it is technically equivalent to the red highlighted box only. Yet the whole expression resembles the PAC learning criterion:</p>

\[\mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon\]

    <p>where $f^<em>$ is the optimal predictor in the class $\mathcal{F}$, such that $f^</em> =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$.</p>

    <ul>
      <li>however, notice that it is <strong>not</strong>, because here we are computing $\text{err}$ which is “generalization error” for both, instead of computing sample error.</li>
    </ul>
  </li>
</ul>

<p>The difference between the two is <strong>important</strong>:</p>

<ul>
  <li>suppose $G(f(X,W))  = 0$ for our model $f$. Then it means our empirical loss is as good as the expected loss. This only implies that our model has done the “best it could”.</li>
  <li>but suppose $\mathrm{err}(f)=0$, this means that our model is performing perfectly on the population, which I think is a stronger statement than the above.</li>
</ul>

<blockquote>
  <p>The key idea is that, if <strong>generalization error is low</strong>, then our model is not overfitting (doing the same performance for both train and test dataset)</p>

  <ul>
    <li>though this metric is <mark>not computable</mark> since we don’t have the population, there are various ways to “estimate” it, like doing Cross Validation with many folds. (<a href="#Cross Validation">Cross Validation</a>)</li>
  </ul>

  <p>However, it is important to remember that <strong>generalization error</strong> depends on <mark>both variance and bias</mark> (and noise) of the model/dataset:</p>

\[\mathbb{E}[y-\hat{f}(x)]^2 =  \text{Var}[\hat{f}] + ( \text{Bias}(\hat{f}))^2 + \text{Var}[\epsilon]\]

  <p>where $\text{Var}[\epsilon]$ is the variance of the <strong>noise of the data</strong>, i.e. $y = W^Tx + \epsilon$ if you think about regression.</p>

  <ul>
    <li>therefore, this is why we want to <mark>reduce bias and variance</mark>!</li>
  </ul>
</blockquote>

<p>In practice, we see things like this:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131908115.png" alt="image-20220127131908115" style="zoom:67%;" /></p>

<p>where usually:</p>

<ul>
  <li>Adding more training data $(X, Y)$ increases the generalization accuracy until a limit, i.e. the best our model can do anyway $\neq$ the optimal bayes</li>
  <li>Unless we have sufficient data, a very complex neural network may <strong>fit the training data very well at the expense of a poor fit to the test data</strong>, resulting in a large gap between the training error and test error, which is over-fitting, as shown above</li>
</ul>

<hr />

<p>However, in Deep Learning, recent results have shown:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127132009196.png" alt="image-20220127132009196" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>for DL it seems that we are double descending, so we may want to “overfit”</li>
</ul>

<h3 id="cross-validation">Cross Validation</h3>

<blockquote>
  <p><strong>Cross validation</strong> allows us to estimate the <strong>mean and variance of the generalization error</strong> using our limited data.</p>

  <ul>
    <li>mean generalization error is the average of the generalization error over all $k$ models and is a good <mark>indicator for how well a model performs on unseen data</mark></li>
  </ul>
</blockquote>

<p>In short, the idea is simple. We randomly split the data <strong>into $k$ folds</strong></p>

<ol>
  <li>take the $i$-th fold to be testing, and the rest, $k-1$ folds being training data</li>
  <li>learn a model $f_i$</li>
  <li>compute generalization error of $f_i$</li>
  <li>repeat 1-3 for $k$ times, but with a <strong>new $i$</strong></li>
</ol>

<p>This is useful because now we can compute the <strong>mean and variance of generalization error</strong> using the $k$ different models we trained.</p>

<ul>
  <li>i.e. think of the evaluation metric being applied on your <em>model choice/architecture</em>, hence the need of mean/variance so it doesn’t depend much on “which data is chosen to be training”</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>We can also use cross validation to select features for building a model. We can build many models with <strong>different subsets of features</strong> and then compute their mean and variance of the generalization error to determine <mark>which subset performs best</mark>.</p>
</blockquote>

<h2 id="regularization-methods">Regularization Methods</h2>

<p>Our general goal is to <strong>reduce both bias and variance</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127190032359.png" alt="image-20220127190032359" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>“better optimization” means using a better optimizer, as covered in the previous chapter</li>
  <li>here, our focus is on the bottom right part: <strong>regularize</strong></li>
</ul>

<p>Some main methods we will discuss</p>

<ol>
  <li>add a penalty term of the weights $W$ directly to the loss function, usually using the <strong>norm</strong></li>
  <li>use <strong>dropouts</strong>, which randomly “disables” some neuron during training</li>
  <li><strong>augment</strong> the data</li>
</ol>

<blockquote>
  <p>Different methods have different specific effects technically, though the overall effect is that they reduce overfitting.</p>
</blockquote>

<h3 id="vector-norms">Vector Norms</h3>

<p>We define vector norms before discussing regularization using different norms.</p>

<p>For all vectors $x$, $y$ and scalars $\alpha$ all vector norms <strong>must satisfy</strong></p>

<ol>
  <li>$\vert \vert x\vert \vert  \ge 0$ and $\vert \vert x\vert \vert =0$ iff $x = 0$</li>
  <li>$\vert \vert x+y\vert \vert  \le \vert \vert x\vert \vert  + \vert \vert y\vert \vert$</li>
  <li>$\vert \vert \alpha x\vert \vert  = \vert \alpha\vert  \,\vert \vert x\vert \vert$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Under this definition:</p>

  <ul>
    <li>$L_p$ for $p &lt; 1$ will have non-convex shapes</li>
    <li>$L_0$ does not count as a norm</li>
  </ul>
</blockquote>

<p>The general equation is simply:</p>

\[||x||_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}\]

<p>Some most common norms include:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127191601903.png" alt="image-20220127191601903" /></p>

<h3 id="regularized-loss-functions">Regularized Loss Functions</h3>

<p>One way to regularize it to use <strong>regularized loss function</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127132745260.png" alt="image-20220127132745260" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>$\lambda$ would be tunable.
    <ul>
      <li>If $\lambda$ = 0, then no regularization occurs.</li>
      <li>If $\lambda$ = 1, then all weights are penalized equally.</li>
      <li>A value between $0$ and $1$ gives us a tradeoff between fitting complex models and fitting simple models</li>
    </ul>
  </li>
  <li>Notice that here $R(w)$ refers to regularizing the <strong>entire weight of the network $W$</strong>.
    <ul>
      <li>if you want to have different regularization for different layers, you need to do $R_1(W^1)+R_2(W^2)$ in the final loss term.</li>
    </ul>
  </li>
  <li>Common types of regularization are $L_1$ and $L_2$
    <ul>
      <li>$L_1$ regularization is a penalty on the sum of absolute weights which <strong>promote sparsity</strong></li>
      <li>$L_2$ regularization is a penalty on the sum of the squares of the weights which prefer <strong>feature contribution being distributed evenly</strong></li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>If we are using SGD, and we added a $L_2$ regularization would cause our <strong>gradient update rules to change</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127133353173.png" alt="image-20220127133353173" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>the extra term is due to regularization. so if $\lambda \to 0$, we get back to regular SGD</li>
  <li>also confirms that $W_t$ in general are smaller if we have regularization</li>
</ul>

<h4 id="ridge-and-lasso-regression">Ridge and Lasso Regression</h4>

<p>Recall that in machine learning, the following is <strong>Ridge Regression</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192337546.png" alt="image-20220127192337546" style="zoom: 67%;" /></p>

<p>The solution for the ridge <mark>regression</mark> can be solved exactly:</p>

\[\vec{w}_{ridge}=(X^TX + \lambda I)^{-1}X^T \vec{y}\]

<p>which basically comes from taking the derivative of the objective and setting it to zero, and note that:</p>

<ul>
  <li>this matrix $X^TX + \lambda I$ is exactly <mark>invertible</mark> since it is now <strong>positive definite</strong> (because we added some positive number to diagonal)</li>
  <li>since $X^TX + \lambda I$ is invertible, this always result in a <mark>unique solution</mark>.</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>analytic solution doesn’t exist for <mark>DL</mark>, since our prediction is no longer:</p>

\[\hat{y} = XI\beta\]

      <p>which is for simple linear regression.</p>

      <p>But in DL, we have a NN with many nonlinear functions nested like:</p>

\[\hat{y} = f_3(W^3 \, f_{2}(W^2\,f_{1}(W^1X)))\]

      <p>where each layer $f$ are the activation functions for each layer. The solution of this is no longer analytic.</p>
    </li>
  </ul>
</blockquote>

<p>Yet, since this problem can be converted to the <strong>constraint optimization problem</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192522192.png" alt="image-20220127192522192" style="zoom: 67%;" /></p>

<p>using <strong>Lagrange Method</strong>, then, the problem basically looks like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Objective Function</th>
      <th style="text-align: center">Contour Projection into $w$ Space</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192856939.png" alt="image-20220127192856939" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192905141.png" alt="image-20220127192905141" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><em>Recall</em>: Lagrange Penalty Method</p>

  <p>Consider the problem of:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192658890.png" alt="image-20220127192658890" /></p>

  <p>This problem will be the same as minimizing the augmented function</p>

\[L(\vec{x}, \vec{\lambda}) := f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})\]

  <p>and recall that :</p>

  <ul>
    <li>our aim was to minimize $f(\vec{x})$ <mark>such that $g_i(\vec{x}) \le 0$ is satisfied</mark></li>
    <li>$\vec{x}$ is the original variable, called primal variable as well</li>
    <li>$\lambda_i$ will be some new variable, called <strong>Lagrange/Dual Variables</strong>.</li>
  </ul>
</blockquote>

<hr />

<p>Similarly, if we use $L_1$ norm, then we have <strong>Lasso’s Regression</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192950886.png" alt="image-20220127192950886" style="zoom:50%;" /></p>

<p>Geographically, we are looking at:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Actual Aim (Sparsity)</th>
      <th style="text-align: center">Lasso’s Approximation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193020129.png" alt="image-20220127193020129" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193026129.png" alt="image-20220127193026129" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Sadly,  there is <strong>no closed form solution</strong> even for simple regression in this case.</p>

<h3 id="dropout-regularization">Dropout Regularization</h3>

<p>The idea is that we <strong>randomly dropout neurons</strong> by setting their activations to be $0$.</p>

<ul>
  <li>
    <p>this will reduce cause the training to be less accurate, but makes it more robust in overfitting as those neurons “won’t fit all the time” to the data</p>
  </li>
  <li>
    <p>this is done in training only. When testing, we don’t drop them out</p>
  </li>
</ul>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Fully connected neural network</th>
      <th style="text-align: center">Dropout regularization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193519465.png" alt="image-20220127193519465" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193527248.png" alt="image-20220127193527248" /></td>
    </tr>
  </tbody>
</table>

<p>Since activations are randomly set to zero during training, we basically <strong>implement it by adding a layer before after activation</strong></p>

\[a_j^l := a_j^l I_{j}^l\]

<p>where $I_j^l$ is like a <strong>mask, deciding whether if it will be dropped</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193726263.png" alt="image-20220127193726263" style="zoom: 67%;" /></p>

<p>Intuitively:</p>

<ul>
  <li>over iterations, some neurons will be dropped -&gt; less overfitting on those neurons. In some other cases, those neurons will need to stand in for others.</li>
  <li>
    <p>overall, we want to keep the <strong>same magnitudes of “neurons”</strong> for that layer even if we dropped out, hence $1/(1-p_l)$ scale up, so that they <mark>stand in</mark> for the dropped out neurons</p>
  </li>
  <li>$p_l$ is a hyper-parameter. In some framework we can set it, in some other like <code class="language-plaintext highlighter-rouge">keras</code> it is <mark>automatically tuned</mark></li>
</ul>

<p>To implement it in code, we use a mask:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
    <span class="c1"># a mask
</span>    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">&gt;</span> <span class="n">prob</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span>
    <span class="n">M</span> <span class="o">/=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">*=</span> <span class="n">M</span> <span class="c1"># applying the mask
</span>    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">M</span>
</code></pre></div></div>

<p>note that:</p>

<ul>
  <li>forward propagation: apply and <em>store the mask</em></li>
  <li>backward propagation: <em>load the mask</em> and apply derivatives
    <ul>
      <li>since $a_j^l := a_j^l I_{j}^l$, then backpropagation equation needs to be updated as well</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Examples</em></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127134329966.png" alt="image-20220127134329966" style="zoom:67%;" /></p>

<h4 id="least-square-dropout">Least Square Dropout</h4>

<p>Dropout is actually not completely new</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127194559956.png" alt="image-20220127194559956" style="zoom:67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>the solution is <strong>exact</strong></li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>again, analytic solution doesn’t exist for DL, since our prediction is not a simple linear regression but concatenating a bunch of nonlinear operations as well</li>
  </ul>
</blockquote>

<h4 id="least-squares-with-noise-input">Least Squares with Noise Input</h4>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127194543146.png" alt="image-20220127194543146" style="zoom:67%;" /></p>

<h3 id="data-augmentation">Data Augmentation</h3>

<blockquote>
  <p><strong>Data augmentation</strong> is the process of <mark>generating new data points by transforming existing ones</mark>.</p>

  <ul>
    <li>For example, if a dataset has a lot of images of cars, data augmentation might generate new images by rotating them or changing their color. Then, it is used to <strong>train a neural network</strong></li>
  </ul>
</blockquote>

<p>Data augmentation may be used to <strong>reduce overfitting</strong>. Overfitting occurs when a model is too closely tailored to the training data and does not generalize well to new data. Data augmentation can be used to generate new training data points that are similar to the existing training data points, but are not identical copies.</p>

<ul>
  <li>This helps the model <strong>avoid overfitting</strong> and generalize better to new data.</li>
</ul>

<p>The general idea here is that we augment the training data by replacing each example pair with a <strong>set of pairs</strong></p>

\[(x_i, y_i) \to  \{(x_i^{*^b} , y_i)\}_{b=1}^B\]

<p>by, transformations including</p>

<ul>
  <li>e.g. rotation, reflection, translation, shearing, crop, color transformation, and added noise.</li>
</ul>

<h4 id="input-normalization">Input Normalization</h4>

<p>This is simply to normalize the input <strong>in the beginning</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127142128055.png" alt="image-20220127142128055" /></p>

<p>Then perform:</p>

\[\hat{x} = \frac{x- \mu}{\sigma}\]

<h4 id="batch-normalization">Batch Normalization</h4>

<p>Batch normalization basically <strong>standardizes the inputs</strong> to a layer <strong>for each mini-batch</strong>. You can think of this as doing normalization for each layer, for each batch</p>

<ul>
  <li>advantage: avoid exploding/vanishing gradients if the inputs are small!</li>
  <li>usually not only normalizing the input, but also for each <strong>layer</strong> over and over again.</li>
</ul>

<p>So basically, <strong>for input of next layer</strong>:</p>

\[Z = g(\text{BN}(WA))\]

<p>where $\text{BN}$ is doing batch normalization. In essence:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127195826143.png" alt="image-20220127195826143" style="zoom: 80%;" /></p>

<p>note that:</p>

<ul>
  <li>this means you <strong>backpropagation equation</strong>/derivatives needs also to include that term.</li>
</ul>

<h2 id="uncertainty-in-dnns">Uncertainty in DNNs</h2>

<p>Sometimes we also want to measure the <strong>confidence about the outputs</strong> of a neural network.</p>

<ul>
  <li>for instance, confidence in our learnt paramotor $\theta$</li>
</ul>

<p>In theory, we want ask the question: <strong>What is the distribution over weights $\theta$ given the data?</strong> (from which we know the confidence of our current learnt parameter)</p>

\[p(\theta |x,y) = \frac{p(y|x,\theta) p(\theta)}{p(y|x)}\]

<p>where $p(\theta)$ would be the <strong>prior</strong>, and $p(\theta\vert x,y)$ would be the <strong>posterior</strong> since we see the data.</p>

<ul>
  <li>this is not possible to compute since we don’t know them.</li>
</ul>

<p>In practice, we can roughly compute <strong>confidence of current prediction</strong> by “dropout masks”</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127135347957.png" alt="image-20220127135347957" style="zoom: 40%;" /></p>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127135432128.png" alt="image-20220127135432128" /></p>

<p>so we are not only <strong>outputting predictions</strong>, but also <strong>outputting confidence of our predictions</strong></p>

<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>

<p>The major aim of this section is to discuss <strong>models that solves ML problems with images</strong></p>

<p>A brief overview of what we will discuss:</p>

<ul>
  <li>CNN is basically a “preprocessing neural network” that replaces Linear part from $W^lA^{l-1}$ to <strong>convolution with kernel</strong> (which is also linear)</li>
  <li>problems with deeper CNN layers causes vanishing gradients, hence models such as Residual NN and DenseNet are introduced</li>
</ul>

<p>On a high level, we should know that treating images means our input vector would be <strong>large in size</strong>, with $n$ dimension (after flattening) means that, if we use vanilla model, we need $O(n n_{1})$ matrix for the first layer with $n$ neurons!</p>

<p>Then, CNN aims to</p>

<ul>
  <li>deal with this storage/computation problem by using <strong>sparse matrix (kernel)</strong>, which are essentially matrices with <mark>repeated elements</mark>.</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201170511507.png" alt="image-20220201170511507" style="zoom:80%;" /></p>

<p>since the output is size $3$, it means we basically have <strong>3 neurons</strong> essentially having <mark>the same weight</mark> ($k_1, k_2, k_3$). Therefore, we also call this <strong>sharing weights across space</strong>.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201142122782.png" alt="image-20220201142122782" style="zoom: 50%;" /></p>

<p>notice that in this case, our number of parameters to learn is $O(3)=O(1)$ is <strong>constant</strong>!</p>

<ul>
  <li>
    <p>another problem of CNN is to encode <strong>spatial and local information</strong>, which would be otherwise lost if we <strong>directly flatten it</strong> and pass it onto a normal NN.</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201133600948.png" alt="image-20220201133600948" style="zoom: 33%;" /></p>

    <p>where you will see the aim of kernels would be that they <strong>captures features such as edges/texture/objects</strong>, which obviously has spatial relationships in the image.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>The fact that weights learnt in CNN preprocessing part of the architecture are constant can be thought of as the <strong>result of the constraints we placed on those weights</strong>: we need them to be kernels, hence we need <em>symmetry, sparsity, and the particular output shape</em> as shown above.</p>
    </li>
    <li>
      <p>After using the kernel, CNN architecture then would add a bias and an activation, all of which would assemble the actions taken in <strong>one layer</strong>.</p>

      <ul>
        <li>essentially the linear $W^l A^{l-1}$ is replaced by convolution</li>
        <li>each filter in a layer is of effectively $O(1)$ in size, but we can learn <strong>multiple such filters</strong>. Finally it may look like this</li>
      </ul>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201171758990.png" alt="image-20220201171758990" /></p>

      <p>where notice that:</p>

      <ul>
        <li>the output after <strong>several such layers</strong> will be <strong>piped to a normal NN</strong>, for instance, for the final specific classification tasks.</li>
        <li>subsampling are basically techniques such as <strong>pooling</strong>, which will be covered later. The aim is to <strong>reduce the dimension (width $\times$ height)</strong>.</li>
        <li>To reduce the number of channels, you can use $f$ number of $1\times 1\times c$ filters, which can reduce to $f$ channels.
          <ul>
            <li>make sense since doing $1\times 1\times c$ convolution is telling how to sum the pixel on each channel into $1$ single pixel.</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="convolution">Convolution</h2>

<p>Basically it is nothing than doing:</p>

<ul>
  <li>elementwise multiplication between a patch of a matrix and a filter/kernel</li>
  <li>summing them up</li>
</ul>

<p>Therefore, convolutions in any dimension can be represented as a <strong>matrix vector multiplication</strong>:</p>

\[k * x = Kx_{\text{flatten}}\]

<p>where:</p>

<ul>
  <li>$K$ is the kernel, and $x_{\text{flatten}}$ is the <strong>flattened</strong> version of the image $x$.</li>
  <li>the exact shape of $K$ would be interesting. Think about how you would realize the above equation.</li>
</ul>

<h3 id="one-dimensional-convolution">One-Dimensional Convolution</h3>

<p>The idea is simple, if we are given a <strong>1D vector</strong> and a <strong>1D kernel</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201172613937.png" alt="image-20220201172613937" style="zoom:67%;" /></p>

<p>so essentially it is a <strong>locally weighted sum</strong>. To think about how we <strong>represent this in linear algrebra</strong>, consider that we have a $1 \times 5$ input with size $3$ kernel:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201172859269.png" alt="image-20220201172859269" /></p>

<p>verify that the above works.</p>

<ul>
  <li>notice that the output is of dimension $3$. This <strong>must be the case</strong> because there are only $3$ unique positions to place the size $3$ filter inside the size $5$ input vector.</li>
  <li>this matrix is also called Koeplitz matrix</li>
</ul>

<p>Therefore, we can reason this as:</p>

\[k * x = Kx = \sum_{i=1}^3 k_iS_i x\]

<p>where $S_i$ are the matrices where only “diagonal” entries are ones, otherwise zeros.</p>

<p>Now, one problem is that we noticed the <strong>output size is smaller</strong>, which can be bad in some cases. We can fix this by adding padding to the edges:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201173324819.png" alt="image-20220201173324819" /></p>

<p>however:</p>

<ul>
  <li>one problem with <strong>zero padding</strong> is that it introduces <strong>discontinuities at boundaries</strong></li>
  <li>another technique is to pad with <strong>reflection</strong>, i.e. replacing the top $0 \to x_1$, and bottom $0 \to x_5$.</li>
</ul>

<hr />

<p><em>For Example</em>: Stride with size 2</p>

<p>The above all assumed a stride with size 1. We can perform the task with stride 2 by doing</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174111406.png" alt="image-20220201174111406" /></p>

<p>this could be useful as the <strong>output size is decreased</strong> by a factor of $2$.</p>

<hr />

<p><em>For Example</em>: A Simple Single Conv Layer</p>

<p>A typical layer looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209143859413.png" alt="image-20220209143859413" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ol>
  <li>pass the 1D image vector to filter (<strong>replacing the linear part</strong> to $Kx$)
    <ul>
      <li>optionally you would then also add the <strong>bias</strong> to the output $Kx + b$</li>
      <li>notice that this linear operation has a <strong>very sparse matrix</strong>, $K$</li>
    </ul>
  </li>
  <li>shortened version of the vector then goes through <strong>activation</strong></li>
</ol>

<p>This particular setup in the end can detect <strong>any block of lonely $1$</strong> in the input.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The fact that we are applying a kernel <strong>everywhere the same</strong> is so that it preserves the property that images are <strong>translational invariant</strong>.</p>
</blockquote>

<hr />

<p><em>Other Filters</em></p>

<p>Sharpening:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174625087.png" alt="image-20220201174625087" style="zoom:50%;" /></p>

<p>more filters are omitted.</p>

<h3 id="multi-dimensional-convolution">Multi-Dimensional Convolution</h3>

<p>Since our images are usually <strong>2D</strong> if grey scale, so we extend the convolution to a 2D kernel.</p>

<ul>
  <li>the pattern you will see is easily generalizable to 3D inputs as well.</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174800911.png" alt="image-20220201174800911" style="zoom: 67%;" /></p>

<p>Then, if we want to <strong>add paddings</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174906311.png" alt="image-20220201174906311" style="zoom:67%;" /></p>

<p>But more importantly, we can <strong>put this in a matrix vector multiplication as well</strong>:</p>

<ul>
  <li>flattening 2D matrix to $[x_{11}, x_{12}, …, x_{nm}]^T$.</li>
  <li>the shape of kernel would be <strong>repeatedly assembling 1D filters</strong></li>
</ul>

<p>Consider the following operatoin:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175143906.png" alt="image-20220201175143906" style="zoom:67%;" /></p>

<p>Can be done by:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175349503.png" alt="image-20220201175349503" /></p>

<p>which is basically:</p>

<ul>
  <li>the lowest diagonal is the <strong>1D Toeplitz matrix for the first row of $k$</strong></li>
  <li>the second lowest diagonal is the <strong>1D Toeplitz matrix for the second row of $k$</strong></li>
  <li>etc.</li>
  <li>finally, the <strong>output is a vector</strong>, which can be interpreted as a flattened 2D image</li>
</ul>

<blockquote>
  <p>Therefore, convolution with 3D images using 3D kernels, basically is equivalent of matrix-vector multiplication with:</p>

  <ul>
    <li>flattened image to 1D</li>
    <li>repeatedly assembling <strong>2D Toeplitz matrix</strong> for the “$i$-th place” to form a 2D matrix.</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em></p>

<p>To find the lonely one:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201135357568.png" alt="image-20220201135357568" style="zoom:33%;" /></p>

<hr />

<h4 id="three-dimensional-convolution">Three Dimensional Convolution</h4>

<p>The technique of expanding 2D Toeplitz matrix for 3D convolution basically does the following for convolution:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182111825.png" alt="image-20220201182111825" style="zoom:80%;" /></p>

<p>which basically <strong>outputs a single 2D matrix</strong>.</p>

<ul>
  <li>makes sense that the <strong>number of channels</strong> in both kernel and input lines up, as in the end we just do a element-wise multiplication and sum up.</li>
</ul>

<p>However, <strong>another way</strong> would be to do <mark>two dimensional convolution on each channel</mark></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182357125.png" alt="image-20220201182357125" style="zoom:80%;" /></p>

<p>notice that:</p>

<ul>
  <li>this means we would have 3 filters (may be the same), each is a <strong>2D matrix/kernel/filter</strong></li>
  <li>the number of channels for both kernel and their respective input is $1$.</li>
  <li><strong>outputs 3 channels instead of $1$</strong>, as compared to the previous case</li>
</ul>

<h3 id="properties-of-convolution">Properties of Convolution</h3>

<p>There are several nice properties of convolution that are handy for <strong>optimizing computation complexity</strong>.</p>

<p>First, the most obvious ones are due to <strong>convolution</strong> are essentially matrix/vector multiplication as they are from linear algebra</p>

<ul>
  <li><strong>Commutative</strong>: $f*g = g * f$</li>
  <li><strong>Associative</strong>: $f<em>(g</em>h) = (f*g) * h$</li>
  <li><strong>Distributive</strong>: $f*(g + h) = f * g + f * h$</li>
  <li><strong>Differentiation</strong>: $\frac{d}{dx} (f * g) = \frac{df}{dx}* g = f * \frac{dg}{dx}$</li>
</ul>

<h4 id="separable-kernels">Separable Kernels</h4>

<p>Some kernels would be separable like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201135511859.png" alt="image-20220201135511859" style="zoom: 50%;" /></p>

<p>Then, we can use the <strong>property that</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175857580.png" alt="image-20220201175857580" style="zoom: 80%;" /></p>

<p>where this is <strong>very useful because</strong>, if the image is size $n \times n$, and separable kernel $k \times k$</p>

<ul>
  <li>directly convovling needs $O(n^2 k^2)$, since each of the $\approx n^2$ output pixel needs $k^2$ computation.</li>
  <li>if we do it with <strong>two simpler convolutions</strong>, then $O(2n^2k)=O(n^2k)$ which is better</li>
</ul>

<h4 id="composition">Composition</h4>

<p>Since we know convolutions is basically matrix-vector multiplication: Repeated convolutions with a small kernel are equivalent to a single convolution with a large kernel</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201180550990.png" alt="image-20220201180550990" style="zoom:80%;" /></p>

<p>where this is useful because it is more efficient.</p>

<h2 id="convolutional-layers">Convolutional Layers</h2>

<p>Now, we discuss what happens in convolutional layers in a NN such as the following</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201171758990.png" alt="image-20220201171758990" /></p>

<p>Notice that we know:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182741514.png" alt="image-20220201182741514" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>convolution of $n \times n \times 3$ with a <strong>single kernel $k \times k \times 3$</strong> produces $n \times n$ (if we have padding)</p>
  </li>
  <li>
    <p>if prepare $4$ different $k \times k \times 3$ kernel and we do this <mark>separately for $4$ times</mark>, we get $n \times n \times 4$ output</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182937454.png" alt="image-20220201182937454" style="zoom: 67%;" /></p>
  </li>
</ul>

<p>Therefore, since in general <strong>a single layer would have many different filters</strong>, if we have $f$ filters, we would produce $n \times n \times f$ as our output size.</p>

<ul>
  <li>however, before we put this through activation, notice that having $n \times n \times f$ is pretty large. (in practice the performance gain is not too huge, so it doesn’t matter if we put it before or after activation)</li>
</ul>

<p>So, before activation, we would use <strong>pooling techniques</strong> to reduce the dimension.</p>

<h3 id="pooling">Pooling</h3>

<p>Pooling is an operation that reduces the dimensionality of the input. Some simple and common ones are</p>

<blockquote>
  <p><strong>Max pooling</strong> takes the maximum over image <mark>patches</mark>.</p>

  <ul>
    <li>
      <p>for example over $2 \times 2$ grids of neighboring pixels $m =\max{x_1, x_2, x_3, x_4}$, hence <strong>reducing dimensionality in half</strong> in each spatial dimension as shown in Figure 5.18.</p>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201183419024.png" alt="image-20220201183419024" style="zoom:67%;" /></p>
    </li>
    <li>
      <p>notice that this <strong>does not reduce the number of channels!</strong></p>
    </li>
  </ul>
</blockquote>

<p>To reduce the number of channels, it is essentially saying that <strong>how do we want to sum pixels in different channel</strong>? Therefore, it makes sense that we can use a <strong>one-dimensional convolution</strong> to solve this</p>

<blockquote>
  <p><strong>One dimensional convolution</strong> with $f$ filters also allows reducing the number of channels to $f$ as shown in Figure 5.19.</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201183701494.png" alt="image-20220201183701494" style="zoom:80%;" /></p>

  <ul>
    <li>which makes sense as if $f=1$, essentially you summed over all channels, collapsing all channels to $1$ channel.</li>
  </ul>
</blockquote>

<h3 id="simple-convolutional-layer">Simple Convolutional Layer</h3>

<p>Putting everything above together, a typical convolutional layer involves <strong>three operations</strong>:</p>

<ul>
  <li>convolution with kernel (linear)</li>
  <li>pooling</li>
  <li>activation (nonlinear)</li>
</ul>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201184433823.png" alt="image-20220201184433823" style="zoom: 80%;" /></p>

<p>where, In this example, the input is a $28 \times 28$ grayscale image and the output is one of ten classes, such as the digits $0-9$</p>

<ul>
  <li>The first convolutional layer consists of $32$ filters, such as $5 \times 5$ filters, which are applied to the image with padding which yields a $28 \times 28 \times 32$ volume.</li>
  <li>Next, a non-linear function, such as the ReLU, is applied pointwise to each element in the volume</li>
  <li>The first convolution layer of the network shown above is followed by a $2 \times 2$ max pooling operation which reduces dimensionality in half in each spatial dimension, to $14 \times 14 \times 32$.</li>
  <li>then, go from $14 \times 14 \times 32$ to $14 \times 14 \times 64$, you would have <strong>64 filters</strong> of size $5 \times 5 \times 32$, for example</li>
</ul>

<h2 id="architectures">Architectures</h2>

<p>Now we talked about some of the <strong>modern architectures</strong> that builds up on the basic CNN we discussed before.</p>

<ul>
  <li>in fact, now as <strong>Vision Transformers</strong> are out, processing with images have now been mostly done using that as Transformers itself is quite a generic model</li>
</ul>

<h3 id="cnn">CNN</h3>

<p>A basic example of CNN is shown in this example</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203202818491.png" alt="image-20220203202818491" style="zoom: 67%;" /></p>

<ul>
  <li>A deeper network of <strong>eight layers may resemble the cortical visual pathways</strong> in the brain (Cichy, Khosla, Pantazis, Torralba &amp; Oliva 2016).</li>
  <li>note that we are doing activation <strong>then pooling here</strong>
    <ul>
      <li>max-pooling and monotonely increasing non-linearities commute. This means that $\text{MaxPool(Relu(x)) = Relu(MaxPool(x))}$ for any input.</li>
      <li>So the result is the same in that case. Technically it is better to first subsample through max-pooling and then apply the non-linearity (if it is costly, such as the sigmoid). <strong>In practice it is often done the other way round</strong> - it doesn’t seem to change much in performance.</li>
    </ul>
  </li>
  <li>the second Convolution Layer comes from applying 64 filters of size $k \times k \times 32$. Usually this can be <strong>abbreviated</strong> to say applying $k \times k$ dimension filters.</li>
  <li>Many early implementations of CNN architectures were <strong>handcrafted for specific image classification tasks</strong>. These include
    <ul>
      <li>LeNet (LeCun, Kavukcuoglu &amp; Farabet 2010),</li>
      <li>AlexNet (Krizhevsky, Sutskever &amp; Hinton 2012)</li>
      <li>VGGNet (Simonyan &amp; Zisserman 2014),</li>
      <li>GoogLeNet (Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich et al. 2015)</li>
      <li>Inception (Szegedy, Vanhoucke, Io↵e, Shlens &amp; Wojna 2016).</li>
    </ul>
  </li>
</ul>

<p>(but now, vision transformers are of big focus due to its <strong>generality</strong>)</p>

<h3 id="resnet">ResNet</h3>

<p>The <strong>deep residual neural network</strong> (ResNet) architecture (He, Zhang, Ren &amp; Sun 2016a), (He, Zhang, Ren &amp; Sun 2016b), introduced <mark>skip connections</mark> between consecutive layers as shown below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Architecture</th>
      <th style="text-align: center">Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201190948331.png" alt="image-20220201190948331" style="zoom:80%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191036841.png" alt="image-20220201191036841" style="zoom:80%;" /></td>
    </tr>
  </tbody>
</table>

<p>The idea is simple, the <strong>pre-activation of a layer</strong> $z^l$ now has a <strong>residual term from previous layer</strong></p>

\[z^{l+1} = f(W^l,a^l)+a^l;\quad a^{l+1} = g(z^{l+1})\]

<p>instead of $z^{l+1} = f(W^l,a^l)$.</p>

<ul>
  <li>adding those skip connections/residual terms allow training deeper neural networks by <strong>avoiding vanishing gradients</strong>. The ResNet architecture enables training very deep neural networks with hundreds of layers.</li>
  <li>Adding a new layer to a neural network with a skip connection <strong>does not reduce its representation power</strong>. Adding a residual layer results in the network being able to represent all the functions that the network was able to represent before adding the layer plus additional functions, thus <strong>increasing the space of functions</strong>.</li>
</ul>

<p>For instance, a three layer network composition <strong>originally</strong> would look like</p>

\[F(x)=f(f(f(x)))\]

<p>Now it becomes, if each layer has a residual:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191454941.png" alt="image-20220201191454941" /></p>

<h3 id="densenet">DenseNet</h3>

<p>A DenseNet (Huang, Liu, van der Maaten &amp; Weinberger 2017) layer <strong>concatenates</strong> the input $x$ and output $f(x)$ of each layer to form the next layer $[f(x), x]$.</p>

<ul>
  <li>because it is concatenating it, the input in the first layer also <strong>directly appears in input to any further layers</strong></li>
  <li>in the ResNet, input in the first layer <strong>indirectly appears</strong> as they are absorbed in, such as  $f(f(x)+x)$</li>
</ul>

<p>Therefore, graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191751076.png" alt="image-20220201191751076" style="zoom:80%;" /></p>

<p>And the formula composition for three layers look like</p>

\[F(x) =  f(f([f(x),x]),[f(x),x]), f([f(x),x]),[f(x),x]\]

<h2 id="understanding-cnns">Understanding CNNs</h2>

<p>Consider the model ImageNet, which has the following architecture</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192419470.png" alt="image-20220201192419470" style="zoom:80%;" /></p>

<p>When trained on a large dataset, since we also <strong>gradient descent to learn kernel/filters</strong> in CNN, we can look at the <strong>learnt kernels</strong> for different layers.</p>

<ul>
  <li>
    <p>In the end the kernel is just a <strong>matrix with some constraints</strong></p>
  </li>
  <li>
    <p>therefore, we can impose those constraints only and let <strong>back propagation</strong> to learn those weights</p>
  </li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192533577.png" alt="image-20220201192533577" style="zoom: 67%;" /></p>

<p>which, interestingly, coincides with many of the handcrafted ones we had before</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201140058415.png" alt="image-20220201140058415" style="zoom:33%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>now we learn filters, but we need to specify the architecture</li>
    <li>this is now superseded with vision transformer, which learns <strong>both the architecture</strong> and the kernel</li>
  </ul>
</blockquote>

<p>However, we are interested in knowing <mark>what patterns do each layer learn</mark>. How do we do that?</p>

<h3 id="input-maximizing-activation">Input Maximizing Activation</h3>

<p>Consider transferring the above to an <strong>optimization problem</strong>: given trained network with weights $W$, <mark>find input $x$ which maximizes activation</mark>.</p>

\[\arg\max_x a^l_{i}(W, x)\]

<p>which we can find by <strong>gradient ascent</strong>.</p>

<ul>
  <li>
    <p>e.g. given some kernel, doing gradient ascent gives:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192921549.png" alt="image-20220201192921549" style="zoom: 67%;" /></p>

    <p>where the first steps are basically initializing with <strong>random noise</strong></p>
  </li>
</ul>

<p>Applying this technique to multiple layers, and we find that</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193034483.png" alt="image-20220201193034483" /></p>

<p>so basically:</p>

<ul>
  <li>first layers learn the edges</li>
  <li>then textures</li>
  <li>then objects</li>
</ul>

<p>Alternatively, you can also use this technique to find out <strong>what patch of images</strong> this kernel is bad at:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193141397.png" alt="image-20220201193141397" /></p>

<h3 id="transfer-learning">Transfer Learning</h3>

<p>The fact that those CNN learn fundamental concepts such as edges and textures means we can do <strong>transfer learning</strong></p>

<p>Task 1: learn to recognize animals given many (10M) examples which are not horses</p>

<p>Task 2: learn to recognize horses given a few (100) examples</p>

<ul>
  <li>Keep layers from task 1, re-train on <strong>last layer</strong></li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193339131.png" alt="image-20220201193339131" style="zoom:50%;" /></p>

<h1 id="sequence-models">Sequence Models</h1>

<p>Applications using sequence models include machine translation, protein structure prediction, DNA sequence analysis, and etc. All of which needs some <strong>representation that remembers previous data/state</strong>.</p>

<p>An overview of what will be discussed</p>

<ul>
  <li><strong>Sequence models</strong></li>
  <li><strong>Recurrent neural networks (RNNs)</strong>: when unrolled, basically modelling Finite State Machine</li>
  <li><strong>Backpropagation through time</strong>: Updating the shared/same weights across states</li>
  <li><strong>GRU and LSTM</strong>: fixing vanishing/exploding gradient in RNN</li>
  <li>Word embeddings, beam search, encoder-decoder attention</li>
  <li>Transformers</li>
</ul>

<p>In general, to reduce computational complexity through this deep network, <strong>weights are shared/same across time</strong>. i.e. shared weights when unrolled.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203131557649.png" alt="image-20220203131557649" style="zoom: 50%;" /></p>

<p>Another interesting thing to know is the <strong>timeline</strong> of model development w.r.t. sequence models:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203131802058.png" alt="image-20220203131802058" style="zoom:33%;" /></p>

<p>where:</p>

<ul>
  <li>for simple deep models like RNN, there were problems of vanishing gradients. So LSTM and GRU are essentially variants to solve this problem in RNN.</li>
</ul>

<h2 id="natural-language-models">Natural Language Models</h2>

<p>Representing language requires a natural language model which may be a <strong>probability distribution over strings</strong></p>

<p>Some basic methods for <strong>treating input text data</strong> include</p>

<ul>
  <li>Bag of words + Classifier</li>
  <li>Feature Vector + Classifier</li>
  <li>Markov Model</li>
</ul>

<h3 id="bag-of-words">Bag of Words</h3>

<blockquote>
  <p><strong>Bag of words</strong> are text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but <strong>keeping multiplicity</strong>/count.</p>
</blockquote>

<blockquote>
  <p><strong>Term Frequency</strong>: In a bag of words we count how prevalent each term $x$ is in a single document $d$ which is the term frequency $TF(x, d)$.</p>

  <ul>
    <li>we assume words are commonly normalized to lowercase, stemmed by removing their suffixes, and common stopwords (such as a, an, the, etc.) are removed.</li>
  </ul>
</blockquote>

<p>However, sometimes we also want to get more weight/focus for words that are rare. Therefore, we may want to consider the entropy of the word using <strong>inverse document frequency</strong>:</p>

\[IDF(x) = 1 + \log( \frac{ \text{total number of documents}}{ \text{number of documents containing $x$} })\]

<blockquote>
  <p><strong>TF-IDF</strong>: Then we can <em>boost</em> the frequency of rare words by considering the product of term frequency and inverse document frequency:</p>

\[TFIDF(x,d) = TF(x,d) \times IDF(x)\]

</blockquote>

<p>Finally, we use some <strong>classifier models</strong>, e.g. ML or DL models for classification using the mentioned above features.</p>

<p><strong>Problem</strong></p>

<ul>
  <li>
    <p>however, such a representation does not preserve order information</p>

\[\text{Alice sent a message to Bob} \quad \text{v.s.} \quad \text{Bob sent a message to Alice}\]

    <p>would have the same score/vector representation</p>
  </li>
</ul>

<h3 id="feature-vector">Feature Vector</h3>

<p>In contrast to a bag of words, using a feature vector to represent a sentence <strong>preserves order information</strong>. However, the problem is that sentences that have the <strong>same meaning</strong> could have a different word order:</p>

\[\text{Alice sent a message on Sunday} \quad \text{v.s.} \quad \text{On Sunday Alice sent a message}\]

<p>would need <strong>different feature vectors</strong> even if same information.</p>

<ul>
  <li>hence, there will be a lot of redundancy</li>
</ul>

<h3 id="n-gram-model">N-Gram Model</h3>

<p>Here we basically model <strong>probability distribution of n-grams</strong></p>

<ul>
  <li>
    <p>A Markov model is a 2-gram or bi-gram model where:</p>

\[P(x_ n | x_1 ,....,x_{n-1}) \approx p(x_n | x_{n-1})\]
  </li>
  <li>
    <p>but we want <strong>long term dependencies</strong></p>

    <ul>
      <li>using large/high-order n-gram model requires <strong>large corpus</strong></li>
      <li>hence not effective at capturing long term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="rnn">RNN</h2>

<blockquote>
  <p><strong>Recurrent Neural Network</strong> both maintain word order and model long term dependencies by sharing parameters across time.</p>

  <ul>
    <li>also allows for inputs and outputs of different length</li>
    <li>model both forward and backward sequence dependencies
      <ul>
        <li>using bidirectional RNN</li>
      </ul>
    </li>
    <li>but generally <em>difficult to train</em>: backpropagation causes gradients to explode/vanish
      <ul>
        <li>hence need LSTM or GRU</li>
      </ul>
    </li>
  </ul>

  <p>Its behavior basically is:</p>

  <ol>
    <li>process the input sequence one word at a time</li>
    <li>attempting to predict the next word from the current word and the previous hidden state $h_{t-1}$.
      <ul>
        <li>RNNs don’t have the limited context problem that n-gram models have, since the hidden state can in principle <strong>represent information about all of the preceding words</strong> all the way back to the beginning of the sequence</li>
      </ul>
    </li>
    <li>output $y_t=f(Vh_t)$ at time $t$ where $V$ will be shared and $f$ is an activation function of your choice.
      <ul>
        <li>exactly when it outputs can be tuned/changed in algorithm, which leads to different architectures such as many-to-one (outputting $y$ only at the last time step)</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>The basic foundation of a RNN is a <strong>finite state machine</strong></p>

<h3 id="state-machine">State Machine</h3>

<p>In a state machine, we have:</p>

<ul>
  <li>$S$: possible states</li>
  <li>$X$: possible inputs</li>
  <li>$f: S \times X \to S$: <strong>transitions</strong></li>
  <li>$Y$: possible outputs</li>
  <li>
    <p>$g: S \to Y$: mapping from state to outputs</p>
  </li>
  <li>$S_0$ initial state</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203205139875.png" alt="image-20220203205139875" style="zoom:50%;" /></p>

<p>The key idea is that <strong>a new state</strong> comes from <strong>both the previous state and an input</strong></p>

\[s_t = f(s_{t-1},x_t)\]

<p>This idea will be the <mark>same in RNN</mark>.</p>

<p>Then, for a <strong>sequence of inputs $x_t$</strong>, the output $y_t$ would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203205331091.png" alt="image-20220203205331091" style="zoom:67%;" /></p>

<p>Essentially depends on all previous state/input.</p>

<ul>
  <li><strong>Recurrent neural networks are state machines</strong> with specific definitions of transition function $f$ and mapping $g$, in which the states, inputs, and outputs are vectors.</li>
</ul>

<h3 id="recurrent-neural-network">Recurrent Neural Network</h3>

<p>Given some sequence of data: $x_1, …, x_n$, we consider some <strong>hidden state $h_1, …, h_n$</strong> that will be used to form <strong>output</strong> $y_1, …, y_t$. This is done by <strong>sharing weights $U,W,V$</strong> across time:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></p>

<p>where essentially we see that RNN is modelling <strong>state transitions and outputting stuff</strong>, which is like a FSM</p>

<ul>
  <li>
    <p>hidden state $h_t$ at time $t$ is computed by:</p>

\[h_t = g(Wh_{t-1} + Ux_t)\]

    <p>which is a <strong>nonlinear function</strong> on <strong>previous state and current input</strong>. ($x_t, h_{t-1}$ would be vectors)</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203132556812.png" alt="image-20220203132556812" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>the output then is:</p>

\[y_t = V h_t\]

    <p>which <strong>can be followed by an activation</strong>, in that case we say:</p>

\[y_t = f(z_t) = f(Vh_t)\]

    <p>where $f$ would be a nonlinear function such as sigmoid.</p>
  </li>
  <li>notice that all weights $V,W,U$ <strong>are the same</strong>!</li>
  <li>this means that we only need to update it once for each matrix!</li>
</ul>

<p>Additionally, since to get to the new hidden state $h_t$ we need $Wh_{t-1} + Ux_t$, this can be computed in one shot by:</p>

\[[W,U] \begin{bmatrix}
h_{t-1}\\
x_t
\end{bmatrix} = Wh_{t-1} + Ux_t\]

<p>Then the algorithm looks as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207183223337.png" alt="image-20220207183223337" style="zoom:67%;" /></p>

<p>which essentially is when we have one layer.</p>

<ul>
  <li>As with feedforward networks (NN), we’ll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks.</li>
  <li>more details on backpropagation through time is shown in later sections.</li>
</ul>

<h3 id="rnn-architecture">RNN Architecture</h3>

<p>Some common architectures used in RNN can be visualized as:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">One to Many</th>
      <th style="text-align: center">Many to One</th>
      <th style="text-align: center">Many to Many</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133619748.png" alt="image-20220203133619748" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203212927731.png" alt="image-20220203212927731" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133652920.png" alt="image-20220203133652920" /></td>
    </tr>
  </tbody>
</table>

<p>where in the end you need to <strong>specify size of output</strong>. Neural networks are always trained with fixed length of input and output.</p>

<ul>
  <li><strong>one to many</strong>: e.g. image to caption</li>
  <li><strong>many to one</strong>: caption to image/stock price prediction/sentiment classification</li>
  <li><strong>many to many</strong>: machine translation, video action classification
    <ul>
      <li>e.g. this can be done by “hardcoding” in the algorithm such that you ask the model to only output $y_t=f(Vh_t)$ if $t \in [T-2,T-1,T]$, for example.</li>
    </ul>
  </li>
</ul>

<p>There are many choices of architectures you can use:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133936167.png" alt="image-20220203133936167" style="zoom:33%;" /></p>

<p>which would be useful for <strong>audio to text</strong></p>

<h3 id="loss-function">Loss Function</h3>

<p>To complete our definition of the RNN architecture requires incorporating a loss function, with which we can <strong>improve our model by gradient descending on the shared weigths</strong>.</p>

<p>The simple idea is that <strong>each output can be compared to the label</strong>, so we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>we have covered how states are computed, but the others:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" /></p>

    <p>so essentially $\hat{y} = \text{softmax}(Vh_t)$</p>
  </li>
</ul>

<p>Then, the <strong>total loss is just the sum over the characters</strong>:</p>

\[\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i) = \sum_{t=1}^{l_i}\mathcal{L}_{\text{character}}(\hat{y}_t^i, y_t^i)\]

<p>note that the $i$th input $x^i$ is a <strong>sequence of characters</strong>.</p>

<ul>
  <li>
    <p>the sequence length $l_i$ has <strong>nothing</strong> to do with dimension of the <strong>$i$th input $x_t^i$ or $y_t^i$ at time $t$</strong>, which are basically feature/output vectors for <strong>each character</strong> along the sequence of length $l_i$.</p>
  </li>
  <li>
    <p>then, since we could have $m$ sequences/sentences in the entire dataset, we would have:</p>

\[\mathcal{L}_{\text{total}}(\hat{y}_t, y_t) = \sum_{i=1}^{m}\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i)\]

    <p>this means that together it will be a double sum.</p>
  </li>
</ul>

<h3 id="deep-rnn">Deep RNN</h3>

<p>We can <strong>stack multiple hidden layers and connecting them</strong> as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203220235023.png" alt="image-20220203220235023" style="zoom: 50%;" /></p>

<p>where essentially:</p>

<ul>
  <li>the output of the lowest layer is treated as the “current state/input” of the second last layer.</li>
  <li>
    <p>the weights $U^l,W^l$ will be <strong>shared within each layer $l$</strong>, but there is still a <strong>single $V$</strong> for output</p>
  </li>
  <li>
    <p>therefore, the new state transition equation at layer $l$ becomes:</p>

\[h_t^l = g(W^lh_{t-1}^l + U^lh_{t}^{l-1})\]

    <p>where $h_{t}^{l-1}$ is the state of previous layer at that time $t$, treated as an “input/current state”.</p>
  </li>
</ul>

<p>Additionally, we can build <strong>dependencies by</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203220609110.png" alt="image-20220203220609110" style="zoom:67%;" /></p>

<p>where then we need to specify a weight for connection  $o_{t-1} \to h_t$, which is certainly doable.</p>

<p>Last but not least, an important variant is the <strong>bidirectional RNN</strong>, which you shall see in the next section.</p>

<h3 id="bidirectional-rnn">Bidirectional RNN</h3>

<p>Basically, the idea is that we not only want to remember <strong>forward information</strong>, but also <strong>backward information</strong> (context in both direction). Therefore, we consider each state $h_t$ being <strong>duplicated into two</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 50%;" /></p>

<p>such that</p>

<ul>
  <li>
    <p>then you have two weights $W, \bar{W}$, where the former is used to flow between $h_{t-1} \to h_t$, the latter $\bar{h}_{t+1} \to \bar{h}_t$:</p>

\[\begin{align*}
h_t &amp;= g(Wh_{t-1} + Ux_t)\\
\bar{h}_t &amp;= g(\bar{W}\bar{h}_{t+1} + Ux_t)\
\end{align*}\]
  </li>
  <li>
    <p>then output basically depends on <strong>both state information</strong> by concatenating them</p>

\[o_t = V\begin{bmatrix}
h_t\\
\bar{h}_t
\end{bmatrix}\]
  </li>
</ul>

<p>Then we can stack those as well</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203134545172.png" alt="image-20220203134545172" style="zoom: 80%;" /></p>

<h3 id="backpropagation-through-time">Backpropagation Through Time</h3>

<p>Having defined the RNN architectures and loss function, our goal is to train the RNN by <strong>finding derivatives and descending.</strong> Essentially we will use <strong>backpropagation</strong> again as it is a deep network.</p>

<p>Then general form of the gradient of a sequence loss $\mathcal{L}_{ \text{sequence}}$ on some parameter $\theta$ is:</p>

\[\frac{d\mathcal{L}_{ \text{sequence}}(\hat{y} , y)}{d\theta} = \sum_{t=1}^{l_i}\frac{d\mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{d\theta} = \sum_{t=1}^{l_i}\sum_{t}\frac{\partial \mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{\partial h_t}\frac{\partial h_t}{\partial \theta}\]

<p>you will see that all derivatives from then on <strong>will have dependence on $t$</strong>, which is why we call it backpropagation through time.</p>

<p>Recall that in a simple RNN:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Label</th>
      <th style="text-align: center">Diagram</th>
      <th style="text-align: center">Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Then first computing the gradient for $V$ at time $t=3$</p>

\[\frac{\partial L_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial o_3}\frac{\partial o_3}{\partial V}\]

<p>notice that it <mark>does not depend on data from previous time</mark>, since $o_t = Vh_t$ only depends on current time.</p>

<ul>
  <li>therefore, updating $V$ at each iteration at time $t$ is simple</li>
</ul>

<p>Now, if we consider updating $W$:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W}\]

<p>but then we know that $h_3  = f(Wh_2 + Ux_3)$ which <strong>depends on previous time</strong>! Hence in this case we would have:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \frac{\partial h_3}{\partial h_i} \frac{\partial h_i}{\partial W}\]

<p>but then, for instance:</p>

\[\frac{\partial h_3}{\partial h_1} = \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1}\]

<p>We can generalize the above even further to:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \left( \prod_{j=i+1}^3 \frac{\partial h_3}{\partial h_{j-1}} \right) \frac{\partial h_i}{\partial W}\]

<p>if we take the activation function to be $\tanh$, then the part in parenthesis above is basically $\prod W^T \, \text{diag}(\tanh’(h_{t-1}))$, which means that as $t » 3$, we would have backpropagation <strong>rasing $W^T$ to a high power</strong>.</p>

<ul>
  <li>
    <p>hence, RNN could suffer vanishing/explode gradients if the eigenvalues are less than one or greater than one, respectively</p>
  </li>
  <li>
    <p>graphically, where $E = L$ that we used above:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203135108631.png" alt="image-20220203135108631" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>remember that we <strong>only update once in the end for $W$</strong>, because we only have <strong>one shared $W$</strong>! (same for $U,V$ if we are using a single layer RNN)</p>
  </li>
</ul>

<p>Finally, the algorithm is then summarized here:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203230211497.png" alt="image-20220203230211497" style="zoom:50%;" /></p>

<p>note that this is only the backpropagation part (second pass). The full algorithm of RNN would have a two-pass algorithm</p>

<ol>
  <li>In the first pass, we perform forward inference, computing $h_t$ , $y_t$ , accumulating the loss at each step in time, <strong>saving the value of the hidden layer</strong> at each step for use at the next time step.</li>
  <li>In the second phase, we process the sequence in reverse, computing the required <strong>gradients</strong> as we go, computing and saving the error term for use in the hidden layer for each step <strong>backward</strong> in time</li>
</ol>

<h3 id="rnn-as-language-models">RNN as Language Models</h3>

<p>This is essentially an example of applying RNN in real life. This idea of how you <strong>treat input/output</strong> as probabilities is used in other models introduced next as well.</p>

<p>Now, consider the task of <strong>predicting next word</strong> again. Here we have:</p>

<ul>
  <li>input as <strong>text/sentences</strong></li>
  <li>output <strong>probability</strong> that each word $w_i \in V$ will be the next word</li>
</ul>

<p>Then, we can have the following in each RNN layer:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207185449002.png" alt="image-20220207185449002" /></p>

<p>where:</p>

<ul>
  <li>
    <p>$E$ is an <strong>embedding matrix</strong>, so $E x_t$ is embedding of current word $t$.</p>
  </li>
  <li>
    <p>the probability that a particular word $w_i$ in the vocabulary is the next word is represented by $y_t[i]$, the $i$th component of $y_t$$.</p>

\[y_t[i] = P(w_{w+1}=w_i| w_1, ..., w_t)\]

    <p>where you can imagine $w_i \in V$ can be indexed easily</p>
  </li>
</ul>

<p>Then, while <strong>training</strong>, you would have a correct distribution $y^<em>_t$, which is essentially a <strong>one-hot encoded vector</strong> for the correct word $w^</em>_t$. Since this can be treated as a probability distribution, and our prediction $y_t$ is also a probability distribution:</p>

\[L_\text{Cross Entropy} = \sum_t L_{\text{CE}}(y_t, y^*_t) = \sum_t - \log y_t[w^*_{t+1}]\]

<p>where essentially $y_t[w^<em>_{t+1}]$ is the probability that our model predicts $w^</em>_{t+1}$ to be the next word correctly.</p>

<p>Graphically, this is what happens when we are training:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207190334237.png" alt="image-20220207190334237" style="zoom: 80%;" /></p>

<p>where it is important to note that the loss is on the <strong>probability for next word</strong>.</p>

<h2 id="gated-recurrent-unit">Gated Recurrent Unit</h2>

<p>One solution for vanishing/exploding gradient would be to use <strong>GRU</strong> instead of an RNN architecture for transition.</p>

<p>In RNN, we had the following structure</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Structure</th>
      <th style="text-align: center">Encapsulation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203231239039.png" alt="image-20220203231239039" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>But now, what GRU does is essentially doing <strong>more complicated thing for transition</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203135552047.png" alt="image-20220203135552047" style="zoom:33%;" /></p>

<p>where:</p>

<ul>
  <li>there are many version of GRU circuits, many of which perform equally well on certain dataset</li>
  <li>the final best design was essentially discovered using a grid search over all possible gates. So there is kind of no theoretical reason why.</li>
</ul>

<p>Schematically, GRU does the following:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">GRU Schematic</th>
      <th style="text-align: center">Equations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203232453595.png" alt="image-20220203232453595" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203231843814.png" alt="image-20220203231843814" /></td>
    </tr>
  </tbody>
</table>

<p>where it emphasized that we need to be skilled at <strong>translating between a graphical representation to equations</strong></p>

<ul>
  <li>the shared weights are now $W_r, W_z, W$ and $U_r, U_z,U$ (and of course $V$ for output)</li>
  <li>the main changes are that we <strong>added three more gates/components</strong> before deciding what is $h_t$</li>
  <li>it is also important to notice the <strong>plus operation</strong> as the last step for state transition. This plus instead of multiply essentially solves the exploding gradient problem</li>
  <li><strong>inputs</strong> of functions/state have arrows pointing in</li>
  <li><strong>weights</strong> are labelled on the arrow</li>
  <li><strong>nonlinear functions</strong> are not shown but applied <strong>if an operator is not on the graph</strong>
    <ul>
      <li>e.g. $h_t$ does not have a nonlinear function because it is specified we have a $+$ operation</li>
    </ul>
  </li>
  <li>the rest of the architecture is the same as RNN</li>
</ul>

<hr />

<p><em>An analogy of the input/state/gates</em></p>

<p>Consider the example:</p>

<ul>
  <li>state $h_{t-1}$ is the cloth we wear yesterday</li>
  <li>$x_t$ is the weather/input on day $t$/today</li>
  <li>$\bar{h}_t$ is the candidate clothes we <em>prepared/predicted</em> to wear</li>
  <li>$h_t$ is the actual clothes we wear on day $t$/today</li>
</ul>

<p>Then, essentially those additional gates (the update and reset gates) determine <mark>to what extent</mark> we <mark>take into account these factors</mark>:</p>

<ul>
  <li>do we <strong>ignore</strong> the weather $x_t$ completely,</li>
  <li>do we <strong>forget</strong> what we wore yesterday $h_{t-1}$</li>
  <li>and do we take into account our candidate clothes we prepared $\bar{h}_t$, and to <strong>which extent.</strong></li>
</ul>

<p>In short, the effect of those can be overviewed as below:</p>

<h3 id="update-gate">Update Gate</h3>

<p>The update gate basically does the following:</p>

\[z_t = \sigma (W_z h_{t-1} + U_z x_t)\]

<p>which is <strong>between 0 and 1</strong>, then is used in $h_t$ as:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234102881.png" alt="image-20220203234102881" style="zoom:80%;" /></p>

<p>Examples of what $z_t$ does is shown below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">output is the new candidate</th>
      <th style="text-align: center">output is the previous hidden state</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233102030.png" alt="image-20220203233102030" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233355981.png" alt="image-20220203233355981" /></td>
    </tr>
  </tbody>
</table>

<h3 id="reset-gate">Reset Gate</h3>

<p>The reset gate again is nonlinear:</p>

\[r_t = \sigma (W_r h_{t-1} + U_r x_t)\]

<p>This is used by the candidate activation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234223392.png" alt="image-20220203234223392" style="zoom: 80%;" /></p>

<p>The effects:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Candidate forgetting the previous hidden state</th>
      <th style="text-align: center">Candidate does same as RNN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233449807.png" alt="image-20220203233449807" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233500860.png" alt="image-20220203233500860" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="function">Function</h3>

<p>The last possible combination is:</p>

<ul>
  <li>$z_t = r_t = 0$, then hidden state is only dependent ton current state as $h_t = \bar{h}_t = \phi(Ux_t)$</li>
  <li>$z_t =0, r_t = 1$, then we get back RNN because $h_t = \bar{h}<em>t = \phi(Wh</em>{t-1} + Ux_t)$</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Output hidden state is only dependent on the current state</th>
      <th style="text-align: center">Reduced to RNN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234523035.png" alt="image-20220203234523035" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234531917.png" alt="image-20220203234531917" /></td>
    </tr>
  </tbody>
</table>

<h2 id="long-short-term-memory">Long Short-Term Memory</h2>

<p>Long short-term memory (LSTM) (Hochreiter &amp; Schmidhuber 1997) was introduced two decades before the GRU (Cho et al. 2014).</p>

<p>The LSTM is easy to train, and includes an <strong>additional input and output compared with the RNN and GRU</strong>. At each time step $t$:</p>

<ul>
  <li>receives as input the current state $x_t$, the hidden state $h_{t-1}$, and <strong>memory cell</strong> $c_{t-1}$ of the previous time step</li>
  <li>outputs the hidden state $h_t$ and memory cell $c_t$</li>
</ul>

<p>An encapsulation would look like this</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000418648.png" alt="image-20220204000418648" style="zoom:50%;" /></p>

<p>It is then combined such that we have</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000434163.png" alt="image-20220204000434163" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>similarly, you can also combine to have a <strong>bidirectional LSTM</strong> by:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Bidirectional RNN</th>
          <th style="text-align: center">Bidirectional LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="http://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f7bdb849dafe17c952bfd88b879e01f74cf59d78/4-Figure3-1.png" alt="Bidirectional LSTM (BiLSTM) Training Task - GM-RKB" style="zoom: 50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where essentially <strong>each LSTM cell at time $t$ is duplicated</strong> (they also have a separate weights like in bidirectional RNN) into a forward direction and a backward direction.</p>
  </li>
</ul>

<p>Each unit of LSTM look like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">LSTM Schematic</th>
      <th style="text-align: center">Another View</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000548449.png" alt="image-20220204000548449" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207190734060.png" alt="image-20220207190734060" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the highlighted part is clear, same as RNN.</p>

<ul>
  <li>
    <p>so we have an additional five components:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000633720.png" alt="image-20220204000633720" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>so now shared weights are $W_f, W_i, W, W_0$ and $U_f, U_i, U, U_0$</p>
  </li>
  <li>
    <p>outputs will be $c_t, h_t$, which will be inputs in the next LSTM unit.</p>
  </li>
</ul>

<p>Alike GRU, those three gate essentially regulates how much information can get through</p>

<h3 id="forget-gate">Forget Gate</h3>

<p>The forget gate has the following equation</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000921911.png" alt="image-20220204000921911" style="zoom: 67%;" /></p>

<ul>
  <li>or somtimes, we can simply this as $f_t = \sigma(W_f \cdot [h_{t-1},x_t]^T )$ for $W_f \equiv [W_f, U_f]$ being concatenated</li>
</ul>

<p>Since it is sigmoid, we can investigate the extreme values</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Memory Cell Ignore Previous Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001008540.png" alt="image-20220204001008540" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="input-gate">Input Gate</h3>

<p>The input gate has the function</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001140743.png" alt="image-20220204001140743" style="zoom:67%;" /></p>

<p>which again is <strong>used by the memory cell</strong></p>

<ul>
  <li>controls <strong>how much $\bar{c}_t$ will be included</strong> in the new cell state $c_t$</li>
</ul>

<p>For instance, since $\sigma \in [0,1]$</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">new candidate memory $\bar{c}_t$ is ignored</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001242584.png" alt="image-20220204001242584" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="memory-cell">Memory Cell</h3>

<p>The memory cell has equation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001326157.png" alt="image-20220204001326157" style="zoom:67%;" /></p>

<p>which basically is updated very iteration to store some new memory, essentially <strong>candidate memory</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001418530.png" alt="image-20220204001418530" style="zoom: 67%;" /></p>

<p>which then stores <strong>information about previous state and input</strong></p>

<ul>
  <li>then, when we are outputing $h_t$, it will <strong>read from memory cell $c_t$</strong></li>
</ul>

<h3 id="output-gate">Output Gate</h3>

<p>The output gate has equation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001608239.png" alt="image-20220204001608239" style="zoom:67%;" /></p>

<p>which is <strong>essentially RNN</strong>, highlighted in green:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207163227530.png" alt="image-20220207163227530" style="zoom:33%;" /></p>

<p>but notice that the next state $h_t$ also <strong>included memory</strong>:</p>

\[h_t = o_t \cdot \phi(c_t)\]

<p>which is a point-wise multiplication.</p>

<ul>
  <li>function $\phi$ is a nonlinear function, such as $\tanh$</li>
  <li>so essentially $h_t$ depends on <strong>output $o_t$ and cell memory $c_t$</strong>, where $o_t$ is essentially the RNN cell</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Recall that the output will essentially comes out from $h_t$, essentially:</p>

\[y_t = f(z_t) = f(Vh_t)\]

<p>where $f$ is an activation function if we are donig classification.</p>

<p>Then, some of the usages for LSTM would look like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207164111464.png" alt="image-20220207164111464" style="zoom: 50%;" /></p>

<p>where your output at time $t$ essentialy comes out from $h_t$.</p>

<hr />

<h2 id="gru-vs-lstm">GRU vs LSTM</h2>

<p><strong>Similarities</strong> between the two:</p>

<ul>
  <li>both units avoid repeated multiplications which cause vanishing or exploding gradients by a similarly positioned <mark>addition</mark></li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001848344.png" alt="image-20220204001848344" style="zoom: 67%;" /></p>

<ul>
  <li>
    <p>update gate $z_t$​ controls the amount of the new candidate to pass in the GRU; whereas the input gate controls the amount of the new candidate memory to pass in the LSTM</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">GRU</th>
          <th style="text-align: center">LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002239125.png" alt="image-20220204002239125" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002134935.png" alt="image-20220204002134935" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Comparing the GRU reset gate controlling the candidate hidden state as highlighted in Figure 6.45 with the LSTM input gate controlling the candidate memory cell as highlighted in Figure 6.46 shows the <strong>modulation of the candidate</strong> in both units.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">GRU</th>
          <th style="text-align: center">LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002408761.png" alt="image-20220204002408761" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002416921.png" alt="image-20220204002416921" /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>Differences</strong></p>

<ul>
  <li>as GRU has fewer gates, they have a <strong>fewer numbe of parameters and trains faster</strong> than LSTM</li>
</ul>

<p>Lastly, an overview of NN, RNN and LSTM as “neuron”:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207191037059.png" alt="image-20220207191037059" /></p>

<h2 id="sequence-to-sequence">Sequence to Sequence</h2>

<p>You can essentially use LSTM/RNN/GRU blocks as follows;</p>

<p><img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>each component LSTM used here can be changed to GRU or RNN</p>
  </li>
  <li>
    <p>the <strong>encoder</strong> takes in the input sequence $(x_1, …, x_s)$ and output a <strong>context vector</strong> $z=f(Vh_t)$</p>

    <ul>
      <li>this is a <strong>single vector</strong>, which is treated as hidden state $h_0$ in the decoder</li>
    </ul>
  </li>
  <li>
    <p>the <strong>decoder</strong> also consists of LSTM/GRU that takes $h_0=z$ as the first hidden state, and generates output <strong>sequence $(y_1,…,y_t)$</strong></p>

    <ul>
      <li>
        <p>therefore, the entire model is doing:</p>

\[(y_1,...,y_t) = \text{decoder}(\text{encoder}(x_1, ..., x_s))\]
      </li>
    </ul>
  </li>
  <li>
    <p>this is often used for <strong>machine translation</strong>, for instance</p>
  </li>
</ul>

<h2 id="adding-attention">Adding Attention</h2>

<blockquote>
  <p>Below introduces the idea of attention as essentially a <strong>weighted sum over inputs</strong>, and talks about <strong>encoder-decoder attention</strong>. For reference, checkout <a href="#Self-Attention">Self-Attention</a> which is relevant but different.</p>
</blockquote>

<p>For many applications, it helps to <strong>add “attention”</strong> to RNNs, so that we can <strong>focus on certain part of the input sequence</strong></p>

<ul>
  <li>
    <p>Allows network to learn to <mark>attend to different parts of the input</mark> at different time steps, shifting its attention to focus on different aspects during its processing.</p>
  </li>
  <li>Used in image captioning to focus on <em>different parts of an image</em> when <em>generating different parts of the output sentence</em>.</li>
  <li>In MT, allows focusing attention on <em>different parts of the source sentence</em> when <em>generating different parts of the translation</em>.</li>
</ul>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207170115608.png" alt="image-20220207170115608" /></p>

<p>where we see that:</p>

<ul>
  <li>the underlined text is our <strong>query</strong></li>
  <li>the image is our <strong>key</strong> (the database of data we have)</li>
  <li>then we want to output the highlighted part</li>
</ul>

<p>To see why this is useful, consider the task of doing <strong>machine translation</strong>, so we are using a Seq2Seq Model which consists of an encoder and a decoder as shown before:</p>

<p><img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" /></p>

<p>Now, Seq2seq models <strong>incorporating attention</strong> can:</p>

<ol>
  <li>the <strong>decoder</strong> receives as input the <strong>encoder encoder output sequence</strong> $c_i=c_i(o_1, …,o_t)$
    <ul>
      <li>where $o_t= V\begin{bmatrix}h_t\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$</li>
    </ul>
  </li>
  <li>different parts of the output sequence <strong>pay attention</strong> to <strong>different parts of the input sequence</strong></li>
</ol>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 60%;" /></p>

<p>where essentially:</p>

<ul>
  <li>
    <p>for each input to <strong>decoder</strong> hidden state $\tilde{h}_t$, a context vector is used which is <strong>attention</strong> w.r.t step $t=1$ and of <strong>all input sequences $x_1, …, x_t$</strong></p>
  </li>
  <li>
    <p>specifically, in this <strong>bidirectional RNN</strong> as encoder, the context vector is computed as:</p>

\[c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T\]

    <p>which is a <strong>weighted sum</strong> over <strong>all hidden states in the encoder</strong> (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:</p>

\[\alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j\]

    <p>which basically represents the <strong>amount of attention</strong> output $o_i$ will give to input word $x_j$, for some score function. (checkout <a href="#Self-Attention">Self-Attention</a>).</p>
  </li>
  <li>
    <p>This context vector, $c_i$, is generated anew with <strong>each decoding step $i$</strong> and takes all of the encoder hidden states into account in its derivation</p>
  </li>
  <li>
    <p>this is also called <mark>encoder-decoder attention</mark>, which is different from <a href="#Self-Attention">Self-Attention</a>, and covered more in detail in <a href="#Attention">Attention</a></p>
  </li>
</ul>

<h2 id="sgns-embeddings">SGNS Embeddings</h2>

<p>Another common problem in language data is <strong>how do we represent texts/words (tokens)</strong>.</p>

<ul>
  <li>one-hot encoding
    <ul>
      <li>problem: every two words have <strong>the same distance</strong>. i.e. losing relationship between them</li>
      <li>problem: sparse vector.</li>
    </ul>
  </li>
  <li><strong>feature embedding</strong> for each word
    <ul>
      <li>we want to somehow learn word embedding from large <strong>unsupervised</strong> text corpus.</li>
    </ul>
  </li>
</ul>

<p>In this section we introduce one method for computing embeddings: <strong>skip-gram SGNS with negative sampling</strong>, sometimes called SGNS.</p>

<ul>
  <li>The skip-gram algorithm is one word2vec of two algorithms in a software package called word2vec</li>
</ul>

<blockquote>
  <p><em>Intuition</em></p>

  <p>The intuition of <mark>word2vec</mark> is to train a classifier on a <strong>binary</strong> prediction task: given a word $w$, <strong>how likely is it to show up near another word</strong> $w_i$, e.g. apricot?</p>

  <ul>
    <li>this can be done in <strong>self-supervision</strong>, which avoids the need for any sort of hand-labeled supervision signal</li>
    <li>essentially training a logistic regression classifier</li>
    <li>this is <strong>static</strong>, in that it learns one <strong>fixed embedding for each word</strong> in the embeddings vocabulary.
      <ul>
        <li>In the <a href="#Transformer">Transformer</a>  chapter we will introduce methods for learning <strong>dynamic contextual embeddings</strong> like the popular family of BERT representations, in which the vector for each word is <strong>different in different contexts</strong></li>
      </ul>
    </li>
  </ul>

  <p>Then, <mark>while we are learning the likelihood</mark>, we would have <mark>needed/learnt some representation $\vec{w}$ for a word $w$</mark>, which will be our embedding!</p>
</blockquote>

<p>Therefore, the model is simple:</p>

<ol>
  <li>Treat the target word $w$ and a <strong>neighboring</strong> context word as <strong>positive</strong> examples.</li>
  <li>Randomly sample other words in the lexicon to get negative samples.</li>
  <li>Use <strong>logistic regression</strong> to train a classifier to distinguish those two cases.</li>
  <li>Use the <strong>learned weights</strong> $W$ as the <strong>embeddings</strong>, commonly represented as $E$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>This means that the embedding for word $w_i$ will be similar to $w_j$ if they are <strong>physically close</strong> to each other.</p>

  <ul>
    <li>i.e. if phrases like “bitter sweet” will create problems in the embedding! (physically close but their meanings are different)</li>
  </ul>
</blockquote>

<h3 id="sgns-classifier">SGNS Classifier</h3>

<blockquote>
  <p><strong>Goal of Classifier</strong>:</p>

  <p>Given a tuple $w,c$ being target word $w$ paired with a candidate word $c$, what is the <strong>probability</strong> that $c$ is a <strong>context word</strong> (i.e. physically next to it)?</p>

  <p>To present such <strong>probability</strong>, we will use:</p>

\[P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\]

  <p>for $\vec{w},\vec{c}$ being the <strong>embedding of word $w,c$</strong>.</p>

  <ul>
    <li>so on our way to learn such classifier, we would have learnt $\vec{w},\vec{c}$ which will be used for embedding</li>
  </ul>
</blockquote>

<p>Consider we want to find out the <strong>embedding of the word $\text{apricot}$</strong>, and we have the following data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208214715503.png" alt="image-20220208214715503" /></p>

<p>where:</p>

<ul>
  <li>
    <p>let us take a window size of $2$, so that we view $[c_1, …, c_4]$ above as <strong>real context word</strong> for $\text{apricot}$.</p>
  </li>
  <li>
    <p>our goal is to have a logistic regression such that:</p>

\[P(+|w,c)\]

    <p>is <strong>high</strong> if $c$ is a <strong>real context word</strong>, and that:</p>

\[P(-|w,c)  = 1-P(+|w,c)\]

    <p>is <strong>high</strong> if $c$ is <strong>not a context word</strong>.</p>
  </li>
</ul>

<p>Then, the question is how do we model such probability? We assumed that <strong>words next to each other</strong> should have <mark>similar embeddings</mark>. This means that:</p>

\[\text{Similarity}(w,c) \approx \vec{w} \cdot \vec{c}\]

<p>for $\vec{w},\vec{c}$ being the embeddings for the word $w,c$. Then, to map this to <strong>probability that $c$ is a real context word for $w$ as</strong>:</p>

\[P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\]

<p>is high if high dot product = similar = next to each other. Then similarly, probability that $c$ is not a context word as:</p>

\[P(-|w,c) = \sigma(-\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(+\vec{w}\cdot \vec{c})}\]

<p>for $c$ being <strong>negative samples</strong> (not context words).</p>

<hr />

<p>Now, this means that we can also assign “<strong>similarity score</strong>” between a target word and a <strong>context window</strong>:</p>

\[P(+|w, c_{1:L}) = \prod_{i=1}^L \sigma(\vec{c}_i \cdot \vec{w})\]

<p>where:</p>

<ul>
  <li>we assumed <strong>all contexts words are independent</strong></li>
  <li>we can also compute the log probability to make it a sum</li>
</ul>

<hr />

<p>Now, we can think of what are are learning graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>where essentially:</p>

<ul>
  <li>Embedding matrix $E$ contains two matrices, $W$ for word embedding and $C$ for context embedding
    <ul>
      <li>i.e. for the $i$-th word (in the dictionary), its word embedding will be the $i$-th column of $W$, and similarly for context embedding</li>
      <li>i.e. every word will have <strong>two embeddings</strong>, one in $W$ and another in $C$. In reality people either only take $W$ or take $W+C$</li>
      <li>if your vocabulary size is $\vert V\vert$, and you want an embedding of dimension $d$, then $W,C \in \mathbb{R}^{d \times \vert V\vert }$ so that you can fetch the embedding from a one-hot vector.</li>
    </ul>
  </li>
  <li>we have <strong>two embeddings</strong> because a word $w$ could be treated as a target, but sometimes it might also be picked as a context $c$, in which we update the embedding separately.</li>
</ul>

<h3 id="learning-the-embedding">Learning the Embedding</h3>

<p>Our target is to learn the matrix:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>Let us begin with an example text, where our target word currently is $w=\text{apricot}$.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208221658252.png" alt="image-20220208221658252" /></p>

<p>Then taking a window size of $2$, we also want to have <strong>negative samples</strong> (in fact, more negative samples than positive ones per target word so that we are called <strong>SGNS</strong>):</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208221831637.png" alt="image-20220208221831637" /></p>

<p>where here:</p>

<ul>
  <li>
    <p>each of the training sample $(w, c_{pos})$ comes with $k=2$ negative samples, for $k$ being tunable</p>
  </li>
  <li>
    <p>the negative samples are sampled <strong>randomly by</strong>:</p>

\[\text{Prob of sampling $w$}= P_\alpha(w) = \frac{ \text{Count}(w)^\alpha }{\sum_{w'} \text{Count}(w')^\alpha}\]

    <p>where we usually take $\alpha = 0.75$ so that <strong>rare words have a  better chance</strong></p>

    <p>e.g. if $P(a)=0.99,P(b)=0.01$, doing the power would give:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222147320.png" alt="image-20220208222147320" style="zoom:80%;" /></p>
  </li>
</ul>

<p>Then, <strong>given a positive pair and the $k$ negative pairs</strong>, our <mark>loss function to minimize would be</mark></p>

\[L_{CE} = -\log{\left[ P(+|w,c_{pos}) \cdot \prod_{i=1}^k P(- |w,c_{neg})\right]} = - \left[ \log \sigma(c_{pos} \cdot w) + \sum_{i=1}^k  \log \sigma(-c_{neg} \cdot w) \right]\]

<p>which we want to <strong>minimize</strong> by <strong>updating $\vec{w} \in W$</strong> for the target word and $\vec{c} \in C$ for the context word.</p>

<p>Therefore, we need to take the derivatives:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222630846.png" alt="image-20220208222630846" style="zoom:80%;" /></p>

<p>Then the update equations</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222649078.png" alt="image-20220208222649078" style="zoom:80%;" /></p>

<p>So graphically, we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222800901.png" alt="image-20220208222800901" style="zoom:80%;" /></p>

<p>notice that we update target word embedding in $W$ but context words in $C$.</p>

<p>Then that is it! You now have leant <strong>two separate embeddings</strong> for each word $i$:</p>

<ul>
  <li>$\vec{w}_i = W[i]$ is the embedding <strong>target embedding</strong> for word $i$</li>
  <li>$\vec{c}_i = C[i]$ is the embedding <strong>context embedding</strong> for word $i$</li>
</ul>

<p>(then it is common to either just add them together as $\vec{e}_i = \vec{w}_i+\vec{c}_i$ as the embedding for word $i$, or just take $\vec{e}_i = \vec{w}_i$.)</p>

<h3 id="other-embeddings">Other Embeddings</h3>

<p>Some problem with SGNS Embedding (Word2Vec) is:</p>

<ul>
  <li>what if we want to know the embedding of an <strong>unknown word</strong> (i.e. unseen in the training corpus)?</li>
  <li><strong>sparsity</strong>: in languages with rich morphology, where some of the <em>many forms for each noun and verb may only occur rarely</em></li>
</ul>

<p>There are many other kinds of word embeddings that could deal with those problems. Here we briefly cover two:</p>

<ul>
  <li>
    <p><strong>Fasttext</strong>: Fasttext deals with these problems by using <strong>subword</strong> models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols <code class="language-plaintext highlighter-rouge">&lt;</code> and <code class="language-plaintext highlighter-rouge">&gt;</code> added to each word</p>

    <p>For example, with $n = 3$ the word where would be represented by the sequence <code class="language-plaintext highlighter-rouge">&lt;where&gt;</code> plus the character n-grams:</p>

\[\text{&lt;wh, whe, her, ere, re&gt;}\]

    <p>then a skipgram embedding is learned for each constituent n-gram, and the word <code class="language-plaintext highlighter-rouge">where</code> is represented by the <strong>sum of all of the embeddings of its constituent n-grams</strong>. Therefore, <mark>Unknown words</mark> can then be presented only by the sum of the constituent n-grams!</p>
  </li>
  <li>
    <p><strong>GloVe</strong>: GloVe model is based on capturing <strong>global corpus statistics</strong>. GloVe is based on ratios of probabilities from the word-word cooccurrence matrix, combining the intuitions of count-based models like PPMI while also capturing the linear structures used by methods like word2vec.</p>
  </li>
  <li>
    <p>and many more</p>
  </li>
</ul>

<h1 id="graph-neural-network">Graph Neural Network</h1>

<p>In this chapter we describe graph neural networks, applied to networks or general graphs sharing weights across neighborhoods as shown below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211223146024.png" alt="image-20220211223146024" style="zoom: 50%;" /></p>

<p>Essentially graphs are everywhere, and we deal with data that can be represented as a graph by the following idea:</p>

<ul>
  <li>Each <strong>node</strong> in a network may have an <strong>associated feature vector</strong> that represents its attributes</li>
  <li>The edge information would be <strong>also encoded</strong> in the <strong>feature vector</strong> of the node
    <ul>
      <li>essentially, a node will <strong>aggregate information from neighbors</strong> to achieve this</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>In the end, a GNN basically attempts to <strong>encode much information about the graph</strong> into <strong>each node of the graph</strong>, resulting in some complicated <strong>embedded vector $h_i^l$ for each node $i$.</strong></p>

  <p>Then, this information is used to do downstream tasks such as node classification.</p>
</blockquote>

<p>Then, essentially we have a list of feature vectors representing the graph. With that, some <strong>common tasks</strong> include:</p>

<ol>
  <li><strong>Node prediction</strong>: Predicting a property of a graph node.</li>
  <li><strong>Link prediction</strong>: Predicting a property of a graph edge.
    <ul>
      <li>For example, in a social network we can predict whether two people will become friends</li>
    </ul>
  </li>
  <li><strong>Graph or sub-graph prediction</strong>: Predicting a property of the entire graph or a sub-graph.
    <ul>
      <li>For example, given a graph representation of a protein we can predict its function as an enzyme or not</li>
    </ul>
  </li>
</ol>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211222822911.png" alt="image-20220211222822911" style="zoom:50%;" /></p>

<h2 id="definitions">Definitions</h2>

<p>A graph $G = (V, E)$ contains a set of $n$ vertices (or nodes) $V$ and set of $m$ edges $E$ between vertices. The edges of the graph can either be undirected or directed.</p>

<p>Two basic graph representations are an <strong>adjacency matrix</strong> and <strong>adjacency list</strong>.</p>

<blockquote>
  <p>An <strong>adjacency matrix</strong> $A$ of dimensions $n \times n$ is defined such that:</p>

\[A_{i,j} = \begin{cases}
1, &amp; \text{if there is an edge between vertex $i$ and $j$}\\
0,&amp; \text{otherwise}
\end{cases}\]

  <p>where if edges have weights, then replace $1\to w$</p>
</blockquote>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Adjacency Matrix</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211230849534.png" alt="image-20220211230849534" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211230856499.png" alt="image-20220211230856499" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>An <strong>adjacency list</strong> is a list of edges for each node.</p>
</blockquote>

<p><strong>Though</strong> adjacency matrix and list encode the same information:</p>

<ul>
  <li>different <strong>permutations of the node labels</strong> result in different adjacency matrices. In contrast, an adjacency list of the edges in the graph is invariant to node permutations.</li>
  <li><strong>storing</strong> an adjacency matrix takes $O(n^2)$ memory for $n$ being the number of nodes; whereas for adjaceny matrix it only takes $O(m)$ for $m$ is the number of edges in the graph. Since most graphs are sparse, this makes it even more appropriate to use adjacency list instead.</li>
</ul>

<blockquote>
  <p>The <strong>degree</strong> of a node $d_i$ represents the number of edges incident to that node (i.e. number of connections that it has to other nodes in the network)</p>

  <ul>
    <li>
      <p>average degree of a graph is the average degree over all its node:</p>

\[\frac{1}{n}\sum_{i=1}^n d_i\]

      <p>which is $2m/n$ for an undirected graph and $m/n$ for a directed graph</p>
    </li>
  </ul>

  <p>You can also represent the degree matrix $D$ being a diagonal matrix (so it can be used for calculating Laplacian):</p>

\[D_{i,i} = \text{degree}(v_i) = \sum_{j=1}^n A_{i,j}\]

</blockquote>

<p>For example, the degree matrix for the above graph:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211231752362.png" alt="image-20220211231752362" style="zoom:50%;" /></p>

<blockquote>
  <p>for directed graphs the <strong>indegree</strong> of a node is the number of edges leading into that node and its <strong>outdegree</strong>, the number of edges leading away from it.</p>
</blockquote>

<blockquote>
  <p>The <strong>graph Laplacian matrix</strong> $L$ is the difference between the degree matrix and adjacency matrix $L = D − A$.</p>

  <ul>
    <li>
      <p>for functions, Laplacian measures the <em>divergence</em>:</p>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211232324624.png" alt="image-20220211232324624" style="zoom:67%;" /></p>
    </li>
    <li>
      <p>for more reference to see how it works: https://mbernste.github.io/posts/laplacian_matrix/</p>
    </li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211232434295.png" alt="image-20220211232434295" style="zoom:50%;" /></p>

<p>notice that:</p>

<ul>
  <li>The adjacency matrix and the degree matrix are symmetric and therefore the Laplacian matrix is symmetric</li>
</ul>

<p>Some other related matrix would be:</p>

<ul>
  <li>
    <p><strong>graph symmetric normalized Laplacian</strong></p>

\[\hat{L}=D^{-1/2} L D^{-1/2}=I-D^{-1/2} A D^{-1/2}\]
  </li>
  <li>
    <p><strong>random walk normalized Laplacian matrix</strong>:</p>

\[L_r=D^{-1}L = I-D^{-1}A\]
  </li>
</ul>

<blockquote>
  <p>A Laplacian matrix $L$ of a graph with $n$ nodes ha<strong>s $n$ eigenvectors with eigenvalues which are non-negative</strong> since the Laplacian matrix $L$ has non-negative eigenvalues.</p>

  <p>The number of <strong>zero eigenvalues</strong> of the Laplacian matrix of a graph is the <strong>number of its connected components</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Sub-graph</strong> of a graph is <strong>a subset of edges</strong> and <strong>all their nodes</strong> in the graph.</p>
</blockquote>

<blockquote>
  <p>A <strong>walk</strong> is a sequence of vertices and edges of a graph i.e. if we traverse a graph then we get a walk.</p>

  <ul>
    <li>a walk can be open or closed (i.e. end same as start)</li>
    <li>vertices and Edges can be <strong>repeated</strong> in a walk</li>
  </ul>
</blockquote>

<p>An example of a walk would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211233110467.png" alt="image-20220211233110467" style="zoom: 50%;" /></p>

<p>here <code class="language-plaintext highlighter-rouge">1-&gt;2-&gt;3-&gt;4-&gt;2-&gt;1-&gt;3</code> is a walk</p>

<blockquote>
  <p><strong>Trail</strong> is an open walk in which <strong>no edge is repeated</strong>.</p>

  <ul>
    <li>vertex can be <strong>repeated</strong></li>
  </ul>
</blockquote>

<p>For example:</p>

<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-4.png" alt="img" style="zoom: 33%;" /></p>

<p>here Here <code class="language-plaintext highlighter-rouge">1-&gt;3-&gt;8-&gt;6-&gt;3-&gt;2</code> is trail</p>

<blockquote>
  <p><strong>Path</strong> is a trail in which neither vertices nor edges are repeated</p>
</blockquote>

<blockquote>
  <p>The matrix $A^k$ from an adjacency matrix contains $A_{i,j}$ being the <strong>number of walks of length $k$</strong> in the graph between the node in row $i$ and the node in column $j$.</p>
</blockquote>

<h2 id="problem-using-graphs">Problem using Graphs</h2>

<p>Polynomial</p>

<ul>
  <li>
    <p><strong>Minimum Spanning Tree (MST)</strong>: For an undirected graph, produce an acyclic tree that is the subset of the graph that spans all of the Vertices (Spanning), and it needs to have a minimum sum in terms of the edges included (minimum)</p>

    <ul>
      <li>Greedy algorithms with time complexity $O(\vert E\vert \log\vert V\vert )$: <strong>Boruvka</strong>, <strong>Prim</strong>, <strong>Kruskal</strong></li>
      <li>similar to Dijkstra, basically a graph without cycles</li>
    </ul>
  </li>
  <li>
    <p><strong>Single-Source Shortest Paths (SSP)</strong></p>

    <ul>
      <li>
        <p>For SSP with nonnegative weights: <strong>Dijkstra’s</strong> algorithm. Complexity $O(\vert V\vert \log\vert V\vert  + \vert E\vert )$ using a heap</p>

        <ul>
          <li>
            <p>essentially the greedy step is that we set the node that is marked with <strong>smallest tentative distance</strong> as the current node/completed.</p>

            <p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Dijkstra_Animation.gif/220px-Dijkstra_Animation.gif" alt="Dijkstra Animation.gif" /></p>
          </li>
        </ul>
      </li>
      <li>
        <p>For general SSP: <strong>Bellman-Ford</strong> runs in $O(\vert V\vert \vert E\vert )$</p>
      </li>
    </ul>
  </li>
</ul>

<p>NP-hard</p>

<ul>
  <li><strong>Traveling Salesman Problem (TSP)</strong>
    <ul>
      <li>e.g. find shortest tour visiting each city once and returns to start.</li>
    </ul>
  </li>
  <li><strong>Vehicle Routing Problem (VRP)</strong></li>
</ul>

<blockquote>
  <p><em>Recall</em></p>

  <p><strong>Dijkstra’s Algorithm</strong>:</p>

  <ol>
    <li>initialization: marks everything as unvisited, and set each $D_v$ field to max. Then set the source $D_v$ to 0 and visited</li>
    <li>vistit each reachable vertices and update the field
      <ul>
        <li>update $D_v$ only when it is smaller</li>
        <li>but not update <em>visited</em>, yet</li>
      </ul>
    </li>
    <li>then we take the greedy step of marking the <strong>unvisited vertex with current smallest $D_v$</strong> to be visited</li>
    <li>continues with that vertex</li>
  </ol>

  <p><strong>Prim’s Algorithm</strong>
this is pretty much the same as Dijkstra’s Algorithm, but since we are constructing a tree, the difference is</p>

  <ul>
    <li><em>known</em> means whether if we have <strong>included that vertex into the tree</strong></li>
    <li>$D_v$ means the current smallest distance we currently know to bring that vertex into the graph (not cumulative)</li>
    <li>$P_v$ vertex that achieves the shortest distance in the $D_v$ field</li>
  </ul>
</blockquote>

<h2 id="node-embeddings">Node Embeddings</h2>

<blockquote>
  <p>Graph embeddings basically means finding “<strong>latent vector representation</strong>” of graphs which captures the <strong>topology</strong> (in very basic sense) of the graph. We can make this “vector representation” rich by also considering the vertex-vertex relationships, edge-information etc.</p>

  <ul>
    <li>the following assumes that <mark>each vertex/node has an associated feature vector</mark></li>
    <li>so in a sense it is “node embedding”</li>
  </ul>
</blockquote>

<p>There are roughly two levels of embeddings in the graph (of-course we can anytime define more levels by logically dividing the whole graph into subgraphs of various sizes):</p>

<ul>
  <li><strong>Vertex Embeddings</strong> - Here you find latent vector representation of <em>every vertex</em> in the given graph. You can then compare the different vertices by plotting these vectors in the space and interestingly “similar” vertices are plotted closer to each other than the ones which are dissimilar or less related. This is the same work that is done in “DeepWalk” by Perozzi.</li>
  <li><strong>Graph Embeddings</strong> - Here you find the latent vector representation of the whole graph itself. For example, you have a group of chemical compounds for which you want to check which compounds are similar to each other, how many type of compounds are there in the group (clusters) etc. You can use these vectors and plot them in space and find all the above information. This is the work that is done in “Deep Graph Kernels” by Yanardag.</li>
</ul>

<p><img src="https://snap-stanford.github.io/cs224w-notes/assets/img/node_embeddings.png?style=centerme" alt="node embeddings" style="zoom: 20%;" /></p>

<p>For example, we may optimize for the similarity between nodes $i$ and $j$, such that their similarity $s(i, j)$ is maintained after the embedding $f(i)^T f(j)$.</p>

<h3 id="shallow-embedding">Shallow Embedding</h3>

<p>“Shallow” encoding is the simplest embedding approach, it means it is just an embedding-lookup :</p>

\[f(v_i) =We_i\]

<p>for $e_i \in \mathbb{I}^{n}$ is essentially a <strong>one-hot encoded</strong> vector, and $W \in \mathbb{R}^{d \times n}$ if there are $n$ nodes.</p>

<ul>
  <li>This results in a problem with shallow embeddings, which is that they do not share weights, i.e. does not scale with the number of nodes</li>
</ul>

<h3 id="node-similarity">Node Similarity</h3>

<p>One key idea of embedding is that <strong>similarity of nodes</strong> is preserved in the embedding space. To formally use that as objective for learning the embedding (e.g. shallow encoding), we need to define <strong>similarity</strong></p>

<ul>
  <li>essentially different similarity metric captures <strong>different properties of a graph</strong>, results in <strong>different loss functions</strong> hence <strong>different embedding algorithm</strong></li>
</ul>

<h4 id="adjacency-based-similarity">Adjacency-based Similarity</h4>

<blockquote>
  <p><strong>Similarity</strong> between nodes $i$ and $j$ is the weight on the edge between them $s(i, j) = A_{i,j}$ where $A$ is the weighted adjacency matrix.</p>

  <ul>
    <li>this is a bit “bad” because non-neighbors will have weight $0$ as it is adjacency matrix</li>
  </ul>
</blockquote>

<p>Them, we can define <strong>loss</strong> to find the <strong>embedding matrix $W$</strong>:</p>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}||^2\]

<p>over all pairs of nodes in the graph</p>

<h4 id="multi-hop-similarity">Multi-hop Similarity</h4>

<blockquote>
  <p>Instead of only considering immediate neighbor in $A$, we can consider $k$-hop neighbors by using $A^k$ to be the <strong>adjacency matrix</strong>.</p>

  <ul>
    <li>an improvement over adjacency based similarity</li>
  </ul>
</blockquote>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}^k||^2\]

<h4 id="overlap-similarity">Overlap Similarity</h4>

<p>Another measure of similarity is the overlap between node neighborhoods as shown in Figure 7.9.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212002606862.png" alt="image-20220212002606862" style="zoom: 67%;" /></p>

<p>Suppose nodes $i$ and $j$ share common nodes. We can then minimize the loss function <strong>measuring the overlap</strong> between neighborhood:</p>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-S_{i,j}||^2\]

<p>where:</p>

<ul>
  <li>$S_{i,j}$ could be Jaccard overlap or Adamic-Adar score</li>
</ul>

<h3 id="random-walk-embeddings">Random Walk Embeddings</h3>

<p>Essentially an alternative of shallow embedding.</p>

<blockquote>
  <p>A <strong>random</strong> walk in a graph begins with a node $i \in V$ and repeatedly walks to one of its neighbors $v\in N(i)$ with probability $1
/d(i)$for$t$steps until reaching and ending node$j$ on the graph.</p>

  <p>In essence:</p>

  <ol>
    <li>given a graph and a starting point, we select a neighbor of it at random</li>
    <li>move to this neighbor; then we select a neighbor of this point at random</li>
    <li>move to it, etc. until $t$ steps are gone</li>
  </ol>

  <p>So $\text{similarity}(u,v)$ is defined as the probability that $u$ and $v$ <strong>co-occur on a random walk</strong> over a network.</p>
</blockquote>

<p>Formally:</p>

\[f(i)^Tf(j) \propto P(\text{$i$ and $j$ co-occur on the random walk}) = p(i|j)\]

<p>So basically we want to learn node embedding such that <strong>nearby nodes are close together in the network</strong>. Hence we want to maximize the likelihood of random walk co-occurrences, we compute loss function as:</p>

\[\mathcal{L} = \sum_{i \in V}\sum_{j \in N(i)} - \log p(j|f(i))\]

<p>for</p>

\[P(j|f(i)) = \frac{\exp(f(i)^T f(j))}{\sum_{j \in V} \exp(f(i)^Tf(j))}\]

<p>Then basically we want to find $W$ for $f(i)=We_i$ such that the loss can be minimized.</p>

<h2 id="graph-embedding">Graph Embedding</h2>

<p>We may also want to embed an entire graph $G$ or subgraph in some applications</p>

<p><img src="https://snap-stanford.github.io/cs224w-notes/assets/img/graph_embedding.png?style=centerme" alt="GraphE" style="zoom:50%;" /></p>

<p>There are several ideas to accomplish graph embedding:</p>

<ol>
  <li>
    <p>The simple idea (Duvenaud et al., 2016) is to run a standard graph embedding technique on the (sub)graph GG, then just sum (or average) the node embeddings in the (sub)graph GG.</p>

    <ul>
      <li>i.e. taking the sum of the embeddings of the nodes in the sub-graph $\sum_{i \ni S} f(i)$</li>
    </ul>
  </li>
  <li>
    <p>Introducing a “virtual node” to <strong>represent</strong> the (sub)graph and run a standard graph embedding technique</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212000807810.png" alt="image-20220212000807810" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>We can also use <strong>anonymous walk embeddings</strong>. In order to learn graph embeddings, we could enumerate all possible anonymous walks aiai of ll steps and record their counts and represent the graph as a probability distribution over these walks. To read more about anonymous walk embeddings, refer to <em>Ivanov et al., Anonymous Walk Embeddings (2018)</em>.</p>
  </li>
</ol>

<h2 id="neighborhood-aggregation">Neighborhood Aggregation</h2>

<p>Once embedding in done, we finally <strong>start with GNN</strong>.</p>

<p>We consider graph neural networks (GNNs) which <mark>take into account neighbors of each node</mark> (i.e. their embeddings)</p>

<ul>
  <li>if you naively think of concatenating feature vector $f(i)$ into things such as adjacency matrix. Then this will be problematic as: what if you wanted to add a node to the graph? Then the graph size changed -&gt; adj matrix size change -&gt; new architecture.</li>
  <li>basically, the number of parameters is <strong>linear in the size of the graph</strong>, the network is dependent on the order of the nodes and does not accommodate dynamic graphs</li>
</ul>

<blockquote>
  <p><strong>Desired Properties</strong></p>

  <ul>
    <li>Invariant to node ordering</li>
    <li>Locality, operations depend on neighbors of a given node</li>
    <li>Number of parameters independent of graph size</li>
    <li>Model independent of graph structure</li>
    <li>Able to transfer across graphs</li>
  </ul>
</blockquote>

<p>The problem above can be <strong>solved</strong> by:</p>

<ul>
  <li>
    <p>aggregating information from neighboring nodes in a BFS manner</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005141979.png" alt="image-20220212005141979" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>aggregating information in a chain, in a DFS manner</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005155734.png" alt="image-20220212005155734" style="zoom:50%;" /></p>
  </li>
</ul>

<p>Here we will discuss the first architecture, which can be visualized as:</p>

<ul>
  <li>for each node in the graph in turn,</li>
  <li>pick up the graph from that node as the root allowing all other nodes to dangle</li>
  <li>building a computation graph where that node is the root</li>
  <li>propagate and transform information from its neighbors, its neighbors’ neighbors etc, as shown below</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005458064.png" alt="image-20220212005458064" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the information to collect from each vertex would be their <strong>node embedding</strong></p>
  </li>
  <li>
    <p>each grey box would contain <strong>weights</strong> for them to “transform”</p>
  </li>
</ul>

<p>Most graph neural networks are based on aggregating information into each node from it’s neighboring nodes in a layer in the above manner:</p>

\[h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}\]

<p>so that essentially:</p>

<ul>
  <li>$h_i^l$ is the feature representation of node $i$ <strong>at layer $l$</strong>
    <ul>
      <li>e.g $h_i^0$ is the 0 layer aggregation so $h_i^0 = f(i)$ is the raw embedding</li>
      <li>the feature vector $h_i^{i-1}$ of the previous <strong>layer embedding</strong></li>
    </ul>
  </li>
  <li>essentially we are doing for each $l$:
    <ul>
      <li>aggregate embedding from <strong>neighbors</strong></li>
      <li>combining with your <strong>previous embedding</strong></li>
    </ul>
  </li>
</ul>

<p>Next, we consider each node in turn, and generate a computation graph for each node where that node is the root. Finally, we will <strong>share the aggregation parameters</strong> across all nodes, for every layer of neighbors, as shown in Figure below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212012049523.png" alt="image-20220212012049523" style="zoom: 80%;" /></p>

<p>where the grey boxes are the aggregation parameter/weights.</p>

<p>Formalizing with weights:</p>

\[h_{i}^l = \sigma\left(B^l h_{i}^{l-1}+W^l \sum_{j \in N(i)}\frac{h_j^{l-1}}{|N(i)|} \right)\]

<p>where:</p>

<ul>
  <li>
    <p>this is essentially a <strong>recursive formula</strong>, the base case is:</p>

\[h_{i}^1 = \sigma\left(B^1 h_{i}^{0}+W^1 \sum_{j \in N(i)}\frac{h_j^{0}}{|N(i)|} \right)=\sigma\left(B^1 x_{i}+W^1 \sum_{j \in N(i)}\frac{x_j}{|N(i)|} \right)\]

    <p>for the $0$-th layer embedding is the same as the <strong>raw node embedding $x_i=f(i)$</strong>. Notice that <mark>embedding of the node itself</mark> is included in this operation (along with its neighbors)!</p>
  </li>
  <li>
    <p>so technically each layer has two weights: <strong>$B^l$ is a matrix of weights for self-embedding</strong> and <strong>$W^l$ for neighbor embedding</strong>.</p>
  </li>
</ul>

<p>With this, we can now perform tasks such as <strong>node classification</strong>!</p>

<h3 id="supervised-node-classification">Supervised Node Classification</h3>

<p>For the task of node classification, given $m$ labeled nodes $i$ with labels $y_i$ we train a GNN by minimizing the objective:</p>

\[\mathcal{J}=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(y^i,\hat{y}^i)\]

<p>where the prediction $\hat{y}^i$ will be some neural network output based on the <strong>layered embedding $h_i^l$</strong> (at the last layer) we discussed before.</p>

<h2 id="gnn-architecture">GNN Architecture</h2>

<p>Essentially each architecture varies by <strong>how they perform aggregation</strong>:</p>

\[h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}\]

<h3 id="graph-convolution-network">Graph Convolution Network</h3>

<p>A graph convolution network (GCN) (Kipf &amp; Welling 2017) has a similar formulation using a single matrix for both the neighborhood and self-embeddings:\</p>

\[\begin{align*}
h_{i}^l
&amp;= \sigma\left( \frac{1}{\hat{d}_i}W^l h_{i}^{l-1} + \sum_{j \in N(i)}\frac{\hat{A}_{i,j}}{\sqrt{\hat{d}_j \hat{d}_i}}W^lh_{j}^{l-1} \right)
\end{align*}\]

<p>where:</p>

<ul>
  <li>where $\hat{A}= A+I$ is the adjacency matrix including <strong>self loops</strong>, $\hat{d}_i$ is the degree in the graph with <strong>self loops</strong>, and $\sigma$ a non-linear activation function</li>
</ul>

<h3 id="grated-graph-neural-networks">Grated Graph Neural Networks</h3>

<p>This is the second architecture mentioned in this section, which is similar to DFS: instead of sharing weights across neighborhoods (i.e. horizontally), <strong>weights are shared across all the layers in each computation graph (vertically)</strong>.</p>

<p>In gated graph neural networks (Li, Tarlow, Brockschmidt &amp; Zemel 2016) nodes aggregate messages from neighbors using a neural network, and similar to RNNs parameter sharing is across layers:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212014256440.png" alt="image-20220212014256440" style="zoom: 67%;" /></p>

<h1 id="transformers">Transformers</h1>

<p>While the addition of gates allows LSTMs/GRUs to handle more <strong>distant information</strong> than RNNs, they don’t completely solve the underlying problem:</p>

<ul>
  <li>passing information through an <strong>extended series of recurrent connections</strong> leads to information <strong>loss</strong>
    <ul>
      <li>i.e. “too distant” information are still lost</li>
    </ul>
  </li>
  <li>Moreover, the inherently sequential nature of recurrent networks makes it <strong>hard to do computation in parallel.</strong></li>
</ul>

<blockquote>
  <p>These considerations led to the transformers development of <strong>transformers</strong> – an approach to sequence processing that <mark>eliminates recurrent connections</mark> and returns to architectures reminiscent of the fully connected networks</p>

  <ul>
    <li>Transformers map sequences of input vectors $(x_1, …,x_n)$ to sequences of output vectors $y_1,…,y_n$ of the <strong>same length.</strong>
      <ul>
        <li>if our input is a sequence of tokens, then we can image each token represented as vector by $x_i=  Ew_i$ for an embedding matrix (e.g. see <a href="#SGNS Embeddings">SGNS Embeddings</a>)</li>
      </ul>
    </li>
    <li>Transformers are made up of stacks of transformer blocks, which are <strong>multilayer</strong> networks made by combining
      <ul>
        <li>simple linear layers</li>
        <li>feedforward networks (i.e. NN)</li>
        <li>self-attention layers/multihead attention (key innovation)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="self-attention">Self-Attention</h2>

<blockquote>
  <p><strong>Self-attention</strong> allows a network to directly <mark>extract</mark> and use information from <mark>arbitrarily large contexts</mark> without the need to pass it through intermediate recurrent connections as in RNNs.</p>

  <ul>
    <li>this means when processing each item in the input for $y_i$, the model <strong>has access to all of the inputs up to and including</strong> the one under consideration</li>
    <li>moreover, the computation performed for each item is <strong>independent of other computations</strong>. This allows for <mark>parallel computation</mark>!</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Heuristics</strong>:</p>

  <p>Given some data points you already know, e.g. $x_i, y_i$ being the <strong>keys</strong> and <strong>values</strong></p>

  <ul>
    <li>
      <p>then, you are <strong>given a query</strong> $x$, which you want to know the result $y$. The idea is to output:</p>

\[y = \sum_{i=1}^n a(x,x_i)y_i\]

      <p>so essentially the output for a query $x$ will <strong>be weighted average</strong> for the $x_i$ (keys) that we already know. Since we can say that <strong>$x_i$ near $x$ would be more important</strong> to consider:</p>

\[\alpha (x,x_i) = \frac{k(x,x_i)}{\sum_{j}k(x,x_j)}\]

      <p>for example $k$ kernel could be a <strong>gaussian kernel</strong>.</p>
    </li>
  </ul>

  <p>Hence, attention is essentially $\alpha(x,x_i)$, a measure of <strong>how relevant keys are to a query</strong></p>
</blockquote>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>inputs and outputs of the same length</li>
  <li>when processing each item in the input for $y_i$, the model <strong>has access to all of the inputs up to and including</strong> the one under consideration</li>
</ul>

<p>Then, a <strong>simple attention model</strong> would be to consider $y_i$ being some <strong>weighted version</strong>:</p>

\[y_i = \sum_{j \le i} \alpha_{ij} x_j\]

<p>where $\alpha_{ij}$ aims to capture the <strong>similarity/relevance</strong> of input $x_j$ for the <strong>current input $x_i$</strong>:</p>

\[\alpha_{ij} = \text{softmax}( \text{score}(x_i, x_j) ) = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j \le i\]

<p>where</p>

<ul>
  <li>
    <p>the key aim is to have $\alpha_{ij}$ compare <strong>an item of interest</strong> (i.e. $x_i$) to a <strong>collection of other items</strong> (i.e. $x_j, \forall j \le i$) in a way that <mark>reveals their relevance</mark> in the current context</p>
  </li>
  <li>
    <p>a simple example would be relevance = similarity:</p>

\[\text{score}(x_i, x_i) = x_i \cdot x_j\]
  </li>
</ul>

<blockquote>
  <p><strong>Intuition</strong></p>

  <p>Though the above is simplified version it represents the core of an attention-based approach:</p>

  <ul>
    <li>a set of <mark>comparisons</mark> to <strong>relevant</strong> items in some context</li>
    <li>normalization of those scores to provide a probability distribution</li>
    <li>the output of self-attention $y$ is a <strong>weighted sum</strong> of the inputs using the above distribution</li>
  </ul>
</blockquote>

<p>Transformers allow us to create a more sophisticated way of representing <strong>how words can contribute</strong> to the representation of longer inputs. In essence, we will have <strong>three input embeddings</strong>:</p>

<ul>
  <li><strong>query</strong>: current focus of attention $q_i = W^Q x_i$</li>
  <li><strong>key</strong>: the role of preceding input $k_j=W^Kx_j$, which will be compared against current focus $q_i$</li>
  <li><strong>value</strong>: used to compute the output for the current focus $v_j = W^V x_j$</li>
</ul>

<p>All the intermediate values are of dimension $d$, which means $W^Q,W^K, W^V$ <strong>all will be $\mathbb{R}^{d\times d}$</strong></p>

<ul>
  <li>this will be changed when we have a multi-headed attention, because technically we <strong>only needed $W^Q, W^K$ to be in the same dimension</strong>. (as we need dot products)</li>
</ul>

<p>These <strong>embedded inputs</strong> are then used for:</p>

\[\begin{align*}
\text{score}(x_i, x_j) &amp;= q_i \cdot k_j\\
\alpha_{ij} &amp;= \text{softmax}( \text{score}(x_i, x_j) )\\
y_i &amp;= \sum_{j \le i} \alpha_{ij} v_j
\end{align*}\]

<p>where:</p>

<ul>
  <li>the second step is the same as in our simple model</li>
  <li>now $q_i \cdot k_j$ measures the <strong>relevance</strong> between a <strong>query and key</strong> (e.g. a SQL search <em>query</em>, and the data <em>keys</em> you have in your table)</li>
  <li>finally, the weighted sum of $v_j = W^V x_j$ consists of the focus $x_i$ itself outputs the result <strong>value</strong></li>
</ul>

<p>Graphically, for computing $y_3$ in our previous example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207203739212.png" alt="image-20220207203739212" style="zoom: 50%;" /></p>

<p>note that:</p>

<ul>
  <li>
    <p>in reality, the dot product for score could be <strong>very large</strong>, so that having $\text{softmax}$ later on rasing it to some power will cause overflow. Hence we usually do:</p>

\[\text{score}(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}\]

    <p>for $d_k$ being the <strong>dimensionality of the query/key vector</strong>.</p>
  </li>
</ul>

<p>Finally, we can convert everything to a <strong>single matrix-matrix multiplication</strong> as each $y_i$ is independent:</p>

\[Q = XW^Q; K = XW^K; V=XW^V\]

<p>where we are packing the input embeddings of the $N$ tokens/words of the input sequence into a single matrix $X \in \mathbb{R}^{N \times d}$.</p>

<ul>
  <li>i.e. each <strong>row vector</strong> is the embedding of a token</li>
  <li>then $Q,K,V \in \mathbb{R}^{N\times d}$</li>
</ul>

<p>Then the final output can be done in a <strong>single shot of $Y \in \mathbb{R}^{N \times d}$</strong> as:</p>

\[Y = \text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

<p>where:</p>

<ul>
  <li>
    <p>this can be parallelized and is fast as it is just a matrix-matrix multiplication</p>
  </li>
  <li>
    <p><mark>but for langue modelling in guess next word</mark>, the part $QK^T$ compute the <strong>full matrix</strong>:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207204740770.png" alt="image-20220207204740770" /></p>

    <p>but we obviously want $q_1 \cdot k_2$ to be not there (so that $\text{Softmax}$ of this gives $0$). Hence we will need <strong>upper-triangular values to be set to $-\infty$</strong>. (otherwise we are “seeing the next word” when we want to predict the next word)</p>
  </li>
  <li>
    <p>this makes it clear that the computation is $O(N^2)$ for $N$ being the length of text you are inputting. Hence, generally you want to <strong>avoid</strong> putting in long texts such as Wikipedia pages/novels.</p>
  </li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Essentially what happens is that each output $y_t$ will take into <strong>account the entire input sequence $x_i$</strong>, but have <strong>weights</strong> on more relevant ones to the key (i.e. $x$) to spit out $y_t$ as <strong>a weighted average of the values</strong> (i.e. $y_i$).</p>

<ul>
  <li>
    <p>think of this as output $y$ being a weighted average of:</p>

\[y = \sum_i \alpha(x,x_i)\cdot y_i\]

    <p>for $x$ being the query, $y$ being what we want, and $x_i,y_i$ are known inputs to be keys and values.</p>
  </li>
</ul>

<p>A concrete example would be. consider computing $y_{t=1}$ from three inputs of dimension $4$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210164535973.png" alt="image-20220210164535973" style="zoom: 50%;" /></p>

<p>Then, essentially this is what happens:</p>

<ol>
  <li>
    <p>Compute key, value, query representation of all input (from some embedding matrix $W^Q,W^K,W^V$)</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210164631592.png" alt="image-20220210164631592" style="zoom: 50%;" /></p>

    <p>so essentially query $q$ is the final question we are interested in</p>
  </li>
  <li>
    <p>Calculate the <strong>attention score</strong> for $y_{t=1}$, essentially meaning how important each data key $k_1,…,k_3$ is relevant to the query</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165117963.png" alt="image-20220210165117963" style="zoom: 50%;" /></p>

    <p>which basically is doing a SoftMax as mentioned before</p>
  </li>
  <li>
    <p>Then, you use the attention score to <strong>scale the values</strong> (i.e. doing $\alpha(x,x_i)y_i$):</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165421872.png" alt="image-20220210165421872" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>Finally, you sum up the weighted values to spit out $y_{t=1}$:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165351646.png" alt="image-20220210165351646" style="zoom: 50%;" /></p>

    <p>(note that technically value would be of dimension $4$, so that the final output has the same dimension as input)</p>
  </li>
</ol>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <p>Like RNN architectures that remembers information of a sequence, self-attention does it even <strong>better</strong> by <strong>taking the entire input sequence into consideration</strong> at each time for output.</p>

  <ul>
    <li>this means parallelization of code</li>
    <li>i.e. <strong>each output</strong> (e.g. token) will have attended to (with weights) the <strong>entire input sequence</strong></li>
  </ul>
</blockquote>

<h2 id="transformer-blocks">Transformer Blocks</h2>

<p>The core of a transformer composes of <strong>transformer blocks</strong>, which essentially consists of:</p>

<ul>
  <li><strong>self-attention</strong> layer (e.g. a multihead attention layer = multiple self-attention layer)</li>
  <li><strong>normalization</strong> layer</li>
  <li><strong>feedforward</strong> layer</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Single Self-Attention Layer</th>
      <th style="text-align: center">Multihead Self-Attention</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209004304005.png" alt="image-20220209004304005" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209004414819.png" alt="image-20220209004414819" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where here:</p>

<ul>
  <li>we will mainly address the transformer block with single self-attention first.</li>
</ul>

<p>From the left figure, we can summarize what we are doing as:</p>

\[\begin{align*}
z &amp;= \text{LayerNorm}(x+ \text{SelfAttn}(x))\\
y &amp;= \text{LayerNorm}(z+ \text{FFNN}(z))
\end{align*}\]

<p>where we are outputting $y$</p>

<ul>
  <li>
    <p>we have <strong>Layer normalization</strong> (or layer norm) because it can be used to <strong>improve training performance</strong> in deep neural networks by keeping the values of a hidden layer in a range that <strong>facilitates gradient-based training</strong>:</p>

\[\text{LayerNorm}(x) = \gamma \hat{x} + \beta\]

    <p>for $\gamma, \beta$ are <strong>learnable parameters</strong> representing gain and offset, and $\hat{x}$ is the <strong>normalized version of $x$</strong>:</p>

\[\hat{x} = \frac{x-\mu}{\sigma}\]
  </li>
  <li>
    <p><strong>residual connection</strong>, as mentioned before, allows information from the activation going forward and the gradient going backwards to skip a layer <strong>improves learning</strong> and gives higher level layers <strong>direct access to information from the past</strong></p>
  </li>
  <li>
    <p><strong>input and output dimensions</strong> of these blocks are <strong>matched</strong>/the same so they can be stacked just as was the case for stacked RNNs.</p>
  </li>
</ul>

<p>(but what are the inputs $x$? You will soon see that the vector $x$ would come from $x+p$ which is the embedding of the input + positional embedding of the input)</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Now you may wonder what is the embedding used for tokens, i.e. $x$?</p>

  <ul>
    <li>e.g. BERT uses Wordpiece embeddings for tokens.</li>
    <li>In fact, the full input embedding for a token is a <strong>sum</strong> of the <strong>token embeddings</strong>, the <strong>segmentation embeddings</strong>, and the <strong>position embeddings</strong>.</li>
  </ul>
</blockquote>

<h3 id="multihead-attention">Multihead Attention</h3>

<p>Why are we not satisfied with single self-attention? A single word in a sentence can relate to each other in <strong>many different ways</strong> simultaneously!</p>

<ul>
  <li>It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. (e.g. syntactic, semantic, and discourse relationships)</li>
  <li>Transformers address this issue with <strong>multihead self-attention</strong> layers.</li>
</ul>

<p>Therefore, the idea is that we have sets of <strong>self-attention layers</strong>, called <mark>heads</mark>, that <strong>reside in a parallel fashion</strong>. The aim is that we want <strong>each self-attention layer/head</strong> capture <strong>different</strong> aspects of the relationships that exists among inputs:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209010338126.png" alt="image-20220209010338126" style="zoom: 67%;" /></p>

<p>where essentially, in the end we need input and output <strong>both</strong> of <strong>dimension $d$</strong>:</p>

<ul>
  <li>
    <p>here we have four heads, $h=4$</p>
  </li>
  <li>
    <p>since we want to capture different relationships, <strong>each head</strong> consist of its <strong>own set of key, query, value embedding matrix</strong> $W_i^K,W_i^Q,W_i^V$ for head $i$</p>
  </li>
  <li>
    <p>then, what we do inside each head/self-attention is the same as what we have covered before</p>
  </li>
  <li>
    <p>remember that embeddings does not need to have the same dimension as input (which is $d$). Also recall that we <strong>only needed key and query to be of the same dimension</strong>, therefore, here we have:</p>

\[W_i^Q,W_i^K \in \mathbb{R}^{d \times d_k};\quad W_i^V \in \mathbb{R}^{d \times d_v}\]

    <p>Then, if we pack them with inputs:</p>

\[Q_i = XW_i^Q \in \mathbb{R}^{N \times d_k}; \quad K_i = XW_i^K \in \mathbb{R}^{N \times d_k};\quad  V_i=XW_i^V\in \mathbb{R}^{N \times d_v}\]

    <p>for <strong>each</strong> head $i$. Then the final output <strong>for each head</strong> is essentially <strong>the same as mentioned before</strong>:</p>

\[A_{i}(Q_i, K_i, V_i) = \text{SelfAttn}_i(Q_i,K_i,V_i) = \left[\text{Softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)\right]V_i\]

    <p>which is of shape $N \times d_v$</p>
  </li>
  <li>
    <p>Remember that input of <strong>$N$ tokens</strong> of dimension $d$ has size $X \in \mathbb{R}^{N \times d}$. Now we have essentially $h \times N \times d_v$. Therefore, we perform:</p>

    <ol>
      <li>concatenating the outputs $A_i$ from each head</li>
      <li>using a linear projection $W^O \in \mathbb{R}^{hd_v \times d}$ for the concatenated outputs.</li>
    </ol>

    <p>Therefore we have:</p>

\[\text{MultiHeadAttn}(X) = [\text{head}_1 \oplus ... \oplus \text{head}_h] W^O\\
\text{head}_i = \text{SelfAttn}_i(Q_i,K_i,V_i)\]

    <p>so that the final output is of size $N \times d$, which is the same as input.</p>
  </li>
</ul>

<p>Then, since output size is the same as input size, we can stack them easily like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209012338218.png" alt="image-20220209012338218" style="zoom: 67%;" /></p>

<p>where in this case the next layer is also a multihead attention layer.</p>

<ul>
  <li>notice that the input is now $X+P$, which is input embedding + positional embedding, which we will cover next</li>
</ul>

<h3 id="positional-embedding">Positional Embedding</h3>

<p>How does a transformer <strong>model the position of each token</strong> in the input sequence? With <strong>RNNs</strong>, information about the order of the inputs was <mark>built into</mark> the structure of the model</p>

<ul>
  <li>we want to learn the relative, or absolute, positions of the tokens in the input</li>
</ul>

<p><strong>Solution</strong>: modify input embedding by $X:= X+P$ for $P$ being <mark>positional embedding</mark>.</p>

<p>For example, we can do the following being the <strong>absolute positional embedding</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209013143904.png" alt="image-20220209013143904" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits</li>
  <li>here we are mapping each input embedding vector $x_i$ to a <strong>scalar $p_i$</strong>, which is absolute positional encoding as it is <strong>relative to a global origin</strong> of $0$.
    <ul>
      <li>problems of this include that numbers/encoding will be big for long sequences, which could cause exploding gradients</li>
      <li>if we simply divide by sequence length, the problem is $4/5=8/10=12/15$ all equals $0.8$ but signifies different position.</li>
    </ul>
  </li>
</ul>

<p>It turns out that what we use is this</p>

\[P_{pos,2i} = \sin\left(\frac{pos}{10000^{2i/d}}\right),\quad P_{pos,2i+1} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\]

<p>where we essentially use a <strong>combination of sine and cosine</strong>, $\text{pos}$ is the position of word in the sentence</p>

<ul>
  <li>(resource on explaining how we got there: https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)</li>
</ul>

<h2 id="encoder-and-decoder">Encoder and Decoder</h2>

<blockquote>
  <p><strong>Encoder-decoder networks</strong>, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences.</p>

  <ul>
    <li>an <strong>encoder</strong> network that takes an input sequence and <strong>creates a contextualized representation</strong>, $c$</li>
    <li>a <strong>decoder</strong> then takes that context $c$ which generates a <strong>task specific output sequence</strong></li>
  </ul>
</blockquote>

<p>To be more specific</p>

<ul>
  <li>An <strong>encoder</strong> that accepts an input sequence, $x_1,…x_n$, and generates a corresponding sequence of contextualized representations, $h_1,…h_n$</li>
  <li>A <strong>context vector</strong>, $c$, which is a function of $h_1,…,h_n$, conveys the essence of the input to the decoder.</li>
  <li>A <strong>decoder</strong>, which accepts $c$ as input (first hidden state) and generates an arbitrary length sequence of hidden states $h_1,…,h_m$, from which a corresponding sequence of output states $y_1,…,y_m$, can be obtained</li>
</ul>

<blockquote>
  <p>The important thing of this idea is that LSTMs, convolutional networks, and Transformers <strong>can all be employed as encoders or decoders</strong>.</p>
</blockquote>

<p>Therefore, you will soon see that there are three main types of transformers:</p>

<ul>
  <li>encoder only</li>
  <li>decoder only</li>
  <li>encoder + decoder</li>
</ul>

<hr />

<p><em>For Example</em>, as you might have seen before, consider the simple RNN encoder-decoder architecture for MT:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210183154677.png" alt="image-20220210183154677" style="zoom:67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>the <strong>context vector</strong> is simply the <strong>last state of encoder</strong> (you will see how useful Attention is soon)</p>
  </li>
  <li>The <strong>decoder</strong> autoregressively generates a sequence of outputs (i.e. output of previous state becomes input of next state, like a regression), an element at a time, until an end-of-sequence marker is generated.</li>
  <li>Each hidden state of decoder is conditioned on the previous hidden state and the output generated in the previous state</li>
</ul>

<hr />

<h3 id="attention">Attention</h3>

<p>This is also referred to as <strong>encoder-decoder attention</strong></p>

<hr />

<p><em>Recall</em>: this is useful when we have a encoder and decoder of RNN/LSTM/GRU like the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209015006640.png" alt="image-20220209015006640" style="zoom:80%;" /></p>

<p>where:</p>

<ul>
  <li><strong>without attention</strong>, the input to decoder would <strong>only be the last state</strong> of encoder. So attention mechanism is a solution to the bottleneck problem, a way of allowing the <strong>decoder</strong> to get information from <strong>all the hidden states of the encoder</strong> (essentially by a weighted sum over all the states)</li>
  <li>then, the <strong>weights $\alpha_{ij}$</strong> will focus on (‘attend to’) a particular part of the source text that is <strong>relevant for the token the decoder is currently producing</strong>.
    <ul>
      <li>this means that $c_i$ will be different for each token in decoding</li>
    </ul>
  </li>
</ul>

<hr />

<blockquote>
  <p><strong>Goal</strong></p>

  <p>We want to find a way that allows the <strong>decoder</strong> to get information from <strong>all the hidden states of the encoder</strong>, not just the last hidden state.</p>

  <ul>
    <li>we can’t use the entire tensor of encoder hidden state vectors directly as the context for the decoder, as the number of hidden states <strong>varies</strong> with the size of the input</li>
  </ul>
</blockquote>

<p>Essentially, the idea of attention is similar to that of <mark>self-attention</mark>, but the difference is that the <strong>query</strong> we care about is the <strong>decoding state $h^d_t$</strong>, whereas in self-attention both query and keys are the input.</p>

<ul>
  <li>
    <p>i.e. here we care about: <strong>how relevant is each information from encoder $h^e_i$ to the current decoding step $h^d_t$?</strong></p>
  </li>
  <li>
    <p>therefore, the idea is the same: we are doing some kind of <strong>weighted average of keys $h^e_i$</strong>.</p>
  </li>
</ul>

<blockquote>
  <p><em>Recall</em></p>

  <p>Given some query $x$, and we want the prediction $y$:</p>

\[y = \sum_{i=1}^n a(x,x_i)y_i\]

  <p><strong>Attention</strong> is essentially weights $\alpha(x,x_i)$, a measure of <strong>how relevant keys $x_i$ are to a query $x$</strong></p>
</blockquote>

<p>Therefore, the idea for <strong>encoder-decoder attention</strong> is to consider relevant between $h^e_j,\forall j$ and the current query $h_{i-1}^d$ (we want $h_i^d$, which is not computed yet)</p>

<ol>
  <li>the <strong>decoder</strong> receives as input the <strong>encoder encoder output sequence</strong> $c_i=c_i(h^e_1, …,h^e_t)$
    <ul>
      <li>essentially everything the <strong>encoder</strong> has got</li>
    </ul>
  </li>
  <li>different parts of the output sequence <strong>pay attention</strong> to <strong>different parts of the input sequence</strong>
    <ul>
      <li>essentially, given that we want to compute $h^d_t$, how much weight should be give to each input/stuff we know $h^e_{j}, \forall j$</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Simple Encoder-Decoder</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210185934352.png" alt="image-20220210185934352" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially:</p>

<ul>
  <li>
    <p>This context vector, $c_i$, is generated anew with <strong>each decoding step $i$</strong> and takes all of the encoder hidden states into account by a weight computed as:</p>

\[\alpha_{ij}  = \frac{\exp \left( \text{score}(h^d_{i-1},h^d_j ) \right)}{\sum_{k=1}^i \text{exp}( \text{score}(h^d_{i-1},h^d_j))}, \quad \forall j\]

    <p>which obviously <strong>depends on which output stage $i=t$ decoder is at</strong>. Then, simply do:</p>

\[c_i = \sum_{j} \alpha_{ij}h_j^e\]

    <p>being a weighted sum of all the encoder hidden states.</p>
  </li>
  <li>
    <p>then, obviously it is left for us to design a <strong>score function</strong> (e.g. dot product) that reflects the <strong>relevance between $h_{i-1}^d,h^e_{j}$</strong>, so that we could “pay more attention” to only specific $h_j^e$</p>
  </li>
</ul>

<p>Another more complicated example would be:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Encoder-Decoder with Bidirectional</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li>
    <p>the context vector is a function $c_i=c_i(o_1, …,o_t)$ instead, for $o_t= V\begin{bmatrix}h_t\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$.</p>
  </li>
  <li>
    <p>specifically, in this <strong>bidirectional RNN</strong> as encoder, the context vector is computed as:</p>

\[c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T\]

    <p>which is a <strong>weighted sum</strong> over <strong>all hidden states in the encoder</strong> (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:</p>

\[\alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j\]

    <p>which basically represents the <strong>amount of attention</strong> output $o_i$ will give to input word $x_j$, for some score function.</p>
  </li>
</ul>

<h2 id="word-embeddings">Word Embeddings</h2>

<p>For more details, please refer to the <strong>NLP notes</strong> on the section “Vector Semantics”.</p>

<p>The general idea is that:</p>

<ul>
  <li>we want to replace 1-hot word representation with lower dimensional feature vector for each word</li>
  <li>we can do this by learning word embedding from <strong>large unsupervised text corpus</strong>
    <ul>
      <li>e.g. distribution models, logistic regression models (e.g. Word2Vec), etc.</li>
    </ul>
  </li>
</ul>

<p>The goal is to find <strong>an embedding matrix $E$</strong> that takes a one-hot encoded word and returns an embedding. The general approach <strong>machine learning</strong> takes is to do the following:</p>

<ol>
  <li>input: each word in a vocabulary being a one-hot encoded vector</li>
  <li>initialize an embedding matrix $E$ randomly.</li>
  <li>for each sentence/window of words, <strong>mask one word out</strong></li>
  <li>now, given the context words $w_c$, our job is to output <strong>vector of probability</strong> $y$ for the masked word, so that $y[i]$ corresponds to the probability that word $i$ in the dictionary is the masked word
    <ol>
      <li>convert all words to $e_c = Ew_c$</li>
      <li>put it in a neural network, e.g. FFNN, to learn the weights and <strong>update the embedding matrix $E$</strong></li>
    </ol>
  </li>
  <li>After minimization, output $E$</li>
</ol>

<p>For instance, consider the following sentence in our training data:</p>

\[\text{Mary had a little lamb whose \textbf{fleece} was white as snow}\]

<p>We would like to mask the word <em>fleece</em> out. Then, we construct the following network:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210135056641.png" alt="image-20220210135056641" style="zoom:50%;" /></p>

<p>The idea is to <strong>learn by predicting the masked word</strong>, e.g. <em>fleece</em></p>

<ul>
  <li>what is the probability that the masked word (i.e. <em>fleece</em> is masked) given the current context words/sentences in the window</li>
  <li>$\theta$ will be the weights in the FFNN, which we don’t care about in the end</li>
  <li>since we are just masking words in a sentence, we are creating a supervised training set from <strong>unsupervised set</strong>!</li>
</ul>

<p>Once you trained your embedding matrix, you can even <strong>fine tune that to your specific task/vocabulary</strong>. For example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210135530591.png" alt="image-20220210135530591" style="zoom: 50%;" /></p>

<p>where input will use an embedding matrix $E$ that we have trained before</p>

<ul>
  <li>
    <p>then use the pre-trained embedding and update that during gradient descent as well</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ENCODER</span><span class="p">.</span><span class="n">get_vocabulary</span><span class="p">())</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="o">=</span><span class="n">LSTM_param</span><span class="p">[</span><span class="s">'units'</span><span class="p">],</span>
    <span class="n">embeddings_initializer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">EMBEDDING_MATRIX</span><span class="p">),</span> <span class="c1"># pretrained embedding
</span>    <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># whether if to fine tune
</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="beam-search">Beam Search</h2>

<p>The decoding algorithm we gave above for generating translations has a <strong>problem</strong> (as does the autoregressive generation we introduced in Chapter 9 for generating from a conditional language model:</p>

<ol>
  <li>at each time step in decoding, the output $y_t$ is chosen by computing a softmax over the set of possible outputs</li>
  <li>then, <strong>only</strong> the vocabulary with the <strong>highest probability</strong> is picked
    <ul>
      <li>this is also called <strong>greedy decoding</strong></li>
    </ul>
  </li>
</ol>

<p>Indeed, greedy search is not optimal, and <strong>may not find the highest probability translation</strong> as in the end we have a list of tokens.</p>

<p>For instance, consider the goal of <strong>generating a sentence</strong> from the decoder:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210200927842.png" alt="image-20220210200927842" style="zoom:67%;" /></p>

<p>where we see that:</p>

<ul>
  <li>the <strong>most probable sentence</strong> should be “ok ok <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>” with probability $0.4 \times 0.7 \times 1$</li>
  <li>the <strong>greedy</strong> algorithm would have picked <em>yes</em> as the first word, which is suboptimal</li>
</ul>

<p>Now, since this tree is growing as $t$ grows, this turns out to be an <strong>exhaustive search</strong> (dynamic programming will not work). This is obviously not good, so we instead consider the method called <strong>beam search</strong>:</p>

<ol>
  <li>instead of choosing the best token to generate at each timestep, we <strong>keep $k$ possible tokens at each step</strong></li>
  <li>At subsequent steps, <strong>each</strong> of the $k$ best hypotheses is extended incrementally by being passed to <strong>distinct decoders</strong>
    <ul>
      <li>i.e. if we have $k=2$ for the first level, then we will have <strong>2 distinct decoders</strong> for the next, as shown below</li>
    </ul>
  </li>
  <li>Each of these hypotheses is scored by $P(y_i \vert  x,y_{&lt;i})$, basically just multiplying the probability.</li>
  <li>Then, we <strong>prune the hypothesis</strong> down to the best $k$, so there are <strong>always only $k$ decoders</strong>.</li>
</ol>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210201708295.png" alt="image-20220210201708295" style="zoom:67%;" /></p>

<p>where we chose $k=2$ as an example:</p>

<ul>
  <li>at $t=2$, we have four hypothesis initially, but then we pruned down to $2$ again. Therefore, we always have only $k=2$ decoders at a time.</li>
</ul>

<p>More details on how we compute the probability score:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202257019.png" alt="image-20220210202257019" style="zoom:67%;" /></p>

<p>Then, we if use <strong>log probability</strong>, at each step, to compute the probability of a partial translation, we simply <strong>add</strong> the log probability of the prefix translation so far to the log probability of generating the next token:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202356694.png" alt="image-20220210202356694" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>here we <strong>terminated with 2 sentences of different length</strong>. This maybe problematic as sentences with shorter length will have a higher probability</li>
</ul>

<p>Therefore, we would consider some <strong>normalization</strong> against the length of sentences:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202626578.png" alt="image-20220210202626578" style="zoom: 80%;" /></p>

<p>for a sentence of length $T$.</p>

<p>This ends up with the following algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202738922.png" alt="image-20220210202738922" style="zoom:80%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>What do we do with the resulting $k$ hypotheses?</p>

  <ul>
    <li>In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that.</li>
    <li>In other cases our downstream application might want to look at all $k$ hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores.</li>
  </ul>
</blockquote>

<h2 id="encoder-decoder-with-transformers">Encoder-Decoder with Transformers</h2>

<p>The encoder-decoder architecture can also be <strong>implemented using transformers</strong> (rather than RNN/LSTMs) as the component modules</p>

<ul>
  <li>An <strong>encoder</strong> essentially consist of <strong>stacks of transformer blocks</strong>, typically $6$ layers</li>
  <li>A <strong>decoder</strong> as well consists of stacks of transformer blocks</li>
</ul>

<p>For instance, it could look like the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210212353425.png" style="zoom:67%;" /></p>

<p>where we notice that:</p>

<ul>
  <li>the only thing we don’t know is <strong>cross-attention</strong>, which is also very <strong>similar to the encoder-decoder attention</strong>, which we have covered before.</li>
  <li>the <strong>input of decoder</strong> is the auto-regressive output of itself generated</li>
</ul>

<hr />

<p><strong>A more detailed visualization at decoding</strong>:</p>

<p>Essentially for decode, we usually do it <strong>auto-regressively</strong> so that suppose our <mark>decoder</mark> has <code class="language-plaintext highlighter-rouge">Attention</code> with <code class="language-plaintext highlighter-rouge">MAX_SEQ_LEN=32</code>. Then:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/transformer_decoding_1.gif" style="zoom: 50%;" /></p>

<p>the input to decoder done <strong>auto-regressively</strong>:</p>

<ul>
  <li>at $t=0$, there is no input/or we have <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;</code> for filling the <code class="language-plaintext highlighter-rouge">32</code> sequence length and a positional embedding</li>
  <li>get cross attention from encoder output</li>
  <li>generate an output “<code class="language-plaintext highlighter-rouge">I</code>” and feed back as input at $t=1$. So we <strong>get <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;I</code> as the input of decoder</strong></li>
  <li>repeat until <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> is generated</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: Comparing with using a RNN based decoder, when we are generating output $o_{t+1}$, the <em>difference</em> is:</p>

  <ul>
    <li><strong>transformer</strong> based can <strong>only condition/read in</strong> $o_{t-31},…,o_t$ for a max sequence length of <code class="language-plaintext highlighter-rouge">32</code> for the attention layer</li>
    <li><strong>RNN</strong> based can use $h_t$ which <mark>encodes all previous information</mark>. However, it has <mark>no attention/is sequential</mark>.</li>
  </ul>
</blockquote>

<hr />

<p>To zoom in a bit, we see that</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210212608845.png" alt="image-20220210212608845" style="zoom: 67%;" /></p>

<p>Now, we have:</p>

<ul>
  <li>
    <p>an <strong>encoder</strong> that takes the source language input words $X=x_1, …., x_t$ and maps them to an output representation $H_{enc}=h_1,…,h_t$, as with the encoders discussed before</p>
  </li>
  <li>
    <p>a <strong>decoder</strong> then attends to the <strong>encoder representation</strong> and generates the target words one by one</p>

    <ul>
      <li>here, at each timestep conditioning on the source sentence (weighted by attention) and the previously generated target language words (see previous figure, essentially output $h^d_{t-1}$ is piped in as input)</li>
    </ul>
  </li>
  <li>
    <p><strong>Cross-attention</strong> has the same form as the <strong>multi-headed self-attention</strong> in a normal transformer block, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder. (i.e. the input/output is similar to the <strong>encoder-decoder attention</strong> discussed in <a href="#Attention">Attention</a>)</p>
  </li>
  <li>
    <p><strong>Self-attention</strong> in <strong>encoder</strong> is allowed to look ahead at the entire source language text, i.e. weights will be distributed on the entire input sequence</p>
  </li>
  <li>
    <p><strong>Casual Self-attention</strong> in <strong>decoder</strong> is what we have covered in the section <a href="#Self-Attention">Self-Attention</a>, so that we only have <strong>previous label inputs available</strong>:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" /></p>

    <p>(otherwise we will be cheating, as we are predicting the next word yet the next word is already available to us). This is sometimes also referred to as <strong>left-to-right attention</strong>, as it attends to input sequentially left to right.</p>
  </li>
</ul>

<p>Since the only thing “new” here is the cross attention, we focus on that. Essentially we need to consider what is $x,x_i,y_i$ analogue from the equation $\sum \alpha(x,x_i)\cdot y_i$ in this case.</p>

<ul>
  <li>
    <p>the <strong>query</strong>, i.e. $x$, will be sequence of <strong>the output of previous decoder layer</strong> $H^{dec[i-1]}$. We want to know the “answer” to this query</p>
  </li>
  <li>
    <p>the <strong>key, value</strong> will be the <strong>final output of encoder</strong> $H^{enc}=h_1 ,…,h_t$ (stuff we want to pay attention to)</p>
  </li>
</ul>

<p>Then, since it is similar to self-attention, we will have <strong>weights</strong> $W^Q,W^K,W^V$:</p>

\[Q=W^QH^{dec[i-1]};\quad K=W^KH^{enc};\quad V=W^VH^{enc}\]

<p>Then the rest is the same as the weights we calculated in self-attention:</p>

\[\text{CrossAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

<p>The cross attention thus allows the decoder to attend to each of the source language words with weights that depend on current focus (query).</p>

<h2 id="transformer-models">Transformer Models</h2>

<p>The big thing to distinguish between models would be:</p>

<ul>
  <li>encoder/decoder or both?</li>
  <li><strong>what is the optimization objective</strong>?
    <ul>
      <li>e.g. masked word to predict next, then likely a <strong>decoder only transformer</strong> (e.g. GPT)</li>
      <li>e.g. masked word/fill in blank prediction and next sentence prediction, then likely a <strong>encoder only transformer</strong> (e.g. BERT)</li>
    </ul>
  </li>
  <li><strong>what is the dataset</strong> that it trained on?</li>
</ul>

<h3 id="bert">BERT</h3>

<p>Let’s begin by introducing the <strong>bidirectional transformer</strong> encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020).</p>

<blockquote>
  <p>When applied to <strong>sequence classification</strong> and <strong>labeling problems</strong> causal models have obvious shortcomings since they are based on an incremental, <strong>left-to-right processing</strong> of their inputs.</p>

  <ul>
    <li>Bidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown below</li>
    <li>hence, it is not suitable for tasks such as predict the next word</li>
  </ul>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Causal Transformer</th>
      <th style="text-align: center">Bidirectional Transformer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210220737829.png" alt="image-20220210220737829" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210220745062.png" alt="image-20220210220745062" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially BERT is doing the one on the right.</p>

<ul>
  <li>as you shall soon notice, BERT is a <strong>encoder only transformer</strong></li>
</ul>

<p>Essentially, inside BERT we have essentially stacks of transformer blocks for encoder/decoder:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221236252.png" alt="image-20220210221236252" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p><strong>bidirectional</strong> encoders use self-attention to map sequences of <strong>input</strong> embeddings $x_1,…,x_n$ to sequences of <strong>output</strong> embeddings $y_1,…,y_n$ of the same length. Again, the aim is to produce <strong>contextualized vectors</strong> that takes in information from the entire input sequence</p>
  </li>
  <li>
    <p>the contextualization in <strong>self-attention</strong> is one in the same manner covered in section <a href="#Self-Attention">Self-Attention</a>, so the formulas are still:</p>

\[\text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

    <p>for input $X$, and <strong>embedding matrix</strong> $W^K,W^Q,W^V$ will be learned.</p>

\[Q = XW^Q; K = XW^K; V=XW^V\]

    <p>However, the difference is that for <strong>left-to-right</strong> models, we had the matrix $QK^T$ being:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Left-To-Right</th>
          <th style="text-align: center">Bidirectional</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221709470.png" alt="image-20220210221709470" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221644852.png" alt="image-20220210221644852" /></td>
        </tr>
      </tbody>
    </table>

    <p>so basically we removed the mask</p>
  </li>
  <li>
    <p>Beyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models</p>
  </li>
</ul>

<p>However, this is very useful because it is a multi-purpose model. best results in <strong>many different tasks!</strong></p>

<ul>
  <li><strong>Inputs</strong> to the model are segmented using <strong>subword tokenization</strong> and are combined with <strong>positional embeddings</strong>. To be more precise, recall that for transformers, <mark>input dimension and output dimension are the same</mark>:
    <ul>
      <li>The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings.</li>
      <li>BERT uses <mark>Wordpiece embeddings</mark> for tokens</li>
    </ul>
  </li>
  <li>always remember, essentially what those model learn are <strong>weights</strong> $W$.</li>
</ul>

<h4 id="training-bert">Training BERT</h4>

<p>Since the entire context is available instead of trying to predict the next word, the model learns to perform a <strong>fill-in-the-blank</strong> task, technically called the <strong>cloze task</strong>.</p>

<ul>
  <li>in fact, a lot of the models are trained in this format. Vision Transformer also does it by masking patches in the image and predicting the patch.</li>
</ul>

<p>For instance, <mark>instead of</mark> doing:</p>

\[\text{Please turn your homework \_\_\_\_}.\]

<p>For bidirectional transformers:</p>

\[\text{Please turn \_\_\_\_ homework in}.\]

<p>Therefore, the idea is that:</p>

<ul>
  <li>during training the model is <strong>deprived of one or more elements of an input sequence</strong> and must generate/<strong>output a probability distribution</strong> over the vocabulary for <strong>each of the missing items</strong></li>
  <li>then, it is essentially again a supervised training from self-supervised data, where loss would simply be <strong>cross entropy.</strong></li>
</ul>

<p>For example, an example architecture would look like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210222426656.png" alt="image-20220210222426656" style="zoom:67%;" /></p>

<p>so we see that:</p>

<ul>
  <li>
    <p>a token part of a training sequence is used in one of three ways:</p>

    <ul>
      <li>It is replaced with the unique vocabulary token <code class="language-plaintext highlighter-rouge">[MASK]</code>.</li>
      <li>It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.</li>
      <li>It is left unchanged.</li>
    </ul>

    <p>For BERT, 80% are masked, 10% replaced with randomly selected tokens, and 10% are unchanged</p>
  </li>
  <li>
    <p>training objective is to <strong>predict the original inputs</strong> for each of the masked tokens using a bidirectional encoder</p>
  </li>
  <li>
    <p>To produce a probability distribution over the vocabulary for each of the masked tokens, the <strong>output</strong> vector from the final transformer layer for <strong>each of the masked tokens</strong> is:</p>

\[y_i = \text{Softmax}(W_Vh_i)\]

    <p>for some weights $W_V$ which will be learnt.</p>
  </li>
</ul>

<h4 id="transfer-learning-through-fine-tuning">Transfer Learning through Fine-Tuning</h4>

<p>The idea of pretrained model is simple. Consider a pretrained BERT (so that weights such as $W^K,W^V,W^Q$ are already learnt), and you want to use it to do tasks such as <strong>sequence classification</strong></p>

<ul>
  <li>e.g. sentiment analysis</li>
</ul>

<p>The kind of question you need to think about is:</p>

<ul>
  <li>transformer based encoders have <strong>output shape same as input shape</strong>. Therefore, essentially we can get embeddings for each <strong>word</strong> in the context. But here we would like to obtain an <strong>embedding of the entire sequence</strong> for classification. Where do we get that?</li>
  <li>Given an embedding of entire sequence, what should we do next?</li>
  <li>Can we <strong>fine tune weights from pretrained models</strong> as well?</li>
</ul>

<p>In essence, this is what happens:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210224802368.png" alt="image-20220210224802368" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>We need an additional vector is added to the model to stand for the <strong>entire sentence sequence</strong>. This vector is sometimes called the <strong>sentence embedding</strong>. In BERT, the <code class="language-plaintext highlighter-rouge">[CLS]</code> token plays the role of this embedding. (i.e. the embedding of this token is what we want)
    <ul>
      <li>This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding</li>
    </ul>
  </li>
  <li>
    <p>onece we have this <strong>sentence embedding</strong>, we can pipe this into our own <strong>neural network/logistic regression classifier</strong>, which basically learns a weight $W_c$. Then, our output for <strong>classification</strong> essentially comes from:</p>

\[y = \text{Softmax}(W_C y_{cls})\]
  </li>
  <li>we can also <strong>update the pretrained weights</strong> during our fine-tuning as well: In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often <strong>limited to updates over the final few layers</strong> of the transformer.</li>
</ul>

<hr />

<p><em>For Example</em>: Sequence Labeling</p>

<p>To demonstrate the multipurposeness of BERT, consider the task of <strong>part-of-speech tagging</strong> or <strong>BIO-based named entity recognition</strong>. Since we know that given a sequence of words, BERT produces a sequence of <strong>embeddings</strong>:</p>

<ul>
  <li>the final output vector corresponding to <strong>each input token embedding</strong> is passed to a classifier (since each token will be tagged)</li>
  <li>then, we just need to <strong>learn the weights in the classifier</strong> $W_K$</li>
</ul>

<p>So essentially:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210225529070.png" alt="image-20220210225529070" style="zoom:67%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>notice that the embedding for <code class="language-plaintext highlighter-rouge">CLS</code> is useless in this case</p>
  </li>
  <li>
    <p>our output tag for each token will come from:</p>

\[y_i = \text{Softmax}(W_K z_i)\]

    <p>for output from BERT will be $z_i$, and then we can <strong>greedily assign tags as</strong>:</p>

\[t_i = \arg\max_k(y_i)\]

    <p>for a tag consist of $k$ classes.</p>
  </li>
</ul>

<h3 id="gpt-2">GPT-2</h3>

<p>The GPT-2 is built using <strong>transformer decoder blocks</strong>. BERT, on the other hand, uses <strong>transformer encoder blocks</strong>.</p>

<p>We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, <strong>outputs one token at a time</strong>:</p>

<p><img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif" alt="img" style="zoom:50%;" /></p>

<p>notice that:</p>

<ul>
  <li>The way these models actually work is that after each token is produced, that <strong>token is added to the sequence of inputs</strong>. And that new sequence becomes the input to the model in its next step. This is an idea called “<strong>auto-regression</strong>”</li>
</ul>

<p>Therefore, essentially the model contains:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232055303.png" alt="image-20220210232055303" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>unlike BERT which uses bidirectional transformer as it wants to <strong>encode contextualized information</strong>, GPT cares more about <strong>next word prediction</strong>, hence it reverted to the <strong>masked/left-to-right self-attention</strong> during training:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232238810.png" alt="image-20220210232238810" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>alike encoders of course, we need some <strong>input to start with</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232603576.png" alt="image-20220210232603576" style="zoom:50%;" /></p>

    <p>it turns out that essentially you would also have input <strong>embedded</strong> before putting into your model.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>If you compare the architecture of encoder and decoder, they are kind of similar:</p>

  <table>
    <thead>
      <tr>
        <th style="text-align: center">Encoder Only</th>
        <th style="text-align: center">Decoder Only</th>
        <th style="text-align: center">Encoder-Decoder</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233157758.png" alt="image-20220210233157758" style="zoom: 67%;" /></td>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233050700.png" alt="image-20220210233050700" /></td>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233259239.png" alt="image-20220210233259239" style="zoom:67%;" /></td>
      </tr>
    </tbody>
  </table>

  <p>where:</p>

  <ul>
    <li>both encoder and decoder are transformer blocks, so <strong>output shape same as input shape</strong> (applications bascially only do something to the output of the last layer)</li>
    <li>encoder and decoder used alone have a slightly different attention mechanism, with the latter being usually a <strong>masked/left-to-right attention</strong></li>
    <li>when you have encoder+decoder, you then need a <strong>cross-attention</strong> to get stuff across from encoder to decoder</li>
    <li>the <strong>output embedding</strong> in the decoder is essentially the auto-regressive genreated output. i.e. at $t=0$, it is fed with <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and suppose it generated <code class="language-plaintext highlighter-rouge">hello</code>. Then at $t=1$, fed with <code class="language-plaintext highlighter-rouge">&lt;s&gt;, hello</code>.</li>
  </ul>
</blockquote>

<h3 id="efficient-transformers">Efficient Transformers</h3>

<p>One problem now is that <strong>attention computation</strong> is quadratic in the length of sentences.</p>

<p><strong>Solution</strong>: linear time transformer by approximations, transfer learning</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210234034616.png" alt="image-20220210234034616" style="zoom:67%;" /></p>

<h1 id="generative-adversarial-network">Generative Adversarial Network</h1>

<p>Generative adversarial networks (GANs) are an <strong>unsupervised generative model</strong> used for <mark>generating samples</mark> that are similar to training examples. GANs have also been used for generating images from text, generating audio from images, and generating images from audio, etc.</p>

<ul>
  <li>we are having a generator that <strong>learns the structure of the data</strong></li>
</ul>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>Essentially you have <strong>two models</strong>, a generator and a discriminator:</p>

  <ul>
    <li><strong>generator</strong> is trained to produce fake examples that fool the discriminator, hence its loss is based on the success of  discriminator</li>
    <li><strong>discriminator</strong> learns to distinguish between the generator’s fake synthesized samples and real data</li>
  </ul>

  <p>Therefore, you will have essentially a <strong>minimax optization problem</strong> that looks like:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]

  <p>where essentially:</p>

  <ul>
    <li>discriminator outputs a <strong>probability</strong> (that it is real) given some data $x$, and generator <strong>generates</strong> some data $G(z)$ from <strong>noise</strong> $z$ from some distribution (e.g. Gaussian)</li>
    <li>the loss says discriminator $D(x)$ should be as high as possible, so that discriminator is doing well, and $D(G(z))$ should be as high as possible as well, so that generator is doing well</li>
    <li>The generator and the discriminator are trained alternately, for $G(z)$ being descent first, and then $D(x)$.</li>
  </ul>

  <p>Then, essentially we update the two models with the two losses:</p>

  <ul>
    <li>
      <p>For <strong>generator</strong>, the the gradient step is:</p>

\[\nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}\]

      <p>in some other algorithm, we could maximize $\log D(G(z))$ instead</p>
    </li>
    <li>
      <p>For <strong>discriminator</strong>, we will have the same as maximization problem as before, so the gradient step is:</p>

\[\nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}\]
    </li>
  </ul>

  <p>In practice</p>

  <ul>
    <li>
      <p>once we finished training, we will take the generator to deploy in our various appications.</p>
    </li>
    <li>
      <p>relevant problems include <strong>saturation problem</strong>, <strong>mode collapse</strong> and <strong>convergence problem</strong>, which will be discussed in more detail later.</p>
    </li>
  </ul>
</blockquote>

<p>Note that:</p>

<ul>
  <li>The idea of minimax is not new. In game theory, we think about <em>minimizing</em> the loss involved when the <strong>opponent</strong> selects the strategy that gives <em>max</em>imum loss</li>
  <li>from a biological point of view, this is essentially co-evolution</li>
  <li>The discriminator and generator form two dueling networks with opposite objectives. This <strong>unsupervised</strong> setting eliminates the need for labels, since the <strong>label is whether the sample is real or not</strong>.</li>
</ul>

<p>On a bigger context, since we are trying to learn the given data distribution:</p>

<ul>
  <li>we can learn it <strong>explicity</strong>
    <ul>
      <li>VAE</li>
      <li>Markvo Chain</li>
    </ul>
  </li>
  <li><strong>implicitly</strong>
    <ul>
      <li>GAN</li>
    </ul>
  </li>
</ul>

<h2 id="minimax-optimization">Minimax Optimization</h2>

<p>Graphically, this is what the architecture looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 67%;" /></p>

<p>the joint optimization problem for the two agents is:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]

<p>where we are taking <strong>expected value</strong> over real data, but in <strong>practice</strong> we can only do empirical mean</p>

<ul>
  <li>The term $D(x)$ is the discriminator’s estimated probability that <strong>(real) data $x$ is real</strong>
    <ul>
      <li>The goal of the discriminator is to classify correctly real and generated data.</li>
    </ul>
  </li>
  <li>$G(z)$ is the output of the generator given random noise $z$.
    <ul>
      <li>the goal is to generate a signal from random noise $z \sim P(z)$ in a way that it will be difficult for the discriminator to distinguish</li>
    </ul>
  </li>
  <li>$D(G(z))$ is the discriminator’s estimated probability that a <strong>synthesized sample is real</strong>.
    <ul>
      <li>the goal of geneartor is to make $D(G(z)) \to 1$</li>
    </ul>
  </li>
</ul>

<p>Since we care about the <strong>network parameters</strong> in the end:</p>

\[\min_\phi\max_\theta V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]\]

<p>then <strong>how do we optimize this</strong>?</p>

<ul>
  <li>Since both the generator and discriminator will be represented by neural networks, problem is non-convex and non-concave</li>
  <li>hence we can only do <strong>gradient descent type solution</strong></li>
  <li>The first term of $\log D(x)$ is independent of the generator and therefore the <strong>generator only minimizes</strong> the function $\log(1 − D(G(z)))$.</li>
</ul>

<p>However, there is a problem with this loss. In practice, it saturates for the generator, meaning that the generator quite frequently <strong>stops training</strong> if it doesn’t catch up with the discriminator, so that:</p>

\[\nabla_\phi \log(1-D_\phi(G_\theta(z))) \to 0\]

<p>when the <strong>discriminator becomes optimal</strong> ($D$ is close to $D^*$). So gradient vanishes and <strong>generator</strong> cannot evolve!</p>

<h3 id="mode-collapse">Mode Collapse</h3>

<p>This issue is on the unpredictable side of things. It wasn’t foreseen until someone noticed that the generator model could only generate one or <strong>a small subset of different outcomes</strong> or modes, instead of a range of outputs.</p>

<blockquote>
  <p>If at each iteration the generator optimizes for the specific discriminator and the <strong>discriminator is not able to correctly classify the synthesized samples</strong> as fake, the generator will synthesize a small set of samples, not diverse samples, which is known as <strong>mode collapse</strong></p>
</blockquote>

<p>This may happen when:</p>

<ol>
  <li>In the process of training, the <strong>generator is always trying to find the one output</strong> that seems most plausible to the discriminator.</li>
  <li>if the <strong>next generation of discriminator gets stuck</strong> in a local minimum and doesn’t find its way out by getting its weights even more optimized, it’d get easy for the next generator iteration to find the most plausible output for the current discriminator.</li>
  <li>This way, it will keep on repeating the same output and refrain from any further training.</li>
</ol>

<p>When the discriminator using <strong>Wasserstein loss</strong> does not get stuck in local minima it learns to reject the generator’s repeated synthesized samples, encouraging the generator to synthesize new samples and diversify.</p>

<p>One solution is that:</p>

<ul>
  <li>In order to void mode collapse, encouraging the generator to diversify the synthesized samples and not optimize for a constant discriminator, the <strong>generator</strong> loss function may be modified to include <strong>multiple subsequent discriminators</strong></li>
</ul>

<h3 id="convergence-problem">Convergence Problem</h3>

<p>The utopian <strong>situation where both networks stabilize and produce a consistent result is hard to achieve in most cases.</strong></p>

<ul>
  <li>If the generator succeeds all the time, the discriminator has a 50% accuracy, similar to that of flipping a coin. This poses a threat to the convergence of the GAN as a whole.</li>
</ul>

<p>If discriminator is just randomly guessing, then as the <strong>discriminator’s feedback loses its meaning</strong> over subsequent epochs by giving outputs with equal probability, the <strong>generator may deteriorate its own quality</strong> if it continues to train on these junk training signals.</p>

<h2 id="divergence-between-distributions">Divergence between Distributions</h2>

<blockquote>
  <p><strong>Key Question</strong>:</p>

  <p><strong>Why</strong> did we choose a loss function in the form of:</p>

\[\begin{align*}
V(D_\phi,G_\theta)
&amp;=\mathbb{E}_{x \sim P_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim P_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]\\
&amp;=\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]
\end{align*}\]

  <p>specifically, why using $\log$ probability?</p>

  <p>The short <strong>answer</strong> is:</p>

  <ul>
    <li>loss function here represents the <strong>distance</strong> between the <strong>distribution</strong> of the synthesized samples and the distribution of the real data</li>
    <li>so essentially we want to <strong>minimize the difference</strong> between our approximate distribution $P_G$ from generator and the true distribution $P_{data}$</li>
  </ul>

  <p>In fact, if $D_\theta(x)$ can be any function, it can be shown that the above is <strong>equivalent to minimizing the JS divergence</strong> between <strong>distribution $P_{data}$ and $P_{G}$</strong>.</p>

  <ul>
    <li>for more detail, checkout https://colinraffel.com/blog/gans-and-divergence-minimization.html</li>
  </ul>
</blockquote>

<p>Essentially, our goal is to <strong>minimize the distribution divergence</strong> between true distribution $P_{data}$ and our generated one $P_G$. To do this, we need a metric to measure <strong>“distance/difference”</strong> between two distributions, and the simplest one would be the KL divergence.</p>

<h3 id="kl-divergence">KL Divergence</h3>

<p>KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data, i.e. how many <strong>bits do we need to specify the data</strong>. This idea is captured in Entropy:</p>

\[H = -\sum_{i=1}^N p(x_i) \cdot \log p(x_i)\]

<p>where:</p>

<ul>
  <li>if $\log_2$, then we can interpret entropy as “the minimum number of bits it would take us to encode our information”.</li>
  <li>just looking at extreme values, if $p(x_i)=0$ or $p(x_i)=1$, then there is nothing random and entropy is $0$. For $p(x_i)=0.5$, entropy is large.</li>
</ul>

<p>Then, KL divergence is basically a measure of “<strong>how many bits of information we expect to lose</strong>” if we use $q(x_i)$ to approximate $p(x_i)$:</p>

\[D_{KL}(p||q) = \sum_{i=1}^N p(x_i)\cdot (\log p(x_i) - \log q(x_i))=\sum_{i=1}^N p(x_i)\cdot \log\frac{p(x_i)}{q(x_i)}\]

<p>Then:</p>

<ul>
  <li>
    <p>this is essentially the same as saying:</p>

\[D_{KL}(p||q)=\mathbb{E}[\log p(x) - \log q(x)]\]

    <p>which reminds us of the <strong>original GAN loss</strong>.</p>
  </li>
  <li>
    <p>for a <strong>continuous distribution</strong>:</p>

\[D_{KL}(p||q)=\int p(x)\cdot \log\frac{p(x)}{q(x)}\,dx\]
  </li>
  <li>
    <p>a <strong>reversed KL divergence</strong> is then:</p>

\[D_{KL}(q||p)=\int q(x)\cdot \log\frac{q(x)}{p(x)}\,dx\]

    <p>for $q$ being our approximate distribution.</p>
  </li>
  <li>
    <p>therefore, this means KL divergence is <strong>non-negative and asymmetric</strong></p>
  </li>
</ul>

<p>The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$</p>

<h3 id="jensen-shannon-divergence">Jensen-Shannon Divergence</h3>

<p>The Jensen-Shannon (JS) divergence $D_{JS}$ is a symmetric smooth version of the KL divergence defined by:</p>

\[D_{JS}(p||q) = \frac{1}{2}D_{KL}(p||m)+\frac{1}{2}D_{KL}(q||m),\quad m=\frac{1}{2}(p+q)\]

<p>The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$. In fact, this formulation of divergence metric leads to the <strong>formulation of loss</strong>:</p>

\[\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]\]

<h3 id="bregman-divergence">Bregman divergence</h3>

<p>The KL divergence and JS divergence are both <strong>special cases of the Bregman divergence</strong>.</p>

<p>The Bregman divergence is defined by a convex function F and is a <strong>measure of distance between two points $p$ and $q$</strong> defined by:</p>

\[D_F(p,q)= F(p)-F(q)-\lang \nabla F(q), p-q \rang\]

<p>where then, many <strong>divergence metrics</strong> come from defining some <strong>convex function $F$</strong>:</p>

<ul>
  <li>
    <p>if $F(p)=p\log p$, then we recover the <strong>generalized KL divergence</strong>:</p>

\[\begin{align*}
D_F(p,q)
&amp;= p\log p - q \log q - (\log q + 1)(p-q)\\
&amp;= p \log(\frac{p}{q}) + (q-p)
\end{align*}\]
  </li>
  <li>
    <p>if $F(p)=p\log p - (p+1)\log(p+1)$, we get <strong>JS divergence</strong>.</p>
  </li>
  <li>
    <p>if $F=(1-p)^2$, we get a <strong>Pearson $\chi^2$ divergence</strong>, which leads to <mark>loss in least square GAN</mark>.</p>

    <p>The loss for discriminator being:</p>

\[\mathbb{E}_{x \sim P_{data}}[D_\phi(x)^2]-\mathbb{E}_{z \sim p_{z}(z)}[D_\phi(G_\theta(z))^2]\]

    <p>and the loss for geneator:</p>

\[\mathbb{E}_{z \sim p_{z}(z)}[(D_\phi(G_\theta(z))-1)^2]\]

    <p>providing a smoother loss</p>
  </li>
</ul>

<h3 id="optimal-objective-value">Optimal Objective Value</h3>

<p>Now, we come back to our discussion of the original loss function. Since the original loss is essentially <strong>minimizing JS divergence</strong> between $P_{data}$ and $P_{G}$, we obviously want the optimal solution to be $P_G=P_{data}$.</p>

<ul>
  <li>in practice, this is <strong>not computable</strong> since it involves knowing the true distribution. Therefore, in practice we just do <strong>gradient descent type updates</strong>.</li>
</ul>

<p>Here, we show that the above statement is true. For <strong>discriminator</strong>, we want:</p>

\[\max_\phi V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{\hat{x}\sim p_G}[\log (1-D(\hat{x}))]\]

<p>Hence:</p>

\[\mathcal{L}(x;\theta) = P_{data}\cdot \log D(x) + P_G \cdot \log(1-D(\hat{x}))\]

<p>taking derivative and setting it to zero, we obtain:</p>

\[D^*(x)=\frac{P_{data}}{P_{data}+P_G}\]

<p>Then plugging this back in to solve for <strong>generator</strong>:</p>

\[\min_G  V(G,D^*) = 2D_{JS}(P_{data}||P_G)-2\log 2\]

<p>Then if you plugin to the equation for JS divergence, you will realize that this is <strong>minimized</strong> if $P_{data}=P_G$.</p>

<h2 id="gradient-descent-ascent">Gradient Descent Ascent</h2>

<p>Since in reality we don’t know the true distribution, we only have samples from it, we can only <strong>update our guesses</strong> in a gradient descent type of manner as the overall loss is non-convex and non-concave.</p>

<p>In our setting we use a stochastic variant of GDA with mini-batches, in which the <mark>descent</mark> update for the <strong>generator</strong> neural network is:</p>

\[\nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}\]

<p>so that we have performed the <strong>$\min_G$</strong> step, and then our <strong>discriminator</strong> will have an <mark>ascent</mark> update:</p>

\[\nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}\]

<p>note that:</p>

<ul>
  <li>
    <p>If $V$ were convex-concave then playing the game simultaneously or in a sequential order would not matter; however, in our case $V$ is non-convex non-concave and the <strong>order matters</strong></p>
  </li>
  <li>
    <p>Unfortunately, GDA may converge to points that are not local minimax or fail to converge to a local minimax. A modification of GDA (Wang, Zhang &amp; Ba 2020) which partially addresses this issue is:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184103651.png" alt="image-20220211184103651" style="zoom:67%;" /></p>

    <p>which is a modified gradient ascent rule for $f=V$​. This converges and <mark>only converges to local minimax points</mark>, driving the gradient quickly to zero and improving GAN convergence</p>
  </li>
</ul>

<h3 id="optimistic-gradient-descent-ascent">Optimistic Gradient Descent Ascent</h3>

<p>When introduced, GANs were implemented using momentum. However, later on the implementations did not use momentum, and using a <strong>negative momentum made the saturating GAN work</strong>. An algorithm which solves the minimax optimization problem by using negative momentum is <strong>optimistic gradient descent ascent (OGDA)</strong> (Daskalakis et al. 2017).</p>

<ul>
  <li>this is fonud in empirical experiences</li>
</ul>

<p>The negative momentum update is:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184326881.png" alt="image-20220211184326881" style="zoom: 67%;" /></p>

<p>where $f=V$ in our case.</p>

<ul>
  <li>the one with $x$ is for gradient descent, the one with $y$ is for gradient ascent</li>
  <li>OGDA yields better empirical results than GDA, and can be interpreted as an approximation of the proximal point method</li>
</ul>

<h2 id="gan-training">GAN Training</h2>

<p>When the generator training is successful the discriminator cannot distinguish between real data and fake samples synthesized
by the generator. So baically the architecture is as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 50%;" /></p>

<p>where both generator and discriminator are represented by neural networks and are both trained by backpropagation.</p>

<p>The algorithm for training is as follows, essentially we train them <mark>alternatingly</mark></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184827501.png" alt="image-20220211184827501" style="zoom:50%;" /></p>

<p>where the end goal is <strong>we get good quality images from the generator</strong> and discriminator won’t be able to differentiate between real and fake images.</p>

<ul>
  <li>Training the <strong>discriminator</strong> uses real data as positive examples and samples synthesized by the generator as negative examples
    <ul>
      <li>When the discriminator is trained, the <mark>generator is not trained</mark> and its parameters are held fixed</li>
      <li>discriminator loss serves as a signal to the generator for updating its parameters by backpropagation</li>
    </ul>
  </li>
  <li>the <strong>generator</strong> learns to synthesize realistic samples by the feedback it receives from the discriminator
    <ul>
      <li>During generator training the <mark>discriminator parameters are held fixed</mark>.</li>
    </ul>
  </li>
  <li>we could have also reversed the training to train generator first, but the result will be difference as order matters.</li>
</ul>

<h2 id="gan-losses">GAN Losses</h2>

<p>As described, different Bregman divergences and loss functions have been explored with the goals of improving GAN training stability and diversity. Here we describe a few that is important.</p>

<h3 id="wasserstein-gan">Wasserstein GAN</h3>

<p>One problem with JS divergence is that, if the real data distribution and generator distribution do not overlap then the JS divergence is zero $D_{JS} = 0$</p>

<ul>
  <li>when distributions have non-overlapping support</li>
  <li>(support of a function is a <strong>closure</strong> $S={x \in \mathbb{R}^n:f(x) \neq 0}$)</li>
</ul>

<p>Graphically:</p>

<p><img src="https://i.stack.imgur.com/NbLrO.png" alt="enter image description here" style="zoom: 40%;" /></p>

<p>Fortunately, this issue has been <strong>resolved</strong> by using the Earth Mover’s Distance (EMD) or <strong>Wasserstein-1 distance</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211200143137.png" alt="image-20220211200143137" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>where $\gamma$ denotes how much mass, or earth, must be moved from $x$ to $y$ in order to transform distribution $P$ into distribution $Q$,</li>
  <li>$\Pi (P,Q)$ denotes the set of all disjoint distributions with marginals $P$ and $Q$.</li>
</ul>

<p>Graphically, we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215142125264.png" alt="image-20220215142125264" style="zoom:33%;" /></p>

<p>where $P_r$ is the real distribution, and the distance between $P_r,P_\theta$ is to measure how many “piles of dirt” dow we need to shovel from $P_r\to P_\theta$ so that the distributions are the same.</p>

<p>Computing $W(P,Q)$ is <strong>intractable</strong> since it requires considering all possible combinations of pairs of points between the two distributions, computing the mean distance of all pairs in each combination, and taking the minimum mean distance across all combinations.</p>

<p>Fortunately, an alternative is to <strong>solve a dual maximization problem</strong> that is tractable, which results in the <mark>Wasserstein loss</mark>:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[ D(x)]-\mathbb{E}_{z \sim p_{z}(z)}[D(G(z))]\]

<p>if you use WGAN, which is Wasserstein loss:</p>

<ul>
  <li>this loss outputs <strong>a real value</strong> that is <strong>larger for real data</strong> than <strong>synthesized</strong> samples
    <ul>
      <li>as compared to the original GAN uses the minimax loss in which the discriminator outputs a probability in $[0, 1]$ of a sample being real or synthesized</li>
      <li>therefore, the WGAN discriminator is called a critic since it does not output values in $[0, 1]$ for performing classification</li>
    </ul>
  </li>
  <li>There is no sigmoid in the final layer of the discriminator and the range is $[−\infty,\infty]$.</li>
</ul>

<p>Then, this means that the <strong>generator</strong> loss functions is:</p>

\[\min_G -\mathbb{E}_{z \sim p_{z}(z)}[D(G(z)]\]

<p>The <strong>discriminator</strong> loss function is the entire loss.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Recall that mode collapse happens when a <strong>discriminator hits a local minium</strong>. Then, if it fails to reject the fake sample from generator, <strong>geneartor would only generate a subset of the distribution</strong>, which is mode collapse.</p>

  <p>However, Wasserstein loss <mark>prevents discriminator from hitting a local minimum</mark>, hence allowing it to learn to escape.</p>
</blockquote>

<h3 id="loss-function-summaries">Loss Function Summaries</h3>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215140041605.png" alt="image-20220215140041605" style="zoom: 50%;" /></p>

<h2 id="gan-architectures">GAN Architectures</h2>

<p>Firs, an example of the choice of generator/discriminator</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215140610509.png" alt="image-20220215140610509" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>this would be an example of generator (left to right). Discriminator <em>could</em> be the same architecture but left to right</li>
  <li>therefore, the generator above is also called a <strong>transposed CNN</strong></li>
</ul>

<hr />

<p>Below are some nowadays application of the GAN idea</p>

<ul>
  <li>
    <p><strong>Progressive GAN</strong>: A coarse-to-fine approach for training allows generating images at increasing resolutions.</p>

    <ul>
      <li>training the generator and discriminator using low-resolution images and incrementally add layers of higher-resolution images during training.</li>
    </ul>
  </li>
  <li>
    <p><strong>Deep Convolutional GAN</strong>: A GAN that uses convolutional neural networks (CNNs) as the generator and discriminator.</p>

    <ul>
      <li>using a CNN as the discriminator network and a deconvolution neural network as the generator</li>
    </ul>
  </li>
  <li>
    <p><strong>Semi-Supervised GAN</strong>: Instead of having the discriminator be a binary classifier for real or fake samples, in a semi-supervised GAN (SGAN) the discriminator is a multi-class classifier</p>

    <ul>
      <li>recall that all the GANs are normally unsupervised, as labels is just synthetic images or real ones</li>
      <li>The discriminator outputs the likelihood of sample to be synthesized or real, and if the sample is <em>classified as real</em> then the discriminator outputs the <em>probability of the $k$ classes</em>, estimating to which class the sample belongs</li>
    </ul>
  </li>
  <li>
    <p><strong>Conditional GAN</strong>: models the conditional probability distribution $P(x\vert y)$ by training the generator and discriminator on <em>labeled</em> data, with labels being $y$.</p>

    <ul>
      <li>
        <p>Replacing $D(x)$ with $D(x\vert y)$ and $G(z)$ with $G(z\vert y)$ for the loss, you get conditional GAN</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z|y)))]\]
      </li>
      <li>
        <p>Providing labels allows us to <mark>synthesize samples in a specific class</mark> or with a specific attribute, providing a level of control over synthesis</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Pix2Pix:</strong> Image-to-Image Translation</p>

    <ul>
      <li>
        <p>Given a training set of unfiltered and filtered image pairs $A : A’$ and a new unfiltered image $B$ the output is a filtered image $B’$ such that the analogy $A:A’::B : B’$ is <strong>maintained</strong></p>
      </li>
      <li>
        <p>An input image is mapped to a synthesized image with different properties. The loss function is a <strong>combination of the conditional GAN loss</strong> with an additional loss term which is a <strong>pixel-wise loss</strong> (i.e. sum of loss per pixel on the image) that encourages the generator to match the source image:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211203343602.png" alt="image-20220211203343602" style="zoom:67%;" /></p>

        <p>with weight $\lambda$</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Cycle Consistent GAN</strong>: learns unpaired image-to image translation using GANs <em>without pixel-wise correspondence</em></p>

    <ul>
      <li>
        <p>The training data are image sets $X \in A$ and $Y \in A’$ from two different domains $A$ and $A’$ <strong>without pixel-wise correspondence</strong> between the images in $X$ and $Y$.</p>
      </li>
      <li>
        <p>CycleGAN consists of</p>

        <ul>
          <li>two generators $G(X) = \hat{Y}$ and $F(Y ) = \hat{X}$</li>
          <li>two discriminators $D_Y$ and $D_X$.</li>
        </ul>

        <p>The generator $F$ maps a real image $X$ to a synthesized sample $\hat{Y}$ and the discriminator $D_Y$ compares between them</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204326281.png" alt="image-20220211204326281" style="zoom:50%;" /></p>
      </li>
      <li>
        <p>CycleGAN maintains two approximate cycle consistencies:</p>

        <ul>
          <li>The first cycle consistency  $F(G(X)) \approx X$ approximately maintains that mapping a real image $X$ to a synthesized image $\hat{Y}$ and back is similar to $X$</li>
          <li>the second cycle consistency $G(F(Y )) \approx Y$ approximately maintains that mapping a real image $Y$ to a synthesized image $\hat{X}$ and back is similar to $Y$.</li>
        </ul>

        <p>Graphically, this is one of the consistency loop</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215143023657.png" alt="image-20220215143023657" style="zoom: 50%;" /></p>
      </li>
      <li>
        <p>The overall loss function is defined by (Zhu et al. 2017):</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204534646.png" alt="image-20220211204534646" style="zoom:67%;" /></p>

        <p>where the cycle consistency loss is defined by:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204602542.png" alt="image-20220211204602542" style="zoom: 67%;" /></p>

        <p>which is weighted by $\lambda$</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="evaluation-and-application">Evaluation and Application</h2>

<p>After training the generator so that discriminator cannot do any better, there is still the chance that <strong>generated samples are garbage</strong> if discriminator is garbage. Therefore, we have some metrics to <strong>evaluate how well generator’s end product</strong> is doing.</p>

<p>The Inception Score (IS) and Frechet Inception Distance (FID) measure the <strong>quality of synthesized examples</strong> using <strong>pre-trained</strong> neural network classifiers</p>

<ul>
  <li><strong>Inception Score</strong>: The Inception Score (IS) (Salimans et al. 2016) automatically evaluates the quality of images synthesized by the generator by using the <strong>pre-trained Inception v3 model</strong> (Szegedy et al. 2016) for classification.
    <ul>
      <li>A <strong>higher</strong> Inception Score is <strong>better</strong>, which corresponds to a larger KL divergence between the distributions.</li>
    </ul>
  </li>
  <li><strong>Frechet Inception Distance</strong>: The Frechet Inception Distance (FID) is also based on the Inception v3 modally.
    <ul>
      <li>The FID uses the feature vectors of the last layer for real and synthesized images to generate multivariate Gaussians that model the real and synthesized distributions</li>
      <li>A <strong>lower</strong> FID is <strong>better</strong>, which corresponds to similar real and synthesized distributions.</li>
    </ul>
  </li>
</ul>

<p>Finally, GAN architectures can be applied in numerous fields such as:</p>

<ul>
  <li>Image Completion</li>
  <li>Super Resolution and Restoration</li>
  <li>Style Synthesis</li>
  <li>De-Raining</li>
  <li>Text-to-Image Synthesis</li>
  <li>Music Synthesis</li>
  <li>etc.</li>
</ul>

<h1 id="deep-variational-inference">Deep Variational Inference</h1>

<p>This chapter begins with a review of variational inference (VI) as a fast approximation alternative to Markov Chain Monte Carlo (MCMC) methods, solving an optimization problem for <strong>approximating the posterior</strong></p>

<ul>
  <li>Amortized VI leads to the <strong>variational autoencoder (VAE)</strong> framework which is introduced using deep neural networks and graphical models and used for learning representations and generative modeling.</li>
</ul>

<p>The setup is as follows. Consider we are given some <strong>data $x$</strong>, so that we can visualize this as a probability distribution $p(x)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215194410664.png" alt="image-20220215194410664" style="zoom:50%;" /></p>

<p>Now, suppose that in reality, this is the actual data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215194455483.png" alt="image-20220215194455483" style="zoom:50%;" /></p>

<p>where essentially:</p>

<ul>
  <li>$z \in [1,2,3]$ is a <mark>hidden variable</mark> that signifies the three cluster within this data distribution</li>
  <li>Essentially, the real distribution $p(x)$ is actually coming from both $x$ and $z$. But only $x$ is observed, and we <strong>do not know what $z$ is in advance</strong></li>
</ul>

<hr />

<p>More formally, the problem task is as follows.</p>

<ul>
  <li>our target model is $p_\theta (x)$</li>
  <li>
    <p>our data is $D = {x_1, x_2, …,x_N}$</p>
  </li>
  <li>we <strong>want to model hidden variables as well</strong></li>
</ul>

<p>Therefore, our <strong>maximum likelihood fit becomes</strong>:</p>

\[\arg\max_\theta \frac{1}{N}\sum_i \log p_\theta (x_i) \to \arg\max_\theta \frac{1}{N}\sum_i \log \left( \int p_\theta(x_i|z)p(z) dz\right)\]

<p>which is <strong>problematic</strong> we would have needed to compute $\int p_\theta(x_i\vert z)p(z) dz$ for every data.</p>

<p>Therefore, the idea is:</p>

<ol>
  <li>observe some data $x$, and take a guess <strong>on the underlying $z$</strong> (e.g. this pile of data belongs to this cluster)</li>
  <li>construct “fake labels $z$” for your each of your data $x_i$
    <ul>
      <li>technically you would guess a $p(z\vert x_i)$ over it</li>
    </ul>
  </li>
  <li>do maximum likelihood on that $x_i$ and $z$ associated</li>
</ol>

<p>Then, the kind of objective you want to do would be:</p>

\[\arg\max_\theta \frac{1}{N} \sum_i  \mathbb{E}_{z \sim p(z|x_i)}[\log p_\theta (x_i,z)]\]

<p>so you are <strong>maximizing over the joint</strong>, so that once done:</p>

<ul>
  <li>obtain a model $\theta$, but also get information on $p(z\vert x_i)$.</li>
  <li>but we are taking an average/expected value over $p(z\vert x_i)$, <mark>how do we calculate this</mark>?</li>
</ul>

<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>

<p>The idea is simple. Our task is to compute some <strong>posterior $p(z\vert x_i)$</strong> for some observation $x_i$. For this section, imagine that you are observing a <strong>coin</strong>, and you want to model $p(\theta \vert x)$ for $\theta$ being the probability of getting a head.</p>

<ul>
  <li>essentially $\theta=z$ is some <strong>hidden information from us</strong>, but carries essential information on how data is generated</li>
  <li>the only observables are $x$</li>
</ul>

<p>Using Bayes, we know that:</p>

\[\underbrace{p(\theta |x)}_{\text{posterior}} = \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}\,\,\frac{1}{p(x)}\]

<blockquote>
  <p><em>Intuition</em></p>

  <p>We can take a <strong>guess on the prior distribution $p(\theta)$</strong>, and with observables available, we can <strong>compute $p(x\vert \theta)$</strong></p>

  <ul>
    <li>e.g. for a coin toss, guess that $p(\theta) = \text{Unif}[0,1]$, and $p(x\vert \theta)=p_\theta(x) \sim \text{Bern}(\theta)$</li>
    <li>essentially, both quantities are now known</li>
    <li>however we often <strong>cannot compute $p(x)$</strong>, since would involve computing some nasty integrals. Hence, we need some technique to estimate $p(\theta \vert  x)$ when we only know $p(x\vert \theta)p(\theta)$</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em>: Coin Toss</p>

<p>The likelihood $p(x=4\vert \theta)$=probability of getting four heads with $\text{Bern}(10,\theta)$, mewing that our traying data has four heads in total. The priors are beta distributions which we guessed. Then, the posterior is the product of the two:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Less Data (for likelihood)</th>
      <th style="text-align: center">More data (for likelihood)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217152847817.png" alt="image-20220217152847817" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217152855335.png" alt="image-20220217152855335" /></td>
    </tr>
  </tbody>
</table>

<p>where the key observation here is that:</p>

<ul>
  <li>if <strong>prior is informative</strong>, then we don’t need very good likelihood data to obtain a reasonable estimate for posterior</li>
  <li>if <strong>likelihood is informative</strong>, i.e. we have lots of data, then we don’t need to have a very informative prior</li>
</ul>

<hr />

<p>Now, lets get back to the task of finding out the <strong>posterior $p(\theta\vert x)$</strong>, or in general $p(z\vert x)$.</p>

<blockquote>
  <p><em>Intuition</em>:</p>

  <p>We do know that</p>

\[\underbrace{p(\theta |x)}_{\text{posterior}} \propto \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}\]

  <p>So the idea is that:</p>

  <ol>
    <li>pick some proposal distribution $q(\theta)$
      <ul>
        <li>e.g. a normal distribution</li>
      </ul>
    </li>
    <li>sample a data point from the proposal distribution $\theta^*$</li>
    <li><mark>save this data point $\theta^*$ if $p(x\vert \theta^*)p(\theta^*)$ is very likely</mark>, reject it otherwise</li>
    <li>repeat step 2 to 3</li>
  </ol>

  <p>Then, you end up with lots of $\theta_i$, and it should <strong>resemble the posterior distribution $p(\theta\vert x)$</strong> due to the save/rejection step.</p>
</blockquote>

<p>In more details, consider the following <strong>MCMC Metropolis Hastings</strong> algorithm</p>

<ol>
  <li>
    <p>select some initial value $\theta_0$ to start with</p>
  </li>
  <li>
    <p>for $i=1,…,m$ do:</p>

    <ol>
      <li>
        <p><strong>Markov</strong>: update the proposal distribution $q(\theta^*\vert \theta_{i-1})$, based on the previous sample</p>
      </li>
      <li>
        <p><strong>Monte Carlo</strong>: pick a candidate $\theta^* \sim q(\theta^* \vert  \theta_{i-1})$</p>
      </li>
      <li>
        <p>consider whether or not to accept the candidate by considering:</p>

\[\alpha = \underbrace{\frac{p(\theta^*|x)}{p(\theta_{t-1}|x)}}_{\text{can't compute}} = \underbrace{\frac{p(x|\theta^*)p(\theta^*)}{p(x|\theta_{i-1})p(\theta_{i-1})}}_{\text{unknown}}\]

        <p>if $\alpha \ge 1$, <em>*accept $\theta^</em>$** so that $\theta_i \leftarrow \theta^*$</p>

        <p>if $0 &lt; \alpha &lt; 1$, accept $\theta^*$ with <strong>probability $\alpha$</strong>, else <strong>reject</strong> it.</p>
      </li>
    </ol>
  </li>
</ol>

<p>The resulting drawn distribution of $\theta^*$s will assemble the posterior distribution $p(\theta\vert x)$.</p>

<ul>
  <li>notice that if our proposal $q$ is <strong>very close to $p(\theta\vert x)$</strong>, then <strong>many samples will be accepted</strong> and we converge fast</li>
  <li>otherwise, we may need a lot of time.</li>
</ul>

<p>Graphically, if we picked</p>

<ul>
  <li>proposal $q(\theta^*\theta_{i-1}) = \mathcal{N}(\theta_{i-1},1)=\text{Beta}(1,1,\theta_{i-1})$,</li>
  <li>likelihood $\text{Bin}(10,\theta)$ and we <strong>observed 4 heads</strong> in our data</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">5 Iterations</th>
      <th style="text-align: center">5000 Iterations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217190211945.png" alt="image-20220217190211945" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217190327902.png" alt="image-20220217190327902" /></td>
    </tr>
  </tbody>
</table>

<p>However, the problem with this approach is that <strong>sampling and adjusting takes time</strong>, and for large number of sampes, it becomes very slow.</p>

<ul>
  <li>this introduces us with the following sections on using ML models such as VAE to solve the <strong>inference problem of $p(\theta\vert x)=p(z\vert x)$</strong> in general.</li>
</ul>

<h2 id="variational-inference">Variational Inference</h2>

<p>We begin with:</p>

<ul>
  <li><strong>observed data $x$</strong>, continuous or discrete
    <ul>
      <li>e.g. $x$ may be an image of a face</li>
    </ul>
  </li>
  <li>suppose that the process <strong>generating the data</strong> involved <strong>hidden latent variables $z$</strong>
    <ul>
      <li>e.g. $z$ a hidden vector describing latent variables such as pose, illumination, gender, or emotion</li>
      <li>e.g. $z$ could represent cluster information</li>
    </ul>
  </li>
</ul>

<p>And you see that constructing a model to model both $x_i,z$ requires knowledge over:</p>

\[p(z|x_i)\]

<p>which is what this method is <strong>attempting to solve</strong>.</p>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>Our aim is to <strong>estimate</strong> $p(z\vert x_i)$: for example, answering the question what are the hidden latent variables (poses, gender, emotion) $z$ for a given observation (image) $x_i$.</p>

  <p>However, this involves computing $p(x_i)$ in the following expression</p>

\[p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}\]

  <p>which is intractable as the denominator cannot be computed.</p>

  <p>Hence, the idea is to <strong>approximate $p(z\vert x_i)$</strong> using $q_{i}(z) \in Q$, and <strong>minimize the distance</strong> using a metric such as KL-divergence. Therefore, this becomes an <strong>optimization problem</strong>, for <mark>each $x_i :=x$:</mark></p>

\[q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz\]

  <p>which can be simplied using Bayes to (since $p(z\vert x)$ we don’t know either)</p>

\[\arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]\]

  <p>where</p>

  <ul>
    <li>$z$ would be sampled from $z \sim q_\phi(z)$ as $q_\phi$ would be picked by us. Hence we can compute $p(x,z)=p(x\vert z)p(z)$ and $q_\phi(z)$</li>
    <li>essentially $p(x,z)$ would involve a likelihood and a prior, which we know.</li>
    <li>the first term is MAP and the second term encourages diffusion (entropy), or spreading of variational distribution.</li>
  </ul>
</blockquote>

<p>First, let us <em>revise</em> Bayes theorem. For <strong>each data point $x_i:=x$</strong></p>

\[p(z,x) = p(z|x)p(x)=p(x|z)p(z)\]

<p>where:</p>

<ul>
  <li>$p(z,x)$ is the joint</li>
  <li>$p(z\vert x)$ is called the <strong>posterior</strong> (as you observed $x$)</li>
  <li>$p(x)$ is call the <strong>evidence/marginal density</strong></li>
  <li>$p(z)$ is the <strong>prior density</strong> (before you observe $x$)</li>
  <li>$p(x\vert z)$ is the <strong>likelihood</strong> (given $z$, probability of seeing $x$)</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>For using deep generative models, we may also want to say:</p>

\[p(x|z)=p(x|z_1)p(z_1|z_2)...p(z_{l-1}|z_l)p(z_l)\]

  <p>for $l$ layers of hidden variables. This will be <strong>discussed later</strong>. Here we think of the simple case where we have only <strong>one hidden variable</strong>.</p>
</blockquote>

<p>Our aim is to estimate $p(z\vert x_i)$:</p>

\[p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}\]

<p>which is intractable to compute because:</p>

<ul>
  <li>
    <p>for most models because the denominator is:</p>

\[p(x_i) = \int p(x_i|z)p(z)dz\]

    <p>this is high-dimensional intractable integral which requires integrating over an exponential number of terms for $z$.</p>
  </li>
  <li>
    <p>in the end, the posterior $p(z\vert x_i)$ is often intractable to compute analytically. For example, if $z$ is a vector of length $d$ (i.e. hidden state with $d$ “features”), <strong>then $p(z\vert x_i)$ is a $d \times d$ matrix</strong>, and the posterior is a function of the parameters of the model $p(z\vert x, \theta)$.</p>
  </li>
</ul>

<p>Therefore, the key idea is to <mark>estimate $p(z_i\vert x)$ by some variational distribution $q_\phi(z)\equiv q_i(z)$</mark> from a family of distributions $Q$ and parameters $\phi$ such that $q_\phi \in Q$. Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215171509021.png" alt="image-20220215171509021" style="zoom:33%;" /></p>

<p>A typical choice for $Q$ is the <strong>exponential family of distribution</strong>:</p>

<blockquote>
  <p><strong>Exponential Family Distribution</strong></p>

  <p>We say that a class of distributions is <strong>in the exponential family</strong> if it can be written in the form:</p>

\[p(y; \eta) = b(y) \exp(\eta \, T(y) - a(\eta))\]

  <p>where:</p>

  <ul>
    <li>$y$ means the labels/target in your dataset</li>
    <li>$\eta$ is the <strong>natural parameter</strong> (also called the canonical parameter) of the <strong>distribution</strong></li>
    <li>$b(y)$ is the <strong>base measure</strong></li>
    <li>$T(y)$ is the <strong>sufficient statistic</strong> (see later examples, you often see $T(y)=y$)</li>
    <li>$a(\eta)$ is the <strong>log partition function</strong>, which basically has $e^{-a(\eta)}$ playing the role of <em>normalization constant</em></li>
  </ul>

  <p>so basically you can expression some distribution with the above form with any choice of $b(y), T(y), a(\eta)$, then that expression is in the exponential family.</p>
</blockquote>

<p>And a choice of <strong>closeness/distance</strong> would be the (reverse) <mark>KL divergence</mark>:</p>

\[KL(q(x) || p(x)) = \int q(x) \log \frac{q(x)}{p(x)}dx\]

<p>Therefore, our <mark>objective</mark> is then (<strong>recall that here $x_i := x$ represents a single data point</strong>)</p>

\[q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz\]

<p>which is intractable since $p(z\vert x)$ is what we want to estimate. Hence we use bayes to simplify:</p>

\[\begin{align*}
\int q(z)\log \frac{q(z)}{p(z|x)}dz 
&amp;= \int q(z)\log \frac{q(z)p(x)}{p(z,x)}dz \\
&amp;= \int q(z)\log \frac{q(z)}{p(z,x)}+q(z) \log (p(x))dz \\
&amp;= \log p(x) + \int q(z)\log \frac{q(z)}{p(z,x)}dz \\
&amp;= \log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz
\end{align*}\]

<p>Now, notice that KL divergence is <strong>non-negative</strong>. Hence we know:</p>

\[\begin{align*}
\log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz 
&amp; \ge  0\\
\log p(x) &amp; \ge \int q(z)\log \frac{p(z,x)}{q(z)}dz \equiv \mathcal{L}
\end{align*}\]

<p>where the integral on the right denoted by $\mathcal{L}$ is known as the <strong>evidence lower bound</strong> (ELBO).</p>

<ul>
  <li>therefore, minimizing KL divergence is the same as <strong>maximizing ELBO</strong></li>
  <li>The ELBO is a <strong>lower bound</strong> on the log-likelihood of the data $x$ given the latent variable $z$. It is a lower bound because it is <mark>not possible to compute the exact log-likelihood</mark> of the data $x$ given the latent variable $z$</li>
  <li>essentially $p(x,z)$ would involve a likelihood and a prior, which we know. See the example algorithm below to see how it works.</li>
</ul>

<p>This finally can be written as:</p>

\[\arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]\]

<p>where we see that there is a tradeoff between these two terms.</p>

<ul>
  <li>the first term places mass on the MAP estimate;</li>
  <li>whereas the second term encourages diffusion, or spreading the variational distribution.</li>
  <li>note that since we would have specified $q_\phi$, we could <strong>sample $z \sim q_\phi$</strong> to compute for $p(x,z)$</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Compared with this formulation, many other methods have short-comings:</p>

  <ul>
    <li>mean-field variational inference (MFVI) <strong>assumes a full factorization of variables</strong> which is inaccurate</li>
    <li>MCMC sampling methods (Brooks, Gelman, Jones &amp; Meng 2011), such as the Metropolis Hastings algorithm, <strong>may not be scalable to very large datasets</strong> and may require <strong>manually specifying a proposal distribution</strong></li>
  </ul>
</blockquote>

<hr />

<p>Recall that:</p>

<p>In the end we also want to recover our $\theta$ for model $p_\theta(x)$. Hence essentially our loss can be rephrased as (rephrasig $p(x,z)=p(x\vert z)p(z)$)</p>

\[\mathcal{L}_i(p,q_i ) =\mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_i(z)]\]

<p>Therefore, our algorithm would be, for each $x_i$:</p>

<ol>
  <li>
    <p>calculate $\nabla_\theta \mathcal{L}_i(p,q_i )$</p>

    <ul>
      <li>
        <p>sample $z \sim q_i(z)$</p>
      </li>
      <li>
        <p>compute the gradient, which is only on the first term:</p>

\[\nabla_\theta \mathcal{L}_i(p,q_i) \approx \log p_\theta(x_i|z)\]

        <p>this we <strong>know because $x_i ,z$</strong> are now “observed”</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Update $\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}_i(p,q_i)$</p>
  </li>
  <li>
    <p>update <strong>$q_i$ to maximize $\mathcal{L}_i(p,q_i)$</strong></p>

    <ul>
      <li>e.g. if you picked $q_i(z) = N(\mu_i, \sigma_i)$, then essentially $\phi = (\mu_i, \sigma_i)$</li>
      <li>hence yuo would try to compute $\nabla_{\mu_i} \mathcal{L}<em>i$ and $\nabla</em>{\sigma_i} \mathcal{L}_i$ to optimize</li>
      <li>but that will be <strong>a lot of parameters</strong>, if we have many data points.</li>
    </ul>
  </li>
</ol>

<p>Therefore, this introduces us to other alternatives such as VAE.</p>

<hr />

<h3 id="optimizing-for-q_phi">Optimizing for $q_\phi$</h3>

<p>In practice, there has been <strong>many methods</strong> in which one can use to update the estimate distribution $q_\phi$. Here we will discuss the approach using</p>

<ul>
  <li><strong>Score function gradient</strong> - generic and has <strong>no requirement on $p_\theta(x)$</strong>, but has <mark>large variance</mark>;</li>
  <li><strong>Reparameterization gradient</strong>. - less general purpose, <strong>$p_\theta(x)$ needs to  be continuous</strong>, much <mark>smaller variance</mark>.</li>
</ul>

<h3 id="score-function-gradient">Score Function Gradient</h3>

<p>To find the best $p_\phi(z\vert x_i)$, we consider finding $\phi$:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)-\log q_{\phi_i}(z)]= \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z)]\]

<p>let us denote $\log p(x_i,z)-\log q_{\phi_i}(z) \equiv f_\phi(z)$, then we have:</p>

\[\begin{align*}
\nabla_\phi \mathbb{E}_{z \sim q_\phi(z)}[f_\phi(z)]
&amp;= \nabla_\phi \int f_\phi(z)q_\phi(z)dz \\
&amp;= \phi \int (\nabla_\phi f_\phi(z))q_\phi(z)+ (\nabla_\phi q_\phi(z))f_\phi(z)dz \\
\end{align*}\]

<p>Now, it turns out that we can <strong>compute</strong> using the following <mark>trick</mark></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215205915090.png" alt="image-20220215205915090" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the <mark>score function</mark> is:</p>

\[\nabla_\phi \log q_\phi(z) = \frac{\nabla_\phi q_\phi(z)}{q_\phi(z)}\]
  </li>
</ul>

<p>This therefore means that:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z) \nabla _\phi \log q_\phi(z)]\]

<p>which we can then <strong>estimate by sampling from the distribution $q_\phi(z)$ that we guessed</strong>:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k[\log p(x_i,z_i)-\log q_{\phi_i}(z_i) \nabla _\phi \log q_\phi(z_i)]\]

<p>which is called <strong>Monte Carlo sampling</strong></p>

<h3 id="reparameterization-gradient">Reparameterization Gradient</h3>

<p>The reparameterization trick utilizes the property that for <strong>continuous</strong> distribution $p_\theta(x)$, the following sampling processes are equivalent:</p>

\[\begin{align*} \hat{x} \sim p(x;\theta) \quad \equiv \quad \hat{x}=g(\hat{\epsilon},\theta) , \hat{\epsilon} \sim p(\epsilon) \end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>instead of directly sampling from the posterior, we typically take random sample from a standard Normal distribution $\hat{\epsilon} \sim \mathcal{N}(0,1)$ and multiply it by the mean and variance</p>
  </li>
  <li>
    <p>e.g. in our case, we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:</p>

\[z = \mu + \sigma \cdot \epsilon\]

    <p>where $\epsilon \sim \mathcal{N}(0,1)$</p>
  </li>
</ul>

<p>Then, this <strong>reparametrization can</strong>:</p>

<ol>
  <li>express the gradient of the expectation</li>
  <li>achieve a <mark>lower variance</mark> than the score function estimator, and</li>
  <li>differentiate through the latent variable $z$ to optimize by backpropagation</li>
</ol>

<p>Then</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215211858216.png" alt="image-20220215211858216" style="zoom: 50%;" /></p>

<p>This finally gives:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k\{\nabla_\phi [\log p(x, g(\epsilon_i,\phi))-\log q_\phi(g(\epsilon_i,\phi))]\}\]

<p>being the update equation.</p>

<h2 id="autoencoders">Autoencoders</h2>

<p>Instead of optimizing a separate parameter $\phi_i$ for <strong>each example</strong>, amortized variational inference (AVI) approximates the posterior across all examples together</p>

<p>The task of our model essentially involves finding <strong>two sets of parameters $\theta, \phi$</strong>. This makes it natural to consider an architecture using <strong>encoder + decoder</strong>, where essentially:</p>

<ul>
  <li><strong>encoder</strong> will perform $q_\phi(z\vert x)$ mapping. In other words, given an <strong>input $x$</strong>, <strong>output a hidden state $z$</strong></li>
  <li><strong>decoder</strong> will perform $p_\theta(\hat{x}\vert z)$, in an attempt to <strong>reconstruct $x$</strong> hence finding $p(x\vert z)$ with <strong>input from hidden state $z$</strong></li>
</ul>

<hr />

<p><em>Recap</em>: Autoencoder</p>

<p>Essentially an autoencoder is doing:</p>

\[F(x) = \text{decode}(\text{encoder}(x))\]

<p>where <strong>making sure that</strong> the dimension $z = \text{encoder}(x)$ is <mark>smaller than $x$</mark>, hence we are “<strong>extracting important features</strong>” from $x$.</p>

<p>Therefore, the architecture for autoencoder looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215220325253.png" alt="image-20220215220325253" style="zoom:67%;" /></p>

<p>where we can say that there are <strong>two networks</strong> (e.g. with two weights):</p>

\[\min_{W_e,W_d} \sum_{i=1}^m ||x_i - (W_d)^T f( (W_e)^T x_i )||^2\]

<p>for each data $x_i \in \mathbb{R}^d$</p>

<ul>
  <li>we want $(W_e)^Tx_i$ to have a <strong>smaller dimension than $d$</strong> (otherwise reconstructing is trivial)</li>
  <li>if $f$ is an <strong>identity matrix</strong>, then this is equivalent of doing <mark>PCA</mark></li>
</ul>

<hr />

<p>This architecture can be used for:</p>

<ul>
  <li>
    <p><strong>denoising</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134248515.png" alt="image-20220217134248515" style="zoom:33%;" /></p>
  </li>
  <li>
    <p><strong>completion</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134306323.png" alt="image-20220217134306323" style="zoom:33%;" /></p>

    <p>for example:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134333967.png" alt="image-20220217134333967" style="zoom:33%;" /></p>
  </li>
</ul>

<h3 id="variational-autoencoder">Variational Autoencoder</h3>

<p>Then, for Variational Autoencoder, we are basically using autoencoder to learn $\theta, \phi$ by <strong>maximizing ELBO</strong>:</p>

\[\begin{align*}
\mathcal{L} 
&amp;= \int q(z)\log \frac{p(z,x)}{q(z)}dz \\
&amp;= \int q(z)\log p(x|z) dz - \int q(z) \log \frac{p(z)}{q(z)}dz\\
&amp;= \mathbb{E}_{z \sim q(z)}[\log p(x|z)] - KL(q(z)||p(z))
\end{align*}\]

<p>Therefore, we can think of the following architecture to <strong>solve this optimization problem</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Network</th>
      <th style="text-align: center">Abstraction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://miro.medium.com/max/1838/1*Q5dogodt3wzKKktE0v3dMQ@2x.png" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215220914001.png" alt="image-20220215220914001" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>$\mathbb{E}_{z \sim q(z)}[\log p(x\vert z)]$ would correspond to <strong>decoder</strong></p>

    <ul>
      <li>given some <strong>sampled $z \sim q(z)$</strong>, this is the log-likelihood of the observed data $x$ (i.e. $x_i := x$).</li>
      <li>Therefore, this measures how well the samples from $q(z)$ explain the data $x$, which can be seen as the <strong>reconstruction error</strong> to get $x$ back from an encoded latent variable $z$</li>
    </ul>
  </li>
  <li>
    <p>$KL(q(z)\vert \vert p(z))$ would correspond to the <strong>encoder</strong>, because we are trying to find out $q(z)$ that is close to $p(z)$</p>

    <ul>
      <li>represents <strong>encoding data from $x$ to latent variable $z$</strong></li>
      <li>hence, if going well, this means that the explanation of the data ($z \sim q(z)$) does not deviate from the prior beliefs $p(z)$ and
is called the <strong>regularization term</strong></li>
    </ul>
  </li>
</ul>

<p><strong>In summary</strong>:</p>

<ul>
  <li>encoder neural network infers a hidden variable $z$ from an observation $x$.</li>
  <li>decoder neural network which reconstructs an observation $\hat{x}$ from a hidden variable $z$.</li>
  <li>The encoder $q_\phi$ and decoder $p_\theta$ are trained end-to-end, optimizing for <strong>both</strong> the <strong>encoder parameters $\phi$</strong> and <strong>decoder parameters $\theta$</strong> by backpropagation</li>
</ul>

<blockquote>
  <p>Now there is a problem, because in the above algorithm, we would need to <strong>sample $z$</strong> in the encoder (not a differentiable operation). However, for backpropagation, we need each operation to be <mark>differentiable</mark>.</p>
</blockquote>

<p>Therefore, we need to <strong>reparametrize</strong> the sampling procedure ot the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134853802.png" alt="image-20220217134853802" style="zoom:33%;" /></p>

<p>therefore, essentially:</p>

<ul>
  <li>
    <p>recall that we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:</p>

\[z = \mu + \sigma \cdot \epsilon\]

    <p>where $\epsilon \sim \mathcal{N}(0,1)$</p>
  </li>
  <li>we are <strong>predicting the mean/variance</strong>, instead of predicting $z$. Therefore, all we need to do is to <strong>update mean/variance</strong> of the distribution instead of update the “sampling procedure”</li>
  <li>the same goes on to decoder.</li>
</ul>

<p>Then together, the model is really <strong>learning</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135020612.png" alt="image-20220217135020612" style="zoom: 50%;" /></p>

<ul>
  <li>which can be seen as a probabilistic model as it involves calculation from samplings using mean and variance</li>
</ul>

<h2 id="probabilistic-programming">Probabilistic Programming</h2>

<p>Essentially you can compute <strong>distributions, conditional distributions, etc</strong> using a program.</p>

<ul>
  <li>e.g. you can infer <strong>conditional distribution $p(a+b+c\vert a+b=1)$</strong> from only knowing the individual probabilities $p(a),p(b),p(c)$</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135414796.png" alt="image-20220217135414796" style="zoom:50%;" /></p>

<p>which is a powerful idea (e.g. used in MCMC and etc)</p>

<h1 id="reinforcement-learning">Reinforcement Learning</h1>

<p>Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning.</p>

<ul>
  <li>In <strong>supervised</strong> learning we are given input-output pairs</li>
  <li>in <strong>unsupervised</strong> learning we are given only input examples</li>
  <li>In <strong>reinforcement</strong> learning we learn from interaction with an environment to achieve a goal</li>
</ul>

<blockquote>
  <p>We have an <strong>agent</strong>, a learner, that makes decisions under uncertainty.</p>

  <p>In this setting there is an <strong>environment</strong> which is what the agent interacts with. The agents selects <strong>actions</strong> and the environment responds to those actions with a new <strong>state</strong> and <strong>reward</strong> .</p>

  <p>The agent goal is to <mark>maximize reward over time</mark> as shown below:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217201444646.png" alt="image-20220217201444646" style="zoom:50%;" /></p>
</blockquote>

<blockquote>
  <p><strong>Resource</strong>: https://web.stanford.edu/class/cs234/index.html</p>
</blockquote>

<h2 id="multi-armed-bandit">Multi-Armed Bandit</h2>

<p>Before considering reinforcement learning, we will consider the <strong>stateless</strong> (policy at time $t$ is identical) setting of a multi-armed bandit.</p>

<p>Given $k$ slot machines:</p>

<ul>
  <li>an <strong>action</strong> is to pull an arm of one of the machines</li>
  <li>at each <strong>time step</strong> $t$ the agent chooses an <strong>action</strong> at among the $k$ actions</li>
  <li>taking action a is pulling arm $i$ which gives a <strong>reward $r(a)$</strong> with probability $p_i$, which of course you don’t know
    <ul>
      <li>i.e. Behind each machine there is a probability distribution, and by pulling an arm we get a <em>sample</em> from that distribution</li>
    </ul>
  </li>
</ul>

<p>Our goal is to <strong>maximize the total expected return</strong>. To do this, consider:</p>

<ul>
  <li>
    <p>each action has an expected or mean reward given that that action is selected; we can it <strong>value</strong> of that action</p>

    <ul>
      <li>
        <p>we denote the <strong>true value of action $a$</strong> as $q(a)$</p>
      </li>
      <li>
        <p>the <strong>estimated value of action $a$ at time $t$ as $Q_t(a)$</strong></p>

\[Q_t(a) =\text{Sample Mean}(a)\]

        <p>which we can update per iteration/action taken</p>
      </li>
    </ul>
  </li>
</ul>

<p>Then a simple idea is, at any time step, <strong>pick the action</strong> whose <mark>estimated value is greatest</mark></p>

\[a_t = \arg\max_{a}Q_t(a)\]

<p>where if we do this, we are doing a <strong>greedy algorithm</strong></p>

<ul>
  <li>If you select a greedy action, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions</li>
  <li>If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong> (could lead to better results in the long run)</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider you are given</p>

<ul>
  <li>two possible actions, picking red or blue (door, pill, etc).</li>
</ul>

<p>At $t=0$, we can randomly pick an action. For instance we picked the red one, and receives a reward $0$:</p>

<table>
  <thead>
    <tr>
      <th>$t=0$</th>
      <th style="text-align: center">$t=1$</th>
      <th style="text-align: center">$t=2$</th>
      <th style="text-align: center">$t=3$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205400203.png" alt="image-20220217205400203" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205412649.png" alt="image-20220217205412649" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205426486.png" alt="image-20220217205426486" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205443361.png" alt="image-20220217205443361" /></td>
    </tr>
  </tbody>
</table>

<p>notice that</p>

<ul>
  <li>at each time step we have <strong>updated our estimate for value of an action $Q_t(a)$</strong></li>
  <li>we are endlessly choosing blue, but it could have been the case that the value we received for the red door of $0$ was simply bad luck, and that value was sampled from the tail of the distribution behind the red door
    <ul>
      <li>hence we need some <strong>balance</strong> between exploration and expoitation</li>
    </ul>
  </li>
</ul>

<h3 id="varepsilon-greedy-approach">$\varepsilon$-greedy Approach</h3>

<blockquote>
  <p>If instead of taking a greedy action, we behave greedily most of the time, for example</p>

  <ul>
    <li>with a small probability $\varepsilon$ we choose a <strong>random</strong> action</li>
    <li>with probability $1-\varepsilon$ we take the <strong>greedy</strong> action then we are acting $\varepsilon$-greedy</li>
  </ul>
</blockquote>

<p>Then the algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217210047233.png" alt="image-20220217210047233" style="zoom: 50%;" /></p>

<p>notice that:</p>

<ul>
  <li>$r(a)-Q(a)$ is like an <strong>error</strong> of current estimate</li>
  <li>as an exercise, try to show that this is the same as calculating the runnning mean $Q_{t+1} = (1/k)\sum_i^k R_i$</li>
</ul>

<h3 id="upper-confidence">Upper Confidence</h3>

<p>We can choose to be optimistic under uncertainty by <strong>using both the mean and variance of the (estimated) reward</strong>, taking the action using the upper confidence bound (UCB) criteria:</p>

\[a_t = \arg\max_a(\mu(r(a)) + \epsilon \sigma(r(a)))\]

<p>then in this case, you would also need to keep track of $\sigma(r(a))$ estimate.</p>

<h2 id="state-machines">State Machines</h2>

<p>The algorithms before are <strong>stateless</strong>. Now we consider adding a state, and the problem can be formalized as a state machine.</p>

<blockquote>
  <p>The tuple $(S,X, f, Y, g, s_0)$ define the <strong>state machine</strong>.</p>

  <ul>
    <li>$S$ is the set of possible states</li>
    <li>$X$ is the set of possible inputs</li>
    <li>$f:S\times X\to S$ transition function</li>
    <li>$Y$ is the set of possible outputs</li>
    <li>$g:S \to Y$ mapping from state to output</li>
    <li>$s_0$ initial state</li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218170329457.png" alt="image-20220218170329457" style="zoom: 50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>$S = { \text{standing, moving} }$, $X = { \text{slow, fast} }$,</li>
  <li>$f$ shown in orange and purple arrows, e.g. $s_1 = f(s_0 , \text{fast}) = \text{moving}$</li>
  <li>$s_0 = \text{standing}$ being the initial state</li>
  <li>$y=g(\text{standing}) = \text{standing}$ is the output in this case</li>
</ul>

<p>Notice when we use a state machine, at each time step $t$ it is essentially resembling <strong>RNN</strong>:</p>

\[\begin{align*}
s_t &amp;= f(s_{t-1},x_t)\\
y_t &amp;= g(s_t)	
\end{align*}\]

<p>notice that</p>

<ul>
  <li>this is the same as RNN if we use hidden state $h_{t-1}$ instead of $s_{t-1}$ here.</li>
  <li>everything is <strong>deterministic</strong>, i.e. given a state and an action, you know for certain what will be the next state</li>
</ul>

<h3 id="markov-processes">Markov Processes</h3>

<blockquote>
  <p>In a <strong>Markov model</strong>, we assume that the probability of a state $s_{t+1}$ is dependent <strong>only on the previous state $s_t$ and an action $a_t$</strong>.</p>

  <p>Formally, we consider</p>

  <ul>
    <li>$S$ a set of possible states</li>
    <li>$A$ a set of possible actions</li>
    <li>$T:S\times A \times S \to \mathbb{R}$ is the transition model with probabilities</li>
    <li>$g$ is the mapping from state to output
      <ul>
        <li>in the following examples they will just be identity operation</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>(notice that we haven’t included reward $R$ in this model. Adding this information essentially makes the model to become a Markov Decision Process)</p>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218172310634.png" alt="image-20220218172310634" style="zoom:50%;" /></p>

<p>where you see</p>

<ul>
  <li>$A = {\text{slow}, \text{fast}}$ denoted by orange and pick arc</li>
  <li>e.g. if the robot is $\text{fallen}$ and takes $\text{slow}$ action then with probability $p=3/5$ the robot will stay fallen, <strong>but with $p=2/5$</strong> the robot will stand up
    <ul>
      <li>notice that this means an action can potentially lead <strong>any number of states</strong></li>
      <li>this is no longer deterministic!</li>
    </ul>
  </li>
</ul>

<p>Notice that since we have <strong>three states and two actions</strong>, we have <strong>two probability matrices</strong> of $3\times 3$ in size:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218221548845.png" alt="image-20220218221548845" style="zoom:67%;" /></p>

<p>which corresponds to the figure above, and notice that:</p>

<ul>
  <li>the $i$-th row denote transition from state $s_i$ to $s_j$. Therefore probability per row <strong>adds up to $1$</strong></li>
</ul>

<blockquote>
  <p>Then, a <strong>policy $\pi_t(a\vert s)=P(a_t=a\vert s_t=s)$</strong> maps a <strong>state $s$ to action $a$</strong>, allowing agent to decide which action to take given the current state at time $t$.</p>

  <ul>
    <li>Reinforcement learning methods specify how the agent changes its policy as a result of its experience</li>
  </ul>
</blockquote>

<p>Graphically, this looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222135047939.png" alt="image-20220222135047939" style="zoom: 33%;" /></p>

<p>This idea can be easily extended when we have observations $o_i \neq s_i$ at each state, and we make decisions <strong>base on the observations $\pi(a_i \vert  o_i)$</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222135329258.png" alt="image-20220222135329258" style="zoom: 50%;" /></p>

<h3 id="markov-decision-processes">Markov Decision Processes</h3>

<p>This is the more relevant introduction to RL.</p>

<blockquote>
  <p>The tuple $(S,A,T,R,\gamma)$ define the <strong>Markov Decision Process</strong>.</p>

  <ul>
    <li>$S$ is the set of possible states</li>
    <li>$A$ is the set of possible actions</li>
    <li>$T:S\times A \times S\to \mathbb{R}$ is the transition model with <em>probabilities</em>
      <ul>
        <li>this is assumed to be known in advance in MDP. It will be unknown in RL.</li>
      </ul>
    </li>
    <li>$R: S \times A \to \mathbb{R}$ is reward function, given a state and action
      <ul>
        <li>not $r(s,a,s’)$ here because we need to “estimate” the reward before knowing what $s’$ is, which is probabalistic, so that we can define our policy based only on $s$ so that $\pi = \pi(a\vert s)$</li>
      </ul>
    </li>
    <li>$\gamma$ is the discount factor</li>
  </ul>
</blockquote>

<p>The idea is simple:</p>

<ol>
  <li>At every time step $t$ the agent finds itself in state $s \in S$ and selects an action $a \in A$.</li>
  <li>the agent then moves to a new state $s \leftarrow s’$ in a <strong>probabalistic manner</strong> and receives a <strong>reward</strong>
    <ul>
      <li>the reward is a function of previous state and action $s,a$ (technically the expected value of $r(s,a,s’)$)</li>
    </ul>
  </li>
  <li>repeat step 1</li>
</ol>

<p>For instance, consider the following example, where we have a robot <strong>collecting cans</strong>. It can either search actively for a can (depletes battery), wait for someone to give a can, or recharge:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" /></p>

<p>where we have provided:</p>

<ul>
  <li>
    <p>state $S$ being the larger nodes, actions $A$ being the smaller nodes</p>
  </li>
  <li>
    <p>transition $T(s,a,s’)$ being the probability on the arrow, and $r(s,a,s’)$ being the reward on the arrow.</p>

    <p>Reward function technically is:</p>

\[R(s,a) = \sum_{s'} r(s,a,s') p(s'|s,a)\]
  </li>
</ul>

<p>They can also be summarized in the table:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" /></p>

<p>notice that the above table essentially provides a <strong>probability distribution for each possible state-action pair</strong></p>

\[p(s',r|s,a) \equiv P(S_{t+1}=s,R_{t+1}=r| S_t = s,A_t=a)\]

<p>since each transition essentially is a two tuple. Then, we can use this to compute useful quantities such as:</p>

<ul>
  <li>
    <p><strong>expected rewards for state-action</strong> pair</p>

\[r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)\]

    <p>or alternatively, if we have $r(s,a,s’)$:</p>

\[r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{s'} r(s,a,s')p(s'|s,a)\]

    <p>for instance, we can compute $r(s=\text{low}, a=\text{search})$ as:</p>

\[\begin{align*}
r(s=\text{low}, a=\text{search})
&amp;= -3 \cdot \sum_{s'} p(s',r=-3|s,a) + r_{search}\cdot \sum_{s'}p(s',r=r_{search}|s,a)\\
&amp;= -3 \cdot (1-\beta) + r_{search}(\beta )
\end{align*}\]

    <p>which is essentially <mark>weighting the reward on the arrow with probability</mark> (see graph above). We can compute this for <strong>every state-action pair</strong> and get a matrix of size $\vert S\vert  \times \vert A\vert$:</p>

\[R(s,a) = \begin{bmatrix}
r(s_0,a_0) &amp; r(s_0,a_1)\\
r(s_1,a_0) &amp; r(s_1,a_1)\\
r(s_2,a_0) &amp; r(s_2,a_1)
\end{bmatrix}\]

    <p>then we can take the action that maximizes the reward from that state per <strong>row</strong> (greedy)</p>
  </li>
  <li>
    <p><strong>state transition probability</strong></p>

\[p(s'|s,a) = P(S_{t+1}=s| S_t = s,A_t=a) = \sum_r p(s',r|s,a)\]
  </li>
  <li>
    <p><strong>expected rewards for state-action-next-state</strong>:</p>

\[r(s,a,s') = \mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] = \frac{\sum_r r\, p(s',r|s,a)}{p(s'|s,a)}\]
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>The latter two quantities will be <strong>the most important</strong>, which are the ones drawn on the graph/in the table, and dynamics will be expressed almost exclusively using those two in further chapters.</li>
    <li>In reality, the agent does not know $T(s,a,s’)=P(s’\vert s,a)$ and $R(s,a)$. We wil lneed to learn them by <strong>sampling</strong> the environment.</li>
  </ul>
</blockquote>

<h2 id="value-functions">Value Functions</h2>

<p>Almost all reinforcement learning algorithms involve estimating some kind of value functions:</p>

<ul>
  <li>either estimating <strong>functions of states</strong> that estimate how good it is for the agent to be in a given state</li>
  <li>or <strong>functions of state-action pairs</strong> that estimate how good it is to perform a given action in a given state</li>
</ul>

<blockquote>
  <p>The notion of “how good” here is defined in terms of <strong>future rewards</strong> that can be expected. To be specific, we want to maximize the <strong>cumulative reward it receives in the long run</strong>, which is called the <mark>expected return $G_t$</mark></p>
</blockquote>

<p>To formalize the above idea, we need to consider the <strong>sequence of rewards</strong> received <mark>at each step</mark> after some time $t$:</p>

\[R_{t+1}, R_{t+2}, R_{t+3}, ...\]

<p>Then, we can define our <mark>goal being maximizing the expected return at time $t$</mark>:</p>

\[G_t = f(R_{t+1},R_{t+2},...,R_{T})\]

<p>for $T$ being the final step. A simple example would be:</p>

\[G_t = R_{t+1} + R_{t+2} + .... + R_T\]

<p>and more commonly we can generalize this with <strong>discounting</strong>:</p>

\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+.... = \sum_{k=0}^T \gamma^k R_{t+k+1}\]

<p>where $\gamma \in [0,1]$ is a discount rate:</p>

<ul>
  <li>a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately.</li>
  <li>if $\gamma=0$, then the agent is <strong>myopic</strong> as it only consider maximizing immediate reward as $G_t = R_{t+1}$</li>
  <li>if $\gamma \to1$, then the objective takes future rewards into account more strongly: the agent becomes more <strong>farsighted</strong>. (if $\gamma =1$ the sum might explode if $T \to \infty$ and your sequence is not bounded.)</li>
</ul>

<p>With this, we can define the following quantity that are crucial in making decisions.</p>

<blockquote>
  <p><strong>State Value Function</strong></p>

  <p>The value of a state $s$ under a policy $\pi$, denoted $V_\pi(s)$, is the <strong>expected return when starting in $s$</strong> and following $\pi$ thereafter.</p>

\[V_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\]

  <p>and we call $T\equiv h$ is the horizon, which can be seen as the <strong>number of time steps left</strong>. Additionally, we define the value of the last state $t=T=h$ being</p>

\[V_\pi^{0}(s) = 0\]

</blockquote>

<p>Of course, in reality we can only estimate this expected value. This means that we need to consider the <strong>most general case</strong> for a <mark>stochastic policy $\pi(a\vert s)$ being a distribution</mark>:</p>

\[\begin{align*}
V_\pi (s)
= \mathbb{E}_\pi [G_t | S_t =s]
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\\\
&amp;= \mathbb{E}_\pi [R_{t+1}|s] + \gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi [R_{t+1}|s] + \sum_a \pi(a|s) \sum_{s'}\sum_r p(s',r|s,a)\gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_{t+1}=s\right]\\
&amp;= \sum _aR(s,a) \pi(a|s) + \gamma  \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)V_\pi(s')\\
\end{align*}\]

<p>is a essentially <strong>recursive formula</strong>, which we can <strong>dynamically update</strong>. Note that:</p>

<ul>
  <li>$s’$ is essentially the <strong>next state</strong> from $s,a$ tuple we chose</li>
  <li>
    <p>we have all the fuss/sums because the policy $\pi(a\vert s)$ is spitting out a <strong>probability for taking each action $a$</strong> when in a state $s$</p>
  </li>
  <li>
    <p>remember that:</p>

\[R(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)\]

    <p>which</p>

    <ul>
      <li>do not forget that $R$ is basically the <strong>expected reward after taken $s,a$ tuple</strong>.</li>
      <li>the $r$ is a random variable, and $p(s’,r\vert s,a)$ would be the joint for all possible $s’,r$ output.</li>
    </ul>

    <p>Therefore, the above essentially becomes the <strong>Bellman equation</strong></p>
  </li>
</ul>

<blockquote>
  <p><strong>Bellman Equation for $V_\pi$</strong></p>

  <p>For a <strong>stochastic policy $\pi(a\vert s)$</strong>, the formula above can be rewritten as:</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

  <p>which can be interpreted as the <strong>expected value over $a,s’,r$</strong> for $r+\gamma V_\pi(s’)$, and we are <strong>weighting it by $\pi(a\vert s)p(s’,r\vert s,a)$</strong>. Hence, graphically, we are essentially considering <strong>all the possibilities from $s$</strong>:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222220640857.png" alt="image-20220222220640857" /></p>

  <p>and summing over <strong>all of them</strong>.</p>

  <p>Notice that this value function $V_\pi(s)$ is the <strong>unique solution to its Bellman equation, given a $\pi(a\vert s)$</strong>.</p>

  <ul>
    <li>
      <p>this <strong>yields $\vert S\vert$ equations to be solved</strong>, because it can be written as:</p>

\[V_\pi = r + TV_\pi\]

      <p>for $V_\pi(s)$ is a <strong>vector of values for each state $s$</strong>, $T$ being the transition matrix, $r$ being the reward for each state.</p>
    </li>
    <li>
      <p>We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn $V_\pi(s)$.</p>
    </li>
  </ul>
</blockquote>

<p>If $\pi(s) \to a \in A$ being some <strong>deterministic</strong> policy, then we can say that:</p>

\[V_\pi(s) =  R(s,\pi(s)) + \gamma  \sum_{s'}p(s'|s,\pi(s))V_\pi(s')\]

<p>so that essentially:</p>

<ul>
  <li>current reward + an expected value summing over all the possible next state $s’$ we could take</li>
  <li>again, a <strong>recursive formula</strong>, which we can dynamically update to solve for $V_\pi(s)$</li>
  <li>since this acts on $\pi(s)\to a \in A$ instead of $\pi(a\vert s)$ being a distribution, e.g. $\pi(s) = \arg\max_a \pi(a\vert s)$, we can use this formula to <mark>find a policy that maximizes the discounted return</mark>.</li>
  <li>this <strong>yields $\vert S\vert$ equations to be solved</strong></li>
</ul>

<hr />

<p>Similarly, we can inductively compute $Q^h(s,a)$ which is the <strong>action value function</strong>.</p>

<blockquote>
  <p><strong>Action Value Function</strong></p>

  <p>We denote $Q_\pi(s,a)$ as the <strong>expected return</strong> starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>

\[Q_\pi(s,a)=\mathbb{E}_\pi [G_t | S_t = s,A_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]\]

  <p>Notice that obviously:</p>

\[V_\pi(s) = \sum_{a} \pi(a|s)Q_\pi(s,a)\]

  <p>are related quantities.</p>
</blockquote>

<p>Similar to the state value function, we can compute the $Q_\pi(s,a)$ using <strong>dynamic programming</strong> with the <strong>Bellman’s Equation</strong> again:</p>

\[\begin{align*}
Q_\pi (s,a)
= \mathbb{E}_\pi [G_t | S_t =s, A_t=a]
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]
\end{align*}\]

<p>which following a similar derivation with the state value function, we can arrive at:</p>

<blockquote>
  <p><strong>Bellman Equation for $Q_\pi$</strong></p>

  <p>For a <strong>stochastic policy $\pi(a\vert s)$</strong>, the formula above can be rewritten as:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

  <p>which can be interpreted as the <strong>expected value over $s’,r$</strong> whatever is in the bracket, meaning covering all possible $s’,r$ as the next step if we did $s,a$. Then we are <strong>weighting it by $p(s’,r\vert s,a)$</strong>. Hence, graphically, we are essentially considering <strong>all the possibilities from $s,a$</strong>:</p>

  <p>Graphically you are doing:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222221819799.png" alt="image-20220222221819799" style="zoom: 80%;" /></p>

  <p>And <strong>summing over all possibilities</strong> weighted by their probability.</p>
</blockquote>

<p>Again, this is not computable, so we usually consider estimating $V_\pi,Q_\pi$ from experience. For example:</p>

<ul>
  <li>if an agent follows policy $\pi$ and <strong>maintains an average</strong>, for each state encountered, <strong>of the actual returns that have followed that state</strong>, then the average will converge to the state’s value, $V_\pi(s)$.</li>
  <li>If <strong>separate averages are kept for each action</strong> taken <strong>in a state</strong>, then these averages will similarly converge to the action values, $Q_\pi(s,a)$.</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider a setup where each cells of the grid correspond to the states of the environment.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222232047833.png" alt="image-20220222232047833" /></p>

<p>where:</p>

<ul>
  <li>at each cell, <strong>four actions are possible</strong>: north, south, east, and west, and we assume the <strong>actions are deterministic</strong> in that $p(s \to s’\vert a)=1$.</li>
  <li>actions that would take the agent out of the grid leave its location unchanged, but also result in a <strong>reward of $-1$</strong></li>
  <li>from state $A$, <strong>all four actions yield a reward of $+10$</strong> and take the agent to $A’$.</li>
  <li>from state $B$, <strong>all actions yield a reward of $+5$</strong> and take the agent to $B’$.</li>
  <li>other actions result in <strong>a reward of $0$</strong></li>
</ul>

<p>Now, to compute the <strong>value function $V_\pi(s)$</strong>, we need to specify a policy: Suppose the agent selects <strong>all four actions with equal probability in all states</strong>. Then, using $\gamma=0.9$, this policy gives:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222232432754.png" alt="image-20220222232432754" /></p>

<p>which is the <strong>value functions $V_\pi(s)$ for all state $s$ given this policy</strong>.</p>

<ul>
  <li>since the policy is stochastic, verify it using the Bellman’s equation</li>
  <li>negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy</li>
  <li>$A$ is the best state to be in under this policy but its <strong>expected return is less than 10</strong> (which is its immediate reward), because from $A$ the agent is taken to $A’$, from which it is likely to run into the edge of the grid.</li>
  <li>State $B$, on the other hand, is <strong>valued more than $5$</strong> (which is its immediate reward), because from $B$ the agent is taken to $B’$, which has a positive value
    <ul>
      <li>because at $B’$ you have a possibility of bumping into $A$ or $B$</li>
    </ul>
  </li>
</ul>

<p>(At this point, you might want to improve the policy, i.e. to move towards $A$ more rather than randomly going in all direction. If you do that, then you will need to <strong>recompute $V_\pi(s)$</strong> because the above is only true for the stochastic policy)</p>

<h3 id="optimal-value-functions">Optimal Value Functions</h3>

<blockquote>
  <p><strong>Goal</strong></p>

  <p>Solving a reinforcement learning task means, roughly, <strong>finding a policy that achieves the most reward over the long run</strong>. Then, with this definition, we can order policies by their <strong>expected return</strong>:</p>

  <p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if</p>

\[\pi \ge \pi' \iff V_\pi(s) \ge V_{\pi'}(s),\quad \forall s\]

  <p>which means its expected return is greater than or equal to that of $\pi’$ <strong>for all states</strong>. And there is <mark>always at least one policy that is better</mark> than or equal to all other policies</p>
</blockquote>

<p>Hence, we can define an <strong>optimal policy $\pi^*$</strong> that must satisfy the following:</p>

\[\pi^* \to \begin{cases}
V_{\pi^*}(s) = V^*(s) &amp;= \max_\pi V_\pi(s)\\
Q_{\pi^*}(s,a) = Q^*(s,a) &amp;= \max_\pi Q_\pi(s,a)
\end{cases}\]

<p>note that there may be more than one $\pi^*$, but then by definition they must share the same constraint above, i.e have the same state value function and action value function.</p>

<p>At this point, you might wonder <strong>why do we need both $V_\pi(s),Q_\pi(s,a)$</strong>? <mark>Technically, we only need one of them to find $\pi^*$</mark></p>

<ul>
  <li>
    <p>If you have the optimal value function, $V^*$, then the actions that appear <strong>best after a one-step search</strong> (i.e. the action that goes to the best valued next state) will be <strong>optimal actions</strong></p>

    <p>Aa greedy policy is actually optimal in the long term sense <strong>because $V^*$ already takes into account the reward consequences of all possible future behavior</strong>.</p>
  </li>
  <li>
    <p>With $Q^*$, the agent does not even have to do a one-step-ahead search: for any state $s$, it can simply <strong>pick any action that maximizes $Q^*(s\vert a)$</strong> by:</p>

\[\pi^*(s) = \arg\max_a Q^*(s,a)\]

    <p>again, a deterministic policy results.</p>

    <p>Hence, at the cost of representing a function of state{action pairs, instead of just of states, the optimal action-value function allows optimal actions to be selected without having to know anything about possible successor states and their values</p>
  </li>
</ul>

<p>Before we discuss <strong>how to solve for the optimal solution</strong>, consider the MDP case of the grid:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235445697.png" alt="image-20220222235445697" /></p>

<p>The <strong>optimal solution of state value function</strong> looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235513192.png" alt="image-20220222235513192" /></p>

<p>which, by greedily looking one step ahead, we have <strong>found the optimal policy $\pi(s):S \to a \in A$</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235610203.png" alt="image-20220222235610203" /></p>

<hr />

<p>Now, consider <strong>solving for $V^*$</strong>. We know that:</p>

<ol>
  <li>
    <p>The solution must satisfy Bellman’s Equation</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]
  </li>
  <li>
    <p>But because it is <strong>optimal</strong> (i.e. corresponds to the optimal policy), <strong>value of a state</strong> under an optimal policy must <strong>equal the expected return for the best action from that state</strong>.</p>
  </li>
</ol>

<p>Therefore, we get:</p>

\[\begin{align*}
V^* (s)
= \max_{a \in A} Q^*(s,a)
&amp;= \max_a \mathbb{E}_{\pi^*}[G_t | S_t =s, A_t=a]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s,A_t=a\right]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

<p>notice that the last two lines have <strong>no reference to the optimal policy</strong>. Those two lines are also called the <mark>optimality equation for $V^*$</mark>.</p>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Bellman’s Optimality equation</th>
      <th style="text-align: center">Bellman’s Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223005457522.png" alt="image-20220223005457522" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223005536963.png" alt="image-20220223005536963" /></td>
    </tr>
  </tbody>
</table>

<p>(recall that once you have $V^<em>$, you can find $\pi^</em>$ easily.)</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>To solve for the optimal state value function, the Bellman optimality equation is actually <strong>a system of equations</strong>, <mark>one such equation for each state</mark>. So if there are $N$ states, then there are $N$ equations in $N$ unknowns (see example at the end)</p>
</blockquote>

<p>The <strong>Bellman’s optimality equation for $Q^*$</strong> also has to satisfy</p>

<ol>
  <li>
    <p>Bellman’s equation:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]
  </li>
  <li>
    <p>Is <strong>optimal</strong>. Hence we consider $\gamma \max_{a \in A} Q^*(s’,a’)$ instead of $\gamma \sum_a \pi(a\vert s)\sum_{s’}  Q_\pi(s’,a’)$</p>
  </li>
</ol>

<p>This gives the following <strong>optimality equation</strong>:</p>

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&amp;= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}\]

<p>and graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Bellman’s Optimality equation</th>
      <th style="text-align: center">Bellman’s Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223010401381.png" alt="image-20220223010401381" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223010502964.png" alt=" but " /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The runtime of solving the optimal action value function will take $O(\vert A\vert \times\vert S\vert )$ for having $\vert A\vert$ possible actions.</p>
</blockquote>

<hr />

<p><em>For Example</em>: Solving Optimal Solution for Robot Collection</p>

<p>Recall the setup being:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transitions</th>
      <th style="text-align: center">Tabluated</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Essentially we know $p(s’,r\vert s,a)$, then since we have <strong>two states</strong>, let us encode the two states as $s_0=\text{high}=h,s_1=\text{low}=l$. Additinally:</p>

<ul>
  <li>$s,w,re$ represents the <strong>actions</strong> search, wait, recharge.</li>
  <li>parameters $\gamma, \beta, \alpha$ are assumed to be known</li>
</ul>

<p>Then, we have <strong>two equations because we have two states</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011058649.png" alt="image-20220223011058649" style="zoom:80%;" /></p>

<p>And</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011119229.png" alt="image-20220223011119229" style="zoom:80%;" /></p>

<p>Once we solved it, say $V^<em>(h)=21$, $V^</em>(l)=10$, we have <strong>solved the Bellman’s Optimality</strong></p>

<ul>
  <li>
    <p>there is exactly one pair of numbers, $V^<em>(h), V^</em>(l)$ that simultaneously satisfy these two nonlinear equations.</p>
  </li>
  <li>
    <p>essentially we can fill in the “cells” with values we found like in this example we discussed before</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011350739.png" alt="image-20220223011350739" style="zoom:67%;" /></p>
  </li>
</ul>

<h2 id="iterative-dp-methods">Iterative DP Methods</h2>

<p>If one can solve the Bellman’s optimality equations, then an optimal policy can be easily found and there is no more work to do. However, in reality you need to face the following problem:</p>

<ul>
  <li>even if all the information is known, computing for the solution takes <strong>huge computation power</strong></li>
  <li>In most cases of practical interest there are <strong>far more states</strong> than could possibly be entries in a table/hence need huge memory, and <strong>approximations</strong> must be made</li>
  <li>in reality, certain aspects of the environment <strong>may not be known</strong>, such as transition probabilities</li>
</ul>

<blockquote>
  <p>In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be <strong>approximated</strong> in some way</p>
</blockquote>

<p>Recall that for the Bellman’s equation for <strong>any policy</strong></p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

<p>and the <strong>optimality equations</strong>:</p>

\[\begin{align*}
V^* (s)
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&amp;= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}\]

<blockquote>
  <p><strong>Essentially</strong> dynamic programming methods:</p>

  <ul>
    <li>
      <p><mark>uses Bellman's equations as update rules</mark> and improve policy by $\arg\max$ gives <a href="#Policy Iteration">Policy Iteration</a></p>
    </li>
    <li>
      <p><mark>uses the optimality constraint as update rules</mark> for improving approximations of the desired value functions. This gives <a href="#Value Iteration">Value Iteration</a></p>
    </li>
  </ul>
</blockquote>

<h3 id="policy-evaluation">Policy Evaluation</h3>

<p>First we consider how to compute the state-value function $V_\pi$ for an arbitrary policy $\pi$, using Bellman’s equation.</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

<p>Then we have $\vert S\vert$ simultaneous linear equations since we have $V_\pi(s):s \in S$. Then the idea is to consider a <strong>sequence of approximate solutions $V_0,V_1,V_2,…$</strong> such that they obey:</p>

\[V_{k+1} (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]\]

<p>Then, it can be shown that $\lim_{k \to \infty} {V_k} = V_\pi$, i.e. converges. Therefore our algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224173242299.png" alt="image-20220224173242299" style="zoom:67%;" /></p>

<p>where the Bellman’s equation essentially will iterate over all possible $s’ \in S$ while using the current $V$.</p>

<ul>
  <li>
    <p>this algorithm is also called <strong>iterative policy evaluation</strong></p>
  </li>
  <li>
    <p>To produce each successive approximation, $V_{k+1}$ from $V_k$, iterative policy evaluation applies the same operation to each state $s$ by looking at the old values of $s$ and <strong>all possible one-step transitions</strong>, which is called a <mark>full backup</mark> since it looks at the entire tree:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222220640857.png" alt="image-20220222220640857" style="zoom:50%;" /></p>
  </li>
</ul>

<p>In reality, Algorithm 11 will need <strong>two arrays</strong>, one to keep track of old $V_{k-1}(s)$ and the other to fill in the new $V_k(s)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224175028170.png" alt="image-20220224175028170" style="zoom:50%;" /></p>

<p>Another variant would be to use one array and <strong>update the values in place</strong>, that is, with each new backed-up value immediately overwriting the old one.  (it can be shown that this <strong>also converges to $V_\pi$</strong>)</p>

<ul>
  <li>Then, depending on the order in which the states are backed up, sometimes new values are used instead of old ones on the right-hand side of Bellman’s equation</li>
  <li>since it uses new data as soon as possible, it actually <strong>converges faster</strong>. Hence this will be used more often in reality.</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the setup of the following environment, with each state/cell assigned a value for easier math definitions:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224175845046.png" alt="image-20220224175845046" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>non-terminal states are $S = {1,2…,14}$, and terminal states are the grey boxes</li>
  <li>We will assign a reward of $-1$ on all transitions until terminal state is reached. Therefore, the expected reward function becomes $r(s,a,s’)=-1$ for all states/actions.</li>
  <li>the movement will be deterministic, so that $p(6\vert 5,\text{Right})=1$, for example.</li>
</ul>

<p>We consider evaluating a policy $\pi$ being equi-probable for all actions. Then, we can <strong>iteratively compute $V_k$</strong> using the above algorithm, and at each $V_k$, we can compute the <strong>greedy policy w.r.t. $V_k$</strong>.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224180250255.png" alt="image-20220224180250255" style="zoom:67%;" /></p>

<p>notice that the greedy policy for $V_3,V_{10},V_\infty$ is actually an <strong>optimal policy in this setup</strong> (though it is obtained from the state value function from random policy). In general, it is only <mark>guaranteed</mark> that the <strong>last policy</strong>/bottom right would be an <mark>improvement over the given policy $\pi$</mark>.</p>

<ul>
  <li>this will hint at what we want to do in the next section</li>
</ul>

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Our reason for computing the value function for a policy is to help <strong>find better policies</strong>, just as what we have seen in the previous section. In general, it can be shown that</p>

<blockquote>
  <p><strong>Policy Improvement Theorem</strong></p>

  <p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s \in S$:</p>

\[Q_\pi(s, \pi'(s)) \ge V_\pi(s)\]

  <p>Then it <mark>must be that</mark></p>

\[V_{\pi'}(s) \ge V(s),\quad \forall s\]

  <p>Moreover, if there is a strict inequality in one of the state $Q_\pi(s, \pi’(s)) &gt; V_\pi(s)$, then there must be at least a strict inequality for $V_{\pi’}(s) &gt; V(s)$.</p>
</blockquote>

<p>A simple simple example to see how the theorem works is to consider a changed policy, $\pi’$, that is identical to $\pi$ except that $\pi(s’)=a \neq \pi(s)$ for <strong>only one $s \in S$.</strong> Then, if we know $Q_\pi(s,a) &gt; V_\pi(s)$ for that $s$, it follows that $V_{\pi’}(s) &gt; V_\pi(s)$.</p>

<p>Then, since it works for <strong>one $s \in S$</strong>, we can extend it to <strong>all $s \in S$</strong>: selecting at each state the action that appears best according to $Q_\pi(s,a)$ <mark>after you computed $V_\pi(s)$</mark>:</p>

\[\begin{align*}
\pi^\prime (s)
= \arg\max_{a \in A} Q_\pi(s,a)
&amp;= \arg\max_a\mathbb{E}_{\pi} \left[R_{t+1}+\gamma V_\pi(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \arg\max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\\
\end{align*}\]

<p>notice that:</p>

<ul>
  <li>By construction, the greedy policy meets the conditions of the policy improvement theorem, so we know that it is as good as, or better than, the original policy: it <strong>guarantees improvement</strong> unless we are optimal already.</li>
  <li>it is essentially a function of $V_\pi(s)$, which depends on the original $\pi$ and a converged $V_\pi$. (this should provide enough hint how to design an algorithm to find out best $\pi^*$)</li>
</ul>

<blockquote>
  <p><strong>Extension to Stochastic Policy</strong></p>

  <p>Recall that for a stochastic policy $\pi(s)=\pi(a\vert s)$ spit out a probability distribution:</p>

\[Q_\pi(s,\pi^\prime(s)) = \sum_a \pi^\prime (a|s)Q_\pi(s,a)\]

  <p>Then the idea is to basically, if there are several actions which the maximum can be achieved, we assign <strong>each of those actions a portion of the probability</strong> in the new greedy policy. A simple example of this would be the case we discussed before:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224191414214.png" alt="image-20220224191414214" style="zoom:67%;" /></p>

  <p>where the LHS was the state function for equi-probable stochastic policy, whereas the RHS is the improved greedy policy, which is still stochastic.</p>
</blockquote>

<h3 id="policy-iteration">Policy Iteration</h3>

<p>At this point, you know how to:</p>

<ol>
  <li>evaluate $V_\pi$ of a policy $\pi$</li>
  <li>improve a policy $\pi \to \pi’$</li>
</ol>

<p>Then we can basically repeat the above loop again and again to improve our policy. We can thus obtain a sequence of monotonically improving policies and value functions:</p>

\[\pi_0 \to V_{\pi_0}\to \pi_1 \to V_{\pi_1}\to .... \pi_* \to V_{\pi_*}\]

<p>Hence the algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224191804784.png" alt="image-20220224191804784" style="zoom:67%;" /></p>

<p>which works because a <strong>finite MDP has only a finite number of policies</strong>, this process must converge to an optimal policy and optimal value function in a finite number of iterations.</p>

<ul>
  <li>
    <p>While this looks like a painful computation heavy algorithm, it converges actually pretty fast, presumably because the value function changes little from one policy to the next are our policy is getting better</p>
  </li>
  <li>
    <p>but still, sometimes it takes a long time, in which case we can look at value iteration methods</p>
  </li>
</ul>

<h3 id="value-iteration">Value Iteration</h3>

<p>The key idea is that <strong>policy evaluation</strong> step of policy iteration can be <strong>truncated in several ways without losing the convergence</strong> guarantees of policy iteration. The idea is to <strong>combine policy improvement and evaluation</strong>:</p>

\[V_{k+1} (s)
= \max_a \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]\]

<p>which if you recall, was the optimality constraint:</p>

\[\begin{align*}
V^* (s)
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

<blockquote>
  <p>And it can be shown that this sequence $\lim_{k \to \infty }{V_k} \to V^*$ <strong>converges</strong> directly to optimal state value function.</p>
</blockquote>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224193336422.png" alt="image-20220224193336422" style="zoom:67%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement</p>
  </li>
  <li>
    <p>we can again use early stopping if it would run for a long time.</p>
  </li>
</ul>

<h2 id="monte-carlo-methods">Monte Carlo Methods</h2>

<p>In this chapter we consider our first learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we <mark>do not assume complete knowledge of the environment</mark>. Therefore, this big change means that MC methods requires only <em>experience</em>, i.e. <mark>sampling state/action/rewards</mark> from an environment.</p>

<blockquote>
  <p>Monte Carlo methods are ways of solving the reinforcement learning problem based on <strong>averaging sample returns</strong>. This places an assumption that it is defined for <strong>episodic tasks</strong>, instead of a continuous space.</p>

  <p>Specifically, features of this method include:</p>

  <ul>
    <li>
      <p>experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected</p>
    </li>
    <li>
      <p>only on the completion of an episode are value estimates and policies changed</p>
    </li>
  </ul>
</blockquote>

<p>Again, the key difference here is that before we <strong>computed value functions</strong> from knowledge of the MDP, here we <strong>learn value functions from sample returns</strong> with the MDP.</p>

<h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3>

<p>We begin by considering Monte Carlo methods for <strong>learning the state-value function</strong> for a given policy</p>

<blockquote>
  <p><strong>Key Idea</strong></p>

  <p>The value of a state $s$ is the <strong>expected return</strong> - expected cumulative future discounted reward - <strong>starting from that state</strong>. So an obvious idea to estimate it is simply to <strong>average the returns observed after visits to that state $s$</strong></p>
</blockquote>

<p>Suppose we wish to estimate $V_\pi(s)$, the value of a <strong>specific state $s$</strong>, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state s in an episode is called a <em>visit</em> to $s$, so that $s$ may be visited many times in an episode.</p>

<ul>
  <li>first-visit MC method estimates $V_\pi(s)$ as the <mark>average of the returns following first visits to $s$</mark></li>
  <li>every-visit MC method averages the returns following all visits to $s$</li>
</ul>

<p>Then the algorithm for First visit MC prediction is:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Abstraction</th>
      <th style="text-align: center">Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201733290.png" alt="image-20220224201733290" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201144309.png" alt="image-20220224201144309" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>the <code class="language-plaintext highlighter-rouge">if</code> statement in the highlight box basically checks if $S_t$ is the first time encountered in the sequence of states, action, reward pair.</li>
</ul>

<p>The every-visit versoin looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201238212.png" alt="image-20220224201238212" style="zoom: 67%;" /></p>

<p><strong>Both first-visit MC and every-visit MC converge</strong> to $V_\pi(s)$ as the number of visits (or first visits) to s goes to infinity. However, they do have certain different theoretic properties, which is not covered here. In reality, the first visit version is used more often.</p>

<p>Last but not least, we can also leverage DP to make the averaging easier as:</p>

\[\mu_t = \mu_{t-1} + \frac{1}{t}(x_t - \mu_{t-1})\]

<p>for each new data $x_t$ that would contribute to $\mu_t$.</p>

<hr />

<p><em>For Example:</em> Black Jack</p>

<p>This would be a nice exercise for you to think of how to convert a real life game into <strong>state/action/reward</strong> where we can use the above method to optimize.</p>

<p>Assuming you are clear of the rules, then</p>

<ul>
  <li>Each game of blackjack is an episode.</li>
  <li>Rewards of $+1,-1,0$ are given for winning, losing, and drawing, respectively.</li>
  <li>All <strong>rewards</strong> within a game are zero, and we do not discount ($\gamma = 1$); therefore these terminal rewards are also the returns.</li>
  <li>The player’s <strong>actions</strong> are to hit or to stick.</li>
  <li>The <strong>states</strong> depend on the player’s cards and the dealer’s showing card.</li>
</ul>

<p>We further impose the assumption that the deck is infinite, so you don’t need to remember what cards are dealt, and you are competing independently against the dealer. This then restricts our states and actions to:</p>

<ul>
  <li>the player makes decisions on the basis of three variables: his current sum  $[12-21]$, the dealer’s one showing card ($A-10$, and whether or not he holds a <em>usable</em> ace (if the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable)</li>
  <li>if you have sum below $12$, then you definitely call hit so that we don’t need to compute.</li>
</ul>

<p>This makes for a total of 200 states, and we can use MC method to compute best policy.</p>

<h3 id="backup-diagram-for-mc-method">Backup Diagram for MC Method</h3>

<blockquote>
  <p>The general idea of a backup diagram is to show at the top the <strong>root node to be updated</strong> and to show below all the transitions and leaf nodes <strong>whose rewards and estimated values contribute to the update</strong></p>
</blockquote>

<p>Graphically, MC methods when given an episode does:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Iterative Method</th>
      <th style="text-align: center">MC Method</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205607347.png" alt="image-20220224205607347" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>so the difference with those DP based method is that:</p>

<ul>
  <li>DP diagram (LEFT) shows <strong>all possible transitions</strong>, the Monte Carlo diagram shows only those <strong>sampled</strong> <strong>on the one episode</strong>.</li>
  <li>DP diagram includes only <strong>one-step transitions</strong>, the Monte Carlo diagram goes <strong>all the way to the end of the episode</strong></li>
</ul>

<blockquote>
  <p>In particular, note that the computational expense of estimating the value $V_\pi(s)$ of a single state is <strong>independent of the number of states</strong>. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states.</p>
</blockquote>

<h3 id="mc-for-action-values">MC for Action Values</h3>

<p>Again, if the model is not available (i.e. no external device can tell you $p(s’,r\vert s,a)$ directly, or $r(s,a)$), you will need sampling to compute action value functions $Q_\pi$. Luckily, you soon realize the idea is the same as Prediction for state value function</p>

<blockquote>
  <p>The policy evaluation problem for action values is to estimate $Q_\pi(s,a)$, the <strong>expected return</strong> when starting in state $s$, taking action $a$, and thereafter following policy $\pi$.</p>

  <p>Therefore, now we talk about <strong>visits to a state-action pair</strong> (state $s$ is visited and action $a$ is taken in it) rather than to a state in an episode. We estimates the value of a state-action pair as the <strong>average of the returns</strong> that have followed from:</p>

  <ul>
    <li>the first time in each episode that the state was visited and the action was selected - First Time Method</li>
    <li>or doing every time method</li>
  </ul>
</blockquote>

<p>The only complication is that many state-action pairs <strong>may never be visited</strong>.</p>

<ul>
  <li>
    <p>If $\pi$ is a deterministic policy, then in following $\pi$ one will observe returns <strong>only for one of the actions from each state</strong>.</p>

    <p>One way to fix this is by <em>specifying</em> that the episodes start in a state-action pair, and that <em>every pair has a nonzero probability</em> of being selected as the start.</p>
  </li>
  <li>
    <p>if policy $\pi$ is stochastic with a nonzero probability of selecting all actions in each state, then there is <strong>no problem</strong>. This is more reliable and used more often.</p>
  </li>
</ul>

<h3 id="monte-carlo-control">Monte Carlo Control</h3>

<p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate <strong>optimal policies</strong> using what we have built up so far (<strong>methods for evaluating value functions</strong>)</p>

<p>To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of <strong>policy evaluation and policy improvement</strong>, beginning with an arbitrary policy $\pi_0$ and ending with the optimal policy and optimal action-value function\</p>

\[\pi_0 \to q_{\pi_0} \to \pi_1 \to q_{\pi_1} \to ...\pi_* \to q_{\pi_*}\]

<p>which is like the DP iterative algorithm, but  now:</p>

<ul>
  <li>
    <p>Policy evaluation step: <strong>Generating episodes</strong> and using MC method to compute $q_{\pi_i}$ following that $\pi_i$</p>
  </li>
  <li>
    <p>Policy improvement: <strong>Making the policy greedy</strong> with respect to the current value function</p>

\[\pi_{k+1}(s) = \arg \max_a q_{k}(s,a)\]
  </li>
</ul>

<p>Hence the full algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224213351523.png" alt="image-20220224213351523" style="zoom:67%;" /></p>

<h2 id="temporal-difference-learning">Temporal-Difference Learning</h2>

<p>TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<ul>
  <li>Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics</li>
  <li>Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).</li>
</ul>

<p>Again this is a model-free approach.</p>

<blockquote>
  <p><em>Recall</em>: Model Free vs Model-Based</p>

  <p><strong>Model-based</strong>: the agent, while learning, can ask the model for the <em>expected</em> next reward, or the full <em>distribution</em> of next states and next reward. i.e. it has <em>complete information for next state, next reward</em>.</p>

  <ul>
    <li>e.g. by computer code that understands the rules of a dice or board game</li>
  </ul>

  <p><strong>Model-free</strong>: have nothing, purely sample from experience.</p>

  <ul>
    <li>e.g. MC methods, TD learning, etc.</li>
  </ul>
</blockquote>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$:</p>

<ul>
  <li>both methods update their estimate $V_\pi(s)$ for the nonterminal states $s_t$ occurring in that experience</li>
</ul>

<p>However, the difference is that:</p>

<ul>
  <li>
    <p>Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V_\pi(s_t)$, i.e.  estimates $V_\pi(s)$ as the average of the returns following first visits to $s$. The every-visit update rule can be generalized to:</p>

\[V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]\]

    <p>for $G_t$ is the actual return following time $t$, and $\alpha$ is a constant step-size parameter</p>
  </li>
  <li>
    <p>Whereas Monte Carlo methods must <strong>wait until the end of the episode</strong> to determine the increment to $V(s_t)$ (only then is $G_t$ known), TD methods need <strong>wait only until the next time step</strong>:</p>

\[V(s_t) \leftarrow V(s_t) + \alpha[R_{t+1} + \gamma V(s_{t+1}) - V(s_t)]\]
  </li>
</ul>

<p>Therefore, the big difference stems from the fact that the target value for MC update is $G_t$, but for TD update it is</p>

\[R_{t+1} + \gamma V(s_{t+1})\]

<p>The idea basically comes from the derivation that</p>

\[\begin{align*}
V_\pi (s)
&amp;= \mathbb{E}_\pi [G_t | S_t =s]\\
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1}+ \gamma V_\pi(s_{t+1})|S_t=s \right]\\
\end{align*}\]

<p>so that MC is using the first equality for estimating $V_\pi$, DP is using the last equality for estimating $V_\pi$.</p>

<ul>
  <li>Monte Carlo target is an estimate because the expected value in the first line is not known</li>
  <li>The DP target is an estimate because $V_{\pi}(s_{t+1})$ is not known</li>
</ul>

<p>Then TD is <strong>combining DP + MC</strong> by sampling form the environment like in the first equality, but uses DP for estimate $V_\pi(s)\approx V_\pi(s’)$ and converge for solution. Hence the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301170155096.png" alt="image-20220301170155096" style="zoom: 67%;" /></p>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Backoff Diagram for TD(0)</th>
      <th style="text-align: center">Backoff Diagram for MC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224223527026.png" alt="image-20220224223527026" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>On a larger scale, you are basically only looking ahead a single step</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224223446750.png" alt="image-20220224223446750" style="zoom: 50%;" /></p>

<hr />

<p><em>For Example</em></p>

<p>Consider predicting the time for you to return to your home from an office.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224225822737.png" alt="image-20220224225822737" style="zoom:67%;" /></p>

<p>then in this example:</p>

<ul>
  <li><strong>state</strong> is given as “leaving office”. “reach car”, etc.</li>
  <li><strong>return</strong> for each state is the actual time to go <em>from that state</em></li>
  <li><strong>value</strong> of each state $V_\pi(s)$ is the expected/actual time to go. Current <strong>estimated value</strong> for each state will there for be the “Predicted Time to Go”</li>
  <li>we pick $\gamma=1$</li>
</ul>

<p>Then, if we do MC method, then essentially updates are proportional to the difference between $G_t - V(s_t)$, where $G_t$ is the actual time in the end:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224230158369.png" alt="image-20220224230158369" /></p>

<p>essentially the error here would be the difference between “Actual Time to Go” and “Predicted Time to Go”</p>

<p>If we do TD method, then the update will be based on the <strong>next immediate observed value</strong> $R_{t+1}+\gamma V(s_{t+1})-V(s_t)$, hence it is <mark>Markov like</mark> and looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224230353766.png" alt="image-20220224230353766" /></p>

<p>where the length of the arrow is proportional to the error, proportional to the <strong>temporal difference</strong>.</p>

<h3 id="td-vs-mc-methods">TD vs MC Methods</h3>

<p>Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\alpha$, as long as $\alpha$ is chosen to be sufficiently small. The constant-$\alpha$ MC method also converges deterministically under the same conditions, but to a <strong>different</strong> answer.</p>

<hr />

<p><em>For Example</em></p>

<p>Suppose you observe the following eight episodes:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301175321270.png" alt="image-20220301175321270" style="zoom:67%;" /></p>

<p>where we would like to estimate $V(A),V(B)$:</p>

<ul>
  <li>
    <p>the true value of $V(B)$ will be clearly $6/8$ because six out of the eight times in state $B$ the process terminated immediately with a return of $1$ and the other two $0$. This would also be the answer that both MC and TD will converge to.</p>
  </li>
  <li>
    <p>the value of $V(A)$ differs:</p>
    <ul>
      <li>
        <p>Observe that $100\%$ of the times the process was in state $A$ it traversed immediately to $B$ (with a reward of $0$); and since we have already decided that $B$ has value $V(B)=3/4$ , therefore $A$ must have value $3/4$ as well. This would be what TD converges to (in TD, the update depends on the difference between current reward and <strong>next expected reward</strong>).</p>

        <p>Graphically:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301175551863.png" alt="image-20220301175551863" style="zoom:67%;" /></p>

        <p>which is basically <strong>Markov</strong></p>
      </li>
      <li>
        <p>The other reasonable answer is simply to observe that we have seen $A$ once and the return that followed it was $0$; we therefore estimate $V(A)$ as $0$​. This is the answer that batch Monte Carlo methods give.</p>

        <p>(recall that in MC, the update depends on the difference between current reward and <strong>final reward in the episode</strong>.)</p>
      </li>
    </ul>
  </li>
</ul>

<p>Consider optimal estimates in the sense that they <strong>minimize the mean-squared error from the actual returns</strong> in the training set, then</p>

<ul>
  <li>MC would converge to the state value that gives minimum squared error on the training data.</li>
  <li>TD estimate would be exactly correct for the maximum-likelihood model of the <strong>Markov process</strong> (i.e. based on first
modeling the Markov process, then computing the correct estimates given the model)</li>
</ul>

<hr />

<p><strong>Advantages of TD Method</strong></p>

<ul>
  <li>
    <p>an advantage over DP methods in that they do not require a model of the environment</p>
  </li>
  <li>most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an <strong>on-line</strong>, fully incremental fashion.</li>
  <li>even as it is online, for any fixed policy $\pi$, the TD algorithm described above has been proved to <strong>converge</strong> to $V_\pi$,</li>
</ul>

<h3 id="td-lambda">TD lambda</h3>

<p>Notice that another view of the difference between TD and MC is essentially their backoff diagram:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301133529126.png" alt="image-20220301133529126" style="zoom: 67%;" /></p>

<p>where we notices that:</p>

<ul>
  <li>TD looks at one step ahead in backoff diagram and updates <strong>only depend on the next state</strong></li>
  <li>MC looks at a full height/depth in backoff diagram and updates depends on all the state until terminal</li>
</ul>

<p>Then in TD $\lambda$, we essentially combine the two:</p>

<p><img src="https://miro.medium.com/max/1838/1*L-LUOyW5W-0gBxx80GHdHQ.png" alt="Reinforcement Learning — TD(λ) Introduction(1) | by Jeremy Zhang | Towards  Data Science" style="zoom:33%;" /></p>

<p>so that basically $TD(0)$ with $\lambda = 0$ is the same as the TD prediction we discussed in the previous section.</p>

<h2 id="onoff-policy-learning">On/Off Policy Learning</h2>

<p>On policy methods:</p>

<ol>
  <li>Estimate the value of a policy while using it for control (i.e. generating episodes)</li>
  <li>Evaluate or improve the policy that is used to make decision (e.g. take greedy step)</li>
</ol>

<p>Off policy methods</p>

<ul>
  <li>Evaluate or improve a policy <strong>different from that used to generate the data</strong></li>
  <li>Separate these two functions</li>
  <li>Behavior policy: policy used to generate behaviour</li>
  <li>Target policy: policy that is imitated and improved</li>
  <li>Follow behavior policy while improving target policy</li>
  <li>Reuse experience generated from old policie</li>
</ul>

<h3 id="sarsa">SARSA</h3>

<p>SARSA is an <strong>on-policy method</strong> using TD methods for the evaluation of a <strong>action value function</strong>. In particular, this is done by:</p>

<ol>
  <li>for an on-policy method we must estimate $Q_\pi(s,a)$ for the current behavior policy $\pi$ and for all states $s$ and actions $a$.</li>
</ol>

<p>Recall that an episode essentially looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301202009996.png" alt="image-20220301202009996" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>In the previous section we considered transitions from state to state and <strong>learned the values of states</strong>. Now we consider transitions from state-action pair to state{action pair, and learn the <strong>value of state-action pairs</strong></p>
  </li>
  <li>
    <p>therefore, as it is TD learning, we consider the update rule being</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) ]\]

    <p>This update is done after every transition from a nonterminal state $s_t$.</p>

    <p>This rule uses every element of the quintuple of events, $(s,a,r’,s’,a’)$, hence it is called SARSA</p>
  </li>
</ul>

<p>As in all on-policy methods, we continually <strong>estimate $Q_\pi$ for the behavior policy $\pi$,</strong> and at the same time <strong>change $\pi$ toward greediness</strong> with respect to $Q_\pi$.</p>

<p>Algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301201435273.png" alt="image-20220301201435273" style="zoom: 50%;" /></p>

<p>where</p>

<ul>
  <li>in Sutton and Bartol, the “improve policy” step is also called “policy derived from $Q$ from $\epsilon$-greedy”</li>
  <li>it can be shown that this will converge to some <strong>optimal $Q_\pi$</strong>, hence returning optimal policy</li>
  <li>note that this update of $Q_\pi$ is <strong>updating  $Q_\pi$ for the $\pi$ it is currently following</strong>. Hence it is called <mark>on policy</mark>.</li>
</ul>

<h3 id="q-learning">Q-Learning</h3>

<p>Recall that we know:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

<p>But we know that here we are model-less, hence we <em>don’t know $p(s’,r\vert s,a)$</em>. The idea is to essentially will learn the $Q$ function directly from experience, known as Q-learning:</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]\]

<p>comes from the <strong>optimality constraint</strong>, where the target is $R_{r+1} + \gamma \max_a Q(s_{t+1},a)$:</p>

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]
\end{align*}\]

<blockquote>
  <p><strong>Therefore</strong>, Q-learning <strong>directly approximates $Q^*$</strong> using MC + DP like approach: TD update rule.</p>
</blockquote>

<p>Hence, the algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301203045085.png" alt="image-20220301203045085" style="zoom: 80%;" /></p>

<p>where in this case</p>

<ul>
  <li>the TD prediction task is to <strong>evaluate $V_\pi(s)$</strong> when given $\pi$, but here we can directly find $Q^<em>$ hence $\pi^</em>$!</li>
  <li>note that this update of $Q$ is <strong>updating  $Q$ for the $\pi^<em>$** by directly estimating $Q^</em>$, but it **behaves by $\pi_b$</strong>, which could be a policy with equal probability for every action. Hence it is <strong>not</strong> estimating $Q_\pi$ for the $\pi$ that generated the action like the one in SARSA. Hence it is called <mark>off policy</mark>.
    <ul>
      <li>this way it is also continuously exploring</li>
    </ul>
  </li>
</ul>

<p>Since this is TD like rule, the backoff tree is also Markov like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224232119132.png" alt="image-20220224232119132" /></p>

<p>where updating the root node only depends on nodes one step away.</p>

<hr />

<p><em>For Example</em>: Cliff Walking</p>

<p>This example aims to compare the difference of the optimal policy learnt by SARSA and Q-learning. Consider the game of cliff walking, where you will have a start state $S$, and an end state $G$. Additionally:</p>

<ul>
  <li>each state along any element will have reward of $-1$</li>
  <li>if you fall off the cliff (i.e. on the cliff cell), you have $-100$ and will be sent back to the start state</li>
  <li>the available actions are still left, right, up and down</li>
</ul>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301205614330.png" alt="image-20220301205614330" /></p>

<p>Interestingly, assuming our action will be $\epsilon$-greedy (i.e. optimal for $1-\epsilon$ of the time)</p>

<ul>
  <li>
    <p><strong>SARSA</strong> will converge to the <strong>safe path</strong> while <strong>Q-learning</strong> to the <strong>optimal</strong> path</p>

    <ul>
      <li>so Q-learning results in its occasionally falling off the cliff because of the $\epsilon$-greedy action selection</li>
      <li>SARSA takes the action selection into account and learns the longer but safer path through the upper part of the grid</li>
    </ul>

    <p>So in a sense Q-learning is <strong>optimistic</strong> about what happens when an action $a$ is taken, while SARSA is <strong>realistic</strong> about it.</p>
  </li>
  <li>
    <p>Of course, if $\epsilon$ were gradually reduced, then <strong>both</strong> methods would asymptotically converge to the optimal policy.</p>
  </li>
</ul>

<p>For those who are interested, the learning curve looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301210416783.png" alt="image-20220301210416783" style="zoom:80%;" /></p>

<h2 id="maximum-entropy-rl">Maximum Entropy RL</h2>

<p>Before, our objective for those algorithms are to learn the <strong>optimal policy by maximizing the expected return</strong>.</p>

\[\pi^* = \arg\max_\pi \mathbb{E}[G_t]\]

<p>However, another interesting objective to consider is to <strong>also maximize the entropy of actions</strong>\pi^* = \arg\max_\pi \mathbb{E}[G_t]</p>

\[\pi^* = \arg\max_\pi \mathbb{E}[G_t] + H_\pi(a|s)\]

<p>where the entropy term is</p>

\[H_\pi(a|s) = \sum_t H_\pi(a_t|s_t) = \sum_t \mathbb{E}[-\log \pi(a_t|s_t)]\]

<p>Optimizing this objective will promote both high return and exploration. This then will lead to designing loss functions when doing NN based reinforcement learning.</p>

<h2 id="summary-of-rl-models">Summary of RL Models</h2>

<p>In general, we have covered RL methods that are</p>

<ul>
  <li><strong>Value-based</strong>
    <ul>
      <li>Estimate value function $Q^<em>(s,a)$ or $V^</em>(s)$</li>
      <li>then return policy by greedy</li>
    </ul>
  </li>
  <li><strong>Policy-based</strong>
    <ul>
      <li>Search directly for optimal policy $\pi$ Achieving maximum future reward</li>
    </ul>
  </li>
  <li><strong>Model-based</strong>
    <ul>
      <li>here it refers to you are either given the model/interaction environment</li>
      <li>or you are learning the model itself
        <ul>
          <li>Build transition model of environment</li>
          <li>Plan by lookahead using model (i.e. ask the model what the reward if action $a$ is taken, then choose best action)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Taxonomy:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301135343105.png" alt="image-20220301135343105" style="zoom: 50%;" /></p>

<hr />

<p><em>For Example</em>: World Model</p>

<blockquote>
  <p>Our <em>world model</em> can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment</p>
</blockquote>

<p>The architecture is basically as follows, where as the observation is the game</p>

<p><img src="https://miro.medium.com/proxy/1*9WsfCbPtdoKzL5IPThLWRg.png" alt="World models — a reinforcement learning story | by SmartLab AI | Medium" style="zoom:50%;" /></p>

<p>then, notice that the controller $C$ output also to the RNN model, which <strong>models the game</strong> (in-game graphics)</p>

<ul>
  <li>therefore, once trained, you can essentially play by just using RNN model, by feeding in actions to it and let it generate the output observaton/images</li>
  <li>since the policy it learns will then be <strong>based on how $M$ models the game</strong>, it is essentially learning the model.</li>
</ul>

<h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>

<p><em>Recall that</em></p>

<p>The previous chapter presents MDPs and reinforcement learning. A key difference between the two is that</p>

<ul>
  <li>when solving MDPs we know the transition function $T$ and reward function $R$</li>
  <li>in reinforcement learning we do not know the transition or reward functions. In reinforcement learning an agent samples the
environment and the previous chapter ends with the Q-learning algorithm which learns $Q^*(s, a)$ from experience.</li>
</ul>

<p>But regardless which one we use, they are all <strong>tabular methods</strong> in nature, i.e. we need to he value function in some table. (e.g. for a state function with $\vert S\vert$ states, your table will be $\vert S\vert$ in size)</p>

<hr />

<p>In many cases, storing the $Q$ values in a table may be infeasible when the state or action spaces are very large or when they are continuous. For example, the game of Go consists of $10^{170}$ states, or when the state or action spaces include continuous variables or complex sensations. A solution is to <strong>approximate the value function or approximate the policy</strong>.</p>

<blockquote>
  <p>In Deep RL, we essentially take examples from a desired function (e.g., a value function) and attempts to generalize from them
to construct an <strong>approximation of the entire function</strong> (e.g. in the entire continuous domain).</p>

  <ul>
    <li>e.g. instead of predicting image classes using CNN, we may <strong>predict</strong> the values of a state or the <strong>probabilities of actions $\pi(s\vert a)$ using a neural network</strong>, and based on these probabilities we can take action</li>
  </ul>
</blockquote>

<p>Then the idea for using NN as function approximation in RL can be shown below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301135857210.png" alt="image-20220301135857210" style="zoom: 67%;" /></p>

<p>where essentially:</p>

<ul>
  <li>Given a state $s$ as input, such as an image of pixels</li>
  <li>neural network outputs an approximated vector of probabilities for each action given the state, i.e. $\pi_\theta(a\vert s)$</li>
  <li>finally we can pick $a_t$ be the action by, e.g. taking the highest probability</li>
</ul>

<p>To train the model in a supervised case, we have in general</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">General Supervised ML</th>
      <th style="text-align: center">Supervised RL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301134342206.png" alt="image-20220301134342206" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301134356226.png" alt="image-20220301134356226" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>So that in a supervised case, we would need ground truth actions $a$ as the label to train the neural network to learn the stochastics policy $\pi_\theta(a\vert s)$ with weights $\theta$.</p>

<h2 id="function-approximations">Function Approximations</h2>

<p>As mentioned before, real-world problems often consists of large or continuous state and action spaces, so tabular methods introduced in the previous section will not work for finding $V_\pi(s)$ or $Q_\pi(a\vert s)$. Hence, we resort to use <strong>deep neural network</strong> to find <strong>approximations such as</strong></p>

<ul>
  <li>$V_\theta(s) \approx V_\pi(s)$ for state value</li>
  <li>or $Q_\theta(s,a)\approx Q_\pi(s,a)$ for action value</li>
  <li>or $\pi_\theta(a\vert s)$</li>
</ul>

<p>using only some finite number of weights usually much smaller than the total number of possible states.</p>

<p>However, the techniques learnt in previous sections aren’t useless, recall that all the <strong>back-off operations</strong> we were doing for estimating some value function at state $s$:</p>

<ul>
  <li>MC does $V(s_t) \to G_t$, shifting the estimated value towards $G_t$ (by some fraction), for $G_t$ being the <strong>target</strong></li>
  <li>TD(0) does $V(s_t) \to R_{t+1} + \gamma V(s’)$</li>
  <li>TD($\lambda$) does $V(s_t) \to G_t^\lambda$</li>
  <li>DP does $V(s_t) \to \mathbb{E}<em>\pi[R</em>{t+1} + \gamma V(s’)\vert S_t=s]$ which is exact since model would be given.</li>
</ul>

<p>Also, notice that the first three will backup the state $s_t$ only when encountered in some episode, whereas the last one will update every single $s$ as they are known.</p>

<blockquote>
  <p>Another way to view this would be to think of the backoff shift as an <strong>input-output pair</strong>: the input of the function will be $s_t$, and you <strong>want</strong> the <strong>output to be close to the RHS of the expression above</strong>. Hence this could be used as a supervised dataset!</p>
</blockquote>

<h3 id="state-value-function-approximation">State Value Function Approximation</h3>

<p>As we are training a NN to approximate $V_\pi(s)$, e.g. w.r.t. <mark>some policy $\pi$</mark>, we first need some definition of <strong>loss function</strong>.</p>

<ul>
  <li>Most supervised learning methods seek to minimize the <strong>mean-squared error (MSE)</strong></li>
  <li>the label/<strong>ground truth</strong> will be $V_\pi(s)$ which is unknown, but can be any of the <strong>approximate target</strong> mentioned above, e.g. $R_{t+1} + \gamma \hat{V}_\theta(s’)$.</li>
</ul>

<p>Therefore, often our loss would be:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (V_\pi(s) - V_\theta(s))^2 = \frac{1}{2}\mathbb{E}_s[(V_\pi(s) - V_\theta(s))^2] \iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]\]

<p>where:</p>

<ul>
  <li>technically this will be a <strong>weighted version</strong> of MSE because $\sum_s \mu(s)=1$ gives relative importance to learn certain states. (because the number of parameters we learn will be much less than $\vert S\vert$, we cannot find the exact solution)</li>
  <li>$V_\theta(s)$ will be our approximate, and we want to learn $\theta$.</li>
  <li>in reality we use the sum term for loss, but for doing math using the $\mathbb{E}_s$ notation will be easier.</li>
  <li>so basically the <strong>label is $V_\pi(s)$</strong>, hint that we essentially have a <mark>supervised training</mark>.</li>
</ul>

<p>This general form is differentiable, simply:</p>

\[\nabla_\theta J(\theta) = - \mathbb{E}_s[(V_\pi(s)-V_\theta(s))\cdot \nabla_\theta V_\theta(s)]\\
\theta_{i+1} = \theta_i - \alpha \nabla_\theta J(\theta_i)\]

<p>for the gradient descent update. If we use <strong>stochastic gradient descent of a single sample</strong>, then we throw away the expectation/weighted average of the samples and do:</p>

\[\theta_{i+1} = \theta_i + \alpha (V_\pi(s_i) - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]

<p>where now the only <strong>known is $V_\pi(s)$</strong>. Hence, we consider using an <strong>estimate such as the four targets mentioned before</strong></p>

<ul>
  <li>
    <p>MC learning with $V_\pi(s_i) \approx G_i$ being the return following state $s_i$:</p>

\[\theta_{i+1} = \theta_i + \alpha (G_i - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]
  </li>
  <li>
    <p>TD(0) learning with</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma V_\theta(s_{i+1})- V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]
  </li>
</ul>

<blockquote>
  <p>Not all approximation targets are <strong>unbiased</strong>. By unbiased, we need:</p>

\[\mathbb{E}[V_i] = V_\pi(s_i)\]

  <p>where $V_i$ we inserted estimates such as $G_i$ in MC method.</p>

  <ul>
    <li>for MC, this is <strong>unbiased</strong> because $\mathbb{E}[G_i] = V_\pi(s_i)$​ by definition of value function. Therefore, this converges to a locally optimal approximation to $V_\pi(s_i)$.</li>
    <li>but for methods such as TD($\lambda$), it can be shown that $\lambda &lt; 1$​ has a <strong>biased estimate</strong>, hence does not actually converge to a local optimum. (Nevertheless, such bootstrapping methods can be quite effective, and other performance guarantees are available for important special cases)</li>
  </ul>
</blockquote>

<p>Essentially, though we do not know the ground truth $V_\pi(s)$, we can assume that every sample we got from reality <strong>is the ground truth</strong>, hence those update rules.</p>

<h3 id="action-value-function-approximation">Action-Value Function Approximation</h3>

<p>Here the neural network inputs are the states $s$ and actions $a$ and the network parameterized by $\theta$ outputs a value $Q_\theta(s, a)$.</p>

<p>Then, in the same line, our objective would be be minimizing the MSE between the approximate action-value function $Q_\theta(s, a)$ and $Q_\pi(s,a)$. The idea is basically the same as above:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q_\pi(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q_\pi(s,a) - Q_\theta(s,a))^2]\]

<p>Then computing the gradient and updating in SGD fashion:</p>

\[\theta_{i+1} = \theta_i + \alpha (Q_\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>where since we don’t know $Q_\pi(s,a)$, we use approximates such as:</p>

<ul>
  <li>
    <p>MC method:</p>

\[\theta_{i+1} = \theta_i + \alpha (G_i - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]
  </li>
  <li>
    <p>TD(0) learning</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma Q_\theta(s_{i+1},a_{i+1}) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Both of the function approximation methods only approximates $Q_\pi(s,a)$ or $V_\pi(s)$ for some $\pi$. It <strong>does not improve $\pi$</strong>, hence it is purely doing the evaluation step.</p>
</blockquote>

<h2 id="model-free-methods">Model-Free Methods</h2>

<p>Challenges of function approximation in the reinforcement learning setting include that</p>

<ul>
  <li>the agent’s experience is <strong>not independent and identically distributed</strong> (IID)</li>
  <li>the agent’s policy affects the future data it will sample</li>
  <li>the environment may change, i.e. our <strong>target is moving</strong> (hence certain NN should not be used as they assume stationary target)</li>
  <li>may not converge (e.g. biased approximations)</li>
</ul>

<p>And this section as well as the following are more models that help overcome those challenges. Here, we discuss model-free methods.</p>

<p>Model-free approaches may be divided into</p>

<ol>
  <li>value-based or Q-leaning methods such as NFQ (Riedmiller 2005) and DQN (Mnih et al. 2015)</li>
  <li>policy-based or policy optimization methods such as PPO (Schulman et al. 2017), and</li>
  <li>actor-critic methods such as DDPG (Lillicrap et al. 2016) which are a combination of both (i) and (ii).</li>
</ol>

<h3 id="experience-replay">Experience Replay</h3>

<p>In supervised learning the training examples may be sampled independently from an underlying distribution, but here data are corelated in time as we are taking actions.</p>

<blockquote>
  <p>A solution to this problem, known as experience replay, is to use a replay buffer that stores a collection of previous states, actions, and rewards, specifically storing tuples of $(s, a, r, s’)$</p>

  <p>Then each saved experience tuple may be <strong>sampled</strong> and used for updating the network weights.</p>
</blockquote>

<p>This means an experience tuple may be used multiple times, which is an efficient use of the data.</p>

<h3 id="neural-fitted-q-learning">Neural Fitted Q-Learning</h3>

<p>Recall that in value approximation we are only approximating $Q_\pi(s,a)$ for some $\pi$. In order to <em>*directly find $\pi^</em>$<strong>, we can use the idea from **Q-learning</strong> which directly finds $\pi^*$:</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]\]

<p>Therefore, the <strong>analogy</strong> here is to do, similarly:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q^*(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q^*(s,a) - Q_\theta(s,a))^2]\iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]\]

<p>where the update rule is the same using SGD:</p>

\[\theta_{i+1} = \theta_i + \alpha (Q^*\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>finally <strong>approximating $Q^*$</strong> using the update from Q-learning:</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q(s_{i+1},a) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<blockquote>
  <p>Q-learning diverges when using a neural network since there are correlations between the samples and the target is non-stationary.</p>

  <p>Therefore, to remove the correlations between samples we may generate a data set from the agent’s experience and use it as a <strong>supervised training</strong></p>
</blockquote>

<p>Then the algorithm becomes:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220302001029747.png" alt="image-20220302001029747" style="zoom:67%;" /></p>

<p>where essentially:</p>

<ul>
  <li>the pocliy we used to explore the world is just using $Q_\theta$, which is just a <strong>forward pass of the NN</strong></li>
  <li>putting this in a <mark>supervised</mark> fashion, labels become $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$.</li>
  <li>notice that states and rewards are generated by the environment and therefore the algorithm is <strong>model free</strong></li>
</ul>

<h3 id="deep-q-network">Deep Q-Network</h3>

<blockquote>
  <p>Deep Q-networks (DQN) (Mnih et al. 2015) build upon fitted Q-learning by incorporating a replay buffer and a <strong>second target neural network</strong>.</p>
</blockquote>

<p>In NFG we consider labels being $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$, but here we consider <strong>another network with $\theta-$</strong> such that:</p>

\[y_i = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a)\]

<p>with $\theta-$ being the <strong>target network</strong>. We uses this by making it appear static as compared to the ever changing $Q_\theta$ so that:</p>

\[\mathcal{L}(\theta_i) = \mathbb{E}_{(s,a,r,s')\sim D}[(y_i - Q_{\theta_i}(s_i,a_i))^2] = \mathbb{E}[(R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i))^2]\]

<p>then doing SGD the gradient update step is:</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) - Q_{\theta}(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>Then the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220302003105418.png" alt="" /></p>

<p>where notice that:</p>

<ul>
  <li>since we have a target $\hat{Q}$ that is updated much less frequently, this is helping the problem of chasing a non-stationary target as in NFQ. The label $y_i$ becomes dependent on $\hat{Q}<em>{\theta-}$ rather than $Q</em>\theta$</li>
  <li>notice that states and rewards are <strong>generated from $Q_\theta$</strong>, but policy learnt is <strong>based on $Q_{\theta-}$</strong>. Hence this is <strong>off-policy</strong> because the action value function learnt is towards a different policy that generated the data.</li>
</ul>

<h3 id="prioritized-replay">Prioritized Replay</h3>

<p>Before we are sampling from the replay buffer uniformly, prioritized experience replay (Schaul, Quan, Antonoglou &amp; Silver 2016) samples <strong>important transitions more frequently</strong> which results in more efficient learning.</p>

<blockquote>
  <p>We define more important transitions as the ones we <strong>made the large DQN error</strong></p>

\[\text{Err}(s_i,a_i) = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i)\]

</blockquote>

<p>Then, we prioritize them by:</p>

\[\text{Priority}(s_i,a_i)=\frac{p_i^\alpha}{\sum_j p_j^\alpha}\]

<p>where $p_i$ would be proportional to DQN error and $\alpha$ is hyper-parameter controlling the amount of prioritization</p>

<ul>
  <li>with $\alpha=0$ you have no prioritization and gets back uniform sampling</li>
</ul>

<h2 id="policy-based-methods">Policy-based Methods</h2>

<p>One problem with value based methods in the previous chapter such as NFQ and QDN cannot <em>easily deal with</em> <strong>continuous actions</strong>. Since we only know/approximated $Q^*(s,a)$, to convert that to action we need to do something like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Value Function</th>
      <th style="text-align: center">Policy/Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304205646509.png" alt="image-20220304205646509" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304205627824.png" alt="image-20220304205627824" /></td>
    </tr>
  </tbody>
</table>

<p>but imagining those cells being <strong>infinitesimally small and infinitesimally many</strong> so that the action space is continuous. This will be a headache to optimize over.</p>

<p>Then, the idea is why not <strong>approximate policy itself as a continuous function</strong>?</p>

<blockquote>
  <p><strong>Policy-based methods</strong> work well in continuous spaces for <strong>learning stochastic policies</strong>, the most common example would be having:</p>

\[a \sim \mathcal{N}(\mu_\theta(s), \sigma^2_\theta(s))\]

  <p>where we learn $\theta$ from NN.</p>
</blockquote>

<p>First, we present the general setup of obtaining the objective function and its gradients.</p>

<p>Consider starting with some policy $\pi_\theta$ (e.g. randomly initialized):</p>

<ol>
  <li>
    <p>an agent then can interact wit the environment to generate (probabilistically) a <strong>trajectory of state-action-reward episodes</strong>:</p>

\[\tau = s_0,a_0,r_0,....,s_t,a_t,r_t\]

    <p>and hence obtain a <strong>return $g(\tau)$</strong>:</p>

\[g(\tau) = \sum_t \gamma^t r_t\]
  </li>
  <li>
    <p>Then the <strong>value of this policy</strong> would be the the <strong>expected/average return</strong> over all trajectories (since the policy is probabilistic):</p>

\[\mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right]\]
  </li>
  <li>
    <p>Therefore, naturally we say that the <strong>best policy $\pi_\theta$ would have the highest value</strong>:</p>

\[\max_\theta \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right] \equiv \max_\theta J(\pi_\theta)\]
  </li>
</ol>

<p>Now, the we can basically do <strong>gradient ascent</strong> by considering:</p>

\[\theta = \theta + \alpha \nabla_\theta J(\pi_\theta)\]

<h3 id="policy-gradient">Policy Gradient</h3>

<p>The final step is to analytically find the gradient $\nabla_\theta J$. We will derive everything exactly and then replace the integral with the practical sum:</p>

\[J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \int g(\tau) p_\theta(\tau) d\tau \iff \sum_\tau p_\theta(\tau)g(\tau)\]

<p>but notice that since $\tau$ is a <strong>trajectory generated by $\pi_\theta$</strong>, we can further find:</p>

\[p_\theta(\tau) = \prod_t p(s_{t+1}|s_t,a_t)\pi_\theta(a_t|s_t)\]

<p>Now, consider taking the derivative:</p>

\[\nabla_\theta J = \int \nabla_\theta[g(\tau) p_\theta(\tau)]d\tau  =\int g(\tau)\nabla_\theta p_\theta(\tau) d\tau\]

<p>Now we <em>can</em> plugin the expression for $p_\theta(\tau)$, but we can <strong>use a trick to remove the product</strong> by considering:</p>

\[\nabla_\theta \log p_\theta(\tau) = \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)}\]

<p>hence:</p>

\[\nabla_\theta J =\int g(\tau)p_\theta(\tau)\nabla_\theta \log p_\theta(\tau) d\tau = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau) \nabla_\theta \log p_\theta(\tau)]\]

<p>Now we substitute in the $p_\theta(\tau)$:</p>

\[\nabla_\theta \log p_\theta(\tau) = \nabla_\theta \sum_t[\log [(s'|s,a) + \log \pi_\theta(a|s)]] =  \sum_t\nabla_\theta \log  \pi_\theta(a|s)\]

<p>so we get finally:</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]\]

<p>which we can then <strong>estimate expected value using sums</strong> in practice.</p>

<h3 id="reinforce-algorithm">REINFORCE Algorithm</h3>

<blockquote>
  <p>The REINFORCE algorithm estimates the policy gradient numerically by Monte- Carlo sampling: using <strong>random samples</strong> to <strong>approximate the policy gradient</strong>.</p>
</blockquote>

<p>The idea is simple:</p>

<ol>
  <li>set $\nabla_\theta J=0$ for the start. We are treating each episode IID.</li>
  <li>sample a trajectory $\tau$</li>
  <li>compute $\nabla_\theta J = \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a\vert s)$ in this case</li>
  <li>ascent and repeat</li>
</ol>

<p>Then the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304214257499.png" alt="image-20220304214257499" style="zoom:67%;" /></p>

<p>however, some problems with this algorithm is that:</p>

<ul>
  <li>updating $\theta$ <mark>will change $\pi_\theta$</mark>, which will be needed to <mark>generate $\tau$, the experiences</mark>. Hence there <strong>might be convergence problems</strong>.</li>
  <li>additionally, the above also means this policy gradient have <strong>high variance</strong></li>
</ul>

<h3 id="score-function-generality">Score Function Generality</h3>

<p>Additionally, the general form of:</p>

\[g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \to g_t(\tau)\cdot \text{score}(\pi_\theta)\]

<p>and the $\text{score}$ expression appears in <strong>many places when you do gradient</strong>. Example include:</p>

<ul>
  <li>Q actor-critic: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot V_t]$</li>
  <li>TD: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot \delta]$</li>
  <li>etc</li>
</ul>

<h3 id="policy-gradient-baseline">Policy Gradient Baseline</h3>

<p>The estimate of a gradient over <strong>many sampled experiences $\tau$</strong> would be:</p>

\[\nabla_\theta J  \approx \hat{g} = \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)})\]

<p>but notice that if we consider a constant/variable $b$ that is <strong>independent on $\theta$</strong>:</p>

\[\frac{1}{n}\sum_{i=1}^n (g(\tau^{(i)}) - b) \nabla_\theta \log p_\theta(\tau^{(i)}) =  \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)}) - \frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)})\]

<p>but notice that for $\lim n\to \infty$</p>

\[\frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)}) \to \sum_\tau b \nabla_\theta \log p_\theta  (\tau)\cdot p_\theta(\tau)\]

<p>and we can show thiat this is zero:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304215546064.png" alt="image-20220304215546064" style="zoom:67%;" /></p>

<p>so to measure performance we will usually <strong>subtract of some baseline $b$</strong> in the model, which is still an unbiased model.</p>

<h2 id="actor-critic-methods">Actor-Critic Methods</h2>

<p>On problem with policy-based method is that it updates $\theta$ whose $\tau$ sampled next depends on. This means that there could be <strong>high variance</strong> in the gradient which might cause <strong>convergence problems</strong>. The idea of this chapter is to <mark>solve</mark> this using <mark>value function approximation to reduce this variance</mark>.</p>

<blockquote>
  <p>Actor-critic methods <strong>combine policy-based methods</strong> with <strong>value-based methods</strong> by using both the policy gradient and value function.</p>
</blockquote>

<p>The basic step up will be a GAN like model:</p>

<ul>
  <li><strong>actor</strong>: a policy network $\pi_\theta$ with parameters $\theta$</li>
  <li><strong>critic</strong>: a value network that contains $V_\phi(s)$ or $Q_\phi(s,a)$, or the advantage function $A_\phi(s,a)=g(\tau) - V_\phi(s)$</li>
  <li><strong>critic provides a loss function for the actor</strong> and the gradients backpropagate from the critic to the actor</li>
</ul>

<p>Since $g(\tau)$ changes every time we sampled. the idea is we <strong>swap out $g(\tau)$</strong> in the update rule:</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]\]

<p>for values such as:</p>

<ul>
  <li>$g(\tau) \to Q_\phi(s,a)$, which leads to <strong>Q-value actor-critic</strong></li>
  <li>$g(\tau) \to A_\phi(s,a) = g(\tau) - V_\phi(s)$ for advantage</li>
</ul>

<p>for some <strong>already fitted value functions</strong>.</p>

<p>Therefore, this means the algorithm looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304220900366.png" alt="image-20220304220900366" style="zoom:67%;" /></p>

<p>where the:</p>

<ul>
  <li>the yellow part updates the critic parameter, and the red updates actor</li>
</ul>

<h3 id="advantage-actor-critic">Advantage Actor-Critic</h3>

<p>Here we essentially consider swapping out $g(\tau)$ for $A_\phi(s,a)$ defined by:</p>

\[A_\phi(s,a) = \mathbb{E}_{r,s'}[r+ \gamma V_{\pi_\theta}(s')  - V_{\pi_\theta}(s)]\]

<p>which can be <strong>estimated by TD error</strong> $r+\gamma V_\phi(s’)-V_\phi(s)$, hence resulting in the objective being</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \nabla_\theta \log  \pi_\theta(a|s) \cdot \gamma^{t-1}(r+\gamma V_\phi(s')-V_\phi(s)\right]\]

<h3 id="natural-policy-gradient">Natural Policy Gradient</h3>

<p>In reinforcement learning the data set collected depends on the policy, which has the following <strong>risk</strong>:</p>

<ul>
  <li>if updated parameters result in a poor policy</li>
  <li>will result in poor samples</li>
  <li>algorithm stuck at poor policy</li>
</ul>

<blockquote>
  <p>Therefore, when optimizing policy-based methods <strong>choosing a step size</strong>/learning rate for updating the policy parameters is (one of the) key.</p>
</blockquote>

<p>In general, we had:</p>

\[\theta ' =\theta \gets \theta + \alpha \nabla_\theta J(\pi_\theta) = \theta + \Delta \theta\]

<p>one idea is to <strong>constrain the size of this change of $J(\pi_\theta)$</strong> by considering Taylor expansion and <strong>restricting the latter term</strong></p>

\[J(\pi_{\theta'}) \approx J(\pi_\theta) + \nabla_\theta J(\pi_\theta)^T \Delta \theta\]

<p>with</p>

\[\max_{\theta'} \nabla_\theta J(\pi_\theta)^T \Delta \theta \quad \text{s.t.}\quad ||\Delta \theta||_2^2 \le \epsilon\]

<p>in our case $\Delta \theta = \alpha \nabla_\theta J(\pi_\theta)$. So this can be solved analytically with:\</p>

\[\Delta \theta = \sqrt{2 \epsilon} \frac{\nabla_\theta J(\pi_\theta)}{||\nabla_\theta J(\pi_\theta)||}\]

<p><strong>Alternatively</strong>, we can directly <strong>instead of constraining $\theta$, which constrains $\pi_\theta$</strong>, we can constrain the <strong>episode trajectory</strong> itself. Since the trajectory is essentially a probability distribution, we can use KL divergence can constraint that:</p>

\[D_{KL}[p(\tau|\pi_\theta)\, |\, p(\tau |  \pi_{\theta'})] \le \epsilon\]

<p>Then this result in</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304223202945.png" style="zoom: 67%;" /></p>

<p>which again can be solved analytically, and this called the <strong>natural gradient</strong></p>

\[\Delta \theta =F^{-1}_\theta \nabla_\theta J(\pi_\theta)\sqrt{\frac{2\epsilon}{||\nabla_\theta J(\pi_\theta)^T F_\theta^{-1}\nabla_\theta J(\pi_\theta)||}}\]

<p>and $\nabla_\theta J(\pi_\theta)$ and $F_\theta$ may be approximated by sampling trajectories using conjugate gradient descent</p>

<blockquote>
  <p><strong>Note</strong> that since we are only dealing with $\Delta \theta$ for the first term in approximation, we are essentially using <strong>first order methods</strong>.</p>
</blockquote>

<h3 id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</h3>

<p>Essentially a method based on the natural gradient, but here we are:</p>

<ul>
  <li>using second order method but <strong>approximating the hessian</strong> by using <strong>conjugate gradient</strong></li>
  <li>alike NPG, constrain the surrogate loss by the <strong>KL divergence</strong> between the new and old policy</li>
</ul>

<p>This results in the following algorithm</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135351156.png" alt="image-20220303135351156" style="zoom: 50%;" /></p>

<p>where surrogate loss is basically the loss that is caused by different policy trajectories.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135530009.png" alt="image-20220303135530009" style="zoom:50%;" /></p>

<h3 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h3>

<p>This method is based on TRPO however:</p>

<ul>
  <li>is a first-order method that avoids computing the Hessian matrix</li>
  <li>also avoids line search (at the end of previous algo) by <strong>clipping</strong> the surrogate objective.</li>
</ul>

<p>This results in the constrained optimization or surrogate objective:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135626037.png" alt="image-20220303135626037" style="zoom:50%;" /></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224444380.png" alt="image-20220304224444380" style="zoom:67%;" /></p>

<p>Then the algorithm looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224501324.png" alt="image-20220304224501324" style="zoom:67%;" /></p>

<h3 id="deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</h3>

<p>DDPG may be used in continuous action spaces and combines <strong>DQN</strong> with <strong>REINFORCE</strong>. Essentially the idea is again, the actor critic loop:</p>

<ul>
  <li>use DQN to estimate $Q_\phi(s,a)$ which will be a <strong>critic</strong></li>
  <li>use REINFORCE to estimate $\pi_\theta(s)$ which will be an <strong>actor</strong></li>
</ul>

<p>The loss and gradients are defined by:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Critic Loss</th>
      <th style="text-align: center">Actor Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224806600.png" alt="image-20220304224806600" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224852363.png" alt="image-20220304224852363" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>and</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224824803.png" alt="image-20220304224824803" style="zoom: 67%;" /></p>

<p>The algorithm being</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224921793.png" alt="image-20220304224921793" style="zoom:67%;" /></p>

<h2 id="model-based-reinforcement-learning">Model-based Reinforcement Learning</h2>

<p>Here, model-based reinforcement learning means we are <strong>learning the environment model</strong>, e.g. the transitions, to give us optimal policies. This can further be split into to kinds:</p>

<ul>
  <li>given model/environment: AlphaZero</li>
  <li>learn model by sampling: World Model</li>
</ul>

<h3 id="monte-carlo-tree-search">Monte-Carlo Tree Search</h3>

<blockquote>
  <p>A tree search starts at the root and explores nodes from there, <strong>looking for one particular node</strong> that satisfies the conditions mentioned in the problem.</p>
</blockquote>

<p>Tree search has been used in cases such as board games, where we essentially want to do the best next move when we have a selection of possible moves $A$ and a selection of states $S$ to be in:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303140820378.png" alt="image-20220303140820378" style="zoom:50%;" /></p>

<p>However, solving this exactly <strong>would need time complexity</strong> of:</p>

\[O\left((|S||A|)^d\right)\]

<p>which is <strong>exponential in time</strong>. Hence, the idea for MC Tree Search is to <strong>sample some branch only</strong> to keep the exploration going.</p>

<blockquote>
  <p>Monte-Carlo Tree Search (MCTS) runs simulations from a given state and therefore has time complexity of $O(nd)$ where $n$ is the number of simulations and $d$ the tree depth.</p>
</blockquote>

<p>Then, once done, we <strong>select actions based on Upper Confidence Bound</strong>:</p>

\[Q(s,a) + c \sqrt{\frac{\log N(s)}{N(s,a)}}\]

<p>for $c$ being an exploration constant and</p>

<ul>
  <li>$N(s,a)$ being the <strong>number of action state pairs</strong></li>
  <li>$N(s) = \sum_a N(s,a)$ is the number of <strong>state visits</strong></li>
</ul>

<p>So we get the algorithm being:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304230357312.png" alt="image-20220304230357312" style="zoom:67%;" /></p>

<h3 id="expert-iteration-and-alphazero">Expert Iteration and AlphaZero</h3>

<blockquote>
  <p>Expert iteration, or AlphaZero, uses</p>

  <ul>
    <li>a neural network to output a policy approximation $\pi_\theta(a\vert s)$ and state value function $V_\phi(s)$ approximation for guiding MCTS.</li>
    <li>use MCTS to output next best action</li>
  </ul>

  <p>Once dones, this is <strong>merged</strong> into a single network $f_\theta(s)$ that:</p>

  <ul>
    <li>receives a state representation as input $s$</li>
    <li>outputs a vector of probabilities <strong>$p_\theta = P(a\vert s)$ over all valid actions</strong> a and <strong>state values $V_\theta (s)$ over states $s$.</strong></li>
  </ul>

  <p>And AlphaZero learns these action probabilities and estimated values from <strong>games of self-play</strong></p>
</blockquote>

<p>The parameters $\theta$ are updated by stochastic gradient descent on the following loss function:</p>

\[L(\theta) = - \pi \log  p + (V- e)^2 + \alpha ||\theta||^2\]

<p>where:</p>

<ul>
  <li>the first term is a <strong>cross entropy loss</strong> between policy vector $p$ and search probabilities $\pi$ you have in reality</li>
  <li>the second term aims to <strong>minimize the difference</strong> between predicted performance $V$ and actual evaluation $e$</li>
  <li>the third is a <strong>regularization term</strong></li>
</ul>

<p>Then AlphaZero uses MCTS which is a stochastic search using upper confidence bound update rule of the action-value function</p>

\[U(s,a) = Q(s,a) + cP(a|s) \frac{\sqrt{N(s)}}{1+N(s,a)}\]

<p>where:</p>

<ul>
  <li>$N(s,a)$ is the number of times action $a$ was taken from state $s$</li>
  <li>$P(a\vert s)=p_\theta(a)$ is the NN output of probability taking action $a$ from state $s$</li>
</ul>

<p>Then at each step we take:</p>

<ol>
  <li>action $\arg\max_a U(s,a)$</li>
  <li>add this new state to a tree</li>
  <li>use MC Tree search on the new state and repeat</li>
</ol>

<p>Graphically we are doing</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303141302315.png" alt="image-20220303141302315" style="zoom: 67%;" /></p>

<h3 id="world-models">World Models</h3>

<blockquote>
  <p>A world model is a neural game simulator that uses a VAE and RNN to take action in an environment, which in turn <strong>can be used to model the world/game itself</strong>.</p>
</blockquote>

<p>The overall structure has been covered before</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303142142395.png" alt="image-20220303142142395" style="zoom:50%;" /></p>

<p>where we essentially learnt $M$.</p>

<ul>
  <li>the VAE is trained on images from the environment learning a <strong>low dimension latent representation</strong> $z$ of state $s$.</li>
  <li>
    <p>RNN is trained on the VAE latent vectors $z_t$ through time <strong>predicting</strong> $p(z_{t+1}\vert a_t, z_t, h_t)$.</p>
  </li>
  <li>$z_t$ and RNN hidden vector $h_t$ are fed into a <strong>neural network controller</strong> which <strong>outputs an action</strong> that effects the environment</li>
</ul>

<p>In addition, the power of this model is that:</p>

<ul>
  <li>essentially the VAE addition could be used to generate and continue playing the game, because we can feed in from RNN and VAE can <strong>“reconstruct” the image</strong></li>
  <li>some difference between GAN and this model is that here we are <strong>learning/reconstructing the latent space $z$</strong>, instead of the real data itself directly.</li>
</ul>

<h2 id="imitation-learning">Imitation Learning</h2>

<p>Rather than learning from rewards, imitation learning <strong>learns from example demonstrations</strong> provided by an expert - <mark>behavior cloning</mark>.</p>

<p>Therefore, we basically consider <strong>learning $\pi_\theta$</strong> that clones the <strong>expert demonstration $(s,a) \sim D$</strong>:</p>

\[\arg\max_\theta \sum_{(s,a) \in D} \log \pi_\theta(a|s)\]

<blockquote>
  <p>However, learning a policy like by imitation has a natural pitfall: if it <strong>encountered situations not well represented by the demonstrations</strong> it may perform poorly, and once encountered may not recover from cascading errors.</p>
</blockquote>

<p>Related models in this area include:</p>

<ul>
  <li>
    <p><strong>Dataset aggregation</strong> (DAgger) (Ross, Gordon &amp; Bagnell 2011) aims to <em>solve the problem of cascading errors by augmenting the data</em> with expert action labels of policy rollouts</p>
  </li>
  <li>
    <p><strong>Stochastic miximg iterative learning</strong> (SMILe) (Ross &amp; Bagnell 2010) trains a <em>new policy only on the augmented data</em> and then mixes the new policy with the previous policies</p>
  </li>
  <li>
    <p><strong>Generative adversarial imitation learning</strong> (GAIL) (Ho &amp; Ermon 2016) uses state and action examples $(s,a) \sim P_{real}$ from expert demonstrations as real samples for a discriminator in a GAN setting, and ask the generator to leanr $\pi_\theta(a\vert s)$:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308131653510.png" alt="image-20220308131653510" style="zoom: 67%;" /></p>

    <p>so that basically discriminators $D_\phi$ wants to distinguish the actions from expert demonstrations $(s,a) \sim P_{real}$ and the generated pair $(s,\pi_\theta(s))$.</p>
  </li>
  <li>
    <p><strong>Inverse reinforcement learning</strong> explicitly derives a <em>reward function from a set of expert demonstrations</em> and <em>uses that reward to learn an optimal policy</em></p>
  </li>
</ul>

<h2 id="maintaining-exploration">Maintaining Exploration</h2>

<p>As we know, if a model has $0$ exploration then it might <strong>not converge</strong> to the optimal solution as it will be stuck with greedy choices. Hence we have introduced $\epsilon$-greedy  approach that does random action with probability $\epsilon$.</p>

<p>Here, we introduce <strong>another way to promote exploration</strong>, which is to add some <strong>internal reward</strong> (i.e. not explicitly given from the environment) each time when a <strong>new state is explored</strong>.</p>

<ul>
  <li>this approach would also be useful for environments with <strong>sparse rewards</strong></li>
</ul>

<p>Related models include</p>

<ul>
  <li>
    <p><strong>Go-Explore</strong>: provide bonus rewards for novel states, you can encourage agents to explore more of the state space, even if they don’t receive any external reward from the environment.</p>

    <p>However,  one problem with these approaches is that they do a poor job at continuing to explore promising areas far away from the start state: <strong>Detachment</strong></p>

    <p><img src="https://www.alexirpan.com/public/go-explore/detachment.png" alt="Detachment diagram" style="zoom: 33%;" /></p>

    <p>And the proposed solution is to maintain a <strong>memory of previously visited novel states</strong>, so that:</p>

    <ol>
      <li>When learning, the agent first <em>randomly samples a previously visited state</em>, biased towards newer ones.</li>
      <li>It travels to that state, then <em>explores from that state</em>.</li>
    </ol>

    <p>Therefore, by chance we will <strong>eventually resample a state near the boundary of visited states</strong>, and from there it is easy to discover unvisited novel states</p>
  </li>
</ul>

<h2 id="multiagent-system">Multiagent System</h2>

<p>Here we consider essentially <strong>other agents/players</strong> in the system, which introduces the following changes:</p>

<ul>
  <li><strong>Environments</strong> that contain multiple agents
    <ul>
      <li>Agents act autonomously</li>
      <li>Outcome depends on actions of all agents</li>
      <li>Agents maximize their own reward</li>
    </ul>
  </li>
  <li><strong>Observation</strong>
    <ul>
      <li>Perfect information (i.e. your action decision can be made deterministically as you <em>already see/have everything you need</em>)
        <ul>
          <li>e.g. chess</li>
        </ul>
      </li>
      <li>Imperfect information (i.e. your action decision would be probabilistic because you <em>do not see/have opponent information yet</em>)
        <ul>
          <li>e.g. rock-paper-scissors</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Competitive - cooperative</strong>
    <ul>
      <li>Fully competitive zero-sum games: checkers, chess, Go</li>
      <li>Fully cooperative</li>
    </ul>
  </li>
</ul>

<h3 id="strategic-game">Strategic Game</h3>

<p>A matrix game (strategic game) can be described as follows</p>

<blockquote>
  <p>Formally, we have the following setup:</p>

  <ul>
    <li>$n$ agents, numbered from $i=1,…,n$</li>
    <li>at each step, we have some <strong>action profile $(a_1, …, a_n)$</strong> representing the action each agent $i$ took</li>
    <li>utility function for each agent $(u_1,…,u_n)$ measures the “utility” of their action $(a_1, …,a_n)$</li>
    <li>an outcome of the game is the joint action of all agents</li>
  </ul>

  <p>We <strong>assume</strong> that <strong>each agent maximizes its own utility over each outcome</strong></p>
</blockquote>

<p>Additionally, we can represent the games they play as a <strong>tree</strong>, for example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308201011752.png" alt="image-20220308201011752" style="zoom:50%;" /></p>

<p>in this example:</p>

<ul>
  <li><strong>nodes</strong> are states, edges are actions, leaves are utilities, e.g. $(u_1,u_2)$</li>
  <li>each node is <strong>labeled with an agent</strong> that can control it, so edge from a node is the action the agent at the node takes</li>
  <li>each path to a leaf is a <strong>run</strong></li>
</ul>

<hr />

<p><em>Perfect Information Game Example</em></p>

<p>in a <strong>perfect information game</strong>, we can explore the state by <strong>not caring how you get to the state</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308133714063.png" alt="image-20220308133714063" style="zoom:50%;" /></p>

<p>therefore, the value of a state is based on the <strong>subtree</strong> where the current node is the root.</p>

<hr />

<p><em>Imperfect Information Game</em></p>

<p>Here, we do not have everything you need to determine your action to win 100%. An example would be <strong>rock-paper-scissor</strong>, which we can characterize by:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Matrix</th>
      <th style="text-align: center">Tree</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308204135128.png" alt="image-20220308204135128" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308204119834.png" alt="image-20220308204119834" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the situation is that, if you are player B on the right figure, you do not know what A chose and hence all three states look the same to you</p>

<ul>
  <li>
    <p>if either player has some deterministic strategy, the player would be <strong>exploited</strong>. Hence the optimal solution, or its <mark>Nash Equilibrium</mark>, is to <strong>choose a random policy of $1/3$ for each action.</strong> This probabilistic policy is often referred to as <strong>mixed strategy</strong>, whereas the deterministic one is called <strong>pure strategy</strong>.</p>
  </li>
  <li>
    <p>This can also be seen as a <strong>zero-sum matrix game</strong>. When we say that it is a zero-sum game, we mean that one wins the same amount as the other loses.</p>
  </li>
</ul>

<h3 id="nash-equilibrium">Nash Equilibrium</h3>

<p>The idea of Nash Equilibrium in a game is a collection of <strong>strategies for all players</strong> such that <strong>no player can do better by changing its own strategy</strong> given that <strong>other players continue playing their NE strategies</strong></p>

<blockquote>
  <p>Formally, the Nash Equilibrium set of strategy for all $n$ players is $(\pi_1^<em>, …,\pi_n^</em>)$ such that</p>

\[V_i(\pi_1^*,...,\pi_i^*,...,\pi_n^*) \ge V_i(\pi_1^*,...,\pi_i,...,\pi_n^*) ,\quad i=1,...,n\]

  <p>where $V_i(\cdot)$ is player $i$’s value function which is player $i$’s <strong>expected reward/utility given all players’ strategies</strong></p>
</blockquote>

<p>It is important to know that:</p>

<ul>
  <li>
    <p>Clearly, the goal of converging to Nash equilibria is a good starting point for multi-agent learning algorithms. However, there commonly can be multiple Nash equilibria with different payouts, leading to greater difficulty in evaluating multiagent learning compared to single agent RL</p>
  </li>
  <li>
    <p>being in Nash Equilibrium therefore <strong>does not equal to maximal gain/utility</strong> attainable in a game, as it is <strong>assuming</strong> that all players want to maximize their <strong>own benefit</strong></p>

    <p>An example would be the following matrix game:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308205612393.png" alt="image-20220308205612393" style="zoom:67%;" /></p>

    <p>where the NE is for each player to <em>take</em>, as it maximizes their own benefit. However, the optimal solution is of course for both to <em>give</em>.</p>
  </li>
</ul>

<blockquote>
  <p>In fact:</p>

  <ul>
    <li>in <strong>Perfect information game</strong>, a <strong>unique Nash equilibrium/optimal</strong> is the value for player of current node in tree (as the value would be representative of all possible outcomes)</li>
    <li>in <strong>Imperfect information game</strong>, there may be <strong>multiple Nash equilibria</strong>, no deterministic strategy may be optimal.</li>
  </ul>
</blockquote>

<p><em>For Example</em>: Simple Nash Equilibrium</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308211944291.png" alt="image-20220308211944291" style="zoom:67%;" /></p>

<p>since the entire action space is shown, we see that:</p>

<ul>
  <li>“Top + Left” is a <strong>Nash Equilibrium</strong> because either can benefit themselves more if the other player stayed there. A can switch to Bottom, but this would reduce his payoff from $5$ to $1$. Player B can switch from left to right, but this would reduce his payoff from $4$ to $3$. “Bottom + Right” is <strong>also a Nash Equilibrium</strong> for the same reason</li>
  <li>other cells are not because one player will be gained more by shifting away.</li>
</ul>

<hr />

<p><em>For Example</em>: Mixed Nash Equilibrium Strategy</p>

<p>To produce mixed strategy, we <strong>must</strong> have at least one player going random policy, as otherwise the optimal solutoin will be determinstic as you can just exploit it.</p>

<p>Consider the example of <em>battle of sexes</em>, where the reward is shown in the table:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" /></p>

<p>We now <strong>suppose that</strong> woman go to the Baseball game with <mark>probability $p$,</mark> and the Man go to the Baseball game with probability $q$. Then, we can look at the <strong>expected return</strong> for each party after choosing an action:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213426800.png" alt="image-20220308213426800" /></p>

<p>hence, notice that:</p>

<ul>
  <li>
    <p>A mixed strategy in the battle of the sexes game requires both parties to randomize</p>
  </li>
  <li>
    <p>for <strong>Man to randomize=indifference between going Baseball or Ballet</strong>, we need</p>

\[1+2p = 2-2p \quad \to \quad p=1/4\]

    <p>for woman</p>
  </li>
  <li>
    <p>for the <strong>Woman to randomize</strong>, the Woman must get equal payoffs from going to the Baseball game and going to the Ballet, which requires:</p>

\[2q = 3-2q \quad \to \quad q=3/4\]
  </li>
  <li>
    <p>notice that the <strong>above are independent decisions</strong>, hence we can pick $q=3/4$ <mark>and</mark> $p=1/4$ to achieve <mark>Nash Equilibrium</mark> so that the probability of man/woman going to each sport being given by:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" /></p>

    <p>which is the result if man and woman following the NE.</p>
  </li>
</ul>

<p>Then, putting together the mixed strategy and the reward, we can compute the <strong>expected payoff</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Reward</th>
      <th style="text-align: center">Random Policy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>basically we do element-wise multiplication for the two matrix and will get</p>

\[\mathbb{E}[\text{man reward}] = \frac{9}{16} + \frac{9}{16} + \frac{6}{16} + 0 = \frac{3}{2}\\
\mathbb{E}[\text{woman reward}] = \frac{6}{16} + \frac{9}{16} + \frac{9}{16} + 0 = \frac{3}{2}\\\]
  </li>
  <li>
    <p>the above Nash equilibrium (following $p,q$) is indeed <strong>smaller than if they had coordination</strong> to go both baseball or both ballet.</p>
  </li>
</ul>

<blockquote>
  <p>Here we have shown that <strong>randomization</strong> requires <strong>equality of expected payoffs</strong> (as otherwise it becomes deterministic), which would then result in Nash Equilibrium.</p>
</blockquote>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the game of guessing coin flip:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308211127704.png" alt="image-20220308211127704" style="zoom: 50%;" /></p>

<p>essentially, if $P1$’s coin is guessed correctly by $P2$ then $P1$ receives a penalty. We want to **consider the Nash Equilibrium **</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Player 2 Guessing Head with $p=3/4$  and tail $p=1/4$</th>
      <th style="text-align: center">Player 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308232956782.png" alt="image-20220308232956782" /></td>
      <td style="text-align: center">Player 1 can always achieve 0 by always playing sell</td>
    </tr>
  </tbody>
</table>

<p>where the key observation for imperfect game is that:</p>

<ul>
  <li>for imperfect information game, you need to consider <strong>the full tree</strong> for possibility to determine your <strong>optimal strategy</strong></li>
</ul>

<h3 id="counterfactual-regret-minimization">Counterfactual Regret Minimization</h3>

<p>Key algorithm used in imperfect information games</p>

<ul>
  <li><strong>Counterfactual</strong>: If I had known</li>
  <li><strong>Regret</strong>: how much better would I have done if I did something else instead?</li>
  <li><strong>Minimization</strong>: what strategy minimizes my overall regret?</li>
</ul>

<p>Then a Monte Carlo CFR does:</p>

<ul>
  <li>Player 1 explores all options and player 2 samples actions</li>
  <li>Traverse entire tree</li>
  <li>Repeat switching roles: player 1 samples actions and player 2 explores all actions</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308141226579.png" alt="image-20220308141226579" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>we know that player 1 played green, which gained 50. But the other options are to get 0 or 100. Hence the regret for them is -50 and 50.</li>
  <li>use neural network to approximate the tree, which <strong>guides the CFR search</strong></li>
</ul>

<h3 id="unification">Unification</h3>

<p>One idea is that perfect information can be seen as a special case of imperfect information:</p>

<ul>
  <li>realize that both basically includes a <strong>Tree Search + DNN</strong> to simulate the tree
    <ul>
      <li>perfect information does DNN + MC tree search</li>
      <li>imperfect information does DNN + MC-CFR</li>
    </ul>
  </li>
  <li>now a unified algorithm has come up which essentially can play both Chess, Go as well as games like Poker.</li>
</ul>

<h1 id="optional">Optional</h1>

<p>Some random stuff added in lecture notes</p>

<h2 id="adversarial-attacks">Adversarial Attacks</h2>

<p>The idea is that we <strong>add some noise</strong> in an image, such that the <strong>classification output</strong> would be <mark>different</mark></p>

<ul>
  <li>targeted</li>
  <li>untargeted</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135858473.png" alt="image-20220217135858473" style="zoom:33%;" /></p>

<p>The idea is therefore to do</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217140014237.png" alt="image-20220217140014237" style="zoom:33%;" /></p>

<p>then</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135942981.png" alt="image-20220217135942981" style="zoom:33%;" /></p>

<p>which obviously requires you to know the network architecture in advance</p>

<h2 id="problem-solving-with-dl">Problem Solving with DL</h2>

<p>https://www.cs.columbia.edu/~idrori/drori2021math.pdf</p>

<blockquote>
  <p>We demonstrate that a neural network pre-trained on text <strong>and</strong> finetuned on code solves Mathematics problems by program synthesis.</p>

  <ul>
    <li>In this work we demonstrate that program synthesis is the key to solving math and STEM courses at scale, by turning questions into programming tasks</li>
  </ul>
</blockquote>

<p>Some of the previous interesting work</p>

<ul>
  <li>
    <p>When <strong>paired with graph neural networks (GNNs)</strong> to predict arithmetic expression trees, Transformers pre-trained on text have been used to solve university level problems in Machine Learning (14) with up to 95% accuracy.</p>

    <p>however this previous work is limited to numeric answers and isolated to a specific course and does not readily scale to other courses</p>
  </li>
  <li>
    <p>it works because <strong>humans solving problems by imaging them into computation trees</strong></p>
  </li>
</ul>

<hr />

<p>Here, the idea is to:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220310205626517.png" alt="image-20220310205626517" /></p>

<ol>
  <li>turn question into programming task
    <ul>
      <li>add context (e.g. what course it is, e.g. use <code class="language-plaintext highlighter-rouge">numpy</code>)
        <ul>
          <li>assume that background knowledges from the available code repos on Github. e.g. if we are to solve a physics problem, it will assume physics knowledge from existing repos.</li>
        </ul>
      </li>
      <li>using codex to prompt codex (e.g. given a question, output a programming task)</li>
    </ul>
  </li>
  <li>automatically genreate program using a Transformer, OpenAI Codex, pre-trained on text and fine-tuned on code</li>
  <li>execute program to obtain and evluate answers</li>
  <li>automatically explain correct solution using codex</li>
</ol>

<blockquote>
  <p>The key takeaways is that:</p>

  <ul>
    <li>transformers pretrained on text does <strong>not directly work</strong> on solving math problems</li>
    <li>program synthesis using codex can assume “knowledge” of physics, math, etc. if there exists related code on Github</li>
  </ul>
</blockquote>

<p>But exactly what context is needed?</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220310210250349.png" alt="image-20220310210250349" /></p>

<p>In short we have mainly three kinds:</p>

<ul>
  <li><strong>Topic Context</strong>:  For example, without context, a question about networks may be about neural networks or communication networks, and therefore specifying the context is required.</li>
  <li><strong>Library Context</strong>: using the Python libraries sympy and streamplot are added for solving the question and plotting a visualization</li>
  <li><strong>Definitions Context</strong>: Often times, Codex doesn’t have real-world grounding for what certain terms are defined as. As an illustrative example, Codex lacks knowledge of what a “Full House” means in poker. Explicitly defining these in terms that Codex understands can better guide its program synthesis.</li>
</ul>

<p>Then, when transforming questions to Codex prompts, a key consideration is <strong>how semantically close the original question is to a prompt that produces a correct solution</strong>:</p>

<ul>
  <li>To measure the distance between the original questions and the successful prompts, we use cosine similarity between Sentence-BERT</li>
  <li>we can also run <strong>clusterings</strong> from thos embeddings to see how courses are interrelated.</li>
</ul>

<p>Finally, we can <strong>create new questions</strong> using Codex:</p>

<ul>
  <li>We use Codex to generate new questions for each course. This is done by creating a numbered list of questions from the dataset. This list is cut off after a random number of questions, and the result is used to <strong>prompt Codex to generate the next question</strong>.</li>
</ul>

<p><strong>More details on using Codex</strong></p>

<ul>
  <li>We use OpenAI’s davinci-codex engine for all of our generations. We fix all of Codex’s hyperparameters to be the same for all experiments: top-$p$ which is the portion p of the token probability mass a language model samples from at each step is set to 1, sampling temperature is set to 0 (i.e. argmax), and response length is set to 200 tokens.</li>
  <li>Both frequency and presence penalty are set to 0, and we do not halt on any stop sequences. Each prompt is structured as a Python documentation comment surrounded by triple quotations and line breaks.</li>
</ul>

<h1 id="tutorials">Tutorials</h1>

<p>Tutorials after classes</p>

<h2 id="remote-jupyter">Remote Jupyter</h2>

<p>Here are some notes on how to setup a remote jupyter server and connect to it locally using VSCode.</p>

<p>Firstly, on the <strong>remote</strong> (e.g. <code class="language-plaintext highlighter-rouge">ssh</code> into it)</p>

<ol>
  <li>
    <p>make sure you have already generated a <strong>settings file</strong> for you jupyter server. This should be located under <code class="language-plaintext highlighter-rouge">~/.jupyter/jupyter_notebook_config.py</code>. If not, you can generate it by:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook <span class="nt">--generate-config</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Make sure that you have the following two lines in your config file</p>

    <div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span>.<span class="n">NotebookApp</span>.<span class="n">allow_origin</span> = <span class="s1">'*'</span> <span class="c">#allow all origins
</span><span class="n">c</span>.<span class="n">NotebookApp</span>.<span class="n">ip</span> = <span class="s1">'0.0.0.0'</span> <span class="c"># listen on all IPs
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>Start your jupyter server by doing (consider putting it into <code class="language-plaintext highlighter-rouge">tmux</code>):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>jupyter notebook <span class="nt">--no-browser</span> <span class="nt">--port</span><span class="o">=</span>9999
<span class="c"># some logs omitted</span>
<span class="o">[</span>I 13:00:56.602 NotebookApp] Jupyter Notebook 6.4.5 is running at:
<span class="o">[</span>I 13:00:56.602 NotebookApp] http://ip_of_your_server:9999/?token<span class="o">=</span>xxx
</code></pre></div>    </div>

    <p>take a note of the line <code class="language-plaintext highlighter-rouge">http://ip_of_your_server:9999/?token=xxx</code>, which will be used later</p>
  </li>
</ol>

<p>Then, on your <strong>local machine</strong>, open up VSCode and:</p>

<ol>
  <li>
    <p>connect to the remote jupyter server by putting in the url <code class="language-plaintext highlighter-rouge">http://ip_of_your_server:9999/?token=xxx</code> into the following popup</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223180806747.png" alt="image-20220223180806747" /></p>
  </li>
  <li>
    <p>Reload the window, and if successful, you will be able to see a <strong>remote kernel</strong> at the following pop up</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223180957821.png" alt="image-20220223180957821" /></p>
  </li>
</ol>

<h2 id="pytorch">PyTorch</h2>

<p>In total there will be three parts of the tutorial.</p>

<ol>
  <li>Part I: introduction, compute derivatives, build a simple NN</li>
</ol>

<p>What is PyTorch? It’s a Python-based scientific computing package targeted at two sets of audiences:</p>

<ul>
  <li>A replacement for NumPy to use the power of GPUs</li>
  <li>a deep learning research platform that provides maximum flexibility and speed</li>
</ul>

<p>See the <code class="language-plaintext highlighter-rouge">tutorial/pytorch</code> for more details</p>

<h3 id="part-i">Part I</h3>

<p>Basically data structures are very similar to the <code class="language-plaintext highlighter-rouge">np.array</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span> <span class="c1"># again same as numpy
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="s">"""
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
"""</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>      <span class="c1"># new_* methods take in sizes
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="s">"""
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
"""</span>
<span class="c1"># Generates random values from **Normal distribution**
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>    <span class="c1"># override dtype!
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        <span class="c1"># result has the same size as x
</span><span class="s">"""
tensor([[-0.0351,  1.3674,  2.3571],
        [ 0.1267,  1.2764, -1.0151],
        [ 1.8572, -3.1128,  0.5551],
        [-0.6236,  0.6248,  0.1378],
        [-0.8379, -0.2349, -1.0931]])
"""</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># instead of x.shape
</span></code></pre></div></div>

<p>note that:</p>

<ul>
  <li>So, essentially, <code class="language-plaintext highlighter-rouge">new_ones</code> allows you to quickly create a <mark>new</mark> <code class="language-plaintext highlighter-rouge">torch.Tensor</code> on the same device and data type <mark>from a *previously existing* tensor</mark> (with ones), whereas <code class="language-plaintext highlighter-rouge">ones()</code> serves the purpose of creating a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> <mark>from scratch</mark> (filled with ones).</li>
</ul>

<p>Some additional feature for arithmetic</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># normal way
</span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="n">q</span><span class="p">)</span>

<span class="c1"># method 2
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> <span class="c1"># as a paramter
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># method 3
</span><span class="n">q</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="c1"># in-place, noitce the UNDERSCORE
</span><span class="k">print</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</code></pre></div></div>

<p>Common Operations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># slicing is the same as numpy
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># all rows, 1st column only
</span><span class="s">"""
tensor([[1.2144, 0.3950, 0.9391],
        [1.0756, 1.0725, 1.1850],
        [1.2121, 1.2862, 0.6270],
        [0.9586, 0.6927, 0.9081],
        [0.8100, 0.7620, 1.6299]])
tensor([0.3950, 1.0725, 1.2862, 0.6927, 0.7620])
"""</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="c1"># instead of reshape
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">z</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
<span class="s">"""
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
"""</span>

<span class="c1"># permute!
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">220</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 1th = 220, 2nd = 224, 0th = 3 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<p>Conversion to list</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span> <span class="c1"># to a list
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">item</span><span class="p">())</span> <span class="c1"># gets the number, works for tensors with single value
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span> <span class="c1">#works for tensors with multiple values
# print(x.item()) # does not work with multiple dimension
</span></code></pre></div></div>

<h4 id="pytorch-and-numpy">PyTorch and Numpy</h4>

<p>PyTorch integrates very nice features with <code class="language-plaintext highlighter-rouge">numpy</code>. <mark>The Torch Tensor and NumPy array will share their underlying memory locations</mark>, and changing one will change the other.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 1-d arrow
</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># converts to numpy
</span>
<span class="n">a</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># has to be inplace
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="s">""" both are changed
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
"""</span>
</code></pre></div></div>

<p>this works because they will be pointing to the <strong>same memory location</strong> (probably share a field).</p>

<p>The reverse <mark>also works</mark></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">a</span><span class="p">)</span> <span class="c1"># in-place
# below does not work because it returns a NEW ARRAY
# a = np.add(a,1) 
# a = a + 1
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="s">"""
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
"""</span>
</code></pre></div></div>

<h4 id="cuda-tensors">CUDA Tensors</h4>

<p>This basically will make your code runs faster</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
# Devices available are 'cuda' and 'cpu'
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>          <span class="c1"># a CUDA device object
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># directly create a tensor on GPU
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># or just use strings ``.to("cuda")``
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="part-2">Part 2</h3>

<p>Too tired, look at the notebook.</p>

<p>Basically you have <code class="language-plaintext highlighter-rouge">pytorch</code> <strong>automatically evaluated gradient</strong> by:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># gradient evaluated AT this value
# x.add_(1) # once requires_grad, you cannot do in-place operation
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span> <span class="c1"># only x will have `require_grad=True`
# y.retain_grad() # if you need the gradient of y as well
</span><span class="n">z</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="s">"""
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)
tensor(27., grad_fn=&lt;MeanBackward0&gt;)
"""</span>
</code></pre></div></div>

<p>Then, to evaluate gradient $\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1}$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Let's backprop now
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># not yet computed
</span><span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># computed
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="s">"""
None

tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
"""</span>
</code></pre></div></div>

<p>We have that $o = \frac{1}{4}\sum_i z_i= \frac{1}{4}\sum_i 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$. Therefore:</p>

\[\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)\]

<p>so evaluated at $x_i=1$:</p>

\[\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5\]

<p>which is what we had.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Under the hood, the <code class="language-plaintext highlighter-rouge">grad</code> is done by keeping track of a computation graph behind all the operations.</p>
</blockquote>

<p>Then, to temporarily disable the gradient:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="s">"""
True
True
False
"""</span>
</code></pre></div></div>

<p>useful when the training is done.</p>

<h3 id="part-3">Part 3</h3>

<p>Now we create a NN for MNIST dataset.</p>

<p>First import a bunch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Import dataset related API
</span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="c1"># Import common neural network API in pytorch
</span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># Import optimizer related API
</span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Check device, using gpu 0 if gpu exist else using cpu
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>We will be mainly dealing with MNIST:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Getting data
</span><span class="n">training_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># how dset works
</span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">cols</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)).</span><span class="n">item</span><span class="p">()</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span>
    <span class="n">figure</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span> <span class="c1">#.squeeze() to convert shape from (1, 28, 28) to (28, 28)
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>note that:</p>

<ul>
  <li>
    <p>the shape is <code class="language-plaintext highlighter-rouge">[28,28,1]</code> because <code class="language-plaintext highlighter-rouge">pytorch </code>set the first dimension to the <strong>channel dimension.</strong> We are using grey scale so it is $1$.</p>
  </li>
  <li>
    <p>squeeze can be quite dangerous if you do not know what you are doing:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div>    </div>

    <p>i.e. all dimension 1 will be removed if not specified</p>
  </li>
</ul>

<h4 id="preparing-data">Preparing Data</h4>

<p><strong>Preparing the data</strong></p>

<ul>
  <li>into batches and shuffle</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Important Library!!
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Sectioning dataset into batches using dataloaders
</span><span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="create-nn">Create NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining the neural network
</span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># define layers
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_fc_layers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_fc_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="c1"># W^1, notice that 784=28*28
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="c1"># W^2, input 128 dimension, output 64 dimension
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="c1"># W^3, input 64 dimension, output 10
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">layers</span>


    <span class="c1"># define forward function
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span> <span class="c1"># in this case, it will be (64, 784) where 64 is the batch_size you had
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># backward pass does not need to be specified because they are done automatically
</span>        

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span> <span class="c1"># Creating an instance of the Network class
</span><span class="n">net</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shifting to cuda if available*
</span>
<span class="c1"># Below is how the network looks
</span><span class="s">"""
Network(
  (layers): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=10, bias=True)
    (5): LogSoftmax(dim=1)
  )
)
"""</span>
<span class="c1"># Defining our Loss Function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Defining optimizer
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1"># e.g. ADAM
</span></code></pre></div></div>

<h4 id="training-nn">Training NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Number of epochs
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># Initialising statistics that we will be tracking across epochs
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="c1"># in batches
</span>        <span class="c1"># get the inputs; data is a list of [inputs, labels]
</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># loading onto cuda if available*
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># zero the parameter gradients: Clean the gradient caclulated in the previous iteration
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1">#reset all graidents to zero for each step as they accumulate over backprop
</span>
        <span class="c1"># forward + backward + optimize
</span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># reshapes [btach_size, 28, 28] to [batch_size, 784]
</span>
        <span class="c1"># 1. forward propagation
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># 2. backward propagation
</span>        <span class="c1"># Calculate gradient of matrix with requires_grad = True
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1">#computes dloss/dx for every parameter x which has requires_grad=True
</span>
        <span class="c1"># 3. Apply the gradient calculate from last step to the matrix
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># x += -lr * x.grad ie updates the weights of the parameters
</span>
        <span class="c1"># housekeeping
</span>        
        <span class="c1"># Adding loss to total loss
</span>        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Checking which output label has max probability
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Tracking number of correct predictions
</span>        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Calculating accuracy, epoch-time
</span>    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span> <span class="n">total_correct</span><span class="o">/</span><span class="n">total</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>    

    <span class="c1"># Printing out statistics
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Epoch no."</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span> <span class="p">,</span><span class="s">"|accuracy: "</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span><span class="s">"%"</span><span class="p">,</span> <span class="s">"|total_loss: "</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="s">"| epoch_duration: "</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">end_time</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s">"sec"</span><span class="p">)</span>
</code></pre></div></div>

<p>you basically have to specify the entire training steps.</p>

<h4 id="evaluating-nn">Evaluating NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">correct</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Since, we're now evaluating, we don't have to track gradients now!
</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>

    <span class="c1"># everything here is similar to the train function, 
</span>    <span class="c1"># except we're not calculating loss and not preforming backprop
</span>    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># just forward propagatoin for computing Y_pred
</span>    <span class="c1"># no more backward updates
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">}</span><span class="s">%'</span><span class="p">)</span>    
</code></pre></div></div>

<p>notice that we asked <code class="language-plaintext highlighter-rouge">pytorch</code> not to track gradients any more.</p>

<h4 id="create-own-dset">Create Own Dset</h4>

<p>We can subclass <code class="language-plaintext highlighter-rouge">Dataset</code>. This gives us more control how the data is to be extracted from source (<strong>before training</strong>)</p>

<ul>
  <li>We can also perform some additional transformatios before returning the data</li>
  <li>will work natively with <code class="language-plaintext highlighter-rouge">DataLoader</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#CustomDataset is subclass of Dataset.
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="c1"># how many data points are there in your dset
</span>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="c1"># get one (x_train, y_train) sample
</span>        <span class="c1"># we can do transformation on the given input before it is returned
</span>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
    
<span class="c1">#Usage
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="tensorflow">Tensorflow</h2>

<p>Implemented in C++ in the back, so highly optimized in performance.</p>

<p>The basic components are <strong>tensors</strong>, which is a multi-dimensional arrays with a <mark>uniform type</mark>. Basicallya generalized matrix.</p>

<ul>
  <li>
    <p>we also talk about ranks in a tensor: a rank is the same idea as “dimension”</p>

    <ul>
      <li>
        <p>rank 1 tensor for vector</p>
      </li>
      <li>
        <p>rank 2 for matrix</p>
      </li>
      <li>
        <p>rank 3 for rgb image</p>
      </li>
    </ul>
  </li>
  <li>
    <p>deep learning basically does <strong>computations using tensors</strong> that are very large.</p>

    <ul>
      <li>e.g. for NN, we can see the system as a graph and passing tensors across. Hence the name <strong>TensorFlow</strong></li>
    </ul>
  </li>
</ul>

<h3 id="basics">Basics</h3>

<p>In general, you should specify:</p>

<ul>
  <li>the size of the tensor</li>
  <li>the datatype</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">dtypes</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
    <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">dtypes</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="s">"""
(3, 4, 3)
&lt;dtype: 'float32'&gt;
"""</span>
</code></pre></div></div>

<p><strong>Integrated with <code class="language-plaintext highlighter-rouge">numpy</code></strong></p>

<ul>
  <li>yet there is no “in-place operation” in tensorflow.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">tf_tensor</span>
<span class="s">"""
&lt;tf.Tensor: shape=(3, 3), dtype=float64, numpy=
array([[42., 42., 42.],
       [42., 42., 42.],
       [42., 42., 42.]])&gt;
"""</span>

<span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf_tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="s">"""
array([[43., 43., 43.],
       [43., 43., 43.],
       [43., 43., 43.]])
"""</span>
</code></pre></div></div>

<p><strong>GPU Support</strong></p>

<ul>
  <li>TensorFlow is able to detect it automatically</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"GPU available and ready to use!"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"Unable to detect GPU"</span><span class="p">)</span>

        <span class="c1"># to run on some specific GPU
</span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"GPU"</span><span class="p">):</span> <span class="c1"># Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
        <span class="c1"># x.device specifies the device for operations on x 
</span>        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">"GPU:0"</span><span class="p">)</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">"On GPU"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'GPU unavailable'</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="variables-and-constants">Variables and Constants</h4>

<p>Basically they are wrappers around Tensor object, but it is <strong>important because</strong></p>

<ul>
  <li>it allows <mark>in-place updates</mark></li>
  <li>they are automatically watched by <code class="language-plaintext highlighter-rouge">tf.GradientTape</code>, which does auto-differentiating
    <ul>
      <li>hence they are used for parameters such as <strong>weights in NN</strong></li>
    </ul>
  </li>
</ul>

<p>Then, to create variables and constants</p>

<ul>
  <li>variables are <strong>tensors</strong> that can be changed. They will be watched if you use <code class="language-plaintext highlighter-rouge">GradientTape</code></li>
  <li>constants are <strong>tensors</strong> that <em>cannot be updated</em>. They will <em>not be</em> watched automatically.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for creating a Variable, you need to provide an initial value
</span><span class="n">my_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]])</span>
<span class="n">my_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">)</span> <span class="c1"># so a wrapper
</span>
<span class="c1"># Variables can be all kinds of types, just like tensors
</span><span class="n">bool_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span>
<span class="n">complex_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">5</span> <span class="o">+</span> <span class="mf">4j</span><span class="p">,</span> <span class="mi">6</span> <span class="o">+</span> <span class="mf">1j</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>In-place Updates</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="c1"># This will keep the same dtype, float32
</span><span class="n">a</span><span class="p">.</span><span class="n">assign</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"a:"</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="c1"># Not allowed as it resizes the variable:
</span><span class="n">a</span><span class="p">.</span><span class="n">assign</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Naming</strong></p>

<ul>
  <li>
    <p>helpful for you to know <strong>what this tensor is for</strong>, .e.g when reloading some model weights</p>
  </li>
  <li>
    <p>Variable names are preserved when saving and loading models.</p>

    <p>By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a and b; they will have the same name but will be backed by
# different tensors.
</span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"Tutorial"</span><span class="p">)</span>

<span class="c1"># A new variable with the same name, but different value
# Note that the scalar add is broadcast
</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"Tutorial"</span><span class="p">)</span>

<span class="c1"># These are elementwise-unequal, despite having the same name
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="gradients">Gradients</h4>

<p>Basically the same as PyTorch, so that you need to do something “special” to enable it to <strong>start memorizing the operations</strong></p>

<p>Consider the function we want to differentiate:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">z</span>
  
<span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">z</span><span class="o">=-</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># -12
</span></code></pre></div></div>

<p>then, suppose we want to compute:</p>

\[\left. \nabla f  \right|_{1,2,-4}\]

<p>Then:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># input needs to be tracked, hence using variables
</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">)]</span> 

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> <span class="c1"># start tracking, i.e. a blank sheet of paper where each operatoin is recorded
</span>    <span class="n">r</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
  
<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="c1"># W.R.T. the params
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Gradients:  drdx: %0.2f, drdy: %0.2f, drdz: %0.2f"</span> <span class="o">%</span> 
      <span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grads</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">numpy</span><span class="p">()))</span>
<span class="s">"""
Gradients:  drdx: -4.00, drdy: -4.00, drdz: 3.00
"""</span>
</code></pre></div></div>

<p>notice that:</p>

<ul>
  <li>we used ` tape.gradient(r, params)<code class="language-plaintext highlighter-rouge"> to differentiate w.r.t. all the </code>params<code class="language-plaintext highlighter-rouge"> hence achieving the gradient. You can also compute $\partial f / \partial x$ by just passing in </code>tape.gradient(r, params[0])`</li>
  <li>you also <strong>cannot call it multiple times</strong></li>
</ul>

<p>Another example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>            <span class="c1"># track the weight      
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>            <span class="c1"># does not need if we do not diff w.r.t x
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">forward</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">x</span><span class="p">).</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># w.r.t the weight only
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Gradient with respect to w:  "</span><span class="p">,</span> <span class="n">grads</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="s">"""
Gradient with respect to w:   [[0.  0. ]
 [0.2 0.4]]
"""</span>
</code></pre></div></div>

<ul>
  <li>
    <p>note that if you have a <strong>constant Tensor</strong> that you want to watch:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
    <span class="n">g</span><span class="p">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># NEEDED!
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">dy_dx</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># Will compute to 6.0
</span><span class="k">print</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="training">Training</h3>

<p>The complete version.</p>

<p><strong>preparing data</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span> 
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="n">mpimg</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">timeit</span>


<span class="c1"># Download a dataset
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="s">"""
(60000, 28, 28)
(10000, 28, 28)
"""</span>

<span class="c1"># normalize the values to [0-1], hsuffle, and generates to batches
# returns an iterator
</span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)).</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1024</span><span class="p">).</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="n">test_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)).</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># A quick example of iterating over a dataset object
</span><span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Batch Shape: "</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1">#  (32, 28, 28) where 32 is the batch size
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Training</strong></p>

<p>Now, in general there are two ways to establish and train a model</p>

<ul>
  <li>using <code class="language-plaintext highlighter-rouge">Sequential</code> - easier to use</li>
  <li>using <code class="language-plaintext highlighter-rouge">subclassing</code> API - more flexible</li>
</ul>

<p>Here we discuss the subclassing idea:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyCustomModel</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span> <span class="c1"># inherits from tf.keras.Model
</span>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyCustomModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># define your network architecture
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()</span> <span class="c1"># Layer 1: flatten
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">dl_1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span> <span class="c1"># Layer 2: dense layer
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">dl_2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span> <span class="c1"># Layer 3: dense layer
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">d1_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span> <span class="c1"># Layer 4: multiclass classification
</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># basically the FORWARD PASS
</span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">d1_3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyCustomModel</span><span class="p">()</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>  <span class="c1"># loss function
# what optimization method we use. i.e. HOW you want to update the weights
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">()</span>

<span class="c1"># For each epoch
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

    <span class="c1"># For each batch of images and labels
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>

        <span class="c1"># Open a GradientTape.
</span>        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

            <span class="c1"># Forward pass
</span>            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="c1"># Calculate loss, done INSIDE the GradientTape since we want to do derivatives afterwards
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

        <span class="c1"># Backprop to calculate gradients
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>

        <span class="c1"># Gradient descent step, apply the `gradients` to `model.trainable_variables`
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="c1"># Calculate loss on the test data
</span>    <span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_ds</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss_on_batch</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_on_batch</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Epoch {}, Test loss: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)))</span>
    
<span class="s">"""
Epoch 1, Test loss: 0.320291668176651
Epoch 2, Test loss: 0.26357364654541016
Epoch 3, Test loss: 0.2302776277065277
Epoch 4, Test loss: 0.20258881151676178
Epoch 5, Test loss: 0.17977921664714813
"""</span>
</code></pre></div></div>

<h3 id="note">Note</h3>

<p>TensorFlow 2.x</p>

<ul>
  <li>uses <strong>eager execution by default</strong>: executes line by line, the same fashion as you wrote the code</li>
</ul>

<p>TensorFlow 1.x</p>

<ul>
  <li>
    <p>builds a graph from your code, and execute according to the graph</p>
  </li>
  <li>
    <p>Graph execution has the obvious advantage that, since you can track the dependency of operations, you can <strong>optimize the run by parallelization</strong></p>
    <ul>
      <li>this means Graph based execution can be faster when you do model training</li>
      <li>however, since the order of computation maybe different from your code, it would be <mark>hard to debug</mark></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model building
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">outputs</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s">"softmax"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 

<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>

<span class="c1"># Eager Execution, by default
</span><span class="n">eager_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Eager time:"</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">eager_model</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="mi">10000</span><span class="p">))</span>

<span class="c1">#Graph Execution 
</span><span class="n">graph_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">eager_model</span><span class="p">)</span> <span class="c1"># Wrap the model with tf.function 
</span><span class="k">print</span><span class="p">(</span><span class="s">"Graph time:"</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">graph_model</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="mi">10000</span><span class="p">))</span>
<span class="s">"""
Eager time: 21.913783847000104
Graph time: 6.057469765000064
"""</span>
</code></pre></div></div>

<h2 id="keras">Keras</h2>

<p>Keras has faster performance</p>

<ul>
  <li>backed by TensorFlow, so <code class="language-plaintext highlighter-rouge">Keras </code>is a <strong>high level API</strong></li>
  <li>easier to use than raw TensorFlow</li>
</ul>

<p>Since <code class="language-plaintext highlighter-rouge">keras</code> is like a subclass of <code class="language-plaintext highlighter-rouge">tensorflow</code>, there is no “basics” per se.</p>

<h3 id="sequential-model">Sequential Model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span> <span class="c1"># Dense layer is a normal NN layer
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>note that we imported <code class="language-plaintext highlighter-rouge">keras</code> from <code class="language-plaintext highlighter-rouge">tensorflow</code></p>

<p>We will use <code class="language-plaintext highlighter-rouge">Sequential</code>:</p>

<ul>
  <li>architecture is basically like a list</li>
  <li>output of previous will be input of next</li>
</ul>

<p>A Sequential model is <strong>not</strong> appropriate when:</p>

<ul>
  <li>Your model has multiple inputs or multiple outputs</li>
  <li>Any of your layers has multiple inputs or multiple outputs</li>
  <li>You need to do layer sharing</li>
  <li>You want non-linear topology (e.g. a residual connection, a multi-branch model)</li>
</ul>

<p>For dealing with these cases, Keras offers a Functional API as well. This handles non-linear topologies, shared layers and even multiple inputs or outputs.</p>

<ul>
  <li>Read more here: https://keras.io/guides/functional_api/</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">clear_session</span><span class="p">()</span>
</code></pre></div></div>

<blockquote>
  <p>More information on <code class="language-plaintext highlighter-rouge">Keras </code>layers: https://keras.io/api/layers/</p>

  <p>Look into <code class="language-plaintext highlighter-rouge">Keras </code>models: https://keras.io/api/models/</p>
</blockquote>

<p>Now let us <strong>prepare the data</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="s">"""
(60000, 28, 28)
(10000, 28, 28)
"""</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># "normalizing it"
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</code></pre></div></div>

<p>The four <mark>most important steps you will use</mark></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> <span class="c1"># define model
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)))</span> <span class="c1"># define architecture
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(...)</span> <span class="c1"># define optimizer
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(...)</span> <span class="c1"># fit
</span></code></pre></div></div>

<p>Let us build a 3 layer model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span> <span class="c1"># because input will be 28*28=724
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="c1"># add how we want to optimize the weights
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="c1"># train
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="c1"># each weigth update occurs per 64 data
</span>                    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<p>where:</p>

<ul>
  <li>Keras optimizers: https://keras.io/api/optimizers/</li>
  <li>Keras Losses: https://keras.io/api/losses/</li>
</ul>

<p>the <code class="language-plaintext highlighter-rouge">model</code> and <code class="language-plaintext highlighter-rouge">history</code> variables are <strong>important</strong>.</p>

<ul>
  <li>
    <p>save/load/visualize the model</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'mnist_recognition.model'</span><span class="p">)</span> <span class="c1"># save
</span>   
<span class="n">loaded_model</span><span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">'mnist_recognition.model'</span><span class="p">)</span> <span class="c1"># load
</span>  
<span class="n">predictions</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span> <span class="c1"># predict
</span></code></pre></div>    </div>

    <p>visualize</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"mnist_model.png"</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211241339.png" alt="image-20220127211241339" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>plotting model performance overtime</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Train'</span><span class="p">,</span> <span class="s">'Test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
  
<span class="c1"># Plot training &amp; validation loss values
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Train'</span><span class="p">,</span> <span class="s">'Test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Acc</th>
          <th style="text-align: center">Loss</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211406254.png" alt="image-20220127211406254" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211415578.png" alt="image-20220127211415578" /></td>
        </tr>
      </tbody>
    </table>

    <p>where it is clear that test performance has stopped improving pretty much after the 1st epoch</p>
  </li>
</ul>

<h3 id="callbacks">Callbacks</h3>

<p>Reading: https://keras.io/api/callbacks/</p>

<p>A callback is an object that can <strong>perform actions at various stages of training</strong> (e.g. at the start or end of an epoch, before or after a single batch, etc).</p>

<p>You can use callbacks to:</p>

<ul>
  <li>Write TensorBoard logs after every batch of training to monitor your metrics</li>
  <li>Periodically save your model to disk</li>
  <li>Do early stopping</li>
  <li>Get a view on internal states and statistics of a model during training</li>
  <li>…and more</li>
</ul>

<p>Common usages include:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s">'model.{epoch:02d}-{val_loss:.2f}.h5'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="s">'./logs'</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">my_callbacks</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="tensorboard">TensorBoard</h4>

<p>TensorBoard is a visualization tool provided with TensorFlow. This callback logs events for TensorBoard, including:</p>

<ul>
  <li>
    <p>Metrics summary plots</p>
  </li>
  <li>
    <p>Training graph visualization</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>The exclamation mark <code class="language-plaintext highlighter-rouge">!</code> is used for executing commands from the uderlying operating system; here is an example using WIndows <code class="language-plaintext highlighter-rouge">dir</code>:</p>

      <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="nb">dir</span>
<span class="c1"># result:
</span><span class="n">Volume</span> <span class="ow">in</span> <span class="n">drive</span> <span class="n">C</span> <span class="n">has</span> <span class="n">no</span> <span class="n">label</span><span class="p">.</span>
 <span class="n">Volume</span> <span class="n">Serial</span> <span class="n">Number</span> <span class="ow">is</span> <span class="mi">52</span><span class="n">EA</span><span class="o">-</span><span class="n">B90C</span>
  
 <span class="n">Directory</span> <span class="n">of</span> <span class="n">C</span><span class="p">:</span>\<span class="n">Users</span>\<span class="n">Root</span>
  
<span class="mi">27</span><span class="o">/</span><span class="mi">11</span><span class="o">/</span><span class="mi">2018</span>  <span class="mi">13</span><span class="p">:</span><span class="mi">08</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">.</span>
<span class="mi">27</span><span class="o">/</span><span class="mi">11</span><span class="o">/</span><span class="mi">2018</span>  <span class="mi">13</span><span class="p">:</span><span class="mi">08</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">..</span>
<span class="mi">23</span><span class="o">/</span><span class="mi">08</span><span class="o">/</span><span class="mi">2016</span>  <span class="mi">11</span><span class="p">:</span><span class="mi">00</span>             <span class="mi">2</span><span class="p">,</span><span class="mi">258</span> <span class="p">.</span><span class="n">adalcache</span>
<span class="mi">12</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">2016</span>  <span class="mi">18</span><span class="p">:</span><span class="mi">06</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">.</span><span class="n">anaconda</span>
</code></pre></div>      </div>

      <p>you can even do crazy things like</p>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">contents</span> <span class="o">=</span> <span class="err">!</span><span class="n">ls</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="k">print</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>
<span class="p">[</span><span class="s">'myproject.txt'</span><span class="p">]</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">directory</span> <span class="o">=</span> <span class="err">!</span><span class="n">pwd</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="k">print</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
<span class="p">[</span><span class="s">'/Users/jakevdp/notebooks/tmp/myproject'</span><span class="p">]</span>
</code></pre></div>      </div>
    </li>
    <li>
      <p>the <code class="language-plaintext highlighter-rouge">%</code> mark is for <strong>magic functions</strong></p>

      <ul>
        <li>
          <p>basically a way to provide richer output than simple CLI, but also allows for customization.</p>

          <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span> <span class="c1"># allows for magic functions in tensorboard
</span>    
<span class="c1"># do something
</span>    
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">logs</span><span class="o">/</span><span class="n">fit</span>
</code></pre></div>          </div>
        </li>
        <li>
          <p>Besides <code class="language-plaintext highlighter-rouge">%cd</code>, other available shell-like magic functions are <code class="language-plaintext highlighter-rouge">%cat</code>, <code class="language-plaintext highlighter-rouge">%cp</code>, <code class="language-plaintext highlighter-rouge">%env</code>, <code class="language-plaintext highlighter-rouge">%ls</code>, <code class="language-plaintext highlighter-rouge">%man</code>, <code class="language-plaintext highlighter-rouge">%mkdir</code>, <code class="language-plaintext highlighter-rouge">%more</code>, <code class="language-plaintext highlighter-rouge">%mv</code>, <code class="language-plaintext highlighter-rouge">%pwd</code>, <code class="language-plaintext highlighter-rouge">%rm</code>, and <code class="language-plaintext highlighter-rouge">%rmdir</code>, any of which can be used without the <code class="language-plaintext highlighter-rouge">%</code> sign if <code class="language-plaintext highlighter-rouge">automagic</code> is on. This makes it so that you can almost treat the IPython prompt as if it’s a normal shell:</p>

          <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">16</span><span class="p">]:</span> <span class="n">mkdir</span> <span class="n">tmp</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="n">ls</span>
<span class="n">myproject</span><span class="p">.</span><span class="n">txt</span>  <span class="n">tmp</span><span class="o">/</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">cp</span> <span class="n">myproject</span><span class="p">.</span><span class="n">txt</span> <span class="n">tmp</span><span class="o">/</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">ls</span> <span class="n">tmp</span>
<span class="n">myproject</span><span class="p">.</span><span class="n">txt</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">rm</span> <span class="o">-</span><span class="n">r</span> <span class="n">tmp</span>
</code></pre></div>          </div>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
<span class="c1"># Clear any logs from previous runs
</span><span class="err">!</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="p">.</span><span class="o">/</span><span class="n">logs</span><span class="o">/</span> 
</code></pre></div></div>

<p>Then we configure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_dir</span><span class="o">=</span><span class="s">"logs/fit/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">)</span>
<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> 
          <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> 
          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">])</span> <span class="c1"># specify callback
</span></code></pre></div></div>

<p>Finally:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">logs</span><span class="o">/</span><span class="n">fit</span>
</code></pre></div></div>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211824150.png" alt="image-20220127211824150" /></p>

<p>note that:</p>

<ul>
  <li>if you are using VSCode, you need to switch your cell output format to <code class="language-plaintext highlighter-rouge">text/html</code> so this thing renders correctly</li>
</ul>

<h4 id="early-stopping">Early Stopping</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">earlystopping_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="c1"># which field in `history` you are looking at
</span>    <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># if stays the same for 2 epochs, stop
</span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'auto'</span><span class="p">,</span> <span class="n">baseline</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Then using this callback</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="n">history</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> 
            <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> 
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">,</span> <span class="n">earlystopping_callback</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="autokeras">AutoKeras</h3>

<p>An even <strong>higher level than <code class="language-plaintext highlighter-rouge">keras</code></strong></p>

<ul>
  <li>encapsulates even some basic preprocessing</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>
</code></pre></div></div>

<p>then the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 
</code></pre></div></div>

<p>finally, the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the image classifier.
# didn't mention what architecture or optimizer to use!
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="p">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># max #models to try
</span>
<span class="c1"># Feed the image classifier with training data.
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Predict with the best model.
</span><span class="n">predicted_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">)</span>


<span class="c1"># Evaluate the best model with testing data.
</span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Validating</strong></p>

<ul>
  <li>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use <code class="language-plaintext highlighter-rouge">validation_split </code>to specify the percentage.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="c1"># Split the training data and use the last 15% as validation data.
</span>    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Predict with the best model.
</span><span class="n">predicted_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">)</span>


<span class="c1"># Evaluate the best model with testing data.
</span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="cnn-with-tf">CNN with TF</h2>

<p>For dense neural network it actually works on MNIST</p>

<ul>
  <li>because the features that are encoded in the images are <strong>easy representations</strong></li>
</ul>

<p>Then if we check this on CIFAR10</p>

<ul>
  <li>simple NN doesn’t work anymore as the feature “extraction from previous layers” is not very representative</li>
</ul>

<p>Remember to use transfer learning for using those models</p>

<ul>
  <li>i.e. train on the last layer</li>
  <li>e.g. use VGG16 on tumor classification</li>
</ul>

<h2 id="gnn-withdglai">GNN with<code class="language-plaintext highlighter-rouge">dgl.ai</code></h2>

<p><code class="language-plaintext highlighter-rouge">dgl.ai</code> (deep graph library), a framework that allows us to implement, experiment, and run graph machine learning techniques. The tutorial will explore how nodes and edges are represented as well as features of edges and nodes</p>

<h2 id="pyro">Pyro</h2>

<p>A framework with pytorch as backend to do probablistic programming.</p>

<p>The idea is that you can do <strong>probability computations</strong> such as $P(x\vert z)$ by sampling $z$ from some prior distirbution of your choice.</p>

<h2 id="gan">GAN</h2>

<p>Basically read the following link https://www.tensorflow.org/tutorials/generative/dcgan, and the real exapmles below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
  <span class="s">'''INSERT YOUR CODE FOR THE MODEL DEFINITION BASED ON THE DESCRIPTION GIVEN ABOVE'''</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">))</span>

  <span class="c1">#Model compilation
</span>  <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">generator</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">):</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
	<span class="n">n_nodes</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">128</span><span class="p">)))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
	<span class="n">X</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span>

<span class="c1"># use the generator to generate n fake examples with class labels
</span><span class="k">def</span> <span class="nf">generate_fake_samples</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
	<span class="n">x_input</span> <span class="o">=</span> <span class="n">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="c1">#Predicting outputs
</span>	<span class="n">X</span> <span class="o">=</span> <span class="n">g_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>
	<span class="n">y</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">generate_real_samples</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
  <span class="c1">#Choosing random instances
</span>	<span class="n">ix</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="c1">#Retrieving Images
</span>	<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
	<span class="c1">#Generating class labels
</span>	<span class="n">y</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">define_gan</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
	<span class="n">d_model</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">g_model</span><span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
	<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>Then training</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">gan_model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_batch</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
	<span class="n">bat_per_epo</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_batch</span><span class="p">)</span>
	<span class="n">half_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_batch</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
	<span class="n">num_steps</span> <span class="o">=</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="n">bat_per_epo</span>
	<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bat_per_epo</span><span class="p">):</span>
			<span class="n">X_real</span><span class="p">,</span> <span class="n">y_real</span> <span class="o">=</span> <span class="n">generate_real_samples</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">half_batch</span><span class="p">)</span>
			<span class="n">X_fake</span><span class="p">,</span> <span class="n">y_fake</span> <span class="o">=</span> <span class="n">generate_fake_samples</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">half_batch</span><span class="p">)</span>
			<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">((</span><span class="n">X_real</span><span class="p">,</span> <span class="n">X_fake</span><span class="p">)),</span> <span class="n">vstack</span><span class="p">((</span><span class="n">y_real</span><span class="p">,</span> <span class="n">y_fake</span><span class="p">))</span>
			<span class="n">d_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">d_model</span><span class="p">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># first train discriminator
</span>			<span class="n">X_gan</span> <span class="o">=</span> <span class="n">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_batch</span><span class="p">)</span>
			<span class="n">y_gan</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">n_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
			<span class="n">g_loss</span> <span class="o">=</span> <span class="n">gan_model</span><span class="p">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">X_gan</span><span class="p">,</span> <span class="n">y_gan</span><span class="p">)</span> <span class="c1"># fix discriminator and train generator
</span>			
			<span class="c1"># print('&gt;%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))
</span>			<span class="n">pbar</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="n">pbar</span><span class="p">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"[Epoch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s">] Step </span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">bat_per_epo</span><span class="si">}</span><span class="s">: d_loss=</span><span class="si">{</span><span class="n">d_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> g_loss=</span><span class="si">{</span><span class="n">g_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">()</span>
<span class="n">g_model</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)</span>
<span class="n">gan_model</span> <span class="o">=</span> <span class="n">define_gan</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_real_samples</span><span class="p">()</span>
<span class="n">train</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">gan_model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="competitions">Competitions</h1>

<blockquote>
  <p><strong>Object Detection in Haze Link</strong></p>

  <ul>
    <li>http://cvpr2022.ug2challenge.org/track1.html</li>
  </ul>
</blockquote>

<p>basically we need to do:</p>

<ol>
  <li>dehazing an image</li>
  <li>object detection in those images
    <ul>
      <li>predict the bounding box for a vehicles</li>
    </ul>
  </li>
</ol>

<p>Some tipcs:</p>

<ul>
  <li>data augmentation likely needed</li>
</ul>

<hr />

<p>Ideas for <strong>dehazing</strong></p>

<ul>
  <li><strong>Pix2Pix:</strong> Image-to-Image Translation
    <ul>
      <li>Given a training set of unfiltered and filtered image pairs $A : A’$ and a new unfiltered image $B$ the output is a filtered image $B’$ such that the analogy $A:A’::B : B’$ is <strong>maintained</strong></li>
      <li>An input image is mapped to a synthesized image with different properties. The loss function is a combination of the conditional GAN loss with an additional loss term which is a pixel-wise loss that encourages the generator to match the source image</li>
    </ul>
  </li>
  <li>https://arxiv.org/pdf/1912.07015.pdf</li>
  <li>https://link.springer.com/article/10.1007/s11042-021-11442-6</li>
</ul>

<hr />

<p>Ideas for <strong>Object BB Drawing</strong></p>

<ul>
  <li>ViT based</li>
</ul>

<h2 id="detr">DETR</h2>

<p>Object detection (drawing bounding boxes).</p>

<p>Useful resources:</p>

<ul>
  <li>https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook</li>
  <li>https://github.com/jasonyux/cvpr_2022 (checkout <code class="language-plaintext highlighter-rouge">models</code> folder)</li>
</ul>

<p><strong>code</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">pathlib</span><span class="p">.</span><span class="n">Path</span><span class="p">(</span><span class="n">__file__</span><span class="p">).</span><span class="n">parent</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">resolve</span><span class="p">()</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">root</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetrFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetrConfig</span><span class="p">,</span> <span class="n">DetrForObjectDetection</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">utils.eval_util</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">utils.utils</span> <span class="kn">import</span> <span class="n">save_model_stats</span>
<span class="kn">from</span> <span class="nn">transformers.models.detr.modeling_detr</span> <span class="kn">import</span> <span class="n">ACT2FN</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>

<span class="n">PRETRAIN</span> <span class="o">=</span> <span class="s">"facebook/detr-resnet-50"</span>

<span class="k">class</span> <span class="nc">DetrAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="s">"""
	Multi-headed attention from 'Attention Is All You Need' paper.

	Here, we add position embeddings to the queries and keys (as explained in the DETR paper).
	"""</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
		<span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
		<span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
		<span class="n">is_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
		<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
		<span class="k">assert</span> <span class="p">(</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span>
		<span class="p">),</span> <span class="sa">f</span><span class="s">"embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s"> and `num_heads`: </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s">)."</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bsz</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">tensor</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">with_pos_embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span>
		<span class="k">return</span> <span class="n">tensor</span> <span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">tensor</span> <span class="o">+</span> <span class="n">position_embeddings</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">key_value_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">key_value_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
		<span class="s">"""Input shape: Batch x Time x Channel"""</span>

		<span class="c1"># if key_value_states are provided this layer is used as a cross-attention layer
</span>		<span class="c1"># for the decoder
</span>		<span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
		<span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>

		<span class="c1"># add position embeddings to the hidden states before projecting to queries and keys
</span>		<span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">hidden_states_original</span> <span class="o">=</span> <span class="n">hidden_states</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">with_pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">position_embeddings</span><span class="p">)</span>

		<span class="c1"># add key-value position embeddings to the key value states
</span>		<span class="k">if</span> <span class="n">key_value_position_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">key_value_states_original</span> <span class="o">=</span> <span class="n">key_value_states</span>
			<span class="n">key_value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">with_pos_embed</span><span class="p">(</span><span class="n">key_value_states</span><span class="p">,</span> <span class="n">key_value_position_embeddings</span><span class="p">)</span>

		<span class="c1"># get query proj
</span>		<span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scaling</span>
		<span class="c1"># get key, value proj
</span>		<span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
			<span class="c1"># cross_attentions
</span>			<span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">key_value_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
			<span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">key_value_states_original</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="c1"># self_attention
</span>			<span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
			<span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states_original</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>

		<span class="n">proj_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
		<span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
		<span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
		<span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>

		<span class="n">src_len</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

		<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

		<span class="k">if</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
			<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
				<span class="sa">f</span><span class="s">"Attention weights should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attn_weights</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
			<span class="p">)</span>

		<span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
				<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
					<span class="sa">f</span><span class="s">"Attention mask should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
				<span class="p">)</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_mask</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

		<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="c1"># this operation is a bit awkward, but it's required to
</span>			<span class="c1"># make sure that attn_weights keeps its gradient.
</span>			<span class="c1"># In order to do so, attn_weights have to reshaped
</span>			<span class="c1"># twice and have to be reused in the following
</span>			<span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights_reshaped</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="bp">None</span>

		<span class="n">attn_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">):</span>
			<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
				<span class="sa">f</span><span class="s">"`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
			<span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights_reshaped</span>

<span class="k">class</span> <span class="nc">DetrEncoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DetrConfig</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="p">.</span><span class="n">activation_function</span><span class="p">]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">activation_dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">encoder_ffn_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">encoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*): position embeddings, to be added to hidden_states.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""</span>
		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span><span class="p">(</span>
			<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
			<span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
			<span class="n">position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
			<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
		<span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">).</span><span class="nb">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">).</span><span class="nb">any</span><span class="p">():</span>
				<span class="n">clamp_value</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">.</span><span class="n">dtype</span><span class="p">).</span><span class="nb">max</span> <span class="o">-</span> <span class="mi">1000</span>
				<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="n">clamp_value</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">clamp_value</span><span class="p">)</span>

		<span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

		<span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">DetrDecoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DetrConfig</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
			<span class="n">is_decoder</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="p">.</span><span class="n">activation_function</span><span class="p">]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">activation_dropout</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">config</span><span class="p">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
			<span class="n">is_decoder</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">decoder_ffn_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">query_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the cross-attention layer.
			query_position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the self-attention layer.
			encoder_hidden_states (`torch.FloatTensor`):
				cross attention input to the layer of shape `(seq_len, batch, embed_dim)`
			encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""</span>
		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

		<span class="c1"># Self Attention
</span>		<span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span><span class="p">(</span>
			<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
			<span class="n">position_embeddings</span><span class="o">=</span><span class="n">query_position_embeddings</span><span class="p">,</span>
			<span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
			<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
		<span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="c1"># Cross-Attention Block
</span>		<span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

			<span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn</span><span class="p">(</span>
				<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
				<span class="n">position_embeddings</span><span class="o">=</span><span class="n">query_position_embeddings</span><span class="p">,</span>
				<span class="n">key_value_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
				<span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
				<span class="n">key_value_position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
				<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
			<span class="p">)</span>

			<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="c1"># Fully Connected
</span>		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">outputs</span>

<span class="k">class</span> <span class="nc">FullDetr</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">lr_backbone</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="c1"># replace COCO classification head with custom head
</span>		<span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">DetrForObjectDetection</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
			<span class="n">PRETRAIN</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
			<span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="bp">True</span>
		<span class="p">)</span>

		<span class="c1"># some customizations
</span>		<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">query_position_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="n">config</span> <span class="o">=</span> <span class="n">DetrConfig</span><span class="p">(</span>
			<span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
			<span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.1</span>
		<span class="p">)</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">)):</span> <span class="c1"># there are 6 encoders
</span>			<span class="n">pretrained_stat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">state_dict</span><span class="p">()</span>
			<span class="n">new_layer</span> <span class="o">=</span> <span class="n">DetrEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
			<span class="n">new_layer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained_stat</span><span class="p">)</span>
			<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_layer</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">)):</span>
			<span class="n">pretrained_stat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">state_dict</span><span class="p">()</span>
			<span class="n">new_layer</span> <span class="o">=</span> <span class="n">DetrDecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
			<span class="n">new_layer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained_stat</span><span class="p">)</span>
			<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_layer</span>
		
		<span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">pretrained_model</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">DetrFeatureExtractor</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">PRETRAIN</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

		<span class="c1"># see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">lr_backbone</span> <span class="o">=</span> <span class="n">lr_backbone</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>

		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">.</span><span class="n">pad_and_create_pixel_mask</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">enc_pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">pixel_mask</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="n">enc_pixel_values</span><span class="p">,</span> <span class="n">pixel_mask</span><span class="o">=</span><span class="n">pixel_mask</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span>

	<span class="k">def</span> <span class="nf">common_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>

		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">.</span><span class="n">pad_and_create_pixel_mask</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">enc_pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">pixel_mask</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

		<span class="n">labels</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
		
		<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="n">enc_pixel_values</span><span class="p">,</span>
							 <span class="n">pixel_mask</span><span class="o">=</span><span class="n">pixel_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
		
		<span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
		<span class="n">loss_dict</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss_dict</span>

		<span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span>

	<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		<span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">common_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
		<span class="c1"># logs metrics for each training_step,
</span>		<span class="c1"># and the average across the epoch
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"training_loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"train_"</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

		<span class="k">return</span> <span class="n">loss</span>

	<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
		<span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">common_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"validation_loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"validation_"</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

		<span class="k">return</span> <span class="n">loss</span>

	<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">param_dicts</span> <span class="o">=</span> <span class="p">[</span>
			<span class="p">{</span><span class="s">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">(</span>
			<span class="p">)</span> <span class="k">if</span> <span class="s">"backbone"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">]},</span>
			<span class="p">{</span>
				<span class="s">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s">"backbone"</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">],</span>
				<span class="s">"lr"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr_backbone</span><span class="p">,</span>
			<span class="p">},</span>
		<span class="p">]</span>
		<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">param_dicts</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
									  <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">optimizer</span>

<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
	<span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
	<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
	<span class="n">batch</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">]</span> <span class="o">=</span> <span class="n">imgs</span>
	<span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
	<span class="k">return</span> <span class="n">batch</span>


<span class="k">class</span> <span class="nc">FullCocoDetection</span><span class="p">(</span><span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">CocoDetection</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_folder</span><span class="p">):</span>
		<span class="n">ann_file</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">img_folder</span><span class="p">):</span>
			<span class="k">if</span> <span class="nb">file</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".json"</span><span class="p">):</span>
				<span class="n">ann_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">img_folder</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">FullCocoDetection</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">img_folder</span><span class="p">,</span> <span class="n">ann_file</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">ann_file</span> <span class="o">=</span> <span class="n">ann_file</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="o">=</span> <span class="bp">None</span>

	<span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
		<span class="c1"># read in PIL image and target in COCO format
</span>		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">FullCocoDetection</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
		
		<span class="n">image_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
		<span class="n">target</span> <span class="o">=</span> <span class="p">{</span><span class="s">'image_id'</span><span class="p">:</span> <span class="n">image_id</span><span class="p">,</span> <span class="s">'annotations'</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
		<span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">target</span>

	<span class="k">def</span> <span class="nf">id_to_filename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="o">=</span> <span class="p">{}</span>
			<span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ann_file</span><span class="p">,</span> <span class="s">"r"</span><span class="p">))</span>
			<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s">'images'</span><span class="p">]:</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'id'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s">'file_name'</span><span class="p">]</span>
		<span class="k">return</span>  <span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
	<span class="n">root</span> <span class="o">=</span> <span class="s">"datasets"</span>
	<span class="n">model_name</span> <span class="o">=</span> <span class="s">"haze_full_detr_50"</span>

	<span class="n">info</span> <span class="o">=</span> <span class="p">{</span>
		<span class="s">"train_dset"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s">'full_train_haze_dset'</span><span class="p">)),</span>
		<span class="s">"test_dset"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s">'dry_test_dset'</span><span class="p">)),</span>
		<span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
		<span class="s">"test_batch_size"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
		<span class="s">"model_lr"</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
		<span class="s">"model_lr_backbone"</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span>
		<span class="s">"model_weight_decay"</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
		<span class="s">"num_epochs"</span><span class="p">:</span> <span class="mi">50</span>
	<span class="p">}</span>
	<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"""
	Training with 
		</span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">
	"""</span><span class="p">)</span>

	<span class="c1">##### prepare data
</span>	<span class="n">train_images</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">'train_dset'</span><span class="p">]</span>
	<span class="n">test_images</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">'test_dset'</span><span class="p">]</span>

	<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">FullCocoDetection</span><span class="p">(</span><span class="n">img_folder</span><span class="o">=</span><span class="n">train_images</span><span class="p">)</span>
	<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">FullCocoDetection</span><span class="p">(</span><span class="n">img_folder</span><span class="o">=</span><span class="n">test_images</span><span class="p">)</span>

	<span class="k">print</span><span class="p">(</span><span class="s">"Number of training examples:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"Number of validation examples:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">))</span>

	<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
		<span class="n">train_dataset</span><span class="p">,</span> 
		<span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
		<span class="n">batch_size</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"train_batch_size"</span><span class="p">],</span> 
		<span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span>
	<span class="p">)</span>
	<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
		<span class="n">test_dataset</span><span class="p">,</span> 
		<span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
		<span class="n">batch_size</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"test_batch_size"</span><span class="p">]</span>
	<span class="p">)</span>

	<span class="c1">##### model
</span>	<span class="n">model</span> <span class="o">=</span> <span class="n">FullDetr</span><span class="p">(</span>
		<span class="n">lr</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_lr"</span><span class="p">],</span> 
		<span class="n">lr_backbone</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_lr_backbone"</span><span class="p">],</span> 
		<span class="n">weight_decay</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_weight_decay"</span><span class="p">]</span>
	<span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

	<span class="c1">##### train
</span>	<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">"num_epochs"</span><span class="p">]</span>
	<span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>

	<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">))</span>

	<span class="n">optimizer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">configure_optimizers</span><span class="p">()</span>

	<span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
		<span class="s">"train_map"</span><span class="p">:[],</span>
		<span class="s">"val_map"</span><span class="p">:[]</span>
	<span class="p">}</span>
	<span class="n">train_base_ds</span> <span class="o">=</span> <span class="n">get_coco_api_from_dataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
	<span class="n">test_base_ds</span> <span class="o">=</span> <span class="n">get_coco_api_from_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>

	<span class="c1"># accelerator = Accelerator()
</span>
	<span class="n">best_vmap</span> <span class="o">=</span> <span class="mf">0.</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
		<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
			<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
			<span class="c1"># accelerator.backward(loss)
</span>
			<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
			<span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
			<span class="n">progress_bar</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
		
		<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
		<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">feature_extractor</span>
		<span class="n">tmap</span> <span class="o">=</span> <span class="n">calculate_full_map</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_base_ds</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">vmap</span> <span class="o">=</span> <span class="n">calculate_full_map</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">test_base_ds</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">history</span><span class="p">[</span><span class="s">"train_map"</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">tmap</span><span class="p">)</span>
		<span class="n">history</span><span class="p">[</span><span class="s">"val_map"</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">vmap</span><span class="p">)</span>
		<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"train map=</span><span class="si">{</span><span class="n">tmap</span><span class="si">}</span><span class="s">, val map=</span><span class="si">{</span><span class="n">vmap</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">vmap</span> <span class="o">&gt;</span> <span class="n">best_vmap</span><span class="p">:</span>
			<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sa">f</span><span class="s">"models/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
			<span class="n">best_vmap</span> <span class="o">=</span> <span class="n">vmap</span>

	<span class="n">info</span><span class="p">[</span><span class="s">'model_saved_checkpoint'</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_vmap</span>
	<span class="n">save_model_stats</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">history</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="data-augmentations">Data Augmentations</h2>

<p>Using <code class="language-plaintext highlighter-rouge">albumentations</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">albumentations</span> <span class="k">as</span> <span class="n">A</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
		<span class="n">A</span><span class="p">.</span><span class="n">HorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
		<span class="n">A</span><span class="p">.</span><span class="n">VerticalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
		<span class="n">A</span><span class="p">.</span><span class="n">Affine</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rotate</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">180</span><span class="p">,</span> <span class="mi">180</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
	<span class="p">],</span> 
	<span class="n">bbox_params</span><span class="o">=</span><span class="n">A</span><span class="p">.</span><span class="n">BboxParams</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s">'coco'</span><span class="p">,</span> <span class="n">min_visibility</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="p">)</span>


<span class="n">sample_data</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># numpy array of an image
</span><span class="n">bboxes</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># bounding boxes
</span><span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s">'annotations'</span><span class="p">]:</span>
	<span class="n">bboxes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="s">'bbox'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s">'car'</span><span class="p">])</span>

<span class="n">transformed</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
<span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="s">'image'</span><span class="p">]</span>
<span class="n">transformed_bboxes</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="s">'bboxes'</span><span class="p">]</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[Logistics and Introduction]]></summary></entry><entry><title type="html">COMS6998 Dialog Systems</title><link href="/lectures/2022@columbia/COMS6998_Dialog_Systems.html/" rel="alternate" type="text/html" title="COMS6998 Dialog Systems" /><published>2022-10-19T00:00:00+00:00</published><updated>2022-10-19T00:00:00+00:00</updated><id>/lectures/2022@columbia/COMS6998_Dialog_Systems</id><content type="html" xml:base="/lectures/2022@columbia/COMS6998_Dialog_Systems.html/"><![CDATA[<p>6998: Conversational AI</p>

<h1 id="logistics-and-related-topics">Logistics and Related Topics</h1>

<ul>
  <li>paper sign up: https://docs.google.com/spreadsheets/d/1qUP7ngFG996foQN017L0gHDrorpZcPYAOcwgFFmeV_k/edit#gid=0</li>
  <li>instead of Piazza, we are using Slack (for a smaller group but more interactions)</li>
</ul>

<p>Grading</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Grading</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Paper Presentation (35min, record and place ppt link in the google shee, this time also includes discussion)</td>
      <td>15%</td>
    </tr>
    <tr>
      <td style="text-align: left">Reading Summary (weekly, 2 papers)</td>
      <td>15%</td>
    </tr>
    <tr>
      <td style="text-align: left">Proposal</td>
      <td>10%</td>
    </tr>
    <tr>
      <td style="text-align: left">Mid-Term Project Report</td>
      <td>15%</td>
    </tr>
    <tr>
      <td style="text-align: left">Final Report</td>
      <td>30%</td>
    </tr>
    <tr>
      <td style="text-align: left">Class Attendance (ask questions in class)</td>
      <td>15%</td>
    </tr>
  </tbody>
</table>

<p>The main conference for NLP would be ACL, and dialog has been a very popular field:</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907163617003.png" alt="image-20220907163617003" style="zoom:50%;" /></p>

<p>Task-oriented conversational agents can help complete task that are <strong>more efficient, standardized, and cheaper</strong> way.</p>

<h2 id="dialog-system-basics">Dialog System Basics</h2>

<p>Usually have two different goals and hence two metrics</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907164320144.png" alt="image-20220907164320144" style="zoom:33%;" /></p>

<ul>
  <li>task-oriented chatbot: relative simple to evaluate
    <ul>
      <li>e.g. flight booking</li>
    </ul>
  </li>
  <li>social chatbot: engage the user to stay in the conversation.</li>
</ul>

<p>Usually, we consider dialog framework (<mark>abstractions</mark>) for task oriented chatbots:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Module</th>
      <th>Task</th>
      <th>Example I/O</th>
      <th style="text-align: left">Solution/Architecture</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907164901763.png" alt="image-20220907164901763" /></td>
      <td>What is the user intent? For instance</td>
      <td><strong>Problem</strong>: user utterance $\to$ a distributed semantic representation<br /><strong>Sub-Tasks</strong>: intent detection, slot filling<br />e.g. I need something that is in east part of the town $\to$ <code class="language-plaintext highlighter-rouge">Inform: location=east</code></td>
      <td style="text-align: left">classification+sequence labeling (BIO tags) or sequence generation (e.g. directly use T5 to generate the desired output)</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907165352093.png" alt="image-20220907165352093" /></td>
      <td>Important information you need to remember over time to complete the task (i.e. <strong>what is happening and what the user still needs</strong>) (e.g. tracking NLU output over time).<br />This is actually very important because if you get this right, then you just need to do some API calls and done.</td>
      <td>Input:<br />User: I am looking for a moderate price ranged Italian restaurant.<br />Sys: De luca cucina and bar is a modern European restaurant in the center.<br />User: I need something that’s in the east part of town<br /><strong>Output</strong>: <code class="language-plaintext highlighter-rouge">inform: price=moderate, location=east</code></td>
      <td style="text-align: left">Sequence classification (e.g. given <code class="language-plaintext highlighter-rouge">price</code>, is it <code class="language-plaintext highlighter-rouge">low</code>, <code class="language-plaintext highlighter-rouge">moderate</code>, or <code class="language-plaintext highlighter-rouge">high</code>) or sequence generation</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907165404636.png" alt="image-20220907165404636" /></td>
      <td>Dialog Policy Planning: plan what the system <em>should</em> say.<br />(e.g. use of offline RL)</td>
      <td><strong>Problem</strong>: dialog state $\to$ system action mean representation(intent + slot)/template<br /><br />e.g. <code class="language-plaintext highlighter-rouge">inform: price=moderate, location=east</code> $\to$ <code class="language-plaintext highlighter-rouge">provide: restaurant_name, price, address</code></td>
      <td style="text-align: left">supervised learning or reinforcement learning</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907165419278.png" alt="image-20220907165419278" /></td>
      <td>How to say it</td>
      <td>e.g. <code class="language-plaintext highlighter-rouge">provide: restaurant_name, price, address</code> $\to$ Curry prince is moderately priced and located at 452 newmarket road.</td>
      <td style="text-align: left">Seq-2-Seq generation</td>
    </tr>
  </tbody>
</table>

<p>why has this been a popular framework?</p>

<ul>
  <li>since this is a modular framework, it is easier to debug/find error and/or employ constraints</li>
  <li>however, it would become difficult to update the entire system since we need all components to be coherent</li>
</ul>

<p>Of course, then you have this simple brute force approach</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220907170317204.png" alt="image-20220907170317204" style="zoom:33%;" /></p>

<p>but then the problem is:</p>

<ul>
  <li>hard to perform error analysis: did the model failed to understand? fail to plan? fail to generate?</li>
  <li>difficult to control the output as desired</li>
</ul>

<blockquote>
  <p><strong>Some challenges</strong>:</p>

  <ul>
    <li>
      <p>Dialog history and/or context tracking is still sub-optimal</p>
    </li>
    <li>No data! No labelled data, and conversational data are mostly from company</li>
    <li>Big domain shifts between different dialog domains</li>
    <li>Difficult to evaluate how good your dialog system is (without human)</li>
  </ul>
</blockquote>

<h1 id="notes-on-presentations">Notes on Presentations</h1>

<ul>
  <li>explain table: what is measured/the metric</li>
  <li>anything wierd about table</li>
  <li>outline, why am I talking about this</li>
</ul>

<h1 id="dialog-datasets">Dialog Datasets</h1>

<p>Contains readings for different dialog datasets. Some common things you need to know is:</p>

<ul>
  <li><strong>Wizard-of-Oz</strong> (a style of collecting dialog data): connects two crowd workers playing the roles of the user and the system. The user is provided a goal to satisfy, and the system accesses a database of entities (basically to resolve the user’s requests), which it queries as per the user’s preferences.</li>
  <li><strong>Machine-machine Interaction</strong> (a style of collecting dialog data): the user and system roles are simulated to generate a complete
conversation flow (e.g. generate what DA to do at each turn), which can then be converted to natural language using crowd workers.</li>
</ul>

<h2 id="multiwoz---a-large-scale-multi-domain-wizard-of-oz-dataset-for-task-oriented-dialogue-modelling">MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling</h2>

<blockquote>
  <p><strong>Aim</strong>: Provide more data and <mark>across more than one domains even within a conversation</mark>. Therefore, they collected a fully labelled (current <em>domain</em>, <em>dialog intent</em>, <em>slot</em>, and its <em>value</em>) human-human written conversations over multiple domains. This can then be used for belief tracking (i.e. what the system believe the user’s intent is), dialog act and response generation.</p>

  <ul>
    <li>cover between 1 and 5 domains per dialogue thus greatly varying in length and complexity. This broad range of domains allows to create scenarios where domains are naturally connected.</li>
    <li>For example, a tourist needs to <em>find a hotel</em>, to get the list of <em>attractions</em> and to <em>book a taxi</em> to travel between both places.</li>
  </ul>
</blockquote>

<p><strong>Dataset Setup</strong>:</p>

<ul>
  <li>
    <p>Each dialogue is annotated with a sequence of <strong>dialogue states</strong> (i.e. what the user is asking for, by tracking which intent/slot-value the user is trying to fulfill) and corresponding <strong>system dialogue acts</strong> (i.e. the DA of the system repones)</p>
  </li>
  <li>
    <p>A domain is defined by an ontology (i.e. set of concepts that are related), which is a collection of slots and values that the system has (know how to deal with):</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911010133911.png" alt="image-20220911010133911" style="zoom: 67%;" /></p>

    <p>and in general, a dialogue act consists of the intent/act type (such as <code class="language-plaintext highlighter-rouge">request </code>or <code class="language-plaintext highlighter-rouge">inform</code>) and slot-value pairs. For example, the act <code class="language-plaintext highlighter-rouge">inform(domain=hotel,price=expensive)</code> has the intent <code class="language-plaintext highlighter-rouge">inform</code>, where the user is informing the system to constrain the search to expensive hotels.</p>
  </li>
</ul>

<p><strong>Dataset Collection Setup</strong>:</p>

<ol>
  <li>
    <p>sample domains to generate some dialog scenario</p>
  </li>
  <li>
    <p>prompt a user with a <strong>task template</strong> (generated by machine but mapped to natural language using a template)</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911010401705.png" alt="image-20220911010401705" style="zoom:67%;" /></p>

    <p>which is presented gradually to the user, and the user needs to <mark>fulfill those tasks by talking to the system</mark>.</p>
  </li>
  <li>
    <p>the system (wizard) is given some API/backend to query whatever he/she needs, and tries to answer the question. Note that the <mark>belief state can be tracked implicitly</mark> here since we can just check what the system is querying the database to know what he/she thinks the user needs.</p>

    <ul>
      <li>note that in the end, it is annotated again by the MT because there could be errors when performing the database queries (e.g. incomplete)</li>
    </ul>
  </li>
  <li>
    <p>perform annotation on dialog act (e.g. <code class="language-plaintext highlighter-rouge">inform(domain=hotel,price=expensive)</code>) by using Amazon Mechanical Turk (by eliminating some poor workers). An example of what they are given is shown here</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911011238227.png" alt="image-20220911011238227" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>This results in the following data statistics:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911011549988.png" alt="image-20220911011549988" style="zoom: 67%;" /></p>

    <p>on the left we see that multi-domain dialogs tends to be longer, and on the right we see that system reponses tend to be longer. And finally, very self-explanatory:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911011454740.png" alt="image-20220911011454740" style="zoom: 67%;" /></p>
  </li>
</ol>

<p><strong>Dataset Benchmarks</strong>: here they consider using this dataset to do three things:</p>

<ul>
  <li>
    <p><strong>dialog state tracking</strong> (identify the correct slot-value pair): since other datasets only have a single domain, only the <code class="language-plaintext highlighter-rouge">restaurant</code> domain in this dataset is used:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911012028270.png" alt="image-20220911012028270" style="zoom:67%;" /></p>

    <p>Here, the model is a SOTA from another <a href="https://arxiv.org/abs/1807.06517">paper</a> which basically considers learning slot-value pairs and domains separately during training, hence enabling model to <strong>learn cross-domain slot-value pairs</strong>:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911012133082.png" alt="image-20220911012133082" style="zoom:67%;" /></p>

    <p>the result is that overall accuracy is lower, hence this dataset is harder.</p>
  </li>
  <li>
    <p><strong>dialog-context-to-text generation</strong> (more end-2-end than next one) given the oracle belief state (tracked when system is doing query). We generate the response by doing:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911012630020.png" alt="image-20220911012630020" style="zoom: 67%;" /></p>

    <p>which basically sets the hidden state $h$ for the decoder to be information about what user said (belief state), while attending to the user’s actual utterances. Since this is generation, metrics such as BLEU for fluency would apply:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911012807592.png" alt="image-20220911012807592" style="zoom: 50%;" /></p>

    <p>where <em>Success</em> measures whether if the system has fulfilled the user’s request, and <em>Inform</em> measure whether the system has provided an appropriate entity.</p>
  </li>
  <li>
    <p><strong>dialog-act-to-text generation</strong>: since we have all the annotated dialog-act as well, we can test systems on their ability to generate utterances from meaning representations (e.g. dialog act to do next). Details skipped here.</p>
  </li>
</ul>

<h2 id="towards-scalable-multi-domain-conversational-agents-the-schema-guided-dialogue-dataset">Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset</h2>

<blockquote>
  <p><strong>Aim</strong>: In reality, there might be a dynamic set of intents and slots for a task, i.e. their possible values not known in advance. Therefore, they propose a <strong>dataset for zero-shot settings</strong> and also propose a <mark>schema-guided paradigm</mark> to train models making <strong>predictions over a dynamic set of intent/slots you can have in the input schema</strong>.</p>

  <ul>
    <li>essentially this dataset is collected by 1) generate outlines via M-M interaction, and then 2) convert to human language by AMT paraphrasing them</li>
  </ul>
</blockquote>

<p><strong>Dataset Setup</strong>:</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911014016544.png" alt="image-20220911014016544" style="zoom: 50%;" /></p>

<ul>
  <li>a <strong>schema</strong> (see above) is a combination of intent and slots with some additional constraints (such as some intent requires at least certain slots to be there)</li>
  <li>there are also <mark>non-categorical</mark> slots</li>
</ul>

<p><strong>Dataset Collection</strong>:</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911014417690.png" alt="image-20220911014417690" style="zoom: 67%;" /></p>

<ol>
  <li>first they use dialog simulators to interact with services to generate dialog outlines, basically a). This is achieved by first seeding the user agent with a scenario (among the 200 they identified), and then generate the dialog act to do for the next turn.</li>
  <li>then the system agent generates in a similar fashion.</li>
  <li>the rest is basically shown in the above flowchart.</li>
  <li>the MT is asked to <strong>exactly repeat the slot values</strong> in their paraphrases. This has the advantage of easily string matching to find the <mark>slot span annotation automatically</mark>. Also, it <mark>preserves all the annotation in a)</mark>, hence there is no more need for human annotators.
    <ul>
      <li>perhasp a disadvantage: reduced noise in human conversation as every/most sentences would be goal oriented? Are the intent natural on a human-basis?</li>
    </ul>
  </li>
</ol>

<p><strong>Dataset Benchmark</strong>: since one aim of this is to do <mark>zero-shot dialog state tracking</mark>, they also made a model to do that</p>

<ul>
  <li>their model is basically done by:
    <ol>
      <li>obtain a <strong>schema embedding</strong> by converting the current information into an embedding (e.g. by BERT). which embeds all the intents, slots, and slot-values.</li>
      <li>obtain an <strong>utterance embedding</strong> of the user and the previous dialog history</li>
      <li>combine the above using the so called “projection” operation, which you can then use to do
        <ul>
          <li><strong>active intent</strong> prediction: what is the current intent?
            <ul>
              <li>only a single intent per utterance?</li>
            </ul>
          </li>
          <li><strong>requested slots</strong> by the users (a classification task)</li>
          <li><strong>user goal</strong> prediction: what is the slot-value currently (up until now) request by the user
            <ol>
              <li>have a classifier to predict for each slot if things have changed</li>
              <li>if yes, predict what is the new value</li>
            </ol>
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>then, from the above we can consider evaluation tasks such as:</p>

    <ul>
      <li><strong>Active Intent Accuracy</strong>: how much active intent of the user I got right</li>
      <li><strong>Request Slot F1</strong>: macro-averaged F1 score for requested slots</li>
      <li><strong>Average Goal Accuracy</strong>: for each turn, the accuracy of the predicted value of each slot (fuzzy machine score used for continous values)</li>
      <li><mark>Joint Goal Accurarcy</mark>: usually more useful/stricter than the above</li>
    </ul>

    <p>some important results are shown here</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911020544880.png" alt="image-20220911020544880" style="zoom: 50%;" /></p>

    <p>where we notice that major factors affecting the performance across domains is:</p>

    <ol>
      <li>the presence of the service in the training data (seen services), so degrade for domains with more unseen services</li>
      <li>Among seen services, ‘<code class="language-plaintext highlighter-rouge">RideSharing</code>’ domain also exhibits poor performance, since it possesses the largest number of the possible slot values across the dataset</li>
      <li>for categorical slots, with similar slot values (e.g. “<code class="language-plaintext highlighter-rouge">Psychologist</code>” and “<code class="language-plaintext highlighter-rouge">Psychiatrist</code>”), there is a very weak signal for the model to distinguish between the different classes, resulting in inferior performance</li>
    </ol>
  </li>
</ul>

<h2 id="can-you-put-it-all-together-evaluating-conversational-agents-ability-to-blend-skills">Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills</h2>

<blockquote>
  <p><strong>Aim</strong>: Recent research has made solid strides towards gauging and improving performance of open domain conversational agents along specific skill. But a good open-domain conversational agent should be able to seamlessly <strong>blend multiple skills</strong> (knowledge, personal background, empathy) all into one cohesive conversational flow. Therefore, in this work they:</p>

  <ul>
    <li>investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training</li>
    <li>propose a new dataset, <code class="language-plaintext highlighter-rouge">BlendedSkillTalk</code>, which blends multiple skills into a single conversation (one skill per turn, but different across turns), to analyze how these capabilities would mesh together in a natural conversation</li>
  </ul>
</blockquote>

<p><strong>Dataset Setup</strong></p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911102517255.png" alt="image-20220911102517255" style="zoom: 67%;" /></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">BendedSkillTalk</code>, a small crowdsourced dataset of about 5k conversations in English where workers are instructed to try and be knowledgeable (<code class="language-plaintext highlighter-rouge">Wizard of Wikipedia</code>), empathetic (<code class="language-plaintext highlighter-rouge">EmpatheticDialogs</code>), or give personal details (<code class="language-plaintext highlighter-rouge">ConvAI2</code>) about their given persona, whenever appropriate.</p>

    <ul>
      <li>
        <p>ConvAI2 example:</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220914173659527.png" alt="image-20220914173659527" style="zoom: 50%;" /></p>

        <p>then the user is asked to converse on telling each other about their personalities</p>
      </li>
      <li>
        <p>WoW example:</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220914173747579.png" alt="image-20220914173747579" style="zoom:33%;" /></p>

        <p>where the topic is used as an initial context</p>
      </li>
      <li>
        <p>ED example:</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220914173900784.png" alt="image-20220914173900784" style="zoom:33%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>labels are the skill per turn, namely which skill they used.</p>
  </li>
</ul>

<p><strong>Dataset Collection</strong></p>

<ul>
  <li>To ensure MT workers not stick with one specific skill or being too generic, they prompt one user (<strong>guided user</strong>, denoted by <code class="language-plaintext highlighter-rouge">G</code>) with responses from models that have been trained towards a specific skill as inspiration (one each from each skill)</li>
  <li>Each starting conversation history is seeded randomly from the <code class="language-plaintext highlighter-rouge">ConvAI2</code>, <code class="language-plaintext highlighter-rouge">WoW</code>, <code class="language-plaintext highlighter-rouge">ED</code> dataset
    <ul>
      <li>if from <code class="language-plaintext highlighter-rouge">WoW</code>, a topic is also given. If from <code class="language-plaintext highlighter-rouge">ED</code>, the situation description is given</li>
    </ul>
  </li>
  <li>In fact, for labelling we have 4 labels because <code class="language-plaintext highlighter-rouge">ED</code> utterances has “Speaker” and “Listener” taking different actions:
    <ul>
      <li>Knowledge, Empathy, Personal situations, Personal background</li>
    </ul>
  </li>
</ul>

<p><strong>Dataset Benchmarks</strong>: since it is combining skills from three datasets essentially, we can have:</p>

<ul>
  <li>
    <p>a base architecture of poly-encoder which is a retrieval model: select from a set of candidates (the correct label combined with others are chosen from the training set). This is pretrained on the pushshift.io Reddit dataset</p>

    <ul>
      <li>
        <p>since it is asked to rank among candidates, metric such as <code class="language-plaintext highlighter-rouge">hit@k</code> applies. This metric works as follows. Consider two correct labels:</p>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Jack   born_in   Italy
Jack   friend_with   Thomas
</code></pre></div>        </div>

        <p>and a bunch of synthetic negatives are generated, where the model is ranking them:</p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s        p         o            score   rank
Jack   born_in   Ireland        0.789      1
Jack   born_in   Italy          0.753      2  *
Jack   born_in   Germany        0.695      3
Jack   born_in   China          0.456      4
Jack   born_in   Thomas         0.234      5
    
s        p         o            score   rank
Jack   friend_with   Thomas     0.901      1  *
Jack   friend_with   China      0.345      2
Jack   friend_with   Italy      0.293      3
Jack   friend_with   Ireland    0.201      4
Jack   friend_with   Germany    0.156      5
</code></pre></div>        </div>

        <p>notice that if we are doing <code class="language-plaintext highlighter-rouge">hit@1</code>, then only $1/2$ times the model did correctly, but if we do <code class="language-plaintext highlighter-rouge">hit@3</code>, then the it is $2/2$.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Finetune on BST</strong>: finetune the pretrained dataset directly on <code class="language-plaintext highlighter-rouge">BlendedSkillTalk </code> Dataset</p>
  </li>
  <li>
    <p><strong>Multi-task Single-Skills</strong>: finetune the model to multi-task on <code class="language-plaintext highlighter-rouge">ConvAI2</code>, <code class="language-plaintext highlighter-rouge">WoW</code>, and <code class="language-plaintext highlighter-rouge">ED</code></p>

    <ul>
      <li>however, note that there could be some stylistic difference between <code class="language-plaintext highlighter-rouge">ConvAI2</code> dialogs and <code class="language-plaintext highlighter-rouge">WoW</code> dialogs. For instance, the prior include a persona context, and the latter include a topic</li>
      <li>therefore, to avoid model exploiting those, all samples are modified to <mark>always include a persona and a topic</mark> (where there is already an alignment of <code class="language-plaintext highlighter-rouge">WoW</code> topics to <code class="language-plaintext highlighter-rouge">ConvAI2</code>)</li>
    </ul>
  </li>
  <li>
    <p><strong>Multi-task Single-Skills + BST</strong>: after the multi-task training, finetune again on the BST dataset</p>
  </li>
  <li>
    <p><strong>Multi-task Two-Stage</strong>: since many single-skilled models have already been trained, we can just use a top level BERT classifier to assign which model gets to score candidates.</p>
  </li>
  <li>
    <p>Then, along with just models trained on their own dataset, we can have 7 models to evaluate:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911104543720.png" alt="image-20220911104543720" style="zoom:80%;" /></p>

    <p>where we observe that:</p>

    <ul>
      <li>
        <p>BST shows balanced performance but failed to match the performance of the single-skill models on their original dset, as well as losing to MT Single-Skills.</p>
      </li>
      <li>
        <p>MT Single-Skills does not do exactly well as single-skill model when evaluated on their own benchmark (for <code class="language-plaintext highlighter-rouge">ConvAI2</code> and <code class="language-plaintext highlighter-rouge">ED</code>). But perhaps this is unfair since those Single-Skill models only have to choose from candidates from their own domain. Hence, the author considers mixing candidate for them to also include samples from other dataset, which gives rise to the <em>Mixed-candidates evaluation</em>.</p>

        <p>Here, MT Single-Skills is doing better, suggesting that multi-task training results in <mark>increased resilience to having to deal with more varied distractor candidates</mark></p>
      </li>
    </ul>
  </li>
  <li>
    <p>finally, there is of course the human evaluation:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220911105544154.png" alt="image-20220911105544154" style="zoom: 67%;" /></p>
  </li>
</ul>

<h1 id="dialog-understanding">Dialog Understanding</h1>

<p>Many models are basically performing the task of:</p>

<ul>
  <li><strong>input</strong> dialog history up to current turn</li>
  <li><strong>output</strong>: “what does the user want from us” in terms of classifying <strong>intent, slot, slot-value</strong>
    <ul>
      <li>then by filling in the values, <em>e.g. book a plane from Paris to London</em>, the system can perform API based queries <code class="language-plaintext highlighter-rouge">source=London, dest=Paris</code></li>
    </ul>
  </li>
</ul>

<h2 id="attention-based-recurrent-neural-network-models-for-joint-intent-detection-and-slot-filling">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling</h2>

<blockquote>
  <p><strong>Aim</strong>: we want to explore how the <mark>alignment information in slot filling</mark> can be best utilized in the encoder-decoder models, and on the other hand, whether the alignment-based RNN slot filling models can be further improved with the attention mechanism that introduced from the encoder-decoder architecture . To this end, we propose an <strong>attention-based neural network model for joint intent detection and slot filling</strong>.</p>

  <p>The main idea is:</p>

  <ul>
    <li>encode input sequence into a dense vector</li>
    <li>then use this vector to decode/generate corresponding output sequence. Here LSTM is used so alignment is natural (see example below)</li>
  </ul>
</blockquote>

<p><strong>Setup</strong></p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916193314825.png" alt="image-20220916193314825" /></p>

<ul>
  <li>the input essentially is the sentence, and slot detection and slot-filling is <strong>done simultaneously from the flat structure</strong>
    <ul>
      <li><mark>potential problem</mark> cannot track implicit information, e.g. slot value not there, or hierarchical information (i.e. tagging v.s. parsing)</li>
    </ul>
  </li>
  <li>let the input sentence be $x=(x_1,…,x_T)$, and output be $y=(y_1,…,y_T)$ and intent in addition.</li>
</ul>

<p><strong>Architecture</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">no aligned inputs</th>
      <th style="text-align: center">aligned inputs</th>
      <th style="text-align: center">aligned inputs and attention</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916193557680.png" alt="image-20220916193557680" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916193613690.png" alt="image-20220916193613690" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916193622327.png" alt="image-20220916193622327" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>essentially you have an RNN for encoder, and an RNN for decoder with inputs using the hidden states from $x$ for alignment</p>

    <ul>
      <li><strong>encoder</strong>: bidirectional LSTM, so we get $[fh_1,…,fh_T]$ and $[bh_1,…,bh_T]$ being the forward/backward hidden states. Then $h_i = [fh_i, bh_i]$ is the total hidden state for each input $x_i$</li>
      <li><strong>decoder</strong>: unidirectional LSTM</li>
    </ul>
  </li>
  <li>
    <p>to model attention vector, we want to measure how each <strong>hidden state $s_j$ in the decoder</strong> relates to the <strong>aligned word’s hidden state $h_k$</strong>:</p>

\[e_{i,k} = g(s_{i-1},h_k)\]

    <p>then essentially $e$ is transformed into weights</p>

\[c_i = \sum_{j=1}^T \alpha_{i,j}h_j,\qquad \alpha_{i,j} = \mathrm{Softmax}(e_{i,j})\]

    <p>essentially $c_i$ provides additional information to answer: which of the aligned word’s hidden state $h_j$ relate to the current state I want to decode $s_{i-1}$. They also provided a visualization of this attention</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916194611133.png" alt="image-20220916194611133" style="zoom:80%;" /></p>

    <p>where we are decoding the last tag, and we are attending to words such as <code class="language-plaintext highlighter-rouge">cleveland</code>.</p>

    <ul>
      <li>they mentioned that the motivation to bring in an additional context $c$ is because <strong>distant information just from $s_j$ tends to be diluted a lot and lost</strong>.</li>
    </ul>
  </li>
  <li>
    <p>therefore, their ultimate model involves giving the decoder both the aligned hidden state $h_i$ and the attended context $c_i$</p>
  </li>
  <li>
    <p><mark>TODO</mark> two objectives and one network</p>
  </li>
  <li>
    <p>to further <strong>utilize the decoded tags</strong>, they can also do it <strong>auto-regressively</strong> by feeding the decoded tag into the forward direction of the RNN</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916195139765.png" alt="image-20220916195139765" style="zoom:67%;" /></p>
  </li>
</ul>

<p><strong>Results</strong></p>

<ul>
  <li>
    <p>They are using ATIS-3 and DEC94 dataset, which has joint intent detection and slot filling task</p>
  </li>
  <li>
    <p>then essentially they were doing an ablation study of a) effect of alignment b) effect of attention:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916195457119.png" alt="image-20220916195457119" style="zoom:67%;" /></p>

    <p>so it is found that <strong>alignment and attention does give improvement</strong>, yet the latter gave only small improvement. They also investigate further and found that the attention was giving mostly uniform weights except for a few cases (hence the small boosts)</p>
  </li>
</ul>

<h2 id="conversational-semantic-parsing-for-dialog-state-tracking">Conversational Semantic Parsing for Dialog State Tracking</h2>

<blockquote>
  <p><strong>Aim</strong>: By formulating DST (dialog state tracking) as a semantic parsing task over <mark>hierarchical representations</mark>, we can incorporate semantic compositionality, cross domain knowledge sharing and co-reference.</p>

  <ul>
    <li>essentially outputting a tree instead of slot-filling</li>
    <li>additionally, they collected their own dataset <code class="language-plaintext highlighter-rouge">TreeDST </code>annotated with tree-structured dialog states and system acts</li>
    <li>they combine essentially the idea of <strong>semantic parsing (e.g. drawing tree from production rules)</strong> to do DS tracking
      <ul>
        <li>in their approach, the tree drawing is done by predicting a node and its parent</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p><strong>Setup</strong>:</p>

<p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916202809150.png" alt="image-20220916202809150" style="zoom:67%;" /></p>

<ul>
  <li>
    <p>As in DST, the task is to <mark>track a user’s goal</mark> as it accumulates over the course of a conversation. To capture a hierarchical representation of domain, verbs, and operators and slots:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Tree Representation</th>
          <th style="text-align: center">Condensed, Dotted Representation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916202659271.png" alt="image-20220916202659271" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916202702538.png" alt="image-20220916202702538" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>this is done for both user and system, and this dotted form is also referred to as a <strong>meaning representation</strong>. Advantages of this include:</p>

    <ul>
      <li>tracking <mark>nested intents</mark> and representing compositions in a single graph</li>
      <li>naturally supports <mark>cross domain slot sharing</mark> and cross-turn co-reference through incorporating the shared slots or the references as sub-graphs in the representation</li>
    </ul>
  </li>
  <li>
    <p>At each turn we have</p>

    <ul>
      <li>$x_t^u$ being user utterance, $y_t^u$ being user dialog-state</li>
      <li>$x_t^s$ being system utterance, $y_t^s$ is system dialog act</li>
      <li>both $y_t^s,y_t^u$ adopt the same structure semantic formalism</li>
    </ul>
  </li>
</ul>

<p><strong>Dataset</strong>:</p>

<ul>
  <li>
    <p>collection is similar to the <a href="#Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset">SGD</a> paper, where they:</p>

    <ol>
      <li>generate agendas/conversation flows by using machines</li>
      <li>convert them via templates to language</li>
      <li>ask MT to paraphrase to natural language</li>
    </ol>
  </li>
  <li>
    <p>specifically, the agenda generation for both user and system is done by:</p>

    <ul>
      <li>
        <p>a module <strong>generating the initial user goal</strong> $P(y_o^u)$. This can be done by sampling a <mark>production rule</mark></p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916204534378.png" alt="image-20220916204534378" style="zoom: 67%;" /></p>
      </li>
      <li>
        <p>a module <strong>generating system act</strong> $P(y_t^s\vert y_t^u)$. This is done by looking at the user’s act tree $y_t^u$, look up the production rules to figure out <strong>how to finish the tree</strong>, and take that as the system act</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916205038811.png" alt="image-20220916205038811" style="zoom:67%;" /></p>
      </li>
      <li>
        <p>a module for <strong>user state update</strong>/DS based on dialog history $P(y_t^u\vert y_{&lt;t}^s,y_{&lt;t}^u)$. There are two details in this task:</p>

        <ul>
          <li>
            <p>model “introduces a new goal, continues with the previous goal, or resumes an earlier unfinished goal”. Therefore, a <strong>stack is used</strong> and updated. Therefore the top of the stack always represents the most recent unfished task $y_{t-1}^{top,u}$ and the corresponding system act $y_{t-1}^{top,s}$</p>
          </li>
          <li>
            <p>the next dialog state $y_t^u$ is generated based on the <strong>top elements of the stack as the dialog history</strong></p>

            <ul>
              <li><mark>precaustion:</mark> implies an additional structure to dialog, hence unfair advantage as their model also stores a stack? (see their modelling choice of dialog history, which takes the top of the stack as well)</li>
            </ul>

            <p>since this is now generated from two trees (there are two at the top of the stack)</p>

            <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916205633058.png" alt="image-20220916205633058" style="zoom:67%;" /></p>

            <p>where the next user state will be combining the two</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>finally, a quality control is also done by asking:</p>

    <ul>
      <li>before the paraphrasing, filter out non-realistic interactions</li>
      <li>after the paraphrasing, ask if the human-generated utterance preserves the meaning of the templated utterance</li>
    </ul>
  </li>
</ul>

<p><strong>Architecture</strong>:</p>

<ul>
  <li>
    <p>our task is to:</p>

    <ul>
      <li>infer <mark>$y_t^u$</mark> since $y_t^s$  is observed/what the system just did</li>
      <li>to also track <mark>goal switching and resumption</mark>, a <strong>stack is used to store dialog states</strong></li>
    </ul>
  </li>
  <li>
    <p>to output tree structure, we essentially just need to decode:</p>

    <ul>
      <li>a new node</li>
      <li>the new node’s parent in the existing tree</li>
    </ul>
  </li>
  <li>
    <p>additionally, they chose to <mark>maneuver their own features</mark>:</p>

    <ul>
      <li>
        <p><strong>dialog history</strong> is computed as a fixed-size history representation derived from the previous conversation flow $(Y_{&lt;t})$, specifically the top of the stat $y_{t-1}^{top,u}$ along with:</p>

\[Y_{t-1}^u = \text{merge}(y_{t-1}^u, y_{t-1}^{top,u})\]
      </li>
      <li>
        <p><strong>encoding the features</strong>:</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916210719872.png" alt="image-20220916210719872" style="zoom:80%;" /></p>

        <p>where essentially we have three features and hence three encoders:</p>

        <ul>
          <li>encoder (bidirectional LSTM) for user utterance $x_t^u$</li>
          <li>encoder (bidirectional LSTM) for system act $y_{t-1}^s$</li>
          <li>encoder (bidirectional LSTM) for dialog state $Y_{t-1}^u$ described above</li>
        </ul>

        <p>where to encode the trees, they are <strong>first linearized into strings by DFS</strong></p>
      </li>
      <li>
        <p><strong>decoding</strong>: they experimented with two versions:</p>

        <ul>
          <li>
            <p><mark>just decode a flattened string</mark>, as in Figure 1. The final aim is to simply <mark>compute the probability of next token</mark> by accessing its probability of generation and copy</p>

            <p>Specifically, the decoder takes</p>

\[g_i = \mathrm{LSTM}(g_{i-1},y_{t,i-1}^u)\]

            <p>which is basically <strong>auto-regressive</strong>, and the <strong>attention</strong> is used together with $g_i$ to compute probability of next token</p>

            <ul>
              <li>
                <p>attention: <strong>attend current state to history</strong></p>

\[a_{i,j} = attn(g_i,H)\]

                <p>where history would be either of the three from the three encoders. Then this is used for weights to produce:</p>

\[\bar{h}_i = \sum_{j=1}^n w_{i,j} h_i,\quad w_{i,j} = \mathrm{Softmax}(a_{i,j})\]
              </li>
              <li>
                <p>finally, the token distribution is computed by <strong>concatenating $\bar{h}_i^x, \bar{h}_i^s, \bar{h}_i^u$</strong> from the encoders and $g_i$ <strong>to give $f_i$</strong></p>

                <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916211928284.png" alt="image-20220916211928284" style="zoom:67%;" /></p>
              </li>
            </ul>

            <p>finally, since this is decoding a flat string, the loss is simply CCE of the correct $y_{y,i}^u$</p>
          </li>
          <li>
            <p><mark>decode a tree by generating nodes and select their parent relationships from the existing tree</mark>: you basically take the hidden state/embedding of the previous node $n_{t,i-1}^u$ and its parent relation $r_{t,i-1}^u$ to be input features as well for decoding:</p>

\[g_i = \mathrm{LSTM}(g_{i-1},n_{t,i-1}^u , r_{t,i-1}^u)\]

            <p>the we perform two predictions using two layers using $g_i$</p>

            <ul>
              <li>
                <p>predict next node probability using equation 4</p>
              </li>
              <li>
                <p>select parent of the node by attending $g_i$ to <strong>previously generated nodes</strong> and choosing the most relevant</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Results</strong></p>

<ul>
  <li>
    <p>experiment on DST dataset, where for models not with hierarchical MR in mind, all training and testing here are flattened:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916212530129.png" alt="image-20220916212530129" style="zoom:67%;" /></p>

    <ul>
      <li><mark>TODO</mark> doesn’t this also mean that flattened based is naturally less accurate than tree based (TED-Flat v.s. TED-Vanilla) given that the <strong>task is tree based</strong>?</li>
    </ul>
  </li>
  <li>
    <p>evidence of compounding error if prediction is done auto-regressively:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220916212637904.png" alt="image-20220916212637904" style="zoom:80%;" /></p>

    <p>where the oracle means substituting the gold previous state for encoding</p>
  </li>
</ul>

<h1 id="task-oriented-dialog">Task-Oriented Dialog</h1>

<p>Essentially models that can complete task by providing/informing the right entities and complete the task successfully. A popular dataset that has been used for finetuning + testing is the <strong>MultiWoZ</strong> dataset, which contains an <strong>automatic evaluation script</strong> as well.</p>

<h2 id="galaxy-a-generative-pre-trained-model-for-task-oriented-dialog">GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog</h2>

<p>In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning.</p>

<blockquote>
  <p>Specifically, we introduce a <mark>dialog act prediction task for policy optimization</mark> during pre-training and employ a consistency  regularization term to refine the learned representation with the help of unlabeled dialogs</p>
</blockquote>

<p><strong>Background</strong>:</p>

<ul>
  <li>there are intrinsic differences between the distribution of human conversations and plain texts. Directly fine-tuning plain-text-trained PLMs on downstream dialog tasks <strong>hinders the model</strong> from effectively capturing conversational linguistic knowledge. Therefore, current attempts to tackle this issue try to <strong>build Pre-trained Conversation Models (PCMs) by directly optimizing vanilla language model</strong> objectives on dialog corpora</li>
  <li>Therefore, we hypothesize that explicitly incorporating the DA annotations into the pre-training process can also <strong>facilitate learning better representations for policy optimization to improve the overall end-to-end performance</strong></li>
  <li>Although DAs are general tags to describe speakers’ communicative behaviors (Bunt 2009), current DA annotations in task-oriented dialog are still limited and <strong>lack of unified taxonomy</strong> because each dataset is small and scattered.</li>
</ul>

<p><strong>Dataset</strong>:</p>

<ul>
  <li>
    <p>To begin with, we build a unified DA taxonomy for TOD (task-oriented-dialog) and examine eight existing datasets to develop a new labeled dataset named <code class="language-plaintext highlighter-rouge">UniDA </code>with a total of 975K utterances. We also collect and process a large-scale unlabeled dialog corpus called <code class="language-plaintext highlighter-rouge">UnDial </code> with 35M utterances</p>
  </li>
  <li>
    <p>We propose a more <strong>comprehensive unified DA taxonomy</strong> for task-oriented dialog, which consists of 20 frequently-used DAs</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220705152203066.png" alt="image-20220705152203066" style="zoom:67%;" /></p>
  </li>
</ul>

<p><strong>Model</strong>:</p>

<ul>
  <li>
    <p>We choose <mark>UniLM</mark> (Dong et al. 2019) as our <mark>backbone</mark> model, which contains a bi-directional encoder for understanding and a unidirectional decoder for generation.</p>
  </li>
  <li>
    <p>We adopt a similar scheme of input representation in Bao et al. (2020), where the <mark>input embeddings consist of four elements: tokens, roles, turns, and positions</mark>.</p>

    <ul>
      <li>Role embeddings are like segmentation embeddings in BERT and are used to differentiate which role the current token belongs to, either user or system</li>
      <li>Turn embeddings are assigned to each token according to its turn number.</li>
      <li>Position embeddings are assigned to each token according to its relative position</li>
    </ul>
  </li>
  <li>
    <p>Four objectives are employed in our dialog pre-training process: response selection, response generation, <mark>DA prediction</mark> and consistency regularization.</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220705152829713.png" alt="image-20220705152829713" /></p>

    <p>where the important components include:</p>

    <ul>
      <li>
        <p><strong>Response Selection</strong>. For a context response pair $(c; r)$ from the corpus, the positive example (with label $l = 1$) is obtained by concatenating $c$ with its corresponding response $r$, and the negative example (with label $l = 0$) is constructed by concatenating $c$ with a response $r^-$ that is randomly selected from the corpus. Then simply for each pair:</p>

\[L_{RS} = -\log p(l=1|c,r)-\log p(l=0|c,r^-)\]

        <p>where the predicted probability is fine-tuned by adding a linear head to the transformer and then a sigmoid on the <code class="language-plaintext highlighter-rouge">[CLS]</code> token:</p>

\[p(l=1|c,r) = \mathrm{sigmoid}(\phi_a(h_{cls}))\]

        <p>note that $\phi_a$ is a fully-connected NN with output size of 1.</p>
      </li>
      <li>
        <p><strong>Response Generation</strong>. The response generation task aims to predict the dialog response r auto-regressively based on the dialog context $c$. Here we use the standard NLL loss per token in each generated sequence:</p>

\[L_{RG} = - \sum_{t=1}^T \log p(r_t|c,r_{&lt;t})\]

        <p>for $r_t$ is the $t$-th word in $r$, and $r_{&lt;t}={r_1,…,r_{t-1}}$. So basically this is the greedy prediction.</p>
      </li>
      <li>
        <p><strong>DA Prediction</strong>: For a context response pair $(c; r)$ sampled from <code class="language-plaintext highlighter-rouge">UniDA</code>, the DA prediction task aims to predict the DA label $a$ of the response $r$ based <mark>merely on the context $c$</mark>. However, since there are responses in <code class="language-plaintext highlighter-rouge">UniDA </code> associated with multiple DA, we model it as a Bernoulli Distribution such that $a\equiv (a_1,a_2,…,a_N)$ for $N$ being the number of dialog acts, and $p(a\vert c)=\prod_i^N p(a_i\vert c)$. Therefore, taking the dialog context $c$ as input, we add a multi-dimensional binary classifiers on $h_{cls}$ to predict each act $a_i$:</p>

\[L_{DA}=-\sum_{i=1}^N \{y_i \log p(a_i|c)+(1-y_i)\log(1-p(a_i|c)\}\]

        <p>and our prediction is a single $N$ dimensional vector</p>

\[p(a|c) = \mathrm{sigmoid}(\phi_b(h_{cls})) \in \mathbb{R}^N\]
      </li>
      <li>
        <p><strong>Consistency Regularization</strong>: because there is no DA label for <code class="language-plaintext highlighter-rouge">UniDial</code>, we do some kind of self-supervision on inferring the DA labels based on a given dialog context $c$. Specifically, we use the same network $\phi_b$ to predict the dialog act twice after a dropout layer:</p>

\[q(a|c)=\mathrm{softmax}(\phi_b(h_{cls})) \in \mathbb{R}^N\]

        <p>which basically predicts DA distribution of a given sequence. Then as we feed the sequence through the same dropout layer we have different hidden features, we can consider to match the two distributions:</p>

\[L_{KL} = \frac{1}{2}\left( D_{KL}(q_1||q_2)+  D_{KL}(q_2||q_1)\right)\]

        <p>essentially making sure that the learnt features are useful for DA prediction.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Hence, the pretraining objective is by mixing:</p>

    <ul>
      <li>
        <p><code class="language-plaintext highlighter-rouge">UniDA</code> samples which includes DA annotation, hence</p>

\[L = L_{RS} + L_{RG} + L_{DA} + L_{KL}\]
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">UniDial</code> samples which does not have DA annotation and some are very <mark>noisy</mark>. Hence we consider a <mark>gating mechanism of weighting the $L_{KL}$</mark> by looking at the entropy of the DA prediction $q(a\vert c)$:</p>

\[E = \sum_{i}^N q(a_i|c)\log(q(a_i|c))\]

        <p>and we want to weight more on samples with low entropy:</p>

\[g=\min \left\{ \max\left\{0, \frac{E_{\max} - (E-\log E)}{E_\max} \right\},1 \right\}\]

        <p>Therefore our loss for those data is:</p>

\[L = L_{RS}+L_{RG}+g L_{KL}\]
      </li>
    </ul>
  </li>
  <li>
    <p>Finally, once we trained our model with mixing <code class="language-plaintext highlighter-rouge">UniDA</code> and <code class="language-plaintext highlighter-rouge">UniDial</code>, it is a <mark>pretrained model and we can fine-tune it with our desired DA dataset</mark> :</p>

\[L_{\text{fine-tune}} = L_{RS} + L_{RG} + \alpha L_{DA}\]

    <p>for $\alpha=1$ if the dataset has annotated DA, and $\alpha=0$ otherwise. Notice that a response selection objective is still here even if we in the end only need generation capability. This is because we want to alleviate the model discrepancy between pretraining and finetuning.</p>
  </li>
</ul>

<h2 id="godel-large-scale-pre-training-for-goal-directed-dialog">GODEL: Large-Scale Pre-Training for Goal-Directed Dialog</h2>

<blockquote>
  <p><strong>Aim</strong>: GODEL leverages a <strong>new phase of grounded pre-training</strong> designed to better support adapting GODEL to a wide range of downstream dialog tasks that <strong>require information external</strong> to the current conversation (e.g., a database or document) (i.e. the environment $E$ in the model) to produce good responses. While GODEL out-performs previous models such as DialoGPT, they also introduce a novel evaluation methodology: the introduction of a notion of <strong>utility</strong> that assesses the usefulness of responses (<strong>extrinsic</strong> evaluation) in addition to their communicative features (<strong>intrinsic</strong> evaluation, e.g. BLEU). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.</p>
</blockquote>

<p><strong>Setup</strong></p>

<ul>
  <li>
    <p>First, it is pre-trained in <strong>three phases</strong>, successively folding in data from web text, publicly available dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks (conditioned on information external to current conversation).</p>
  </li>
  <li>
    <p>we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this <strong>extrinsic</strong> dimension of functional <strong>utility</strong>, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.</p>

    <p>Therefore, a evaluation metric called <mark>Utility</mark> is proposed, so that cross-dataset comparison can be made instead of the adhoc metrics for a dataset (e.g. Success-rate and Inform-rate).</p>

    <ul>
      <li>currently the <strong>utility</strong> can only be human-evaluated</li>
    </ul>
  </li>
  <li>
    <p>after large scale pretraining, the model is <mark>tested</mark> on <strong>Multi-WOZ</strong>, <strong>CoQA</strong>, <strong>Wizard of Wikipedia</strong>, and <strong>Wizard of the Internet</strong></p>
  </li>
</ul>

<p><strong>Pretraining</strong></p>

<ul>
  <li>
    <p>GODEL is pre-trained in three phases:</p>

    <ol>
      <li>Linguistic pre-training on <strong>public web documents</strong> to provide a basic capability for text generation.</li>
      <li>Dialog pre-training on public <strong>dialog data</strong> to improve the models’ handling of general conversational behavior.</li>
      <li><strong>Grounded dialog pre-training</strong> to enable grounded response generation</li>
    </ol>

    <p>and since we can have grounded dialog, the input can have $S,E$ for $S$ being the dialog context history, and the additional information needed (e.g. price of a hotel) is the environment $E$</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220922214111334.png" alt="image-20220922214111334" style="zoom:67%;" /></p>

    <p>then, the loss for all pretraining task is the cross entropy for decoding each word:</p>

\[p(T|S,E) = \prod_{n=1}^N p(t_n | t_1, ..., t_{n-1}, S,E)\]

    <p>so $T = {t_1,…,t_n}$ is the target sentence. Note that in tasks that does not require extra information, $E$ is left as empty.</p>
  </li>
  <li>the <strong>public dialog dataset</strong> comes from the Reddit comment chains used for DialoGPT</li>
  <li>the <strong>grounded dialog dataset</strong> contains a collection of: DSTC7 Task 2 corps, MS MARCO, UnifiedQA, SGD</li>
</ul>

<p><strong>Model</strong></p>

<ul>
  <li>backbone based on T5,  T5-Large, and GPT-J is used</li>
  <li>the models are trained for at most 10 epochs, and we select the best versions on the validation set</li>
</ul>

<p><strong>Experiments</strong></p>

<ul>
  <li>
    <p>we would like to test the <strong>few-shot finetuning</strong> ability of the mode, as well as full finetuning, because in general labeled task-oriented data is small in size</p>
  </li>
  <li>
    <p>dataset used for testing therefore include the <mark>untouched</mark>: MultiWOZ, Wizard of Wikipedia, Wizard of Internet, and CoQA</p>

    <ul>
      <li>specifically, for few-shot we consider tuning 50 dialogs for each task for finetuning</li>
      <li><strong>automatic evaluation metrics are often setup already</strong> for those datasets, especially for MultiWOZ</li>
    </ul>
  </li>
  <li>
    <p>results for few-shot finetuning and full finetuning</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Few-shot Fine-tuning</th>
          <th style="text-align: center">Full Fine-tuning</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220922215023283.png" alt="image-20220922215023283" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220922215110548.png" alt="image-20220922215110548" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h1 id="social-chatbot">Social Chatbot</h1>

<p>If the aim is to <strong>extend conversations</strong> by engaging user in anyway, what can we do?</p>

<h2 id="alquist-40-towards-social-intelligence-using-generative-models-and">Alquist 4.0 Towards Social Intelligence Using Generative Models and</h2>

<blockquote>
  <p><strong>Aim</strong>: The system Alquist has a goal to conduct a <strong>coherent and engaging conversation</strong>, which essentially uses a <mark>hybrid of hand-designed responses</mark> and <mark>generative models</mark></p>

  <ul>
    <li>usually it is the hand designed responses tree that is functioning</li>
    <li>when OOD is detected from the tree (using intent classifier), response generative model is used.
      <ul>
        <li>note that <strong>a lot of control is needed</strong> for the generative model to merge nicely with the hand-written responses</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p><strong>Setup</strong></p>

<ul>
  <li>
    <p>in order to entertain the conversational partner, one has to learn what entertains the partner first and then utilize the knowledge in the following conversation</p>

    <ul>
      <li><em>exploration</em> part, in which Alquist learns the preferences of the user, the main research and development emphasis was put on <strong>Skimmer</strong>, <strong>User Profile building</strong>, and <strong>Entity and Knowledge Utilization</strong></li>
      <li><em>exploitation</em> part, in which Alquist utilizes the knowledge about the user, the main emphasis was put on the research and development of the <strong>Dialogue Manager</strong>, <strong>Trivia Selection</strong>, <strong>Intent and Out-of-Domain classification</strong></li>
    </ul>
  </li>
  <li>
    <p>the basic flow looks like this:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930210844601.png" alt="image-20220930210844601" style="zoom:80%;" /></p>

    <p>and on a high level, we have on the left:</p>

    <ol>
      <li>First, the <strong>Skimmer</strong> analyses the user input for the mentioned pieces of information. The pieces of information are stored in the <strong>User Profile</strong>.</li>
      <li>Based on the values stored in the user profile, the <strong>Dialogue Management</strong> selects the next dialogue to start, or selects and presents some trivia related to the actual topic of a conversation.
        <ul>
          <li>The dialogue is directed according to the <strong>Intent classification</strong> of the user input.</li>
          <li>and if needed, knowledge base is queried for updated information</li>
        </ul>
      </li>
      <li>And finally, if the <strong>Out-of-domain classification</strong> recognizes an unexpected user input whenever there is, the Neural Response Generator produces a coherent response based on the context of the conversation. Otherwise, it is the scripted dialog.</li>
    </ol>
  </li>
</ul>

<p><strong>Main Components</strong></p>

<ul>
  <li>
    <p><strong>Skimmer</strong>: extract user information from sentences using regular expressions, and then stored the attribute-value into <strong>User Profile</strong></p>
  </li>
  <li>
    <p><strong>User Profile</strong>: essentially stores information about the user talking to Alexa:</p>

    <ul>
      <li>long-term profile: global information the bot needs when the same user converses again later</li>
      <li>short-term profile: stores information discussed during the current session/dialog</li>
    </ul>

    <p>so that short-term profile is reset at the beginning of each session</p>
  </li>
  <li>
    <p><strong>Entity and Knowledge Utilization</strong>: include <mark>factual information</mark> about the entity that the user is interested in. This means you need to do:</p>

    <ul>
      <li><em>entity recognition</em>: a sequence tagging task performed by Bi-LSTM</li>
      <li><em>entity linking and knowledge base</em>: utilize external public domain-specific dataset to obtain <strong>information about the recognized entity</strong></li>
    </ul>
  </li>
  <li>
    <p><strong>Dialog Management</strong>: there are <mark>several small scripted dialogs (of intents)</mark>, which are of high quality and will be of focus of this system. Then, since there are several dialogs, a <strong>dialog selector</strong> is used when a small dialog finished and needs to find a continuation dialog:</p>

    <ol>
      <li>the <strong>dialog selector</strong> collects information about the previous context, such as user profile, topics discussed (from tags), and some additional constraints if the new dialog can start (prerequisites)</li>
      <li>if there is no trivia presented for the presently discussed topic, <strong>choose a trivia</strong></li>
      <li>if there are some scripted dialogs that fulfill the constraints, consider them</li>
      <li>if there is none, use the <strong>neural response generator</strong></li>
    </ol>
  </li>
  <li>
    <p><strong>Trivia Selection</strong>: trivia scraped from reddit, and a model for outputting an embedding for <strong>scoring cosine similarity</strong> of a trivia and the current context is used</p>

    <ul>
      <li>during scraping, vector embeddings are also stored with texts</li>
      <li>during runtime, a candidate trivia list is retrieved using <strong>full-text search</strong></li>
      <li>context of $n=2$ most recent utterance-response pairs is encoded</li>
      <li>cosine similarity computed and most relevant trivia is selected</li>
    </ul>

    <p>experimentally, the choice of model is determined empirically:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930212606488.png" alt="image-20220930212606488" style="zoom:67%;" /></p>
  </li>
  <li>
    <p><strong>Intent and OOD Classification</strong>: since each user utterance is seen as intents, the scripted dialog cannot continue if an intent is OOD from the script:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930212722131.png" alt="image-20220930212722131" style="zoom:80%;" /></p>

    <p>where there are two intents in question:</p>

    <ul>
      <li>global intent: used/can occur at anywhere</li>
      <li>local intent: should be in the tree</li>
    </ul>

    <p>again, a combination of a) cosine similarity using a model b) filtering out intents if cosine similarity is not high enough c) if non is left, the intent is OOD.</p>

    <p>Which model to use is again chosen empirically, but notice that since such a task is not common in other dsets, they had to artificially create one using existing dset by leaving some intents out, and also hand-annotated a new one</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930213028860.png" alt="image-20220930213028860" style="zoom:80%;" /></p>

    <p>notice that now performance speed also matters</p>
  </li>
  <li>
    <p><strong>Neural Response Generator</strong>: used in two tasks: a) if <strong>OOD</strong>, use this model to generate b) generates a follow up question when a <strong>trivia is selected</strong> in the dialog manager. Notice that the aim of this is to <mark>compensate</mark> the incomplete scripted dialog in some cases, but the major focus should be on the success/controllability of the scripted dialog:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930213317181.png" alt="image-20220930213317181" style="zoom:80%;" /></p>

    <p>where notice that the hand-designed dialog is restored at the end. To have this controllability, the model:</p>

    <ul>
      <li><strong>either generates a question or a statement</strong> (otherwise content generated is quite random)</li>
      <li>so that statements followed by a question would be engaging</li>
      <li>and statement itself can be used to bridge the gap between generated and hand-designed content</li>
    </ul>

    <p>To achieve this, essentially:</p>

    <ul>
      <li>DialoGPT model is used, where the special token <code class="language-plaintext highlighter-rouge">QUESTION</code> and <code class="language-plaintext highlighter-rouge">STATEMENT</code> is appended/prompted along with the context</li>
      <li>when DialoGPT generated a few candidate questions/statements, a DialoRPT is used to <strong>rank the generated content</strong></li>
      <li>tested on several datasets, by:
        <ul>
          <li>using NLTK tokenizer and CoreNLP to annotate each sentence as a statement or a question</li>
          <li>then can convert many existing dialog dataset into such a format for generation</li>
        </ul>
      </li>
    </ul>

    <p>results:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20220930213854750.png" alt="image-20220930213854750" style="zoom:80%;" /></p>

    <p>where for evaluation both automatic metric and human evaluation is used:</p>

    <ul>
      <li>automatic metric is straightforward as the labels are known</li>
      <li>human evaluation is done on if the generated content is <strong>relevant and factually true</strong>, i.e. it is <code class="language-plaintext highlighter-rouge">OK</code></li>
      <li>this is done similarly for checking the follow-up question generation after a trivia/fun facts, hence the two columns under <code class="language-plaintext highlighter-rouge">OK</code></li>
    </ul>
  </li>
</ul>

<h2 id="jason-wetson-guest-lecture">Jason Wetson Guest Lecture</h2>

<ul>
  <li>
    <p>Long term research goal</p>
  </li>
  <li>
    <p>ParlAI - collection of datasets, architectures, and integeration with Amazon MT</p>
  </li>
  <li>
    <p>find things that don’t work, but make <strong>your fix general</strong></p>
  </li>
  <li>
    <p>what is missing in current LM such as GPT-2/3:</p>

    <ul>
      <li>knowledge and hallucination in GPT-3</li>
      <li>hook on a retrieval system to incorporate knowledge and hence hallucinate less</li>
    </ul>
  </li>
  <li>
    <p>making an open domain dialog agent</p>

    <ul>
      <li>wanted to have various skills such as Peronality, Emphathy, etc.</li>
      <li>Hence collected trainning data on those skills</li>
      <li>trained Blenderbot
        <ul>
          <li>Blenderbot 1: just stacking transformers</li>
          <li>Blenderbot 2: lemon pick examples, and fix them. For exmaple, <strong>very forgetful</strong>, and a lot of <strong>factual errors</strong>
            <ul>
              <li>to solve factual errors, added internet search being part of the bot (generate an internet search query)</li>
              <li>to solve the memory problem, had an addiitonal long term memory module to retrieve information</li>
              <li>can even be used to recommend pizza places</li>
            </ul>
          </li>
          <li>Blenderbot 3: a bigger transformer, but collect more data to fix prior errors (e.g. still 3% hallucinations)
            <ul>
              <li>takes feedback live from people to learn during interactions, so <strong>data from adversarial testers could be used</strong>!</li>
              <li>an big architecture, deciding if to do internet search, geneatre response, access memory, etc.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>how do we use those feedbacks from humans?</p>

    <ul>
      <li>
        <p><mark>Director</mark>: for each token, add a classification head on thumb-up or down. Therefore, when you generate, you can combine the two scores</p>

        <ul>
          <li>
            <p>https://arxiv.org/pdf/2206.07694.pdf</p>
          </li>
          <li>
            <p>when you thumbs down, label entire sentence as negative. Will that smudge down good words, hence the how do you evaluate that classifier. This is what it does, but it still works.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>however, in some cases that labelling can be more sensitive: e.g. if you have a gold correction sentence corresponding to a sentence, then you can align and <em>extract the part of the sentence</em> that is wrong.</p>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Still Problems on Chatbots</strong></p>

<ul>
  <li>why is repetition <mark>neuron degenration</mark>, liable to repeat</li>
</ul>

<p><strong>Future of Chatbots</strong>:</p>

<ul>
  <li>how controllable is it to be applied in real life? Jason: probably not medical. But for entertainment, for gaming, recommendations pretty confident.</li>
  <li>Future of incoporating multimodal input into conversations? Jason: if text can be toxic, images can be even more toxic</li>
</ul>

<h1 id="knowledge-enriched-dialog-systems">Knowledge Enriched Dialog Systems</h1>

<p>Dialog system but aims to use/find external information to make responses more factual</p>

<h2 id="increasing-faithfulness-in-knowledge-grounded-dialogue-with-controllable-features">Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features</h2>

<blockquote>
  <p><strong>Aim</strong>: Make a dialog system that is <strong>controlled to stay faithful</strong> to the evidence. They approach this by:</p>

  <ul>
    <li>adding control tokens prepended to the input sequence</li>
    <li>train dataset using additional information on the <em>objectiveness</em>, <em>lexical precision</em>, and <em>entailment</em> (it generated response follows from the given evidence)</li>
  </ul>
</blockquote>

<p><strong>Setup</strong></p>

<ul>
  <li>
    <p>the general idea is to use a piece of evidence (provided beforehand) added to the conversation history as input</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008141417379.png" alt="image-20221008141417379" style="zoom:80%;" /></p>
  </li>
  <li>
    <p>to train this type of model, dataset with only informative utterances are needed, but that kind does not exist. Since the only available ones (e.g. Wizard of Wikipedia) are generated by humans, they <strong>mix chi-chat utterances/subjectivity</strong> to <strong>objective informative facts</strong> in their responses</p>

    <ul>
      <li>
        <p>to deal with it, you either remove all subjective ones (e.g. has personal pronouns), but that leaves too little training data left</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008141727577.png" alt="image-20221008141727577" style="zoom:80%;" /></p>
      </li>
      <li>
        <p>or, as used in this paper, they score existing samples based on <em>objective voice</em>, <em>lexical precision</em> (sticking to fact), and <em>entailment</em> to provide additional <mark>signal during training and hence control during inference</mark></p>

        <ul>
          <li>ablation study shows that those control does change performance</li>
          <li>those control tokens are discussed next</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>for training, <strong>Wizard of Wikipedia</strong> is used because there is a <strong>gold-labelled evidence</strong> provided in the dataset</p>
  </li>
</ul>

<p><strong>Model</strong></p>

<ul>
  <li>
    <p>tested on GPT2 and T5, hence overall pipeline looks like:</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008142202615.png" alt="image-20221008142202615" style="zoom: 67%;" /></p>
  </li>
  <li>
    <p>they used two approaches in total to add control</p>

    <ul>
      <li>added the <strong>special control code</strong> during training:
        <ul>
          <li><em>objective voice</em>: estimated as a binary variable whether if first person pronoun is included</li>
          <li><em>lexical precision</em>: want most of the words in the response to be contained somewhere in the evidence (drawback is semantic similarity missing). This is then mapped to <code class="language-plaintext highlighter-rouge">&lt;high-prec&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;med-prec&gt;</code>, and <code class="language-plaintext highlighter-rouge">&lt;low-prec&gt;</code></li>
          <li><em>entailment</em>: a binary variable to encourage response semantically entailed by the evidence. During training those are scored by a SOTA natural language inference model to estimate its entailment. Always <code class="language-plaintext highlighter-rouge">&lt;entailed&gt;</code> during inference</li>
        </ul>
      </li>
      <li><strong>resampling</strong>: sample for $d$ times until a satisfactory response is found</li>
    </ul>
  </li>
  <li>
    <p>after training, performance is evaluated by</p>

    <ul>
      <li>
        <p><strong>automatic metric scoring</strong> the responses on <em>objective voice</em>, <em>lexical precision</em>, <em>entailment</em>, and <em>BLEU</em> score compared to the original gold response</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008143707179.png" alt="image-20221008143707179" style="zoom:67%;" /></p>

        <p>where ablation study is also here. However, since those metrics are all self-designed, human eval is needed.</p>
      </li>
      <li>
        <p><strong>human evaluation:</strong> subsample examples from generated response from the previous experiment and ask MT to score:</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Table 4</th>
              <th style="text-align: center">Table 5</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008143835131.png" alt="image-20221008143835131" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008143850486.png" alt="image-20221008143850486" /></td>
            </tr>
          </tbody>
        </table>

        <p>where it seems that there is <strong>better faithfulness (to evidence) and objectivity</strong></p>

        <ul>
          <li>
            <p>those scores are also found to be highly correlated to the automatic eval scores</p>
          </li>
          <li>
            <p>notice that the gold-evidence is itself relevant to the conversation. So if I just copy-paste, or just summarize the evidence, won’t that give me near perfect score on all three dimensions + probably also highly relevant and high fluency (since the evidence is a fluent text)? Trade off between abstractive and extractive summarization</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="language-models-that-seek-for-knowledge-modular-search--generation-for-dialogue-and-prompt-completion">Language Models that Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion</h2>

<blockquote>
  <p><strong>Aim</strong>: An e2e model that includes capability of <strong>Search-Engine, Knowledge Extraction, and Response Generation</strong> all into one single model, while treating them as separate modular functionality. The final aim is again to be an open-domain knowledge grounded conversational agent.</p>

  <ul>
    <li>achieved by using appending <mark>special tokens in the encoder</mark> (or decoder) to indicate which module is being invoked</li>
    <li>done study on treating each module as a separate model as well, but only found marginal improvement while model size becomes 3x big</li>
  </ul>
</blockquote>

<p><strong>Setup</strong>: the overall pipeline would look like:</p>

<ol>
  <li>
    <p><mark>search module</mark>: input dialog context, <strong>generate a relevant search query</strong> for internet search engine (Bing)</p>
  </li>
  <li>
    <p><mark>knowledge module</mark>: input the dialog context + returned documents (intersecting with Common Crawl and take top 5) and <strong>generate their most relevant portion</strong> to the context (i.e. extracting the useful portion)</p>

    <ul>
      <li>
        <p>for GPT based backbone, just append</p>
      </li>
      <li>
        <p>for T5 based backbone, do a <em>fusion-in-decoder</em> which is essentially processing Question + Each Passage in parallel by an encoder, then concatenate the hidden encoder states, and feed into decoder (the model thus performs evidence fusion in the decoder only, and we refer to it as Fusion-in-Decoder)</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008145447935.png" alt="image-20221008145447935" style="zoom:80%;" /></p>

        <p>note that this style was also used a lot in Multi-Modal (visual-semantic grounding) work</p>
      </li>
    </ul>
  </li>
  <li>
    <p><mark>response module</mark>: input context and the extracted knowledge, and <strong>generate a response</strong>.</p>
  </li>
</ol>

<p><strong>Models</strong>:</p>

<ul>
  <li>in addition to GPT and T5, they additionally trained an encoder-decoder model from scratch, and called it <strong>SeeKeR</strong></li>
  <li>specifically, it is pretrained on Reddit as well as LM tasks used in RoBERTa and CC100en</li>
</ul>

<p><strong>Training</strong>: two trainable tasks are proposed:</p>

<ul>
  <li>
    <p>Tasks for dialog</p>

    <ul>
      <li><strong>search module task</strong>: using Wizard of Internet which has relevant search queries as labels, train supervised</li>
      <li><strong>knowledge module tasks</strong>: need to extract knowledge from documents. Therefore, knowledge grounded dialog datasets with gold knowledge annotations are used, as well as QA datasets.</li>
      <li><strong>response module tasks</strong>: can reuse much dataset in the knowledge task by using context + gold knowledge response (and their special tokens) to generate the gold label response</li>
    </ul>
  </li>
  <li>
    <p>Tasks for LM: improve language ability for each component, hence they are based on Common Crawl and is large in size. Also, this will be <em>directly training for the prompt completion task</em>, which is essentially LM</p>

    <ul>
      <li><strong>search module tasks</strong>: predict document titles from document</li>
      <li><strong>knowledge module task</strong>: constructed a dataset where document contain the retrieved sentence in addition to the document, and the task is to get the retrieved sentence (i.e. extract only the portion that is relevant to the question)</li>
      <li><strong>response module task</strong>: input context plus the knowledge sentence and target is next sentence, using the same dset as above</li>
    </ul>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008150843877.png" alt="image-20221008150843877" /></p>
  </li>
</ul>

<p><strong>Evaluation</strong></p>

<ul>
  <li>
    <p>compare against models such as BlenderBot 1,2, etc</p>
  </li>
  <li>
    <p><strong>Automatic Evaluation</strong>: can be done using Knowledge F1 (overlap of the dialog response with annotated gold knowledge)</p>

    <ul>
      <li>seems that only with gold knowledge response the model is working</li>
      <li><mark>TODO</mark> knowledge F1 ignores the semantic, and a lot of discrepancy with the human evaluation?</li>
    </ul>
  </li>
  <li>
    <p><strong>Human Evaluation</strong>: since this is e2e, it can converse with MT. Then for each turn in a conversation, they are asked to score several attributes from how knowledgeable it is to its engagingness.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Automatic</th>
          <th style="text-align: center">Human</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008151133688.png" alt="image-20221008151133688" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008151125898.png" alt="image-20221008151125898" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>Ablation Study</strong>:</p>

    <ul>
      <li>
        <p>done on the pretraining objectives with those additional dialog datasets</p>
      </li>
      <li>
        <p>testing if the additional LM task is helpful or not</p>

        <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221008151408879.png" alt="image-20221008151408879" /></p>

        <p><mark>TODO</mark> it seems that with LM it is better, so are the modules compared the one with LM or the “standard SeeKeR”?</p>
      </li>
      <li>
        <p>tested if separating each task to a different module would help. It does marginally but 3x the size</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Prompt Completion</strong>: in addition to conversing, it can perform prompt (part of a factual statement) and complete it with factual information</p>

    <ul>
      <li>used topical prompts, ranges from Prime Minister of Haiti to the Rio Carnival</li>
      <li>prompts look like <code class="language-plaintext highlighter-rouge">In recent developments we have learned the following about &lt;TOPIC&gt;</code> and then ask the model to continue</li>
      <li>evaluation is done by human as well</li>
    </ul>
  </li>
  <li>
    <p><strong>Effect of Multi-Task Training</strong></p>

    <ul>
      <li>Prompt Completion: a fully multi-tasked SeeKeR model performs very well, superior to all our GPT2-based SeeKeR models on every metric</li>
      <li>Open-Domain Dialog: The model performs comparably, if not better, in all automated metrics on the task. In human evaluations, results suffer compared to the dialogue fine-tuned only model, with most metrics being lower</li>
    </ul>
  </li>
</ul>

<h1 id="mixed-social-and-task-oriented-dialogue-systems">Mixed Social and Task-Oriented Dialogue Systems</h1>

<p>There is a goal, such as donation, but to be <strong>effective social strategies are required</strong> (e.g. to persuade, might need personal stories)</p>

<h2 id="inspired-toward-sociable-recommendation-dialog-systems">INSPIRED: Toward Sociable Recommendation Dialog Systems</h2>

<blockquote>
  <p><strong>Aim</strong>: Lack of dataset annotated with sociable strategies, but want to validate <strong>whether sociable recommendation strategies are effective</strong> for making a successful recommendation (which is task-oriented)</p>

  <ul>
    <li>design an annotation scheme related to recommendation strategies based on social science theories</li>
    <li>analyze and show that strategies such as sharing personal opinions or communicating with encouragement more frequently lead to successful recommendations</li>
  </ul>
</blockquote>

<p><strong>Dataset</strong>: the <code class="language-plaintext highlighter-rouge">Inspired</code> dataset</p>

<ul>
  <li>since we are doing recommendation, they <strong>curate a database with movies</strong> and make sure they include movie trailers and metadata information</li>
  <li>then a dataset is constructed by MT to
    <ul>
      <li>as a <strong>recommender</strong>: gather preference information and make recommendation</li>
      <li>as a <strong>seeker</strong>: looks of recommendation, and gets to watch the trailer at the end (used as an indicator of task success)</li>
    </ul>
  </li>
  <li>additional collection details
    <ul>
      <li>first fill out personality traits</li>
      <li>perform the conversation</li>
      <li>perform a post-task survey of demographic questions</li>
      <li>Seeker asked to rate the recommendation and get a chance to skip or watch the trailer</li>
    </ul>
  </li>
  <li><strong>strategy annotation</strong>
    <ul>
      <li>divide the recommendation strategy into two categories
        <ul>
          <li><strong>sociable strategies:</strong> eight strategies related to recommendation task to build rapport with seeker
            <ul>
              <li>e.g. personal opinion, personal experience, similarity, encouragement, etc.</li>
              <li><mark>what they want to test</mark>, if this is effective in task-oriented setting</li>
            </ul>
          </li>
          <li><strong>preference elicitation</strong>: to know the seeker’s taste directly
            <ul>
              <li>e.g. experience inquiry, opinion inquiry</li>
            </ul>
          </li>
          <li>(non-strategy: other utterances such as greeting)</li>
        </ul>
      </li>
      <li>first ask expert to label, then use those to evaluate MT’s labels and get <strong>consistent ones based on Kappa agreement</strong></li>
    </ul>
  </li>
  <li><strong>recommendation success annotation</strong>: success if defined if seekers finished watching a substantial portion (50%) of the recommended movie trailer and rate the trailer with a high score</li>
</ul>

<p><strong>Results</strong></p>

<ul>
  <li>
    <p>found that <mark>social strategies does correlate to the probability of successful recommendation</mark></p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014201122513.png" alt="image-20221014201122513" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>examine if the <strong>quality of the movie matters more than recommendation</strong></p>

    <ul>
      <li><mark>TODO</mark>: “<em>adding movie attributes</em> such as genre, recent release data have an impact on successful recommendation” how is this related to the question? I imagine something like measuring the correlation between quality of the movie v.s. successful recommendation and failed ones.</li>
      <li>found that 96% of recommended movies are covered by the top five genres</li>
    </ul>
  </li>
</ul>

<p><strong>Modeling</strong>: recommendation dialog system</p>

<ul>
  <li>
    <p>evaluate and show that using the strategies in this <code class="language-plaintext highlighter-rouge">Inspired </code> dataset can create a <strong>better recommendation system</strong></p>
  </li>
  <li>
    <p><strong>baseline</strong> dialog model uses two separate pretrained language models to learn the recommender and the seeker <strong>separately</strong></p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014201740089.png" alt="image-20221014201740089" /></p>

    <p>additionally, key terms such as movie names and actor names are delexicalized to terms such as  <code class="language-plaintext highlighter-rouge">[Movie_Title_0]</code> for later replacement</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014201838409.png" alt="image-20221014201838409" /></p>
  </li>
  <li>
    <p><strong>strategy-incorporated model</strong>: generate both the sentences but also strategies</p>

    <ul>
      <li>The model first generates five candidate sentences. Then, it randomly selects a generated candidate that either contains “encouragement” strategy or has the greatest sentence length</li>
      <li>so the model is <em>only</em> either doing encouragement or any other DA but has is the longest = i.e. <strong>prioritize encouragement, then longest</strong></li>
    </ul>
  </li>
  <li>
    <p>use human evaluation to see which system is better (or can’t tell which one is better) in terms of fluency, naturalness, persuasion …</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014202221999.png" alt="image-20221014202221999" /></p>

    <p><mark>TODO</mark> why, if you only prioritize encouragement, you can better fluency, consistency, naturalness? How much is this due to selecting long sentences?</p>
  </li>
</ul>

<h2 id="effects-of-persuasive-dialogs-testing-bot-identities-and-inquiry-strategies">Effects of Persuasive Dialogs: Testing Bot Identities and Inquiry Strategies</h2>

<blockquote>
  <p><strong>Aim</strong>: investigate how <strong>identities</strong> (if you know the bot is actually a bot) and <strong>inquiry strategies</strong> (ask personal related questions v.s. task related questions) influence the conversation’s effectiveness. Specifically, it is measured by the performance on <code class="language-plaintext highlighter-rouge">Persuasion4Good</code> dataset.</p>
</blockquote>

<p><strong>Hypothesis</strong></p>

<ol>
  <li>Hypothesis 1: Both identities (whether if you think it is chatbot or human) yield <strong>equivalent persuasive and interpersonal outcomes</strong>.</li>
  <li>Hypothesis 2: <strong>Personal inquires will yield greater persuasive</strong> and interpersonal outcomes than non-personal inquiries.</li>
  <li>Hypothesis 3: There is an interaction effect between <strong>chatbot identity and persuasive inquiry type</strong> on persuasive and interpersonal outcomes.
    <ul>
      <li>We speculate that if the chatbot tries to interact in a personal and human-like way (e.g., by asking personal questions), people may feel uncomfortable, which can subsequently degrade the interpersonal perceptions of the partner as well as their persuasiveness</li>
      <li>e.g. if chatbot perceived as human + doing personal inquiry makes it more persuasive</li>
    </ul>
  </li>
</ol>

<p><strong>Setup</strong></p>

<ul>
  <li>use <code class="language-plaintext highlighter-rouge">Persuasion4Good</code> dataset</li>
  <li>categorized the 10 persuasion strategies into two groups
    <ul>
      <li><strong>persuasive appeals:</strong> do persuasion such as <em>emotional appeal</em></li>
      <li><strong>persuasive inquires:</strong> ask questions to facilitate persuasion. This is further split into
        <ul>
          <li>Non-personal Inquiry refers to relevant questions without asking personal information. It include two sub-categories: 1) source-related inquiry that asks if the persuadee is aware of the organization, and 2) task-related inquiry that asks the persuadee’s opinion and experience related to the donation task.</li>
          <li>Personal Inquiry asks about persuadee’s personal information relevant to donation for charity but not directly on the task, such as “Do you have kids?”</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Dialog System</strong></p>

<ul>
  <li>
    <p>use <strong>agenda-based dialog system</strong>, meaning that the flow/intent of what to say is more or less predefined</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014203710682.png" alt="image-20221014203710682" style="zoom:80%;" /></p>
  </li>
  <li>
    <p>the overall model thus contains three main components</p>

    <ul>
      <li><strong>NLU</strong>: input user utterance and output user dialog act
        <ul>
          <li>a hard task and hence enhanced the input with sentiment score, context information output from CNN and pretrained character embedding, got around 62%</li>
          <li><mark>TODO</mark> uses a regular expression and pre-defined rules to reach 84.1%. What happened here?</li>
        </ul>
      </li>
      <li><strong>Dialogue Manager</strong>: outputs the next system dialog act, but follows the agenda shown above
        <ul>
          <li>the first green block is the control experiment: want to measure if having each of the four strategy would affect overall persuasion = <mark>if personal inquires yield more persuasiveness</mark></li>
          <li>then, the system proceeds to persuasive appeal, where a <strong>fixed strategy order</strong>  is used</li>
          <li>finally, <mark>when user dialog act is `agree-donation`</mark> (predicted by NLU), enter <code class="language-plaintext highlighter-rouge">Agree Donation</code> stage and always present the three task in the order: ask donation amount, propose more donation, …</li>
          <li>in addition, a factual QA component is there in case if user asked fact related questions related to the charity</li>
        </ul>
      </li>
      <li><strong>Natural Language Generation</strong>: there are three ways to generating: a) template base, b) retrieval-based from the training dataset and c) generation
        <ul>
          <li><mark>template-based for Persuasive Inquiry:</mark> we want to study the effects of different persuasive inquiries <strong>instead of the impact of the surface-form</strong>; therefore, the surface-forms of the persuasive inquiries should be a controlled variable that stays the same across experiments.</li>
          <li><strong>retrieval-based persuasive appeal</strong>: want to also be templated based but now you have a large context, hence that doesn’t work anymore</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Evaluation</strong></p>

<ul>
  <li>designed 2x4 cases
    <ul>
      <li>2 cases of having bot being labelled as <code class="language-plaintext highlighter-rouge">Jessie</code> or <code class="language-plaintext highlighter-rouge">Jessie (bot)</code></li>
      <li>4 cases of each of the persuasive inquiry strategy: personal + non-personal inquiry vs. personal inquiry vs. nonpersonal inquiry vs. no inquiries</li>
    </ul>
  </li>
  <li>
    <p>first found not much difference in which strategy used overall</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014204957608.png" alt="image-20221014204957608" /></p>

    <p>but then they found that participants are <strong>perceiving bots are humans or vice versa despite the given label</strong></p>
  </li>
  <li>
    <p>found that if bots are <mark>perceived as human</mark>, then the above it matters</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014205113538.png" alt="image-20221014205113538" /></p>

    <p>this shows that being a human is more persuasive hence:</p>

    <ul>
      <li><mark>nullifies Hypothesis 1</mark>, which claims being perceived as human or machine does not matter</li>
      <li><mark>supports Hypothesis 2</mark>, that using personal inquires help</li>
    </ul>
  </li>
  <li>
    <p>finally, we can check cases when the person believed in the given labelled identity</p>

    <p><img src="/lectures/images/2022-10-19-COMS6998_Dialog_Systems/image-20221014205642372.png" alt="image-20221014205642372" /></p>

    <p>where this means that if bot’s identity does not matter, then in each pair of blue and green bar it should be same height:</p>

    <ul>
      <li>when participants talked to “Jessie (bot)” but perceiving it as a human, they were also more likely to donate than those in the same condition but perceiving it as a bot. In contrast, when participants talked to “Jessie” but suspected it was a bot, they were <mark>least likely to make a donation, which supported the UVM in Hypothesis 3</mark></li>
      <li>also, result showed that “Jessie (bot)” would decrease the donation probability ($\beta$=−0.52,  $p$&lt;0.05). So the <strong>bot’s identity matters in the persuasion outcome, which again disproves Hypothesis 1</strong></li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="2022@Columbia" /><summary type="html"><![CDATA[6998: Conversational AI]]></summary></entry></feed>