<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>STFCS229 Machine Learning | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="STFCS229 Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Machine Learning - MIT6.036" />
<meta property="og:description" content="Machine Learning - MIT6.036" />
<link rel="canonical" href="/lectures/2021@columbia/STFCS229_Machine_Learning.html/" />
<meta property="og:url" content="/lectures/2021@columbia/STFCS229_Machine_Learning.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-18T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="STFCS229 Machine Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-01-18T00:00:00-05:00","datePublished":"2022-01-18T00:00:00-05:00","description":"Machine Learning - MIT6.036","headline":"STFCS229 Machine Learning","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2021@columbia/STFCS229_Machine_Learning.html/"},"url":"/lectures/2021@columbia/STFCS229_Machine_Learning.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/learning">Blog</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
  <script>
    MathJax = {
      // 
      loader: {
        load: ['[tex]/ams', '[tex]/textmacros', '[tex]/boldsymbol']
      },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']}
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">STFCS229 Machine Learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-01-18T00:00:00-05:00" itemprop="datePublished">
        Jan 18, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Machine Learning - MIT6.036</p>

<ul>
  <li>https://www.youtube.com/watch?v=0xaLT4Svzgo&amp;list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV
    <ul>
      <li>full course time</li>
    </ul>
  </li>
  <li>http://people.csail.mit.edu/tbroderick//ml.html
    <ul>
      <li>Lecture PDFs ^</li>
    </ul>
  </li>
</ul>

<p>Stanford CS229</p>

<ul>
  <li>http://cs229.stanford.edu/syllabus-autumn2018.html</li>
</ul>

<h1 id="introduction-and-fundamentals">Introduction and Fundamentals</h1>

<p>Machine Learning is about, on a high level, ==making decisions or predictions based on data==. So the conceptual basis of learning from data is the problem of <strong>induction</strong>.</p>

<ul>
  <li>Great examples are <strong>face detection</strong> and <strong>speech recognition</strong> and many kinds of language-processing tasks.</li>
</ul>

<p>Some general steps that you will have to take involve:</p>

<ol>
  <li>acquire and organize data</li>
  <li>design a space of possible solutions</li>
  <li>select a learning algorithm <strong>and</strong> its parameters</li>
  <li>apply the algorithm to the data</li>
  <li>validate the resulting solution to decide whether it’s good enough to use, etc.</li>
</ol>

<p>The goal is usually to find</p>

<ul>
  <li><strong>generalization:</strong> How can we predict results of a situation or experiment that we have <em>never encountered before in our data set</em>?</li>
</ul>

<p>In a more concrete manner, we can ==describe== ==problems== and their ==solutions== ==using six characteristics==, three of which characterize the problem and three of which characterize the solution:</p>

<ol>
  <li><strong>Problem class:</strong> What is the nature of the training data and what kinds of queries will be made at testing time?</li>
  <li><strong>Assumptions:</strong> What do we know about the source of the data or the form of the solution?</li>
  <li><strong>Evaluation criteria:</strong> What is the goal of the prediction or estimation system? How will the answers to individual queries be evaluated? How will the overall performance of the system be measured?</li>
  <li><strong>Model type:</strong> Will an intermediate model be made? What aspects of the data will be modeled? How will the model be used to make predictions?</li>
  <li><strong>Model class:</strong> What particular parametric class of models will be used? What criterion will we use to pick a particular model from the model class?</li>
  <li><strong>Algorithm:</strong> What computational process will be used to fit the model to the data and/or to make predictions?</li>
</ol>

<h2 id="problem-class">Problem Class</h2>

<p>There are many different <em>problem classes</em> in machine learning. They vary according to <strong>what kind of data is provided</strong> and what kind of <strong>conclusions</strong> are to be drawn from it. Here we discuss the most typical:</p>

<ul>
  <li><a href="#Supervised_Learning">Supervised Learning</a></li>
  <li><a href="#Unsupervised_Learning">Unsupervised Learning</a></li>
  <li><a href="#Reinforcement_Learning">Reinforcement Learning</a></li>
  <li><a href="#Sequence_Learning">Sequence Learning</a></li>
</ul>

<h3 id="supervised-learning">Supervised Learning</h3>

<blockquote>
  <p><strong>Supervised Learning</strong></p>

  <ul>
    <li>
      <p>The learning system is given ==both== inputs and (the correct) outputs. The goal is then to predict an output for a future, unknown input.</p>

      <p>In other words, the <strong>training dataset $D_n$</strong> is the form of pairs:</p>

\[D_n = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)}) \}\]

      <p>where:</p>

      <ul>
        <li>$x^{(i)}$ are ==vectors== in $d$ dimensions.</li>
        <li>$y^{(i)}$ are ==elements== of a discrete set of values.</li>
        <li>$x^{(i)}$ represents an <strong>input/object to be classified</strong></li>
        <li>$y^{(i)}$ represents an <strong>output/target values</strong></li>
      </ul>
    </li>
    <li>
      <p>The goal in is ultimately, given a new input value $x^{(n+1)}$, to predict the value of $y^{(n+1)}$.</p>
    </li>
  </ul>
</blockquote>

<p>Additionally, we divide up supervised learning based on whether:</p>

<ul>
  <li>the ==outputs== are drawn from a small <strong>finite</strong> set (<strong>classification</strong>)
    <ul>
      <li>A classification problem is <em>binary</em> or <em>two-class</em> if $y^{(i)}$ is drawn from a set of two possible values, $y^{(i)} \in {+1,-1}$ .Otherwise, it is called <em>multi-class</em>.</li>
    </ul>
  </li>
  <li>or the ==outputs== are a <strong>large</strong> finite or <strong>continuous</strong> set (<strong>regression</strong>).
    <ul>
      <li>Regression is similar, except that $y^{(i)} \in \mathbb{R}^k$</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>Note</em>:</p>

<ul>
  <li>
    <p>In reality, we might be dealing with inputs such as an actual song. So to abstract those entities to vectors, we use something called a <strong>feature mapping</strong>:</p>

\[x \mapsto \varphi(x) \in \mathbb{R}^d\]

    <p>where:</p>

    <ul>
      <li>$x$ is your actual entity, such as the actual song</li>
      <li>$\varphi(x)$ is a function that extracts information/feature from the data to a vector</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="unsupervised-learning">Unsupervised Learning</h3>

<blockquote>
  <p><strong>Unsupervised Learning</strong></p>

  <ul>
    <li>Here, one is given a data set and generally expected to <strong>find some patterns</strong> or structure inherent in it.</li>
  </ul>
</blockquote>

<p>Common examples include:</p>

<ul>
  <li><strong>Density estimation</strong>: Given samples $x^{(1)}, …, x^{(n)} \in \mathbb{R}^D$ drawn <u>IDD</u> from some distribution $\text{Pr}(X)$, and predict the probability $\text{Pr}(x^{(n+1)})$ of an element drawn from the same distribution.
    <ul>
      <li><u>IID</u> stands for <em>independent and identically distributed</em>, which means that the elements in the set are related in the sense that they all come from the <strong>same underlying probability distribution</strong>, but not in any other ways.</li>
    </ul>
  </li>
  <li><strong>Dimensionality Reduction</strong>: Given samples $x^{(1)}, …, x^{(n)} \in \mathbb{R}^D$, the problem is to re-represent them as points in a $d$-dimensional space, where $d&lt;D$. The goal is typically to <strong>retain information</strong> in the data set that will, e.g., allow elements of one class to be discriminated from another.</li>
</ul>

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<blockquote>
  <p><strong>Reinforcement Learning</strong></p>

  <ul>
    <li>The goal is to learn a mapping from input values $x$ to output values $y$, but without a direct supervision signal to specify which output values $y$ are best for a particular input.</li>
    <li>In a sense there is no training set specified a priori.</li>
    <li>Example:
      <ul>
        <li>fantastic can playing games,  e.g. AlphaGo</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>This reinforcement is basically similar to the <em>positive/negative reinforcement in psychology</em>, such that the general setting is:</p>

<ol>
  <li>The agent observes the current state $x^{(0)}$</li>
  <li>The agent selects an action $y^{(0)}$</li>
  <li>The agent receives a <strong>reward</strong>, $r^{(0)}$, which is based on $x^{(0)}$ and possibly $y^{(0)}$</li>
  <li>The environment transitions <em>probabilistically</em> to a new state, $x^{(1)}$, with a distribution that depends only on $x^{(0)}, y^{(0)}$
    <ul>
      <li>i.e. the set of possible states after your action is a distribution</li>
    </ul>
  </li>
  <li>The agent observes the current state $x^{(1)}$</li>
  <li>repeats</li>
</ol>

<p>The goal is to find a policy $\pi$, mapping $x$ to $y$, (that is, states to actions) such that ==some long-term sum== or ==average of rewards== $r$ is ==maximized==.</p>

<h3 id="sequence-learning">Sequence Learning</h3>

<blockquote>
  <p><strong>Sequence Learning</strong></p>

  <ul>
    <li>In sequence learning, the goal is to learn a mapping from <em>input sequences</em> $x_0,…,x_n$ to <em>output sequences</em> $y_1,…,y_m$.</li>
    <li>The mapping is typically represented as a <strong>state machine</strong>:
      <ul>
        <li>with one function $f$ used to compute the ==next hidden internal state== given the input</li>
        <li>and another function $g$ used to compute the ==output given the current hidden state==.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="ml-example">ML Example</h2>

<p>Consider an example of predicting whether if a <strong>newborn baby in the hospital will have a seizure</strong>.</p>

<p>We are provided <strong>beforehand</strong> with:</p>

<ul>
  <li>
    <p>$n$ training data points</p>

\[\mathcal{D}_n = \{ (x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)}) \}\]

    <ul>
      <li>each data point, $x^{(i)}, i \in {1,…,n}$ is a ==feature vector==, so that $x^{(i)} \in \mathbb{R}^d$
        <ul>
          <li>for example, $x^{(i)}_1$ could represent the amount of oxygen the baby is breathing, and $x^{(i)}_2$ could represent the amount of movement the baby has</li>
        </ul>
      </li>
      <li>each data point, $x^{(i)}$ has a corresponding ==label==, $y^{(i)} \in {-1,+1}$, so that
        <ul>
          <li>$-1$ means newborn baby $i$ did not have a seizure</li>
          <li>and $+1$ means newborn baby $i$ did have a seizure</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>So that we have the following:</p>

<ul>
  <li>in this simple example, $d=2$</li>
</ul>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701231519528.png" alt="image-20210701231519528" /></p>

<p>==Questions to Answer==</p>

<ol>
  <li>How can we label those points?</li>
  <li>What are <strong>good</strong> ways to label them?</li>
</ol>

<p><strong>Possible Solutions</strong></p>

<ol>
  <li>
    <p>Labeling ==Hypothesis==: we need a “function” $h$ that does:</p>

\[h: \mathbb{R}^d \mapsto \{-1, +1\}\]
  </li>
  <li>
    <p>Linear Classifiers is a good <strong>set</strong> of hypothesis to use.</p>

    <ul>
      <li>e.g. the hypothesis class $\mathcal{H}: \text{set of }h \in \mathcal{H}$ such that:
        <ul>
          <li>labels $+1$ on one side of a <strong>line</strong>, and $-1$ on the other side of the <strong>line</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="linear-classifiers">Linear Classifiers</h2>

<p>For the same given (training) data:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701231519528.png" alt="image-20210701231519528" /></p>

<p>Consider the hypothesis class $\mathcal{H}= \text{set of }h \in \mathcal{H}$ such that:</p>

<ul>
  <li>labels $+1$ on one side of a <strong>line</strong>, and $-1$ on the other side of the <strong>line</strong></li>
  <li>below are <em>some</em> hypothesis in this class $\mathcal{H}$</li>
</ul>

<p>| <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701233000013.png" alt="image-20210701233000013" style="zoom: 67%;" /> | <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701233020906.png" alt="image-20210701233020906" style="zoom: 67%;" /> |
| ———————————————————— | ———————————————————— |</p>

<blockquote>
  <p><strong>Review</strong></p>

  <ul>
    <li>
      <p>Recall that if we have the two vectors $\theta, x$:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701233553950.png" alt="image-20210701233553950" style="zoom: 67%;" /></p>

      <p>then the dot product:</p>

\[\frac{x \cdot \theta}{||\theta||} = \frac{x^T \theta }{||\theta||}\]

      <p>represents the <strong>projection of $x$ onto $\theta$</strong>.</p>
    </li>
    <li>
      <p>Therefore, an ==important point== is that:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701234326496.png" alt="image-20210701234326496" style="zoom: 67%;" /></p>

      <p>where the red line (set of $x$) can be represented by:</p>

\[x : \frac{\theta \cdot x}{||\theta||} = 0\]

      <p>and that:</p>

\[\begin{align*}
x : \frac{\theta \cdot x}{||\theta||} &amp;= a \\
x : \frac{\theta \cdot x}{||\theta||} &amp;= -b
\end{align*}\]

      <p>defines the following additional two lines:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210701234654201.png" alt="image-20210701234654201" style="zoom: 67%;" /></p>

      <p>where the advantage of defining lines here is that:</p>

      <ul>
        <li>all we used are <strong>data points</strong></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Therefore, we can say that for we can describe ==any line in 2 dimension== as:</p>

\[\begin{align*}
\frac{\theta^T x}{||\theta||} &amp;= -b \\
\theta^T x + b ||\theta|| &amp;= 0\\
\theta^T x + \theta_0 &amp;= 0
\end{align*}\]

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210703001004237.png" alt="image-20210703001004237" style="zoom: 67%;" /></p>

<p>So the linear classifier can just look at the ==sign of $x$ relative to the line==</p>

\[h(x) = h(x; \theta, \theta_0)=\text{sign}(\theta^T x + \theta_0) = \begin{cases}
+1, &amp; \text{if } \theta^T x + \theta_0 &gt; 0\\
-1, &amp; \text{if } \theta^T x + \theta_0 \le 0\\
\end{cases}\]

<p>where:</p>

<ul>
  <li>the equality $\le$ is arbitrary. It could be applied on $&gt;$ alternatively.</li>
  <li>$h(x; \theta, \theta_0)$ means $h$ is a function of input $x$, with <strong>parameter</strong> $\theta, \theta_0$ that has nothing to do with input</li>
</ul>

<p>Now, we can define the <strong>linear classifier class</strong> to be:</p>

\[\mathcal{H} = \{h(x;\theta, \theta_0), \forall \theta, \theta_0\}\]

<p>where again the input is $x$</p>

<hr />

<p><em>Note</em>:</p>

<p>At this point, all that we have to do is:</p>

<ol>
  <li>be able to <strong>evaluate</strong> how good a <em>hypothesis $h$</em> is</li>
  <li>be able to <strong>find</strong> the best $h$ in the class</li>
</ol>

<hr />

<h2 id="evaluating-a-classifier">Evaluating a Classifier</h2>

<hr />

<p><em>Note:</em></p>

<ul>
  <li>This discussion applies to the general case of <em>any classifier</em></li>
</ul>

<hr />

<p>To be able to determine if a hypothesis is good or not, we can look at the ==loss function==:</p>

\[L(g,a)\]

<p>where:</p>

<ul>
  <li>$g$ means the <strong>our guess from hypothesis</strong></li>
  <li>$a$ means the <strong>actual value</strong></li>
  <li><em>note that</em> it ==does not depend on input==, since this basically just describes how bad it is ==if we guessed incorrectly==</li>
</ul>

<p><em>For Example</em>:</p>

<p><strong>Symmetric Loss</strong></p>

\[L(g,a) = \begin{cases}
0, &amp; \text{if }g =a \text{, i.e. guessed correctly}\\
1, &amp;\text{otherwise}
\end{cases}\]

<p>however, a <strong>better</strong> loss might be:</p>

\[L(g,a) = \begin{cases}
0, &amp; \text{if }g =a \text{, i.e. guessed correctly}\\
1, &amp; \text{if }g=1,a=-1\\
100, &amp;\text{if }g=-1,a=1
\end{cases}\]

<p>where:</p>

<ul>
  <li>$g=-1,a=1$ represents we <em>predicting newborn NOT having a seizure</em>, but they <em>actually had</em>. Therefore, it is detrimental and <strong>weighted</strong> more.</li>
</ul>

<p>Therefore, we can define:</p>

<blockquote>
  <p><strong>Test Error</strong></p>

  <ul>
    <li>
      <p>For $n’$ points of <strong><em>new data</em></strong>, the test error <strong><em>of a hypothesis $h$</em></strong> is:</p>

\[\mathcal{E}(h) = \frac{1}{n'} \sum_{i=n+1}^{n+n'}L(h(x^{(i)}), y^{(i)})\]

      <p>where basically:</p>

      <ul>
        <li>we are ==averaging== over the loss function, and the <strong>guess</strong> comes from the <strong>hypothesis $h$</strong></li>
      </ul>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Training Error</strong></p>

  <ul>
    <li>
      <p>For $n$ points of <strong><em>old/given data</em></strong>, the training error <strong><em>of a hypothesis $h$</em></strong> is:</p>

\[\mathcal{E}_n(h) = \frac{1}{n} \sum_{i=1}^{n}L(h(x^{(i)}), y^{(i)})\]

      <p>where basically:</p>

      <ul>
        <li>we are using existing data as input for $h$</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Then, we can say I ==prefer hypothesis $h$ over $\bar{h}$== if (for example):</p>

\[\mathcal{E}_n(h) &lt; \mathcal{E}_n(\bar{h})\]

<p>i.e. $h$ has a <strong>smaller <em>training</em> error</strong> (common) than $\bar{h}$</p>

<h2 id="picking-a-classifier">Picking a Classifier</h2>

<p>Obviously we <strong>cannot just try out every classifier</strong> and compute the $\mathcal{E}$. Therefore, we think about</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704224254015.png" alt="image-20210704224254015" style="zoom:67%;" /></p>

<p>So in general, this is what is happening:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710150310521.png" alt="image-20210710150310521" /></p>

<p>where:</p>

<ul>
  <li>the output of your algorithm is a <strong>hypothesis</strong>, which is basically a <strong>function</strong> that can give you predicted output based on some input</li>
</ul>

<p>where we can use a ==learning algorithm to spit out a <em>good</em> classifier== by:</p>

<ul>
  <li>
    <p>taking in an <strong>entire dataset</strong> $\mathcal{D_n}$</p>

    <p>e.g.</p>

    <p>| <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704224453749.png" alt="image-20210704224453749" style="zoom:50%;" /> | <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704224447096.png" alt="image-20210704224447096" style="zoom:50%;" /> |
| :———————————————————-: | :———————————————————-: |</p>
  </li>
</ul>

<p><em>For Example</em></p>

<p>Your hypothesis class is generated by your friend to be:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704225356849.png" alt="image-20210704225356849" style="zoom: 67%;" /></p>

<p>where basically you get a <em>trillion hypothesis</em>.</p>

<p>A ==simple== learning algorithm would be:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704225554171.png" alt="image-20210704225554171" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>$k$ is a <strong>hyperparameter</strong>, which does not change the logic of algorithm at all. It is just a parameter we can choose as programmer</li>
  <li>we basically found $h^{(j^*)}$ such that it has the <strong>smallest training error among $k$ classifiers</strong></li>
</ul>

<hr />

<p><em>Note:</em></p>

<ul>
  <li>notice that it means $\text{Ex_learning_alg}(\mathcal{D}_n;2)$ must output an algorithm that is at least as good as $\text{Ex_learning_alg}(\mathcal{D}_n;1)$.</li>
  <li>this means that as $k$ increases, we will get <strong>better classifiers</strong></li>
</ul>

<hr />

<p>In practice, if we took:</p>

\[L(g,a) = \begin{cases}
0, &amp; \text{if }g =a \text{, i.e. guessed correctly}\\
1, &amp;\text{otherwise}
\end{cases}\]

<p>and that we use <strong>training error</strong> to decide upon the better hypothesis.</p>

<p>Then the <strong>process</strong> of $\text{Ex_learning_alg}(\mathcal{D}_n;k)$ looks like:</p>

<ol>
  <li>
    <p>On the first run with $h^{1}$</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704231714575.png" alt="image-20210704231714575" style="zoom: 33%;" /></p>

    <p>where the top graph has:</p>

    <ul>
      <li>a dotted line indicating our hypothesis $h^{(1)}$</li>
      <li>current training error <strong>set to $1$</strong> which is the maximum</li>
      <li>$h^{(1)}$ is having a training error of $0.47$</li>
    </ul>

    <p>the bottom graph is the plot of training errors with respect to $k$-th hypothesis</p>

    <ul>
      <li>therefore $j^*=1$ currently</li>
    </ul>
  </li>
  <li>
    <p>The next proposal/hypothesis $h^{(2)}$ has a better training error:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704232139571.png" alt="image-20210704232139571" style="zoom:33%;" /></p>

    <p>now $j^*=2$</p>
  </li>
  <li>
    <p>….</p>
  </li>
  <li>
    <p>Over 413 runs, we get up to $0.01$ for error. But obviously there <em>is one that would give $\mathcal{E}(h)=0$</em></p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704232404271.png" alt="image-20210704232404271" style="zoom:33%;" /></p>
  </li>
</ol>

<p>Therefore, we kind of <strong>need a better learning algorithm</strong></p>

<h1 id="machine-learning-techniques">Machine Learning Techniques</h1>

<p>The examples in the last lecture talks about problem of <strong>classification</strong>, to be specific, a <strong>binary classification</strong>.</p>

<pre><code class="language-mermaid">classDiagram
	Supervised_Learning &lt;|-- Regression
	Supervised_Learning &lt;|-- Classification
	Unsupervised_Learning &lt;|-- _
	Classification &lt;|-- Binary_Classification
	Classification &lt;|-- Multi_Class_Classification
</code></pre>

<blockquote>
  <p><strong>Classification</strong>:</p>

  <ul>
    <li>
      <p>Learning an input mapping to a <em>discrete set</em></p>

\[\mathbb{R}^d \mapsto \{...\}\]

      <p>where:</p>

      <ul>
        <li>input is $\mathbb{R}^d$</li>
        <li>output is ${…}$</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>In specific, what we have met is the <strong>binary classification</strong>:</p>

<blockquote>
  <p><strong>Binary Classification</strong></p>

  <ul>
    <li>
      <p>Learning an input mapping to a <em>discrete set of two values</em></p>

\[\mathbb{R}^d \mapsto \{-1,+1\}\]
    </li>
    <li>
      <p>for example, we can have a <strong>linear classifier</strong> (i.e. a hyperplane) to solve such problem</p>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Multi-class Classification</strong></p>

  <ul>
    <li>
      <p>Learning an input mapping to a <em>discrete set of more than two values</em></p>

      <p>e.g.</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210707231904047.png" alt="image-20210707231904047" /></p>
    </li>
    <li>
      <p>usually in this case, a linear classifier would <strong>not</strong> work properly</p>
    </li>
  </ul>
</blockquote>

<p>In other cases, we might want get some ==continuous output==. For example, the real-time temperature in a room. Then we usually use regression</p>

<blockquote>
  <p><strong>Regression</strong></p>

  <ul>
    <li>
      <p>Learning an input mapping to <em>continuous values</em></p>

\[\mathbb{R}^d \mapsto \mathbb{R}^k\]

      <p>e.g.</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210707231927438.png" alt="image-20210707231927438" /></p>
    </li>
  </ul>
</blockquote>

<p>Lastly, <strong>unsupervised learning</strong> talks about:</p>

<blockquote>
  <p><strong>Unsupervised Learning</strong></p>

  <ul>
    <li>No labels for input. The aim is to <strong>find patterns</strong> in a given dataset.</li>
  </ul>
</blockquote>

<h2 id="general-steps-for-ml-analysis">General Steps for ML Analysis</h2>

<p>In general, you would have needed to:</p>

<ol>
  <li>
    <p>Establish a goal &amp; find data</p>

    <ul>
      <li>Example goal: diagnose whether people have heart disease based on their available information</li>
    </ul>
  </li>
  <li>
    <p>Encode data in useful form for the ML algorithm</p>

    <ul>
      <li>some sort of transformation (i.e. feature function) to the dataset</li>
    </ul>
  </li>
  <li>
    <p>Run the ML algorithm &amp; return a <strong>classifier</strong> (i.e. a function/hypothesis $h$)</p>

    <ul>
      <li>
        <p>if the problem is supervised learning</p>
      </li>
      <li>
        <p>Example algorithms include perceptron algorithm, average perceptron algorithm, etc.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Interpretation and Evaluation</p>
  </li>
</ol>

<blockquote>
  <p><strong>Feature</strong>:</p>

  <ul>
    <li>
      <p>A function that takes the <em>input/raw data</em> and <strong>converts</strong> it to some <strong>ML usable form</strong> for later processing</p>

\[x \mapsto \phi(x)\]
    </li>
    <li>
      <p>for example, one hot encoding (see section below)</p>
    </li>
  </ul>
</blockquote>

<hr />

<p><strong>Note</strong>:</p>

<ul>
  <li>sometimes, feature functions $\phi(x)$ are just <strong>referred to as features $\phi$</strong>, since it is usually the ==resulting value/vector== that we care about in ML</li>
  <li>Those feature functions does ==not== have to be reversible</li>
</ul>

<hr />

<h3 id="example">Example</h3>

<p>Consider the task of <strong>diagnose whether people have heart disease</strong> based on their available information</p>

<ol>
  <li>First, need goal &amp; data</li>
</ol>

<p>e.g.</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708000006163.png" alt="image-20210708000006163" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>there are of course more data to these four</li>
  <li>this is <strong>labelled</strong> and is <strong>binary</strong> (has heart disease?)
    <ul>
      <li>so this is a <em>binary/two-class classification</em></li>
    </ul>
  </li>
  <li>the <strong>input features</strong> are basically all the rest</li>
</ul>

<hr />

<p><strong>Note that:</strong></p>

<ul>
  <li>Even if we have the <em>input features</em> ready, we need to find some way to ==convert string (non-numeric) information== to numbers. Otherwise we cannot use our established models</li>
</ul>

<hr />

<ol>
  <li>
    <p>Encode data in a usable form</p>

    <ul>
      <li>
        <p>We can easily convert the <strong>labels=”has heart disease”</strong> to:</p>

\[\{\text{'yes'}, \text{'no'}\} \iff \{ +1,-1 \}\]
      </li>
      <li>
        <p>The data/inputs needs some work. We need to use some <strong>feature $\phi$</strong> that:</p>

\[x \mapsto \phi(x)\]

        <p>where:</p>

        <ul>
          <li>old/raw features is $x$</li>
          <li>new feature for ML is $\phi(x)$</li>
        </ul>

        <p>e.g.</p>

        <ul>
          <li>
            <p>some interesting <strong>encoding</strong> to see would be:</p>

            <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003122522.png" alt="image-20210708003122522" style="zoom:33%;" /></p>

            <p>where:</p>

            <ul>
              <li>this is also called a ==one hot encoding==</li>
            </ul>
          </li>
          <li>
            <p>and ==factored encoding== (since there are <strong>overlaps</strong>):</p>

            <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003803602.png" alt="image-20210708003803602" style="zoom: 33%;" /></p>
          </li>
        </ul>

        <p>Overall:</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">$x$, old data/feature</th>
              <th style="text-align: center">$\phi(x)$, new data/feature</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003545789.png" alt="image-20210708003545789" style="zoom: 67%;" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003627793.png" alt="image-20210708003627793" style="zoom: 67%;" /></td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>Note</strong></p>

<ul>
  <li>
    <p>In step 2, some other ideas could be:</p>

    <p>For <strong><em>categorial jobs:</em></strong></p>

    <ol>
      <li>
        <p>if we turned categorial information into some <em>unique natural numbers</em>:</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708002534142.png" alt="image-20210708002534142" style="zoom:67%;" /></p>

        <p>then we are <strong>imposing some sort of ORDERINGS</strong> to the categories. This sometimes is <em>not a good idea</em>.</p>
      </li>
      <li>
        <p>A <em>good</em> idea is to turn <strong>each category</strong> into a <strong>unique feature</strong>. For example:</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003122522.png" alt="image-20210708003122522" style="zoom: 33%;" /></p>

        <p>then the advantage is that you can ==always “separate/group”== those categories as they are always “apart”</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003414544.png" alt="image-20210708003414544" style="zoom: 67%;" /></p>
      </li>
    </ol>

    <p>For <strong>overlapping “categories”</strong>:</p>

    <ol>
      <li>
        <p>Notice that since there are overlaps, something like this would be nice:</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003803602.png" alt="image-20210708003803602" style="zoom: 33%;" /></p>

        <p>such that it <em>should</em> still be separable (think about cases when it is not):</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708004522145.png" alt="image-20210708004522145" style="zoom: 67%;" /></p>
      </li>
    </ol>

    <hr />

    <p><strong>Note</strong></p>

    <ul>
      <li>this is strictly <em>different from</em> a binary encoding, because we kind of <strong>chose some useful orderings/groupings</strong> to show those overlaps.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="encode-ordinal-data">Encode Ordinal Data</h2>

<p>In general, you would meet <strong>three kinds of data</strong> to go through your <strong>feature</strong> (function):</p>

<ol>
  <li>
    <p><strong>Numerical Data</strong>: order on data values, and differences in value are meaningful</p>

    <ul>
      <li>e.g. heart rate = 56, 67, 60, etc</li>
      <li>e.g. filtering useless ones; <em>standardization</em></li>
    </ul>
  </li>
  <li>
    <p><strong>Categorical Data</strong>: no order information</p>

    <ul>
      <li>e.g. job = doctor, nurse, pharmacist</li>
      <li>e.g. <em>one hot encoding</em></li>
    </ul>
  </li>
  <li>
    <p><strong>Ordinal Data</strong>: ==order== on data values, but ==differences not meaningful== without some processing</p>

    <ul>
      <li>
        <p>e.g. Likert Scale</p>

        <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708224857623.png" alt="image-20210708224857623" style="zoom: 80%;" /></p>

        <p>is the difference between each state <em>exactly the same</em>?</p>
      </li>
    </ul>
  </li>
</ol>

<p><em>For Example</em></p>

<p><strong>Unary/Thermometer Code</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$x$</th>
      <th style="text-align: center">$\phi(x)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708224857623.png" alt="image-20210708224857623" style="zoom: 80%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708225322102.png" alt="image-20210708225322102" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that the advantage is:</p>

<ul>
  <li><em>order information</em> still exists</li>
  <li><em>difference information</em> is “<strong>removed</strong>”</li>
</ul>

<hr />

<p><em>For Example: Filtering</em></p>

<p>Even numerical data <em>could be useless</em>. Consider</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708225731566.png" alt="image-20210708225731566" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>in this case, “Weekly # Garlic Clove Eaten” is useless</li>
  <li>maybe ==ask some experts on this==. This might be important unless you are sure.</li>
</ul>

<hr />

<p><em>For Example: Standardization</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$x$</th>
      <th style="text-align: center">$\phi(x)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708230131866.png" alt="image-20210708230131866" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708230058875.png" alt="image-20210708230058875" /></td>
    </tr>
  </tbody>
</table>

<p>where basically each data point has gone through:</p>

<blockquote>
  <p><strong>Standardization</strong></p>

  <ul>
    <li>
      <p>Given some data point $x^{(d)} \in \mathcal{D}_n$:</p>

\[\phi_d^{(k)} = \frac{x_d^{(k)}-\text{mean}_d}{\text{stddev}_d}\]

      <p>where:</p>

      <ul>
        <li>doing $\frac{1}{\text{stddev}_d}$ basically <em>zooms in the data</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>where the advantage of this is that:</p>

<ul>
  <li>such classifier would be ==independent of the scale of axis== that you chose to plot at</li>
  <li>also, it would sometimes be <strong>easier to interpret the result/classifier</strong></li>
</ul>

<hr />

<p>Therefore, we could ==further== encode the example in the previous lecture to be:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$x$</th>
      <th style="text-align: center">$\phi(x)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708003545789.png" alt="image-20210708003545789" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708231000696.png" alt="image-20210708231000696" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>“resting heart rate” and “family income (USD)” has now been <strong>standardized</strong></li>
</ul>

<h2 id="nonlinear-boundaries">Nonlinear Boundaries</h2>

<hr />

<p><strong><em>Review</em></strong></p>

<ul>
  <li>
    <p>Recall that a plane can be defined ==a point $\vec{r}_0$ in the plane AND its normal vector $\vec{n}$==:</p>

\[\begin{align*}
\vec{n} \cdot (\vec{r}-\vec{r}_0) &amp;= 0 \\
\vec{n} \cdot \vec{r} &amp;= \vec{n} \cdot \vec{r}_0
\end{align*}\]

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708233908934.png" alt="image-20210708233908934" style="zoom: 67%;" /></p>

    <p>where:</p>

    <ul>
      <li>$\vec{r}$ basically is “input”</li>
      <li>$\vec{n}\cdot \vec{r}_0$ basically is a <strong>constant</strong></li>
    </ul>
  </li>
</ul>

<hr />

<p>Therefore, consider classifying:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708234245129.png" alt="image-20210708234245129" style="zoom: 67%;" /></p>

<p>where now your “hyperplane classifier” is:</p>

<ul>
  <li>$\theta^T x + \theta_0 = 0$, $x \in \mathbb{R}^2$</li>
</ul>

<p>By adding a <strong>new dimension $z$</strong>, and then you have</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708233411794.png" alt="image-20210708233411794" style="zoom:67%;" /></p>

<p>now your “hyperplane classifier” becomes:</p>

<ul>
  <li>$\theta^T x + \theta_0 = z$, $x \in \mathbb{R}^2$</li>
</ul>

<p>An advantage of this idea is to classify:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">dataset</th>
      <th style="text-align: center">hyperplane/classifier: $f(x)=z,x\in \mathbb{R}^2$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708235143142.png" alt="image-20210708235143142" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210708235153965.png" alt="image-20210708235153965" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<h1 id="linear-regression">Linear Regression</h1>

<p>Consider the following data:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710150713957.png" alt="image-20210710150713957" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>$x^{(i)}_1$is the living area of the $i$-th house in the training set, and $x^{(i)}_2$ is its number of bedrooms.</li>
  <li>$y$ is a <em>continuous</em> (assuming continuous price) output</li>
  <li>since output/labels are given, this is <strong>supervised learning</strong></li>
</ul>

<p>Then, an example would be to use <strong>linear regression</strong></p>

\[h(x;\theta)=h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2\]

<p>Alternatively, letting $x_0\equiv 1$, we can have a compacter form:</p>

\[h(x) = \sum_{i=0}^n \theta_i x_i = \theta^T x\]

<p>where now we basically have:</p>

<ul>
  <li>
    <p><strong>input</strong>/<strong>feature</strong> $x=\begin{bmatrix} 1 \x_1 \x_2 \ \end{bmatrix}=\begin{bmatrix} x_0 \x_1 \x_2 \ \end{bmatrix}$</p>
  </li>
  <li>
    <p><strong>parameter</strong> $\theta = \begin{bmatrix} \theta_0 \\theta_1 \\theta_2 \ \end{bmatrix}$ would be the ==job of Learning Algorithm to choose==</p>
  </li>
</ul>

<p>Therefore, the task of Learning Algorithm is to <strong>decide $\theta$</strong> such that $h(x) \approx y$ for $x \in \mathcal{D}_m$ being the <strong>training dataset</strong> consisting of $m$ data pairs.</p>

<p>Some sensible idea would be to minimize the <em>square of the error</em>:</p>

<ul>
  <li><strong>Gradient Descent Algorithm</strong></li>
  <li><strong>Normal Equation</strong></li>
</ul>

<h2 id="lms-algorithm">LMS Algorithm</h2>

<p>The aim of Learning Algorithm is to <strong>decide $\theta$</strong> such that $h(x) \approx y$ for $x \in \mathcal{D}_m$ being the <strong>training dataset</strong> consisting of $m$ data pairs.</p>

<blockquote>
  <p><strong>LMS Algorithm</strong></p>

  <ul>
    <li>
      <p>We first define the <strong>cost function $J$</strong> to be:</p>

\[J(\theta) = \frac{1}{2} \sum_{i=1}^{m}\left( h_\theta(x^{(i)})-y^{(i)} \right)^2\]

      <p>where:</p>

      <ul>
        <li>the $\frac{1}{2}$ there is just so that taking <strong>derivatives</strong> later would look nicer</li>
      </ul>
    </li>
    <li>
      <p>Therefore, now the aim is to ==choose $\theta$ to minimize $J(\theta)$==</p>
    </li>
  </ul>
</blockquote>

<h3 id="batch-gradient-descent-algorithm">Batch Gradient Descent Algorithm</h3>

<p>To achieve this, one way is to use (Batch) Gradient Descent Algorithm.</p>

<p>The idea is that if we plot $\theta_1,\theta_2$ and $J$ (<strong>not</strong> the $J$ defined above, which should be a <strong>convex</strong> function), we see something like:</p>

<p>| <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710153413120.png" alt="image-20210710153413120" style="zoom: 33%;" /> | <img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710153525021.png" alt="image-20210710153525021" style="zoom:33%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

<p>where the aim is to find the <em>minima point</em>:</p>

<ol>
  <li>pick some point to start with</li>
  <li>for each point, decide where to move down next based on <strong>gradient vector</strong></li>
  <li>move in that direction and repeat step 2</li>
</ol>

<p>However, notice that the final <em>local minima</em> sometimes depends on where you start with.</p>

<blockquote>
  <p><strong>Gradient Descent Algorithm</strong></p>

  <ul>
    <li>
      <p>Basically start with some initial $\theta=\begin{bmatrix}\theta_0 \ \theta_1 \ …\end{bmatrix}$, and then repeatedly <strong>move</strong>, $\forall j$:</p>

\[\begin{align*}
\theta_j &amp;:=\theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
&amp;=\theta_j - \alpha \sum_{i=1}^m \left[ \left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} \right]\\
\end{align*}\]

      <p>where:</p>

      <ul>
        <li>$:=$ is the <strong>assignment</strong>, which takes value from the right and assign it to the left</li>
        <li>$\alpha$ is called the <strong>learning rate</strong>, usually set to $0.01$ and manipulate around</li>
        <li>the $-\alpha$ indicates that we are <strong>moving DOWN the slope to minima</strong>.</li>
        <li>This method looks at <em>every data point</em> in the entire training set for each step (of computing the next $\theta_j$ value), and is called ==batch gradient descent== (i.e. so a batch here would mean the entire dataset $\mathcal{D}_m$).</li>
      </ul>
    </li>
    <li>
      <p>Then you <strong>repeat until convergence</strong></p>

      <ul>
        <li>
          <p>notice that the definition of $J$ would cause our function to only have ==one global minimum==. Therefore with appropriate $\alpha$, you should <strong>always</strong> get your data to converge</p>

\[J(\theta) = \frac{1}{2} \sum_{i=1}^{m}\left( h_\theta(x^{(i)})-y^{(i)} \right)^2\]

          <ul>
            <li>note that this is a convex function. This is because the <em>sum of convex function</em> $\left( h_\theta(x^{(i)})-y^{(i)} \right)^2$ is ==still== a convex function. (for a simple case, consider the sum of quadratics still being a quadratic)</li>
          </ul>

          <p>Therefore $J$ typically looks like:</p>

          <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710195520612.png" alt="image-20210710195520612" style="zoom: 25%;" /></p>
        </li>
      </ul>
    </li>
    <li>
      <p>So in summary, this algorithm does:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710203245402.png" alt="image-20210710203245402" style="zoom: 67%;" /></p>
    </li>
  </ul>
</blockquote>

<p><strong>Note</strong>:</p>

<ul>
  <li>
    <p>If we plot the contour, then the <strong>gradient</strong> will always look like:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710195720806.png" alt="image-20210710195720806" /></p>

    <p>where:</p>

    <ul>
      <li>basically we are moving <strong>perpendicular</strong> (gradient) to the contours</li>
    </ul>
  </li>
  <li>
    <p>This also implies some ==choice of $\alpha$==</p>

    <ul>
      <li>if you choose your $\alpha$ to be too large, then the last steps might <strong>overshoot</strong> pass the minima</li>
      <li>if you choose your $\alpha$ to be too small, then you need <strong>a lot of iterations</strong> to get to the minima</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong><em>Proof</em></strong></p>

<p>Consider the case when there is <em>only one data point</em> in $\mathcal{D}_m$, then:</p>

\[\begin{align*}
\theta_j &amp;:=\theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
&amp;:=\theta_j -  \alpha \frac{\partial}{\partial \theta_j} \left( \frac{1}{2} (h_\theta(x) - y)^2\right)\\
&amp;:=\theta_j -  \alpha (h_\theta (x) - y)\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&amp;:=\theta_j -  \alpha (h_\theta (x) - y)\frac{\partial}{\partial\theta_j}(\theta^Tx-y)\\
&amp;:=\theta_j -  \alpha (h_\theta (x) - y)\frac{\partial}{\partial\theta_j}\left[ (\theta_0 x_0 + \theta_1x_1+...) -y\right]\\
&amp;:=\theta_j -  \alpha (h_\theta (x) - y)x_j
\end{align*}\]

<p>Now, if you take all the data points, recall that:</p>

\[\begin{align*}
 \frac{\partial}{\partial \theta_j} \left( \sum_{i=1}^m \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2\right)
 &amp;=\sum_{i=1}^m\frac{\partial}{\partial \theta_j} \left(  \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2\right)\\
 &amp;=\sum_{i=1}^m \left[ \left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} \right]\\
\end{align*}\]

<p>Therefore you get:</p>

\[\begin{align*}
\theta_j &amp;:=\theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \\
&amp;=\theta_j - \alpha \sum_{i=1}^m \left[ \left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)} \right]\\
\end{align*}\]

<hr />

<p>In the example of:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710150713957.png" alt="image-20210710150713957" style="zoom:67%;" /></p>

<p>The <strong>gradient descent algorithm</strong> will result in a $\theta$ such that our line looks like:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710201817275.png" alt="image-20210710201817275" /></p>

<blockquote>
  <p><strong>Problem of Gradient Descent</strong></p>

  <ul>
    <li>This method of Gradient Descent looks at <em>every data point</em> in the <strong>entire</strong> training set for each step (i.e. so a batch here would mean the entire dataset $\mathcal{D}_m$). This means we for <strong>each step</strong>, we need to <strong>compute for all data points</strong>, which is very expensive if there are a lot of data points.</li>
  </ul>
</blockquote>

<h3 id="stochastic-gradient-descent-algorithm">Stochastic Gradient Descent Algorithm</h3>

<blockquote>
  <p><strong>Stochastic Gradient Descent Algorithm</strong></p>

  <ul>
    <li>
      <p>Instead of making <strong>one update</strong> for every scan through dataset, we can make <strong>one update per data point</strong>. Therefore, the algorithm looks like:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710203100703.png" alt="image-20210710203100703" style="zoom: 67%;" /></p>

      <p>where:</p>

      <ul>
        <li>in practice, this does make you move <em>faster</em> towards the minima</li>
      </ul>
    </li>
    <li>
      <p>However, the upshot of this is that it ==may never converge==, since you are now only updating per each data point</p>

      <ul>
        <li>this could be “minimized” by continuously <strong>decreasing the learning rate $\alpha$</strong>, so that the oscillation would be slower</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="the-normal-equation">The Normal Equation</h2>

<p>Gradient descent gives one way of minimizing $J$. A second way of doing so is to <strong>perform the minimization explicitly</strong> and without
resorting to an iterative algorithm.</p>

<ul>
  <li>therefore, this is much faster than the algorithms above</li>
  <li>but this works <em>iff</em> there is a solution</li>
</ul>

<hr />

<p><em>Recall/Notations</em></p>

<ul>
  <li>
    <p>if you have $J(\theta)$, where ==input== $\theta \in \mathbb{R}^{m+1}$, then, if for example $m=2$, we have:</p>

\[\nabla_\theta J(\theta) = \begin{bmatrix}
\frac{\partial }{\partial \theta_1}J\\
\frac{\partial }{\partial \theta_2}J\\
\frac{\partial }{\partial \theta_3}J
\end{bmatrix}\]
  </li>
  <li>
    <p>now, you can also have input as <strong>matrix</strong>, e.g. ==input== being $A \in \mathbb{R}^{2 \times 2}$ sand that a function $f$ maps $f:\mathbb{R}^{2\times 2} \to \mathbb{R}$</p>

\[\nabla_A f(A) = \begin{bmatrix}
\frac{\partial f}{\partial A_{11}} &amp; \frac{\partial f}{\partial A_{12}}\\
\frac{\partial f}{\partial A_{21}} &amp; \frac{\partial f}{\partial A_{22}}
\end{bmatrix}\]
  </li>
  <li>
    <p>If we have a <strong>square matrix $A \in \mathbb{R}^{n \times n}$</strong>, then the <strong>trace of $A$</strong> $\text{tr}(A)$ is:</p>

\[\text{tr}(A) = \sum_{i=1}^n A_{ii}\]

    <p>some common properties:</p>

    <ol>
      <li>
        <h6 id="texttrabctexttrcabtexttrbca">$\text{tr}ABC=\text{tr}CAB=\text{tr}BCA$</h6>
      </li>
      <li>
        <p>$\nabla_A \text{tr}AB = B^T$</p>
      </li>
      <li>
        <p>$\nabla_{A^T}f(A) = (\nabla_A f(A))^T$</p>
      </li>
      <li>
        <p>$\nabla_A \text{tr} AA^TC = CA+C^TA$</p>
      </li>
      <li>
        <p>$\nabla_A \text{tr}ABA^TC = CAB + C^TAB^T$</p>

        <ul>
          <li>from 2 and 3 above</li>
        </ul>
      </li>
      <li>$\nabla_A\vert A\vert  = \vert A\vert (A^{-1})^T$</li>
    </ol>
  </li>
</ul>

<hr />

<p>The aim is to achieve:</p>

\[\nabla_\theta J(\theta) = 0\]

<p>but:</p>

<ul>
  <li>computing it directly might be expensive since $J(\theta)$ needs to look through all data</li>
</ul>

<p>Then consider the <strong>design matrix</strong>, which is $X$ by <strong>stacking input data point $x^{(i)} \in \mathbb{R}^{n+1}$</strong> to be:</p>

\[X = \begin{bmatrix}
-(x^{(1)})^T- \\
-(x^{(2)})^T-\\
\vdots \\
-(x^{(m)})^T-
\end{bmatrix}\]

<p>where:</p>

<ul>
  <li>$X \in \mathbb{R}^{m \times (n+1)}$</li>
</ul>

<p>Similarly, define the labels to be:</p>

\[\vec{y} = \begin{bmatrix}
y^{(1)} \\
y^{(2)}\\
\vdots \\
y^{(m)}
\end{bmatrix}\]

<p>Therefore, we have:</p>

\[\begin{align*}
X\theta
&amp;=\begin{bmatrix}
-(x^{(1)})^T- \\
-(x^{(2)})^T-\\
\vdots \\
-(x^{(m)})^T-
\end{bmatrix}\begin{bmatrix}
\theta_1 \\
\theta_2\\
\vdots \\
\theta_n
\end{bmatrix} 
= \begin{bmatrix}
(x^{(1)})^T \theta \\
(x^{(2)})^T \theta \\
\vdots\\
(x^{(m)})^T \theta 
\end{bmatrix}
= \begin{bmatrix}
h_\theta(x^{(1)})\\
h_\theta(x^{(2)}) \\
\vdots\\
h_\theta(x^{(m)}) 
\end{bmatrix}\\
\end{align*}\]

<p>And <strong>more importantly</strong>:</p>

\[\begin{align*}
J(\theta) 
&amp;= \frac{1}{2} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 \\
&amp;= \frac{1}{2} (X\theta -\vec{y})^T(X\theta -\vec{y})
\end{align*}\]

<blockquote>
  <p><strong>Normal Equation</strong></p>

  <ul>
    <li>
      <p>Therefore, since we are finding the minimum point, we basically just do:</p>

\[\begin{align*}
\nabla_\theta J(\theta) &amp;= 0\\
X^T (X\theta - \vec{y}) &amp;= 0
\end{align*}\]

      <p>where:</p>

      <ul>
        <li>$X^T (X\theta - \vec{y}) = 0$ is the also called the <strong>normal equation</strong></li>
      </ul>
    </li>
    <li>
      <p>The solution $\theta$ therefore satisfies:</p>

\[\theta = (X^TX)^{-1}X^T \vec{y}\]
    </li>
  </ul>

</blockquote>

<hr />

<p><strong><em>Proof</em></strong></p>

<p>we already know that:</p>

\[\begin{align*}
J(\theta) 
&amp;= \frac{1}{2} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 \\
&amp;= \frac{1}{2} (X\theta -\vec{y})^T(X\theta -\vec{y})
\end{align*}\]

<p>Then:</p>

\[\begin{align*}
\nabla_\theta J(\theta)
&amp;= \nabla_\theta\frac{1}{2} (X\theta -\vec{y})^T(X\theta -\vec{y}) \\
&amp;= \frac{1}{2}\nabla_\theta (\theta^TX^T -\vec{y}^T)(X\theta -\vec{y}) \\
&amp;= \frac{1}{2}\nabla_\theta (\theta^TX^T X\theta -\vec{y}^T X\theta  -\theta^TX^T \vec{y} + \vec{y}^T\vec{y}) \\
&amp;= \frac{1}{2}\nabla_\theta \text{tr}(\theta^TX^T X\theta -\vec{y}^T X\theta  -\theta^TX^T \vec{y} + \vec{y}^T\vec{y}) \\
&amp;= \frac{1}{2}\nabla_\theta (\text{tr }\theta^TX^T X\theta -2\text{tr }\vec{y}^TX\theta) \\
&amp;= \frac{1}{2}\left( X^TX\theta + X^TX\theta -2X^T \vec{y}  \right)\\
&amp;= X^TX \theta - X^T \vec{y}
\end{align*}\]

<p>Therefore, setting it to zero gives:</p>

\[\begin{align*}
\nabla_\theta J(\theta)
= X^TX \theta - X^T \vec{y} 
&amp;= 0\\
X^T (X\theta - \vec{y}) &amp;= 0
\end{align*}\]

<p>Therefore, the <strong>optimum value for $\theta$</strong> becomes:</p>

\[\begin{align*}
X^T X\theta
&amp;= X^T \vec{y}\\
\theta
&amp;= (X^TX)^{-1}X^T \vec{y}
\end{align*}\]

<h2 id="nonlinear-fits">Nonlinear Fits</h2>

<p>Now suppose you want to fit some of your data in a <strong>nonlinear form</strong>.</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210711233725927.png" alt="image-20210711233725927" style="zoom: 110%;" /></p>

<p>Or even:</p>

\[\theta_0 + \theta_1 x^{(i)}_1 + \theta_2 \sqrt{x^{(i)}_2} + \theta_3 \log x^{(i)}_3\]

<p>What you can do is <strong>simply</strong>:</p>

<ol>
  <li>let $x^{(i)}_2 := \sqrt{x^{(i)}_2}$, let $x^{(i)}_3 := \log x^{(i)}_3$</li>
  <li>then you are backed to a <strong>linear regression problem</strong>, and can apply the same techniques as above</li>
</ol>

<blockquote>
  <p><strong>Problem</strong></p>

  <ul>
    <li>the problem with this is that you <strong>need to know which equation in advance</strong>. e.g. are you sure it is square root for a feature $x^{(i)}_2$? Logarithmic for $x^{(i)}_3$? etc.</li>
  </ul>
</blockquote>

<h3 id="overfitting-and-underfitting">Overfitting and Underfitting</h3>

<p>The idea for both is common but <em>straight-forward</em></p>

<ul>
  <li>==Underfitting==: model has <strong>not captured</strong> the structure for data in the training set</li>
  <li>==Overfitting==: model imitated the data <strong>but NOT the structure</strong>. i.e. the data is distributed in $x^2$, but $x^3$, or $x^4$ is used.</li>
</ul>

<p><em>For Example</em>:</p>

<p>Consider the data being <em>actually</em> distributed in $x \mapsto O(x^2)$, such that we have:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Underfitting</th>
      <th style="text-align: center">Actual</th>
      <th style="text-align: center">Overfitting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204918094.png" alt="image-20210716204918094" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204945061.png" alt="image-20210716204945061" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204955455.png" alt="image-20210716204955455" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>first figure is <em>underfitted</em> as we only imagined $y = \theta_0 + \theta_1 x$​​.</li>
  <li>third figure is <em>overfitting</em> as we imaged $y=\theta_0 + \theta_1 x + \theta_2 x^2 + … + \theta_5 x^5$
    <ul>
      <li>we could interpret this as having ==five features==, $x_1 = x,\,x_2 = x^2,\,x_3=x^3,\,…\,x_5=x^5$,and fitting them all in a linear manner</li>
    </ul>
  </li>
  <li>These notions will be formalized later.</li>
</ul>

<blockquote>
  <p><strong>Take-away Message</strong></p>

  <ul>
    <li>the ==choice of features/hypothesis== is important to ensuring good performance of a learning algorithm. (When we talk about model selection, we’ll also see algorithms for automatically choosing a good set of features.)</li>
  </ul>
</blockquote>

<h2 id="locally-weighted-linear-regression">Locally Weighted Linear Regression</h2>

<blockquote>
  <p><strong>Advantage for Overfitting</strong></p>

  <ul>
    <li>One advantage of locally weighted linear regression (LWR) algorithm is, <strong>assuming</strong> there is sufficient training data, makes the choice of features <strong>less</strong> critical. (==as it is a local model==)</li>
  </ul>
</blockquote>

<p>The Linear Regression discussed in section <a href="#Linear_Regression">Linear Regression</a> is a type of <strong>“Parametric” Learning Algorithm</strong>, because the output/aim is a ==fixed size of parameters $\theta =  \begin{bmatrix} \theta_0 \\vdots\\theta_n \ \end{bmatrix}$ to a dataset==</p>

<ul>
  <li>this means that the <em>output</em> ($\theta$) is constant in size for any dataset</li>
</ul>

<p>However, the LWR (Locally Weighted Linear Regression) would be <strong>“Non-parametric” Learning Algorithm</strong>, because the output/aim are ==parameters that grows/increases linearly with size of dataset==</p>

<ul>
  <li>this means that you <em>can easily fit nonlinear dataset</em>, but the <em>output</em> might be large for large dataset</li>
</ul>

<p>The idea of LWR is as follows.</p>

<p>Consider the following dataset:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210712101427622.png" alt="image-20210712101427622" style="zoom:50%;" /></p>

<p>where, for each desired output of an input $x$, we consider:</p>

<ol>
  <li>mostly importantly the <strong>neighborhood</strong> around/near the input $x$</li>
  <li>slightly consider (less weight) on the data <strong>far away</strong> from the input $x$</li>
  <li>Fit $\theta$ that minimizes $J$ using the above two steps</li>
</ol>

<blockquote>
  <p><strong>LWR</strong></p>

  <ul>
    <li>
      <p>Instead, we have the error function $J$ to be:</p>

\[J(\theta) = \frac{1}{2} \sum_{i=1}^{m} w^{(i)}\left( h_\theta(x^{(i)})-y^{(i)} \right)^2\]

      <p>where:</p>

      <ul>
        <li>basically we are adding weight $w^{(i)}$ to each data point,</li>
      </ul>

      <p>A common $w$ to use is:</p>

\[w^{(i)} = \exp{\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)}\]

      <p>so that:</p>

      <ul>
        <li>$x$ is the <strong>input which we want to get the prediction of</strong></li>
        <li>so if $x$ is close to some points $x^{(i)}$, then the weight is close to $1$. Otherwise, far away points have weight close to $0$</li>
        <li>$\tau$ specifies the <strong>width</strong> of the bell shaped curve, and is called the <strong>bandwidth parameter</strong>. This will be a hyper-parameter.</li>
      </ul>

      <p>Therefore, performing $J(\theta)$ means:</p>

      <ul>
        <li>we are “only” ==computing prediction errors near input point $x$==</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="probabilistic-interpretation">Probabilistic Interpretation</h2>

<p>This section aims to answer questions such as:</p>

<ul>
  <li>Why using Least Squares for Error?</li>
</ul>

<p>Consider the case for housing data:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210710150713957.png" alt="image-20210710150713957" style="zoom:67%;" /></p>

<p>And ==assuming== the ==actual distribution of data is==:</p>

\[y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}\]

<p>where:</p>

<ul>
  <li>so housing prices is a linear function on the features</li>
  <li>$\epsilon^{(i)}$ means the <strong>error term</strong> that captures either <strong>unmodeled effects</strong> or <strong>random noise</strong>.
    <ul>
      <li>e.g. the seller’s mood on that day</li>
    </ul>
  </li>
</ul>

<p><em>Pictorially</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model:</th>
      <th style="text-align: center">Actual Data:</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717221916347.png" alt="image-20210717221916347" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717221943322.png" alt="image-20210717221943322" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>And that we ==assume== the error term $\epsilon^{(i)}$ is distributed ==IID== (independently and identically distributed according to a <strong>Gaussian Distribution</strong>, also called a Normal Distribution):</p>

\[\epsilon^{(i)} \sim  \mathcal{N}(0, \sigma^2)\]

<p>where this normal distribution has:</p>

<ul>
  <li>mean being $0$</li>
  <li>standard deviation of $\sigma$</li>
  <li>in general, for those types of data, the distributions would be Gaussian due to the <em>Central Limit Theorem</em></li>
</ul>

<p>So that this means the ==density== of $\epsilon^{(i)}$ is given by:</p>

\[p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(\epsilon^{(i)})^2}{2 \sigma^2} \right)}\]

<p>Therefore, since now we know the density of $\epsilon^{(i)}$ <strong>and we know $y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$</strong>, this ==gives==:</p>

\[p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)}\]

<p>where:</p>

<ul>
  <li>
    <p>$p(y^{(i)} \vert  x^{(i)}; \theta)$ means the distribution of $y^{(i)}$ given $x^{(i)}$ and parametrized by $\theta$</p>

    <ul>
      <li>so only $x^{(i)}$ is the <em>random variable</em></li>
    </ul>
  </li>
  <li>
    <p>alternatively, you can write it as:</p>

\[y^{(i)} | x^{(i)}; \theta \sim \mathcal{N}(\theta^T x^{(i)}, \sigma^2)\]
  </li>
</ul>

<p>Now, the important step is to ask: ==given the input/design matrix $X$, what is the distribution of $y^{(i)}$?== i.e. How probable is each of the $y^{(i)}$ in your dataset?</p>

<ul>
  <li>
    <p>By definition, this quantity is <strong>typically</strong> viewed a function of $\vec{y}$ (and perhaps $X$), for a fixed value of $\theta$.</p>
  </li>
  <li>
    <p>However, remember that our aim is to get $\theta$, so we want to ==view== it as a <strong>function of $\theta$</strong>, i.e. the <strong>likelihood function $L(\theta)$</strong>:</p>

\[L(\theta) = L(\theta; X, \vec{y}) = p(\vec{y}|X;\theta)\]
  </li>
</ul>

<p>Hence, <strong>using the independent assumption (IID)</strong>, we get:</p>

\[\begin{align*}
L(\theta) 
=p(\vec{y}|X;\theta)
&amp;= \prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta) \\
&amp;= \prod_{i=1}^m \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)}
\end{align*}\]

<p>Therefore, all we need to do it to get the ==maximum likelihood for $\theta$==.</p>

<ol>
  <li>
    <p>Instead of doing it for $L(\theta)$, it is easier to first consider $l(\theta)=\log L(\theta)$</p>

\[\begin{align*}
l(\theta)
&amp;= \log L(\theta) \\
&amp;= \log \prod_{i=1}^m \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)}\\
&amp;= \sum_{i=1}^m \left( \log\frac{1}{\sqrt{2\pi} \sigma} - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2}  \right)\\
&amp;= m \log\frac{1}{\sqrt{2\pi} \sigma} -\sum_{i=1}^m\left( \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2}  \right)\\
&amp;= m \log\frac{1}{\sqrt{2\pi} \sigma} -\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2
\end{align*}\]
  </li>
  <li>
    <p>Therefore, to maximize $l(\theta)$ is the same as ==minimizing==:</p>

\[\frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2 = J(\theta)\]
  </li>
</ol>

<blockquote>
  <p>Therefore, the take away message is that:</p>

  <ul>
    <li>the Least Square Algorithm defining $J(\theta)=\frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2$ is <strong>equivalent as saying that</strong> the ==error term== $\epsilon$ from $y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$ is ==distributed IDD==</li>
    <li>therefore, the aim of minimizing $J(\theta)$ is the same as ==maximizing likelihood $L(\theta)$== given $X, \vec{y}$</li>
  </ul>
</blockquote>

<h1 id="classification-and-logistic-regression">Classification and Logistic Regression</h1>

<p>Let’s now talk about the classification problem. This is just like the regression problem, except that the values $y$ we now want to predict take on only a <strong>small set of discrete values</strong>.</p>

<p>For now, we will focus on the ==binary classification problem== in which $y$ can take on only two values, 0 and 1.</p>

<ul>
  <li>However, most of what we say here will also <em>generalize to the multiple-class case</em>.</li>
</ul>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>The idea is that, if we are given some <em>binary data</em>, such that a linear fit would be bad because:</p>

<ul>
  <li>gradient changes if we add more data on both ends</li>
</ul>

<p>Therefore, we consider functions that looks like:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210712141123925.png" alt="image-20210712141123925" /></p>

<p>which is the <strong>logistic function</strong> or <strong>sigmoid function</strong></p>

\[g(z) = \frac{1}{1+e^{-z}}\]

<p>which basically:</p>

<ul>
  <li>outputs result between $0,1$</li>
  <li>we chose this function <em>specifically</em> because this is actually the special case of the <strong>Generalized Linear Model</strong></li>
</ul>

<hr />

<p><em>Useful Properties of Sigmoid Function</em></p>

<ol>
  <li>
    <p>Consider the derivative of the function $g$:</p>

\[\begin{align*}
g'(z)
&amp;= \frac{d}{dz} \frac{1}{1+e^{-z}} \\
&amp;= \frac{1}{(1+e^{-z})^2}(-e^{-z}) \\
&amp;= \frac{1}{1+e^{-z}} \frac{-e^{-z}}{1+e^{-z}} \\
&amp;= \frac{1}{1+e^{-z}}\left( 1 - \frac{1}{1+e^{-z}} \right) \\
&amp;= g(z)\left(1-g(z)\right)
\end{align*}\]
  </li>
</ol>

<hr />

<p>Therefore, now we consider the <strong>hypothesis</strong>:</p>

\[h_\theta(x)=g(\theta^T x)=\frac{1}{1+e^{-\theta^T x}}\]

<p>And the task now is to find out a good ==cost function==.</p>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <ul>
    <li>
      <p>Recall that for linear regression, we used least squares error which could be derived using the ==probabilistic model== as the <strong>maximum likelihood estimator</strong> under a set of assumptions, by the following setups:</p>

      <ol>
        <li>
          <p>In the linear regression model, <em>assume</em> the distribution data ==follows== <em>the form of our hypothesis</em></p>

\[y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}\]

          <p>where $\epsilon$ is the perturbation which basically introduces the second step</p>
        </li>
        <li>
          <p><em>Assuming</em> the perturbation being IID, consider the ==probability of actually getting $y$ from some $x$==:</p>

\[p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)}\]
        </li>
        <li>
          <p>==Maximize== the probability of $p(y^{(i)}\vert  x^{(i)};\theta)$ by optimizing $\theta$, which reaches the conclusion of <em>minimizing</em>:</p>

\[\frac{1}{2}\sum_{i=1}^m (y^{(i)} - \theta^T x^{(i)})^2 = J(\theta)\]

          <p>which becomes the <strong>error function</strong></p>
        </li>
      </ol>
    </li>
  </ul>
</blockquote>

<p>Therefore, first we do a similar step of:</p>

<ol>
  <li>
    <p>Considering a <strong>single point</strong>, and <strong>assume</strong> that the distribution of data follows the form of the hypothesis:</p>

\[\begin{align*}
P(y = 1 | x;\theta) &amp;= h_\theta (x) \\
P(y = 0 | x; \theta)&amp;= 1 - h_\theta(x)
\end{align*}\]

    <p><em>Pictorially</em></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Model:</th>
          <th style="text-align: center">Actual Data:</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717223157895.png" alt="image-20210717223157895" style="zoom: 67%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717223206039.png" alt="image-20210717223206039" style="zoom: 67%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>or compactly, since $y \in {0,1}$:</p>

\[p(y | x; \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}\]

    <p>which is also called the ==Bernoulli Distribution==, which has</p>

    <ul>
      <li>the <em>probability mass function</em> being $f(k;\phi) = \phi^k(1-\phi)^{1-k}$ for $k \in {0,1}$</li>
      <li>https://en.wikipedia.org/wiki/Bernoulli_distribution</li>
    </ul>
  </li>
  <li>
    <p><strong>Assuming</strong> that the training samples are generated IID, such that we have the ==likelihood of parameter/probability of data== being given by:</p>

\[\begin{align*}
L(\theta) &amp;= p(\vec{y} | X; \theta) \\
&amp;= \prod_{i=1}^m p(y^{(i)}| x^{(i)}; \theta) \\
&amp;= \prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
\end{align*}\]

    <p>Again, to convert the product to easier sums, we consider the <strong>log likelihood</strong> which is monotonic (i.e. if $f(x)$ increases/decreases, then $\log(f(x))$ also increases/decreases)</p>

\[l(\theta) = \log(L(\theta)) = \sum_{i=0}^m y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)}) \log \left( 1-h_\theta(x^{(i)}) \right)\]

    <blockquote>
      <p><strong>Note</strong></p>

      <ul>
        <li>This function turns out to be a <strong>concave</strong> function, in other words, there is only <strong>one maxima</strong></li>
      </ul>
    </blockquote>
  </li>
  <li>
    <p>Now, to ==maximize== the (log) likelihood:</p>

    <ul>
      <li>
        <p>as compared to the <em>normal equation in linear regression</em>, doing it directly seems <em>non-trivial</em>. It turns out that there is <strong>no “normal equation equivalent”</strong> for logistic regression</p>
      </li>
      <li>
        <p>another “brute-force” away is to consider the <strong>(batch) gradient ascent method</strong>, where we want to do:</p>

\[\theta := \theta + \alpha \nabla_\theta l(\theta)\]

        <p>Or written in the form of individual components:</p>

\[\theta_j := \theta_j + \alpha\frac{\partial}{\partial \theta_j}l(\theta)\]

        <blockquote>
          <p><strong>Differences against Linear Regression</strong></p>

          <ul>
            <li>
              <p>For linear regression, we had <strong>batch gradient Descent</strong>, which involved:
\(\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta)\)
so we see that the differences are:</p>

              <ol>
                <li>the “cost” function is now $l(\theta)$</li>
                <li>we are <strong>ascending</strong> with a plus sign in $+ \frac{\partial}{\partial \theta_j}l(\theta)$, instead of <strong>descending</strong> with a minus sign in $- \frac{\partial}{\partial \theta_j}J(\theta)$. Even though both are <em>maximizing likelihood</em></li>
              </ol>
            </li>
          </ul>
        </blockquote>
      </li>
    </ul>
  </li>
</ol>

<h3 id="batch-gradient-ascent-algorithm">Batch Gradient Ascent Algorithm</h3>

<blockquote>
  <p><strong>Batch Gradient Ascent</strong></p>

  <ul>
    <li>
      <p>The aim is to compute</p>

\[\theta_j := \theta_j + \alpha\frac{\partial}{\partial \theta_j}l(\theta)\]

      <p>And below we computed the derivative, so in the end we are computing:</p>

\[\theta_j := \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j\]

      <p>which again does it by batches (of the entire dataset)</p>
    </li>
  </ul>
</blockquote>

<p><strong><em>Proof for Derivative of $l(\theta)$</em></strong>:</p>

<p>The gradient ascent algorithm needs the $\frac{\partial}{\partial \theta_j} l(\theta)$ quantity. Like before, we first consider having on <strong>one training sample $x^{(1)}$</strong></p>

\[\begin{align*}
\frac{\partial}{\partial \theta_j} l(\theta)
&amp;= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)\frac{\partial}{\partial \theta_j}g(\theta^T x)\\
&amp;= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)\left( g'(\theta^Tx) \cdot \frac{\partial}{\partial \theta_j} \theta^T x \right)\\
&amp;= \left( y \frac{1}{g(\theta^T x)} - (1-y)\frac{1}{1-g(\theta^T x)} \right)\left( g(\theta^T x)(1-g(\theta^T x)) \cdot \frac{\partial}{\partial \theta_j} \theta^T x \right)\\
&amp;= \left[ y (1-g(\theta^T x)) - (1-y)g(\theta^Tx)\right] x_j \\
&amp;= (y - h_\theta(x)) x_j
\end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>obviously $h_\theta(x)=g(\theta^T x)=\frac{1}{1+e^{-\theta^T x}}$</p>
  </li>
  <li>
    <p>from step 2 to step 3 we used the fact $g’(z) = g(z)(1-g(z))$ from above (property 1 of sigmoid function)</p>
  </li>
</ul>

<p>Therefore, for <strong>$m$ training samples</strong>, we have:</p>

\[\frac{\partial}{\partial \theta_j} l(\theta)
= \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j\]

<h4 id="perceptron-algorithm">Perceptron Algorithm</h4>

<p>Consider modifying the logistic regression method to be <strong>stricter</strong> such that it outputs 0 or 1 <strong>exactly</strong>:</p>

\[g(z) = \begin{cases}
1, &amp; \text{if $z \ge 0$}\\
0, &amp; \text{if $z &lt; 0$}
\end{cases}\]

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/1200px-Dirac_distribution_CDF.svg.png" alt="Heaviside step function - Wikipedia" style="zoom: 25%;" /></p>

<p>Therefore, we can see this as the stricter version of the sigmoid function:</p>

<p><img src="https://www.researchgate.net/profile/Mashor-Housh/publication/317693770/figure/fig1/AS:627583494418432@1526638973245/Approximation-of-the-Heaviside-step-function.png" alt="Approximation of the Heaviside step function. | Download Scientific Diagram" style="zoom: 33%;" /></p>

<p>And more importantly:</p>

<blockquote>
  <p><strong>Perceptron Algorithm</strong></p>

  <ul>
    <li>
      <p>using this modified definition of $g$, and we use the update rule</p>

\[\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j\]
    </li>
    <li>
      <p>however, the problem with this is that there turns out to have <strong>no obvious probabilistic interpretation of this</strong></p>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Perceptron Algorithm implementation</strong></p>

  <ul>
    <li>
      <p>The perceptron algorithm $\text{Perceptron}(\mathcal{D}_n; \tau)$:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210704233329185.png" alt="image-20210704233329185" style="zoom:67%;" /></p>

      <p>where notice that:</p>

      <ul>
        <li>$n$ is the <strong>number of data points</strong> in your data set</li>
        <li>
          <p>$\tau$ is again a <strong>hyperparameter</strong> (nothing to do with the logic of algorithm) that you can adjust</p>
        </li>
        <li>
          <p>$\theta$ is initialized to be a vector in $d$ dimension (same as data points $x$)</p>
        </li>
        <li>
          <p>$\theta_0$ initialized to be $0$</p>
        </li>
        <li>
          <p><code class="language-plaintext highlighter-rouge">changed</code> indicates whether if our prediction on <strong>each data point</strong> is correct or not.</p>

          <p>for example, $y^{(i)}\cdot (\theta^T x^{(i)} + \theta_0) \le 0$,  if</p>

          <ul>
            <li>a data point is <em>not on the line AND current prediction is wrong</em></li>
            <li>a data point is <em>on the line</em></li>
            <li>==initial step/loop== of iteration</li>
          </ul>

          <p>Therefore this actually specifies ==incorrect prediction of a data point==</p>
        </li>
        <li>
          <p>The part that updates $\theta, \theta_0$ basically does aims to ==improve $y^{(i)}\cdot (\theta^T x^{(i)} + \theta_0) \le 0$==. So that the next iteration looks like:</p>

\[\begin{align*}
y^{(i)}\cdot \left((\theta + y^{(i)}x^{(i)})^Tx^{(i)} +(\theta_0 + y^{(i)})\right) 
&amp;= y^{(i)} (\theta^T x^{(i)} + \theta_0)+(y^{(i)})^2(x^{(i)^T} x^{(i)} + 1)\\
&amp;= y^{(i)} (\theta^T x^{(i)} + \theta_0) + (||x^{(i)}||^2+1)
\end{align*}\]

          <p>where:</p>

          <ul>
            <li>basically we moved the <em>original prediction $y^{(i)} (\theta^T x^{(i)}+\theta_0)$</em> by some positive value $\vert \vert x^{(i)}\vert \vert ^2+1$</li>
            <li>we assumed that $y^{(i)} \in {-1, +1}$ in this case</li>
          </ul>
        </li>
        <li>we return the <strong>best line by return the parameter $\theta, \theta_0$</strong></li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />

<p><strong><em>Note</em></strong></p>

<ul>
  <li>
    <p>recall first that:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210703001004237.png" alt="image-20210703001004237" style="zoom: 67%;" /></p>

\[\begin{align*}
\frac{\theta^T x}{||\theta||} &amp;= -b \\
\theta^T x + b ||\theta|| &amp;= 0\\
\theta^T x + \theta_0 &amp;= 0
\end{align*}\]

    <p>so that the <strong>more positive the $\theta_0$</strong>, the ==lower/downwards== we are moving the line</p>
  </li>
  <li>
    <p>notice the sign on $\theta_0$ update:</p>

\[\theta_0 = \theta_0 + y^{(i)}\]

    <p>this means that if <strong>we predicted wrong</strong> and that</p>

    <ul>
      <li>$y^{(i)} &gt; 0$, then we need to ==move the line downwards==</li>
      <li>$y^{(i)} &lt; 0$, then we need to ==move the line upwards==</li>
    </ul>
  </li>
  <li>
    <p>the $\theta$ update:</p>

    <ul>
      <li>rotate the current vector $\theta$ ==towards== the correct/actual result</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Running the above perceptron algorithm would look like:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210705001505622.png" alt="image-20210705001505622" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>we finished when $t=1, i=44$ (#steps=44)</li>
  <li>the “random alg train” in the bottom plot is the $\mathrm{ExLearningAlg}(\mathcal{D}_n;k)$ mentioned before</li>
  <li><strong>note that</strong> having more steps ==might increase train error==, since
    <ul>
      <li>$\text{ExLearningAlg}(\mathcal{D}_n;k)$ is iterating a step per hypothesis</li>
      <li>$\text{Perceptron}(\mathcal{D}_n; \tau)$ here is iterating over $\tau$ <strong>and</strong> each point in dataset, and that the ==adjustment in $\theta, \theta_0$== only moves more towards the ==current== data point</li>
    </ul>
  </li>
</ul>

<h4 id="linearly-separable-dataset">Linearly Separable Dataset</h4>

<p>Notice that the above $\text{Perceptron}(\mathcal{D}_n, \tau)$ works <strong>if and only if</strong> there <strong>exists a line that can distinctly separate</strong> the binary dataset.</p>

<blockquote>
  <p><strong>Linearly Separable</strong></p>

  <ul>
    <li>
      <p>A training set $\mathcal{D}_n$ is linearly separable if there exist $\theta, \theta_0$ such that, for every point index $i \in {1,…,n}$, we have:</p>

\[y^{(i)}\cdot (\theta^T x^{(i)} + \theta_0) &gt; 0\]

      <p>i.e. correct prediction <strong>for all data points</strong></p>
    </li>
  </ul>
</blockquote>

<p>==TODO: How do we prove that if a training set is NOT linearly separable?==</p>

<p><em>For Example</em>:</p>

<p>This would <strong>not</strong> be a linearly separable data set:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210705231722413.png" alt="image-20210705231722413" style="zoom:67%;" /></p>

<h4 id="margin-of-training-set">Margin of Training Set</h4>

<p><strong><em>Review</em></strong>:</p>

<ul>
  <li>
    <p>Consider the <strong>signed distance</strong> (green part) from a hyperplane defined by $\theta, \theta_0$ to a point $x^*$ below:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210705232626944.png" alt="image-20210705232626944" style="zoom:67%;" /></p>

    <p>This green part is ==equivalent to== the <em>purple part minus the orange part</em>:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210705233058717.png" alt="image-20210705233058717" style="zoom:67%;" /></p>

    <p>where the</p>

    <ul>
      <li>purple part would be $\frac{\theta^T x^*}{\vert \vert \theta\vert \vert }$</li>
      <li>orange part be $-\frac{\theta_0}{\vert \vert \theta\vert \vert }$</li>
    </ul>

    <p>so this is easily:</p>

\[\text{Signed Distance of $x^*$ from Hyperplane}=\frac{\theta^T x^* + \theta_0}{||\theta||}\]
  </li>
</ul>

<hr />

<blockquote>
  <p><strong>Margin of a Labelled Point</strong></p>

  <ul>
    <li>
      <p>Consider now a labelled point $(x^<em>, y^</em>)$, where $y^{<em>} \in {-1,+1}$. Then the <strong>margin of the labelled point</strong> $x^</em>, y^*$ with respect to the hyperplane defined by $\theta, \theta_0$ is:</p>

\[\text{Margin of $(x^*, y^*)$}=y^* \left( \frac{\theta^T x^* + \theta_0}{||\theta||} \right)\]

      <p>where:</p>

      <ul>
        <li>notice the part $\theta^T x^* + \theta_0$ is basically the <strong>our prediction of $h(x;\theta, \theta_0)$</strong></li>
        <li>$\text{margin}$ of a labelled point would therefore be <strong>positive</strong> if your hypothesis <strong>guessed correctly</strong>.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p><em>For Example</em></p>

<p>The hyperplane (blue line) of a specific $\theta, \theta_0$ in the dataset below should have a <strong>large margin</strong>:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210705234211738.png" alt="image-20210705234211738" style="zoom: 50%;" /></p>

<blockquote>
  <p><strong>Margin of a Training Set</strong></p>

  <ul>
    <li>
      <p>The margin of a training set $\mathcal{D}_n$ with respect to a <em>==hyperplane==</em> defined by $\theta, \theta_0$ is:</p>

\[\min_{i \in \{1,...,n\}}y^{(i)}\left(\frac{\theta^T x^* + \theta_0}{||\theta||}\right)\]

      <p>where notice that:</p>

      <ul>
        <li>if ==any== data point is ==misclassified==, then the margin would be ==negative==</li>
        <li>if ==all== data points are classified ==correctly==, then the margin would be ==positive==</li>
      </ul>
    </li>
    <li>
      <p>The more positive the margin, the <em>easier</em> the classifier is/<em>more separable</em> the dataset</p>
    </li>
  </ul>
</blockquote>

<h4 id="perceptron-algorithm-performance">Perceptron Algorithm Performance</h4>

<blockquote>
  <p><strong>Perceptron Performance</strong></p>

  <ul>
    <li>
      <p>The perceptron algorithm will make ==at most $(R/\gamma)^2$ updates to $\theta$== (i.e. rotate the hyperplane at most $(R/\gamma)^2$ times) util it hits the solution hypothesis, if and only if the following <strong>assumptions holds true</strong>:</p>

      <ol>
        <li>
          <p>The Hypothesis Class is the set of all <strong>linear classifiers with separating hyperplanes</strong> that <strong>pass through the origin</strong>.</p>

          <ul>
            <li>this means that $\theta_0=0$ and the hyperplane is completely determined by $\theta$</li>
          </ul>
        </li>
        <li>
          <p>There exists a $\theta^*$ and $\gamma$ such that $\gamma &gt; 0$ <strong>and</strong> for every $i \in {1,…,n}$ we have:</p>

\[\text{Margin of $(x^{(i)}, y^{(i)})$}=y^{(i)} \left( \frac{\theta^T x^{(i)} + \theta_0}{||\theta||} \right) &gt; \gamma\]

          <ul>
            <li>this means that the dataset <strong>is linearly separable</strong></li>
          </ul>
        </li>
        <li>
          <p>There exists a <strong>radius $R$</strong> such that for every $i \in {1,…,n}$, we have $\vert \vert x^{(i)}\vert \vert  \le R$</p>
        </li>
      </ol>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210706000650447.png" alt="image-20210706000650447" /></p>
    </li>
  </ul>
</blockquote>

<hr />

<p><strong><em>Proof of Assumption 1</em></strong></p>

<p>Consider a classifier <strong>with offset</strong> described by $\theta, \theta_0$:</p>

\[x:\theta^Tx + \theta_0 = 0, \quad \text{so that } x\in \mathbb{R}^d,\theta\in \mathbb{R}^d,\theta_0\in \mathbb{R}\]

<p>Then we can <strong>convert</strong> this equation and eliminate $\theta_0$ by considering:</p>

\[x_\text{new}=[x_1,...,x_d,1]^T, \theta_\text{new}=[\theta_1, ..., \theta_d, \theta_0]^T, \quad \text{so that }x_\text{new} \in \mathbb{R}^{d+1}, \theta_{\text{new}} \in \mathbb{R}^{d+1}\]

<p>Then we have the following being <strong>equivalent</strong>:</p>

\[\begin{align*}
x:\theta^Tx + \theta_0 &amp;= 0 \\
x_{\text{new},1:d}:\theta^T_{\text{new}}x_{\text{new}}&amp;=0
\end{align*}\]

<p>where:</p>

<ul>
  <li>$x_{\text{new},1:d}$ means the first $d$ dimension of the vector $x_\text{new}$</li>
  <li>$\theta_0$ has been included in the <strong>expanded feature space</strong> of $\theta_\text{new}$</li>
</ul>

<h3 id="newtons-method">Newton’s Method</h3>

<hr />

<p><strong>Comparison against Batch Gradient Ascent</strong></p>

<ul>
  <li>one <strong>advantage</strong> of this over <em>Batch Gradient Ascent</em> is that it will take <strong>less iterations</strong> (quadratic convergence) to achieve a good result</li>
  <li>one <strong>disadvantage</strong> is that it is <strong>more computationally expensive</strong>. See section <a href="#Higher Dimensional Newton's Method">Higher Dimensional Newton’s Method</a></li>
</ul>

<hr />

<blockquote>
  <p><strong>Newton’s Method</strong></p>

  <p>Task: given a function $f$, the idea is to <strong>find $\theta$</strong> such that $f(\theta) = 0$</p>

  <ul>
    <li>
      <p>The idea is to <em>approximate</em> the function near $\theta$ to be a <strong>straight line</strong>, therefore the next guess of zero is located at:</p>

\[\theta := \theta - \frac{f(\theta)}{f'(\theta)}\]
    </li>
  </ul>

  <p>Usage:</p>

  <ul>
    <li>
      <p>Since we want to <strong>maximize</strong> $l(\theta)$, we can see it as figuring at $l’(\theta) = 0$ since there is only one global maximum, hence:</p>

\[\theta := \theta - \frac{\ell'(\theta)}{\ell''(\theta)}\]
    </li>
  </ul>

</blockquote>

<p><strong><em>Proof</em></strong>:</p>

<p>The idea is simple:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717185036038.png" alt="image-20210717185036038" /></p>

<p>where we want to perform the update of:</p>

\[\theta_1 := \theta_0 - \Delta\]

<p>And we know that:</p>

\[\begin{align*}
f'(\theta_0) &amp;= \frac{f(\theta_0)}{\Delta}\\
\Delta &amp;= \frac{f(\theta_0)}{f'(\theta_0)}\\
\end{align*}\]

<p>Therefore, we arrive simply at:</p>

\[\theta := \theta - \frac{f(\theta)}{f'(\theta)}\]

<hr />

<p><em>Example</em></p>

<table>
  <thead>
    <tr>
      <th>Steps</th>
      <th style="text-align: center">Graphical Illustration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Step 0:  given $f$, and a $\theta$ to start with</td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717181953294.png" alt="image-20210717181953294" /></td>
    </tr>
    <tr>
      <td>Step 1: Assume that the $f(\theta’)=0$ can be found by going down <em>linearly</em></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717182112493.png" alt="image-20210717182112493" /></td>
    </tr>
    <tr>
      <td>Step 2: Repeat step 1 until $f(\theta’)=0$</td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717182340380.png" alt="image-20210717182340380" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Quadratic Convergence</strong></p>

  <ul>
    <li>The idea (informally) is that: if newton’s method on the first iteration has error $0.01$ (away from the true zero), then:
      <ol>
        <li>second iteration error=$0.0001$</li>
        <li>third iteration error=$0.00000001$</li>
        <li>etc.</li>
      </ol>
    </li>
  </ul>
</blockquote>

<h4 id="higher-dimensional-newtons-method">Higher Dimensional Newton’s Method</h4>

<p>Since in some cases we have $\theta$ being a vector in $\mathbb{R}^{n+1}$, then:</p>

<blockquote>
  <p><strong>Newton’s Method in Higher Dimension</strong></p>

  <ul>
    <li>
      <p>The generalization of Newtons method to this multidimensional setting is:</p>

\[\theta := \theta - H^{-1} \nabla_\theta \ell (\theta)\]

      <p>as compared to the one dimensional $\theta := \theta - \frac{1}{\ell’’(\theta)}\ell’(\theta)$, where you now have:</p>

      <ul>
        <li>
          <p><strong>Hessian $H$</strong>  being a $\mathbb{R}^{(n+1)\times(n+1)}$ matrix such that</p>

\[H_{ij} = \frac{\partial^2 \ell(\theta)}{\partial \theta_i \partial \theta_j}\]
        </li>
      </ul>
    </li>
  </ul>

</blockquote>

<p>However, obviously now it becomes computationally difficult to compute $H^{-1}$ if $n+1$ is large for your dataset, i.e. you have <strong>lots of features</strong>.</p>

<h1 id="generalized-linear-models">Generalized Linear Models</h1>

<p>So far, we’ve seen a regression example, and a classification example. In the regression example, we had $y\vert x; \theta ∼ N(\mu, \sigma^2)$, and in the classification one, $y\vert x; \theta ∼ \text{Bernoulli}(\phi)$, for some appropriate definitions of $\mu$ and $\phi$ as functions of $x$ and $\theta$.</p>

<p>In this section, we will show that both of these methods are <strong>special cases of a broader family of models</strong>, called Generalized Linear Models (GLMs).</p>

<hr />

<p>==Terminologies==</p>

<ul>
  <li><strong>Probability Mass Function</strong>: distribution for <strong>discrete values</strong>.
    <ul>
      <li>e.g. Poisson Distribution, Bernoulli Distribution</li>
    </ul>
  </li>
  <li><strong>Probability Density Function (PDF)</strong>: distribution for <strong>continuous values</strong>.
    <ul>
      <li>e.g. Normal Distribution</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="exponential-family">Exponential Family</h2>

<p>To work our way up to GLMs, first we need to define the exponential family distributions.</p>

<blockquote>
  <p><strong>Exponential Family Distribution</strong></p>

  <ul>
    <li>
      <p>We say that a class of distributions is <strong>in the exponential family</strong> if it can be written in the form:</p>

\[p(y; \eta) = b(y) \exp(\eta \, T(y) - a(\eta))\]

      <p>where:</p>

      <ul>
        <li>$y$ means the labels/target in your dataset</li>
        <li>$\eta$ is the <strong>natural parameter</strong> (also called the canonical parameter) of the <strong>distribution</strong></li>
        <li>$b(y)$ is the <strong>base measure</strong></li>
        <li>$T(y)$ is the <strong>sufficient statistic</strong> (see later examples, you often see $T(y)=y$)</li>
        <li>$a(\eta)$ is the <strong>log partition function</strong>, which basically has $e^{-a(\eta)}$ playing the role of <em>normalization constant</em></li>
      </ul>

      <p>so basically you can expression some distribution with the above form with any choice of $b(y), T(y), a(\eta)$, then that expression is in the exponential family.</p>
    </li>
  </ul>
</blockquote>

<p>We now show that the Bernoulli and the Gaussian distributions are examples of exponential family distributions.</p>

<hr />

<p><em>Example: Bernoulli Distribution</em></p>

<p>For $y \in {0,1}$, we have the Bernoulli Distribution (recall that we used $h_\theta(x)=\phi$):</p>

\[p(y ; \phi) = \phi^y(1-\phi)^{1-y}\]

<p>We can convert this to the exponential family by:</p>

\[\begin{align*}
\phi^y(1-\phi)^{1-y}
&amp;= \exp\left( \log(\phi^y(1-\phi)^{1-y}) \right) \\
&amp;= \exp\left(y \log \phi + (1-y)\log (1-\phi)\right)\\
&amp;= 1 \cdot \exp\left( \log\left( \frac{\phi}{1-\phi} \right) y + \log(1-\phi)\right)\\
\end{align*}\]

<p>Therefore, this is in the exponential family with:</p>

<ul>
  <li>$b(y)$ = 1</li>
  <li>$T(y) = y$</li>
  <li>$\eta = \log\left( \frac{\phi}{1- \phi} \right)$, so that we get $\phi = \frac{1}{1+e^{-\eta}}$
    <ul>
      <li>recall that $\phi = h_\theta(x)$, and that $h_\theta(x)= \frac{1}{1+e^{-\theta^T x}}$ in a similar form!</li>
    </ul>
  </li>
  <li>$a(\eta) = -\log(1-\phi) = \log(1+e^\eta)$</li>
</ul>

<hr />

<p><em>Example: Gaussian/Normal Distribution</em></p>

<p>For algebraic simplicity, assume that $\sigma^2 = 1$ (recall that when deriving linear regression, the value of $\sigma$ had <strong>no effect</strong> on our final choice of $\theta$. Therefore, this is also “justified”)</p>

<p>Then we have:</p>

\[p(y; \mu) = \frac{1}{\sqrt{2 \pi}} \exp\left( - \frac{(y-\mu)^2}{2} \right)\]

<p>and doing a similar step as above:</p>

\[\begin{align*}
\frac{1}{\sqrt{2 \pi}} \exp\left( - \frac{(y-\mu)^2}{2} \right)
&amp;= \frac{1}{\sqrt{2 \pi}} e^{-y^2/2} \cdot \exp\left( \mu y - \frac{1}{2}\mu^2 \right)
\end{align*}\]

<p>where now this is matched easily:</p>

<ul>
  <li>$b(y) = \frac{1}{\sqrt{2 \pi}} e^{-y^2/2}$</li>
  <li>$T(y) = y$</li>
  <li>$\eta = \mu$ which is the <em>natural parameter</em></li>
  <li>$a(\eta) = \mu^2/2 = \eta^2/2$</li>
</ul>

<h2 id="properties-of-exponential-family">Properties of Exponential Family</h2>

<p>We use the GLM because they have some very nice properties:</p>

<ol>
  <li>Maximum Likelihood Estimate (MLE) with respect to $\eta$ is <strong>concave</strong>, and the Negative Log Likelihood (NLL) with respect to $\eta$ is <strong>convex</strong></li>
  <li>The expected value $E[y; \eta] = \frac{\partial}{\partial \eta} a(\eta)$</li>
  <li>The variance $\text{Var}[y;\eta] = \frac{\partial^2}{\partial \eta^2} a(\eta)$</li>
</ol>

<p>Note that the expected value and the variance do not involve integrals now.</p>

<h2 id="constructing-the-glms">Constructing the GLMs</h2>

<p>More generally, consider a classification or regression problem where we would like to <strong>predict</strong> the value of some random variable $y$ as a function of $x$.</p>

<ul>
  <li>e.g. you would like to build a model to estimate the number $y$ of customers arriving in your store (or number of page-views on your website) in any given hour, based on certain features $x$ such as store promotions, recent advertising, weather, day-of-week, etc. (you might think about <strong>Poisson Distribution</strong>, but that is also a GLM)</li>
</ul>

<p>To <strong>derive a GLM</strong> for a problem, we will make the following ==three assumptions== about the conditional distribution of $y$ given $x$ and about our model:</p>

<ol>
  <li>The <strong>distribution</strong> is in the exponential family, such that $y\vert x;\theta \sim \text{Exponential Family}(\eta)$</li>
  <li>The goal is to predict $T(y)$, (in most of the time, $T(y)=y$), so we want to compute $h(x)$. But that $h(x)$ <strong>needs</strong> to be $h_\theta(x)=E[y\vert x;\theta]$</li>
  <li>The natural parameter $\eta$ and inputs $x$ are related <strong>linearly</strong>, so that $\eta = \theta^T x$. (if $\eta$ is a vector, $\eta_i = \theta^T_i x$)</li>
</ol>

<p>Pictorially, you think of the following:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717214958603.png" alt="image-20210717214958603" style="zoom:67%;" /></p>

<p>where:</p>

<ol>
  <li>
    <p>you start with some input $x$</p>
  </li>
  <li>
    <p>you <strong>assume</strong> that it is a <em>linear model</em>, so that there exists some <em>learnable parameter $\theta$</em></p>

    <ul>
      <li>this is the most important assumption here</li>
    </ul>
  </li>
  <li>
    <p>you convert your input to $\theta^Tx = \eta$</p>
  </li>
  <li>
    <p>$\eta$ now will give some <strong>distribution</strong> in the exponential family, by then <strong>choosing</strong> some appropriate $b(y),a(\eta), T(y)$</p>

    <ul>
      <li>basically your <em>task</em> will give you some hint on what distribution to choose. e.g. modelling website clicks/counts -&gt; use Poisson Distribution, etc.</li>
    </ul>
  </li>
  <li>
    <p>Using the exponential family:</p>

    <ul>
      <li>
        <p>your hypothesis function is then simply $h_\theta(x) = E[y;\eta] = E[y;\theta^Tx]=E[y\vert x;\theta]$</p>
      </li>
      <li>
        <p>you want to train it to find $\theta$ by <strong>maximize likelihood</strong>, using the learning update rule</p>

\[\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}_j\]

        <p>note that this holds for ==any distribution in exponential family== (or you could use the <em>Batch Gradient Descent</em>, which adds a summation, or the <em>Newton’s Method</em> that we covered above)</p>
      </li>
    </ul>
  </li>
</ol>

<p>Basically, if you your data <strong>can be modelled</strong> by some distribution in the <strong>Exponential Family</strong>, you can use GLM above and do the learning.</p>

<hr />

<p>==Terminologies==</p>

<ul>
  <li>$\mu = E[y; \eta]= g(\eta)$ is called the <strong>canonical response function</strong>
    <ul>
      <li>recall that $g(\eta) = \frac{\partial}{\partial \eta} a(\eta)$</li>
    </ul>
  </li>
  <li>$\eta = g^{-1}(\mu)$ is called the <strong>canonical link function</strong></li>
  <li>Here, we have <strong>three types of parameters</strong> involved in the GLM,:
    <ul>
      <li><strong>model parameter</strong> $\theta$, which we ==learn in model==</li>
      <li><strong>natural parameter</strong> $\eta$, which is ==assumed== $\eta = \theta^T x$ to be linear</li>
      <li><strong>canonical parameter</strong>, $\phi$ for Bernoulli, $\mu,\sigma$ for Gaussian, $\lambda$ for Poisson, …
        <ul>
          <li>use $g(\eta)$ to get those canonical parameters <em>from</em> natural parameter</li>
          <li>use $g^{-1}$ to swap back to natural parameter <em>from</em> canonical parameter</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="linear-regression-as-glm">Linear Regression as GLM</h3>

<p>Recall that we assumed in Linear Regression:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model:</th>
      <th style="text-align: center">Actual Data:</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717221916347.png" alt="image-20210717221916347" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717221943322.png" alt="image-20210717221943322" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where we model the conditional distribution of $y$ given x as as a Gaussian $N(\mu, \sigma^2)$.</p>

<p>Since we have proven that the Gaussian <strong>is in Exponential Family</strong>, and that we have proven:</p>

<ul>
  <li>$\mu = \eta$</li>
</ul>

<p>Therefore, we get from the second assumption in <a href="#Constructing the GLMs">Constructing the GLMs</a>:</p>

\[\begin{align*}
h_\theta(x)
&amp;= E[y|x; \theta]\\
&amp;= \mu\\
&amp;= \eta \\
&amp;= \theta^T x
\end{align*}\]

<p>where we get back the linear hypothesis:</p>

<ul>
  <li>the second equality comes from the fact that $y\vert x;\theta \sim \mathcal{N}(\mu, \sigma^2)$</li>
  <li>the last equality comes from the third assumption in <a href="#Constructing the GLMs">Constructing the GLMs</a>:</li>
</ul>

<h3 id="logistic-regression-as-glm">Logistic Regression as GLM</h3>

<p>Recall that for logistic regression:</p>

<p><em>Pictorially</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model:</th>
      <th style="text-align: center">Actual Data:</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717223157895.png" alt="image-20210717223157895" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717223206039.png" alt="image-20210717223206039" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where we model the conditional distribution of $y$ given x as as the <em>Bernoulli Distribution</em> $y\vert x;\theta \sim \text{Bernoulli}(\phi)$.</p>

<p>Since we have also proven that Bernoulli Distribution <strong>is in the Exponential Family</strong>, and that:</p>

<ul>
  <li>$\phi = 1/(1+e^{-\eta})$</li>
</ul>

<p>Following a similar derivation from above:</p>

\[\begin{align*}
h_\theta(x)
&amp;= E[y|x; \theta]\\
&amp;= \phi\\
&amp;= 1/(1+e^{-\eta}) \\
&amp;= 1/(1+e^{-\theta^T x})
\end{align*}\]

<p>which gives back the hypothesis for <strong>logistic regression</strong>.</p>

<ul>
  <li>once we assume that $y$ conditioned on $x$ is Bernoulli, the sigmoid function arises as a consequence of the definition of GLMs
and exponential family distributions.</li>
</ul>

<h3 id="softmax-regression">Softmax Regression</h3>

<p>Here we cover the <em>cross entropy</em> interpretation of this method. For the graphical method please refer to the note.</p>

<p>Consider the case when we have to classify <em>more than one class</em></p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717224956677.png" alt="image-20210717224956677" style="zoom: 50%;" /></p>

<p>Then we consider the following <strong>representation</strong> of this task:</p>

<ul>
  <li>we have $k$ classes</li>
  <li>input $x^{(i)} \in \mathbb{R}^n$</li>
  <li>label $y$ is a <em>one hot vector</em>, such that $y={0,1}^k$
    <ul>
      <li>e.g. if there are 3 classes, then being in class 1 means $y=[1,0,0]^T$</li>
    </ul>
  </li>
  <li>each class has its own parameter $\theta_1,\theta_2,…,\theta_k$, or basically $\phi_1, \phi_2, … \phi_k$ (since $\phi = \theta^T x$)
    <ul>
      <li>technically we only need $k-1$ of those parameters, since it must be that $\sum_{i=1}^k \phi_i = 1$</li>
    </ul>
  </li>
</ul>

<p>Then the idea is as follows:</p>

<ol>
  <li>
    <p>Consider that we have already solved the $\theta_1$ for triangle class, $\theta_2$ for square class, etc</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717230536584.png" alt="image-20210717230536584" style="zoom:33%;" /></p>

    <p>where:</p>

    <ul>
      <li>recall that $\theta^T x \mapsto 1/(1+e^{-\theta^Tx})$, which basically gives the probability of being in the class or not
        <ul>
          <li>i.e. $\theta^Tx &gt; 0$ means being in the class</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Then this means that for <em>any given $x$</em>, we can get compute the $\theta_{\text{class}}^T x$:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231025039.png" alt="image-20210717231025039" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>Then we can <strong>convert</strong> this to distribution like values by first making them all positive, applying $\exp$</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231207403.png" alt="image-20210717231207403" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p><strong>Normalize</strong> the distribution to be a <em>probability distribution</em></p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231419649.png" alt="image-20210717231419649" style="zoom:50%;" /></p>

    <p>Therefore, this means that given an input $x$, we are ==outputting a probability distribution==</p>
  </li>
  <li>
    <p>Now, suppose that the actual answer for that $x$ is a triangle, so that $[0,1,0]^T$ is the correct answer. We can convert the answer/label into a distribution as:</p>

    <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231828115.png" alt="image-20210717231828115" style="zoom: 50%;" /></p>

    <p>Therefore, then the ==task/learning algorithm== needs to ==minimize== the ==”difference/distance”=cross entropy between the following two==:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Current Model</th>
          <th style="text-align: center">Target</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231419649.png" alt="image-20210717231419649" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210717231828115.png" alt="image-20210717231828115" style="zoom: 50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>and then output the optimal $\theta_i$ for each class</p>
  </li>
</ol>

<p>Therefore, then we need to define:</p>

<blockquote>
  <p><strong>Cross Entropy</strong></p>

  <ul>
    <li>
      <p>The cross entropy between a prediction $\hat{y}$ and the actual class $y \in {\text{classes}}$</p>

\[\begin{align*}
\text{Cross Entropy}(p,\hat{p}|x)
&amp;=  -\sum_{y \in \text{classes}}p(y) \log \hat{p}(y) \\
&amp;=  -\log \hat{p}(y_{\text{correct class}}) \\
&amp;= -\log \frac{e^{\theta_{\text{correct class}}^T x}}{\sum_{i \in \text{classes}}e^{\theta_i^T x}}
\end{align*}\]

      <p>where:</p>

      <ul>
        <li>notice that iff we guessed it <strong>right</strong>, then this quantity will be zero</li>
        <li>otherwise, this will be <strong>positive due to the $-$ sign</strong></li>
      </ul>
    </li>
    <li>
      <p>Our ==goal== is to minimize this quantity, and then we just use ==gradient descent==</p>
    </li>
  </ul>
</blockquote>

<h1 id="generative-learning-algorithms">Generative Learning Algorithms</h1>

<p>Algorithms that try to <strong>learn $p(y\vert x)$</strong> directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs $X$ to the labels ${0, 1}$, (such as the perceptron algorithm) are called <strong>discriminative learning algorithms</strong>. Here, we’ll talk about algorithms that instead try to <strong>model $p(x\vert y)$</strong> (and $p(y)$). These algorithms are called <strong>generative learning algorithms</strong>.</p>

<ul>
  <li>for example, consider the case of classifying between elephants $y=1$ and dogs $y=0$ . A <em>discriminative algorithm</em> would learn the mapping from input features to the animals. A different approach is to build a <em>feature model for each class</em>, which is called the <em>generative learning algorithm</em></li>
</ul>

<p>The idea is as follows. If we know $p(x\vert y)$ and $p(y)$, then we can use <strong>Bayes’ Rule</strong>:</p>

\[p(y|x) = \frac{p(x|y)p(y)}{p(x)} = \frac{p(x|y)p(y)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}\]

<hr />

<p>For instance, given an input $x$, the probability of it <strong>being an elephant $y=1$</strong> is:</p>

\[p(y=1|x) = \frac{p(x|y=1)p(y=1)}{p(x)}\]

<p>where:</p>

<ul>
  <li>$p(y)$ is called the <strong>class priors</strong>. Basically the probability of something happening regardless of your “features/condition”</li>
  <li>$p(x\vert y)$ is your modelled feature</li>
  <li>$p(x)$ basically indicates the probability of this set of feature $x$ occurring at all in your sample space</li>
</ul>

<hr />

<h2 id="gaussian-discriminative-analysis">Gaussian Discriminative Analysis</h2>

<p>The big assumptions in this model is that $p(x\vert y)$ are distributed as a <strong>Gaussian</strong>. In other words, if you have many features for $x \in \mathbb{R}^n$ (notice we dropped $x_0=1$ since we are now learning $x$ from $y$), then we assume that $p(x\vert y)$ is distributed as a ==Multivariate Gaussian==.</p>

<h3 id="the-multivariate-normal-distribution">The Multivariate Normal Distribution</h3>

<p>This is basically converting the 1-D Gaussian into something like this:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718170727148.png" alt="image-20210718170727148" /></p>

<blockquote>
  <p><strong>Multivariable Normal Distribution</strong></p>

  <ul>
    <li>
      <p>This distribution $\mathcal{N}(\mu, \Sigma)$  is parametrized by a <strong>mean vector</strong> $\mu \in \mathbb{R}^n$ and a <strong>covariance matrix</strong> $\Sigma \in \mathbb{R}^{n \times n}$, where $\Sigma \ge 0$ and is <strong>symmetric and positive semi-definite</strong>.</p>
    </li>
    <li>
      <p>The <strong>density function</strong> is then given by:</p>

\[p(x;\mu, \Sigma) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu) \right)\]

      <p>where obviously now $x \in \mathbb{R}^n$ is a <strong>vector</strong>, and that:</p>

      <ul>
        <li>$\vert \Sigma\vert$ is the <em>discriminant</em> of $\Sigma$</li>
      </ul>
    </li>
    <li>
      <p>The <strong>mean</strong> of a random variable $X \sim \mathcal{N}(\mu, \Sigma)$ is given by:</p>

\[E[X] = \int_x x \,p(x;\mu, \Sigma)dx = \mu\]
    </li>
    <li>
      <p>The <strong>covariance</strong> of a vector-valued random variable $Z$ is defined as $\text{Cov}(Z)$:</p>

\[\text{Cov}(Z) = E[\,(Z-E[Z])(Z-E[Z])^T\,] = E[ZZ^T] - (E[Z])(E[Z])^T = \Sigma\]
    </li>
  </ul>

</blockquote>

<p><em>Examples</em>:</p>

<p>The by varying the covariance $\Sigma$, we could:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; 0 \ 0 &amp; 1\end{bmatrix}$</th>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 0.6&amp; 0 \ 0 &amp; 0.6\end{bmatrix}$</th>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 2&amp; 0 \ 0 &amp; 2\end{bmatrix}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718170727148.png" alt="image-20210718170727148" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192451655.png" alt="image-20210718192451655" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192502695.png" alt="image-20210718192502695" /></td>
    </tr>
  </tbody>
</table>

<p>which makes sense because:</p>

<ul>
  <li>as covariance $\Sigma \to 0.6\Sigma$, the variation becomes smaller and hence a higher peak</li>
  <li>as covariance $\Sigma \to 2\Sigma$, the variation becomes larger and hence a more spread out shape</li>
</ul>

<p>Additionally, we could have non-zero terms to off-diagonal entries</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; 0 \ 0 &amp; 1\end{bmatrix}$</th>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; 0.5 \ 0.5 &amp; 1\end{bmatrix}$</th>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; 0.8 \ 0.8 &amp; 1\end{bmatrix}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193049951.png" alt="image-20210718193049951" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193028380.png" alt="image-20210718193028380" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193038418.png" alt="image-20210718193038418" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192939612.png" alt="image-20210718192939612" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192948650.png" alt="image-20210718192948650" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192957923.png" alt="image-20210718192957923" /></td>
    </tr>
  </tbody>
</table>

<p>where we basically see that:</p>

<ul>
  <li>increasing off-diagonal entries makes the shape more compressed along $x_1=x_2$ $\to$ increases the <strong>positive correlation between $x_1, x_2$</strong></li>
  <li>technically bottom left contour plots should be <em>circles</em>. Due to aspect ratio problems, they are rendered as ellipses</li>
</ul>

<p>On the contrary, having negative entries creates puts $x_1, x_2$ to have <strong>negative correlation</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; 0 \ 0 &amp; 1\end{bmatrix}$</th>
      <th style="text-align: center">$\Sigma=\begin{bmatrix} 1&amp; -0.8 \ -0.8 &amp; 1\end{bmatrix}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718192939612.png" alt="image-20210718192939612" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193550047.png" alt="image-20210718193550047" /></td>
    </tr>
  </tbody>
</table>

<p>Lastly, varying $\mu$ basically <strong>shifts the distribution</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\mu = \begin{bmatrix}1\0\end{bmatrix}$</th>
      <th style="text-align: center">$\mu = \begin{bmatrix}-0.5\0\end{bmatrix}$</th>
      <th style="text-align: center">$\mu = \begin{bmatrix}-1\-1.5\end{bmatrix}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718170727148.png" alt="image-20210718170727148" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193730843.png" alt="image-20210718193730843" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210718193740822.png" alt="image-20210718193740822" /></td>
    </tr>
  </tbody>
</table>

<h3 id="the-gaussian-discriminative-analysis-model">The Gaussian Discriminative Analysis Model</h3>

<p>Consider the case when we have classification for <em>benign tumors</em> $y=0$ and <em>malignant tumors</em> $y=1$ based on a set of features $x$.</p>

<p>Then if the input features are <em>continuous variables</em>, we start by ==assuming== the following distribution:</p>

\[\begin{align*}
y &amp;\sim \text{Bernoulli}(\phi) \\
x|y =0 &amp;\sim \mathcal{N}(\mu_0, \Sigma) \\
x|y =1 &amp;\sim \mathcal{N}(\mu_1, \Sigma)
\end{align*}\]

<p>or equivalently:</p>

\[\begin{align*}
p(y) &amp;= \phi^y (1-\phi)^{1-y}  \\
p(x|y=0) &amp;= \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_0)^T \Sigma^{-1}(x-\mu_0) \right) \\
p(x|y=1) &amp;= \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right) \\
\end{align*}\]

<p>where notice that the parameters of the models are:</p>

<ul>
  <li>$\phi, \Sigma, \mu_0, \mu_1$, so that they <strong>share</strong> the same covariance matrix
    <ul>
      <li>this is assumed that $\Sigma_0=\Sigma_1=\Sigma$, which is <em>usually</em> done, but it does <em>not</em> have to be.</li>
    </ul>
  </li>
</ul>

<p>Therefore, given a training set ${(x^{(i)}, (y^{(i)})}_{i=1}^m$, we consider <strong>maximizing the (log) Joint Likelihood</strong></p>

\[\begin{align*}
L(\phi, \Sigma, \mu_0, \mu_1)
&amp;= \prod_{i=1}^m p(x^{(i)}, y^{(i)} ; \phi, \mu_0, \mu_1, \Sigma) \\
&amp;= \prod_{i=1}^m p(x^{(i)}|y^{(i)} ; \phi, \mu_0, \mu_1, \Sigma)p(y^{(i)};\Sigma)
\end{align*}\]

<p>where notice that:</p>

<ul>
  <li>now we consider $p(x^{(i)}, y^{(i)} ; \phi, \mu_0, \mu_1, \Sigma)$, which is the probability of $x^{(i)}$ AND $y^{(i)}$ <em>happening at the same time</em>, instead of the linear models which maximizes the <em>conditional probability</em> $p(y\vert x)$</li>
</ul>

<blockquote>
  <p><strong>GDA Model</strong></p>

  <ul>
    <li>
      <p>Consider the case when we have a classification problem where features $x$ are all continuous values. Then we can consider the following distribution:</p>

\[\begin{align*}
y &amp;\sim \text{Bernoulli}(\phi) \\
x|y =0 &amp;\sim \mathcal{N}(\mu_0, \Sigma) \\
x|y =1 &amp;\sim \mathcal{N}(\mu_1, \Sigma)
\end{align*}\]

      <p>And then <strong>maximize the log of the Joint Likelihood</strong></p>

\[\begin{align*}
l(\phi, \Sigma, \mu_0, \mu_1)
=\log L(\phi, \Sigma, \mu_0, \mu_1)
= \log \prod_{i=1}^m p(x^{(i)}|y^{(i)} ; \phi, \mu_0, \mu_1, \Sigma)p(y^{(i)};\Sigma)
\end{align*}\]
    </li>
    <li>
      <p>By maximizing the log Joint Likelihood (taking derivatives and setting it to zero), you will find that:</p>

\[\begin{align*}
\phi &amp;= \frac{1}{m} \sum_{i=0}^m y^{(i)} = \frac{1}{m} \sum_{i=0}^m 1\{y^{(i)}=1\} \\
\mu_0 &amp;= \frac{\sum_{i=1}^m 1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=0\}} \\
\mu_1 &amp;= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=1\}} \\
\Sigma &amp;= \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T
\end{align*}\]

      <p>where:</p>

      <ul>
        <li>the indicator function behaves as $1{\text{true}}=1$, and $1{\text{false}}=0$</li>
        <li>the $\phi$ quantity just computes the <strong>average number of patients with malignant tumor</strong></li>
        <li>the $\mu_0$ computes the <strong>mean of all feature vectors $x$ that corresponds to a benign tumor $y=0$</strong></li>
        <li>the $\mu_1$ computes the <strong>mean of all feature vectors $x$ that corresponds to a malignant tumor $y=1$</strong></li>
        <li>the $\Sigma$ computes the covariance of all feature vectors from their corresponding $\mu_{y^{(i)}}$</li>
      </ul>
    </li>
    <li>
      <p>Therefore, to make a ==prediction given some $x$==, we compute the quantity:</p>

\[\arg\max_y p(y|x) = \arg\max_y \frac{p(x|y)p(y)}{p(x)} =  \arg\max_y p(x|y)p(y)\]

      <p>where:</p>

      <ul>
        <li>since $p(x)$ is a constant given a $x$, in $\arg \max$ it does not matter</li>
      </ul>
    </li>
  </ul>

  <hr />

  <p><em>Reminder:</em></p>

  <ul>
    <li>
      <p>$\arg \max$ or $\arg \min$ works as follows:</p>

\[\begin{align*}
\min (z-5)^2 &amp;= 0 \\
\arg \min_z (z-5)^2 &amp;= 5
\end{align*}\]

      <p>where basically you care about the <em>argument</em> instead of the <em>output</em></p>
    </li>
  </ul>

  <hr />
</blockquote>

<p>Pictorially, the computation for $\phi, \mu_0, \mu_1, \Sigma$ is doing the following:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720103733751.png" alt="image-20210720103733751" /></p>

<p>where:</p>

<ul>
  <li>$\Sigma$ fits the contours/shapes of the two Gaussian
    <ul>
      <li>made the two Gaussians have contours that are the same shape and orientation</li>
      <li>one outcome is that the <em>decision boundary will be linear</em></li>
    </ul>
  </li>
  <li>$\mu_0, \mu_1$ shifts the Gaussian to the “best” place</li>
  <li>$\phi$ draws the decision boundary
    <ul>
      <li>technically it is unknown already once the Gaussians are in place, i.e. you know which $y$ is more probable given an $x$</li>
    </ul>
  </li>
</ul>

<h3 id="gda-and-logistics-regression">GDA and Logistics Regression</h3>

<p>First let’s recall what each algorithm does.</p>

<p>Beginning with the data:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720213613215.png" alt="image-20210720213613215" style="zoom:25%;" /></p>

<p>Then <strong>logistic regression</strong> does:</p>

<table>
  <thead>
    <tr>
      <th>Iteration 1. initialize randomly</th>
      <th style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720213938496.png" alt="image-20210720213938496" style="zoom: 67%;" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Iteration 2.</td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720214008073.png" alt="image-20210720214008073" style="zoom:67%;" /></td>
    </tr>
    <tr>
      <td>Iteration …</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Iteration 20.</td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720213957351.png" alt="image-20210720213957351" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>The <strong>GDA Model</strong> does:</p>

<table>
  <thead>
    <tr>
      <th>Step 1. Fit Gaussian for each label</th>
      <th style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720214114067.png" alt="image-20210720214114067" style="zoom:67%;" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Step 2. Decision boundary is implied by the probability for $y$ of each $x$</td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720214226659.png" alt="image-20210720214226659" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Comparison of the two algorithms:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720214310580.png" alt="image-20210720214310580" style="zoom:33%;" /></p>

<hr />

<p>On the other hand, if we consider the function from GDA, which considers $p(x\vert y)$, and the Bayes Rule:</p>

\[p(y|x) =  \frac{p(x|y)p(y)}{p(x)}\]

<p>Consider viewing the quantity $p(y=1\vert x; \Sigma,\mu_0, \mu_1, \phi)$ as a function of $x$, then the following data:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720215828019.png" alt="image-20210720215828019" style="zoom:33%;" /></p>

<p>would compute to:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720215921296.png" alt="image-20210720215921296" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>basically points to the left has a <em>very low probability of being $y=1$</em></li>
  <li>points to the far right is almost certainly $y=1$</li>
</ul>

<p>In fact, it can be proven that this is <em>exactly a sigmoid function</em></p>

\[p(y=1|x,\phi, \Sigma, \mu_0, \mu_1) = \frac{1}{1+\exp(-\theta^T x)}\]

<p>for $\theta$ being some appropriate function of $\phi, \Sigma, \mu_0, \mu_1$.</p>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>
      <p>The above basically argues that if $p(x\vert y)$ is <strong>multivariate gaussian</strong> (with shared $\Sigma$), i.e.</p>

\[\begin{align*}
y &amp;\sim \text{Bernoulli}(\phi) \\
x|y =0 &amp;\sim \mathcal{N}(\mu_0, \Sigma) \\
x|y =1 &amp;\sim \mathcal{N}(\mu_1, \Sigma)
\end{align*}\]

      <p>then $p(y\vert x)$ ==necessarily== follows a logistic function:</p>

\[\begin{align*}
p(y=1|x) &amp;= \frac{1}{1+e^{-\theta^T x}}\\
p(y=0|x) &amp;=1- \frac{1}{1+e^{-\theta^T x}} 
\end{align*}\]

      <p>(in fact, this is true for any pair of distributions from the <em>exponential family</em>)</p>
    </li>
    <li>
      <p>The ==converse==, however, is ==not true==. So in a way the GDA algorithm makes a <strong>stronger set of assumptions</strong>.</p>

      <ul>
        <li>but this means that if assumptions in GDA are <em>correct</em>, then GDA does <strong>better</strong> than logistic regression since you are telling it more information.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="naïve-bayes">Naïve Bayes</h2>

<p>In GDA, the feature vectors $x$ were continuous, real-valued vectors. Let’s now talk about a different learning algorithm in which the $x$’s are <strong>discrete valued</strong>.</p>

<p>For instance, consider the case of building a <em>email span detector</em>. First you will need to <em>encode</em> text-data into feature vectors. One way to do this is to represent an email by a vector whose length is the number of words in a dictionary:</p>

<ul>
  <li>if an email contains the $i$-th word of the dictionary, then we will set $x_i = 1$. e.g.:</li>
</ul>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210720223041878.png" alt="image-20210720223041878" /></p>

<ul>
  <li>so that $x \in {0,1}^n$</li>
</ul>

<p>The task is to model $p(x\vert y)$ and $p(y)$ for this generative algorithms.</p>

<ul>
  <li>note that if the size of vocabulary (number of words encoded in the feature vector) is $10,000$, then other models might not work well since you have excessive number of parameters.</li>
</ul>

<hr />

<p><em>Reminder</em></p>

<ul>
  <li>
    <p>by the <strong>chain rule of probability</strong>, we know that:</p>

\[p(x_1, x_2, ...,x_{10,000} | y) = p(x_1|y)\,p(x_2|x_1,y)\,p(x_3|x_1,x_2,y)...p(x_{10,000}|...)\]
  </li>
</ul>

<hr />

<blockquote>
  <p><strong>Naive Bayes Algorithm</strong></p>

  <p>Therefore, in the Naïve Bayes algorithm, we:</p>

  <ul>
    <li>
      <p>==assume== that $x_i$ are <strong>conditionally independent</strong> given $y$:</p>

\[p(x_1, x_2, ...,x_{10,000} | y) = p(x_1|y)\,p(x_2|y)\,p(x_3|y)...p(x_{10,000}|y) = \prod_{i=1}^{10000}p(x_i|y)\]

      <p>which means that:</p>

      <ul>
        <li>given a $y$, e.g. the email being a spam, the chance of the word $\text{buy}$ appearing ==does not depend on== whether if the word $\text{price}$ has appeared in the email (in reality this is usually not true)</li>
      </ul>

      <p>This assumption is also called the <strong>Naïve Bayes Assumption</strong></p>
    </li>
    <li>
      <p>then we can consider the following parameters:</p>

\[\begin{align*}
\phi_{j|y=1} &amp;= p(x_j=1|y=1) \\
\phi_{j|y=0} &amp;= p(x_j=1|y=0)\\
\phi_y &amp;= p(y=1)
\end{align*}\]

      <p>where:</p>

      <ul>
        <li>$y=1$ represents email being spam</li>
        <li>$\phi_y = p(y=1)$ is the class prior</li>
      </ul>
    </li>
    <li>
      <p>hence the <strong>joint likelihood</strong> becomes:</p>

\[L(\phi_y, \phi_{j|y=0}, \phi_{j|y=1}) = \prod_{i=1}^m p(x^{(i)},y^{(i)};\phi_y,\phi_{j|y})\]

      <p>and solving for the <strong>Maximized Likelihood Estimate</strong> becomes:</p>

\[\begin{align*}
\phi_y &amp;= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}}{m} \\
\phi_{j|y=1} &amp;= \frac{\sum_{i=1}^m1\{x_j^{(i)}=1 \and y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}\\
\phi_{j|y=0} &amp;= \frac{\sum_{i=1}^m1\{x_j^{(i)}=1 \and y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}
\end{align*}\]

      <p>where the results are easy to interpret</p>

      <ul>
        <li>For instance, $\phi_{j\vert y=1}$ is just the fraction of the spam ($y = 1$) emails in which word $j$ does appear.</li>
      </ul>
    </li>
    <li>
      <p>==Finally==, now if we need to <strong>compute probability $y=1$</strong> given $x$, we just need to consider:</p>

\[\begin{align*}
p(y=1|x) &amp;= \frac{p(x|y=1)\,p(y=1)}{p(x)}\\
&amp;= \frac{\left( \prod_{i=1}^n p(x_i | y=1) \right)p(y=1)}{\left( \prod_{i=1}^n p(x_i | y=1) \right)p(y=1) + \left( \prod_{i=1}^n p(x_i | y=0) \right)p(y=0)}
\end{align*}\]
    </li>
  </ul>

</blockquote>

<p><strong>Advantage</strong></p>

<ul>
  <li>this is very fast since there is almost no iterative stuff about it. All it needs to do is to count.</li>
</ul>

<p><strong>Disadvantage</strong></p>

<ul>
  <li>
    <p>Naive Bayes assumption is usually not true</p>
  </li>
  <li>
    <p>A more critical problem is when you get a <strong>new word</strong>, e.g. <code class="language-plaintext highlighter-rouge">NIPS</code>, which corresponds to, say $x_{6017}$. Then you will get:</p>

\[\begin{align*}
p(y=1|x) &amp;= \frac{p(x|y=1)\,p(y=1)}{p(x)}\\
&amp;= \frac{\left( \prod_{i=j}^n p(x_j | y=1) \right)p(y=1)}{\left( \prod_{j=1}^n p(x_j | y=1) \right)p(y=1) + \left( \prod_{j=1}^n p(x_j | y=0) \right)p(y=0)}\\
&amp;= \frac{0}{0+0}
\end{align*}\]

    <p>since $x_{j=6017}$ has never appeared in any past emails, hence $p(x_{6017}\vert y=1) = p(x_{6017}\vert y=0) = 0$</p>
  </li>
</ul>

<h3 id="laplace-smoothing">Laplace Smoothing</h3>

<p>The idea for this technique is to <strong>address the above problem of Naive Bayes</strong>, such that having a new sample would not lead to $\frac{0}{0+0}$.</p>

<ul>
  <li>In general, it is statistically a bad idea to estimate the probability of some event to be zero just because you <em>haven’t seen it before</em> in your finite training set.</li>
</ul>

<p>Consider the case where you have observed your school Football Team’s performance:</p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Against</th>
      <th>Win=1/Lose=0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>09/12</td>
      <td>AAA</td>
      <td>0</td>
    </tr>
    <tr>
      <td>09/13</td>
      <td>BBB</td>
      <td>0</td>
    </tr>
    <tr>
      <td>09/14</td>
      <td>CCC</td>
      <td>0</td>
    </tr>
    <tr>
      <td>09/15</td>
      <td>DDD</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p>where if you think about the probability of the game at 09/15, then your estimate might be:</p>

<ul>
  <li>$p(x=1) = \frac{\text{# Wins}}{\text{# Wins} + \text{# Loses}} = \frac{0}{0+3}=0$​</li>
</ul>

<p>However, this might not be a good idea just because you haven’t seen a win.</p>

<blockquote>
  <p><strong>Laplace Smoothing</strong></p>

  <ul>
    <li>
      <p>The idea is simple: even if you haven’t seen an event, pretend you did. Therefore, you add $1$​ to $\text{# Win}$​, and <strong>also</strong> to all the other cases (to balance out). Therefore, you end up predicting:</p>

\[\frac{(0+1)}{(0+1)+(3+1)}=\frac{1}{5}\]
    </li>
    <li>
      <p>More generally, if you have $k$​ possible outcomes $x \in {1,2,…k}$​, then estimating:</p>

\[p(x=j) = \frac{\sum_{i=1}^m 1\{ x^{(i)} = j\} + 1}{m + k}\]
    </li>
    <li>
      <p>This turns out to be the <em>optimal smoothing</em> if data is distributed uniformly in Bayesian Statistics.</p>
    </li>
  </ul>
</blockquote>

<p>Therefore, <strong>applying this to Naïve Bayes</strong></p>

\[\begin{align*}
\phi_y &amp;= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\}}{m} \\
\phi_{j|y=1} &amp;= \frac{\sum_{i=1}^m1\{x_j^{(i)}=1 \and y^{(i)}=1\} + 1}{\sum_{i=1}^m1\{y^{(i)}=1\} + 2}\\
\phi_{j|y=0} &amp;= \frac{\sum_{i=1}^m1\{x_j^{(i)}=1 \and y^{(i)}=0\} + 1}{\sum_{i=1}^m1\{y^{(i)}=0\} + 2}
\end{align*}\]

<p>and notice that:</p>

\[\sum_{j=1}^{k} \phi_j = 1\]

<p><strong>still holds</strong>.</p>

<h3 id="event-model-for-text-classification">Event Model for Text Classification</h3>

<p><em>Recall that for Naïve Bayes above (also called Multivariate Bernoulli Model)</em>:</p>

<ol>
  <li>we first assumed that emails generated are randomly determined to be spam or none spam (according to the class priors $p(y)$)</li>
  <li>then we assumed that the words in the emails are also chosen <em>independently</em> according to the probability $p(x_i=1\vert y)=\phi_{i\vert y}$​</li>
</ol>

<p>Another shortcoming for this is that we are only dealing with <em>each word for once</em>, hence ignoring some information which might be useful.</p>

<hr />

<p>Here. we consider a <strong>multinomial event model</strong>, which states that:</p>

<ol>
  <li>Let $x_i$​ denote the identity of the $i$​-th word in the email. This means that an email of $n$​ words is now represented by a vector $[x_1, x_2,\dots , x_n]^T$​ of length $n$​​. For instance, if an email starts with “A NIPS . . . ,” then $x_1 = 1$​ (“a” is the first word in the dictionary), and $x_2 = 35000$​.
    <ul>
      <li>therefore, this means that inputs are of <strong>variable size</strong></li>
    </ul>
  </li>
  <li>Then we assume that emails generated are randomly determined to be spam or none spam (according to the class priors $p(y)$​)
    <ul>
      <li>this is the same as before</li>
    </ul>
  </li>
  <li>Then we assume that the sender composes the email by first generating $x_1$​​ from some <strong>multinomial distribution</strong> over words $p(x_1\vert y)$​​​. Next the second word $x_2$ is chosen <strong>independently</strong> of $x_1$ but from the same distribution, i.e. $p(x_2\vert y)$.</li>
</ol>

<p>Therefore, we can say a similar assumption from the Naïve Bayes:</p>

\[\begin{align*}
p(x) &amp;= p(x|y)p(y) \\
&amp;= p(y)\prod_{i=1}^n p(x_i | y)
\end{align*}\]

<p>This means that we have the following parameters:</p>

\[\begin{align*}
\phi_{y} &amp;\equiv p(y=1)\\
\phi_{k|y=1} &amp;\equiv p(x_j=k|y=1)\\
\phi_{k|y=0} &amp;\equiv p(x_j=k|y=0)
\end{align*}\]

<p>where:</p>

<ul>
  <li>this looks similar to the Naive Bayes before, but with a different meaning: $p(x_j=k\vert y=0)$​ means the <strong>chance of the $i$​-th word of a message $x$​​ being the $k$​-th word of the dictionary given that the email is not spam ($y=0$​​)</strong></li>
  <li>notice that $\phi_{k\vert y=0} \equiv p(x_j=k\vert y=0)$ has $j$ not appearing on the LHS, we assumed that the chance of $x_1=k$ is the same as $x_2=k$​, etc.</li>
</ul>

<p>Therefore, given a new email $x$, you can compute everything if you know the $\phi_{y}, \phi_{k\vert y=1}, \phi_{k\vert y=0}$​, by considering:</p>

\[\begin{align*}
p(y=1|x) &amp;= \frac{p(x|y=1)\,p(y=1)}{p(x)}\\
&amp;= \frac{\left( \prod_{i=1}^n p(x_i | y=1) \right)p(y=1)}{\left( \prod_{i=1}^n p(x_i | y=1) \right)p(y=1) + \left( \prod_{i=1}^n p(x_i | y=0) \right)p(y=0)}
\end{align*}\]

<blockquote>
  <p><strong>Multinomial Event Model</strong></p>

  <ul>
    <li>
      <p>Therefore, given the traning set ${(x^{(i)},y^{(i)});i=1,…,m}$ where $x^{(i)}$ is of variable size $(x_1^{(i)},…,x_{n_i}^{(i)})$, then we consider the <strong>likelihood of the parameter given the dataset​</strong> being:</p>

\[\begin{align*}
\mathcal{L}(\phi_y,\phi_{k|y=0}, \phi_{k|y=1})
&amp;= \prod_{i=1}^m p(x^{(i)}, y^{(i)})\\
&amp;= \prod_{i=1}^m \left( p(y^{(i)};\phi_y) \prod_j^{n_i} p(x^{(i)}_j|y^{(i)} ; \phi_{k|y=0},\phi_{k|y=1})\right)
\end{align*}\]

      <p>Maximizing the <strong>likelihood estimates</strong> we get:</p>

\[\begin{align*}
\phi_{k|y=1} &amp;= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i}1\{x_j^{(i)}=k \and y^{(i)} = 1\}}{\sum_i^m 1\{y^{(i)}=1\}n_i}\\
\phi_{k|y=0} &amp;= \frac{\sum_{i=1}^m \sum_{j=1}^{n_i}1\{x_j^{(i)}=k \and y^{(i)} = 0\}}{\sum_i^m 1\{y^{(i)}=0\}n_i}\\
\phi_{y} &amp;= \frac{\sum_{i=1}^m 1\{y^{(i)}=1\} }{m}\\
\end{align*}\]

      <p>where basically:</p>

      <ul>
        <li>$\phi_{k\vert y=1}$ means the number of <strong>word $k$​​ appearing in all spam email</strong> divided by the total number of words in all spam email</li>
        <li>other parameters are obvious</li>
      </ul>
    </li>
    <li>
      <p>Using Laplace Smoothing, if our dictionary is of size $10,000$ words, then:</p>

\[\phi_{k|y=1} = \frac{1+\sum_{i=1}^m \sum_{j=1}^{n_i}1\{x_j^{(i)}=k \and y^{(i)} = 1\}}{10,000 + \sum_i^m 1\{y^{(i)}=1\}n_i}\]

      <p>because:</p>

      <ul>
        <li>the denominator basically indicates the $p(x_j = k \vert y=1)$, which has <strong>$10,000$ possible outcomes</strong> since our dictionary is of size $10,000$</li>
        <li>in other words, you can think about the case when you have <em>no dataset</em>, then the probability of a new spam email having $j$-th word being $k$ is $p(x_j = k \vert y=1) = 1/10,000$​.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h1 id="support-vector-machines">Support Vector Machines</h1>

<p>One advantage of using SVM is that it can fit <strong>nonlinear boundaries</strong></p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210724231200206.png" alt="image-20210724231200206" style="zoom: 67%;" /></p>

<p>The idea is that this thing can:</p>

<ul>
  <li>take some input features, say two dimensional $[x_1,x_2]$, and map them to some higher <em>dimensional feature</em> $[x_1,x_2,x_1^3,x_2^{-5}]$, etc.</li>
  <li>then do a <em>linear classifier</em> on those higher dimensional features</li>
</ul>

<p>(This model is in general great, not as good as neural networks, but easy to use and quite robust.)</p>

<h2 id="margins-intuitions">Margins: Intuitions</h2>

<p>We’ll start our story on SVMs by talking about margins. In general, there are two ways to think about <em>confidence</em> of a prediction:</p>

<ul>
  <li>Functional Margin</li>
  <li>Geometric Margin</li>
</ul>

<blockquote>
  <p><strong>Functional Margin Intuition</strong></p>

  <ul>
    <li>
      <p>Consider the case for a <em>logistic regression</em>, where we defined $g(z) = \frac{1}{1+e^{-z}}$ and that $h_\theta(x) = g(\theta^Tx)$. Since the output of this model is between $(0,1)$, then we would ==want== that, <strong>for a training sample $x$</strong>:</p>

\[\begin{cases}
\theta^T x \gg 0, &amp;\text{if $y$ turns out to be $1$}\\
\theta^T x \ll 0, &amp;\text{if $y$ turns out to be $0$}
\end{cases}\]

      <p>in other words, if $h_\theta(x)$ predicts probability very close to $1$ if $y$ is actually $1$, and vice versa.</p>
    </li>
    <li>
      <p>This seems to be a nice goal to aim for, and we will soon formalize this idea</p>
    </li>
  </ul>
</blockquote>

<p>Another idea is to think about it geometrically</p>

<blockquote>
  <p><strong>Geometric Margin Intuition</strong></p>

  <ul>
    <li>
      <p>In a similar case, consider two working hypothesis</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210724234019145.png" alt="image-20210724234019145" style="zoom:67%;" /></p>

      <p>where:</p>

      <ul>
        <li>the <strong>black</strong> lines seems to be <em>better</em> than the <strong>red</strong> line, because points are <strong>further away</strong> from the decision boundary.</li>
        <li>in a sense this idea is similar to the functional margin</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="notations">Notations</h2>

<p>To make our discussion of SVMs easier, we’ll first need to introduce a new notation for talking about classification.</p>

<ul>
  <li><strong>Labels:</strong> $y \in {-1,+1}$​ for binary class.
    <ul>
      <li>Therefore, $g(z)=1$ if $z\ge 0$, and $g(z)=-1$ if $z &lt; 0$​, where $g$ is the sigmoid function</li>
    </ul>
  </li>
  <li><strong>Hypothesis</strong>: $h_{w,b}(x) = g(w^T x + b)$​​
    <ul>
      <li>now we remove the $x_0=1$​ from $x$​, and made it to be $b$​​ explicitly</li>
      <li>therefore, basically $[\theta_0, \theta_1,\dots,\theta_n] \mapsto [b,w_1,\dots,w_n]$​​</li>
    </ul>
  </li>
</ul>

<h2 id="functional-and-geometric-margins">Functional and Geometric Margins</h2>

<p>Now we can <strong>formalize</strong> the idea of functional and geometrical margins.</p>

<ul>
  <li>for this section, we ==assume== that the data is ==linear separable==</li>
</ul>

<blockquote>
  <p><strong>Functional Margin</strong></p>

  <ul>
    <li>
      <p>Given a training sample $(x^{(i)},y^{(i)})$​, we define the <strong>functional margin​ $\hat{\gamma}$ of parameters</strong> $(w,b)$​ to be:</p>

\[\hat{\gamma}^{(i)} \equiv y^{(i)}(w^Tx + b)\]

      <p>where basically we want to <em>maximize $\hat{\gamma}$</em> because we <strong>want</strong>:</p>

      <ul>
        <li>when $y^{(i)}=1$​​, then $(w^Tx + b) \gg 0$​​</li>
        <li>when $y^{(i)}=-1$​​, then $(w^Tx + b) \ll 0$​</li>
        <li>and $\hat{\gamma} &gt; 0$ means prediction is correct</li>
      </ul>

      <p>Therefore, larger $\hat{\gamma}$​ means <strong>more confidence and correct</strong> prediction</p>
    </li>
    <li>
      <p>Therefore, the functional margin for a <strong>given dataset</strong> is defined to be:</p>

\[\hat{\gamma} = \min_{i =1,...,m} \hat{\gamma}^{(i)}\]

      <p>where basically we are taking the <strong>worst</strong> margin.</p>

      <ul>
        <li>and assuming that it is linearly separable</li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />

<p><strong>Note</strong></p>

<ul>
  <li>You might want to also impose the constraint that $(w,b) \to (w/\vert \vert w\vert \vert , b/\vert \vert w\vert \vert )$​ to prevent the case where you can increase $\hat{\gamma}$​ by just having $w \to 10w, \, b \to 10b$​​ without really changing anything (same line equation).</li>
</ul>

<hr />

<blockquote>
  <p><strong>Geometric Margin</strong></p>

  <ul>
    <li>
      <p>Given some training samples $(x^{(i)},y^{(i)})$​, we consider the hypothesis:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210725000653700.png" alt="image-20210725000653700" style="zoom: 67%;" /></p>

      <p>where for a single data point $A$​, the geometric margin is the <strong>Euclidean distance</strong> of it, line $AB$, from the hyperplane/line</p>
    </li>
    <li>
      <p>Therefore, we can formalize this to be:</p>

\[\gamma^{(i)} = \frac{w^T x^{(i)} + b}{||w||}\]

      <p>where this is for the <em>positive example</em>, which is also proven below.</p>

      <p>To be generalized, and notice that $w^T x^{(i)} + b = \hat{\gamma}^{(i)}$ which is the <strong>functional margin</strong>, we hence get:</p>

\[\gamma^{(i)} = \frac{\hat{\gamma}^{(i)}}{||w||}\]

      <p>which clarifies the relationship between the two margins.</p>
    </li>
    <li>
      <p>Finally, for the <strong>entire</strong> training set, we have the geometric margin being:</p>

\[\gamma = \min_{i=1,...,m} \gamma^{(i)}\]
    </li>
  </ul>

</blockquote>

<hr />

<p><em>Proof</em></p>

<p>Consider the point $B$, which lies on the boundary. This point $B$ can be calculated by:</p>

\[x^{(i)} - \gamma^{(i)} \frac{w}{||w||}\]

<p>Since this point lies on the line, this means that:</p>

\[w^T\left( x^{(i)} - \gamma^{(i)} \frac{w}{||w||} \right) + b = 0\]

<p>Therefore, solving for the above yields:</p>

\[\gamma^{(i)} = \frac{w^T x^{(i)} + b}{||w||}\]

<hr />

<h2 id="the-optimal-margin-classifier">The Optimal Margin Classifier</h2>

<p>Again, for now we ==assume== that the data is ==linearly separable==.</p>

<p>Now, the sensible idea for a <em>good</em> fit is to find a decision boundary that <strong>maximizes</strong> the <strong>geometric margin</strong>.</p>

<p>Mathematically, to maximize it, we can pose the following problem:</p>

\[\begin{align*}
\max_{\gamma, w,b} &amp;\quad \gamma\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge \gamma, \quad i=1,...,m\\
&amp;\quad ||w||=1
\end{align*}\]

<p>where:</p>

<ul>
  <li>i.e. we want to <strong>maximize $\gamma$</strong>, such that each training example has geometric/functional margin of at least $\gamma$​.</li>
  <li>notice that since $\vert \vert w\vert \vert =1$, functional margin is the same as geometric margin.</li>
</ul>

<p>However, this turns out to be very difficult to solve since $\vert \vert w\vert \vert =1$ constraint is a non-convex one.</p>

<p>It turns out that the above problem can be <strong>rephrased</strong> as the following:</p>

\[\begin{align*}
\min_{w,b} &amp;\quad \frac{1}{2}||w||^2\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1,\quad i=1,...,m
\end{align*}\]

<p>whose solution of $w,b$ gives the <strong>optimal margin classifier</strong>.</p>

<hr />

<p><em>Proof</em></p>

<p>First, we consider moving the $\vert \vert w\vert \vert$ away from the constraint. Recall that $\gamma = \hat{\gamma}/\vert \vert w\vert \vert$:</p>

\[\begin{align*}
\max_{\gamma, w,b} &amp;\quad \frac{\hat{\gamma}}{||w||}\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge \hat{\gamma},\quad  i=1,...,m
\end{align*}\]

<p>which introduced the problem to the $\max$ statement.</p>

<p>Now, recall that scaling $w,b$​ has the same as scaling $\hat{\gamma}$​ without changing anything. Therefore, we can impose a <strong>scaling constraint on $w,b$​</strong> w.r.t. the training set such that the <strong>functional margin $\hat{\gamma}$​ must be 1</strong>. Now we can change the problem to ==forcing $\vert \vert w\vert \vert  = 1/ \gamma$​​==. Therefore, the problem becomes:</p>

\[\begin{align*}
\max_{\gamma, w,b} &amp;\quad \frac{1}{||w||}\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1,\quad  i=1,...,m
\end{align*}\]

<table>
  <tbody>
    <tr>
      <td>so instead of maximizing $1/</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>$​, you can <strong>minimize</strong> $$</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>^2$$. Adding a coefficient to make derivatives easier:</td>
    </tr>
  </tbody>
</table>

\[\begin{align*}
\min_{w,b} &amp;\quad \frac{1}{2}||w||^2\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1,\quad i=1,...,m
\end{align*}\]

<p>which can be <strong>more easily solved</strong>, since now you have $w$ becoming a <strong>concave function</strong></p>

<hr />

<p>Now, what if the size of feature is much much greater than we thought? Consider the case that $x \in \mathbb{R}^{10,000}$​. To do something with this, we need to ==assume== that the <strong>parameter $w$</strong> is some linear combination of data points $x^{(i)}$:</p>

\[w = \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}\]

<p>where:</p>

<ul>
  <li>$\alpha_i$​ is some coefficient $\in \mathbb{R}$</li>
  <li>$y^{(i)}$ in the label. In this example we assume $y^{(i)} \in {-1,+1}$​</li>
  <li>$x^{(i)}$ is the $i$-th input feature</li>
</ul>

<blockquote>
  <p><em>Intuition</em></p>

  <ul>
    <li>
      <p>This assumption is actually <strong>not</strong> that absurd. Recall that in (stochastic) <strong>gradient descent</strong> algorithms that we did, we chose to update $\theta$ by:</p>

\[\begin{align*}
\theta &amp;:=\theta - \alpha  \left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}\\
\end{align*}\]

      <p>since we initialized $\theta = 0$​​, this is essentially a <strong>linear combination</strong> of all data points (this is also true for batch gradient descent).</p>
    </li>
    <li>
      <p>The graphical idea is that your $w$​ should <strong>always</strong> be in the <strong>span of $x^{(i)}$​</strong>, i.e. $\text{Span}{x^{(1)}, …, x^{(m)}}$​</p>

      <p>For example, if you have three features yet they only lie in a 2D space, then:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210726232153196.png" alt="image-20210726232153196" style="zoom: 33%;" /></p>

      <p>where notice that your $w$​ hence should be in the $\text{Span}{x^{(1)}, …, x^{(m)}}$​</p>
    </li>
  </ul>
</blockquote>

<p>The formal proof of this is called the <strong>representor theorem</strong>, which is much more complicated than needed here.</p>

<p>Therefore, by <em>plugging in the new assumption</em>, we can then rewrite the optimal margin problem to be:</p>

\[\begin{align*}
\min_{w,b} &amp;\quad \frac{1}{2}\left( \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}\right)^T \left( \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}\right)\\
\text{s.t.} &amp;\quad y^{(i)}\left(\left( \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}\right)^T x^{(i)} + b\right) \ge 1,\quad i=1,...,m
\end{align*}\]

<p>using the (almost) <strong>braket notation</strong> (recall from quantum mechanics), which states $\langle{x^{(i)}},{x^{(i)}}\rangle=x^{(i)^T}x^{(i)}$​, we then have:</p>

\[\begin{align*}
\min_{\alpha} &amp;\quad \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_jy^{(i)}y^{(j)} \lang x^{(i)},x^{(j)} \rang \\
\text{s.t.} &amp;\quad y^{(i)}\left(\sum_{i=1}^m \alpha_i y^{(i)} \lang x^{(i)},x^{(i)} \rang + b\right) \ge 1,\quad i=1,...,m
\end{align*}\]

<p>notice that:</p>

<ul>
  <li>here the parts that relates to the input is the <strong>inner product</strong> $\lang x^{(i)},x^{(j)} \rang$​. Therefore, if we could somehow compute this part <em>efficiently​</em>, then we would be able to ==compute this even if features are in infinite dimension==.</li>
  <li>the only parameter now is $\alpha$ instead of $w, b$</li>
</ul>

<p>Lastly, you can further simplify this optimization problem using <strong>Lagrange Duality</strong>:</p>

\[\begin{align*}
\max_{\alpha} &amp;\quad \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m \alpha_i \alpha_jy^{(i)}y^{(j)} \lang x^{(i)},x^{(j)} \rang\\
\text{s.t.} &amp;\quad \alpha_i \ge 0,\, i=1,...,m\\
&amp;\quad \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align*}\]

<p>after we found the optimal parameter $\alpha$​, we can then compute back the original, optimal parameters $w^<em>,b^</em>$​​ by:</p>

\[w^* = \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}\]

\[b^* = -\frac{ \max_{i:y^{(i)}=-1} w^{*^T}x^{(i)} + \min_{i:y^{(i)}=1}w^{*^T}x^{(i)}}{2}\]

<p>where notice that the first equation was the ==”assumption” we made that the <strong>parameter $w$</strong> is some linear combination of data points $x^{(i)}$==. (To see the whole proof, see section <a href="#Lagrange Duality">Lagrange Duality</a>)</p>

<blockquote>
  <p><strong>Optimal Margin Classifier</strong></p>

  <ul>
    <li>
      <p>The problem is that to <strong>maximize $\gamma$</strong> by figuring out parameters $w,b$. It turns out that the above problem can be rephrased as the following:</p>

\[\begin{align*}
\min_{w,b} &amp;\quad \frac{1}{2}||w||^2\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1,\quad i=1,...,m
\end{align*}\]

      <p>which can be then converted to solving for $\alpha$:</p>

\[\begin{align*}
\max_{\alpha} &amp;\quad \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m \alpha_i \alpha_jy^{(i)}y^{(j)} \lang x^{(i)},x^{(j)} \rang\\
\text{s.t.} &amp;\quad \alpha_i \ge 0,\, i=1,...,m\\
&amp;\quad \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align*}\]

      <p>where the optimal value of $\alpha$ can then be used to solve back the parameters $w, b$ by:</p>

\[\begin{align*}
w^* &amp;= \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} \\
b^* &amp;= -\frac{ \max_{i:y^{(i)}=-1} w^{*^T}x^{(i)} + \min_{i:y^{(i)}=1}w^{*^T}x^{(i)}}{2}
\end{align*}\]

      <p>whose solution of $w,b$​ gives the <strong>optimal margin classifier</strong>.</p>
    </li>
    <li>
      <p>Then, to make a prediction when a new input $x$ comes in:</p>

\[h_{w,b}(x) = g(w^Tx+b)\]

      <p>where since $g(z)$​ is the <em>sigmoid</em> function, we basically predicts class-1 iff $w^Tx+b &gt; 0$. Hence this becomes <strong>computing the quantity</strong>:</p>

\[\begin{align*}
w^Tx + b
&amp;= \left( \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} \right)^T x + b \\
&amp;= \sum_{i=1}^m \alpha_i y^{(i)}\lang x^{(i)},x \rang + b
\end{align*}\]

      <p>which:</p>

      <ul>
        <li>basically only needs to compute the inner product between an input $x$ against all data points $x^{(i)}$</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="lagrange-duality">Lagrange Duality</h3>

<p>First lets us do some reviews on <strong>gradient vector</strong> and <strong>Lagrange multiplier</strong></p>

<hr />

<p><em>Review: Gradient Vector</em></p>

<p>For some intuition, consider a function $F$​ takes some point/vector in 3-dimension. So $F:\mathbb{R^3} \to \mathbb{R}$ is a function of three variables.</p>

<p>Now, by definition, the <strong>total derivative</strong> of $F$​ at a point $p \in \mathbb{R^3}$ is the <strong>best linear approximation​</strong> near $p=(a,b,c)$:</p>

\[\left.dF\right|_{p}(\Delta x, \Delta y, \Delta z) = \left.\frac{\partial F}{\partial x}\right|_p\Delta x +  \left.\frac{\partial F}{\partial y}\right|_p\Delta y +  \left.\frac{\partial F}{\partial z}\right|_p\Delta z\]

<p>Additionally:</p>

\[\left.dF\right|_{p}(\Delta x, \Delta y, \Delta z) + F(p) \approx F(a+\Delta x, b+\Delta y, c+\Delta z)\]

<p>Therefore, the <strong>total derivative</strong> can be understood as:</p>

\[\left.dF\right|_{p}(\Delta x, \Delta y, \Delta z) = \lang \left.\frac{\partial F}{\partial x}\right|_p, \left.\frac{\partial F}{\partial y}\right|_p, \left.\frac{\partial F}{\partial z}\right|_p \rang \cdot \lang \Delta x, \Delta y, \Delta z \rang\]

<p>Therefore, we can <strong>define</strong> ==gradient vector== at $p$ to be $\nabla F\vert _p = \lang \left.\frac{\partial F}{\partial x}\right\vert _p, \left.\frac{\partial F}{\partial y}\right\vert _p, \left.\frac{\partial F}{\partial z}\right\vert _p \rang$​. Since $p$ can be any point, we get that in general:</p>

\[\nabla F = \lang \frac{\partial F}{\partial x},  \frac{\partial F}{\partial y}, \frac{\partial F}{\partial z}\rang\]

<p>and hence, the ==total derivative is the DOT PRODUCT== of:</p>

\[dF|_p (\vec{\mathrm{v}}) = \nabla F|_p \cdot \vec{\mathrm{v}}\]

<p>In words, to see how much $F(p)$ changes when you move away a little bit from $\vec{p}$ to $\vec{p} + \vec{v}$ for some unit vector $\vec{v}$ just dot product $\nabla F\vert _p \cdot \vec{\mathrm{v}}$​.</p>

<p><strong>Therefore</strong>, it is easy to see that if you have a level curve, $F(\vec{x})=0$​​, then <em>gradient vector of a level curve</em> is <em>perpendicular</em> to the level curve.</p>

<hr />

<p><em>Review: Lagrange Multiplier</em></p>

<p>Consider finding local maxima/minima of a multivariate function $f(\vec{x})\mapsto \mathbb{R}$ subject to some constraint $g(\vec{x}) = 0$.</p>

<p>Since the constraint is basically a level curve, local maxima/minima points $\vec{x}$ on the level curve must satisfy:</p>

\[\nabla{f} = - \lambda \nabla g\]

<p>for some constant multiplier $\lambda$​ which is also called the <em>Lagrange Multiplier</em>.</p>

<p>Now, what if you have inequality constraint, for example $g(\vec{x}) \le 1$​? Then one intuitive approach would be:</p>

<ol>
  <li>First we can look at the area within the region. Compute the normal optimization problem of $f(\vec{x})$​, and then see if there are maxima/minima within $g(\vec{x}) &lt; 1$.</li>
  <li>Then we compute the optimization problem <strong>on the boundary</strong> $g(\vec{x}) = 1$.</li>
  <li>Lastly, we look at the <em>corners</em> of the boundary if there are.
    <ul>
      <li>the absolute maxima on the boundary are either local optima on the boundary, or absolute optima at the corner of the boundary</li>
    </ul>
  </li>
  <li>Now we just compare the values of the points found, and pick the larges/smallest if you are finding maxima/minima.</li>
</ol>

<blockquote>
  <p><strong>Resources</strong></p>

  <ul>
    <li>https://math.stackexchange.com/questions/49473/lagrange-multipliers-with-inequality-constraints-minimize-f-on-the-region-0</li>
    <li>https://users.wpi.edu/~pwdavis/Courses/MA1024B10/1024_Lagrange_multipliers.pdf
      <ul>
        <li>another method</li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />

<p>Now, coming back to the topic. Consider some constraint optimization problem:</p>

\[\begin{align*}
\min_w &amp;\quad f(w)\\
\mathrm{s.t.} &amp;\quad h_{i}(w)=0,\,\,\,\, i=1,...,l
\end{align*}\]

<p>Though such problems can be solved using Lagrange Multipliers, here we consider another related approach. Define the <strong>Lagrangian</strong> to be:</p>

\[\mathcal{L}(w, \beta) = f(w) + \sum_{i=1}^l \beta_i h_i(w)\]

<p>where now $\beta_i$​ are the Lagrange multipliers. Then using the fact from Lagrange Multiplier that $\nabla f =  -\beta_i \nabla h_i$, the optimization problem becomes solving:</p>

\[\frac{\partial \mathcal{L}}{\partial w_i} = 0; \quad \frac{\partial \mathcal{L}}{\partial \beta_i} = 0;\]

<p>which then you need to solve for $w, \beta$.</p>

<p>In this section, we will generalize this to constrained optimization problems in which we may have <strong>inequality as well as equality constraints</strong>.</p>

<p>First, consider the <strong>primal optimization problem</strong>:</p>

\[\begin{align*}
\min_w &amp;\quad f(w)\\
\mathrm{s.t.} &amp;\quad g_i(w) \le 0, \,\, i=1,...,k \\
&amp;\quad h_i(w) = 0, \,\, i=1,...,l
\end{align*}\]

<p>Then we have the <strong>generalized Lagrangian</strong>:</p>

\[\mathcal{L}(w, \alpha, \beta) = f(w) + \sum_{i=1}^k\alpha_i g_i(w)+ \sum_{i=1}^l \beta_i h_i(w)\]

<p>where $\alpha_i, \beta_i$ are the Lagrange multipliers.</p>

<blockquote>
  <p><strong>Primal Problem</strong></p>

  <ul>
    <li>
      <p>Consider the primal optimization problem above. We consider the quantity:</p>

\[\theta_\mathcal{P}(w) = \max_{\alpha,\beta ; \alpha_i \ge 0} \mathcal{L}(w,\alpha, \beta)\]

      <p>where the $\mathcal{P}$​ stands for <strong>primal</strong>. Then we know that:</p>

\[\theta_{\mathcal{P}}(w) = \begin{cases}
f(w) &amp;\text{if $w$ satisfies primal constraints} \\
\infty &amp;\text{otherwise}
\end{cases}\]

      <p>where this is easy to see since, if some $w$ violates the <strong>primal constraints</strong> that either $g_i(w) &gt; 0$ or $h_i(w) \neq 0$, then notice that:</p>

\[\begin{align*}
\theta_{\mathcal{P}} &amp;= \max_{\alpha, \beta: \alpha_i \ge 0} f(w) + \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_ih_i(w) \\
&amp;= \infty
\end{align*}\]

      <p>because either $g_i(w)$ term could explode to infinity or $h_i(w)$​ term would explode.</p>

      <p>Therefore, the <strong>primal problem can be rephrased as</strong>:</p>

\[\min_w \theta_{\mathcal{P}}(w) = \min_w \,\, \max_{\alpha,\beta:\alpha_i \ge 0} \mathcal{L}(w, \alpha, \beta)\]
    </li>
    <li>
      <p>For later use, let us define the optimal value/solution of the above problem be $p^* = \min_w \theta_{\mathcal{P}}(w)$​. This value will be referred to as the <strong>value of the primal problem</strong></p>
    </li>
  </ul>
</blockquote>

<p>Now let us consider a slightly different problem, which is:</p>

\[\theta_\mathcal{D}(\alpha, \beta) = \min_w \mathcal{L}(w,\alpha, \beta)\]

<p>where $\mathcal{D}$ stands for <strong>dual</strong>.</p>

<blockquote>
  <p><strong>Dual Optimization Problem</strong></p>

  <ul>
    <li>
      <p>Using the above definition, we can pose another (related but not directly related) problem:</p>

\[\max_{\alpha,\beta : \alpha_i \ge 0} \theta_\mathcal{D}(\alpha, \beta) = \max_{\alpha,\beta : \alpha_i \ge 0} \min_w \mathcal{L}(w,\alpha, \beta)\]

      <p>which is the same <em>primal optimization problem</em> but <em>$\min$ and $\max$​​ are reversed.</em></p>

      <p>Similarly, we can define the value $d^*=\max_{\alpha,\beta : \alpha_i \ge 0} \theta_\mathcal{D}(\alpha, \beta)$​ to be the optimal value of the <strong>dual problem</strong>.</p>
    </li>
    <li>
      <p>This is related to the original primal problem by:</p>

\[d^* = \max_{\alpha,\beta : \alpha_i \ge 0} \min_w \mathcal{L}(w,\alpha, \beta) \le \min_w \,\, \max_{\alpha,\beta:\alpha_i \ge 0} \mathcal{L}(w, \alpha, \beta) = p^*\]

      <p>since the “max min” of a function is less than or equal to the “min max”.</p>
    </li>
  </ul>
</blockquote>

<p>Now, the interesting part comes. ==Under some conditions==, we will have:</p>

\[d^* = p^*\]

<p>This is useful because we can just solve <strong>this equivalence problem</strong> instead of the original primal optimization problem.</p>

<blockquote>
  <p><strong>Karush-Kuhn-Tucker (KKT) Conditions</strong></p>

  <ul>
    <li>
      <p>The idea is that: suppose $f,g_i$ are convex, and $h_i$ are affine functions. Suppose further that the constraints $g_i$ are (strictly) feasible. This means that there exists some $w$ so that $g_i(w) &lt; 0$ for all $i$.</p>

      <p>Under the above assumptions, there <strong>must</strong> exist some $w^<em>, \alpha^</em>, \beta^<em>$ so that $w^</em>$ is the solution to the primal problem; $\alpha^<em>, \beta^</em>$​ is the solution to the dual problem, and that $p^<em>=d^</em>=\mathcal{L}(w^<em>,\alpha^</em>,\beta^*)$. Moreover, these value would ==satisfy the KKT conditions==:</p>

\[\frac{\partial }{\partial w_i} \mathcal{L}(w^*, \alpha^*, \beta^*) = 0,\quad i=1,...,n\]

\[\frac{\partial }{\partial \beta_i} \mathcal{L}(w^*, \alpha^*, \beta^*) = 0,\quad i=1,...,n\]

\[\alpha^*_i g_i(w^*) = 0, \quad i=1,...,k\]

\[g_i(w^*) \le 0, \quad i=1,...,k\]

\[\alpha^* \ge 0, \quad i=1,...,k\]

      <p>where the 3rd equation (123) from above is called the <strong>KKT dual complementarity</strong> condition. This will be used later for SVM.</p>
    </li>
    <li>
      <p>Moreover, if some $w^<em>, \alpha^</em>, \beta^*$ satisfy the KKT conditions, then it is also a solution to the ==primal and dual problem==.</p>
    </li>
  </ul>
</blockquote>

<h2 id="kernel">Kernel</h2>

<p>The overall idea behind the “Kernel Trick” is as follows.</p>

<blockquote>
  <p><em>General Steps for using Kernel</em></p>

  <ol>
    <li>Write your algorithm in terms of $\lang  x^{(i)},x^{(j)}\rang$​</li>
    <li>Let there be some mapping to transform your feature $x \in \mathbb{R}^{n} \mapsto \phi(x) \in \mathbb{R}^{n’}$, where $n’$​ could even be infinity
      <ul>
        <li>one example of $\phi$ would be: $x = [x_1, x_2]^T \mapsto [x_1, x_2, x_1x_2, x_1^2, …]^T$​</li>
      </ul>
    </li>
    <li>Find a way to compute the <strong>kernel</strong> $K(x,z) = \phi(x)^T\phi(z)=\lang \phi(x),\phi(z) \rang$​
      <ul>
        <li>this is the key step. If we can do this, we can replace in $x=x^{(i)},z=x^{(j)}$​</li>
      </ul>
    </li>
    <li>Replace the $\lang  x^{(i)},x^{(j)}\rang$ in your algorithm with $K(x^{(i)},x^{(j)})=\lang x^{(i)},x^{(j)} \rang$.
      <ul>
        <li>this means that you have basically learnt <em>nonlinear boundaries</em></li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>To see how the kernel step would work, let’s consider some examples.</p>

<p><em>For Example:</em></p>

<p>Consider the input being:</p>

\[x = \begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}, \quad \phi(x) = \begin{bmatrix}
x_1x_1\\
x_1x_2\\
x_1x_3\\
x_2x_1\\
x_2x_2\\
x_2x_3\\
x_3x_1\\
x_3x_2\\
x_3x_3\\
\end{bmatrix}\]

<p>so basically  if $x\in \mathbb{R}^n$, then  $\phi(x) \in \mathbb{R}^{n^2}$​. This means that:</p>

<ul>
  <li>if we need to compute $K(x,z) = \phi(x)^T \phi(z)$ ==explicitly==, then we need $O(n^2)$ time.</li>
</ul>

<p>However, it turns out that <strong>in this definition of $\phi$</strong>:</p>

\[K(x,z) = \phi(x)^T\phi(z) = (x^Tz)^2\]

<p>now notice that $x^Tz$ can be computed in $O(n)$ time!</p>

<hr />

<p><em>Proof:</em></p>

<p>Consider expanding the part $x^Tz$:</p>

\[\begin{align*}
(x^T z)^2
&amp;= \left( \sum_{i=1}^n x_iz_i \right)\left( \sum_{j=1}^n x_jz_j \right)\\
&amp;= \sum_{i=1}^n\sum_{j=1}^n  x_iz_i x_jz_j \\
&amp;= \sum_{i=1}^n\sum_{j=1}^n  (x_ix_j) (z_iz_j)\\
&amp;= \phi(x)^T \phi(z)
\end{align*}\]

<p>where:</p>

<ul>
  <li>the second equality can be thought of simply if you imagine something like $n=2$</li>
  <li>the last equality can be understood since if you compute $\phi(x)^T \phi(z)$, you are just <strong>summing up</strong> $x_1x_1\cdot z_1z_1 + x_1x_2\cdot z_1z_2 + …$, which is exactly the third last step.</li>
</ul>

<hr />

<p><em>For another example:</em></p>

<p>Other useful kernels are:</p>

\[K(x,z) = (x^T z +c)^2 = \sum_{i=1}^n\sum_{j=1}^n  (x_ix_j) (z_iz_j) + \sum_{i=1}^n(\sqrt{2c} x_i)(\sqrt{2c} z_i) + c^2\]

<p>which corresponds to something like:</p>

\[x = \begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}, \quad \phi(x) = \begin{bmatrix}
x_1x_1\\
x_1x_2\\
x_1x_3\\
x_2x_1\\
x_2x_2\\
x_2x_3\\
x_3x_1\\
x_3x_2\\
x_3x_3\\
\sqrt{2c} x_1\\
\sqrt{2c} x_2\\
\sqrt{2c} x_3\\
c
\end{bmatrix}\]

<p>More broadly, the kernel $K(x, z) = (x^T z + c)^d$​ corresponds to a feature mapping to an $\binom{n+d}{d}$​ feature space, corresponding of all monomials of the
form $x_{i_1}x_{i_2}\dots x_{i_k}$​ that are up to order $d$​​​.</p>

<p>Notice that the advantage of this is obviously that computing $\phi_(x)^T\phi(z)$ takes time that <strong>increases exponentially</strong> as we increase $d$, but comupting $K(x,z) = (x^Tz +c)^d$ <strong>only takes $O(n)$</strong></p>

<blockquote>
  <p><strong>Support Vector Machine</strong></p>

  <ul>
    <li>Now, the rest is simple. The idea for SVM is basically <strong>Optimal Margin Classifier + Kernel</strong>, which allows <em>some nonlinearity</em> defined by the feature mapping $\to$​ kernel function. This allows you to compute very high dimensions just with $O(n)$.</li>
    <li>In general, the idea of SVM can be visualized as:
      <ol>
        <li>Your data points is <em>not</em> linearly separable in some simple, e.g. 2D, dimension</li>
        <li>You use some mapping to transform your data into <strong>higher dimensions</strong></li>
        <li>Optimal Margin Classifier puts a <strong>linear</strong> hyperplane separating the data in <em>that higher dimension</em></li>
        <li>Once you have that best hyperplane, you are done
          <ul>
            <li>(visually, you can project it back down to 2D and get a nonlinear boundary)</li>
          </ul>
        </li>
      </ol>
    </li>
  </ul>
</blockquote>

<p>Now, the only problem is to thinking about <strong>making those kernel functions directly</strong>, alternatively, ==does a mapping $\phi$ exists for a given kernel $K$==.</p>

<p>Intuitively, since kernels comes from “dot products”, you might think that if $\phi(x),\phi(z)$ are <strong>close</strong> to each other (geographically), then $K(x,z)=\phi(x)^T\phi(z)$ should be <strong>large</strong>. Otherwise, the kernel should be small.</p>

<p>Following this line, consider the kernel:</p>

\[K(x,z) = \exp( -\frac{||x-z||^2}{2\sigma^2} )\]

<p>this seems to satisfy the intuition, but:</p>

<ul>
  <li>does there exist $\phi(x)^T\phi(z)$​​ that <em>gives</em> this Gaussian Kernel?</li>
  <li>(the answer is yes. In fact, it corresponds to $\phi(x)\in \mathbb{R}^{\infty}$ of <em>all polynomial features</em>, but giving a smaller weighting to higher dimensional)</li>
</ul>

<p>Some important constraint where:</p>

<ol>
  <li>For a kernel function $K(x,z)$, there better exists $\phi$ such that $\phi(x)^T\phi(z) = K(x,z)$</li>
  <li>$K(x,x) = \phi(x)^T\phi(x) \ge 0$​</li>
  <li>Kernel Matrix $K$​, where $K_{ij}=K(x^{(i)},x^{(j)})=K(x^{(j)}, x^{(i)})=K_{ji}$ is <strong>symmetric</strong></li>
  <li>Kernel Matrix $K$​​ is positive semidefinite (the <em>other direction is also true!!, but not proven here</em>)</li>
  <li>Mercer Kernel Theorem</li>
</ol>

<hr />

<p><em>Proof of 4</em></p>

<p>Consider some arbitrary vector $z$, and that:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210730221824385.png" alt="image-20210730221824385" /></p>

<p>which showed that $K$ is <strong>positive semi-definite</strong>.</p>

<hr />

<blockquote>
  <p><strong>Theorem: Mercer</strong></p>

  <p>Let $K: \mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}$ be given. Then for $K$ to be a valid Mercer Kernel, it is <strong>necessary and sufficient</strong> that for any ${x^{(1)}, x^{(2)},…,x^{(m)}}$, the corresponding kernel matrix $K$ is <strong>symmetric positive semi-definite</strong></p>
</blockquote>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>The Kernel trick is actually more <strong>fundamental</strong> than the SVM. In general, if you have <em>any learning algorithm</em> that uses $\lang x^{(i)}, x^{(j)} \rang$​, you can swap it with $K(x,z)$ and apply the learning in a much higher dimensional space.</li>
  </ul>
</blockquote>

<h2 id="regularization-and-the-non-separable-case">Regularization and the Non-Separable Case</h2>

<p>However, the algorithms above still have the ==assumption== that data is <strong>separable in some higher dimensions</strong>. What if due to some noise in the data, they are (<em>intrinsically</em>) not separable (or you want to prevent it from overfitting)?</p>

<p>e.g. data with some noise:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210730225856492.png" alt="image-20210730225856492" /></p>

<p>where having just <em>one outlier</em>, we need to move the boundary dramatically.</p>

<p>To make the algorithm work for non-linearly separable datasets as well as be less sensitive to outliers, we <strong>reformulate our optimization task</strong> to use the <strong>$\ell_1$​ regularization</strong>:</p>

\[\begin{align*}
\min_{w,b,\xi} &amp;\quad \frac{1}{2}||w||^2 + C \sum_{i=1}^m \xi_i\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1 - \xi_i,\quad i=1,...,m \\
&amp;\quad \xi_i \ge 0 , \quad i=1,...,m
\end{align*}\]

<p>where:</p>

<ul>
  <li>on the bottom, we relaxed the constraint to be $1 - \xi_i$, which is easier to satisfy. In fact, as long as $1 - \xi_i &gt; 0$, we have a correct guess</li>
  <li>on the top, we are minimizing $\frac{1}{2}\vert \vert w\vert \vert ^2 + C \sum_{i=1}^m \xi_i$, with the additional term added such that the $1-\xi_i$ will <strong>not​​</strong> have $\xi_i$​ being too large</li>
  <li>The parameter $C$ controls the relative weighting between the <strong>twin goals</strong> of making the $\vert \vert w\vert \vert ^2$ small (which we saw earlier makes the margin large) and of ensuring that most examples have functional margin at least $1$.</li>
</ul>

<p>Graphically, you can think of the error term $\xi$​ being the red cross in the image:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210730230435358.png" alt="image-20210730230435358" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>if you required strictly $\ge 1$, then the boundary cannot be the one in the image since the closest samples <strong>must</strong> have functional margin of $1$​ (where as the red cross is clearly less than 1)</li>
  <li>if you <strong>relaxed</strong> the condition and asked for $\ge 0.5$​​ say, then the red cross (suppose if having a functional margin of 0.51) would be allowed in this given boundary.</li>
</ul>

<p>Lastly, we use the same approach in Lagrange Duality and simplify $\ell_1$ optimization problem to:</p>

\[\begin{align*}
\max_{\alpha} &amp;\quad \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m \alpha_i \alpha_jy^{(i)}y^{(j)} \lang x^{(i)},x^{(j)} \rang\\
\text{s.t.} &amp;\quad 0 \le \alpha_i \le C,\, i=1,...,m\\
&amp;\quad \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align*}\]

<p>where:</p>

<ul>
  <li>after computing the optimal $\alpha$, you can then compute $w$ using Equation 102. However the optimal $b$​ would be changed.</li>
  <li>the parameter $C$ is something you need to choose. This will be discussed in the future sections.</li>
</ul>

<h2 id="the-smo-algorithm">The SMO Algorithm</h2>

<p>Check notes</p>

<h1 id="learning-theory">Learning Theory</h1>

<p>This section talks about many techniques that you can use in real life when dealing with ML.</p>

<h2 id="bias-and-variance">Bias and Variance</h2>

<p>Recall that in section <a href="#Overfitting and Underfitting">Overfitting and Underfitting</a>, we discussed things that could go wrong when we chose a “wrong” hypothesis:</p>

<p>Consider the data being <em>actually</em> distributed in $x \mapsto O(x^2)$, such that we have:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Underfitting/High Bias</th>
      <th style="text-align: center">Actual</th>
      <th style="text-align: center">Overfitting/High Variance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204918094.png" alt="image-20210716204918094" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204945061.png" alt="image-20210716204945061" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210716204955455.png" alt="image-20210716204955455" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>first figure is <em>underfitted</em> as we only imagined $y = \theta_0 + \theta_1 x$​​​. We also call it <strong>high bias</strong>.</li>
  <li>third figure is <em>overfitting</em> as we imaged $y=\theta_0 + \theta_1 x + \theta_2 x^2 + … + \theta_5 x^5$​. We also call it <strong>high variance</strong></li>
  <li>both first and third figure will have a very <strong>large generalization error</strong>. (informally, think of it as expected error on examples not necessarily in
the training set.)</li>
</ul>

<blockquote>
  <p><strong>Bias (informal)</strong></p>

  <ul>
    <li><em>high bias</em>: then even if we were fitting a linear model to a very large amount of training data, the linear model would still <strong>fail to accurately capture the structure</strong> in the data.</li>
    <li><em>Informally</em>, we define the bias of a model to be the expected generalization error even if we were to fit it to a very (say, infinitely) large
training set.</li>
  </ul>

  <p><strong>Variance (informal)</strong></p>

  <ul>
    <li>fitting patterns in the data that happened to be present in our small, finite training set, but that <strong>do not reflect the wider pattern</strong> of the
relationship between $x$ and $y$.</li>
  </ul>
</blockquote>

<p>Most of the time, we need to consider tradeoffs:</p>

<ul>
  <li>If our model is too “<em>simple</em>” and has very few parameters, then it may have <em>large bias</em> (but small variance)</li>
  <li>if it is too “<em>complex</em>” and has very many parameters, then it may suffer from <em>large variance</em> (but have smaller bias).</li>
</ul>

<p>In a slightly more formal view, consider the case where you need to fit 2 parameters, $\theta_1, \theta_2$​​, from $4$​ different <strong>learning models</strong>. Suppose you have some distribution $\mathcal{D}$​ where you draw samples $(x,y)$​ from:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210808231550177.png" alt="image-20210808231550177" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>the red cross is the “true value” of $\theta_1^<em>, \theta^</em>_2$​</li>
  <li>each black dot represent, for each $m$ times you sampled, which obtained a training set $S$ of size $m$​, you compute the ERM of $\theta_1, \theta_2$ for <strong>each model</strong></li>
</ul>

<p>Therefore, you see that:</p>

<ul>
  <li>a low bias corresponds to the case where $\theta_1, \theta_2$​ are close to the “true parameter” across different samples of training set $\mathcal{D}$​
    <ul>
      <li>so a zero bias would be mean $E[\hat{\theta}] = \theta^*$​ for some $m$​.</li>
    </ul>
  </li>
  <li>a low variance corresponds to the case where $\theta_1, \theta_2$ are close to each other across different samples of training set $\mathcal{D}$​</li>
</ul>

<p>This means that:</p>

<ol>
  <li>if you were able to collect an infinite number of samples, $m \to \infty$​, then the data points will be more concentrated. This means that $\mathrm{Var}[\theta] \to 0$​​​​. This is also called <strong>statistical efficiency</strong> (the rate of which $\mathrm{Var}[\theta] \to 0$​​).
    <ul>
      <li>this would be related to variance</li>
    </ul>
  </li>
  <li>if $m \to \infty$​ and $\hat{\theta} \to \theta^*$​, then this is called <strong>statistical consistency</strong>.
    <ul>
      <li>this would be related to bias. If bias is too high, then it might not be.</li>
    </ul>
  </li>
</ol>

<h2 id="preliminaries">Preliminaries</h2>

<p>This section aims to answer questions such as: Why should doing well on the training set tell us anything about generalization error? Specifically, can we relate error on the training set to generalization error?</p>

<blockquote>
  <p><strong>Lemma</strong> Union Bound</p>

  <ul>
    <li>
      <p>Let $A_1, … A_k$ be $k$ different events (that may not be independent). Then:</p>

\[P(A_1 \cup \dots \cup A_k) \le P(A_1) + \dots  +P(A_k)\]
    </li>
  </ul>

</blockquote>

<blockquote>
  <p><strong>Lemma</strong> Hoeffding Inequality</p>

  <ul>
    <li>
      <p>Let $Z_1, …,Z_m$​ be $m$​ independent and identically distributed (IID) random variables (i.e. if it is binary classification, then $Z_i = {0,1}$) drawn from a Bernoulli $(\phi)$​ distribution. I.e. $P(Z_i =1) = \phi$​, then $P(Z_i=0)=1-\phi$​.</p>

      <p>Let $\hat{\phi}$ be the mean of those variables, such that:</p>

\[P(|\phi - \hat{\phi}| &gt; \gamma) \le 2 \exp{ (-2 \gamma^2 m )}\]

      <p>which basically says that if take the mean $\hat{\phi}$​ to be the estimate for $\phi$, then the probability of us being far away from the true value is small if $m$ is large.</p>

      <ul>
        <li>for instance, if you have a biased coin with a chance of $\phi$ landing on heads, then each toss independently has a $P(Z_i=1)$​ probability of being head. If you toss it $m$ times and compute the mean, i.e. fraction of times that it landed up in head, then that will be a <strong>good estimate</strong> of the true $\phi$ if $m$ is large enough.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>To start off, we restrict our attention to binary classification in which the labels are $y\in {0,1}$. Though everything here can be generalized to others, including regression and multi-class classification.</p>

<p>Consider a training set $S = { (x^{(i)},y^{(i)}) }_{i=1}^m$​, where the training samples are drawn ==IID from the same distribution $\mathcal{D}$​==. Then for a hypothesis $h$​, we can define the ==training error== (also called the <strong>empirical risk</strong> or <strong>empirical error</strong>) to be:</p>

\[\hat{\varepsilon}_S(h) = \hat{\varepsilon}(h) = \frac{1}{m} \sum_{i=1}^m 1\{h(x^{(i)}) \neq y^{(i)}\}\]

<p>which is basically fraction of times the hypothesis misclassified.</p>

<ul>
  <li>note that we used $\hat{\varepsilon}_S(h)$ because this also depends on the dataset we are using for training</li>
</ul>

<p>Then, we define the ==generalization error== to be:</p>

\[\varepsilon(h) = P_{(x,y) \sim \mathcal{D}} (h(x) \neq y)\]

<p>which means that the probability of $h$ <em>misclassifying</em> if we draw a <em>new sample $(x,y)$</em> from the <em>same distribution $\mathcal{D}$</em>.</p>

<ul>
  <li>the assumption that both the training data and the test data are from the same distribution $\mathcal{D}$ is called the <strong>PAC</strong> (probably approximately correct) assumptions.</li>
</ul>

<blockquote>
  <p><strong>Empirical Risk Minimization (ERM)</strong></p>

  <ul>
    <li>
      <p>Consider the setting of linear classification such that our hypothesis is $h_\theta(x) = 1{ \theta^T x \ge 0 }$. Then one approach for fitting the parameter $\theta$​ is to <strong>minimize the training error</strong>:</p>

\[\hat{\theta} = \arg \min_{\theta} \hat{\varepsilon}(h_\theta)\]
    </li>
    <li>
      <p>To be more abstract, we can consider any hypothesis by defining a <strong>hypothesis class $\mathcal{H}$​​</strong> to be the set of all classifiers/hypothesis considered by it. A hypothesis class of linear classifiers would be:</p>

\[\mathcal{H} = \{ h_\theta : h_\theta(x) = 1\{ \theta^T x \ge 0 \}, \theta \in \mathbb{R}^{n+1} \}\]

      <p>Therefore, empirical risk minimization would be though of minimizing over the class of functions in $\mathcal{H}$.</p>

\[\hat{h} = \arg \min_{h \in \mathcal{H}} \hat{\varepsilon} (h)\]
    </li>
  </ul>

</blockquote>

<p>Last but not least, sometimes it is useful to think of <strong>generalization error</strong> as components:</p>

<p><img src="STF_CS229/image-20210814185619252.png" alt="image-20210814185619252" style="zoom:50%;" /></p>

<p>where each point on the graph represents a <strong>hypothesis</strong>.</p>

<p>Given some hypothesis class $\mathcal{H}$, and some data set $\mathcal{D}$, consider some best possible hypothesis $g$​ (e.g. having the smallest generalization error, which <em>might not be $0$</em>). Then, we define the following:</p>

<ul>
  <li>$\varepsilon(g)$​ = Bayes Error/ ==Irreducible== Error</li>
  <li>$\varepsilon(h^*) - \varepsilon(g)$ = ==Approximation== Error</li>
  <li>$\varepsilon(\hat{h})  - \varepsilon(h^*)$ = ==Estimation== Error</li>
  <li>therefore $\varepsilon(\hat{h}) = \text{Estimation Error} + \text{Approximation Error} + \text{Irreducible Error}$​</li>
</ul>

<p>Finally, this can be further broken down to:</p>

\[\begin{align*}
\varepsilon(\hat{h}) &amp;= \text{Estimation Error} + \text{Approximation Error} + \text{Irreducible Error} \\
&amp;= \text{Estimation Variance} + \text{Estimation Bias} + \text{Approximation Error} + \text{Irreducible Error}
\end{align*}\]

<p>where:</p>

<ul>
  <li>Estimation Variance is often called <strong>variance</strong></li>
  <li>Estimation Bias + Approximation Error is often called <strong>bias</strong></li>
</ul>

<h3 id="fighting-variance">Fighting Variance</h3>

<p>Some techniques to reduce variance would be:</p>

<ul>
  <li>increase $m$</li>
  <li>add regularization to the model
    <ul>
      <li>the net effect can be thought of as <em>decreasing</em> the size of your hypothesis class</li>
    </ul>
  </li>
</ul>

<h3 id="fighting-bias">Fighting Bias</h3>

<p>Some techniques to reduce bias would be:</p>

<ul>
  <li>make the hypothesis class $\mathcal{H}$ bigger
    <ul>
      <li>note that this might increase variance</li>
    </ul>
  </li>
</ul>

<p>The problem is we kind of need to know the “true value” of parameter $\theta$ <strong>in advance</strong>.</p>

<h2 id="the-case-of-finite-mathcalh">The case of finite $\mathcal{H}$​​</h2>

<p>Consider a finite hypothesis class $\mathcal{H} = { h_1, …, h_k }$ consisting of $k$ hypothesis, i.e. $\mathcal{H}$ is a a set of $k$ mappings from $\mathcal{X}$ to ${0,1}$. The empirical risk minimization selected a hypothesis $\hat{h}$​.</p>

<p>We would like to give <strong>guarantees on the generalization error</strong> of $\hat{h}$. Our strategy for doing so will be in two parts:</p>

<ol>
  <li>First, we will show that $\hat{\varepsilon}(h)$​ is a reliable <strong>estimate</strong> of $\varepsilon(h)$​ for all $h$​.</li>
  <li>Second, we will show that this implies a connection between <strong>generalization error of the ERM</strong> $\varepsilon(\hat{h})$ and that of the <strong>best possible hypothesis</strong> $\varepsilon(h^*)$​.</li>
</ol>

<p>Take one fixed $h_i \in \mathcal{H}$​​​. Consider a Bernoulli random variable $Z$​​​, such that we sample $(x,y) \sim \mathcal{D}$​​​​​, and $Z=1{ h_i(x) \neq y }$​​​. Similarly, we define $Z_j = 1{ h_i(x^{(j)}) \neq y^{(j)} }$​​​​. Since our samples are drawn IID from $\mathcal{D}$​​​, then random variable $Z$​ and $Z_i$​​​​ has the <strong>same distribution</strong>.</p>

<p><img src="STF_CS229/image-20210814223242524.png" alt="image-20210814223242524" style="zoom:50%;" /></p>

<p>where basically:</p>

<ul>
  <li>
    <p>we sampled ==some randomly sampled== dataset $\mathcal{D}$ from the distribution, and for all hypothesis $h \in \mathcal{H}$, we calculated the <strong>generalization error</strong> $\varepsilon$ and the <strong>training error</strong> based on the sampled dataset $\hat{\varepsilon}_S$​</p>
  </li>
  <li>
    <p>for any hypothesis $h_i$, we can say that, if we do the sampling of dataset many times and for each time compute the training error, we will get:</p>

\[E[\hat{\varepsilon}(h_i)] = \varepsilon(h_i)\]

    <p>i.e. the average of those training error lines will converge to the generalization error.</p>
  </li>
</ul>

<p>What is the misclassification probability on a randomly drawn example? (if we have only sampled a finite dataset $\mathcal{D}$)</p>

<ul>
  <li>This is exactly the <strong>expected value of $Z$​​</strong>.</li>
</ul>

<p>Moreover, since the training error can be written as:</p>

\[\hat{\varepsilon}(h_i) = \frac{1}{m} \sum_{j=1}^m Z_j\]

<p>now, notice that:</p>

<ul>
  <li>$\hat{\varepsilon}(h_i)$ is exactly the mean of $m$ random variables $Z_j$ that are drawn <em>IID from a Bernoulli Distribution with mean $\varepsilon(h_i)$</em>.</li>
  <li>now we can use Hoeffding Inequality, since we have the <strong>mean of some random variables from some distribution</strong>.</li>
</ul>

<p>Therefore, applying the Hoeffding Inequality, we obtain:</p>

\[P(|\varepsilon(h_i) - \hat{\varepsilon}(h_i)| &gt; \gamma) \le 2 \exp( -2\gamma^2 m )\]

<p>which shows that</p>

<ul>
  <li>for our particular $h_i$​, training error will be <strong>close to generalization error</strong> with high probability, assuming $m$​​ is large.</li>
  <li>basically, the green line above will be closer to the blue line if $m$ is large.</li>
</ul>

<p>This proved that for <em>one particular $h_i$​, training error is close to generalization error</em>. We want to prove that this will be true for simultaneously $\forall h \in H$​​.</p>

<ul>
  <li>i.e. we want to have the <strong>entire green line</strong> converge uniformly to the blue line in the figure above.</li>
</ul>

<p>To do so, let $A_i$​ denote the event that $\vert \varepsilon(h_i) - \hat{\varepsilon}(h_i) \vert  &gt; \gamma$​. Then for any particular $A_i$​, we know that $P(A_i) \le 2 \exp(-2\gamma^2 m)$​. Using the union bound, we get:</p>

\[\begin{align*}
P(\exists h \in \mathcal{H}, |\varepsilon(h_i) - \hat{\varepsilon}(h_i) | &gt; \gamma)
&amp;= P(A_1 \cup \dots \cup A_k) \\
&amp;\le \sum_{i=1}^k P(A_i) \\
&amp;\le \sum_{i=1}^k 2 \exp(-2 \gamma^2 m)\\
&amp;= 2k \exp(-2\gamma^2  m)
\end{align*}\]

<p>which is pretty intuitive because we have $k$ hypothesis.</p>

<p>Therefore, we get:</p>

\[P(\exists h \in \mathcal{H}, |\varepsilon(h_i) - \hat{\varepsilon}(h_i) | \le \gamma) \ge 1- 2k \exp(-2\gamma^2  m)\]

<p>so that with at least $1-2k \exp(-2\gamma^2  m)$​ probability we will get $\varepsilon(h)$​ within $\gamma$​ to $\hat{\varepsilon}(h)$​ <strong>for all $h \in \mathcal{H}$​​​</strong>. This is called a ==uniform==
==convergence result==, because this is a bound that holds simultaneously for all (as opposed to just one) $h \in H$​​.</p>

<p>So the important parameters here would be:</p>

<ul>
  <li>$\delta \equiv 2k \exp(-2\gamma^2  m)$ probability of error</li>
  <li>$\gamma$ margin of error</li>
  <li>$m$ sample size</li>
</ul>

<hr />

<p><em>For instance</em></p>

<p>we can ask the following question: Given $\gamma$​ and some $\delta &gt; 0$​, how large must $m$​ be before we can guarantee that with probability at least $1 − \delta$​, training error will be within $\gamma$​ of generalization error?</p>

<p>We can solve this by setting $\delta = 2k \exp(-2\gamma^2  m)$ and solve for $m$, which gives:</p>

\[m \ge \frac{1}{2\gamma^2} \log{\frac{2k}{\delta}}\]

<p>such that:</p>

<ul>
  <li>with probability at least $1-\delta$, we have $\vert \varepsilon(h_i) - \hat{\varepsilon}(h_i) \vert  &gt; \gamma$ for $\forall h \in \mathcal{H}$​.</li>
  <li>This bound tells us how many training examples we need in order make a guarantee, which is also called the ==sample complexity==.</li>
</ul>

<hr />

<p>Previously we have shown a connection between training error $\hat{\varepsilon}(h)$ and the generalization error $\varepsilon(h)$.​</p>

<p>Next, we show the connection between the generalization error from the ERM $\varepsilon(\hat{h})$ and that of the best in class $\varepsilon(h^*)$.</p>

\[\begin{align*}
\varepsilon(\hat{h}) &amp;\le \hat{\varepsilon}(\hat{h}) + \gamma \\
&amp;\le \hat{\varepsilon}(h^*) + \gamma \\
&amp;\le \varepsilon(h^*) + 2\gamma
\end{align*}\]

<p>where:</p>

<ul>
  <li>The first line used the fact that $\vert \varepsilon(\hat{h}) - \hat{\varepsilon}(\hat{h})\vert  \le \gamma$​ (by our uniform convergence assumption).</li>
  <li>The second line used the fact that $\hat{h}$ minimizes training error</li>
  <li>The third line used the <em>uniform convergence again</em>, since this ==$\gamma$​ is “shared” across all hypothesis==.</li>
</ul>

<p>We can solve for $\gamma$ using the equation 141 above, and this gives the thorem:</p>

<blockquote>
  <p><strong>Bound on Generalization Error</strong>:</p>

  <p>Let $\vert \mathcal{H}\vert  = k$, and let some $m, \delta$ be fixed. Then with probability at least $1-\delta$, the following is true:</p>

\[\varepsilon(\hat{h}) \le \varepsilon(h^*) + 2 \sqrt{\frac{1}{2m}\log \frac{2k}{\delta}}\]

  <p>where:</p>

  <ul>
    <li>we basically solved for the $\gamma$​ in the equation mentioned previously.</li>
    <li>the $h^*$ would be the hypothesis with lowest point on the blue line</li>
  </ul>
</blockquote>

<h2 id="the-case-of-infinite-mathcalh">The case of infinite $\mathcal{H}$</h2>

<p>Basically the idea is to “assign size” to hypothesis class with infinite numbers of hypothesis. This is done by the concept of VC-dimension.</p>

<p>Key results shown first, which is similar to equation 143:</p>

\[\varepsilon(\hat{h}) \le \varepsilon(h^*) + O\left(\sqrt{\frac{d}{m}\log \frac{m}{d}+ \frac{1}{m}\log \frac{1}{\delta}}\right)\]

<p>where:</p>

<ul>
  <li>$d=\mathrm{VC}(\mathcal{H})$​ is the VC-dimension of some hypothesis class $\mathcal{H}$​</li>
</ul>

<h1 id="regularization-and-model-selection">Regularization and Model Selection</h1>

<p>Regularization is a very useful technique to prevent use from <em>overfitting</em>.</p>

<p><em>For Example:</em></p>

<p>Recall the simple <em>least square error</em> that we are trying to minimize:</p>

\[\min_\theta \frac{1}{2}\sum_{i=1}^m || y^{(i)} - \theta^T x^{(i)} ||^2 + \frac{\lambda}{2} ||\theta||^2\]

<p>where we have added the term $(\lambda/2 )\vert \vert \theta\vert \vert ^2$​ to <strong>prevent $\theta$​​ being to large</strong>. If $\theta$ is unconstraint, then learning algorithms might end up with (assuming a hypothesis of fifth order polynomial)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Overfitting with $\lambda = 0$​</strong></th>
      <th style="text-align: center"><strong>$\lambda = 1$</strong></th>
      <th style="text-align: center"><strong>Underfitting with $\lambda = 100$</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210803215832992.png" alt="image-20210803215832992" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210803220004441.png" alt="image-20210803220004441" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210803220108650.png" alt="image-20210803220108650" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>the idea is that there is usually some optimial $\lambda$ that can result in some really good fits.</p>

<p>Similarly, for logistic regression, we can have:</p>

\[\arg\max_\theta \sum_{i=1}^m \log{\left(p(y^{(i)}|\theta^Tx^{(i)}) \right)} - \frac{\lambda}{2} ||\theta||^2\]

<p>in a similar manner.</p>

<hr />

<p><strong>Note</strong></p>

<p>In fact, the term $\vert \vert w\vert \vert ^2$​ in SVM:</p>

\[\begin{align*}
\min_{w,b,\xi} &amp;\quad \frac{1}{2}||w||^2 + C \sum_{i=1}^m \xi_i\\
\text{s.t.} &amp;\quad y^{(i)}(w^T x^{(i)} + b) \ge 1 - \xi_i,\quad i=1,...,m \\
&amp;\quad \xi_i \ge 0 , \quad i=1,...,m
\end{align*}\]

<p>also prevents SVM to be overfitting. It has a similar effect as the ones mentioned above.</p>

<hr />

<p><em>Proof (Informal)</em></p>

<p>The proof for the regularization idea is as follows.</p>

<p>Consider a training set $S={(x^{(i)}, y^{(i)})}_{i=1}^m$, and that our model relies on some parameter $\theta$​. Then consider the probability:</p>

\[\begin{align*}
\arg\max_{\theta} p(\theta | S) 
&amp;=\arg\max_{\theta} \frac{p(S|\theta)p(\theta)}{p(S)}\\
&amp;=\arg\max_{\theta} p(S|\theta)p(\theta)\\
\end{align*}\]

<p>for logistic regression (or other generalized linera model):</p>

\[\begin{align*}
\arg\max_{\theta} p(\theta | S) 
&amp;=\arg\max_{\theta} p(S|\theta)p(\theta)\\
&amp;= \arg\max_{\theta}\left( \prod_{i=1}^m p(y^{(i)}|x^{(i)}, \theta) \right)p(\theta)
\end{align*}\]

<p>and ==assuming== probability of $\theta$ being distributed normally:</p>

\[p(\theta): \theta \sim \mathcal{N}(0, \tau^2 I)\]

<p>then computing the quantity $\arg\max_{\theta} p(\theta \vert  S)$ will yield the term $\vert \vert \theta\vert \vert ^2$ for regularization.</p>

<hr />

<blockquote>
  <p><strong>Frequentist School:</strong></p>

  <ul>
    <li>
      <p>the canonical idea is to find the <strong>most likely $\theta$</strong> given the dataset $S$. Therefore, it talks about <strong>maximum likelyhood estimation (MLE)</strong></p>

\[\arg\max_\theta p(S|\theta)\]
    </li>
  </ul>

  <p><strong>Bayesian School:</strong></p>

  <ul>
    <li>
      <p>the idea here is that the parameters $\theta$ are distributed by a normal distribution $\theta \sim \mathcal{N}(0, \tau^2 I)$​ <strong>a priori</strong> to the dataset. Hence it computes the probability of having a $\theta$ given the dataset $S$​ by the <strong>maximum a posterior (MAP)</strong></p>

\[\arg\max_{\theta}p(\theta|S)\]
    </li>
    <li>
      <p>in fact, the regularization idea would be also <em>derived from the Bayesian prior</em></p>
    </li>
  </ul>
</blockquote>

<p><strong>Graphically</strong>, the difference between biases and variance is shown below:</p>

<p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210808193802944.png" alt="image-20210808193802944" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>obviously our aim is to get the $\lambda$ regularization such that our model ended up in the middle, which has the smallest generalization error.</li>
</ul>

<h2 id="cross-validation">Cross Validation</h2>

<p>Let’s suppose we are, as usual, given a training set $S$​. Given what we know about empirical risk minimization, here’s what <strong>might initially seem like a algorithm</strong>:</p>

<ol>
  <li>Train each model $M_i$ on $S$, to get some hypothesis $h_i$ form each model</li>
  <li>Pick the hypothesis with <em>smallest training error</em></li>
</ol>

<p>This naïve idea would not work. Consider the case of having models $M_i$ of polynomial with degree $i$. Then hypothesis from model with a higher order of polynomial will always fits better on the training set $S$.</p>

<blockquote>
  <p><strong>Hold-Out Cross Validation/Simple Cross Validation</strong></p>

  <p>One remedy from the above approach it to split the data into $S_{\mathrm{train}}, S_{\mathrm{cv}}, S_{\mathrm{test}}$, such that:</p>

  <ol>
    <li>Randomly split the data into $S_{\mathrm{train}}$​, e.g. 70% of the data, and $S_{\mathrm{train}}$​ (optionally $S_{\mathrm{test}}$​​) the remaining
      <ul>
        <li>sometimes, the cross validation set is also called the “dev set” $S_{\mathrm{dev}}$.​</li>
      </ul>
    </li>
    <li>Train each model $M_i$ on $S_{\mathrm{train}}$ only, to get some hypothesis $h_i$</li>
    <li>Select and output the hypothesis with smallest error ==on $S_{\mathrm{cv}}$​==, such that $\hat{\varepsilon}<em>{S</em>{\mathrm{cv}}}(h_i)$​ is smallest.</li>
    <li>(Optionally) If you need to publish your result/use it somewhere, you can then compare the hypothesis by putting them through $S_{\mathrm{test}}$​
      <ul>
        <li>the take-away message is that your $S_{\mathrm{test}}$ set should ==not be seen== any where in the training/decision of your models. Otherwise such evaluation metrics is meaningless as you are fitting your data towards it.</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Some of the commonly used splitting size would be $S_{\mathrm{train}}=60\%, S_{\mathrm{cv}}=20\%, S_{\mathrm{test}}=20\%$. However, if your data set is <em>really large</em>, then having lots of test samples might not make sense <strong>unless you care about very small improvements</strong>, e.g. $0.01\%$ difference. Therefore, often it might become $S_{\mathrm{cv}}=5\%, S_{\mathrm{test}}=5\%$​​​ if your data size is very big.</p>

<p>However, one disadvantage of this is that <em>some of the data is “wasted”</em> as they are used for evaluation but not training. When your dataset is small, this might be a problem.</p>

<p>Now to make the illustration easier, consider the case where we have <strong>already prepared our $S_{\mathrm{test}}$</strong>. Then one approach to “save” some data when you are at a small dataset is to have:</p>

<blockquote>
  <p><strong>$k$-fold Cross Validation</strong></p>

  <ol>
    <li>
      <p>Randomly split $S$ into $k$ disjoint subsets of equal size. Let’s call those subsets $S_{1},\dots, S_k$</p>
    </li>
    <li>
      <p>For each model $M_i$, do the following:</p>

      <p><img src="/lectures/images/2022-01-18-STFCS229_Machine_Learning/image-20210808203157586.png" alt="image-20210808203157586" style="zoom:50%;" /></p>

      <p>so that basically you have used all your data at least once for training.</p>
    </li>
    <li>
      <p>Pick the model $M_i$ which outputted the hypothesis with the lowest estimated averaged generalization error.</p>
    </li>
    <li>
      <p>(Optionally) Once you have decided which model to use, i.e. model with polynomial of degree 3, then <strong>retrain</strong> the model on the entire training set for later production use.</p>
    </li>
  </ol>
</blockquote>

<p>In practice, $k=10$ is most commonly used. However, the problem with this is that it is <strong>computationally more expensive</strong>, since each model is trained $k$ times. Therefore, this is a good idea only if your data size is small.</p>

<h2 id="feature-selection">Feature Selection</h2>

<p>Another useful technique to prevent from overfitting is to consider a subset of $n$​​ features that are <strong>relevant</strong> to the learning task.</p>

<p>Consider the case where you want to find a subset of $n$​ given features for your training. Given $n$​ features, there are $2^n$​ possible feature subsets (since each of the $n$​ features can either be <em>included or excluded</em> from the subset), and thus feature selection can be posed as a model selection problem over $2^n$ possible models, which is <strong>a lot</strong>.</p>

<p>For large $n$, usually some other heuristic search procedure is used to save some computational power.</p>

<blockquote>
  <p><strong>Forward Search Procedure</strong></p>

  <ol>
    <li>Start with the set of features to use $\mathcal{F} = \empty$.</li>
    <li>Repeat, until no improvements on generalization error (or other stopping condition):
      <ol>
        <li>For feature $F_i,i=1,\dots, n$, and that $F_i \notin \mathcal{F}$, try $\mathcal{F}_i = \mathcal{F}\cup {F_i}$​. Then we use the cross validation technique to check if there is any improvements on generalization error.</li>
        <li>Select $\mathcal{F}$ to be the best feature subset (e.g. with smallest generalization error) for the next iteration</li>
      </ol>
    </li>
    <li>Output the found $\mathcal{F}$</li>
  </ol>
</blockquote>

<p>In other words, this is basically a <strong>greedy algorithm</strong> that takes the best result at current iteration and use it for the future. However, this is <em>still computationally expensive</em>, but useful.</p>

<h1 id="decision-tree-and-ensembles">Decision Tree and Ensembles</h1>

<p>Consider the prediction for which month of the year we can go to <em>ski</em>. Depending on the latitude (“height”) you are living at, we know that:</p>

<p><img src="STF_CS229/image-20210818145741272.png" alt="image-20210818145741272" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>at high latitudes (north), winter comes earlier; at low latitudes (south). winter comes later</li>
</ul>

<p>Since the problem now becomes distinguishing the <strong>regions</strong>, we could:</p>

<ul>
  <li>use SVM with casting those features into <em>higher dimensional</em> space</li>
  <li>more naturally, the <strong>decision tree model</strong></li>
</ul>

<h2 id="decision-tree-model">Decision Tree Model</h2>

<p>The model is our first example of a intrinsically <em>non-linear</em> model.</p>

<p>In general, it is a model of a <strong>greedy, top-down, recursive</strong> approach.</p>

<hr />

<p><em>For example</em></p>

<p>To solve the problem above, we might end up with <em>asking the questions</em>:</p>

<pre><code class="language-mermaid">graph TD
    Lat_larger_30 --&gt; |Yes|Month_less_3
    Lat_larger_30 --&gt; |No|Month_larger_3
    Month_less_3 --&gt; |Yes| ...
    Month_less_3 --&gt; |No| ....
</code></pre>

<p>where:</p>

<ul>
  <li>this is a greedy algorithm because in each step, it decides the “currently best split”</li>
  <li>it is recursive since for each new question <em>becomes the root</em></li>
</ul>

<p>In the end, we will basically obtain:</p>

<p><img src="STF_CS229/image-20210818150841435.png" alt="image-20210818150841435" style="zoom:50%;" /></p>

<hr />

<p>Therefore, the idea is for a region of data $R_p$, we need some split $s_p$ such that:</p>

\[s_p(j,t) = \begin{cases}
\{ x|x_i &lt; t,\,x \in R_p \} &amp; \equiv R_1\\
\{ x|x_i \ge t,\,x \in R_p \} &amp; \equiv  R_2
\end{cases}\]

<p>where:</p>

<ul>
  <li>$j$ is the j-th feature you are splitting/looking at</li>
  <li>$t$​ is the threshold value for splitting</li>
  <li>this splitting splits the region $R_p$ into two regions $R_1, R_2$</li>
</ul>

<p>To decide <em>which split</em> to take on, we need to define a ==loss for each split==, and then pick the smallest lost. First we need to define a few things to make this formal:</p>

<ul>
  <li>$L(R)$ is the <strong>loss</strong> on a region $R$</li>
  <li>$C$ is the number of classes in our data</li>
  <li>$\hat{p}_c$ is the <strong>proportion</strong> of data in $R$​ that are in the class $c$</li>
</ul>

<p>In our tree model, we would have ==for each node==:</p>

\[\max_{j,t}(L(R_p) - (L(R_1) + L(R_2)))\]

<p>where:</p>

<ul>
  <li>$L(R_p)$ is basically defined by the <em>precious parent node</em>, so it is a constant here</li>
  <li>$L(R_1), L(R_2)$ are the loss of the children, so they needs to be optimized with $j,t$​</li>
  <li>so basically we want to minimize the loss of children</li>
</ul>

<hr />

<p><em>For Example: Simple Misclassification Loss</em></p>

<p>Then a one naive way of ==minimizing misclassification== is to define:</p>

\[L_{\mathrm{misclass}} = \text{number of misclassified data}\]

<p>Then the following splits would be <em>evenly good</em></p>

<p><img src="STF_CS229/image-20210818153520069.png" alt="image-20210818153520069" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>the root note basically predicts everything to be positive (since there are more positives), so 100 samples are missclassified</li>
  <li>all the children nodes <em>in this case</em> predicts positive, since there are more positives in the node.</li>
</ul>

<p>So you see that even though the number of misclassified samples is the same for both models, many would argue that the <strong>right one is better</strong> since it <em>isolated out more positive samples</em>.</p>

<p>So if you draw the shape:</p>

<p><img src="STF_CS229/image-20210823223214589.png" alt="image-20210823223214589" style="zoom: 50%;" /></p>

<hr />

<p>Therefore, we need one possibility would be something like ==cross-entropy loss==:</p>

\[L_\mathrm{cross} = - \sum_c \hat{p}_c \log_2 \hat{p}_c\]

<p>This has the advantage of a concave curve for loss.</p>

<p>If the split is <em>even</em>, i.e. 500 data to the left and 500 data to the right, then we will see:</p>

<p><img src="STF_CS229/image-20210823222948936.png" alt="image-20210823222948936" style="zoom:50%;" /></p>

<p>where the shape for $L$ gives:</p>

<ul>
  <li>the change in loss $\max_{j,t}(L(R_p) - (L(R_1) + L(R_2)))$ would be the gap drawn in the figure.</li>
</ul>

<p>Another would be the ==Gini loss==:</p>

\[\sum_c \hat{p}_c (1-\hat{p}_c)\]

<p>and this has a similar shape to that of the <strong>cross-entropy loss</strong> above.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>decision trees like this face a problem of <strong>overfitting</strong> if you let the tree grow freely, such that it will have eventually <strong>each node itself being a region</strong>. This demonstrates the ==high variance== nature of decision tree. To solve this issue, check out the section <a href="#Regularization of Decision Trees">Regularization of Decision Trees</a></p>
    </li>
    <li>
      <p>Another problem is that it has <strong>no additive structure</strong>. If you have a data generated where the separation is simply a <em>slant line</em>, then decision tree would need to do <em>lots of work</em>.</p>

      <p><img src="STF_CS229/image-20210826212716075.png" alt="image-20210826212716075" style="zoom:50%;" /></p>
    </li>
  </ul>
</blockquote>

<h3 id="regression-tree">Regression Tree</h3>

<p>The above basically uses decision tree for <strong>classification</strong>. We could also do the problem of <strong>regression</strong> using this model.</p>

<p>Consider the same case for skiing, where we have some continues values such as:</p>

<p><img src="STF_CS229/image-20210823223859969.png" alt="image-20210823223859969" style="zoom:50%;" /></p>

<p>For prediction, you would basically predict the <strong>average value</strong> inside some region $R_m$:</p>

\[\hat{y}_m = \frac{\sum_{i \in R_m} y_i}{|R_m|}\]

<p>In this case, your <strong>loss</strong> would be defined as:</p>

\[L_{\mathrm{squared}} = \frac{\sum_{i \in R_m}(y_i - \hat{y}_m)^2}{|R_m|}\]

<p>which basically would  deal with the <em>purity of data</em> inside some region $R_m$.</p>

<h3 id="regularization-of-decision-trees">Regularization of Decision Trees</h3>

<p>Some common approaches in this case would be:</p>

<ul>
  <li>min leaf size (how many nodes in each leaf/region)</li>
  <li>max depth of tree</li>
  <li>max number of nodes</li>
  <li>minimum decrease in loss (this might cause an issue of early-stopping, if you have some <em>higher order correspondence</em>)</li>
  <li>pruning (grow the tree first, then prune it using misclassification of validation set data)</li>
</ul>

<h3 id="runtime">Runtime</h3>

<p>What actually happens at run time would be the following.</p>

<p>First, consider the case of having:</p>

<ul>
  <li>$n$​ examples/data samples</li>
  <li>$f$ features for each sample</li>
  <li>$d$ depth of our result tree</li>
</ul>

<p>Then we would have gone through:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Train Time</th>
      <th>Test Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Computation Time</td>
      <td>$O(nfd)$</td>
      <td>$O(d)$ since we are going through the tree</td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>commonly $d &lt; log_2n$​</li>
  <li>train time is $O(nfd)$ because the cost of <strong>each sample at each node</strong> is $O(f)$, since you basically just need try $f$ questions, and <strong>each sample</strong> is part of $O(d)$​​ nodes (do it $O(d)$ times), i.e. your data ==matrix== would be $nf$​.</li>
  <li>Therefore, since depth is usually $log_2 n$​, so it is generally a fast algorithm</li>
</ul>

<h3 id="summary">Summary</h3>

<p>For the decision tree model we covered <strong>just now</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Advantage</th>
      <th>Disadvantage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Easy to explain</td>
      <td>High Variance</td>
    </tr>
    <tr>
      <td>Result/model interpretable</td>
      <td>Bad at data being additive</td>
    </tr>
    <tr>
      <td>Fast</td>
      <td>Low predictive accuracy (due to the two above)</td>
    </tr>
    <tr>
      <td>Categorical Variables also work</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>Low predictive accuracy hurts a lot</li>
</ul>

<p>Therefore, usually what we do is to have an <strong>ensemble of decision trees</strong>.</p>

<h2 id="ensemble">Ensemble</h2>

<p>Consider the case of having $n$ random variables $x_i$​ being IID. And suppose that the distribution has $\mathrm{Var}(x_i) = \sigma^2$.</p>

<p>Then given the assumption results in:</p>

\[\mathrm{Var}(\bar{x}) = \mathrm{Var}\left(\frac{1}{n}\sum_i x_i\right) = \frac{\sigma^2}{n}\]

<p>Sometimes, your random variable will be <strong>correlated</strong>. Suppose they are correlated by $\rho$. Then we have $x_i$ being distributed ID, and that:</p>

\[\mathrm{Var}(\bar{x}) =\rho\sigma^2 + \frac{1-\rho}{n}\sigma^2\]

<p>wheree:</p>

<ul>
  <li>notice that if they are completely correlated, $\rho=1$, then the average of the variance is just the variance of <em>one variable</em></li>
  <li>if they are completely <em>independent</em>, $\rho=0$, then you restore equation 157.</li>
</ul>

<blockquote>
  <p><strong>Idea</strong></p>

  <ul>
    <li>Now, the idea is to consider each random variable being our <strong>model itself</strong>, sampled from some distribution. Our aim is to get a <strong>final aggregated model</strong> that who comes from a distribution of a <em>lower variance</em>. Therefore, effectively we want to ==”average/ensemble” different models== to improve the performance, i.e. achieve lower variance.</li>
  </ul>
</blockquote>

<p>Now, some ways to achieve ensemble mentioned above, some methods include:</p>

<ul>
  <li>having different learning algorithms (e.g. decision tree, regression, SVM, etc.)</li>
  <li>having different training data set</li>
  <li>[<strong>more commonly used</strong>] bagging, e.g. <em>random forest</em></li>
  <li>[<strong>more commonly used</strong>] boosting, e.g. <em>xgboost</em></li>
</ul>

<h3 id="bagging---bootstrap-aggregation">Bagging - Bootstrap Aggregation</h3>

<p>Bagging stands for <strong>b</strong>ootstrap <strong>agg</strong>regation.</p>

<p>Basically, the idea is to have</p>

<ol>
  <li>==assume== your entire train dataset to be the true population $P$</li>
  <li>each training set $Z_i$ is sampled without replacement from $P$​</li>
  <li>each tree model $G_i$​ is then trained on $Z_i$​</li>
  <li>the final result is <strong>the average result of all models $G_i$​</strong>​</li>
</ol>

<p>To be more specific, suppose you have $M$​ models trained. Now given a new input $x’$​, you basically do:</p>

\[G_{\mathrm{final}}(x') = \frac{\sum_{i=1}^m G_i(x')}{M}\]

<p>The net effect is that the ==variance== of your model decreases since:</p>

<ul>
  <li>$\frac{1-\rho}{n}\sigma^2$ in equation 158 will be decreased since $M=n$ basically increases as the number of trained model increases</li>
  <li>$\rho$ in equation 158 will also be decreased since bootstrapping make them less corelated</li>
</ul>

<p>However, the drawback is that your ==bias== would increase (slightly) since:</p>

<ul>
  <li>each model is trained on <em>less data</em>/random subsampling</li>
</ul>

<blockquote>
  <p><strong>Bagging + Decision Tree</strong></p>

  <ul>
    <li>Recall that decision trees have ==high variance but low bias==. Therefore, this works <strong>nicely with Bagging</strong>.</li>
  </ul>
</blockquote>

<h4 id="random-forest">Random Forest</h4>

<p>Basically it is the Bagging approach + <strong>more decorrelation</strong> of models, to further drive $\rho$​ down.</p>

<p>This is achieved by considering <strong>only a fraction of features at each split</strong>, which would result in lower correlation between models.</p>

<ul>
  <li>think of the case where there is a <em>strong feature</em> that works nicely for classification. Then most of your trees will have that being the first split, hence somewhat correlated. However, this can be avoided if you ==force== <em>only a fraction of features</em> being considered.</li>
</ul>

<h3 id="boosting">Boosting</h3>

<p>This is more like the opposite, it is <strong>decreasing the bias</strong> but <strong>increasing the variance</strong>. The key difference is that this approach is about ==addition==.</p>

<p>The basic idea is as follows (this is actually ADABoost):</p>

<ol>
  <li>train a tree $G_i$ by forcing a <em>depth of $1$​​</em> (so by only allowing to ask one question, we want to minimize variance)</li>
  <li>compute the error of this model $\mathrm{err}_i$ and assign this <strong>model</strong> a weight $\alpha_i \propto \log(\frac{1-\mathrm{err_i}}{\mathrm{err_i}})$</li>
  <li><em>reweight</em> the misclassified samples in this model</li>
  <li>repeat step from step 1</li>
</ol>

<p>Now, when you get $M$ trees, your final result $G$ is basically, for a new data $x’$:</p>

\[G(x') = \sum_{i=1}^M \alpha_i G_i(x')\]

<p>For more details, read the extra-notes.</p>

<h1 id="introduction-to-neural-network">Introduction to Neural Network</h1>

<p>Heuristics:</p>

<ul>
  <li>we will start with <em>logistic regression</em> (revision), and then go to <em>neural network</em> from it</li>
  <li>then we talk a little bit about deep learning (a set of techniques/algorithms)
    <ul>
      <li>different from other algorithm, DL are computationally expensive, and the data used is usually large</li>
    </ul>
  </li>
</ul>

<p>For instance, consider the task of finding cats in images. If there is a cat in the image, output 1, otherwise 0.</p>

<ul>
  <li>suppose images are of size $64\times 64$. Then you basically have a $64 \times 64 \times 3$ numbers.</li>
</ul>

<p>Then, what we can do is:</p>

<ol>
  <li>convert each picture to a vector, of size $(64\times 64 \times 3, 1)$</li>
  <li>feed the vector into a logistic regression algorithm, which basically learns $\theta$ for the <strong>sigmoid</strong> function we discussed before: $1/(1+e^{-\theta^T x}) \equiv \sigma(\theta^Tx) = \sigma(w^Tx + b)$</li>
  <li>finally, we just output $\hat{y} = \sigma(\theta^T x) = \sigma(w^Tx + b)$. Usually $w$ is called <strong>weights</strong> and $b$ is called <strong>bias</strong> here.</li>
</ol>

<p>Therefore, the learning algorithm basically does:</p>

<ol>
  <li>initiate some $w, b$</li>
  <li>find the optimal $w, b$. This means that we need to ==define a loss function==.</li>
  <li>output $w,b$</li>
</ol>

<p>Recall that for logistic regression, we had the <em>logistic loss</em>, and we want to minimize:</p>

\[\mathcal{L} = - [y\log(\hat{y})+(1-y)\log(1-\hat{y})]\]

<p>which comes from <em>maximizing the log likelihood.</em></p>

<p>Then, we can basically find the optimal by doing <strong>gradient descent</strong>:</p>

<ul>
  <li>at every batch of data, compute $w:=w - \alpha \frac{\partial \mathcal{L}}{\partial w}$, $b:=b - \alpha \frac{\partial \mathcal{L}}{\partial b}$</li>
</ul>

<p>Now, we show you how it can evolve into a <em>neuron network</em>.</p>

<h2 id="terminologies-and-introduction">Terminologies and Introduction</h2>

<p>Some common terms to know would be:</p>

<ol>
  <li>
    <p><strong>Neuron = Linear + Activation</strong></p>

    <p>In the case of logistic regression, the part $w^T x + b$ is the ==linear part==, and the $\sigma(w^Tx + b)$ is the ==activation==</p>
  </li>
  <li>
    <p><strong>Model = Architecture + Parameter</strong></p>

    <p>In the case of logistic regression, we have an ==architecture== of 1 neuron, and the ==parameter== $w, b$</p>
  </li>
</ol>

<p><em>Example</em></p>

<p>Consider now the goal of finding a cat/lion/iguana in the image.</p>

<p>Then, since we have three “tasks” to find, we can use <strong>three neurons</strong>:</p>

<p><img src="STF_CS229/image-20210827215318068.png" alt="image-20210827215318068" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>basically the input is linked/goes through all neurons $a_i^{[1]}$</li>
  <li>the subscript $[1]$ means we are at the <strong>first layer</strong>. Later on we will show a multilayered one.</li>
  <li>therefore, basically the parameters becomes $\left(w_1^{[1]}, b_1^{[1]}\right), \left(w_2^{[1]}, b_2^{[1]}\right), \left(w_3^{[1]}, b_3^{[1]}\right)$</li>
  <li>since we have this architecture, then the <strong>label</strong> should look like $[1, 0, 0]^T$ (if it is a cat). For a lion $[0,1,0]^T$. The good part of this is that then the <strong>job of each neuron</strong> is clear and simple.</li>
</ul>

<p>Note that in this case, this will be robust since if you have a picture containing <em>both a cat and a lion</em>, i.e. $[1,1,0]^T$, then it will <strong>still work</strong>. In fact, this is due to the neurons as being <strong>decoupled</strong>, and also our images are generated independently.</p>

<p>Therefore, the <strong>loss</strong> technically becomes:</p>

\[\mathcal{L}_{3\mathrm{neurons}} = -\sum_{k=1}^3 [y_k\log(\hat{y}_k)+(1-y_k)\log(1-\hat{y}_k)]\]

<p>which sums up the three components, but additionally note that (taking $\mathcal{L}<em>{3N} = \mathcal{L}</em>{3\mathrm{neurons}}$):</p>

<ul>
  <li>$\partial \mathcal{L}_{3N}/{\partial w_i^{[1]}}$ has the same result as $\partial \mathcal{L}/{\partial w}$ since the variables $w_i^{[1]}$ are <strong>decoupled</strong></li>
</ul>

<p>However, one problem is that if the data is ==unbalanced==, then some neurons ==might not be trained as much as others==.</p>

<ul>
  <li>one way to deal with it is to use the <em>SoftMax regression</em>, which basically outputs the <em>most likely</em> instead of all three</li>
</ul>

<hr />

<p><em>For Example:</em></p>

<p>Consider the case where our goal is to find cat/lion/iguana in the image, but we are <strong>told that each image only contains one animal</strong>.</p>

<p>Then we can use the SoftMax regression, which basically gives the probability of the <em>best possible classification</em> (we talked about it before in multi-class classification):</p>

<p><img src="STF_CS229/image-20210828174632045.png" alt="image-20210828174632045" style="zoom:50%;" /></p>

<p>where each neuron outputs:</p>

<ul>
  <li>$e^{z_i^{[1]}}/\sum_{k=1}^3 e^{z_k^{[1]}}$ and $z_i^{[1]} \equiv w_{i}^{[1]} x + b_{i}^{[1]}$ includes the parameters</li>
</ul>

<p>Since now they are coupled, we need to define a <strong>different</strong> loss function to make the gradient descent easier:</p>

\[\mathcal{L}_{CE} = \sum_{k=1}^3 y_k \log{\hat{y}_k}\]

<p>where $\mathcal{L}_{CE}$ is the <strong>cross entropy</strong> loss.</p>

<h2 id="neural-networks">Neural Networks</h2>

<p>Now let’s consider the same task, to find a cat in an image, but with a <strong>more complicated neural network</strong>:</p>

<p><img src="STF_CS229/image-20210828184546911.png" alt="image-20210828184546911" style="zoom: 50%;" /></p>

<p>where the restriction is that:</p>

<ul>
  <li>for classification of $n$-classes, we should have the <strong>last layer</strong> consisting of $n$ neurons</li>
  <li>for regression, the last layer should have only $1$ neuron</li>
  <li>in each layer, neurons are disconnected</li>
  <li>the second layer is also called the <strong>hidden layer</strong>, since it does not directly touch the input or the output</li>
</ul>

<p>Notice that if the input vector has size $(n,1)$, then we will have $(3n+3) + (2\times 3 + 2) + (1\times 2 + 1)$ parameters. If we train this network, we will get typically the <em>first layer</em> recognizing edges in the image, <em>second layer</em> recognizng things like “ears”, “mustache” of the cat, and the <em>last neuron</em> figures out if there is a cat or not.</p>

<hr />

<p>Another example would be to predict the house price given information such as size, zip code, etc.</p>

<p>One sensible idea to <strong>do it by hand</strong> would be:</p>

<p><img src="STF_CS229/image-20210828233117700.png" alt="image-20210828233117700" style="zoom:50%;" /></p>

<p>note that some key differences here are:</p>

<ul>
  <li>
    <p>inputs are ==no longer fully connected== to the neurons in the first level</p>

    <p>(as we go on, neural network will usually be able to figure which inputs are interesting by themselves. The idea is to <em>first fully connect the network</em>, and then <em>allow weights</em>.)</p>
  </li>
  <li>
    <p>sometimes this is also called End-to-End learning/Black Box. This is because usually we will have <strong>no idea</strong> how/how to interpret the hidden layers in the middle will do.</p>
  </li>
</ul>

<h3 id="propagation-equation">Propagation Equation</h3>

<p>Now let’s deal with the math of how this learning mentioned above work. Recall this setup:</p>

<p><img src="STF_CS229/image-20210828184546911.png" alt="image-20210828184546911" style="zoom: 50%;" /></p>

<p>First, we have:</p>

<ul>
  <li>$z^{[1]} = w^{[1]}x + b^{[1]}$ being the linear of first layer, and $a^{[1]} = \sigma(z^{[1]})$ being the activation</li>
  <li>$z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$ being the linear of second layer, and $a^{[2]} = \sigma(z^{[2]})$ being the activation</li>
  <li>$z^{[3]} = w^{[3]}a^{[2]} + b^{[3]}$ being the linear of third layer, and $a^{[3]} = \sigma(z^{[3]})$ being the activation</li>
</ul>

<p>Note that a difference here is that we are using <strong>one equation</strong> for the entire layer. This means the shape of parameters is becoming:</p>

<ul>
  <li>$z^{[1]}\to (3,1);\quad w^{[1]}\to (3,n);\quad x\to (n,1)$ and the activation $a^{[1]}\to (3,1)$</li>
  <li>$z^{[2]}\to (2,1);\quad w^{[2]}\to (2,3);\quad a^{[1]}\to (3,1)$ and the activation $a^{[2]}\to (2,1)$</li>
  <li>$z^{[3]}\to (1,1);\quad w^{[3]}\to (1,2);\quad a^{[2]}\to (2,1)$ and the activation $a^{[3]}\to (1,1)$</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>the architecture and the idea of having input propagated forward to get the output is also called ==forward propagation==.</li>
  </ul>
</blockquote>

<p>Now, if we have $m$ data points, then we consider the input of:</p>

\[X = \begin{bmatrix}
| &amp; ... &amp; |\\
x^{(1)} &amp; ... &amp; x^{(m)}\\
| &amp; ... &amp; |
\end{bmatrix}\]

<p>Then, the first linear part should have shape:</p>

\[Z^{[1]} = w^{[1]}X + \tilde{b}^{[1]} = \begin{bmatrix}
| &amp; ... &amp; |\\
z^{[1](1)} &amp; ... &amp; z^{[1](m)}\\
| &amp; ... &amp; |
\end{bmatrix}\]

<p>where:</p>

<ul>
  <li>$Z^{[1]} \to (3,m)$</li>
  <li>$w^{[1]} \to (3,n)$ is the ==same as before==</li>
  <li>$\tilde{b}^{[1]} = \begin{bmatrix}
| &amp; … &amp; |<br />
b^{[1]} &amp; … &amp; b^{[1]}<br />
| &amp; … &amp; |
\end{bmatrix}$basically repeats the original stuff$m$ times to match the size​</li>
</ul>

<p>One advantage of using <em>vectors</em> is that we want to utilize our GPU to full capacity (parallel computation).</p>

<h3 id="optimization">Optimization</h3>

<p>Now we have the equations for each neuron, we need to consider the ==loss function== and the ==cost function== (cost usually refers to the error of the batch of $m$ examples, whereas loss refers to error of $1$ example).</p>

<p>Our task here is to optimize $w^{[i]}, b^{[i]}; i \in [1,3]$. Since the output is the last layer $\hat{y} = a^{[3]}$, we have the <strong>cost</strong> being:</p>

\[\mathcal{J}(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}^{(i)}\]

<p>and the <strong>loss</strong> can simply be the logistic loss:</p>

\[\mathcal{L}^{(i)} = - [y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})]\]

<p>For a batch gradient descent, we know that we should get something like:</p>

\[\forall l=1,...,3: \begin{cases}
w^{[l]} := w^{[l]} - \alpha \frac{\partial \mathcal{J}}{\partial w^{[l]}} \\
b^{[l]} := b^{[l]} - \alpha \frac{\partial \mathcal{J}}{\partial b^{[l]}}
\end{cases}\]

<p>The question now is which <strong>layer should we start first</strong>? Since the idea of gradient descent is to move the parameters closer to the correct output, it makes more sense to <strong>start with the last layer first</strong>, since its effect on the output is the most obvious:</p>

\[\frac{\partial \mathcal{J}}{\partial w^{[3]}} = \frac{\partial \mathcal{J}}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial w^{[3]}}\]

<p>this computation of would make the later calculation easier, since:</p>

\[\frac{\partial \mathcal{J}}{\partial w^{[2]}} = \frac{\partial \mathcal{J}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial w^{[2]}} = \frac{\partial \mathcal{J}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial w^{[2]}}\]

<p>notice that the quantity $\partial \mathcal{J}/{\partial z^{[3]}}$ was already computed in the previous equation 169, so it saved some work. Similarly, we do:</p>

\[\frac{\partial \mathcal{J}}{\partial w^{[2]}} =  \frac{\partial \mathcal{J}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial a^{[1]}}\frac{\partial a^{[1]}}{\partial z^{[1]}}\frac{\partial z^{[1]}}{\partial w^{[1]}}\]

<p>Lastly, we just need to figure out the formula for $\partial \mathcal{J}/{\partial w^{[3]}}$.</p>

<hr />

<p><em>Proof</em></p>

<p>First recall that when we take derivatives of a sigmoid function $\sigma$:</p>

\[\sigma'(x) = \sigma(x)(1-\sigma(x))\]

<p>in this case, since the activation layers are the sigmoid function, we have $a^{[i]} = \sigma(z^{[i]})$. For the last layer, notice that:</p>

\[\hat{y} = a^{[3]} = \sigma(z^{[3]})=\sigma(w^{[3]}a^{[2]}+b^{[3]})\]

<p>Therefore, some useful derivatives to compute beforehand would be:</p>

\[\begin{align*}
\frac{\partial \hat{y}}{\partial w^{[3]}}
&amp;=\frac{\partial \sigma(z^{[3]})}{\partial w^{[3]}} \\
&amp;= \sigma'(z^{[3]}) \frac{\partial z^{[3]}}{\partial w^{[3]}} \\
&amp;= \sigma(z^{[3]})(1-\sigma(z^{[3]})) \frac{\partial (w^{[3]}a^{[2]}+b^{[3]})}{\partial w^{[3]}}\\
&amp;= a^{[3]}(1-a^{[3]})\frac{\partial (w^{[3]}a^{[2]}+b^{[3]})}{\partial w^{[3]}}\\
&amp;=  a^{[3]}(1-a^{[3]}) a^{[2]T}
\end{align*}\]

<p>Now, tarting with $\partial \mathcal{J}/{\partial w^{[3]}}$ by computing $\partial \mathcal{L}/{\partial w^{[3]}}$:</p>

\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w^{[3]}}
&amp;= - \left[y^{(i)}\frac{\partial }{\partial w^{[3]}}\log(\hat{y}^{(i)})+(1-y^{(i)})\frac{\partial }{\partial w^{[3]}}\log(1-\hat{y}^{(i)})\right] \\
&amp;= - \left[ y^{(i)} \frac{1}{a^{[3]}}a^{[3]}(1-a^{[3]})a^{[2]T}+(1-y^{(i)})\frac{-1}{1-a^{[3]}}a^{[3]}(1-a^{[3]})a^{[2]T} \right] \\
&amp;= -[y^{(i)}(1-a^{[3]})a^{[2]T}-(1-y^{(i)})a^{[3]}a^{[2]T}] \\
&amp;= -[y^{(i)}a^{(2)T}-a^{[3]}a^{[2]T}]\\
&amp;= - (y^{(i)}-a^{[3]})a^{[2]T}
\end{align*}\]

<p>where we assume $a^{[3]}$ is the activation/prediction for the $i$-th data sample.</p>

<p>Therefore, we easily obtain:</p>

\[\frac{\partial \mathcal{J}}{\partial w^{[3]}} = \frac{-1}{m} \sum_{i=1}^m (y^{(i)}-a^{[3]})a^{(2)T}\]

<p>This completes the computation for $\partial \mathcal{J}/{\partial w^{[3]}}$.</p>

<hr />

<p>After we have $\partial \mathcal{L}/{\partial w^{[3]}}$, we need to consider how to <strong>obtain</strong> $\partial \mathcal{L}/{\partial w^{[2]}}$ using the result of $\partial \mathcal{L}/{\partial w^{[3]}}$, as we mentioned before. Notice that since $a^{[2]T}=\partial z^{[3]}/{\partial w^{[3]}}$:</p>

\[\frac{\partial \mathcal{L}}{\partial w^{[3]}}=- (y^{(i)}-a^{[3]})a^{[2]T}=\frac{\partial \mathcal{L}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial w^{[3]}}\]

<p>Therefore, knowing that $- (y^{(i)}-a^{[3]})={\partial \mathcal{L}}/{\partial z^{[3]}}$, we consider:</p>

\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial w^{[2]}} 
&amp;= \frac{\partial \mathcal{L}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial w^{[2]}}\\
&amp;= (a^{[3]}-y)\cdot w^{[3]T} \cdot a^{[2]}(1-a^{[2]}) \cdot a^{[1]T} \\
&amp;=  w^{[3]T} * a^{[2]}(1-a^{[2]})\cdot (a^{[3]}-y)\cdot a^{[1]T}
\end{align*}\]

<p>where the last step needs some explanation:</p>

<ul>
  <li>the $*$ means <strong>element-wise</strong> multiplication between two vectors. This comes from the fact that we are taking derivatives of the sigmoid function.</li>
  <li>since we need the result to be the same shape as $w^{[2]} \to (2,3)$, the second last step has the <strong>wrong shape</strong>. Some rigorous proof is omitted here, and it turns out that we need some change in order of the terms in the last step to make it correct.</li>
</ul>

<p>Lastly, the term $\partial \mathcal{L}/{\partial w^{[1]}}$ can be done in a similar manner and is left as an exercise.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>therefore, since we are <strong>computing the optimization</strong> backward, this part is called the ==backward propagation==</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>In essence:</strong></p>

  <ul>
    <li>forward propagation remembers the <strong>connection/path between activation/neurons</strong></li>
    <li>backward propagation <strong>optimizes</strong> the parameters</li>
  </ul>
</blockquote>


  </div><a class="u-url" href="/lectures/2021@columbia/STFCS229_Machine_Learning.html/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
