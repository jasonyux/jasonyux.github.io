<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COMS4705 NLP part2 | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="COMS4705 NLP part2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Continuation from:" />
<meta property="og:description" content="Continuation from:" />
<link rel="canonical" href="/lectures/2021@columbia/COMS4705_NLP_part2.html/" />
<meta property="og:url" content="/lectures/2021@columbia/COMS4705_NLP_part2.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-09T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COMS4705 NLP part2" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-09T00:00:00-04:00","datePublished":"2022-05-09T00:00:00-04:00","description":"Continuation from:","headline":"COMS4705 NLP part2","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2021@columbia/COMS4705_NLP_part2.html/"},"url":"/lectures/2021@columbia/COMS4705_NLP_part2.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/learning">Blog</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
	<script>
		MathJax = {
		  tex: {
			inlineMath: [['$', '$'], ['\\(', '\\)']],
			displayMath: [['$$', '$$'], ['\\[', '\\]']]
		  }
		};
	</script>
	<script id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
	</script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COMS4705 NLP part2</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-05-09T00:00:00-04:00" itemprop="datePublished">
        May 9, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Continuation from:</p>

<h1 id="semantic-role-labeling">Semantic Role Labeling</h1>

<p>Here we consider solving the problem of, for each clause, determine the <strong>semantic role played by each noun phrase</strong> that is an argument to the verb.</p>

<p>We’ll introduce</p>

<ul>
  <li><strong>semantic role labeling</strong>, the task of assigning roles to spans in sentences, and</li>
  <li><strong>selectional restrictions</strong>, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.</li>
</ul>

<p>Examples would be:</p>

<p><img src="NLP/image-20220302195745957.png" alt="image-20220302195745957" style="zoom:50%;" /></p>

<p>Then, this would be useful as it can act as a <strong>shallow meaning representation</strong> that can let us <strong>make simple inferences</strong> that aren’t possible from the pure surface string of words:</p>

<ul>
  <li><strong>Question Answering</strong>, e.g. “Who” questions usually use Agents</li>
  <li><strong>Machine Translation Generation</strong></li>
</ul>

<h2 id="semantic-roles">Semantic Roles</h2>

<p>A variety of semantic role labels have been proposed, common ones are:</p>

<ul>
  <li><strong>Agent</strong>: Actor of an action</li>
  <li><strong>Patient</strong>: Entity affected by the action</li>
  <li><strong>Instrument</strong>: Tool used in performing action.</li>
  <li><strong>Beneficiary</strong>: Entity for whom action is performed</li>
  <li><strong>Source</strong>: Origin of the affected entity</li>
  <li><strong>Destination</strong>: Destination of the affected entity</li>
</ul>

<p>Although there is no universally agreed-upon set of roles, the above list some thematic roles that have been used in various  computational papers</p>

<hr />

<p>However, there are many <strong>problems</strong> with using those as-as. For instance, consider these possible realizations of the thematic arguments of the verb <em>break</em>:</p>

<p><img src="NLP/image-20220302201246632.png" alt="image-20220302201246632" style="zoom: 80%;" /></p>

<p>These examples suggest that <em>break</em> has (at least) the possible arguments <code class="language-plaintext highlighter-rouge">AGENT</code>, <code class="language-plaintext highlighter-rouge">THEME</code>, and <code class="language-plaintext highlighter-rouge">INSTRUMENT</code>.</p>

<blockquote>
  <p>The <strong>set of thematic role arguments</strong> taken by a verb is often called the <strong>thematic grid</strong>, q-grid, or <strong>case frame</strong></p>
</blockquote>

<p>An example would be for <em>break</em>:</p>

<p><img src="NLP/image-20220302201350908.png" alt="image-20220302201350908" style="zoom:80%;" /></p>

<p>Additionally, researchers attempting to define role sets often find they need to <strong>fragment</strong> a role like <code class="language-plaintext highlighter-rouge">AGENT </code>or <code class="language-plaintext highlighter-rouge">THEME </code>into many specific roles. And it has proved difficult to <strong>formally define</strong> the thematic roles. This essentially leads to two class of models for solution</p>

<ul>
  <li>
    <p>define <strong>generalized semantic roles</strong> that abstract over the specific thematic roles.</p>

    <p>For example, <code class="language-plaintext highlighter-rouge">PROTO-AGENT</code> and <code class="language-plaintext highlighter-rouge">PROTO-PATIENT</code> are generalized roles that express <strong>roughly</strong> agent-like and roughly patient-like meanings, and those roles are not defined by some “necessary tabular conditions”, but rather by <strong>a set of heuristic features</strong> that accompany more agent-like or more patient-like meanings.</p>

    <p>Then the <strong>more an argument displays agent-like properties</strong> (e.g. being volitionally involved in the event), the more likely it is to be classified as <code class="language-plaintext highlighter-rouge">PROTO-AGENT</code>.</p>
  </li>
  <li>
    <p>define <strong>semantic roles that are specific</strong> to a particular verb or a particular group of semantically related verbs or nouns</p>
  </li>
</ul>

<p>The first of them leads to the <strong>PropBank</strong> dataset, which uses both <code class="language-plaintext highlighter-rouge">proto</code>-roles and verb-specific semantic roles.</p>

<p>The second leads to <strong>FrameNet</strong> dataset, which uses semantic roles that are specific to a general semantic idea called a <em>frame</em>.</p>

<blockquote>
  <p><strong>PropBank</strong> is a verb-oriented resource, while <strong>FrameNet</strong> is centered on the more abstract notion of frames, which generalizes descriptions across similar verbs.</p>

  <p>Of course, both of which have sentences annotated with semantic roles.</p>
</blockquote>

<h2 id="the-proposition-bank">The Proposition Bank</h2>

<p>Because of the difficulty of defining a universal set of thematic roles, the semantic roles in PropBank are defined with <strong>respect to an individual verb sense</strong>. Basically we have <strong>verb sense-specific labels</strong> (i.e. depending on the verb-sense, we will have different labels for arguments.)</p>

<p>In general, we will use labels names: <code class="language-plaintext highlighter-rouge">Arg0</code>, <code class="language-plaintext highlighter-rouge">Arg1</code>, <code class="language-plaintext highlighter-rouge">Arg2</code>, and so on:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Arg0 </code>represents the <code class="language-plaintext highlighter-rouge">PROTO-AGENT</code></li>
  <li><code class="language-plaintext highlighter-rouge">Arg1</code>, the <code class="language-plaintext highlighter-rouge">PROTO-PATIENT</code></li>
  <li><code class="language-plaintext highlighter-rouge">Arg2 </code>is often the benefactive, instrument, attribute, or end state</li>
  <li><code class="language-plaintext highlighter-rouge">Arg3 </code>the start point, benefactive, instrument, or attribute,</li>
  <li><code class="language-plaintext highlighter-rouge">Arg4 </code>the end point</li>
</ul>

<p>Examples include:</p>

<p>| <img src="NLP/image-20220302204216699.png" alt="image-20220302204216699" /> | <img src="NLP/image-20220302204206118.png" alt="" /> |
| ———————————————————– | ———————————— |</p>

<p>Additionally, PropBank also has a number of non-numbered arguments called <code class="language-plaintext highlighter-rouge">ArgMs</code>, (<code class="language-plaintext highlighter-rouge">ArgMTMP</code>, <code class="language-plaintext highlighter-rouge">ArgM-LOC</code>, etc.) which represent modification or adjunct meanings. As those are pretty much the same across verb senses, they are not listed with each frame file. However, they could be useful for training systems. Some examples of <code class="language-plaintext highlighter-rouge">ArgM</code>’s include:</p>

<p><img src="NLP/image-20220302204509497.png" alt="image-20220302204509497" style="zoom:80%;" /></p>

<h2 id="framenet">FrameNet</h2>

<blockquote>
  <p>Whereas roles in the PropBank project are specific to an individual <strong>verb</strong>, roles in the FrameNet project are specific to a <strong>frame</strong>.</p>
</blockquote>

<p>A <strong>frame</strong> is basically the holistic background knowledge that unites these words, such as:
\(\text{reservation, flight, travel, buy, price, cost, fare}\)
all defined with respect to a coherent chunk of common-sense <strong>background information concerning air travel</strong>.</p>

<blockquote>
  <p>Therefore, FrameNet defines a set of <strong>frame-specific semantic roles,</strong> called <strong>frame elements,</strong> and includes a set of predicates that use these roles. (i.e. those roles of a word will be different across different frames)</p>

  <ul>
    <li>additionally, FrameNet also codes relationships between frames, allowing frames to inherit from each other, or representing relations between frames like causation</li>
  </ul>
</blockquote>

<p>For example, the <code class="language-plaintext highlighter-rouge">change_position_on_a_scale</code> frame is defined as follows:</p>

<ul>
  <li>This frame consists of words that indicate the change of an Item’s position on a scale (the <code class="language-plaintext highlighter-rouge">Attribute</code>) from a starting point (<code class="language-plaintext highlighter-rouge">Initial value</code>) to an end point (<code class="language-plaintext highlighter-rouge">Final value</code>).</li>
</ul>

<p>And example sentences with their labels include:</p>

<p><img src="NLP/image-20220302205208263.png" alt="image-20220302205208263" style="zoom: 80%;" /></p>

<p>so if a word/verb (predicate) is in this frame, the above would be the labels. Verbs/predicates used in this frame looks like:</p>

<p><img src="NLP/image-20220302205628334.png" alt="image-20220302205628334" style="zoom:80%;" /></p>

<h2 id="semantic-role-labeling-models">Semantic Role Labeling Models</h2>

<p>Now we get to the task of Semantic role labeling (sometimes shortened as SRL): automatically <strong>finding the semantic roles of each argument of each predicate</strong> in a sentence. This is often done by:</p>

<ul>
  <li>supervised machine learning</li>
  <li>using FrameNet and PropBank to <em>specify predicates, define roles, and provide training and test sets</em>.</li>
</ul>

<h3 id="feature-based-algorithm-for-srl">Feature-based Algorithm for SRL</h3>

<p>A simplified feature-based semantic role labeling algorithm basically do:</p>

<ol>
  <li>assign a parse (e.g. constituency parsing) of an input string</li>
  <li>traverse the tree to find <strong>all predicates</strong></li>
  <li>for each <strong>node that is a predicate</strong>:
    <ol>
      <li>do a classification on that node, any standard classification algorithms can be used.</li>
      <li>this can be done either by finding some <strong>feature representation</strong> of it, or using idea such as GNN</li>
    </ol>
  </li>
</ol>

<p>This results in a 1-of-$N$ classifier, i.e. where basically choosing one out of $N$ potential semantic roles (plus 1 for an extra <code class="language-plaintext highlighter-rouge">None</code> role for non-role constituents). The algorithm hence looks like:</p>

<p><img src="NLP/image-20220302210527944.png" alt="image-20220302210527944" style="zoom:80%;" /></p>

<p>And the parse we did in step one could look like:</p>

<p><img src="NLP/image-20220302210628528.png" alt="image-20220302210628528" style="zoom: 80%;" /></p>

<p>The general idea of those algorithms can be summarized into the following steps:</p>

<ol>
  <li><strong>Pruning</strong>: Since only a small number of the constituents in a sentence are arguments of any given predicate, many systems use simple heuristics to prune unlikely constituents.</li>
  <li><strong>Identification</strong>: a binary classification of each node as an argument to be labeled or a <code class="language-plaintext highlighter-rouge">NONE</code>.</li>
  <li><strong>Classification</strong>: a 1-of-N classification of all the constituents that were labeled as arguments by the previous stage</li>
</ol>

<p>However, since this <strong>labels each argument of a predicate independently</strong>, it is ingoing interactions between arguments, as we know the semantic roles are not independent.</p>

<p>Therefore, thus often <strong>add a fourth step to deal with global consistency across the labels in a sentence</strong>. This can be done by:</p>

<ul>
  <li>local classifiers can return a list of possible labels associated with probabilities for each constituent,</li>
  <li>a <strong>second-pass</strong> Viterbi decoding or re-ranking approach can be used to choose the best consensus label.</li>
</ul>

<hr />

<p><strong>Features</strong> for Semantic Labelling Nodes</p>

<p>Common basic features templates mentioned above include</p>

<ul>
  <li>Phrase type: The syntactic label of the candidate role, the <strong>filler</strong> (e.g. <code class="language-plaintext highlighter-rouge">NP</code>).</li>
  <li>Parse tree path: The path in the parse tree between the <strong>predicate</strong> and the candidate role filler.</li>
  <li>Position: Does candidate role filler precede or follow the predicate in the sentence?</li>
  <li>Voice: Is the predicate an active or passive verb?</li>
  <li>Head Word: What is the head word of the candidate role filler?</li>
</ul>

<p>An example would be for the predicate <em>bit</em>:</p>

<p><img src="NLP/image-20220302215527363.png" alt="image-20220302215527363" style="zoom: 50%;" /></p>

<hr />

<p>Problems that this method will suffer:</p>

<ul>
  <li>Due to <strong>errors in syntactic parsing</strong>, the parse tree is likely to be incorrect.</li>
  <li>Can have <strong>many other useful features</strong>.</li>
</ul>

<h3 id="neural-algorithm-for-srl">Neural Algorithm for SRL</h3>

<blockquote>
  <p>A simple neural approach to SRL is to treat it as a <strong>sequence labeling task</strong> like named-entity recognition, using the BIO approach.</p>
</blockquote>

<p>Assume that we are given the predicate and the task is just <strong>detecting and labeling spans</strong>. This means that we will have:</p>

<ul>
  <li><strong>Input</strong>: sentence + the predicate, which will be separated by the <code class="language-plaintext highlighter-rouge">SEP</code> tag as shown in the example below</li>
  <li><strong>Output</strong>: semantic labels + BIO tags, as each label is a constituent (can span over more than one word)
    <ul>
      <li>recall that BIO tags are: beginning, inside, outside.</li>
    </ul>
  </li>
</ul>

<p>An example architecture would be feeding it into a transformer:</p>

<p><img src="NLP/image-20220302211635684.png" alt="image-20220302211635684" style="zoom:80%;" /></p>

<p>where of course those input word is mapped to pretrained embeddings.</p>

<p>Some problems that this method would encounter would be:</p>

<ul>
  <li>Results may <strong>violate constraints</strong> like “an action has at most one agent”?</li>
</ul>

<h3 id="evaluation-metric-for-srl">Evaluation Metric for SRL</h3>

<p>Since essentially we have <strong>Identification</strong> (should label or not) and <strong>Classification</strong> (which label), their performance can be evaluated separately:</p>

<ul>
  <li>each <strong>argument label</strong> must be assigned to the <strong>exact same parse constituent</strong> as in the ground truth</li>
  <li>then, accuracy and recall can be used, for combined to look at F-score.</li>
</ul>

<h2 id="selectional-restrictions">Selectional Restrictions</h2>

<p>We turn in this section to another way to represent facts about the <strong>relationship between predicates and arguments</strong>.</p>

<blockquote>
  <p>Frequently semantic role is indicated by a <strong>particular syntactic position</strong> . Examples include:</p>

  <ul>
    <li>Agent: subject</li>
    <li>Patient: direct object</li>
    <li>Instrument: object of “with” <code class="language-plaintext highlighter-rouge">PP</code></li>
    <li>Beneficiary: object of “for” <code class="language-plaintext highlighter-rouge">PP</code></li>
    <li>Source: object of “from” <code class="language-plaintext highlighter-rouge">PP</code></li>
    <li>Destination: object of “to” <code class="language-plaintext highlighter-rouge">PP</code></li>
  </ul>
</blockquote>

<p>However, obviously this is <strong>not always correct</strong>.
\(\text{The hammer hit the window.}\)
then by the above logic <em>hammer</em> would be the <code class="language-plaintext highlighter-rouge">AGENT</code>, but it is actually an <code class="language-plaintext highlighter-rouge">INSTRUMENT</code> since it is not active.</p>

<p>Therefore, this means we also need to add <strong>Selectional Restrictions</strong>.</p>

<blockquote>
  <p>A <strong>selectional restriction</strong> is a semantic type <strong>constraint that a verb imposes</strong> on the kind of concepts that are allowed to fill its argument roles.</p>
</blockquote>

<p>For instance, consider the following two sentences:
\(\text{I want to eat someplace nearby.}\\
\text{I want to eat Malaysian food.}\)
where both involve the predicate <em>eat</em>:</p>

<ul>
  <li>in the first case, we would want <em>someplace nearby</em> is an <em>adjunct</em> that gives the location of the eating event, <strong>instead of direct object</strong></li>
  <li>in the second case, we would want <em>Malaysian food</em> to be <em>direct object</em>, <strong>instead of adjunct</strong></li>
</ul>

<p>Therefore, we see that:</p>

<ul>
  <li>selectional restrictions are associated with senses, not entire lexemes</li>
  <li>yet another way is to instead specify <strong>preference</strong>, i.e. which one is preferred instead of which one is deferred, which we will see soon.</li>
</ul>

<p>This can be very <strong>useful</strong> in that it can rule out <strong>many ambiguities include</strong>:</p>

<ul>
  <li><strong>Syntactic Ambiguity</strong>: “John ate the spaghetti with <em>chopsticks</em>.” would be wrong as <code class="language-plaintext highlighter-rouge">PATIENTS</code> of <em>eat</em> must be <em>edible</em></li>
  <li><strong>Word Sense Disambiguation</strong>: “John <em>fired</em> the secretary.” vs “John <em>fired</em> the rifle.”</li>
</ul>

<p>But of course, it is difficult to acquire all of the selectional restrictions and taxonomic knowledge and applying them is also a problem.</p>

<hr />

<p>In reality, taxanomic abstraction hierarchies or ontologies (e.g. hypernym links in <strong>WordNet</strong>) can be used to <strong>determine if such constraints are met</strong>.</p>

<ul>
  <li>e.g. “<em>John</em>” is a “Human” which is a “Mammal” which is a “Vertebrate” which is an “Animate”</li>
</ul>

<hr />

<h3 id="selectional-preferences">Selectional Preferences</h3>

<p>Early word sense disambiguation systems used this idea to rule out senses that violated the selectional restrictions of their governing predicates. However, soon it became clear that these <strong>selectional restrictions were better represented as preferences</strong> rather than strict constraints.</p>

<p>Basically there will be measurements being a probabilistic measure of the <strong>strength of association between a predicate and a class</strong> dominating the argument to the predicate. More details see the book.</p>

<h1 id="advanced-semantics">Advanced Semantics</h1>

<p>This section aims to give a quick recap of <strong>why we are using Pre-trained Language Models</strong> today for so many tasks.</p>

<p>A quick recap. We used pre-trained <strong>Embeddings</strong> to do:</p>

<ul>
  <li>e.g. skipgram word embeddings, compact dense representation for words</li>
  <li>captures <strong>static semantic information</strong> of a word</li>
</ul>

<p>Yet in reality, tasks we need to consider usually includes doing things <strong>after you got some embeddings</strong>. This means having NN such as RNN/transformer for downstream tasks such as sentiment analysis. However, there we <strong>also face problems if we want to train a model from scratch</strong>:</p>

<ul>
  <li>need a collection of data + manual labeling</li>
  <li>often end up doing <strong>transfer learning</strong>: the training dataset is for move reviews, but for test we are working on Amazon product review</li>
</ul>

<h2 id="simple-transfer-learning">Simple Transfer Learning</h2>

<p>The idea with word embedding to have a <em>model learning some task agnostic data</em>, and then fine tune it to task specific data.</p>

<p>This has been commonly used for <strong>fine-tuning word embeddings</strong>, but you will see that we can also have a model <strong>learn task agonistic objective</strong> (e.g. a language model), and then <strong>fine tune it for downstream task</strong>:</p>

<p><img src="NLP/image-20220309162325596.png" alt="image-20220309162325596" style="zoom:50%;" /></p>

<p>Then the <strong>advantage of pre-trained model in this setting</strong> would be:</p>

<ul>
  <li>
    <p>no need of large corpus, only a few for fine-tuning</p>
  </li>
  <li>
    <p>Can plug learned embeddings into the first layer</p>
  </li>
</ul>

<p>Additionally, for learning the task agnostic model, often:</p>

<ul>
  <li>No labelled data required – just a large text corpus and do self-learning such as <strong>masked word prediction</strong> or <strong>next word prediction</strong></li>
</ul>

<h3 id="problems-with-pre-trained-word-embeddings">Problems with Pre-trained Word Embeddings</h3>

<blockquote>
  <p>Why can we <strong>not</strong> just use the pretrained embeddings? (but use pretrained language models)</p>
</blockquote>

<p>One assumption/property we had so far for a word embedding include:</p>

<ul>
  <li><strong>they are static</strong>, hence does not care about the context, meaning the following would have the same
    <ul>
      <li>“She broke the windshield with a <em>bat</em>.”</li>
      <li>“He was driving like a <em>bat</em> out of hell.”</li>
    </ul>
  </li>
  <li>word embedding before learnt from <strong>co-occurrence</strong>, which <strong>does not care about order!</strong></li>
</ul>

<p>However, for an <strong>entire network</strong> trained only tasks such as language model related ones:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Pretrained Word Embeddings</th>
      <th style="text-align: center">Pretrained Language Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220310005529542.png" alt="image-20220310005529542" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220310005553330.png" alt="image-20220310005553330" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>then you basically <strong>remove the $\hat{y}$ layer and add a linear layer at the end</strong> for transfer learning.</p>

<ul>
  <li>e.g. include recurrent or Transformer layers to incorporate context</li>
  <li>Better parameter initializations, for make it fine-tunable</li>
</ul>

<p>Now, we discuss <strong>language model as a effective pretrain task</strong> (e.g. next word prediction, masked word prediction, etc)</p>

<ul>
  <li>It’s self-supervised, so there’s no need for labelled data. e.g. Large text collections are readily available on the web</li>
  <li>it’s actually a difficult task, even for humans
    <ul>
      <li>A model would need to learn about syntax, semantics, and even some world knowledge in order to do well at this task</li>
      <li>Luckily with a large number of parameters, large corpora, and lots of GPUs (or TPUs), we can do a pretty reasonable job</li>
    </ul>
  </li>
</ul>

<h2 id="neural-language-model">Neural Language Model</h2>

<p><em>Recall</em>: N-gram Language Model</p>

<p>The N-gram language model (e.g. for next word prediction) has limitations:</p>

<ul>
  <li><strong>Sparsity</strong> (e.g. bigram matrix)</li>
  <li>Need to store ngrams (and counts)
    <ul>
      <li>Increases with n and corpus size</li>
    </ul>
  </li>
  <li>Don’t take into account word similarity</li>
</ul>

<hr />

<p>The solution to use <strong>Neural Language Model</strong>: predict the next word given the current context window</p>

<p><img src="NLP/image-20220309164017685.png" alt="image-20220309164017685" style="zoom: 80%;" /></p>

<p>where the architecture is simply:</p>

<ul>
  <li>input -&gt; embedding layer to start with</li>
  <li>one or more intermediate layers, e.g. <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">RNN</code>, etc</li>
  <li>output $\vec{h}$ and attach a SoftMax layer
    <ul>
      <li>for fine-tuning the entire model, consider removing the softmax and attach another linear layer on top of $\vec{h}$ for your downstream task</li>
    </ul>
  </li>
</ul>

<p>This model <strong>solved</strong> several issues from N-gram models such as:</p>

<ul>
  <li>we have no sparsity problem</li>
  <li>no need to store n-grams</li>
  <li>can capture word-similarity via embeddings</li>
</ul>

<p>However, several <strong>unsolved problems</strong> include:</p>

<ul>
  <li>Context window is limited (cannot just put in the entire document)
    <ul>
      <li>Still can’t capture long range dependencies</li>
    </ul>
  </li>
  <li>Increasing context window == increasing parameters</li>
</ul>

<h2 id="lms-and-transfer-learning">LMs and Transfer Learning</h2>

<p>Then the idea is that the LM task could be <strong>useful for any downstream task</strong>.</p>

<ul>
  <li>intuition: predict the next word positive fromthe sentence: the movie is great; positive”. Then we can use it as <strong>sentiment analysis</strong></li>
</ul>

<p>Therefore, you basically take the entire pre-trained model before the softmax layer as <strong>initialization</strong>, or as fine-tuning, or your downstream task. For instance:</p>

<ul>
  <li>use an pretrained LSTM LM on a large corpus</li>
  <li>Use weights of embeddings and LSTM layers as initialization for the target task</li>
</ul>

<p>Nowadays, besides LSTM LM, we have <strong>many pretrained architectures</strong> to choose from.</p>

<h3 id="architecture-choices">Architecture Choices</h3>

<p>In NLP we have <strong>a lot of pretained language models</strong>. For each, be careful to consider the following aspects:</p>

<ul>
  <li>Model Architecture</li>
  <li>Pre-training Objective</li>
  <li>Pre-training Data</li>
  <li>Adaptation to downstream tasks</li>
</ul>

<h3 id="architecture-examples">Architecture Examples</h3>

<blockquote>
  <p>In reality, since the ==pretained model has a lot of parameters==, it is common to <strong>freeze those weights</strong> for the following models and <strong>add a linear/NN layer</strong> on top for your downstream task.</p>
</blockquote>

<p><strong>GPT</strong></p>

<ul>
  <li>Transformer decoder with 12 layers
    <ul>
      <li>768 hidden units, 12 heads, 3072-dim feed forward layer
(117M params)</li>
    </ul>
  </li>
  <li>Pre-training objective: next word prediction</li>
  <li>Data:
    <ul>
      <li>BooksCorpus</li>
      <li>7K unpublished books covering a variety of genres (800M words)</li>
      <li>Allows model to condition on long-range dependencies</li>
    </ul>
  </li>
  <li>For GPT-2 and GPT-3
    <ul>
      <li>Larger models + more data = stronger LMs</li>
    </ul>
  </li>
</ul>

<p>then, the paper tested this model and applied to a variety of different downstream tasks.</p>

<hr />

<p><strong>BERT</strong></p>

<ul>
  <li>Pre-training objective: <strong>masked word prediction</strong> + <strong>next sentence prediction</strong>
    <ul>
      <li>LMs are unidirectional, but language understanding is bidirectional.</li>
      <li>next sentence prediction: whether if the next sentence follows from the previous sentence</li>
    </ul>
  </li>
  <li>12 layer transformer encoder, 768 hidden units, 12 attention heads = 110M parameters</li>
  <li>Data
    <ul>
      <li>BooksCorpus (800M words)</li>
      <li>English Wikipedia (2,500M words)
        <ul>
          <li>Text passages only</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Training time
    <ul>
      <li>4 days on 4x4 or 8x8 TPU v2 slices</li>
      <li>~$7K to train BERT Large</li>
    </ul>
  </li>
</ul>

<p>Input architecture looks like</p>

<p><img src="NLP/image-20220309170209027.png" alt="image-20220309170209027" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>segment embedding is to denote where we are putting the <strong>separator</strong>.</li>
</ul>

<p>Then, using this model for <strong>other downstream tasks</strong> include</p>

<p><img src="NLP/image-20220309170441560.png" alt="image-20220309170441560" style="zoom:50%;" /></p>

<p>Some importnat variants of BERT:</p>

<ul>
  <li>RoBERTa
    <ul>
      <li>Next sentence prediction not necessary as objective</li>
      <li>BERT + data + training steps</li>
    </ul>
  </li>
  <li>SpanBERT
    <ul>
      <li>Mask out spans instead
        <ul>
          <li>e.g. which span of the reading/resource relates to the question that was asked</li>
        </ul>
      </li>
      <li>Significant improvements on span selection tasks (such as QA)</li>
    </ul>
  </li>
</ul>

<p>It turns out that this also can be tuned for <strong>other languages</strong>: https://github.com/google-research/bert/blob/master/multilingual.md</p>

<h2 id="future-of-pretrained-models">Future of Pretrained Models</h2>

<p>Currently the trend is</p>

<ol>
  <li>to have an increasing size of model and data:</li>
  <li>consider <strong>zero/one/few-shot learning</strong> instead of transfer learning</li>
</ol>

<hr />

<p>Model size</p>

<ul>
  <li>BERT/GPT (2018) -&gt; ~100M params</li>
  <li>GPT-2 (2019) -&gt; 1.5B params</li>
  <li>T5 (2020) -&gt; 11B param</li>
  <li>GPT-3 (2020) -&gt; 175B params</li>
</ul>

<p>Dataset sizes</p>

<ul>
  <li>GPT (2018) -&gt; 800M words</li>
  <li>BERT (2018) -&gt; 3B words</li>
  <li>GPT-2 (2019) -&gt; 40B words</li>
  <li>GPT-3 (2020) -&gt; 500B words</li>
</ul>

<hr />

<p>Additionally, <strong>different from the transfer learning paradigm</strong>, recently we have been looking at <strong>zero/one/few-shot learning</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Zero-shot</th>
      <th style="text-align: center">One-shot</th>
      <th style="text-align: center">Few-hot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220310010926634.png" alt="image-20220310010926634" /></td>
      <td style="text-align: center"><img src="NLP/image-20220310010905618.png" alt="image-20220310010905618" /></td>
      <td style="text-align: center"><img src="NLP/image-20220310010915069.png" alt="image-20220310010915069" /></td>
    </tr>
  </tbody>
</table>

<p>one key difference here is that ==we do NOT update weights==, we are only providing exmaples = providing a <strong>context</strong>.</p>

<h1 id="machine-translation">Machine Translation</h1>

<blockquote>
  <p>The task here is to <strong>translate</strong> one natural language into another. Some common usages include:</p>

  <ul>
    <li><strong>pure translation</strong>: Google translate</li>
    <li><strong>computer-aided translation</strong>: used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This is commonly used as part of <strong>localization</strong>: the task of <em>adapting</em> content or a product to a <em>particular language community</em></li>
    <li><strong>incremental translation</strong>: translating speech on-the-fly before the entire sentence is complete</li>
    <li><strong>Image-centric translation</strong>: use OCR of the text on a phone camera image as input to an MT system to translate menus or street signs</li>
  </ul>
</blockquote>

<p>The standard algorithm for MT:</p>

<ul>
  <li>
    <p><strong>statistical</strong> methods (not used a lot today)</p>
  </li>
  <li>
    <p>the <strong>encoder-decoder</strong> network, also called the decoder sequence to sequence network, an architecture that can be implemented with RNNs or with Transformers. Why can we not just use an encoder/just an decoder?</p>

    <blockquote>
      <p>Machine Translation needs a map from a sequence of input words or tokens to a sequence of tags that are <strong>not merely direct mappings</strong> from individual words, which is exactly what such an architecture is doing.</p>
    </blockquote>
  </li>
</ul>

<p>An example of MT task would be:
\(\text{English}: \quad \text{He wrote a letter to a friend}\\
\text{Japanese}: \quad \underbrace{\text{tomodachi}}_{\text{friend}}\,\,\underbrace{\text{ni tegami-o}}_{\text{to letter}}\,\,\underbrace{\text{kaita}}_{\text{wrote}}\)
which evinces some <strong>key challenges</strong> that makes the task difficult:</p>

<ul>
  <li><strong>syntactical difference</strong> amongst languages: in English, <code class="language-plaintext highlighter-rouge">verb</code> is in the middle while in Japanese, <code class="language-plaintext highlighter-rouge">verb</code> is at the end.
    <ul>
      <li>e.g. word ordering difference: <code class="language-plaintext highlighter-rouge">SVO</code> (e.g. English), <code class="language-plaintext highlighter-rouge">SOV</code> (e.g. Hindi), and <code class="language-plaintext highlighter-rouge">VSO</code> (e.g. Arabic) languages.</li>
      <li>e.g. In some SVO languages (like English and Mandarin) <code class="language-plaintext highlighter-rouge">adjectives </code>tend to appear before <code class="language-plaintext highlighter-rouge">verbs</code>, while in others languages like Spanish and Modern Hebrew, <code class="language-plaintext highlighter-rouge">adjectives </code>appear after the <code class="language-plaintext highlighter-rouge">noun</code></li>
    </ul>
  </li>
  <li><strong>Pro-drop languages</strong>: regularly omit subjects that must be inferred.
    <ul>
      <li>e.g. Chinese sometimes drop subjects</li>
    </ul>
  </li>
  <li><strong>Morphological difference</strong></li>
  <li><strong>Lexical gaps</strong>: there might not exist a one to one mapping for a word in foreign languages</li>
</ul>

<blockquote>
  <p><strong>Encoder-decoder</strong> networks are very successful at handling these sorts of complicated cases of sequence mappings.</p>
</blockquote>

<p>Indeed, the encoder-decoder algorithm is not just for MT; it’s the state of the art for many other tasks where complex mappings between two sequences are involved:</p>

<ul>
  <li>summarization (where we map from a long text to its summary, like a title or an abstract)</li>
  <li>dialogue (where we map from what the user said to what our dialogue system should respond)</li>
  <li>semantic parsing (where we map from a string of words to a semantic representation like logic or SQL)</li>
  <li>and many others.</li>
</ul>

<p>However, the current translation quality is <strong>not perfect</strong>:</p>

<ul>
  <li>Existing MT systems can generate rough translations that at least convey the gist of a document</li>
  <li>High quality translations possible when specialized to narrow domains, e.g. weather forecasts.</li>
</ul>

<h2 id="language-divergence">Language Divergence</h2>

<p>This section discusses a bit more on the differences between languages that makes the task of MT difficult</p>

<blockquote>
  <p>Languages differ in many ways, and an understanding of what causes translation such divergences will help us <strong>build better MT</strong> models. The study of these systematic cross-linguistic similarities and differences is called <strong>linguistic typology</strong></p>
</blockquote>

<h3 id="word-order-typology">Word Order Typology</h3>

<p>As we hinted it in our example above comparing English and Japanese, languages differ in the basic word order:</p>

<ul>
  <li>Subject-Verb-Object order</li>
  <li>In some SVO languages (like English and Mandarin) <code class="language-plaintext highlighter-rouge">adjectives </code>tend to appear before <code class="language-plaintext highlighter-rouge">verbs</code>, while in others languages like Spanish and Modern Hebrew, <code class="language-plaintext highlighter-rouge">adjectives </code>appear after the <code class="language-plaintext highlighter-rouge">noun</code></li>
</ul>

<p>Visual example of word order difference hence complex mapping:</p>

<p><img src="NLP/image-20220321190648445.png" alt="image-20220321190648445" style="zoom:67%;" /></p>

<h3 id="lexical-divergences">Lexical Divergences</h3>

<p>Here we need to deal with problems such as:</p>

<ul>
  <li>appropriate word/translation can vary depending on the <strong>context</strong>.
    <ul>
      <li>e.g. German uses two distinct words for what in English would be called a <code class="language-plaintext highlighter-rouge">wall</code>: <code class="language-plaintext highlighter-rouge">Wand </code>for walls inside a building, and <code class="language-plaintext highlighter-rouge">Mauer </code>for walls outside a building.</li>
    </ul>
  </li>
  <li>Sometimes one language places more grammatical <strong>constraints on word choice</strong> than another.</li>
  <li><strong>lexical gap</strong>: no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language.</li>
</ul>

<h3 id="morphological-typology">Morphological Typology</h3>

<blockquote>
  <p>Recall that a <em>morpheme</em> is the smallest unit of meaning that a word can be divided into.</p>
</blockquote>

<p>Then, in many languages we have:</p>

<ul>
  <li>difference in the <strong>number of morphemes per word</strong>
    <ul>
      <li><strong>isolating</strong> languages like Vietnamese and Cantonese, one morpheme per word</li>
      <li><strong>polysynthetic</strong> languages like Siberian Yupik (“<code class="language-plaintext highlighter-rouge">Eskimo</code>”), in which a single word may have very many morphemes, <em>corresponding to a whole sentence</em> in English</li>
    </ul>
  </li>
  <li>difference in the <strong>degree a morpheme is separable</strong>
    <ul>
      <li><strong>agglutinative</strong> languages like Turkish, in which morphemes have relatively clean boundaries</li>
      <li><strong>fusion</strong> languages like Russian, in which a single affix may conflate multiple morphemes</li>
    </ul>
  </li>
</ul>

<p>This means that translating between languages with rich morphology requires dealing with structure below the word level, .e.g. use <strong>subword models</strong> such as BPE.</p>

<h2 id="rule-based-mt-model">Rule-Based MT Model</h2>

<p><strong>Rules-based machine translation</strong> (<strong>RBMT</strong>) is a machine translation approach based on hardcoded linguistic rules. The rules used here would include:</p>

<ul>
  <li>lexical transfer</li>
  <li>lexical reordering</li>
  <li>etc.</li>
</ul>

<h3 id="direct-transfer">Direct Transfer</h3>

<p>The task is to use rules to translate between, e.g. English to Spanish:</p>

<ol>
  <li>
    <p>Use <strong>morphological analysis</strong></p>

    <p><img src="NLP/image-20220321194326179.png" alt="image-20220321194326179" style="zoom: 67%;" /></p>
  </li>
  <li>
    <p>Use <strong>lexical transfer rules</strong> to find syntactic ==one to many mapping== of the translation of each word:</p>

    <p><img src="NLP/image-20220321194444415.png" alt="image-20220321194444415" style="zoom: 67%;" /></p>

    <p>notice that here we did two things: do the translation per word (e.g. using a dictionary) + translated into basic grammar structure in Spanish</p>
  </li>
  <li>
    <p><strong>Lexical Reordering</strong>: fixing some more detailed word orders</p>

    <p><img src="NLP/image-20220321194642643.png" alt="image-20220321194642643" style="zoom:67%;" /></p>
  </li>
  <li>
    <p><strong>Morphological generation</strong>: generate the morphology back from the first step</p>

    <p><img src="NLP/image-20220321194835167.png" alt="image-20220321194835167" style="zoom:67%;" /></p>
  </li>
</ol>

<p>But of course even this rule-based approach has shortcomings in quality:</p>

<ul>
  <li>
    <p>lexical reordering does not adequately handle <strong>more dramatic reordering</strong> such as that required to translate from an SVO to an SOV language. This means we need <strong>syntactic transfer rules</strong> that map parse tree for one language into one for another.</p>

    <p>For example:</p>

    <p><img src="NLP/image-20220321195042900.png" alt="image-20220321195042900" style="zoom:60%;" /></p>
  </li>
  <li>
    <p>some transfer requires <strong>semantic information</strong>. For example, in Chinese <code class="language-plaintext highlighter-rouge">PP</code> expressing <em>a goal</em> semantically should occur <em>before</em> <code class="language-plaintext highlighter-rouge">verb</code>, but in English, it occurs <em>after</em> the <code class="language-plaintext highlighter-rouge">verb</code>.</p>

    <p>Hence we need rules such as</p>

    <p><img src="NLP/image-20220321195242729.png" alt="image-20220321195242729" style="zoom: 60%;" /></p>
  </li>
</ul>

<h2 id="statistical-mt">Statistical MT</h2>

<p>Of course rule-based approach have big problems</p>

<ul>
  <li>
    <p>difficult to come up with <strong>good rules</strong> between two languages</p>
  </li>
  <li>
    <p>it <strong>does not scale</strong> as it requires hand-written rules.</p>
  </li>
</ul>

<p>Instead of rule based direct transfer, consider a statistical model which at least scales</p>

<blockquote>
  <p>SMT acquires knowledge needed for translation from a <strong>parallel corpus</strong> or bitext that contains the same set of documents in two languages.</p>
</blockquote>

<h3 id="noisy-channel-model">Noisy Channel Model</h3>

<p>The idea is to consider, for example translating French to English:</p>

<blockquote>
  <p>Source sentence (e.g. French) was generated from some ==noisy transformation== of the target sentence (e.g. English), as we have done in Spelling Correction.</p>
</blockquote>

<p>Therefore, we consider finding the translation $\hat{E}$:
\(\hat{E} = \arg\max P(E |F)\)
for $F=f_1,f_2…,f_m$ being a sentence in French composing $m$ words, and $E = e_1,e_2,…,e_n$ being a sentence in English composed of $n$ words. Then using Bayesian rules:</p>

<p>\(\hat{E} = \arg\max_E P(E|F)  = \arg\max_E \underbrace{P(F|E)}_{\text{likelihood of $E$}}\quad\underbrace{P(E)}_{\text{prior of $E$ occuring}}\)
where $P(F|E)$ would then be computed by a <strong>translation model</strong> and $P(E)$ by a <strong>language model</strong>.</p>

<ul>
  <li>
    <p>e.g. $P(E)$ could come form a n-gram model. or a PCFG which captures syntactic structure as well, etc.</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>to compute $P(F</td>
          <td>E)$, we would ideally want to do:</td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>find phrase alignments from a given $E$ to $F$ and translate each phrase</li>
      <li>but this is hard to do, so in reality we consider <strong>word alignment $A=a_1,…,a_k$</strong> then translation</li>
    </ul>

\[P(F|E) = \sum_A P(F|E,A)P(A|E)\]

    <p>more details on how this works is covered in the next section.</p>
  </li>
</ul>

<h3 id="word-alignment-for-mt">Word Alignment for MT</h3>

<table>
  <tbody>
    <tr>
      <td>Recall that our task is to compute $P(F</td>
      <td>E)$, for $F$ being a random variable and $E$ of length $n$.</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>To simplify the problem, typically assume the following:</p>

  <p><img src="NLP/image-20220321202333366.png" alt="image-20220321202333366" style="zoom: 50%;" /></p>

  <p>where notice that:</p>

  <ul>
    <li>
      <p>given a $E$, each word in $E$ aligns to <strong>one or more</strong> words in $F$</p>
    </li>
    <li>
      <p>each word in $F$ aligns to 1 word in $E$ (so it is a vector, as shown below)</p>
    </li>
  </ul>

  <p>Then, an alignment in basically becomes a size $9$ vector which looks like:
\([1,2,3,3,3,0,4,6,5]^T\)
which is for each word in $F$, the <strong>index of the word in $E$ which generated it</strong>. (then you can apply word-word level translation)</p>
</blockquote>

<p>In general, such an alignment can be learnt from</p>

<ul>
  <li>
    <p>supervised word alignments, but human-aligned bitexts are rare and expensive to construct.</p>
  </li>
  <li>
    <p>so typically obtained using an <strong>unsupervised EM-based approach</strong> to compute a word alignment from unannotated parallel corpus.</p>
  </li>
</ul>

<h3 id="ibm-model-1">IBM Model 1</h3>

<table>
  <tbody>
    <tr>
      <td>Now, ==assume that $P(F</td>
      <td>E,A)$== is computable. The IBM model for SMT can generated a single $F$ from $E=e_1,…,e_n$ of length $n$ by:</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>choose length $k$, so that we would have $F=f_1,…,f_k$</li>
  <li>choose an alignment $A=a_1,…,a_k$ which represents which English <strong>word</strong> it should comes from</li>
  <li>For each position of word in $F$, generated a word $f_j$ <strong>from</strong> the aligned English version $e_{a_j}$</li>
</ol>

<table>
  <tbody>
    <tr>
      <td>Next, we can define how to compute $P(F</td>
      <td>E)$ of $E$ having length $n$ by:</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>given some length distribution $P(K=k</td>
          <td>E)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>assuming all alignments are qually likely, then there are $(n+1)^k$ possible alignments. Hence:
\(P(A=a_1,...,a_k|E=e_1,...,e_n) = \frac{P(K=k|E)}{(n+1)^k},\quad \forall a_i\)
i.e. same probability if given the same length. e.g. $P(A=0,1,2|E)=P(A=1,0,2|E)$</p>
  </li>
  <li>
    <p>Given some translation probability per word from $e_y \to f_x$, let it be $t(f_x|e_y)$ we then have:
\(P(F|E,A) = \prod_{j=1}^kt(f_j|e_{a_j})\)
where the alignment would be given by previous step</p>
  </li>
  <li>Finally, we sum over all possible alignments:
\(P(F|E) = \sum_A P(F|E,A)P(A|E) = \sum_A \frac{P(k|E)}{(n+1)^k} \prod_{j=1}^kt(f_j|e_{a_j})\)
where the alignments $A$ would <strong>vary both in</strong> length $k$ and in the elements/indices within an alignment of same length.</li>
</ol>

<p>Typically use an unsupervised EM-based approach to compute a word alignment from unannotated parallel corpus, e.g. you could have $A=0,1,2$, $A=1,0,2$, $A=1,0,0,2$, etc.</p>

<ul>
  <li>notice that this is only a <strong>forward algorithm</strong>, so if we need to decode, this would be not very computational efficient.</li>
</ul>

<hr />

<p>Lastly, the decoding produce for finding the <strong>best alignment</strong> can be done by:
\(\hat{A} = \arg\max_A \frac{P(k|E)}{(n+1)^k} \prod_{j=1}^kt(f_j|e_{a_j}) = \arg\max_A \prod_{j=1}^kt(f_j|e_{a_j})\)
then, how do we maximize a product of terms? Since each term is independent, we can maximize it by <strong>maximizing each term independently</strong>. Hence:
\(\hat{a}_j = \arg\max_{i} t(f_j,e_i),\quad \forall j\)
which tells you which English word $f_j$ should align to.</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>of course you can compute this once you know all the probabilities $P(k</td>
          <td>E), t(f_y</td>
          <td>e_x)$.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="hmm-based-word-alignment">HMM-Based Word Alignment</h3>

<p>Obviously, one problem with IBM Model 1 is that it assumes all alignments are <strong>equally likely</strong> and ==does not take into account locality==, e.g. <em>next to each other words are likely to be next to each other in another language as well.</em></p>

<blockquote>
  <p>To solve this issue, <strong>HMM models</strong> can be used which models the <strong>jump width</strong> as hidden state, i.e. :</p>

  <ol>
    <li>translate current word</li>
    <li>decide which next word to jump to for translation</li>
    <li>repeat from step 1</li>
  </ol>
</blockquote>

<p>First, an example would be</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Iteration 1</th>
      <th style="text-align: center">Iteration 3</th>
      <th style="text-align: center">Iteration 4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220321224922541.png" alt="image-20220321224922541" /></td>
      <td style="text-align: center"><img src="NLP/image-20220321225048967.png" alt="image-20220321225048967" /></td>
      <td style="text-align: center"><img src="NLP/image-20220321225055378.png" alt="image-20220321225055378" /></td>
    </tr>
  </tbody>
</table>

<p>notice that:</p>

<ul>
  <li>
    <p>the jump could jump to the current word itself, as there could be one-to-many mapping</p>
  </li>
  <li>
    <p>the jump could jump both forward and backward discontinuously. E.g.</p>

    <p><img src="NLP/image-20220321225219480.png" alt="image-20220321225219480" style="zoom: 25%;" /></p>
  </li>
</ul>

<p>Therefore, now we can define what this model really is</p>

<blockquote>
  <ul>
    <li><strong>Hidden states</strong> are current English word $e_i$ being translated</li>
    <li><strong>State transition</strong> would be modelling the <strong>jumps</strong> for the next word, which is $a_{ij}=P(s_i \to s_j)$</li>
    <li><strong>Observations</strong> are the translated French word $f_j$</li>
    <li>
      <table>
        <tbody>
          <tr>
            <td><strong>Emission probability</strong> is therefore $b_j(f_i)=P(f_i</td>
            <td>e_j)$, which is basically probability of translation from $e_j \to f_i$</td>
          </tr>
        </tbody>
      </table>
    </li>
  </ul>
</blockquote>

<p>Then this means that:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>==Observation likelihood $P(F</td>
          <td>E)$== can be computed by the <strong>forward algorithm</strong> with HMM</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>==Decoding $\hat{E}=\arg\max_E P(E</td>
          <td>F)$== can be computed by <strong>Viterbi algorithm</strong> with HMM</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="training-word-alignment-models">Training Word Alignment Models</h3>

<p>Both the IBM model 1 and HMM model require the following parameters</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(f_i</td>
          <td>e_j)$ probability of individual word translation (shown in the example below)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(K=k</td>
          <td>E)$, length of target translation sentence</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>which can be obtained/trained on a <strong>parallel corpus</strong> to set the required parameters, including the specific ones such as:</p>

<ul>
  <li>for HMM Model, we also need $P(e_i)$ and $P(e_i \to e_j)$ being transition probabilities</li>
</ul>

<blockquote>
  <p><strong>In general</strong></p>

  <ul>
    <li>if we have a labelled/<strong>supervised</strong> (hand-aligned) training data, parameters can be estimated directly using frequency counts; e.g. <strong>sentence alignment</strong>. Which sentence in a corpus corresponds to which sentence is another corpus.</li>
    <li>most often we have an <strong>unsupervised</strong> piece of parallel corpus. Then we need to estimate the probabilities using EM type algorithm.</li>
  </ul>
</blockquote>

<p>A sketch of the algorithm looks like</p>

<p><img src="NLP/image-20220321230852813.png" alt="image-20220321230852813" style="zoom: 50%;" /></p>

<p>For example, given two data in the training corpus:
\(\text{green house} \iff \text{casa verde}\\
\text{the house} \iff \text{la casa}\)</p>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Our aim is to be able to compute $P(s_i</td>
        <td>e_j)$ for $s_i$ being a Spanish word, and perhaps vice versa.</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>notice that you should expect $\text{house} \iff \text{casa}$ to have a higher probability as this pair occurs more often</li>
    <li>this is exactly what the EM algorithm tries to do</li>
  </ul>
</blockquote>

<ol>
  <li>
    <p>Step one: initialization. Here we can assume a uniform distribution such that each row/column sums to one</p>

    <p><img src="NLP/image-20220321231613695.png" alt="image-20220321231613695" style="zoom: 33%;" /></p>

    <table>
      <tbody>
        <tr>
          <td>notice that each cell would represent $P(s_i</td>
          <td>e_j)$ and vice versa.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>Expectation Step</strong>: we impute the missing data, which is to consider all possible alignments/individual translations. Here we have four possible cases:
\(\text{green} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{verde};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\\
\text{green} \iff \text{verde}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\)
and that
\(\text{the} \iff \text{la}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\\
\text{the} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{la};\quad p=\frac{1/9}{2/9}=\frac{1}{2}\)</p>
  </li>
  <li>
    <p><strong>Maximization Step</strong>: then we fill in the table using the previous probabilities</p>

    <p><img src="NLP/image-20220321232324142.png" alt="image-20220321232324142" style="zoom:33%;" /></p>

    <p>And <strong>normalizing to sum to one</strong>:</p>

    <p><img src="NLP/image-20220321232358667.png" alt="image-20220321232358667" style="zoom: 33%;" /></p>
  </li>
  <li>
    <p>Repeat step 2-3 until convergence. Here just to be clear we show one more iteration of step 2. Possible alignments and their probability:</p>

    <p><img src="NLP/image-20220321233011833.png" alt="image-20220321233011833" style="zoom: 67%;" /></p>

    <p>Hence
\(\text{green} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{verde};\quad p=\frac{1/8}{3/8}=\frac{1}{3}\\
\text{green} \iff \text{verde}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/4}{3/8}=\frac{2}{3}\)
and
\(\text{the} \iff \text{la}\quad \text{AND} \quad  \text{house} \iff \text{casa};\quad p=\frac{1/4}{3/8}=\frac{2}{3}\\
\text{the} \iff \text{casa}\quad \text{AND} \quad  \text{house} \iff \text{la};\quad p=\frac{1/8}{3/8}=\frac{1}{3}\)</p>
  </li>
</ol>

<p>Note that the above works by the assumption that many words will be <strong>repeated</strong>.</p>

<h2 id="neural-machine-translation">Neural Machine Translation</h2>

<p>As mentioned before, translation often involves a complex map between two sequences, hence usually we do</p>

<ul>
  <li>
    <p><strong>encoder-decoder</strong> model (e.g. LSTM blocks)</p>

    <p><img src="NLP/image-20220321234653968.png" alt="image-20220321234653968" style="zoom: 33%;" /></p>

    <p>e.g. transformer based encoder-decoder</p>
  </li>
  <li>
    <p>integrate an LSTM with language model using “deep fusion.”</p>

    <p><img src="NLP/image-20220321234901398.png" alt="image-20220321234901398" style="zoom:67%;" /></p>

    <p>which basically is for decoder to <strong>predict the next word</strong> from a concatenation of the <strong>hidden states of both</strong> the translation and language LSTM models.</p>
  </li>
</ul>

<h2 id="evaluating-mt">Evaluating MT</h2>

<p>Translations are evaluated along <strong>two dimensions</strong>:</p>

<ol>
  <li><strong>adequacy</strong>: how well the translation captures the exact ==meaning== of the source sentence. Sometimes called <strong>faithfulness</strong> or fidelity.</li>
  <li><strong>fluency</strong>: how ==fluent== the translation is in the target language (is it grammatical, clear, readable, natural).</li>
</ol>

<p>The most accurate metric is to have human to score the translations based on the above criterion, but that is inefficient and expensive. This in reality is done often in the following manner:</p>

<ol>
  <li>Collect one or more human <strong>reference translations</strong> of the source, i.e. gold standard</li>
  <li>Compare MT output to these reference translations.</li>
  <li>Score <strong>result based on similarity</strong> to the reference translations.</li>
</ol>

<p>Some automatic scoring system implemented today include BLEU, NIST, TER, etc.</p>

<h3 id="bleu">BLEU</h3>

<blockquote>
  <p><strong>BLEU (Bilingual Evaluation Understudy)</strong>: scores based on the following criteria:</p>

  <ul>
    <li>What percentage of machine n-grams can be found in the reference translation?</li>
    <li>Brevity Penalty: if translated sentence is too short, e.g. <code class="language-plaintext highlighter-rouge">the.</code>, it matches/precision $1.0$ but it is cheating!</li>
  </ul>
</blockquote>

<p>To answer the first question, an example would be:</p>

<ul>
  <li>
    <p>finding shared unigrams:</p>

    <p><img src="NLP/image-20220321235856265.png" alt="image-20220321235856265" style="zoom: 25%;" /></p>

    <p>where basically we count a match if the unigram appears in <strong>at least one of the reference translation</strong></p>
  </li>
  <li>
    <p>finding shared bigrams</p>

    <p><img src="NLP/image-20220322000121945.png" alt="image-20220322000121945" style="zoom:25%;" /></p>
  </li>
  <li>
    <p>until some fixed size $N$, typically $4$.</p>
  </li>
</ul>

<p>Finally, scoring the <strong>first criteria</strong> involves finding a <strong>geometric mean</strong>:
\(\text{Precision}_{gm} = \sqrt[N]{\prod_{n=1}^NPr_n}\)
However, this alone will <strong>not work</strong>, because you can have a <strong>shorter sentence</strong> which would give a higher score. Therefore, we also need a <strong>brevity penalty</strong>/the second criteria:</p>

<ul>
  <li>
    <p>ideally, we might compute $\text{Recall}$​. which could solve the problem. But this is problematic since there are <em>multiple alternative gold-standard references</em>.</p>
  </li>
  <li>
    <p>therefore, we cook up with the following metric:</p>

    <p>Define effective reference length, $r$, as the <strong>length of the reference sentence</strong> with the <strong>largest number of</strong>
<strong>n-gram matches</strong>. Then, if $c$ is the candidate sentence length:
\(BP = \begin{cases}
1,&amp; \text{if }c &gt; r\\
e^{1-(r/c)},&amp; \text{if }c \le r
\end{cases}\)</p>
  </li>
</ul>

<p>So that the final score is:
\(BLUE = \text{Precision}_{gm} \times BP\)</p>

<h2 id="challenges-and-futures-in-mt">Challenges and Futures in MT</h2>

<p>Certain challenges in MT include:</p>

<ul>
  <li>OOV word in test set. We need smoothing or other techniques, such as subword.</li>
  <li>domain mismatch: e.g. corpus in movie but test in Amazon product review</li>
  <li>translating long context is hard, even with attention it is not completely solved</li>
  <li>low-resource language pairs</li>
  <li>NMT can pick up biases (e.g. gender bias)</li>
</ul>

<p>Some future directions include:</p>

<ul>
  <li>unsupervised MT attempts to learn language laignment form monolinguial data</li>
  <li>multilingual NMT, learn shared representation across all languages (which can solve low resource problem)</li>
  <li>Neural LSTM methods are currently the state-of-the-art.</li>
</ul>

<h1 id="sentiment-analysis-and-classification">Sentiment Analysis and Classification</h1>

<blockquote>
  <p>In this chapter we introduce the algorithms such as Naïve text Bayes algorithm and apply it to <strong>text categorization</strong>, the task of assigning a label or category to an entire text or document. In particular, we focus on categorizing text based on its sentiments, i.e. <strong>sentiment analysis</strong>.</p>

  <ul>
    <li>e.g. positive or negative orientation that a writer expresses toward some object.</li>
    <li>simplest version of sentiment analysis is a binary classification task</li>
  </ul>

  <p>Other commonly used names for sentiment analysis include: Opinion extraction; Opinion mining; Sentiment mining; Subjectivity analysis.</p>
</blockquote>

<p>So now are are dealing with <strong>classification task</strong>, which is to be set in contrast to <code class="language-plaintext highlighter-rouge">Seq2Seq</code> task. Here we only need to output a <strong>single</strong> label/or a small fixed set for the <strong>entire document</strong>, e.g. a single written product review.</p>

<p>Other text categorization other than sentiment include:</p>

<ul>
  <li><strong>Spam detection</strong>: another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam</li>
  <li><strong>Authorship attribution</strong>: whether if a given text is written by the person</li>
  <li><strong>Subject category classification</strong>: which library category does a piece of text belong to? Science? Humanities? etc.</li>
</ul>

<blockquote>
  <p>The goal of classification is to take a single observation, i.e. a test sample, <strong>extract</strong> some useful features, and thereby <strong>classify</strong> the observation into one of a set of discrete classes.</p>
</blockquote>

<p>And again recall that there are two broad class of classification algorithms:</p>

<ul>
  <li><strong>Generative</strong> classifiers like Naïve Bayes</li>
  <li><strong>Discriminative</strong> classifiers like logistic regression</li>
</ul>

<hr />

<p><em>Real Life Example</em>: Twitter posts sentiment analysis</p>

<p>For posts from Twitter:</p>

<p><img src="NLP/image-20220323202111044.png" alt="image-20220323202111044" /></p>

<p>where we see “Happiness” surges around Election day and Thanksgiving. Other commonly used cases include:</p>

<ul>
  <li>
    <p>Movie: is this review positive or negative?</p>
  </li>
  <li>Products: what do people think about the new iPhone?</li>
  <li>Politics: what do people think about this candidate or issue?</li>
  <li>Used as an input/feature to other task, such as predicting market trends from sentiment</li>
  <li>etc.</li>
</ul>

<h2 id="scherer-typology-of-affective-states">Scherer Typology of Affective States</h2>

<p>Some labels you can have for affective states human have:</p>

<ul>
  <li><strong>Emotion</strong>: brief organically synchronized … evaluation of a major event
    <ul>
      <li>angry, sad, joyful, fearful, ashamed, proud, elated</li>
    </ul>
  </li>
  <li><strong>Mood</strong>: diffuse non-caused low-intensity long-duration change in subjective feeling
    <ul>
      <li>cheerful, gloomy, irritable, listless, depressed, buoyant</li>
      <li>more for <em>longer term predictions</em></li>
    </ul>
  </li>
  <li><strong>Interpersonal stances</strong>: affective stance toward another person in a specific interaction
    <ul>
      <li>friendly, flirtatious, distant, cold, warm, supportive, contemptuous</li>
      <li>used often for Social Media analysis, how users interact online between each other</li>
    </ul>
  </li>
  <li>==Attitudes==: enduring, affectively colored beliefs, dispositions towards objects or persons
    <ul>
      <li>liking, loving, hating, valuing, desiring</li>
      <li>used a lot for product reviews and ==sentiment analysis==</li>
    </ul>
  </li>
  <li><strong>Personality Traits</strong>
    <ul>
      <li>nervous, anxious, reckless, morose, hostile, jealous</li>
      <li>not very commonly used, but related to those personality test you take</li>
    </ul>
  </li>
</ul>

<p>Of course, which ones to use depends on the particular application.</p>

<h2 id="sentiment-analysis-and-attitudes">Sentiment Analysis and Attitudes</h2>

<p>Often sentiment analysis is framed as the <strong>detection of attitudes</strong>. In particular, we want to find out</p>

<ul>
  <li>
    <p><strong>Holder</strong> (source) of attitude</p>
  </li>
  <li>
    <p><strong>Target</strong> (aspect) of attitude</p>
  </li>
  <li><strong>Type</strong> of attitude
    <ul>
      <li>From a set of types: Like, love, hate, value, desire, etc.</li>
      <li>Or (more commonly) simple weighted <strong>polarity</strong>: positive, negative, neutral, together with <em>strength</em></li>
    </ul>
  </li>
  <li><strong>Text</strong> containing this attitude: which sentence/document</li>
</ul>

<p>In reality:</p>

<ul>
  <li><strong>Simplest task:</strong> Is the attitude of this text positive or negative? This will be our focus in this chapter as a ==classification task==.</li>
  <li><strong>More complex:</strong> Rank the attitude of this text from 1 to 5, i.e. include <em>strength</em></li>
  <li><strong>Advanced:</strong> all the 4 subtasks.</li>
</ul>

<h2 id="sentiment-classification">Sentiment Classification</h2>

<p>Here we take on the <strong>simplest task</strong>: Is an IMDB movie review positive or negative?</p>

<p>For instance, we could have</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Positive</th>
      <th style="text-align: center">Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">When Han solo goes light speed , the stars change to bright lines, going towards the viewer in lines that converge at an invisible point .<br />Cool.</td>
      <td style="text-align: center">“snake eyes” is the most aggravating kind of movie : the kind that shows so much potential then becomes unbelievably disappointing</td>
    </tr>
  </tbody>
</table>

<p>In general, the steps we need to go through before and at classification time include:</p>

<ol>
  <li>
    <p><strong>Tokenization</strong>:</p>

    <ul>
      <li>Deal with HTML and XML markup</li>
      <li>Deal with Emoticons</li>
      <li>Deal with Twitter mark-up (names, hash tags), e.g. <code class="language-plaintext highlighter-rouge">@xxx</code></li>
    </ul>
  </li>
  <li>
    <p><strong>Feature Extraction</strong></p>

    <ul>
      <li>
        <p>Some how handle <em>negation</em>, which could flip the meaning of a sentence:
\(\text{I didn’t like this movie.}\quad v.s.\quad \text{I did like this movie.}\)
one idea is to convert the former to:
\(\text{I didn’t NOT\_like NOT\_this NOT\_movie}.\)
basically adding NOT to every word between negative and the following punctuation.</p>
      </li>
      <li>
        <p>Then perhaps we don’t need all words? Only need the adjectives? (It turns out using All words work better)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Classification</strong> using different classifiers:
\(\text{class} = \arg\max_{c_j \in C} P(C|W=w_1,...,w_n)\)</p>

    <ul>
      <li>Naive Bayes</li>
      <li>MaxEntropy</li>
      <li>SVM</li>
    </ul>
  </li>
</ol>

<p>We will first introduce some <strong>baseline algorithms</strong> that works at least.</p>

<h3 id="baseline-multinomial-naïve-bayes">Baseline: Multinomial Naïve Bayes</h3>

<blockquote>
  <p><strong>Multinomial Naïve Bayes</strong> is essentially a Naive Bayes classifier that is able to output a class among many classes (more than two). This is essentially done by:</p>

  <ul>
    <li>(==assumption==: Naive Bayes) probabilities $P(w_i|c)$ are independent given the class $c$. Therefore it allows
 \(P(w_1,...,w_n|c_j) = \prod_i P(w_i|c_j)\)
hence
\(c = \arg\max_{c_j \in C}  P(c_j) P(w_1,...,w_n |c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)\)</li>
  </ul>
</blockquote>

<p>So for <strong>Naive Bayes</strong>, we consider:
\(c = \arg\max_{c_j \in C} P(W|c_j)P(c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)\)
meaning that we ==assume each word to be independently contributing to $c_j$==. Note that</p>

<ul>
  <li>
    <p>this is a generative model because this equation can be understood as:</p>

    <ol>
      <li>generate a class $P(c_j)$</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>generate the words from the class $P(w_i</td>
              <td>c_j)$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ol>
  </li>
  <li>
    <p>in many cases Naive Bayes is a linear classifier</p>

    <p><img src="NLP/image-20220323222229514.png" alt="image-20220323222229514" style="zoom:80%;" /></p>

    <table>
      <tbody>
        <tr>
          <td>where you see the decision boundary for $P(c_1</td>
          <td>x)=P(c_2</td>
          <td>x)$ is a line.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>To avoid numeric underflow, we would convert the estimate to log space:
\(c_{NB} = \arg\max_{c_j \in C} \,\,\log P(c_j) + \sum_i \log P(w_i|c_j)\)
But how do we ==learn== $P(c_j)$ and $P(w_i|c_j)$?</p>

<ul>
  <li>
    <p>learning the prior $P(c_j)$ is easy. Given $N_{doc}$ documents (e.g. posts), let $N_{c_j}$ be the number of document with sentiment class $c_j$. Then:
\(\hat{P}(c_j) = \frac{N_{c_j}}{N_{doc}}\)</p>
  </li>
  <li>
    <p>learning the likelihood means counting how often is $w_i$ associated with class $c_j$. Hence we consider:
\(\hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)}{\sum_{w\in V}\text{Count}(w_i, c_j)}\)
where:</p>

    <ul>
      <li>$\text{Count}(w_i, c_j)$ represent the number of times the word $w_i$ appears among all words in all documents of class $c_j$.</li>
      <li>In other words, we first concatenate all documents with class $c_j$ together, then count the number of occurrence of $w_i$.</li>
      <li>since eventually we need this for all words, $V$ presents the <strong>entire vocabulary</strong> instead of vocabulary in class $c_j$.</li>
    </ul>
  </li>
</ul>

<p>Since we are using count, consider <strong>smoothing</strong> as well for unmet $w_i,c_j$ pair:
\(\hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)+1}{\sum_{w\in V}\text{Count}(w_i, c_j)+1}=\frac{\text{Count}(w_i, c_j)+1}{\text{Count(total words in $c_j$)}+|V|}\)
which is ==critically needed== as the likelihood term is a multiplication.</p>

<ul>
  <li>for <strong>OOV words</strong>, we can use the technique introduced before by inducing <code class="language-plaintext highlighter-rouge">OOV</code> vocab in the training set. However, for Naive Bayes, it is more common to ==ignore those words completely== (as if you didn’t see it)</li>
  <li>other processing step include <strong>removing stop words</strong> such as <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">the</code> for both train and test sets. Though the performance gain from this is not significant.</li>
</ul>

<p>Therefore, the algorithm for learning is</p>

<p><img src="NLP/image-20220323211934552.png" alt="image-20220323211934552" style="zoom:67%;" /></p>

<p>Then for testing, we simply perform:
\(\hat{c} = \arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)\)
Hence the algorithm for test is</p>

<p><img src="NLP/image-20220323212120485.png" alt="image-20220323212120485" style="zoom:80%;" /></p>

<hr />

<p><em>Example</em></p>

<p>We’ll use a sentiment analysis domain with the <strong>two classes positive (+) and negative (-).</strong> The train and test set is provided below</p>

<p><img src="NLP/image-20220323223034920.png" alt="image-20220323223034920" style="zoom:67%;" /></p>

<table>
  <tbody>
    <tr>
      <td>What are the parameters $P(c_j)$ and $P(w_i</td>
      <td>c_j)$ in this case?</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>the prior $P(c_j)$ is simply counts:
\(P(-) = \frac{3}{5},\quad P(+) = \frac{2}{5}\)</p>
  </li>
  <li>
    <p>then the smoothed likelihood essentially is
\(\hat{P}(w_i|c_j) = \frac{\text{Count}(w_i, c_j)+1}{\sum_{w\in V}\text{Count}(w_i, c_j)+1}=\frac{\text{Count}(w_i, c_j)+1}{\text{Count(total words in $c_j$)}+|V|}\)
where $|V|=20$ and we see that $\text{Count(total words in $+$)}=14$ and $\text{Count(total words in $-$)}=9$. Hence some examples:</p>

    <p><img src="NLP/image-20220323223909939.png" alt="image-20220323223909939" style="zoom:67%;" /></p>

    <p>and the rest is trivial.</p>
  </li>
</ul>

<p>For estimation, first we realize that the word <em>with</em> is OOV. Hence we ignored it as we are doing Naive Bayes. Then we are basically predicting $\text{“Prediction no fun”}$:</p>

<p><img src="NLP/image-20220323224008083.png" alt="image-20220323224008083" style="zoom:67%;" /></p>

<p>hence the result is $-$ as it has a higher probability.</p>

<h3 id="improvements-from-baseline">Improvements From Baseline</h3>

<p>While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance.</p>

<ul>
  <li>
    <p>does the $\text{Count}(w_i,c_j)$ really matters? Maybe all it matters is that fact that it <strong>occurred</strong> in $c_j$ document at least once!</p>
  </li>
  <li>
    <p>dealing with <strong>negation</strong> improves Naive Bayes accuracy as well:
\(\text{I didn’t like this movie.}\quad v.s.\quad \text{I did like this movie.}\)
notice that the negation of <em>didn’t</em> completely changed $P(\text{like}|c_j)$, for instance. The baseline that deals with this is mentioned before by converting them to
\(\text{I didn’t NOT\_like NOT\_this NOT\_movie}.\)
where newly formed words such as $\text{NOT_like}$ will be treated as a word.</p>
  </li>
  <li>
    <p><strong>insufficient labeled training data</strong> to train accurate naive Bayes classifiers using all words in the training set. In those cases we will have to derive word features (e.g. labelled emotion carried in a word) using <strong>sentiment lexicons</strong>. Some popular ones online include:</p>

    <ul>
      <li>
        <p>General Inquirer, LIWC, The Opinion Lexicon, MPQA</p>
      </li>
      <li>
        <p>example of annotated words from MPQA include:</p>

        <p>$$</p>
        <ul>
          <li>: admirable, \,\,beautiful, \,\,confident, \,\,dazzling, \,\,ecstatic,\,\, favor,\,\, glee,\,\, great\</li>
          <li>: awful,\,\, bad,\,\, bias,\,\, catastrophe, \,\,cheat, \,\,deny,\,\, envious,\,\, foul,\,\, harsh, \,\,hate
$$
A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Here we will go over details on how to implement the Binary NB variant.</p>

<blockquote>
  <p>If we believe that word <strong>occurrence</strong> may matter more than word frequency: then we can do “Binary Naive Bayes”, which clips all the word counts in <strong>each document</strong> at $1$. (i.e. it’s like a Boolean switch for each word)</p>

  <ul>
    <li>
      <p>this results in <strong>binary multimodal naive Bayes</strong>, or binary NB</p>
    </li>
    <li>
      <p>in practice this seems to be true, that performance is better.</p>
    </li>
  </ul>
</blockquote>

<p>In algorithm, basically you can <strong>first reduce all duplicate words ==in each document== to one occurrence</strong>, and then perform the same algorithm.</p>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Raw Naive Bayes</th>
      <th style="text-align: center">Boolean Naive Bayes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220323212718953.png" alt="image-20220323212718953" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220323212725092.png" alt="image-20220323212725092" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the highlighted words are duplicates <strong>within the document</strong>. Hence the counts become:</p>

<p><img src="NLP/image-20220323212755987.png" alt="image-20220323212755987" style="zoom:80%;" /></p>

<p>Then for testing, the we <strong>do not remove words</strong> as this is just for making the probability work better.</p>

<h3 id="problems-in-sentiment-classification">Problems in Sentiment Classification</h3>

<p>Some cases where Naive Bayes’ assumption essentially <strong>fails</strong>:</p>

<ul>
  <li>
    <p><strong>Subtlety</strong> in words: each word itself is neutral, but overall there is a sentiment:</p>

    <p>“<em>If you are reading this because it is your darling fragrance, please wear it at home exclusively, and tape the windows shut.</em>”</p>
  </li>
  <li>
    <p><strong>Thwarted Expectations and Ordering Effects:</strong> where words such as <em>However</em> would suddenly change the sentiment</p>

    <p>“<em>This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance. <strong>However, it can’t hold up.</strong></em>”</p>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>In those cases just modelling $P(w_i</td>
      <td>c_j)$ won’t work. Neural Models that remember/see contexts would work better.</td>
    </tr>
  </tbody>
</table>

<h2 id="polarity-in-sentiment-analysis">Polarity in Sentiment Analysis</h2>

<p>Here, the idea is that each <strong>word/phrase</strong> themselves could have polarity, meaning that they themselves <strong>could be indicative of the sentiment of the sentence/document.</strong></p>

<blockquote>
  <p><strong>Polarity</strong> is float which lies in the range of $[-1,1]$ where $1$ means positive statement and $-1$ means a negative statement.</p>
</blockquote>

<p>Some simple measurement of such is to consider:</p>

<ul>
  <li>==how likely each word== is to ==appear in each sentiment class== (discussed here)</li>
  <li>use some form of embedding, which could be learnt in a Word2Vec approach</li>
</ul>

<p>For instance, consider the polarity of the word “<em>bad</em>”:</p>

<p><img src="NLP/image-20220328202939691.png" alt="image-20220328202939691" style="zoom:50%;" /></p>

<p>where</p>

<ul>
  <li>each category $c$ would represent the word “<em>bad</em>” appearing in a 1-star, 2-star, 3-star review, etc</li>
  <li>notice that even in 10-star comments, we still see the word “<em>bad</em>” quite often. This could happen due to ==negations==.</li>
</ul>

<p>Then, since we have the counts, we can compute the likelihood
\(P(w|c) = \frac{\text{Count}(w,c)}{\sum_{w}\text{Count}(w,c)}\)
where $c$ would represent 1-star, 2-star, … Then we could <strong>scale</strong> it so it is <strong>comparable between words</strong>
\(\frac{P(w|c)}{P(w)}\)
for $P(w)$ would be a the probability of observing the word at all.</p>

<hr />

<p><em>Exmaple</em>: Polarity of each word in IMDB</p>

<p>Plotting the scaled likelihood for some common words in the 1-10 star IMDB reviews look like:</p>

<p><img src="NLP/image-20220328203514745.png" alt="image-20220328203514745" style="zoom: 25%;" /></p>

<p>where:</p>

<ul>
  <li>notice that “<em>good</em>” looks relatively flat across ratings (perhaps due to negations). This means “<em>good</em>” is ==not a good indicator== for positive sentiment/rather <strong>neutral polarity</strong>.</li>
  <li>words such as “<em>great</em>” and “<em>excellent</em>” would be a better “metric” for the positive rating of the movie/positive polarity.</li>
  <li>words such as “<em>somewhat</em>” and “<em>fairly</em>” are attenuators as they most happen in th middle of the rating; they dampen extremely positive/negative reviews</li>
  <li>the upshot is that even seemingly positive/negative words <strong><em>could</em> happen in all contexts</strong></li>
</ul>

<h3 id="logical-negation-and-polarity">Logical Negation and Polarity</h3>

<blockquote>
  <p>Is <strong>logical negation</strong> (no, not) associated with negative sentiment? (we know there are problematic cases such as “I don’t hate this movie” is a positive review with double negative.)</p>
</blockquote>

<p>In general, there are <strong>slightly more negation in negative sentiments</strong>. For word such as “<em>no/not</em>”, the plot looks like:</p>

<p><img src="NLP/image-20220328163202281.png" alt="image-20220328163202281" style="zoom:50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>the scaled likelihood is slightly higher for low ratings for negation word such as “<em>no/not</em>”</li>
  <li>so even if we have double negatives such as “I don’t hate this movie”, this is still a useful feature</li>
</ul>

<h2 id="learning-sentiment-lexicons">Learning Sentiment Lexicons</h2>

<p>Here we consider the case that you want to have some sort of word polarity for your domain task, <strong>but there is no dataset with lexicons and ratings</strong> so that you cannot really know which words are more positive/negative.</p>

<ul>
  <li>e.g. maybe dealing with a academic domain, where polarity of words would be different from well-established datasets such as movie reviews</li>
</ul>

<blockquote>
  <p>Can we use a few hand-built patterns/lexicons to <strong>bootstrap and build a larger lexicon list</strong>?</p>

  <ul>
    <li>more lexicons learnt means your model could be more robust to test set</li>
    <li>useful for domain transfer</li>
  </ul>
</blockquote>

<p>The general idea/a simple rule:</p>

<ul>
  <li>adjectives conjointed by “<em>and</em>” tend to have same/similar polarity. e.g. “<em>fair and legitimate</em>”</li>
  <li>adjectives conjointed by “<em>but</em>” tend to have different polarity. e.g. <em>fair but brutal</em></li>
</ul>

<p>Then, we can consider some kind of program that does:</p>

<p><img src="NLP/image-20220328163454985.png" alt="image-20220328163454985" style="zoom: 50%;" /></p>

<p>where seeds are the <strong>hand-built small lexicon list</strong> you made. Now, the question become:</p>

<blockquote>
  <p>Where do you find similar/different words? Use a database or search engine!</p>
</blockquote>

<p><em>Example</em>:</p>

<ol>
  <li>
    <p>Start with a labelled seed set of words. For instance a <strong>labelled seed set of 1336 adjectives</strong> such that:</p>

    <ul>
      <li>there are 675 <strong>positive</strong> ones: <em>adequate, central, clever, etc.</em></li>
      <li>there are 679 <strong>negative</strong> ones: <em>contagious, drunken, ignorant, etc</em></li>
    </ul>
  </li>
  <li>
    <p>Then we can find similar words from a database/a search engine by <strong>conjoining with <em>“and”</em></strong>:</p>

    <p><img src="NLP/image-20220328163757433.png" alt="image-20220328163757433" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>However, words might appear in many pairs. For instance, we could have a “<em>fair and nice</em>”, and but possibly <em>“fair and corrupt”</em> as well.</p>

    <p><img src="NLP/image-20220328164217042.png" alt="image-20220328164217042" style="zoom: 33%;" /></p>

    <p>Therefore, from the <strong>seed set</strong>, we can train a supervised classifier to assign a <strong>polarity similarity score</strong> to a given pair of word.</p>
  </li>
  <li>
    <p>Then we can determine the cluster by:</p>

    <ul>
      <li>for any two pair, if unknown polarity similarity, use the trained classifier</li>
      <li>closer polarity words are grouped/clustered together</li>
    </ul>

    <p>Hence we would arrive at:</p>

    <p><img src="NLP/image-20220328205610887.png" alt="image-20220328205610887" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Output the clusters and words inside it</p>

    <p><img src="NLP/image-20220328164411821.png" alt="image-20220328164411821" style="zoom:33%;" /></p>

    <p>note that of course we can get errors.</p>
  </li>
</ol>

<h3 id="turney-algorithm">Turney Algorithm</h3>

<p>Notice the above is a semi-supervised approach to learn a lexicon list with polarity. Then with those polarity, we could potentially use to classify sentiment of documents containing those words.</p>

<p>However, is there are way to do it in a <strong>unsupervised</strong> approach?</p>

<blockquote>
  <p><strong>Unsupervised</strong> classificaiton of reviews! Done by:</p>

  <ol>
    <li><strong>Extract</strong> a phrasal lexicon from reviews (following some pre-defined rules)</li>
    <li><strong>Learn polarity</strong> of each phrase (by co-occurance with words such as “<em>excellent</em>” and “<em>poor</em>” )</li>
    <li>Rate a review by the <strong>average</strong> polarity of its phrases</li>
  </ol>
</blockquote>

<p>For computation and simplicity, we only extract <strong>two-word phrases</strong> with <strong>adjectives</strong>. Then, the algorithm does</p>

<ol>
  <li>
    <p>First we extract the phrases. We extract two-word phrases that satisfy the following POS tags:</p>

    <p><img src="NLP/image-20220328164637033.png" alt="image-20220328164637033" style="zoom:33%;" /></p>

    <p>note that RB, RBR, RBS are the comparitive/superlative form of <strong>adjectives</strong> JJ.</p>
  </li>
  <li>
    <p>Then we need to know the <strong>polarity of phrase</strong>. We hypothesize that:</p>

    <ul>
      <li>Positive phrases co-occur more with “<em>excellent</em>”</li>
      <li>Negative phrases co-occur more with “<em>poor</em>”</li>
      <li>the choice of the two words should at least come from the graphs in section <a href="#Polarity in Sentiment Analysis">Polarity in Sentiment Analysis</a></li>
    </ul>

    <p>We can measure the <strong>co-occurance</strong> by PMI, which comes from a co-occurance matrix. Recall that it is a measure of <strong>how often two events $x$ and $y$ occur together</strong>, compared with what we would expect if they were <strong>independent</strong>:
\(I(x,y) = \log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)\)
Hence, the <strong>pointwise mutual information</strong> between a target word $w$ and a context word $c$ (for ==some window size such as 7==) is then defined as:
\(\text{PMI}(w,c) = \log_2\left(\frac{P(w,c)}{P(w)P(c)}\right)\)
for probabilities can be estimated with <strong>word-context matrix</strong>, we consider target words $w$ and context $c$. In this case, we don’t really care about context but rather how often $w$ appears with “<em>excellent</em>” and “<em>poor</em>” as:
\(\text{PMI}(w, \text{excellent}) = \log_2\left(\frac{P(w,\text{excellent})}{P(w)P(\text{excellent})}\right)\)
and similarly for “<em>poor</em>”. Then:</p>

    <ul>
      <li>
        <p>To get those <strong>counts and the co-occurance matrix</strong>, we will use the search engine:
\(\hat{P}(w) = \frac{\text{hits}(w)}{N}, \quad N=\sum_w \text{hits}(w)\)
Then $\hat{P}(w_1, w_2)$ can be esitmated by:
\(\hat{P}(w_1, w_2) = \frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)}{\sum_{w_1, w_2}\text{hits}(w_1\,\, \text{Near}\,\, w_2)}=\frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)}{kN}\)
for $k$ being the size of the window, i.e. being $k$ words apart. (we often drop $k$ in subsequent calculation as it is a constant)</p>
      </li>
      <li>
        <p>Finally, the PMI is therefore
\(\text{PMI}(w, \text{excellent}) = \log_2\left(\frac{\text{hits}(w_1\,\, \text{Near}\,\, w_2)/N}{\text{hits}(w)\text{hits}(w_2)/N^2}\right)\)</p>
      </li>
    </ul>

    <p>Finally, we define the polarity of a phrase by doing the ==PMI difference between “<em>excellent</em>” or “<em>poor</em>”==:</p>

    <p><img src="NLP/image-20220328165248858.png" alt="image-20220328165248858" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>Now for evaluating the <strong>polarity of a single review</strong>, we simply first compute the polarity for each extracted phrase in the review:</p>

    <p><img src="NLP/image-20220328165526774.png" alt="image-20220328165526774" style="zoom:33%;" /></p>

    <p>notice that</p>

    <ul>
      <li>“<em>true service</em>” has a low score, i.e. co-occur more often with negative word such as “<em>poor</em>”, hence indicative of bad reviews. The obvious one would be “<em>inconveniently located</em>”</li>
      <li>then the <strong>final score</strong> for the review will be the <strong>average</strong>. Here it is $0.32$, which means this review is slighltly positive.</li>
    </ul>
  </li>
</ol>

<p>Results from this algorithm on the Epinions dataset:</p>

<ul>
  <li>170 (41%) negative</li>
  <li>240 (59%) positive</li>
  <li>baseline (59%): guessing all positive</li>
  <li>Turney algorithm: 74%</li>
</ul>

<p>Note that again, this is good given that it is fully unsupervised!</p>

<h3 id="summary-on-learning-lexicon">Summary on Learning Lexicon</h3>

<p>Both the algorithm covered above share the following pattern:</p>

<ol>
  <li>start with some seed set of words, e.g. “<em>good</em>”, “<em>excellent</em>”, “<em>poor</em>”</li>
  <li>find <strong>other words</strong> that have similar polarity from some external dataset/search engine
    <ul>
      <li>using “<em>and</em>” and “<em>but</em>”</li>
      <li>using co-occurance</li>
    </ul>
  </li>
  <li>add them to lexicon</li>
</ol>

<h2 id="other-sentiment-task">Other Sentiment Task</h2>

<p><em>Recall that</em>:</p>

<p>Often sentiment analysis is framed as the <strong>detection of attitudes</strong>. In particular, we want to find out</p>

<ul>
  <li>
    <p><strong>Holder</strong> (source) of attitude</p>
  </li>
  <li>
    <p><strong>Target</strong> (aspect) of attitude</p>
  </li>
  <li><strong>Type</strong> of attitude
    <ul>
      <li>From a set of types: Like, love, hate, value, desire, etc.</li>
      <li>Or (more commonly) simple weighted <strong>polarity</strong>: positive, negative, neutral, together with <em>strength</em></li>
    </ul>
  </li>
  <li><strong>Text</strong> containing this attitude: which sentence/document</li>
</ul>

<p>In reality:</p>

<ul>
  <li><strong>Simplest task:</strong> Is the attitude of this text positive or negative? This will be our focus in this chapter as a ==classification task==.</li>
  <li><strong>More complex:</strong> Rank the attitude of this text from 1 to 5, i.e. include <em>strength</em></li>
  <li><strong>Advanced:</strong> all the 4 subtasks.</li>
</ul>

<hr />

<blockquote>
  <p>Here we discuss some approaches of identifying the <strong>target/aspect</strong> of a positive/negative review:</p>

  <ul>
    <li>target: “<em>The <strong>food</strong> was <strong>great</strong> but the <strong>service</strong> was <strong>awful</strong></em>”.</li>
    <li>aspect: automatically find out it is talking about “<em>food</em>” and “<em>service</em>”</li>
  </ul>
</blockquote>

<h3 id="finding-targetaspect-of-a-sentiment">Finding Target/Aspect of a Sentiment</h3>

<p>Some simple approaches for extracting <strong>aspects</strong> mentioned in some review:</p>

<ul>
  <li>rule-based: simple
    <ol>
      <li>find all frequenct phrases across reviews, e.g. “<em>fish tacos</em>”</li>
      <li>filter again by some rules such as “an spect should <strong>occur after a sentiment word</strong>”. e.g. we need “<em>… great fish tacos</em>” to keep “<em>fish tacos</em>” unfiltered.</li>
    </ol>
  </li>
  <li>ML based: some aspects might not be in the sentence, i.e. its <strong>meaning is hidden</strong>. e.g. “<em>… the place smells bad</em>” correspond to something like “<em>odor</em>”.
    <ol>
      <li><strong>hand-label</strong> a small corpus of reviews sentences with aspect, e.g. whether it is about “<em>food, decor, service</em>”, etc.</li>
      <li>train a classifier on it</li>
    </ol>
  </li>
</ul>

<p>Then, once you have some way to extract aspect from a sentence, consider the following pipeline:</p>

<p><img src="NLP/image-20220328170839607.png" alt="image-20220328170839607" style="zoom: 50%;" /></p>

<p>hence the general idea would be:</p>

<ul>
  <li>first do a <strong>sentence</strong> level sentiment classification</li>
  <li>from the positive/negative reviews, extract the aspects using the trained classifier</li>
  <li>hence obtain <strong>aspects level classifcation</strong>, essentially by combing the extracted aspect + whether if they are positive/negative from the sentence level</li>
</ul>

<p>For Example, results would look like:</p>

<p><img src="NLP/image-20220328214159748.png" alt="image-20220328214159748" style="zoom: 25%;" /></p>

<h2 id="explaining-sentiment-classification">Explaining Sentiment Classification</h2>

<blockquote>
  <p>Why is certain sentence clasified as positive/negative?</p>
</blockquote>

<p>Still an active research topic, some current approaches:</p>

<ul>
  <li>
    <p>we just highlight the words associated with positive/negative score</p>
  </li>
  <li>
    <p>WT5, comes form T5. It is a text-to-text model, hene it can directly genrate a <strong>text explaining</strong> why it is a positive/negative post and classifying it</p>

    <p><img src="NLP/image-20220328171519774.png" alt="image-20220328171519774" style="zoom:50%;" /></p>
  </li>
</ul>

<h2 id="summary-on-sentiment-classification">Summary on Sentiment Classification</h2>

<p>Generally modeled as <strong>simple classification</strong> or regression task</p>

<ul>
  <li>predict a binary or ordinal label</li>
</ul>

<p>Some common process/task/problems it involves:</p>

<ul>
  <li>
    <p>how to deal with <strong>negation</strong> is important</p>
  </li>
  <li>
    <p>Using all words (in <strong>naive bayes</strong>) works well for some tasks
\(c = \arg\max_{c_j \in C} P(W|c_j)P(c_j) =\arg\max_{c_j \in C}  P(c_j)\prod_i P(w_i |c_j)\)</p>
  </li>
  <li>
    <p>Finding subsets of words may help in other tasks</p>

    <ul>
      <li>
        <p>Hand-built polarity lexicons</p>
      </li>
      <li>
        <p>Use seeds and semi-supervised learning to induce lexicons</p>
      </li>
    </ul>
  </li>
  <li>
    <p>A fully unsupervised approach such as Turney’s Algorithm</p>
  </li>
</ul>

<h1 id="statistical-significance-testing">Statistical Significance Testing</h1>

<blockquote>
  <p>In building systems we often need to compare the performance of two systems. How can we know if the <strong>new system we just built is better</strong> than our old one? How <strong>certain are we</strong> if one model performend better than nother on some test set?</p>
</blockquote>

<p>Let us have:</p>

<ul>
  <li>two models we want to compare, model $A$ being the new one and $B$ being some baseline.</li>
  <li>we are interested in testing on some metric $M$, such as $F_1$ score or accurarcy.</li>
  <li>suppose we have some test set $x$ to evaluate on</li>
</ul>

<p>Obviously we could measure:
\(\delta(x) = M(A,x) - M(B-x) \equiv \text{Effect Size}\)
for $M(A,x)$ is the performance of model $A$ on test set $x$ using metric $M$. Of course the <strong>larger the effect size the better</strong>, but another question we usually want to ask is:</p>

<blockquote>
  <p>if $A$’s superiority over $B$ is likely to hold again if we checked another test set $x’$</p>
</blockquote>

<p>In the paradigm of statistical hypothesis testing, we test this by formalizing two hypotheses. Suppose you found $\delta(x)=0.2$, i.e. we have a $0.2$ higher e.g. accurarcy:
\(H_0: \text{what you observed is just a random effect}\\
H_1: \text{it is not random. Our model is better}\)
We want to show that $H_0$, the <strong>null hypothesis</strong>, has a ==low probability of happening==, so that what we observed is <strong>not</strong> random/by chance:
\(\text{p-value}(x) \equiv P(\delta(X) \ge \delta(x) | H_0 \text{ is true})\)
i.e. we try on some ==random== test set $X=x$ (with mean $0$) to get $\delta(x)$ again and again. If it is ==not by chance==, then $\text{p-value}(x)$ should be small.</p>

<ul>
  <li>e.g. consider playing Poker, I claim that I am a better player, so the null hypothesis is that I win by luck.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>then, suppose $\delta(x)$ is I win 10 “random” games in a roll. We know that $P(\delta(X) \ge \delta(x)</td>
          <td>\text{I win by luck})$ is small. Hence I can reject the null hypothesis and conclude that I am actually a better player, if $\delta(x)$ happened.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<blockquote>
  <p>Therefore, p-value gives the <strong>probability of observing a test statistic as extreme as the one observed $\delta(x)$,</strong> if the null hypothesis is true.</p>

  <p>Hence, if the p-value is small, the observed test statistic is very <strong>unlikely under the null hypothesis</strong></p>
</blockquote>

<ul>
  <li>the expected value of $\delta(X)$ over many test sets, if assuming $A$ is not better than $B$ is $\mathbb{E}_X \delta(X) = 0$ for $X$ comes from a distribution with zero mean</li>
  <li>how small should the p-value be? Often <strong>threshold</strong> such as $0.05,0.01$ is fine.</li>
</ul>

<blockquote>
  <p>How do we compute the probability needed?</p>

  <ul>
    <li>In NLP we generally don’t use parametric tests such as t-tests or ANOVAs as they make <strong>assumptions on the distributions of the test statistic</strong> (such as normality) that don’t generally hold in our cases
So in NLP we usually ==use non-parametric tests based on sampling==.</li>
  </ul>
</blockquote>

<p>Either:</p>

<ul>
  <li>if we had lots of different <strong>random test sets $x^\prime$ with mean $0$</strong> we could just measure all the $\delta(x^\prime)$ for all the $x^\prime$. This gives a distribution, from which we can compute probability of at least $\delta(x)$ happening if it is a random effect</li>
  <li>use a bootstrap test by repeatedly drawing large numbers of smaller samples <strong>with replacement</strong>, under the assumption that the sample is representative of the population.</li>
</ul>

<h2 id="the-paired-bootstrap-test">The Paired Bootstrap Test</h2>

<p>The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation.</p>

<blockquote>
  <p>The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with replacement (called bootstrap samples) from an original larger sample.</p>

  <ul>
    <li>in fact, the idea is to ==virtually create random test sets $X$== by ==sampling with replacement from a fixed test set $x$==.</li>
    <li>since this means we have $X$ with mean of $\delta(x)$, we would have to change the formula a bit</li>
  </ul>
</blockquote>

<p>Consider a tiny text classification example with a test set $x$ of $10$ documents. And suppose we have $M$ being accuracy and we have two models $A$, and $B$:</p>

<p><img src="NLP/image-20220328231849869.png" alt="image-20220328231849869" style="zoom: 67%;" /></p>

<p>where a slash means the model got it wrong. For this test set $x$, the effect size is $\delta(x)=0.2$.</p>

<p>We need to create a large distribution of test sets $X$. We can do this by:</p>

<ol>
  <li>pick a large number $b$, e.g. $b=10^5$ being the number of tests $x^{(i)}$ we want to create</li>
  <li>each test $x^{(i)}$ will have $n=10$ same as $x$. Hence we repeatedly <strong>sample $n$ times from $x$ with replacement</strong>.</li>
  <li>do until we created $x^{(b)}$</li>
</ol>

<p><img src="NLP/image-20220328233452958.png" alt="image-20220328233452958" style="zoom:67%;" /></p>

<p>Now that we have the ==$b$ random test sets==, providing a sampling distribution, we can compute how likely $A$ made $\delta(x)$ by pure chance. We might naively consider:</p>

\[\text{p-value}(x) = \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)}) - \delta(x) \ge 0 \right)\]

<p>so that if ==p-value is low==, then $\delta(x)$ is ==not by pure chance==. However, this would be wrong as
\(\text{p-value}(x) \equiv P(\delta(X) \ge \delta(x) | H_0 \text{ is true})\)
<strong>assumes</strong> the $X$ would yield a mean of $\delta(X)=0$. Here we would yield a $\delta(X=x)=0.2$ since it all came from the test set $x$.</p>

<p>Therefore, the correct one would be
\(\begin{align*}
\text{p-value}(x) 
&amp;= \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)}) - \delta(x) \ge \delta(x) \right)\\
&amp;= \frac{1}{b} \sum_{i=1}^b \mathbb{1}\left( \delta(x^{(i)})  \ge 2\delta(x) \right)
\end{align*}\)</p>

<p>So that if we have $10^5$ tests and for $47$ of them it happened that $ \delta(x^{(i)})  \ge 2\delta(x)$, then it means p-value is $.0047$, i.e. it is very rare by chance. If we take a threshold of $0.01$ being significant, then we can say that this result $\delta(x)$ is statistically significant.</p>

<h1 id="information-extraction-and-ner">Information Extraction and NER</h1>

<p>structed textual document: tables</p>

<blockquote>
  <p>Transform <strong>unstructured</strong> information in a corpus of documents or web pages <strong>into a structured data</strong>, e.g. populating a relational database, to enable further processing.</p>
</blockquote>

<p>Consider receiving an email from job posting:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Raw Data/Email</th>
      <th style="text-align: center">Extracted Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220330162319299.png" alt="image-20220330162319299" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220330162434115.png" alt="image-20220330162434115" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>The task would be to <strong>extract</strong> information from some <strong>pre-defined attributes</strong> we want to extract.</p>

<ul>
  <li>i.e. givne the input, fill out the table (on the right) as much as we can</li>
  <li>notice that each extracted info is an <strong>entity</strong></li>
</ul>

<p>The general pipeline you want to consider is:</p>

<ul>
  <li><strong>named entity recognition</strong></li>
  <li><strong>relation extraction</strong></li>
  <li><strong>template filling</strong></li>
</ul>

<h2 id="named-entity-recognition">Named Entity Recognition</h2>

<blockquote>
  <p>Specific type of information extraction in which the <strong>goal is to extract formal names of particular types of entities</strong> such as people, places, organizations, etc.</p>
</blockquote>

<p>Usually this is <strong>used as a preprocessing step</strong> for some future tasks, such as the template filling task we had, or question answering. Notice that this is a task in between sequence level and token level task - it is a ==span-oriented application==.</p>

<p>Formally, given an input $x$ with $T$ tokens, $x_1,…,x_T$, a span is a continuous sequence of tokens with start $i$ and end $j$ such that $1 \le i \le j \le T$. Then in total  for all possible span length we could have:
\(\frac{T(T-1)}{2}\)
total possible spans. Most application literally <strong>iterate through all possible spans</strong>. Hence they often have some application-specific length limit $L$ such that $j-i &lt; L$ is required/legal span. We refer tot the set of <strong>legal span in $x$ as $S(x)$.</strong></p>

<p>An example would be</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input</th>
      <th style="text-align: center">Output Desired</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220330163241380.png" alt="image-20220330163241380" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220330163309719.png" alt="image-20220330163309719" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where note that we only find NER for the three entities we care here. Hence entities such as “<em>Geneva Conventions</em>” SHOULD NOT be highlighted. Hence this task is generally not easy.</p>

<p>In general, today we have two approaches:</p>

<ul>
  <li>train a BIO tagger. This will essentially be a <strong>Seq-labelling model</strong></li>
  <li>train an end-to-end model using GPT-3. This will be a <strong>Span-based Model</strong></li>
</ul>

<h3 id="bio-based-ner">BIO-based NER</h3>

<p>Some idea for <strong>training BIO-based model</strong> would be a ==Seq-labelling model==</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Training Data</th>
      <th style="text-align: center">BIO version of Training Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220331153159045.png" alt="image-20220331153159045" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220331153208512.png" alt="image-20220331153208512" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Hence this is essentially a Seq-2-Seq decoding. A fuller example would be:</p>

<p><img src="NLP_part2/image-20220331153630932.png" alt="image-20220331153630932" style="zoom: 50%;" /></p>

<p>==TODO: CRF==   on page 178-Chapter 8</p>

<h3 id="span-based-ner">Span-based NER</h3>

<p>Some idea for <strong>training an end-to-end NN based model</strong> would be a ==span-oriented application==.</p>

<ol>
  <li>
    <p>generate <strong>representations for all the spans</strong> in the input. This often contains</p>

    <ul>
      <li>representations of the span boundaries, $h_i,h_j$ being the embeddings</li>
      <li>representation of the content in the span, $f(h_{i+1:j-1}) \to g_{ij}$
        <ul>
          <li>an example of such as function could be $f(h_{i+1:j-1}) = \bar{h}_{i+1:j-1}$ being the average</li>
        </ul>
      </li>
      <li>combine the two representation, $\text{SpanRep}<em>{ij} = [h_1;h_j;g</em>{ij}]$</li>
    </ul>

    <p>this can be done using contextualized input embeddings from the model</p>
  </li>
  <li>
    <p>then, we have a span representation $g_{ij}$ for each span in $S(x)$. Then this is a <strong>classification problem</strong> where each span in an input is assigned a class label $y$:
\(y_{ij} = \text{softmax}(\text{FFNN}(\text{SpanRep}_{ij})) \in \mathbb{R^{|Y|}}\)
since most spans will not have a label, we add $y = \text{null} \cup y$ to the set of labels, and hence $|Y|=|y|+1$.</p>
  </li>
  <li>
    <p>then for <strong>decoding</strong>, take $\arg\max y_{ij}$  to get the tag.</p>
  </li>
</ol>

<blockquote>
  <p><strong>Note</strong> that some post-processing steps will need to be done to prevent overlapping classifications, as this scores all $T(T-1)/2$ spans. However, it does have a benefit as it naturally accommodate embedded named entities:</p>

  <ul>
    <li>e.g. both “<em>United Airlines</em>” and “<em>United Airlines Holding</em>” would be evaluated</li>
    <li>a BIO based tagging approach would have not looked at this.</li>
  </ul>
</blockquote>

<p>A detailed <strong>example</strong> would be:</p>

<ul>
  <li>
    <p>instead of taking average, we consider using the embeddings as
\(g_{ij} = \text{SelfAttn}(h_{i:j})\)
so that the representation would be centered around the head of the phrase corresponding to the span. Then combining to get:
\(\text{SpanRep}_{ij} = [h_1;h_j;g_{ij}]\)</p>
  </li>
  <li>
    <p>finally doing the same:
\(y_{ij} = \text{softmax}(\text{FFNN}(\text{SpanRep}_{ij}))\)</p>
  </li>
</ul>

<p>Hence graphically</p>

<p><img src="NLP_part2/image-20220331125718227.png" alt="image-20220331125718227" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>cross-entropy loss would be used</li>
</ul>

<h2 id="relation-extraction">Relation Extraction</h2>

<p>Now we have detected named entities, our next step is to <strong>discern relationships between those entities</strong>. For instance:</p>

<p><img src="NLP_part2/image-20220331154117733.png" alt="image-20220331154117733" style="zoom:50%;" /></p>

<p>where form the highlighted part, we know that:</p>

<ul>
  <li>“<em>Tim Wagner</em>” is a spokesman for “<em>American Airlines</em>”</li>
  <li>“<em>United</em>” is a unit of “<em>UAL Corp</em>”</li>
  <li>etc.</li>
</ul>

<p>All of those are simple ==binary relationships== which fall under some generic categorization such as:</p>

<ul>
  <li>
    <p>basic <strong>3 relations</strong>: employed-by; located-at; part-of</p>
  </li>
  <li>
    <p>a more <strong>complicated 17 relations</strong> used in ACE relation extraction</p>

    <p><img src="NLP_part2/image-20220331154420120.png" alt="image-20220331154420120" style="zoom:50%;" /></p>
  </li>
</ul>

<p>Hence, a graphical representation of what are doing now is:</p>

<p><img src="NLP_part2/image-20220331154717302.png" alt="image-20220331154717302" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>the first two steps can be done in a NER</li>
  <li>the <strong>relation</strong> can be seen as consisting of a <strong>set of ordered tuples</strong> over elements of a domain (e.g. named entities)</li>
</ul>

<h3 id="relation-extraction-algorithms">Relation Extraction Algorithms</h3>

<p>Now, we answer the question: how do we find those relations? In general, there are four main ways to do it</p>

<ul>
  <li>handwritten patterns</li>
  <li>supervised machine learning</li>
  <li>semi-supervised (via bootstrapping or distant supervision)</li>
  <li>unsupervised</li>
</ul>

<h3 id="using-patterns-to-extract-relations">Using Patterns to Extract Relations</h3>

<p>Consider the example of:</p>

<p><img src="NLP_part2/image-20220331160956115.png" alt="image-20220331160956115" style="zoom: 67%;" /></p>

<p>where we notice that “<em>Gelidium</em>” is a hyponym of “<em>red algae</em>”, which can be identified using the following pattern
\(NP_0 \text{ such as } NP_1 \{,NP_2,...,(\text{and,or})NP_i\}\)
implies the relation
\(\forall NP_i,i\ge 1, \text{hyponym}(NP_i, NP_0)\)
which inspires pattern such as</p>

<table>
  <thead>
    <tr>
      <th>Pattern</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="NLP_part2/image-20220331161313184.png" alt="image-20220331161313184" style="zoom:50%;" /></td>
      <td><img src="NLP_part2/image-20220331161322250.png" alt="image-20220331161322250" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>but of course, with hand-crafted rules we have:</p>

<ul>
  <li>advantage of high precision as they are tailored to specific domains</li>
  <li>low recall, missing a lot</li>
  <li>not scalable</li>
</ul>

<h3 id="relation-extraction-via-supervised-learning">Relation Extraction via Supervised Learning</h3>

<p>Consider now having the</p>

<ul>
  <li><strong>input</strong> of: a fixed set of entities</li>
  <li><strong>label</strong> of: a fixed set of pre-defined relations</li>
</ul>

<p>Then approaches to make it a classification problem could be: for each possible pair, apply a <strong>multiclass classification</strong> for each class being the relation</p>

<p><img src="NLP_part2/image-20220331161750368.png" alt="image-20220331161750368" style="zoom: 67%;" /></p>

<p>where the <strong>feature</strong>/embeddings of the named entity $e_1,e_2$ could be:</p>

<ul>
  <li>word level embeddings of the named entity $e_i$</li>
  <li>head word embedding of the named entity</li>
  <li>encoding the named entity’s type, e.g. if it is <code class="language-plaintext highlighter-rouge">ORG</code>, or <code class="language-plaintext highlighter-rouge">PER</code></li>
  <li>number of entities in this sentence</li>
  <li>encoding some syntactic structure of the sentence</li>
  <li>etc</li>
</ul>

<p>then the architecture of the classifier could be</p>

<p><img src="NLP_part2/image-20220331162335505.png" alt="image-20220331162335505" style="zoom: 67%;" /></p>

<p>where notice that</p>

<ul>
  <li>
    <p>essentially using a transformer, hence using context embeddings for the content</p>
  </li>
  <li>
    <p>we also included the entire sentence to give context for the two named entities in this case</p>
  </li>
</ul>

<p>of course, this can be optimized as we can skip some pairs for certain relation as it won’t happen at at all</p>

<blockquote>
  <p>But labeling a large training set is extremely expensive and supervised models are brittle: they <strong>don’t generalize well</strong> to different text genres.</p>
</blockquote>

<p>For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next.</p>

<ul>
  <li>other problem include: what if the relation set is <strong>not fixed</strong>? Then of course supervised version would not work.</li>
</ul>

<h3 id="semi-supervised-relation-extraction">Semi-supervised Relation Extraction</h3>

<p>One idea is to <strong>bootstrap</strong> more relation-labeled entity pairs from some <strong>known small sample labelled pair</strong>. For instance, suppose you want to get a <strong>relation being <code class="language-plaintext highlighter-rouge">airline/hub</code> pair</strong>, and you already have
\(\text{Ryanair has a hub at Charleroi} \iff \text{[ORG] has a hub at [LOC]}\)
being a known pair, you can:</p>

<ol>
  <li>
    <p>finding other mentions of this relation in our corpus</p>

    <p><img src="NLP_part2/image-20220331163230149.png" alt="image-20220331163230149" style="zoom: 67%;" /></p>
  </li>
  <li>
    <p>use features such as context to extract <strong>general patterns</strong> such as the following</p>

    <p><img src="NLP_part2/image-20220331163324364.png" alt="image-20220331163324364" style="zoom:67%;" /></p>

    <p>which signifies a “rule” for <code class="language-plaintext highlighter-rouge">airline/hub</code> pair</p>
  </li>
  <li>
    <p>use these new patterns can then be used to search for additional tuples.</p>
  </li>
  <li>
    <p>assign <strong>confidence</strong> values to new tuples, and add to the dataset if high confidence</p>

    <ul>
      <li>this is to <strong>avoid semantic drift</strong>. In semantic drift, an erroneous pattern leads to the introduction of erroneous tuples, which, in turn, lead to the creation of problematic patterns and the meaning of the extracted relations ‘drifts’.</li>
    </ul>
  </li>
</ol>

<p>Hence, given some seed samples that have relation $R$, we can <strong>populate this set</strong> by</p>

<p><img src="NLP_part2/image-20220331163657605.png" alt="image-20220331163657605" style="zoom: 67%;" /></p>

<p>Once done, apply supervised classification as mentioned above.</p>

<h3 id="unsupervised-relation-extraction">Unsupervised Relation Extraction</h3>

<blockquote>
  <p>The goal of unsupervised relation extraction is to <strong>extract relations from the web</strong> when we have no labeled training data, and not even any list of relations.</p>

  <p>This task is often called <strong>open information extraction</strong> or Open IE.</p>
</blockquote>

<p>For example, the ReVerb system (Fader et al., 2011) extracts a relation from a sentence s in 4 steps:</p>
<ol>
  <li>Run a part-of-speech tagger and entity chunker over $s$</li>
  <li>For each verb in $s$, find the longest sequence of words $w$ that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.</li>
  <li>For each phrase $w$, find the nearest noun phrase $x$ to the left which is not a relative pronoun, wh-word or existential “<em>there</em>”. Find the nearest noun phrase $y$ to the right.</li>
  <li>Assign confidence $c$ to the relation $r = (x;w;y)$ using a confidence classifier and return it.</li>
</ol>

<h2 id="template-filling">Template Filling</h2>

<blockquote>
  <p><strong>Task</strong>: fill the slots in the associated templates with <strong>fillers</strong> extracted from the text.</p>

  <ul>
    <li>those fillers could come from text segments extracted directly from the text</li>
    <li>consist of concepts that have been inferred from text elements through some additional processing</li>
  </ul>
</blockquote>

<p>A template looks like</p>

<p><img src="NLP_part2/image-20220331164508528.png" alt="image-20220331164508528" style="zoom:67%;" /></p>

<p>where we have four slots.</p>

<p>The next section describes a standard <strong>sequence-labeling approach to filling slots</strong>.</p>

<h3 id="supervised-approach-to-template-filling">Supervised Approach to Template Filling</h3>

<p>We are given</p>

<ul>
  <li>training documents with text spans <strong>annotated</strong> with predefined templates and their slot fillers.</li>
  <li>note that there can be <strong>multiple different templates</strong></li>
</ul>

<p>Our goal is to</p>

<ul>
  <li>create one template for each event in the input</li>
  <li>fill in the slots with text spans.</li>
</ul>

<p>Hence this means that we can split the task to train <strong>two classifiers</strong></p>

<ol>
  <li>
    <p>==first system== decides whether the template is present in a particular sentence</p>

    <ul>
      <li>called <strong>template recognition</strong> or <strong>event recognition</strong></li>
    </ul>

    <p>Then the</p>

    <ul>
      <li><strong>input</strong> could be a <strong>sequence of words</strong>, with the usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags, syntactic chunk tags, and named entity tags.</li>
      <li><strong>output</strong> is indicating any slot from a template being detected</li>
    </ul>

    <p>Then if detected, feed that sequence of words into the second system with that detected template</p>
  </li>
  <li>
    <p>==second system== has the job of role-filler extraction.</p>

    <ul>
      <li><strong>for each role</strong> in the template, e.g. <code class="language-plaintext highlighter-rouge">LEAD-AIRLINE</code>, <code class="language-plaintext highlighter-rouge">AMOUNT</code>, etc</li>
      <li>train a <strong>binary classifier</strong> that on each possible span to decide if it  would fill in the blank</li>
    </ul>

    <p>However, this means that you would need to resolve conflicts as multiple non-identical text segments could be labeled for the same slot.</p>
  </li>
</ol>

<h2 id="ie-examples">IE Examples</h2>

<p>below contains examples over some typical datasets you would deal with.</p>

<h3 id="muc">MUC</h3>

<blockquote>
  <p>Message Understanding Conference (MUC) was an annual event/competition where results were presented.</p>
</blockquote>

<p>But since this is found by DARPA, it cares more about military related. Focused on extracting information from news articles:</p>

<ul>
  <li>
    <p>Terrorist events</p>
  </li>
  <li>
    <p>Industrial joint ventures</p>
  </li>
  <li>
    <p>Company management changes</p>
  </li>
</ul>

<h3 id="medline-corpus">Medline Corpus</h3>

<p>Contains medical related data</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input</th>
      <th style="text-align: center">NER: Proteins</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220330164520567.png" alt="image-20220330164520567" /></td>
      <td style="text-align: center"><img src="NLP/image-20220330164559910.png" alt="image-20220330164559910" /></td>
    </tr>
  </tbody>
</table>

<h3 id="web-extraction">Web Extraction</h3>

<p>Another example would be <strong>web extraction</strong></p>

<ul>
  <li>because those dynamically webpages generally reads data from database, it is often a <strong>semi-structured</strong> data</li>
  <li>i.e., we want to view the webpage back as a <strong>structured table/database</strong>. An extractor for this case is often called a <strong>wrapper</strong></li>
</ul>

<p>An example would be the Amazon Book Descriptions pages:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input Information</th>
      <th style="text-align: center">Extracted Information</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220330164859648.png" alt="image-20220330164859648" /></td>
      <td style="text-align: center"><img src="NLP/image-20220330164910953.png" alt="image-20220330164910953" /></td>
    </tr>
  </tbody>
</table>

<p>where note that here:</p>

<ul>
  <li>
    <p>the <strong>template could be learnt</strong> as well</p>
  </li>
  <li>
    <p>since it is from HTML, we could image many rule-based approach works equally well</p>
  </li>
</ul>

<p>e.g. using <strong>Enhanced REGEX</strong> including</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">\b</code> for word boundary</li>
  <li><code class="language-plaintext highlighter-rouge">\d</code> for a digit</li>
  <li><code class="language-plaintext highlighter-rouge">{}</code> repetition operator, e.g. <code class="language-plaintext highlighter-rouge">A{1,5}</code> means you can have one-to-five <code class="language-plaintext highlighter-rouge">A</code></li>
</ul>

<p>Some examples include (in Perl):</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">/\b(\(\d{3}\)\s?)?\d{3}-\d{4}\b/</code> for US phone number</li>
</ul>

<p>Hence for filling in each field:</p>

<ul>
  <li>
    <p>Price pattern: <code class="language-plaintext highlighter-rouge">\b\$\d+(\.\d{2})?\b</code>. However, we also need to to <strong>identify proper context</strong>.</p>

    <ul>
      <li>Pre-filler pattern: <code class="language-plaintext highlighter-rouge">&lt;b&gt;List Price:&lt;/b&gt; &lt;span class=listprice&gt;</code></li>
      <li>Post-filler pattern: <code class="language-plaintext highlighter-rouge">&lt;/span&gt;</code></li>
    </ul>

    <p>those could also be <strong>induced</strong>, e.g  by RAPIER</p>

    <p><img src="NLP_part2/image-20220331171232523.png" alt="image-20220331171232523" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>of course, for other non-html format, REGEX may not work as well hence you need to train classifiers</p>
  </li>
</ul>

<h2 id="evaluating-ie-accuracy">Evaluating IE Accuracy</h2>

<p>Consider we have a <strong>fixed template</strong> to begin with, which has $N$ slots. Then:</p>

<ul>
  <li>our model extracted $C$ correct slot/value pairs in the template</li>
  <li>extracted a total number of $E$ pairs</li>
</ul>

<p>Then we can apply</p>

<ul>
  <li>precision=$C/N$</li>
  <li>recall=$C/E$</li>
  <li>$F_1$ measure from the above</li>
</ul>

<h1 id="question-answering">Question Answering</h1>

<blockquote>
  <p>The goal of question answering is to build systems that <strong>automatically answer questions</strong> posed by humans in a natural language</p>
</blockquote>

<p>Most question answering systems focus on a particular subset of these information needs: <strong>factoid questions</strong>, questions that can be answered with simple facts expressed in short texts:</p>

<ul>
  <li>e.g. “<em>Where is the Louvre Museum located?</em>”</li>
</ul>

<p>In general, the two common paradigms that solve this task are</p>

<ul>
  <li><strong>Information-retrieval (IR) based QA</strong>, sometimes called <strong>open domain QA</strong>, which relies on the vast amount of text on the web:
    <ol>
      <li>Given a user question, perform pre-processing of the question including tokenization/normalization</li>
      <li>information retrieval is used to <em>find relevant passages</em>, i.e. find documents that contain an answer
        <ul>
          <li>using cosine similarity of some vector representation of the query and documents you have</li>
        </ul>
      </li>
      <li>Then neural <strong><em>reading comprehension</em></strong> algorithms read these retrieved passages</li>
      <li>Draw the answer as a <em>span of text</em> from the passage (like SAT, GRE exams)</li>
    </ol>
  </li>
  <li><strong>knowledge-based question answering</strong>, which
    <ol>
      <li>builds a semantic representation of the query</li>
      <li>These meaning representations are then used to <strong><em>query databases of facts</em></strong></li>
    </ol>
  </li>
</ul>

<blockquote>
  <p><strong>Bonus</strong>: for large pretrained language models (i.e. on next word prediction), they can sometimes <em>directly answer the question</em>. This work because huge pretrained language models have already encoded a lot of factoids</p>
</blockquote>

<h2 id="information-retrieval">Information Retrieval</h2>

<blockquote>
  <p><strong>Information retrieval</strong> or IR is the name of the field encompassing the retrieval of all manner of media based on user information needs. The resulting IR system is often called a <strong>search engine</strong>.</p>
</blockquote>

<p>In particular, the IR we consider is called <strong>ad hoc retrieval</strong>, which:</p>

<ul>
  <li>a user poses a <strong>query</strong> to a retrieval system</li>
  <li>we output an <strong>ordered set of documents</strong> from some collection</li>
</ul>

<p><img src="NLP_part2/image-20220404202909433.png" alt="image-20220404202909433" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>A <strong>document</strong> refers to whatever unit of text the system indexes and retrieves (web pages, scientific papers, etc)</li>
  <li>A <strong>term</strong> refers to a word in a collection, but it may also include phrases</li>
  <li>the vector hence can be a TF-IDF vector or a bag of words. Then we can measure similarity for ranking.</li>
</ul>

<h3 id="term-weighting-and-document-scoring">Term weighting and document scoring</h3>

<p>Recall that if we use TF-IDF</p>

<blockquote>
  <p><strong>TF-IDF</strong> essentially consist of two components:</p>

  <ul>
    <li><strong>term frequency</strong> in log, essentially captures the frequency of words occurring in a document (i.e. the word-document matrix)</li>
    <li><strong>inverse document frequency</strong>, essentially giving a <strong>higher weight</strong> to words that occur <strong>only in a few documents</strong> (i.e. they would carry important discriminative meanings)</li>
  </ul>
</blockquote>

<p>The <strong>term frequency</strong> in a document $d$ is computed by:
\(\text{tf}(t,d) = \log_{10}(\text{count}(t,d) + 1)\)
where:</p>

<ul>
  <li>we added $1$ so that we won’t do $\log 0$, which is negative infinity</li>
</ul>

<p>The <strong>document frequency</strong>  of a term $t$ is the number of documents it occurs in.</p>

<ul>
  <li>note that this is different from <strong>collection frequency</strong> of a term, which is the total number of times the word appears in any document of the whole collection</li>
</ul>

<p>For example: consider in the collection of Shakespeare’s 37 plays the two words <em>Romeo</em> and <em>action</em></p>

<p><img src="NLP_part2/image-20220210001201947.png" alt="image-20220210001201947" style="zoom:50%;" /></p>

<p>Therefore, we want to <strong>emphasize discriminative</strong> words like Romeo via the <strong>inverse document frequency</strong> or IDF term weight:
\(\text{idf}(t) = \log_{10}\left( \frac{N}{\text{df}(t) } \right)\)
where:</p>

<ul>
  <li>apparently $\text{df(t)}$​ is the <strong>number of documents</strong> in which term $t$ occurs and $N$ is the total number of documents in the collection</li>
  <li>so essentially, the fewer documents in which a term occurs, the higher this weight.</li>
  <li>
    <p>notice that we don’t need $+1$ here because the minimum document frequency of a word in your corpus would be $1$</p>
  </li>
  <li>again, because the number could be large, we use a $\log$.</li>
</ul>

<p>Here are some IDF values for some words in the Shakespeare corpus</p>

<p><img src="NLP_part2/image-20220210001906935.png" alt="image-20220210001906935" style="zoom: 50%;" /></p>

<p>Finally, the TF-IDF basically then does:
\(\text{TF-IDF}(t,d) = \text{tf}(t,d) \times \text{idf}(t)\)
An example would be:</p>

<p><img src="NLP_part2/image-20220210002036715.png" alt="image-20220210002036715" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>essentially it is <strong>still a term-document matrix</strong>, but it is <strong>weighted by IDF</strong>.</li>
  <li>notice that because $\text{idf(good)}=0$​, the row vector for <em>good</em> becomes all zero: this word appears in every document, the tf-idf weighting leads it to be ignored (as it is not very informative anymore)</li>
</ul>

<h3 id="document-scoring">Document Scoring</h3>

<p>Then, the above provide a way to <strong>represent a document by a vector</strong> (vertically). Then essentially:</p>

<ul>
  <li>
    <p>encode document $d$ to vector $\vec{d}$</p>
  </li>
  <li>
    <p>encode query $q$ to vector $\vec{q}$</p>
  </li>
  <li>
    <p>measure similarity as score:
\(\text{score}(q,d) = \cos (\vec{q},\vec{d}) = \frac{\vec{q}\cdot \vec{d}}{|\vec{q}||\vec{d}|}\)</p>
  </li>
</ul>

<p>However, since <strong>most queries are short and contain unique words</strong>, we want to approximate this by:
\(\begin{align*}
\text{score}(q,d) 
&amp;= \frac{\vec{q}}{|\vec{q}|}\frac{\vec{d}}{|\vec{d}|}\\
&amp;= \sum_{t \in q} \frac{\text{tf-idf}(t,q)}{\sqrt{\sum_{t'\in q}\text{tf-idf}(t',q)^2}} \cdot \frac{\text{tf-idf}(t,d)}{\sqrt{\sum_{t'\in d}\text{tf-idf}(t',d)^2}}\\
&amp;\approx \sum_{t \in q}  \frac{\text{tf-idf}(t,d)}{\sqrt{\sum_{t'\in d}\text{tf-idf}(t',d)^2}}\\
&amp;= \sum_{t \in q} \frac{\text{tf-idf}(t,d)}{|\vec{d}|}
\end{align*}\)
where the third approximation comes from:</p>

<ul>
  <li>
    <p>since we assume sparse query and unique words, hence
\(\text{TF-IDF}(t,q) = \text{tf}(t,q) \times \text{idf}(t)\approx \text{idf}(t)\)
as term frequency is approximately one</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>then the inverse document frequencies will be a constant for each document $d$, hence $</td>
          <td>\vec{q}</td>
          <td>$ will not affect ranking</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<hr />

<p><em>For Example</em>: Consider utilizing the above to rank the following query/document</p>

<p><img src="NLP_part2/image-20220404204729668.png" alt="image-20220404204729668" style="zoom:50%;" /></p>

<p>then, we first compute the TF-IDF like a <strong>term-document matrix</strong></p>

<p><img src="NLP_part2/image-20220404204951658.png" alt="image-20220404204951658" style="zoom:50%;" /></p>

<p>where as an example, we see that document 2 which is “<em>Sweet sorrow</em>” has:</p>

<ul>
  <li>
    <p>term frequency of only those two words appearing once, then TF is computed by
\(\text{tf}(t,d) = \log_{10}(\text{count}(t,d) + 1)=\log_{10}(1+1) = \log_{10}(2)\)
for both word</p>
  </li>
  <li>
    <p>document frequency of “<em>sweet</em>” appears in <strong>three of the collection of documents</strong>, and “<em>sorrow</em>” only appeared here hence a count of one.</p>
  </li>
</ul>

<p>Finally, we use this forumula to compute similarity by <strong>iterating through words in the query</strong>
\(\sum_{t \in q} \frac{\text{tf-idf}(t,d)}{|d|}\)
which basically iterates over the word “<em>sweet</em>” and then “<em>love</em>”, hence we get:</p>

<p><img src="NLP_part2/image-20220404205323421.png" alt="image-20220404205323421" style="zoom: 50%;" /></p>

<blockquote>
  <p><strong>Note</strong> that in this formula, if ==document $d$ contains none of the words in query $q$, then it will have a score of zero==. This is why we needed the <strong>inverted index</strong> to quickly filter documents and only get the ones containing terms in $q$.</p>
</blockquote>

<hr />

<p>In the previous example, we have included <strong>preprocessing steps</strong> such as:</p>

<ul>
  <li>remove punctuations and convert to lower case</li>
  <li>(deprecated) remove high-frequency words (e.g. <strong>stop words</strong>) from both the query and document before representing them</li>
</ul>

<p>where the latter is no longer used as it is partly handled by the TF-IDF weighting already and it could make query such as “<em>to be or not to be</em>” to be reduced to “<em>not</em>”, which makes IR hard.</p>

<h3 id="inverted-index">Inverted Index</h3>

<p>As mentioned before, In order to compute scores, we need to efficiently <strong>find documents that contain words in the query</strong>.</p>

<blockquote>
  <p>The basic search problem in IR is thus to find all documents $d \in C$ a collection that contain a term $q\in Q$ a query.</p>
</blockquote>

<p>An inverted index basically maps each word to <strong>indices of documents $d$</strong> that contains the word. For instance, consider the same example as before</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Text</th>
      <th style="text-align: center">Example TF-IDF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220404204729668.png" alt="image-20220404204729668" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220404210338771.png" alt="image-20220404210338771" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>The idea is to establish an <strong>index of all words in our database</strong>, and we get</p>

<p><img src="NLP_part2/image-20220404210357269.png" alt="image-20220404210357269" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>each word containing its <strong>document frequency</strong> in ${}$, which is the same as the ones computed in previous example</li>
  <li>a <strong>pointer</strong> to a postings list that contains document IDs and term counts in $[]$.
    <ul>
      <li>e.g. the word “<em>nurse</em>” appeared in document ID 1 and 4, each once.</li>
    </ul>
  </li>
</ul>

<p>Then, with this structure, we can:</p>

<ul>
  <li>given a list of terms in query</li>
  <li>very efficiently get lists of all candidate documents <strong>and</strong> together with the TF and DF values, hence easily compute the scores.</li>
</ul>

<h3 id="ir-with-dense-vectors">IR with Dense Vectors</h3>

<p>The score and hence collection of document returned using TF-IDF heavily depended us to have <strong>overlapping words in the document</strong>.</p>

<blockquote>
  <p>In other words, the user posing a query (or asking a question) needs to <strong>guess exactly what words</strong> the writer of the <strong>answer</strong> might have used to discuss the issue.</p>

  <ul>
    <li>e.g. we want to search for “<em>tragic love story</em>” but Shakespeare writes instead about “<em>star-crossed lovers</em>”.</li>
  </ul>

  <p>This is called the <strong>vocabulary mismatch problem</strong></p>
</blockquote>

<p>Therefore, the solution is to find a representation that can handle ==synonymy==: instead of (sparse) word-count vectors, ==using (dense) embeddings==, e.g. encoders like BERT.</p>

<p>In fact, in what is sometimes called a <strong>bi-encoder</strong> we use two separate encoder models $\text{BERT}_Q, \text{BERT}_D$:</p>

<ul>
  <li>
    <p>one to <strong>encode the query</strong> $q$
\(h_q = \text{BERT}_Q(q)[\text{CLS}]\)
where since we ==need a single vector==, we take the embedding of the $[\text{CLS}]$ token which encodes all information of the sequence</p>
  </li>
  <li>
    <p>one to <strong>encode the document $d$</strong>
\(h_d = \text{BERT}_D(d)[\text{CLS}]\)</p>
  </li>
  <li>
    <p>use the dot product between these two vectors as the score
\(\text{score}(d,q) = h_d \cdot h_q\)</p>
  </li>
</ul>

<p>Hence graphically</p>

<p><img src="NLP_part2/image-20220404211226917.png" alt="image-20220404211226917" style="zoom:50%;" /></p>

<hr />

<p>Some challenges in this area is:</p>

<ul>
  <li>Among the many areas of active research are <strong>how to do the fine-tuning</strong> of the encoder modules on the IR task</li>
  <li>Efficiency is also an issue. For TF-IDF version we had inverted index which allowed us to quickly rank documents. But here, with <strong>dense vectors we need a Nearest Neighbor search</strong>, which is inefficient.
    <ul>
      <li>Modern systems therefore make use of approximate nearest neighbor vector search algorithms like ==Faiss==</li>
    </ul>
  </li>
</ul>

<h2 id="ir-based-factoid-qa">IR-Based Factoid QA</h2>

<p>Finally, we come back to <strong>how to perform QA task using IR</strong>.</p>

<blockquote>
  <p>The goal of IR-based QA (sometimes called open domain QA) is to answer a user’s question by</p>

  <ol>
    <li>finding a <strong>related document</strong> among the web or some other large collection of documents. (IR task)</li>
    <li>finding <strong>short text segments</strong> from the selected related documents (what we are doing here)</li>
  </ol>
</blockquote>

<p>Some examples of what we need to do include:</p>

<p><img src="NLP_part2/image-20220404212126291.png" alt="image-20220404212126291" style="zoom:50%;" /></p>

<p>Therefore the dominant paradigm implementation basically includes two steps:</p>

<p><img src="NLP_part2/image-20220404212600688.png" alt="image-20220404212600688" style="zoom:67%;" /></p>

<p>which is essentially a ==retrieve and read model==</p>

<ol>
  <li>use <strong>retriever</strong> to retrieve relevant passages from a text collection, usually using a search engines
    <ul>
      <li>this we have discussed before in IR, e.g. use Dense Vectors for embedding</li>
    </ul>
  </li>
  <li>a neural <strong>reading comprehension</strong> algorithm passes over each passage and ==finds spans that are likely to answer the question==
    <ul>
      <li>given the query $q$ and a sample passage $p$ that could contain the answer</li>
      <li>return an answer $s$ or perhaps declare there is no answer</li>
      <li>hence here it becomes a <strong>close domain QA</strong> as we only have a fixed set of selected documents to look at</li>
    </ul>
  </li>
</ol>

<p>For the above to work, we will need a few components in the pipeline (we have already discussed the retriever in IR):</p>

<ul>
  <li>datasets for reading comprehension</li>
  <li>how to make a neural reader</li>
</ul>

<h3 id="ir-based-qa-datasets">IR-based QA: Datasets</h3>

<p>Our aim is to <strong>train a reading comprehension systems</strong>, so that given a passage and a question, and predicts a span in the passage as the answer. Therefore, we consider <strong>comprehension datasets</strong> containing tuples of <em>(passage, question, answer)</em>.</p>

<ul>
  <li><strong>Stanford Question Answering Dataset (SQuAD)</strong>, consists of passages from Wikipedia and associated questions whose answers are spans from the passage</li>
  <li>Squad 2.0 in addition adds some questions that are designed to be unanswerable</li>
</ul>

<p>Examples look like:</p>

<p><img src="NLP_part2/image-20220404213333825.png" alt="image-20220404213333825" style="zoom:50%;" /></p>

<p>However, those datasets are often constructed by having annotators first read the passage, then construct QAs. This could make the questions easier. Hence, we also attempted to make datasets from <strong>questions that were not written with a passage in mind</strong>.</p>

<ul>
  <li>
    <p><strong>TriviaQA dataset</strong> resulting in 650K question-answer-evidence triples</p>
  </li>
  <li>
    <p>etc.</p>
  </li>
</ul>

<h3 id="ir-based-qa-reader">IR-based QA: Reader</h3>

<p>The retriever have now found a couple of relevant documents. Hence</p>

<blockquote>
  <p>The <strong>reader’s</strong> job is to take a passage and a question as input and <strong>produce the answer</strong>.</p>

  <ul>
    <li>In the <strong>extractive QA</strong> we discuss here, the answer is a <strong>span of text</strong> in the passage</li>
    <li>(skipped )more difficult task of <strong>abstractive QA</strong>, in which the system can write an answer which is not drawn exactly from the passage.</li>
  </ul>
</blockquote>

<p>For example:</p>

<ul>
  <li>given a question like <em>“How tall is Mt. Everest?”</em> and a passage that contains the clause <em>“Reaching 29,029 feet at its summit, a reader will output 29,029 feet.”</em></li>
  <li>output “<em>29,029 feet.</em>”</li>
</ul>

<blockquote>
  <p>Then this is a <strong>span-labelling task</strong> (e.g. <a href="#Span-based NER">Span-based NER</a>), i.e. we <strong>output a span</strong> that we think constitutes the answer</p>

  <ul>
    <li>given a question $q$ of $n$ tokens, $q_1,…,q_n$, and a passage $p$ of $m$ tokens that could contain the answer $p_1,…,p_m$</li>
    <li>
      <table>
        <tbody>
          <tr>
            <td>return $p(a</td>
            <td>q,p)$ for each span $a \in S(p)$ a set of possible spans in $p$</td>
          </tr>
        </tbody>
      </table>
    </li>
  </ul>
</blockquote>

<p>Alike NER, we would need to score all possible spans, but we can make a <strong>assumption/simplification</strong> that
\(P(a|q,p) = P_{start}(a_s|q,p)P_{end}(a_e|q,p)\)
for basically $P_{start}(i|q,p)$ means token $p_i$ is the start of the span, and similarly for $P_{end}(j|q,p)$. How do we model such a probability? A standard baseline would be using BERT</p>

<p><img src="NLP_part2/image-20220404215201459.png" alt="image-20220404215201459" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>since we are conditioned on $q,p$, we pass <strong>both as input to the encoder</strong></p>
  </li>
  <li>
    <p>we add two new special vectors: a <strong>span-start embedding $S$</strong> and <strong>a span-end embedding $E$</strong>, which will be learned in fine-tuning so that
\(P_{start}(i|q,p) = \text{Softmax}(S \cdot \vec{p}_i') = \frac{\exp(S \cdot \vec{p}_i')}{\sum_j \exp(S \cdot \vec{p}_j')}\)
where the embedding vector $\vec{p}_i’$ of token $p_i$ would have already contained information from $q$. Similarly
\(P_{end}(i|q,p) = \text{Softmax}(E \cdot \vec{p}_i') = \frac{\exp(E \cdot \vec{p}_i')}{\sum_j \exp(E \cdot \vec{p}_j')}\)</p>
  </li>
  <li>
    <p>then, the training objective would be  maximizing the probability of correct start and end positions, hence minimizing negative log likelihood:
\(L = -\log P_{start}(i^*|q,p) - \log P_{end}(j^*|q,p)\)
where $i^<em>, j^</em>$ are the gold labels.</p>
  </li>
  <li>
    <p>for inference, we output the <strong>highest $\arg\max_{i,j} P_{start}(i|q,p)P_{end}(j|q,p)$ ** which is the same as finding the **highest score of</strong>
\(\arg\max_{i,j} P_{start}(i|q,p)P_{end}(j|q,p) = \arg\max_{i,j} S \cdot \vec{p}_i' + E \cdot \vec{p}_j'\)
as products of the exponentials (from the Softmax) are basically comparing sums of exponents</p>
  </li>
</ul>

<hr />

<p>Other model prior to BERT include BiDAF, Bidirectional Attention Flow model:</p>

<p><img src="NLP_part2/image-20220404225037681.png" alt="image-20220404225037681" /></p>

<p>where the output is the same, $i,j$,  but here the major difference is that</p>

<ul>
  <li>
    <p><strong>attention</strong>: query to context attention, i.e. $q\to d$, and context to query attention, i.e. $d \to q$</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">C2Q</th>
          <th style="text-align: center">Q2C</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP_part2/image-20220404225356688.png" alt="image-20220404225356688" /></td>
          <td style="text-align: center"><img src="NLP_part2/image-20220404225408943.png" alt="image-20220404225408943" /></td>
        </tr>
      </tbody>
    </table>

    <p>so that:</p>

    <ul>
      <li>Context-to-query attention: For each context word, choose the most relevant words from the query words.</li>
      <li>Query-to-context attention: choose the context words that are most relevant to one of query words.</li>
    </ul>
  </li>
  <li>
    <p><strong>modelling</strong>: uses Bidirectional LSTM for <strong>encoding</strong>, which is <strong>sequential</strong> and hence not efficient. So, its speed is suboptimal compared to Transformer based which suitable for <strong>parallelization</strong>.</p>
  </li>
</ul>

<h2 id="knowledge-based-qa">Knowledge-based QA</h2>

<blockquote>
  <p><strong>Knowledge-based question answering</strong>: the idea of answering a natural language question by mapping it to a query over a <strong>structured database</strong>.</p>
</blockquote>

<p>Two common paradigms are used for knowledge-based QA.</p>

<ol>
  <li><strong>graph-based QA</strong>, models the knowledge base as a graph, i.e. entities as nodes and relations or propositions as edges between nodes.</li>
  <li><strong>QA by semantic parsing</strong>, using the semantic parsing methods</li>
</ol>

<p>Both of the methods above would need some entity linking step. Hence this will be discussed first.</p>

<h3 id="entity-linking">Entity Linking</h3>

<blockquote>
  <p><strong>Entity linking</strong> is the task of associating a <strong>mention in text</strong> with the representation of some <strong>real-world entity</strong> in an ontology.</p>

  <ul>
    <li>The most common ontology for factoid question-answering is Wikipedia, since Wikipedia is often the source of the text that answers the question</li>
  </ul>
</blockquote>

<p>For example,</p>

<ul>
  <li>given the sentence “<em>Paris is the capital of France</em>”</li>
  <li>the idea is to determine that “<em>Paris</em>” refers to the city of Paris and not to Paris Hilton or any other entity that could be referred to as “<em>Paris</em>”.</li>
</ul>

<p>In many practical cases, the set of entities will be the <strong>set of Wikipedia articles</strong>, where each article itself is an unique entity.</p>

<ul>
  <li>skipped, but can be treated as a sequence labelling task and use NN based methods</li>
</ul>

<h3 id="knowledge-based-qa-from-rdf-stores">Knowledge-Based QA from RDF stores</h3>

<p>Here we focus on the very simplest case of <strong>graph-based QA</strong>, in which the dataset is a set of factoids in the form of <strong>RDF triples</strong></p>

<blockquote>
  <p>RDF triple is a 3-tuple, a <strong>predicate</strong> with <strong>two arguments</strong>, expressing some simple relation or proposition. An example would look like</p>

  <p><img src="NLP_part2/image-20220404221421943.png" alt="image-20220404221421943" style="zoom:67%;" /></p>

  <p>Then, we can use this by:</p>

  <ul>
    <li>locate the entity asked in the query $Q$</li>
    <li>check which relation is it asking, e.g. <em>birth-year</em>?</li>
    <li>return the answer in this RDF triplet</li>
  </ul>
</blockquote>

<p>Let’s assume we’ve already done the stage of entity linking introduced in the prior section. Thus we’ve <strong>mapped already</strong> from a textual mention like <em>Ada Lovelace</em> to the canonical entity ID in the knowledge base. Then:</p>

<ul>
  <li>
    <p>determine which relation is being asked about. e.g.
\(\text{“When was ... born”} \to \text{"birth-year"}\)
so that the question becomes:
\(\text{“When was Ada Lovelace born?”} \to \text{birth-year (Ada Lovelace, ?x)}\)
For simple questions, where we <strong>assume the question has only a single relation</strong>, relation detection and linking can be done in a way resembling the neural entity linking models: computing <strong>similarity (generally by dot product) between the encoding of the question text and an encoding for each possible relation</strong>.</p>
  </li>
  <li>
    <p>then, once we located the relation like above, return the answer by fetching in the RDF database</p>
  </li>
</ul>

<h3 id="knowledge-based-qa-by-semantic-parsing">Knowledge-Based QA by Semantic Parsing</h3>

<p>The second kind of knowledge-based QA uses a <strong>semantic parser</strong> to <strong>map the question to a structured program</strong> to produce an answer.</p>

<ul>
  <li>basically into a query language like SQL or SPARQL</li>
</ul>

<p><em>Examples</em> of input question and output formatted query looks like:</p>

<p><img src="NLP_part2/image-20220404222033378.png" alt="image-20220404222033378" style="zoom:50%;" /></p>

<p>where the logical form of the question is thus either in the form of a query or can easily be converted into one (predicate calculus can be converted to SQL, for example).</p>

<blockquote>
  <p>In a <strong>supervised</strong> case, the task is then to take those pairs of training tuples (question and logical form pair) and produce a system that maps from new questions to their logical forms.</p>
</blockquote>

<p>A common baseline algorithm is a simple sequence-to-sequence model, for example using BERT:</p>

<ul>
  <li>using BERT to represent question tokens</li>
  <li>passing them to an encoder-decoder</li>
</ul>

<p><img src="NLP_part2/image-20220404222259514.png" alt="image-20220404222259514" style="zoom:67%;" /></p>

<hr />

<p><em>Recall</em>: Encoder-Decoder</p>

<p>Essentially for decode, we usually do it <strong>auto-regressively</strong> so that suppose our ==decoder== has <code class="language-plaintext highlighter-rouge">Attention</code> with <code class="language-plaintext highlighter-rouge">MAX_SEQ_LEN=32</code>. Then:</p>

<p><img src="NLP_part2/transformer_decoding_1.gif" style="zoom: 50%;" /></p>

<p>the input to decoder done <strong>auto-regressively</strong>:</p>

<ul>
  <li>at $t=0$, there is no input/or we have <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;</code> for filling the 32 sequence length and a positional embedding</li>
  <li>get cross attention from encoder output</li>
  <li>generate an output “<code class="language-plaintext highlighter-rouge">I</code>” and feed back as input at $t=1$. So we <strong>get <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;I</code> as the input of decoder</strong></li>
  <li>repeat until <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> is generated</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: Comparing with using a RNN based decoder, when we are generating output $o_{t+1}$, the <em>difference</em> is:</p>

  <ul>
    <li><strong>transformer</strong> based can <strong>only condition/read in</strong> $o_{t-31},…,o_t$ for a max sequence length of <code class="language-plaintext highlighter-rouge">32</code> for the attention layer</li>
    <li><strong>RNN</strong> based can use $h_t$ which ==encodes all previous information==. However, it has ==no attention/is sequential==.</li>
  </ul>
</blockquote>

<p>Essentially it is a Seq-2-Seq architecture, hence often done auto-regressively.</p>

<h2 id="using-language-models">Using Language Models</h2>

<blockquote>
  <p>An alternative approach to doing QA is to query a pretrained language model, forcing a model to answer a question <strong>solely from information stored in its parameters</strong>.</p>
</blockquote>

<p>The popular model in this case is the <strong>T5</strong> language model, which is</p>

<ul>
  <li>an <strong>encoder-decoder</strong> architecture (mentioned above)</li>
  <li>pretrained to fill in masked <strong>spans of task</strong></li>
</ul>

<p>The general picture of what it does is</p>

<p><img src="NLP_part2/image-20220404223704483.png" alt="image-20220404223704483" style="zoom:67%;" /></p>

<p>where as it is trained as a language model, we needed to <strong>finetune</strong> the T5 system to the question answering task, by giving it a question, and training it to output the answer text in the decoder.</p>

<h2 id="evaluation-of-factoid-answers">Evaluation of Factoid Answers</h2>

<p><strong>Factoid question answering</strong> is commonly evaluated using mean reciprocal rank, or <strong>MRR</strong></p>

<blockquote>
  <p><strong>MRR</strong> is designed for systems that <strong>return a short ranked list of answers</strong> or passages for each test set question, so that:</p>

  <ol>
    <li>each test set <strong>question</strong> is <strong>scored</strong> with the <strong>reciprocal of the rank</strong> of the first correct answer</li>
    <li>then MRR is the <strong>average of the scores</strong> for each question in the test set</li>
  </ol>
</blockquote>

<p>For example, if the system returned</p>

<ul>
  <li>five answers to a question (with its own ranking)</li>
  <li>but the first three are wrong (so the highest-ranked correct answer is ranked fourth)</li>
</ul>

<p>Then the reciprocal rank for that question is $1/4$. (The score for questions that return no correct answer is $0$.)</p>

<p>Therefore, more formally, for a system returning ranked answers to each question in test set $Q$, then MRR is
\(\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}\)</p>

<hr />

<p>On the other hand, <strong>reading comprehension systems</strong> on datasets like SQuAD are evaluated by:</p>

<ul>
  <li><strong>Exact match</strong>: The % of predicted answers that match the gold answer exactly</li>
  <li><strong>F1 score</strong>: The average word/token overlap between predicted and gold answers.
    <ol>
      <li>Treat the prediction and gold as a bag of tokens,</li>
      <li>compute F1 for each question</li>
      <li>return the average F1 over all questions.</li>
    </ol>
  </li>
</ul>

<h1 id="language-generation">Language Generation</h1>

<blockquote>
  <p><strong>Text generation</strong> is a subfield of natural language processing (NLP). It leverages knowledge in computational linguistics and artificial intelligence to <strong>automatically generate natural language texts</strong></p>
</blockquote>

<p>Typically, you will see architectures generating a text be:</p>

<p><img src="NLP_part2/image-20220406211349237.png" alt="image-20220406211349237" style="zoom: 67%;" /></p>

<p>where essentially:</p>

<ul>
  <li>the language model’s aim is to output a distribution for next word $w_{n+1}$, given the input $w_1,..,w_n$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>hence essentially it is outputting $P(w_{n+1}</td>
          <td>w_{i},…,w_n)$, hence called the language model</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>this is usually done in an <strong>auto-regressive</strong> way, i.e. generated output is fed back as input <strong>until</strong> we generated some end of sentence token, such as <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>.</li>
  <li>this architecture is related to many down stream tasks such as Machine Translation, Dialog Generation, etc.</li>
</ul>

<h2 id="e2e-challenge">E2E Challenge</h2>

<p>Here, we will discuss an example application, which is the <strong>E2E challenge</strong>:</p>

<ul>
  <li><strong>domain</strong>: restaurant recommendation</li>
  <li><strong>task</strong>: we want to generate texts (one or more sentences) to recommend restaurants from an <strong>input</strong> <strong>meaning representation</strong> (MR)</li>
</ul>

<p>An example of meaning representation include</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Example MR</th>
      <th style="text-align: center">Labelled</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406162455714.png" alt="image-20220406162455714" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406162446553.png" alt="image-20220406162446553" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially your output should fulfill the above requirement specified in the MR.</p>

<ul>
  <li>
    <p><strong>dialogue act</strong>: in this competition, <em>request</em> or <em>inform</em> (here we only have two)</p>
  </li>
  <li>
    <p><strong>attributes/values</strong>: unordered attributes of the generated text that you need to fulfill (all those attributes needs to be mentioned) this will be a fixed set, which is called ==domain ontology==:</p>

    <p><img src="NLP_part2/image-20220406163143626.png" alt="image-20220406163143626" style="zoom:33%;" /></p>

    <p>so in this task, there will only be 8 types.</p>
  </li>
</ul>

<p>An example output that fulfills the about MR would be</p>

<p><img src="NLP_part2/image-20220406163005861.png" alt="image-20220406163005861" style="zoom:50%;" /></p>

<h3 id="e2e-datasets">E2E Datasets</h3>

<p>Finally, before we discuss how to construct a model, it is important to know <strong>how training data is collected</strong>:</p>

<ul>
  <li>Crowd sourcing on <strong>CrowdFlower</strong> (similar to Amazon Mechanical Turk)</li>
  <li>essentially it asks you to generate the text given the MR (either in a pure text format or pictorial)</li>
</ul>

<p>An example would be</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Pictorial</th>
      <th style="text-align: center">Text MR</th>
      <th style="text-align: center">Sample Training Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406212209031.png" alt="image-20220406212209031" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406212215241.png" alt="image-20220406212215241" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406212221398.png" alt="image-20220406212221398" /></td>
    </tr>
  </tbody>
</table>

<p>why did we also provided a pictorial representation to the CrowdFlower works?</p>

<ul>
  <li>The aim is to generate good training text from <strong>human workers</strong>.</li>
  <li>it turs out that they generate better texts if a picture is provided.</li>
</ul>

<p>Then, you get <strong>training dataset</strong>:</p>

<ul>
  <li>
    <p>6K MRs with average of 8.27 texts per MR</p>
  </li>
  <li>
    <p>5 slots (attribute-key pairs) per MR</p>
  </li>
</ul>

<h2 id="text-generation-modelling">Text Generation Modelling</h2>

<p>Traditionally, for conditional text generation tasks, you will need to consider</p>

<ul>
  <li><strong>Content selection</strong> (Done for us as MR in the E2E task)</li>
  <li><strong>Aggregation</strong>: which pieces of content go into which sentence? e.g. generate abstract syntax tree from selected content</li>
  <li><strong>Realization</strong>: tree to real sentence</li>
</ul>

<p>Which results in two paradigms:</p>

<ul>
  <li><strong>Two step generation</strong>: first find a sentence planning. Then, given a plan, realize it into natural language.</li>
  <li><strong>Joint one-step approach</strong>: usually use Seq-2-Seq model, where input sequence would be a string of the requirements (e.g. MR)</li>
</ul>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input MR</th>
      <th>Input MR into Tree</th>
      <th style="text-align: center">Realization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406164446805.png" alt="image-20220406164446805" /></td>
      <td><img src="NLP_part2/image-20220406164427972.png" alt="image-20220406164427972" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406164433839.png" alt="image-20220406164433839" /></td>
    </tr>
  </tbody>
</table>

<h3 id="generating-using-seq-2-seq">Generating using Seq-2-Seq</h3>

<p>This is often the architecture used today, where we essentially have:</p>

<ol>
  <li>
    <p>encode the input MR as as string</p>

    <ul>
      <li>
        <p>either make it a sequence of triples <code class="language-plaintext highlighter-rouge">(Dialogue Act, Slot, Value)</code>, so that you could get input like</p>

        <p><img src="NLP_part2/image-20220406213200708.png" alt="image-20220406213200708" style="zoom: 50%;" /></p>
      </li>
      <li>
        <p>or find a syntax tree and then use the string flattened version of the tree</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Tree</th>
              <th style="text-align: center">String Version</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="NLP_part2/image-20220406213258926.png" alt="image-20220406213258926" style="zoom: 50%;" /></td>
              <td style="text-align: center"><img src="NLP_part2/image-20220406213304043.png" alt="image-20220406213304043" style="zoom:50%;" /></td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>input</strong> the above string</p>
  </li>
  <li>
    <p><strong>output</strong> generated text</p>
  </li>
</ol>

<p>Then, the Seq-2-Seq model basically does</p>

<p><img src="NLP_part2/image-20220406164807069.png" alt="image-20220406164807069" style="zoom: 50%;" /></p>

<p>which is <strong>basically a auto-regressive generation</strong>.</p>

<ul>
  <li>
    <p>of course, you could also replace LSTM blocks with transformers</p>
  </li>
  <li>
    <p>the output at each state is essentially $P(y_{t}|y_1,…,y_{t-1},h_x)$ where $y_i$ are the generated ones and $h_x$ is the hidden state representation of the input sequence. Here we are only outputting the <strong>single best word</strong> at a time, hence
\(o_t = \arg\max_y P(y_{t}|y_1,...,y_{t-1},h_x)\)
but remember this is a ==greedy search==. In reality you can use <strong>beam search</strong> to allow for keeping more than one options.</p>
  </li>
</ul>

<h3 id="problems-with-seq-2-seq">Problems with Seq-2-Seq</h3>

<p>But, as we are using NN, there are some bigger problems. We <strong>will not be able to enforce constraints</strong> such as</p>

<ul>
  <li>Missing to fulfill an attribute</li>
  <li>Added an attribute/value that did not exist in MR</li>
  <li>Wrong value for an attribute (hallucination).</li>
</ul>

<p><em>For instance</em>: the red parts are the wrong values not mentioned in the MR</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input MR</th>
      <th style="text-align: center">Seq-2-Seq output with Beam Search of width 3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406214731888.png" alt="image-20220406214731888" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406214756242.png" alt="image-20220406214756242" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>some intuition of why, instead of <em>Burger King</em>, we get those random place names:</p>

<ul>
  <li>
    <p>essentially the network learns some hidden representation of the words in the MR. So suppose we have the following training data</p>

    <p>| <img src="NLP_part2/image-20220406215139282.png" alt="image-20220406215139282" style="zoom: 50%;" /> | <img src="NLP_part2/image-20220406215144295.png" alt="image-20220406215144295" style="zoom: 50%;" /> |
| :———————————————————-: | :———————————————————-: |</p>

    <p>then the model would have learnt a <strong>similar vector representation</strong> between “<em>Burger King</em>” and “<em>Yippee Noodle Bar</em>”</p>
  </li>
  <li>
    <p>then as what the models see are representations of “<em>Burger King</em>”, it is likely that “<em>Yippee Noodle Bar</em>” could be outputted</p>
  </li>
</ul>

<p>How do we deal with those problems?</p>

<ul>
  <li>
    <p><strong>Missing/Added value</strong>: use a <strong>Re-ranker</strong>, which is an additionally penalty using <strong>hamming distance</strong></p>

    <p><img src="NLP_part2/image-20220406165243051.png" alt="image-20220406165243051" style="zoom: 33%;" /></p>

    <p>where the sequence <code class="language-plaintext highlighter-rouge">110100</code> will be the output of an additional classifier (which we hope is doing the correct job)</p>

    <ul>
      <li>it is called reranker as it adds the penalty and reranks the output</li>
      <li>since we need to rank outputs, we would use a <strong>beam search</strong></li>
      <li>of course, this does not solve the problem entirely, but does help a bit</li>
    </ul>
  </li>
  <li>
    <p><strong>Wrong Value</strong>: using data augmentation techniques - <strong>faithful speaker</strong></p>
  </li>
</ul>

<p>Results</p>

<p><img src="NLP_part2/image-20220406170039892.png" alt="image-20220406170039892" style="zoom:33%;" /></p>

<p>where we see:</p>

<ul>
  <li>re-ranked + beam search does better than raw beam search</li>
  <li>raw string input does better than tree as it has more flexibilities.</li>
</ul>

<blockquote>
  <p><strong>However</strong>, on many hard constraint tasks, seq2seq often fail to correctly express a meaning representation hence can be outperformed by hand-engineered solutions. But the key advantage is that it is much less costly in time and money.</p>
</blockquote>

<h3 id="e2e-data-augmentation">E2E Data Augmentation</h3>

<p>Here, we discuss how to use data augmentation to solve the <strong>wrong value problem</strong>. We will discuss two approaches:</p>

<ul>
  <li><strong>delexicalization</strong>: simplest data augmentation without ML</li>
  <li><strong>faithful speaker</strong>: data augmentation using ML. Done by noise injection and self-training.</li>
</ul>

<h4 id="delexicalization">Delexicalization</h4>

<p>Basically we can</p>

<ol>
  <li>identify the lexical from the MR in the generated text</li>
  <li>remove (some of) those lexicals, hence we obtained a <strong>template</strong> from MR -&gt; text</li>
  <li>generate new MR -&gt; text using that template</li>
</ol>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Train MR</th>
      <th style="text-align: center">Train Text</th>
      <th style="text-align: center">Delex Train Text Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406220208189.png" alt="image-20220406220208189" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406220221854.png" alt="image-20220406220221854" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406220237532.png" alt="image-20220406220237532" /></td>
    </tr>
  </tbody>
</table>

<h4 id="noise-injection-and-self-training">Noise Injection and Self-Training</h4>

<blockquote>
  <p>Aim: use data augmentation to create <strong>diverse MR/utterance pairs</strong> not seen in the training distribution</p>

  <ul>
    <li>the delexicalization technique would have created pairs of the same syntax/from same template</li>
  </ul>
</blockquote>

<p>Basically we would need two constraints for this to work:</p>

<ul>
  <li>text of a MR being diverse but is still related to the task (<strong>Base Speaker</strong>)</li>
  <li>the correct MR corresponding to the above text (<strong>MR Parser</strong>)</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Without Noise</th>
      <th style="text-align: center">With Noise = Data Augmentation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220406170914463.png" alt="image-20220406170914463" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220406224952166.png" alt="image-20220406224952166" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Hence we have a <strong>synthetic text and MR</strong> pair, but the pair is ==still faithful==! The overall algorithm looks like</p>

<p><img src="NLP_part2/image-20220406171201842.png" alt="image-20220406171201842" style="zoom: 50%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>to make sure the new pair is not too bad/unclean, we also added a filter.</p>
  </li>
  <li>
    <p>in the end you return $A$ which is the <strong>augmented dataset.</strong></p>
  </li>
</ul>

<blockquote>
  <p>Train new seq2seq model on training + synthetic data: $D \cup A$. The new trained model is the called the <strong>Faithful Speaker</strong></p>
</blockquote>

<hr />

<p><em>For example:</em></p>

<p><img src="NLP_part2/image-20220406171433324.png" alt="image-20220406171433324" style="zoom:50%;" /></p>

<hr />

<p>The advantage of this v.s. the <strong>delexicalization</strong> is that the generated text would have:</p>

<ul>
  <li>variety in <strong>words</strong> used (which delexicalization also can do)</li>
  <li>variety in <strong>sentence structure</strong> (which delexicalization cannot)</li>
</ul>

<h3 id="e2e-metrics-and-baseline">E2E Metrics and Baseline</h3>

<p>Some common metrics used include</p>

<ul>
  <li>
    <p><strong>Precision</strong>: want fulfilled tokens being correct</p>
  </li>
  <li>
    <p><strong>Recall</strong>: want to have fulfilled most of the tokens.</p>
  </li>
  <li>
    <p><strong>Semantic Error Rate</strong> (SER): 
\(\frac{\text{\# missing}+\text{\# incorrect}+\text{\# added}}{\text{\# attributes}}\)
basically the denominator is the sum of cases where the attributed was not fulfilled.</p>
  </li>
</ul>

<p><strong>Baselines</strong> used in this task</p>

<ul>
  <li><strong>SLUG</strong>: ensemble of seq2seq models (e.g. transformer based, LSTM based, etc)
    <ul>
      <li>pooled overgeneration+reranking</li>
    </ul>
  </li>
  <li><strong>DANGNT</strong> (Nguyen and Tran, 2018): rule-based model
    <ul>
      <li>basically from the corpus, extract statistically</li>
    </ul>
  </li>
  <li><strong>TUDA</strong> (Puzikov and Gurevych, 2018): template-based model</li>
</ul>

<p>With their performance being</p>

<p><img src="NLP_part2/image-20220406225848342.png" alt="image-20220406225848342" style="zoom:67%;" /></p>

<p>where we see <strong>Faithful speakers</strong> are performing better.</p>

<h2 id="next-word-sampling">Next Word Sampling</h2>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td><strong>Next Word Sampling</strong> means deciding <strong>how to pick the next word</strong> $w_t$ according to its <em>conditional probability distribution</em> $P(w_t</td>
        <td>w_{1:t-1})$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Here we will see:</p>

<ul>
  <li><strong>Greedy Search</strong>: what we do most of the time, so that at each time $t$ of decoding, we pick the single most probable words. However, the problem is that this ==might not generate the most probable sequence==</li>
  <li><strong>Beam Search</strong>: Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. But still, it does not solve the problem.</li>
  <li><strong>Top-k Sampling</strong>: it is a <em>sampling</em> method (i.e. randomly choose) from the Top-$k$ most likely next word based on redistributing the probability mass only among those $k$ words</li>
  <li><strong>Temperature Sampling</strong>: essentially rescaling the probability by temperature $T$, such that with $T$ is low only high energy (probability) would be “survive”, and when $T$ is high, low energy states gets a greater chance than before to be sampled.</li>
</ul>

<p><em>Recap</em>:</p>

<p><strong>Beam Search</strong>: recall that the basic idea under beam search is that it keeps the $k$ most probable at each time step</p>

<p><img src="NLP_part2/image-20220406230644107.png" alt="image-20220406230644107" style="zoom:67%;" /></p>

<p>which can be done by:</p>

<ul>
  <li>have $k$ decoders ready</li>
  <li>at each time step:
    <ul>
      <li>keep $k$ most probable choice</li>
      <li>generate the next word from the $k$ most probable using the decoders</li>
      <li><strong>prune down</strong> to the $k$ most probable and repeat</li>
    </ul>
  </li>
</ul>

<h3 id="top-k-sampling">Top-k Sampling</h3>

<table>
  <tbody>
    <tr>
      <td><strong>Top-k Sampling</strong>: at each time step we get $P(w_t</td>
      <td>w_{1:t-1})$ from our model:</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>sample from the Top-$k$ most likely next word based on <strong>redistributing the probability mass</strong> only among those $k$ words</p>
  </li>
  <li>
    <p>this is used in GPT-2</p>
  </li>
</ul>

<p>This works because notice that at $t=2$, the top-k choices would have included most of the probability distribution already:</p>

<p><img src="NLP_part2/image-20220406231742503.png" alt="image-20220406231742503" style="zoom:67%;" /></p>

<h3 id="temperature-sampling">Temperature Sampling</h3>

<blockquote>
  <p><strong>Temperature sampling</strong> works by increasing the probability of the most likely words before sampling. The idea is borrowed from thermodynamics where essentially ==energy $\approx$ probability== so that:</p>

  <ul>
    <li>
      <p><em>high temperature</em> means low energy (probability) states are more likely (than the were before) to be encountered</p>
    </li>
    <li>
      <p><em>lowering the temperature</em> allows you to focus on higher energy (probability) state</p>
    </li>
  </ul>
</blockquote>

<p>So that given a probability distribution $P(x)$:</p>

<ul>
  <li>scaled probability by temperature $T$ is $z_i = \log P(x_i)/T$</li>
  <li>then get back probability using softmax $\exp(z_i)/\sum \exp(z_i)$</li>
</ul>

<p><em>For instance</em>, we can take $T=2$ being a low temperature:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$P(x)$</th>
      <th style="text-align: center">$z_i$</th>
      <th style="text-align: center">Temperature Rescaled</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220407001537601.png" alt="image-20220407001537601" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220407001426680.png" alt="image-20220407001426680" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220407001432607.png" alt="image-20220407001432607" /></td>
    </tr>
  </tbody>
</table>

<p>where we see that with low temperature, only high probability terms “survives”. Result-wise:</p>

<ul>
  <li><strong>high temperature</strong> sample displays greater <strong>linguistic variety</strong></li>
  <li><strong>low temperature</strong> sample is more <strong>grammatically correct</strong></li>
</ul>

<h1 id="text-summarization">Text Summarization</h1>

<p>The high level task of summarization would include:</p>

<ul>
  <li><strong>input</strong>: a database, an image, texts, software traces, etc</li>
  <li><strong>output</strong>: a textual summary</li>
</ul>

<blockquote>
  <p><strong>Essentially</strong> those types of generation models can <strong>all be seen as next word generation</strong>, but conditioned on different things:</p>

  <ul>
    <li>
      <p><strong>pure next word generation</strong>:
\(p(y_{i+1}|y_{i-L+1:i})\)
for a window size of $L$ (depending on your machine’s memory for NN models) and $y_{i-L+1:i}$ are the words generated so far. Note that this is basically ==language models==
\(p(w_{i+1}|w_{i-L+1:i})\)
which we have learnt in the beginning of NLP.</p>
    </li>
    <li>
      <p><strong>next word generation conditioned</strong>/based on some input sentence $\vec{x}=x_1,…,x_n$ as well:
\(p(y_{i+1}|\vec{x},y_{i-L+1:i})\)
which you will see is what ==summarization/conditional generation== is essentially about: modelling the above probability with some neural network.</p>
    </li>
  </ul>
</blockquote>

<p>Currently this field does not have the best performances. Why is this task hard?</p>

<ul>
  <li>require <strong>both interpretation and generation</strong> of text</li>
  <li>handle <strong>input</strong> documents from <strong>unrestricted domains</strong> robustly (e.g. trained on one domain but genearlize to news, military, entertainment)</li>
</ul>

<h2 id="types-of-summarization">Types of Summarization</h2>

<p>In general, we can have the following types of summarization tasks:</p>

<ul>
  <li><strong>informative</strong> v.s. <strong>indicative</strong>: generated content can replacing the document v.s. just describes the document</li>
  <li><strong>extractive</strong> v.s. <strong>abstractive</strong>: pick best $k$ words that most important v.s. generate $k$ words</li>
  <li><strong>single</strong> v.s. <strong>multi-document:</strong> many coming form different domains</li>
  <li><strong>generic</strong> v.s. <strong>user-focused</strong>: decides if we should use layman terms (e.g. commonly when in medical/legal domain, use professional terms)</li>
</ul>

<p>Here, we will focus on mainly doing ==informative== summarization and compare:</p>

<ul>
  <li>simple task of <strong>extractive</strong> summary</li>
  <li>using a NN model for <strong>abstractive</strong> summary</li>
</ul>

<h2 id="extractive-summarization">Extractive Summarization</h2>

<p>The formal definition of the task would be as follows.</p>

<ul>
  <li>
    <p><strong>given</strong>: a fixed vocabulary $V$</p>
  </li>
  <li>
    <p><strong>input</strong>: a sequence of $M$ words with $x_1,…,x_M$</p>
  </li>
  <li>
    <p><strong>output</strong>: a sequence of $N &lt; M$ words (e.g. known before generation) $x_{m_i} \in {x_1,…,x_M}$ such that
\(\arg\max_{m \in \{1,...,M\}^N}s(\vec{x},\vec{x}_{m})\)
is largest for $\vec{x}=x_1,…,x_M$ and $\vec{x}<em>m=x</em>{m_1},x_{m_2},…,x_{m_N}$, and $s$ is a <strong>scoring function</strong> yuo can generally customize. The most common one you will see for LM are:
\(s(\vec{x},\vec{x}_m) = \log p(\vec{x}_m | \vec{x})\)</p>
  </li>
</ul>

<p>The common solution for this is to treat it as a <strong>classification task</strong>:</p>

<ul>
  <li>for input word $x_i$, should it be included in output list</li>
  <li>repeat, and output by <strong>concatenating</strong> those words</li>
</ul>

<p>In fact, for humans summary writes, they often do cut and paste</p>

<ul>
  <li>81% of the sentences were constructed by cutting and pasting</li>
</ul>

<h2 id="abstractive-summarization">Abstractive Summarization</h2>

<p>Not only pick $k$ words/phrase, but also <strong>generate</strong> words/phrase that summarizes the content.</p>

<ul>
  <li>if we just concatenate sentences as above, they might not be coherent</li>
</ul>

<p>Formally, such a task can be defined as</p>

<ul>
  <li>
    <p><strong>given</strong>: a fixed vocabulary $V$</p>
  </li>
  <li>
    <p><strong>input</strong>: a sequence of $M$ words with $x_1,…,x_M$</p>
  </li>
  <li>
    <p><strong>output</strong>: a sequence of $N &lt; M$ words (e.g. known before generation) $y_1,…,y_N$ such that
\(\arg\max_{\vec{y}\in \mathcal{Y}}s(\vec{x},\vec{y})\)
basically for $y_i \in V$, and $s$ is a <strong>scoring function</strong> you can generally customize. The most common one you will see for LM are:
\(s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})= \sum_{i=0}^{N-1} \log p(y_{i+1}|\vec{x},y_{1:i})\)
which is again(conditional) next word generation, and is often solved <strong>greedily using auto-regressive decoder</strong>.</p>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>You will soon seen in the <a href="#Attention Based Summarizer">Attention Based Summarizer</a> how this probability $\log p(\vec{y}</td>
      <td>\vec{x})$ can be ==modelled using a neural network==.</td>
    </tr>
  </tbody>
</table>

<h3 id="attention-based-summarizer">Attention Based Summarizer</h3>

<p>Here we discuss essentially a model proposed here: https://arxiv.org/abs/1509.00685, which is used for <strong>headline generation</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Input:</th>
      <th>A detained iranian-american academic accused of acting out against national security has been released from a tehran prison yesterday after a hefty bail was posted, a top judiciary official said Tuesday.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Output:</td>
      <td>Detained iranian-american academic released from prison after hefty bail.</td>
    </tr>
  </tbody>
</table>

<p>Recall that we essentially need to model:
\(s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})\)
Since you will be using attention based model, the hidden representation for <strong>output</strong> will be applied over some attention layer. First, just like how we derived unigram/bigram/etc models:
\(p_\theta(\vec{y}|\vec{x})=p_\theta(y_1,...,y_N|x_1,...,x_N) = \prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{1:i})\)
being exactly true, but since for attention based model we need some <strong>predefined sequence length</strong> $C$ (as opposed to RNN type where you can encode a variable length by just looping it) such that:
\(\prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{1:i}) \approx \prod_{i=1}^{N-1}p_\theta(y_{i+1}|\vec{x},y_{i-C+1:i})\)
and hence for $s(\vec{x},\vec{y}) = \log p(\vec{y} | \vec{x})$ we get:
\(\log p(\vec{y}|\vec{x}) \approx \sum_{i=0}^{N-1} \log p_\theta(y_{i+1}|\vec{x},y_{i-C+1:i})\equiv  \sum_{i=0}^{N-1} \log p_\theta(y_{i+1}|\vec{x},\vec{y}_C)\)
being our model.</p>

<blockquote>
  <p>Then for ==NN based model==, essentially the idea is to do:
\(p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}(W \, \text{repr}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}\)
so that $W \in \mathbb{R}^{|V| \times H}$ for $ \text{repr}(\vec{x},\vec{y}_C)$ being some vector of hidden dimension $H$.</p>
</blockquote>

<p>For this paper, we consider:
\(p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}(V\vec{h} + W\,\text{enc}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}\)
for the first part being empirically useful to have as:</p>

<ul>
  <li>$\tilde{y}<em>C = [Ey</em>{i-C+1},…,Ey_{i}]$ are the <strong>embedded generated outputs</strong></li>
  <li>$h = \tanh(U \tilde{y}_C  )$ producing a single vector summarizing what we have so far</li>
</ul>

<p>So basically we have the <strong>general architecture of</strong> splitting this to an encoder-decoder architecture:</p>

<ul>
  <li>
    <p><strong>encoder</strong>: do the $\text{enc}(\vec{x},\vec{y}_C)$ part and ==get a vector of hidden dimension $H$==</p>
  </li>
  <li>
    <p><strong>decoder</strong>: do the $\text{Softmax}(V\vec{h} + W\,\text{enc}(\vec{x},\vec{y}_C))$ part with ==auto-regressive generation==</p>
  </li>
</ul>

<p><img src="NLP_part2/image-20220412010502499.png" alt="image-20220412010502499" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>for attention based summarizer, the encoder part is essentially an <strong>attention based encoder</strong></li>
  <li>however, of course you can customize what you do at that encoder</li>
</ul>

<h4 id="baseline-bow-encoder">Baseline BoW Encoder</h4>

<p>Consider the simple framework of doing:</p>

<ol>
  <li>
    <p>Encode all the input but <strong>ignoring generated context</strong>:
\(\tilde{x} = [Fx_{1},...,Fx_{M}]\)
for $F \in \mathbb{R}^{H \times |V|}$</p>
  </li>
  <li>
    <p>Then we consider a uniform distribution
\(\vec{p} = [1/M,...,1/M]\)
so that
\(\text{enc}_1(\vec{x},\vec{y}_c) = \vec{p}^T \tilde{x}\)</p>
  </li>
</ol>

<p>so then the entire encoder network only needs $\theta_{enc} = {F}$</p>

<h4 id="convolutional-encoder">Convolutional Encoder</h4>

<p><img src="NLP_part2/image-20220412011723758.png" alt="image-20220412011723758" style="zoom:80%;" /></p>

<h4 id="attention-based-encoder">Attention Based Encoder</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Algorithm</th>
      <th style="text-align: center">Architecture</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220412011753872.png" alt="image-20220412011753872" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220412011804443.png" alt="image-20220412011804443" /></td>
    </tr>
  </tbody>
</table>

<p>which performs better than the others:</p>

<p><img src="NLP_part2/image-20220411170238208.png" alt="image-20220411170238208" style="zoom: 67%;" /></p>

<h2 id="bart">BART</h2>

<p>Now, essentially for <strong>text generation</strong> we want some architecture that does:</p>

<ul>
  <li><strong>encoder</strong>: encode input $\vec{x}$</li>
  <li><strong>decoder</strong>: uses $\text{enc}(\vec{x})$ and some representation of $\vec{y}_C$ to generate:</li>
</ul>

<p>Hence we can use a classical architecture:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Idea</th>
      <th style="text-align: center">BART Pretrain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220412015746372.png" alt="image-20220412015746372" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220412020010963.png" alt="image-20220412020010963" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>which essentially does:
\(p_\theta(y_{i+1}|\vec{x},\vec{y}_C) = \text{Softmax}( \text{repr}(\vec{x},\vec{y}_C)) \in \mathbb{R}^{|V|}\)
where here $\text{repr}(\vec{x},\vec{y}_C)$ would come from weigths that does:</p>

<ul>
  <li>embedding of $\vec{x}, \vec{y}<em>c$ using ==two networks== $f</em>\theta(\vec{x}),g_\phi(\vec{y})$
    <ul>
      <li>in contrast to generation with <strong>GPT</strong>, which only has a single network for encoding $f_\theta(\vec{x}), f_\theta(\vec{y})$</li>
    </ul>
  </li>
  <li>doing multi-head attentions</li>
  <li>doing FFNN and others.</li>
</ul>

<blockquote>
  <p>But essentially we <strong>comes down to having some good encoding network</strong>. Therefore BART consider ==pre-train tasks== of corrupting documents and then optimizing a <strong>reconstruction</strong> loss:</p>

  <p><img src="NLP_part2/image-20220412020232001.png" alt="image-20220412020232001" style="zoom:50%;" /></p>

  <p>and notice that In the extreme case, where all information about the source is lost, BART is equivalent to a language model.</p>
</blockquote>

<p>Then, once trained, we <strong>fine-tune on a range of tasks</strong></p>

<ul>
  <li><strong>Sequence Classification</strong>: the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier.
    <ul>
      <li>basically related to the <code class="language-plaintext highlighter-rouge">CLS  </code>token in BERT</li>
    </ul>
  </li>
  <li><strong>(Conditional) Sequence Generation</strong>: the encoder input is the input sequence (to be conditioned on), and the decoder generates outputs autoregressively</li>
</ul>

<h3 id="comparison-against-gpt-and-bert">Comparison Against GPT and BERT</h3>

<p>What is special about using both encoder-decoder for the task of generation? Recall that:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">BERT</th>
      <th style="text-align: center">GPT</th>
      <th style="text-align: center">BART</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220412021011283.png" alt="image-20220412021011283" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220412021017237.png" alt="image-20220412021017237" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220412021023236.png" alt="image-20220412021023236" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>so that:</p>

<ul>
  <li><strong>BERT</strong>: Random tokens are replaced with masks, and the document is encoded bidirectionally. Missing tokens are predicted independently, so <strong>BERT cannot easily be used for generation</strong></li>
  <li><strong>GPT</strong>: Tokens are predicted auto-regressively, meaning GPT can be used for generation. However words can only <strong>condition on leftward context</strong>, so it cannot learn bidirectional interactions.
    <ul>
      <li>also it uses the same encoding network at all times</li>
    </ul>
  </li>
  <li><strong>BART</strong>: Inputs to the encoder need not be aligned with decoder outputs, ==allowing arbitrary noise transformations==, i.e. can generate “new words” for tasks such as summarization.
    <ul>
      <li>then the <em>likelihood of the original document</em> (right) is calculated with an autoregressive decoder</li>
      <li>notice that the <strong>order changed</strong> as this is allowed for encoder-decoder type model! We are using ==two networks== $f_\theta(\vec{x}),g_\phi(\vec{y})$ for encoding the input and the output generated differently, as their interaction could be complex</li>
    </ul>
  </li>
</ul>

<h1 id="chatbots--dialogue-systems">Chatbots &amp; Dialogue Systems</h1>

<p>This chapter introduces the <strong>fundamental</strong> algorithms of dialogue systems, or conversational agents. These programs communicate with users in natural language (text, speech, or both), and fall into two classes:</p>

<ul>
  <li>
    <p><strong>Task-oriented dialogue agents</strong> use conversation with users to help complete tasks. Examples include Siri, Alexa, Google Now/Home, Cortana, etc., which can give directions, control appliances, find restaurants, or make calls.</p>

    <ul>
      <li>
        <p>for those products, we also needed Automatic Speech Recognition and Text to Speech with an architetcure</p>

        <p><img src="NLP_part2/image-20220413161715955.png" alt="image-20220413161715955" style="zoom:33%;" /></p>

        <p>where basically dialogue manager decides what actions to take</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>chatbots</strong> are systems designed for extended conversations, set up to mimic the unstructured conversations or ‘chats’ characteristic of human-human interaction</p>

    <ul>
      <li>mainly for entertainment</li>
      <li>but also for practical purposes like making task-oriented agents more <strong>natural</strong>.</li>
    </ul>
  </li>
</ul>

<p>we’ll discuss the :</p>

<ul>
  <li>three major <strong>chatbot</strong> architectures: rule-based systems, information retrieval systems, and encoder-decoder generators.</li>
  <li>for <strong>task-oriented agents</strong>, we introduce the frame-based architecture (the GUS architecture) that underlies most task-based systems</li>
</ul>

<h2 id="properties-of-human-conversation">Properties of Human Conversation</h2>

<p>Before we attempt to design a conversational agent to converse with humans, it is crucial to understand something about how humans converse with each other. For example:</p>

<p><img src="NLP_part2/image-20220413234847649.png" alt="image-20220413234847649" style="zoom: 67%;" /></p>

<p>where <code class="language-plaintext highlighter-rouge">C </code>stands for client and <code class="language-plaintext highlighter-rouge">A </code>stands for Agent</p>

<ul>
  <li>
    <p>A dialogue is a sequence of <strong>turns</strong> (C1, A2, C3, and so on)</p>

    <ul>
      <li>
        <p>A system has to know when to <em>stop talking</em>; the client <em>interrupts</em> (in A16 and C17), so the system must know to stop talking</p>
      </li>
      <li>
        <p>A system also has to know when to <em>start talking</em>. e.g. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond.</p>

        <p>This task is called endpointing or <strong>endpoint detection</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Speech/Dialog Acts</strong>: each utterance in a dialogue is a kind of action being performed by the speaker. Basically expresses an important component of the ==intention of the speaker== (or writer) in saying what they said</p>
  </li>
  <li>
    <p><strong>Grounding</strong>: acknowledging that the hearer has understood the speaker (e.g. saying “OK”, as the agent does in A8 or A10.)</p>
  </li>
  <li>
    <p><strong>Sub-dialogues</strong> and <strong>Dialogue Structure</strong>: QUESTIONS and ANSQWE, PROPOSALS and ACCEPTANCE/REJECTION, etc. These pairs, called <strong>adjacency pairs</strong> are composed of a first pair part and a second pair part</p>

    <ul>
      <li>However, dialogue acts aren’t always followed immediately by their second pair <strong>side sequence</strong> part. The two parts can be separated by a side sequence</li>
      <li>e.g. utterances C17 to A20 constitute a correction subdialogue; another common one would be <strong>clarification questions</strong></li>
    </ul>
  </li>
  <li>
    <p><strong>Initiative</strong>: whoever is taking the lead in the conversation is taking initiative. In normal human-human dialogue, however, it’s more common for initiative to shift back and forth between the participants</p>

    <ul>
      <li><strong>Mixed initiative</strong>, when initiative shifts between the two people, is very difficult for dialogue systems to achieve.</li>
      <li>In contrast, the QA system would be a <strong>user-initiative</strong> system: user specifies a query, and the systems passively responds.</li>
      <li>It’s much easier to design dialogue systems to be <em>passive responders</em>.</li>
    </ul>
  </li>
  <li>
    <p><strong>Inference</strong> and <strong>Implicature</strong>: consider</p>

    <p><img src="NLP_part2/image-20220413235809097.png" alt="image-20220413235809097" style="zoom:67%;" /></p>

    <p>Notice that the client does not in fact explicitly answer the agent’s question. We need to inference the answer.</p>
  </li>
</ul>

<h2 id="chatbots">Chatbots</h2>

<blockquote>
  <p>The simplest kinds of dialogue systems are <strong>chatbots</strong> systems that can carry on extended conversations with the goal of <strong>mimicking the unstructured conversations</strong> or characteristic of informal human-human interaction</p>
</blockquote>

<p>A recent example would be Facebook’s BlenderBot:</p>

<p><img src="NLP_part2/image-20220414000102746.png" alt="image-20220414000102746" style="zoom:67%;" /></p>

<p>Like practically everything else in language processing, chatbot architectures fall into two classes: <strong>rule-based systems</strong> and <strong>corpus-based systems</strong></p>

<ul>
  <li><strong>Rule-based systems</strong> include the early influential ELIZA and PARRY systems</li>
  <li><strong>Corpus-based systems</strong> are more modern:
    <ol>
      <li>mine large datasets of human-human conversations,</li>
      <li>generate responses by either
        <ul>
          <li>using information retrieval to copy a human response from a previous conversation</li>
          <li>using an encoder-decoder system to generate a response from a user utterance</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="corpus-based-chatbots">Corpus-based Chatbots</h3>

<p><strong>Corpus-based chatbots</strong>, instead of using hand-built rules, mine conversations of human-human conversations. These systems are enormously <strong>data-intensive</strong>, requiring hundreds of millions or even billions of words for training.</p>

<p>Most corpus based chatbots <strong>produce their responses</strong> to a user’s turn in context either by</p>

<ul>
  <li><strong>retrieval</strong> methods: using information retrieval to grab a response from some corpus that is appropriate given the dialogue context</li>
  <li><strong>generation</strong> methods: using a language model or <em>encoder-decoder</em> to generate the response given the dialogue context</li>
</ul>

<p>Essentially the two architectures look like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Retrieval Based</th>
      <th style="text-align: center">Generation Based</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220414001622581.png" alt="image-20220414001622581" style="zoom:80%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220414001637909.png" alt="image-20220414001637909" style="zoom:80%;" /></td>
    </tr>
  </tbody>
</table>

<p>So that given a training corpus $C$ which contains many conversations, and given a user’s query being $q$, we consider:</p>

<hr />

<p><strong>Retrieval based</strong>: return some appropriate turn $r$ as response by:
\(r = \arg\max_{r \in C} \frac{q \cdot r}{|q||r|}\)
where $q,r$ could be vectors computed by:</p>

<ul>
  <li>TF-IDF encoding</li>
  <li>train an <strong>encoder</strong> for query and one for response (left figure in the above)</li>
</ul>

<p>For instance, we could train two encoders by fine-tuning BERT so that we get $\text{BERT}_Q,\text{BERT}_R$:
\(h_q = \text{BERT}_Q(q)[CLS]\\
h_r = \text{BERT}_R(r)[CLS]\)
where recall that the <code class="language-plaintext highlighter-rouge">CLS</code> token is used to encode the entire sentence. Then:
\(r = \arg\max_{r \in C} h_q \cdot h_r\)</p>

<hr />

<p><strong>Generation Based</strong>: think of response production as an encoder-decoder task — ==transducing from the user’s prior turn to the system’s turn==. So basically can treat it as a kind of translation.</p>

<p><img src="NLP_part2/image-20220414002318125.png" alt="image-20220414002318125" style="zoom:50%;" /></p>

<p>Recall that for encoder-decoder models, it is auto-regressively generating but <strong>conditioned on the entire query $q$</strong>:
\(\hat{r}_{t+1} = \arg\max_{w \ni V} P(w|q,r_1,...,r_{t})\)
but sometimes instead of just user’s query, we can encode the <strong>entire conversation</strong> to be conditioned on.</p>

<hr />

<p>Other ideas for generation:</p>

<ul>
  <li>fine-tune a large language model/<strong>decoder only</strong> on a conversational dataset and use the language model directly as a response generator.</li>
  <li>https://arxiv.org/pdf/1606.01541.pdf using Deep Reinforcement Learning</li>
  <li>https://aclanthology.org/D17-1230.pdf using Adversarial Learning</li>
</ul>

<h2 id="gus-simple-frame-based-dialogue-systems">GUS: Simple Frame-based Dialogue Systems</h2>

<blockquote>
  <p>We turn now to <strong>task-based dialogue</strong>, in which a dialogue system has the goal of helping a user solve some task like making an airplane reservation</p>
</blockquote>

<p>First some terminologies:</p>

<ul>
  <li>A <strong>frame</strong> is a kind of knowledge structure representing the kinds of intentions the system can extract from user</li>
  <li>A frame consists of a collection of <strong>slots</strong>, each of which can take a set of possible <strong>values</strong>.
    <ul>
      <li>basically specifies what the system ==needs to know==</li>
    </ul>
  </li>
  <li>A set of frames is sometimes called a <strong>domain ontology</strong>.</li>
</ul>

<p>For example, in the travel domain:</p>

<p><img src="NLP_part2/image-20220414003439552.png" alt="image-20220414003439552" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>a lot might be of type <em>city</em>, <em>date</em>, <em>time</em></p>
  </li>
  <li>
    <p>this would be an <strong>example of a frame</strong></p>
  </li>
  <li>
    <p>technically, many <code class="language-plaintext highlighter-rouge">Types</code> in such a system is itself a frame. For instance</p>

    <p><img src="NLP_part2/image-20220414003637003.png" alt="image-20220414003637003" style="zoom:67%;" /></p>
  </li>
</ul>

<h3 id="control-structure-for-frame-based-dialogue">Control structure for frame-based dialogue</h3>

<blockquote>
  <p>The system’s goal is to <strong>fill the slots</strong> in the frame with the fillers the user intends, and <strong>then perform the relevant action</strong> for the user (answering a question, or booking a flight).</p>
</blockquote>

<p>To do this, the system need to do:</p>

<ol>
  <li><strong>asks questions</strong> of the user (using pre-specified question templates associated with each slot of each frame)
    <ul>
      <li>If a user’s response fills multiple slots, the system fills all the relevant slots, and then continues asking questions to fill the remaining slots</li>
      <li>there are also <strong>condition-action rules</strong> attached to slots. For example: a rule attached to the <code class="language-plaintext highlighter-rouge">DESTINATION </code>slot for the plane booking frame, once the user has specified the destination, might automatically enter that city as the default <code class="language-plaintext highlighter-rouge">StayLocation </code>for the related hotel booking frame.</li>
    </ul>
  </li>
  <li>Many domains <strong>require multiple frames</strong>. Then this control system needs to disambiguate <strong>which slot</strong> of which frame a given input is supposed to fill</li>
  <li>Once the system has enough information it <strong>performs the necessary action</strong> (like querying a database of flights) and returns the result to the user</li>
</ol>

<p>Exactly how do we do step 1 and 2? This is in the section covered below.</p>

<h3 id="determining-domain-intent-and-slot-fillers-in-gus">Determining Domain, Intent, and Slot fillers in GUS</h3>

<p>How do we implement the above control system? We need to essentially do three things:</p>

<ul>
  <li>The first task is <strong>domain classification</strong>: is this user for example talking about airlines?</li>
  <li><strong>user intent determination</strong>: what general task or goal is the user trying to accomplish?</li>
  <li>Finally, we need to do <strong>slot filling</strong>: extract the particular slots and fillers that the user intends the system to understand</li>
</ul>

<blockquote>
  <p>The <strong>slot-filling</strong> method used in the original GUS system, and still quite common in industrial applications, is to use <strong>handwritten rules</strong></p>
</blockquote>

<h3 id="generation-in-gus">Generation in GUS</h3>

<p>Finally, once all information are collected and necessary actions are performed by the system, we <strong>perform language generation</strong> to respond to user.</p>

<ul>
  <li>Frame-based systems tend to use <strong>template-based generation</strong></li>
  <li>or <strong>encoder-decoder network</strong> with dependency on the dialogue state (i.e. the question user just asked)</li>
</ul>

<h2 id="the-dialogue-state-architecture">The Dialogue-State Architecture</h2>

<blockquote>
  <p>Modern research systems for task-based dialogue are based on a more sophisticated version of the frame-based architecture called the <strong>dialogue-state</strong> or or <strong>belief-state</strong> architecture.</p>
</blockquote>

<p>Moderm architecture looks like the following:</p>

<p><img src="NLP_part2/image-20220414011150489.png" alt="image-20220414011150489" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>for audio input/output, we had this extra two modules on the left</li>
  <li>the other four components are part of both spoken and textual dialogue systems
    <ul>
      <li>For example, like the GUS systems, the dialogue-state architecture has a component (<strong>Natural Language Understanding Module</strong>) for extracting slot fillers from the user’s utterance, but generally using machine learning rather than rules.</li>
      <li>The <strong>dialogue state tracker</strong> maintains the current state of the dialogue (which include the user’s most recent dialogue act, plus the entire set of slot-filler constraints the user has expressed so far).</li>
      <li><strong>dialogue policy</strong> decides what the system should do or say next: e.g. wen to ask clarification question, when to make a suggestion, etc.</li>
      <li><strong>natural language generation</strong>: condition on the exact context to produce turns that seem much more natural</li>
    </ul>
  </li>
</ul>

<h3 id="dialogue-acts">Dialogue Acts</h3>

<p>Different types of dialogue systems <strong>require labeling different kinds of acts</strong>, and so the tagset—defining what a dialogue act is exactly— tends to be designed for particular tasks. For example</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dialogue Act</th>
      <th style="text-align: center">Labelled Conversation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220414020147489.png" alt="image-20220414020147489" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220414020239096.png" alt="image-20220414020239096" style="zoom:80%;" /></td>
    </tr>
  </tbody>
</table>

<p>note that the above example also showed the content of each dialogue acts, which are the <strong>slot fillers</strong> being communicated</p>

<h3 id="slot-filling">Slot Filling</h3>

<blockquote>
  <p>Here, we discuss how the task of slot-filling, and the simpler tasks of domain and intent classification, are typically done. This is still part of the job of the <strong>Natural Language Understanding Module.</strong></p>
</blockquote>

<p>We consider the following situation:</p>

<ul>
  <li>
    <p><strong>input</strong>: <em>“I want to fly to San Francisco on Monday afternoon please”</em></p>
  </li>
  <li>
    <p><strong>output</strong>: domain is AIRLINE, intent is SHOWFLIGHT, and the fillers being</p>

    <p><img src="NLP_part2/image-20220414020743955.png" alt="image-20220414020743955" style="zoom:67%;" /></p>
  </li>
</ul>

<p>A simple architecture that does is is simply:</p>

<p><img src="NLP_part2/image-20220414020838190.png" alt="image-20220414020838190" style="zoom:67%;" /></p>

<p>which is basically a <strong>Sequence classification</strong> model such that:</p>

<ul>
  <li>we use BERT to embed each input</li>
  <li>then pass though FFNN with a softmax so that:
    <ul>
      <li>for each word, decide which tag it is</li>
      <li>for the last tag <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>, use SoftMax to classify which intent and domain it is</li>
    </ul>
  </li>
</ul>

<p>Then, once the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., “San Francisco”), and these word strings can then be normalized to the correct form in the ontology</p>

<h3 id="dialogue-state-tracking">Dialogue State Tracking</h3>

<blockquote>
  <p>The job of the dialogue-state tracker is to determine both the <strong>current state of the frame</strong> (the fillers of each slot), as well as the most <strong>recent dialogue act</strong>.</p>
</blockquote>

<p>Since dialogue acts place some constraints on the slots and values, the tasks of <strong>dialogue-act detection</strong> and <strong>slot-filling</strong> are often performed jointly.</p>

<p><img src="NLP_part2/image-20220414021745317.png" alt="image-20220414021745317" style="zoom:67%;" /></p>

<p>The simplest dialogue state tracker might just take the output of a slot-filling sequence-model. But more complicated cases include:</p>

<ul>
  <li>detecting correction acts</li>
  <li>value being changed in the current sentence or should be carried over from previous sentences</li>
</ul>

<p>Essentially it would be a <strong>condensed information</strong> that related to the entire conversation so far. This will be used in the next module to decide what policy to take.</p>

<h3 id="dialogue-policy">Dialogue Policy</h3>

<blockquote>
  <p>The goal of the <strong>dialogue policy</strong> is to decide <strong>what action the system should take next</strong>, that is, what dialogue act to generate.</p>
</blockquote>

<p>Formally, at turn $i+1$ we want to predict <strong>what action $a_{i+1}$ to take</strong>, which would be based on the entire sequence of dialogue:
\(\hat{a}_{i+1} = \arg\max_{a_{i+1} \in A}P(a_{i+1}|s_i)\)
for $s_i$ the state being encoded with representation of:</p>

<ul>
  <li>current state of Frame $i$</li>
  <li>previous user and system dialogue actions</li>
  <li>etc.</li>
</ul>

<p>Learning a <strong>best policy</strong> would be done by ==reinforcement learning== system gets a <strong>reward at the end of the dialogue</strong>, and uses that reward to train a policy to take actions.</p>

<p>Additionally, this module might need to take on task of choosing some attributes (slots and values) which plans on what to say to the user, before feeding into the NLG module.</p>

<h3 id="natural-language-generation-module">Natural Language Generation Module</h3>

<p>Once a dialogue act has been decided, we need to generate the text of the response to the user.</p>

<blockquote>
  <p>The task of natural language generation (NLG) in the information-state content architecture is often modeled in two stages:</p>

  <ul>
    <li><strong>content planning</strong> (what to say)</li>
    <li><strong>sentence sentence realization</strong> (how to say it).</li>
  </ul>
</blockquote>

<p>Here we assume that the previous module has chosen the dialogue act to generate, and chosen some attributes (slots and values) that the planner wants to say to the user.</p>

<p>Example input/output looks like</p>

<p><img src="NLP_part2/image-20220414023112417.png" alt="image-20220414023112417" style="zoom: 50%;" /></p>

<p>where In the first example:</p>

<ul>
  <li>the content planner has chosen the dialogue act <code class="language-plaintext highlighter-rouge">RECOMMEND </code>and some particular slots <code class="language-plaintext highlighter-rouge">(name, neighborhood, cuisine)</code> and their fillers</li>
  <li>The goal of the sentence realizer is to generate a sentence like lines 1 or 2 shown in the figure (here we have two samples as we can sample the generation module again and get a different output)</li>
</ul>

<p>However, the problem here is <strong>training data</strong>: it is hard ot get a good combination of every possible act/slots:</p>

<ul>
  <li>Therefore it is common in sentence realization to increase the generality of the training examples by <strong>delexicalization</strong> (basically remove the formal nouns, so we get a template)</li>
  <li>or any other <strong>data augmentation</strong> tricks such as <a href="#Noise Injection and Self-Training">Noise Injection and Self-Training</a></li>
</ul>

<h2 id="persuasive-system">Persuasive System</h2>

<p>This is a more advanced system, which is technically a task-oriented system but with the aim of <strong>persuading people to do something</strong>, So, to achieve that goal, certainly some social aspects of the conversation is needed.</p>

<p><img src="NLP_part2/image-20220418162151114.png" alt="image-20220418162151114" style="zoom: 33%;" /></p>

<p>Some potential uses of this system include:</p>

<ul>
  <li>exercise persuation</li>
  <li>suisicde counseling (train counseller by acting as a visitor)</li>
  <li>social distancing</li>
</ul>

<p>Again, the aim is to <strong>change people’s belief and behavior</strong> (might have ethnical issues). How do we do that using machine learning?</p>

<h3 id="challenges-in-persuasion">Challenges in Persuasion</h3>

<p>The basics of data-driven methods for persuasion includes the following three modules:</p>

<p><img src="NLP_part2/image-20220418162626800.png" alt="image-20220418162626800" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>like all tasks, it is important to collect some good data</li>
  <li>then, we want our chatbot to be able to <strong>understand the semantics</strong> of user’s utterance (i.e. what they said)</li>
  <li>finally, it moves on to persuade by picking some <strong>strategy</strong> (and then generate text response)</li>
</ul>

<hr />

<p><strong>Data Challenges</strong></p>

<blockquote>
  <p>To start a good project/model, first we need to <strong>collect good data</strong>.</p>
</blockquote>

<p>Like many ML tasks, collecting good data is always important. But that is not easy because even chaning a single word in a sentence could have lead to an entire new utterance. Therefore, the set of <strong>conversation path</strong> with each node being a sentence is exponential</p>

<p><img src="NLP_part2/image-20220418201834432.png" alt="image-20220418201834432" style="zoom:33%;" /></p>

<p>so ideally we want our training data to cover <em>all possible dialog trajectories</em>, but that is intractable. Hence some approaches include</p>

<ul>
  <li>learning a subset of the tree and hope for the best</li>
  <li>try to merge different branches to make the tree smaller</li>
  <li>etc.</li>
</ul>

<p>Additionally, as our aim is an <strong>optimization</strong> task, there could be different optimal path for a different user to be persuaded</p>

<p><img src="NLP_part2/image-20220418202109304.png" alt="image-20220418202109304" style="zoom: 33%;" /></p>

<p>so that given some user information/background, the system needs to pick an optimal path down the tree <strong>for that particular user</strong></p>

<hr />

<p><strong>Semantic Challenges</strong></p>

<p>Many large models are able to make language generation fluent, but they often ==do not understand deep level semantics== of the language (e.g. Microsoft’s Tay AI), which can lead to unsafe systems.</p>

<p><img src="NLP_part2/image-20220418202331875.png" alt="image-20220418202331875" style="zoom: 33%;" /></p>

<p>one potential task on this direction would be: how do we build a system that can “filter out” those dangerous generated languages?</p>

<hr />

<p><strong>Strategy Challenges</strong></p>

<p>In certain scenario, some persuasive strategy would be more effective than another.</p>

<p><img src="NLP_part2/image-20220418202610174.png" alt="image-20220418202610174" style="zoom: 33%;" /></p>

<p>for instance, hearts and cakes could be more persuasive than pizzas in this scenario.</p>

<h3 id="dialog-data-collection">Dialog Data Collection</h3>

<p>How do we get dialog data?</p>

<ul>
  <li>production dialog (i.e. companies with their customers) are not available due to privacy issues.
    <ul>
      <li>a good source would be customer services, but again there are privacy issues</li>
    </ul>
  </li>
  <li>role-play persuasion would be ineffective if there is <strong>lack of incentives</strong>. So persuasive quality might not be good.</li>
</ul>

<p>Then people decided to <strong>use monetary incentive</strong> (i.e. using CrowdFlower/Mechanical Turks), with the persuasion task of donation:</p>

<p><img src="NLP_part2/image-20220418203147938.png" alt="image-20220418203147938" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>persuaders receive bonus when persuadees donate</p>
  </li>
  <li>
    <p>persuadees donation sent to the charity</p>
  </li>
  <li>
    <p>also collect big-five personality of the persuadee (for personalization system)</p>
  </li>
</ul>

<p>Hence the overall work flow looks like:</p>

<p><img src="NLP_part2/image-20220418203304657.png" alt="image-20220418203304657" style="zoom: 50%;" /></p>

<hr />

<p>Some examples include: (Persuasion for Good dataset)</p>

<p><img src="NLP_part2/image-20220418203511141.png" alt="image-20220418203511141" style="zoom: 33%;" /></p>

<p>where the person on the left will be the crowd-funded persuader.</p>

<h4 id="hierarchical-intent-annotation">Hierarchical Intent Annotation</h4>

<blockquote>
  <p>We also want to build a system that understand some <strong>semantics</strong> of the sentences/dialogs.</p>

  <ul>
    <li>so not just fluency, but also semantics</li>
  </ul>
</blockquote>

<p>The idea is to annotate each sentence an <strong>intent/dialog act</strong>, which can be hierarchical</p>

<p><img src="NLP_part2/image-20220418163705297.png" alt="image-20220418163705297" style="zoom:50%;" /></p>

<p>where MIDAS were used for social chatbot annotation.</p>

<p>An example of <strong>annotated</strong> piece of dialog would be</p>

<p><img src="NLP_part2/image-20220418163827199.png" alt="image-20220418163827199" style="zoom: 50%;" /></p>

<h3 id="classic-modular-dialog-system">Classic Modular Dialog System</h3>

<p>Recall that we had the framework for dialog system, usually it looks like</p>

<p><img src="NLP_part2/image-20220418163940380.png" alt="image-20220418163940380" style="zoom: 50%;" /></p>

<p>where in particular:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Module Name</th>
      <th style="text-align: center">Input/Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">NLU</td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418204125487.png" alt="image-20220418204125487" style="zoom: 33%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">DST</td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418204220033.png" alt="image-20220418204220033" style="zoom: 33%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">DPL</td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418204457199.png" alt="image-20220418204457199" style="zoom: 33%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">NLG</td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418204555107.png" alt="image-20220418204555107" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>NLU</strong>: will be to understand the user’s utterances (i.e. semantic intent of the sentence)
    <ul>
      <li>usually used for <strong>intent detection</strong>, then any Seq-classification model would work (as we have a limited intent)</li>
      <li>usually it is used per sentence based, which is why we needed some kind of accumulator later which is the DST</li>
    </ul>
  </li>
  <li><strong>DST</strong>: input dialog history, output <strong>slot and values</strong>
    <ul>
      <li>the aim is to track dialog state: sequence of <strong>information</strong> that is <strong>critical to remember</strong></li>
      <li>e.g. for restaurant domain: how much food you want, location of restaurant</li>
      <li>e.g. for donation persuasion: how much donation we got for far, etc.</li>
    </ul>
  </li>
  <li><strong>DPL</strong>: given the current state and user’s input, <strong>what policy/action should we take</strong> now?
    <ul>
      <li>e.g. maybe we should provide some facts now</li>
    </ul>
  </li>
  <li><strong>NLG</strong>: given the action we want to take, generate the output languages</li>
</ul>

<p>Now with a modular network, we can train it:</p>

<ul>
  <li>each module separately</li>
  <li>jointly/end-to-end</li>
  <li>some other way</li>
</ul>

<h4 id="modular-training-drawbacks">Modular Training Drawbacks</h4>

<p>The problem with this kind of training is that:</p>

<ul>
  <li>heavy expert involvement, to tune each module</li>
  <li>hard to tune individual modules, hence usualy need end-to-end trainng</li>
  <li>heavy manual annotation needed for training data
    <ul>
      <li>because each module needs different input/output</li>
    </ul>
  </li>
</ul>

<h4 id="end-to-end-drawbacks">End-to-End Drawbacks</h4>

<p>Avoiding the training/data annotation problem for modular training, we can train end-to-end:</p>

<ul>
  <li>which has been done on social chatbots a lot, such as GPT-2</li>
</ul>

<p><img src="NLP_part2/image-20220418164610593.png" alt="image-20220418164610593" style="zoom:50%;" /></p>

<p>However, here we also have problems:</p>

<ul>
  <li>difficult for error analysis: you <strong>don’t know which module went wrong</strong>! (DST error? NLG error?)</li>
  <li>since they don’t have semantic controls, we nede a large amount of data to train</li>
</ul>

<h4 id="moss-a-combination">MOSS: A combination</h4>

<p>Here we combine the best of both types:</p>

<ul>
  <li>use one encoder (shared), but four decoders (separated)</li>
</ul>

<p>The aim is to build a system that is <strong>end-to-end trainable</strong> but also <strong>trainable in a modular fashion</strong> (without going out of sync):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Architecture</th>
      <th style="text-align: center">Pipeline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220418205320854.png" alt="image-20220418205320854" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418205627564.png" alt="image-20220418205627564" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>for end-to-end training, the shared encoder weights will only be updated once</li>
  <li>
    <p>for modular training, shared encoder will <strong>also be updated</strong> so its “knowledge” can still be used in other modules (in sync)</p>
  </li>
  <li>as you see in the pipeline, essentially each module uses output of the shared encoder but <strong>conditions/uses</strong> on the output of the previous module as well</li>
</ul>

<blockquote>
  <p>One key advantage of this architecture is that we can train it even if we only have <strong>partial labels</strong> (e.g. only have 30% fully labelled, or only 40% labelled with intent)</p>
</blockquote>

<p>For instance:</p>

<p>Fine-tuning on donation persuasion: we only have 300 labeled but 700 unlabeled data. We can</p>

<ol>
  <li>
    <p>first do <em>end-to-end training</em>: train all 1000 unlabeled on NLG, which does not need intent labeling</p>

    <p><img src="NLP_part2/image-20220418210339653.png" alt="image-20220418210339653" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>then do <em>modular training</em>: we can tune all again with <strong>300 labelled</strong> to update the other three</p>

    <p><img src="NLP_part2/image-20220418210356822.png" alt="image-20220418210356822" style="zoom:50%;" /></p>
  </li>
</ol>

<p>One take-away here is that it kind of depends how much data you have:</p>

<p><img src="NLP_part2/image-20220418210805942.png" alt="image-20220418210805942" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>if you have a lot of training dialogs then you might not even need to label them and it could work</li>
  <li>if you don’t have much training dialogs, then having high quality labeled ones are needed</li>
</ul>

<hr />

<p>Other advantage of this kind of architecture is that:</p>

<ul>
  <li>easy to perform error analysis (we can decode the output for each module for a given input)</li>
  <li>can handle complex task</li>
</ul>

<h3 id="human-evaluation-on-persuasion">Human Evaluation on Persuasion</h3>

<p>To analyze the performance of the model simply:</p>

<ul>
  <li>compare against human persuaders: how many percent of persuadee did it succeed to persuade?</li>
  <li>build a baseline: take GPT-2 and just tune end-to-end on all 1000 data</li>
</ul>

<p><img src="NLP_part2/image-20220418165357936.png" alt="image-20220418165357936" style="zoom:50%;" /></p>

<p>Why can human do so well? Where ==error analysis becomes important==!</p>

<h4 id="error-analysis-unfaithful-outputs">Error Analysis: Unfaithful Outputs</h4>

<p>Checking on some outputs, we see that sometimes chatbot lies when It has never seen an utterance.</p>

<ul>
  <li>it <strong>failed to understand the semantics</strong>, and generated something related some the training data without understanding it</li>
</ul>

<p><img src="NLP_part2/image-20220418165535004.png" alt="image-20220418165535004" style="zoom:50%;" /></p>

<p>This can be dangerous in <strong>deployment</strong>. Therefore one approach is to have a ==safety net== that can <strong>rank</strong> which responses is better</p>

<ol>
  <li>
    <p>use beam search, nucleus sampling, temperature sampling, to get $N$ candidates</p>

    <p><img src="NLP_part2/image-20220418165726392.png" alt="image-20220418165726392" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>gather training data by having <strong>humans</strong> ranking it</p>

    <p><img src="NLP_part2/image-20220418165846745.png" alt="image-20220418165846745" style="zoom:50%;" /></p>

    <p>given some sampled $N$ samples, with the intent and semantic slot, rank those responses by a human in the loop</p>
  </li>
  <li>
    <p>once we have samples of human ranking, ==train a classifier== to rank it</p>

    <p><img src="NLP_part2/image-20220418211910164.png" alt="image-20220418211910164" style="zoom: 33%;" /></p>
  </li>
</ol>

<p>Then, after this fix, we got a 10% boost from the MOSS architecture</p>

<p><img src="NLP_part2/image-20220418170003541.png" alt="image-20220418170003541" style="zoom:50%;" /></p>

<p>but still a bit less than human performance.</p>

<h4 id="error-analysis-persuasion-strategies">Error Analysis: Persuasion Strategies</h4>

<p>Consider the following example</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Failed Cases</th>
      <th style="text-align: center">Success Cases</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220418170147913.png" alt="image-20220418170147913" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220418212225807.png" alt="image-20220418212225807" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where we see that there is a wide variety of actions to take than <strong>credibility appeals</strong> when persuading people.</p>

<ul>
  <li>Therefore, we also need <strong>persuasion strategies</strong> (i.e. <em>how</em> to persuade), ==in addition== of the dialog act of <code class="language-plaintext highlighter-rouge">propose-donation</code> (by the DPL module, on <em>what</em> to do next)</li>
</ul>

<p><img src="NLP_part2/image-20220418170330288.png" alt="image-20220418170330288" style="zoom: 50%;" /></p>

<ul>
  <li>
    <p>so we added this module of <strong>DSP: dialog strategy planning</strong>, into the pipeline</p>

    <p><img src="NLP_part2/image-20220418170541413.png" alt="image-20220418170541413" style="zoom:50%;" /></p>

    <p>note that of course this module needs past trajectory of user’s intent/feedback as well as system’s intent to decide what is the best strategy to pick</p>
  </li>
</ul>

<p>Hence essentially this module will be learning the <strong>successful trajectories</strong></p>

<ul>
  <li>e.g. a finite state transducer, or any RL type algorithm</li>
</ul>

<p><img src="NLP_part2/image-20220418170724440.png" alt="image-20220418170724440" style="zoom:50%;" /></p>

<hr />

<p>Finally, once added, this can further improve can be made</p>

<p><img src="NLP_part2/image-20220418213013312.png" alt="image-20220418213013312" style="zoom: 33%;" /></p>

<h4 id="error-analysis-personality-adaptation">Error Analysis: Personality Adaptation</h4>

<p><img src="NLP_part2/image-20220418213107004.png" alt="image-20220418213107004" style="zoom: 33%;" /></p>

<h3 id="dialogs-is-more-than-nlp">Dialogs is More than NLP</h3>

<p>For production use in reality, dialogs requires much more than just NLP:</p>

<p><img src="NLP_part2/image-20220418171011546.png" alt="image-20220418171011546" style="zoom:50%;" /></p>

<p>where the Language understanding, Dialog Management, and Language Generation would be handled by NLP. But we also need:</p>

<ul>
  <li>most of the time conversations are spoken: hence we have <strong>multi-modal data</strong> such as hand gestures and audio inputs
    <ul>
      <li>e.g. speeches have much noise. Can we still recover the intent with noises?</li>
      <li>e.g. given some emotion of the speaker, what is the best strategy?</li>
    </ul>
  </li>
  <li>need <strong>common senses</strong> to deal with out-of-distribution conversations (e.g. erratic inputs)</li>
  <li>other human factors used for judging the quality of conversations</li>
</ul>

<h2 id="ethnical-consideration-of-chatbots">Ethnical Consideration of Chatbots</h2>

<p>36% of the people thought that they are humans (but they are mechanical turkers, so it depends on the education level)</p>

<p><img src="NLP_part2/image-20220418213919831.png" alt="image-20220418213919831" style="zoom: 33%;" /></p>

<p>which can be very dangerous</p>

<ul>
  <li>
    <p>e.g. scamming people for money.</p>
  </li>
  <li>
    <p>but you could also train a bot to depend against scammers as well</p>
  </li>
</ul>

<p>But anyway</p>

<p><img src="NLP_part2/image-20220418214009877.png" alt="image-20220418214009877" style="zoom: 33%;" /></p>

<p>though it is rarely exercised.</p>

<h1 id="meta-learning-in-nlp">Meta Learning in NLP</h1>

<p>Useful when you have <strong>low resource</strong> (training sets), but you still want to achieve some task. Commonly, the familar methods you might know include:</p>

<ul>
  <li>taking a pre-trained model then <strong>fine-tune</strong> on your limited resource task (transfer learning)</li>
  <li>but here, we will introduce a <strong>new method</strong> to solve this kind of problem</li>
</ul>

<h2 id="what-is-meta-learning">What is Meta-Learning</h2>

<p>To understand what meta-learning is doing, we will see how we got <strong>from the transfer learning to the meta-learning</strong> task</p>

<ul>
  <li>transfer learning</li>
  <li>multitask learning</li>
  <li>meta-learning</li>
</ul>

<hr />

<p><strong>Transfer Learning</strong></p>

<p>Assume we have learnt a network and is trained for english to french translation with enough data.</p>

<p>But now we have a new task, which <strong>does not have</strong> enough data. But notice that we could had a simliar architecture</p>

<p><img src="NLP_part2/image-20220420161918427.png" alt="image-20220420161918427" style="zoom: 40%;" /></p>

<p>so that:</p>

<ul>
  <li>encoder layers <strong>should work in both cases</strong> because they are used to “understand english”</li>
  <li>then, all we need to do is to <strong>fine-tune</strong> the decoder layers, which is essentially transfer learning.</li>
</ul>

<p>This works pretty well because in reality, because good models typiocally have large encoder layers but only few decoder layers.</p>

<hr />

<p><strong>Multitask learning</strong></p>

<p>Now suppose we are given a training dataset that have:</p>

<ul>
  <li>a lot of task (e.g. english to french, english to chinese, etc)</li>
  <li>ach task has only a few training data</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Data</th>
      <th style="text-align: center">Architecture</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220420162059537.png" alt="image-20220420162059537" style="zoom: 40%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220420162320284.png" alt="image-20220420162320284" style="zoom:40%;" /></td>
    </tr>
  </tbody>
</table>

<p>the data for each language pair is too small, but it could be that <strong>all of them combined is enough</strong>. Then:</p>

<ul>
  <li>
    <p>you have the architecture on the right, which has a <strong>shared encoder</strong> which learns through all training data</p>
  </li>
  <li>
    <p>but what about the loss of that shared layer?</p>

    <ul>
      <li>the loss will <strong>most often be a linear combination of $L_i$ for task $T_i$</strong>
\(\mathcal{L}(\theta^{sh}, \theta^1, ...,\theta^T) =\sum_{i=1}^T \alpha_i \mathcal{L}_i(\theta^{sh}, \theta^i, D_i)\)
so that each task essentially is $ \mathcal{L}_i(\theta^{sh}, \theta^i, D_i)$ for having a dataset $D_i$</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Meta-Learning:</strong> two types of views of the task</p>

<ul>
  <li><strong>Mechanistic View</strong>:
    <ul>
      <li>normally we train by reading in the training dataset of samples, then inference on new sample</li>
      <li>here, we train with a dataset where <strong>each training sample is a dataset of a task $T_i$</strong>. This dataset will be called a <strong>meta-dataset</strong></li>
      <li>of course, the aim of this is to <strong>perform well on a new task $T$ with its dataset</strong></li>
    </ul>
  </li>
  <li><strong>Probabilistic View</strong>:
    <ul>
      <li>from the meta-dataset, we aim to extract a <strong>good prior</strong> that should be shared from those set of tasks</li>
      <li>then, doing a few-shot learning on a new task will <strong>use this prior</strong> and then train to infer posterior parameters</li>
    </ul>
  </li>
</ul>

<p>In general, on common application of this is to make it ==perform well under few-shot conditions==.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Typical View</th>
      <th style="text-align: center">Meta-Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220420163130731.png" alt="image-20220420163130731" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220420163134630.png" alt="image-20220420163134630" /></td>
    </tr>
  </tbody>
</table>

<p>so essentially we have:</p>

<ul>
  <li>at training: train support, train query sets</li>
  <li>at testing: test support, test query sets</li>
  <li>we want the model to <strong>perform well in test support and test query</strong></li>
</ul>

<p>Therefore, we want to ==simulate the testing environment== during our training procedure</p>

<ul>
  <li>just like normal model training where our train/test environment are <strong>both classification</strong>, here our train/test environment should <strong>both be few-shot learning</strong></li>
  <li>how do we do this? Essentially see <a href="#Model Agnostic Meta Learning">Model Agnostic Meta Learning</a></li>
</ul>

<h2 id="meta-learning-algorithms">Meta-Learning Algorithms</h2>

<p>Broadly speaking, there are two types of meta-learning algorithms:</p>

<ul>
  <li><strong>Optimization Based</strong>: we still optimize over some loss of a model, but we introduce a <strong>new training paradigm</strong> that is suitable for donig few-shot learning at test time
    <ul>
      <li>i.e. at test time, we can <em>fine-tune</em> our model fast when only given a few-shot training samples</li>
    </ul>
  </li>
  <li><strong>Metric Based:</strong> learning a model that gives a <strong>good representation/embedding</strong> for each class $c_i$ (under few-shots), from which we can apply some metrics (e.g. distance) to measure, for a query sample $x_q$ which class $c_i$ is belongs to
    <ul>
      <li>i.e. at test time, we <em>directly use this model</em> (which learns some metrics) when only given a few-shot training samples</li>
    </ul>
  </li>
</ul>

<h3 id="model-agnostic-meta-learning">Model Agnostic Meta Learning</h3>

<p>Suppose we have some model $f_\theta$ we want to use, for example a CNN.</p>

<blockquote>
  <p>The goal of few-shot meta-learning is to train a model $f_\theta$ that can quickly adapt to a new task using only a few datapoints and training update iterations to $\theta$.</p>

  <ul>
    <li>in other words, we want some ==good initialization of $\theta_0$== (learnt from other tasks as a “warm start”) such that in a new task it learns fast with only few samples</li>
  </ul>
</blockquote>

<p>So how do we get such a handy $\theta_0$? In effect, the meta-learning problem <strong>treats entire tasks as training examples</strong>, i.e. we consider</p>

<ul>
  <li>==a distribution over tasks $p(T)$== that we want our model to adapt to</li>
  <li>for each task $T_i$ could have its own loss $\mathcal{L}_{T_i}$ and its training data distribution $q_i(x)$</li>
</ul>

<p>Then, during <strong>meta-training</strong> (in a $K$-shot learning setting), the idea is, given a model $f_\theta$:</p>

<ol>
  <li>
    <p>consider some random initialization of $\theta_0 \equiv \theta \equiv \phi$</p>
  </li>
  <li>
    <p>while not done:</p>

    <ol>
      <li>
        <p>sample a batch of tasks $T_{i}$s to train from $p(T_i)$:</p>

        <ol>
          <li>sample  a task $T_i$, e.g. compare “cat v.s. dog”</li>
          <li>draw $K$ samples from $q_i(x)$ for training, known as ==support set==</li>
          <li>compute its loss $L_{T_i}(f_\theta)$ using the $\theta$ initialization</li>
          <li>update/see how well this initialization works by doing a few descents:</li>
        </ol>

\[\theta_i' = \theta - \alpha \nabla_\theta L_{T_i}(f_\theta)\]

        <p>for that specific $T_i$.</p>
      </li>
      <li>
        <p>update the <strong>initialization $\theta\equiv \phi$</strong> as how the <strong>total loss over all tasks can be decreased if I have a better $\theta$</strong> to begin with
\(\theta \leftarrow \theta - \beta \nabla_\theta\sum_{T_i} L_{T_i}(f_{\theta_i'})\)
where:</p>

        <ul>
          <li>
            <p>$f_{\theta_i’}$ is like the same model “fine-tuned” on task $T_i$.</p>
          </li>
          <li>
            <p>$L_{T_i}(f_{\theta_i’})$ will now be evaluated on the <strong>new samples</strong> from $q_i(x)$, known as the ==query set== to test its generalization ability as well</p>
          </li>
          <li>
            <p>since we are taking derivative w.r.t $\theta$, and we know (if only doing a single iteration)
\(\theta_i' = \theta - \alpha \nabla_\theta L_{T_i}(f_\theta)\)
essentially this term include <strong>loss of losses</strong>.</p>
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>output the learnt $\theta$ being the ==better “initialization” parameters for your model $f_\theta$ which is ready for other tasks==</p>
  </li>
</ol>

<p>Hence the algorithm is</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Algorithm</th>
      <th style="text-align: center">Update Tree</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220331182301680.png" alt="image-20220331182301680" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220420163707807.png" alt="image-20220420163707807" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>which we know is agonistic of what model $f$ we picked.</p>

<ul>
  <li>so that essentially we update $\theta \to \theta’$, where the intermediate $\theta_i$ for each task will be only used for computing the loss for $\theta$ and updating it.</li>
  <li>
    <p>since $\theta_i$ comes from computing task loss $\mathcal{L}<em>i$ and doing $\nabla</em>{\theta_i}\mathcal{L}_i$, and the final $\mathcal{L}(\theta)$ will depend on $\theta_i$, we essentially would need to compute <strong>Hessian of $\nabla^2_{\theta_i} \mathcal{L}_i$</strong> when we update $\theta \to \theta’$. This is computationally expensive, and hence we had work such as the Amazon’s Reptile system that <strong>approximates the hessian</strong> and saves compute power.</p>
  </li>
  <li>resource https://arxiv.org/pdf/1703.03400.pdf</li>
</ul>

<h3 id="metric-based-network">Metric-Based Network</h3>

<p>The most notable example of this is the Siamese network, which has the generic task of classifing whether if the two images are the same.</p>

<p><img src="NLP_part2/image-20220420164128809.png" alt="image-20220420164128809" style="zoom: 50%;" /></p>

<p>however, the difference with normal training is:</p>

<ul>
  <li>
    <p>the two input image could come from <strong>any domain</strong>, e.g. kid’s drawings, signatures, facial photographs, etc.</p>
  </li>
  <li>
    <p>The idea of this network is to use a shared CNN backbone to get an embedding, then then compute probability by looking at the distance, so that the shared CNN backbone in the middle is generalizable.</p>
  </li>
</ul>

<blockquote>
  <p>In general, metric-based network aims at training a network that can produce <strong>good embeddings</strong> for a sample in a new domain (haven’t seen in training).</p>

  <ul>
    <li>with few-shots, then $K$ samples for each class in the domain</li>
  </ul>
</blockquote>

<p>Some other networks should should already know from the Deep Learning course notes:</p>

<ul>
  <li>
    <p><strong>Matching Network</strong></p>

    <p><img src="NLP_part2/image-20220420215419650.png" alt="image-20220420215419650" style="zoom:50%;" /></p>

    <p>which basically creates an embedding for each input $x_i$, then for the new test sample $x_q$:
\(\hat{y} = \sum_{i}a(x_i,x_q)y_i\)
basically predicting by a weighted average.</p>
  </li>
  <li>
    <p><strong>Prototypical Network</strong></p>

    <p><img src="NLP_part2/image-20220420215631978.png" alt="image-20220420215631978" style="zoom:50%;" /></p>

    <p>which basically produces a class prototype vector $c_k$, and then for the new sample $x$ classify the class by looking at the distance between its embedding $f_\phi(x)$ and the class vectors $c_k$</p>
  </li>
</ul>

<p>Basically they all tend to learn a metric/embedding to <strong>produce a good latent space representation</strong> of the class.</p>

<h2 id="applications-in-nlp">Applications in NLP</h2>

<p>How do we apply the idea of meta-learning in NLP applications?</p>

<h3 id="meta-leanring-for-dialog">Meta Leanring for Dialog</h3>

<p>Consider the task of</p>

<ul>
  <li>building a dialog system in domain $A, B, C$</li>
  <li>
    <p>training data of dialogs are in domain $A,E,D,F$</p>
  </li>
  <li>preparing a model $M$ that would work not only in $A,B,C$, but perhaps new domains as well</li>
</ul>

<p>Then essentially it becomes learning a model $M$ that <strong>performs well</strong> given a few-shot learning samples. Hence we consider utilizing MAML algorithm mentioned above:</p>

<p><img src="NLP_part2/image-20220420165535700.png" alt="image-20220420165535700" style="zoom:50%;" /></p>

<p>where</p>

<ul>
  <li>$M_k’$ are the temporary models with weights $\theta_k$</li>
  <li>The aim is to update $M$ with $\theta \to \theta’$, such that the final $\theta^*$ would work well under few-shot learning</li>
</ul>

<p>The performance comparision:</p>

<p><img src="NLP_part2/image-20220420165657359.png" alt="image-20220420165657359" style="zoom:50%;" /></p>

<p>where when the dialog system is trained with meta-learing is is better than just fine-tuning the model to another task</p>

<h3 id="meta-learning-for-machine-translatoin">Meta Learning for Machine Translatoin</h3>

<p>As mentioned before, it is common to have a MT task that, given a few samples of translating from language $A$ to language $B$, we want it to work even if our training data only contained $C\to D, A\to C, D\to F$, etc.</p>

<p>Graphically, the comparision between Meta-learning and other methods mentioend above is:</p>

<p><img src="NLP_part2/image-20220420165920052.png" alt="image-20220420165920052" style="zoom: 50%;" /></p>

<p>so that:</p>

<ul>
  <li>
    <p>for transfer learning, we want to start with $\theta$ that works best in one domain and then <strong>move to others</strong></p>
  </li>
  <li>
    <p>for meta learning, you want to get $\theta$ such that it works ==best at adapation== (in all directions)</p>
  </li>
</ul>

<h3 id="meta-in-context-learning">Meta In Context Learning</h3>

<p>Some research has found that</p>

<blockquote>
  <p>(Large) Language models are <strong>few-shot learners</strong></p>

  <ul>
    <li>basically they seem to work well with few-shot even if the training procedure is just masked language/whatever that is used normally for a language model training</li>
  </ul>
</blockquote>

<p>An example in practice is GPT 3. GPT 3 has huge parameters, and is trained on huge text data.</p>

<ul>
  <li>
    <p>hard to fit in normal school GPUs</p>
  </li>
  <li>
    <p>also hard to fine-tune as the model is huge</p>
  </li>
</ul>

<p>Therefore a new procedure is needed for <strong>few-shot “training”</strong></p>

<ul>
  <li>normally you would imagine we update $\theta$ per step with each new (few-shot) traing data</li>
  <li>but as that is intractable, we can just dump all the examples along with the query as input</li>
</ul>

<p><img src="NLP_part2/image-20220420170504178.png" alt="image-20220420170504178" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the grey ones are the few-shot examples, the black texts are the machines’ output</p>
  </li>
  <li>
    <p>so that essentially it is few shot learning ==without updating weights==</p>
  </li>
</ul>

<p>How does that even work? As we have transformers in the model:</p>

<ul>
  <li>the model not only attends to the input, it also attends to the context</li>
  <li>therefore, we expect models to <strong>use the attention mechanism</strong> to understand the few-shot</li>
</ul>

<h4 id="in-context-tuning">In Context Tuning</h4>

<p>Recent results shown that, given that in-context learning works, we can also do the following (for smaller models such as GPT2):</p>

<ul>
  <li>instead of doing one step update for the intermediate weights $\theta_i$ in MAML</li>
  <li>we put all the task $T_i$ as a single input with context and fine-tune $\theta \to \theta’$ directly</li>
</ul>

<p><img src="NLP_part2/image-20220420171303276.png" alt="image-20220420171303276" style="zoom:67%;" /></p>

<p>so that essentially:</p>

<ul>
  <li>it is <strong>fine-tuning</strong> but the training set is a ==concatenation of support set and query set==</li>
  <li>since it is a concatenation, the problem becomes memory constraint.</li>
  <li>such an update method is also model agnostic</li>
</ul>

<h1 id="bias-in-nlp">Bias in NLP</h1>

<p>Related to ethics and safety of NLP models. Consider cases where:</p>

<ul>
  <li>Language models on Job description preferers/assigns better competency for male engineers over females</li>
  <li>Products such as Amazon Alexa outputting racist texts (just for example)</li>
</ul>

<p>Therefore, in this section we will talk about some methods on</p>

<ul>
  <li>how to <strong>detect biases</strong>
    <ul>
      <li>https://arxiv.org/abs/1608.07187</li>
      <li>https://www.aclweb.org/anthology/N19-1063.pdf</li>
    </ul>
  </li>
  <li>how do to <strong>de-biasing</strong>
    <ul>
      <li>https://www.aclweb.org/anthology/N19-1061.pdf</li>
    </ul>
  </li>
</ul>

<h2 id="bias-in-word-embeddings">Bias in Word Embeddings</h2>

<blockquote>
  <p>In machine learning, essentially bias refers to <strong>prior information/the priors</strong>. The are ==problematic when== such information is derived from aspects of human culture known to lead to harmful behavior</p>

  <ul>
    <li>e.g. Word2Vec: “Man is to computer programmer as woman is to homemaker.”</li>
  </ul>
</blockquote>

<p>Just as there are tests to measure implicit bias in humans, there can be parallel test to measure bias in machine, and today we mostly do this by looking at the <strong>learnt embeddings</strong>. Here, we can consider word level embeddings.</p>

<blockquote>
  <p>Word embedding representations, in addition to encoding semantic meaning, also encode <strong>semantic analogy</strong>.</p>

  <ul>
    <li>using that, we can analyze whether if certain words are gender/race neutral, for example.</li>
  </ul>
</blockquote>

<h3 id="implicit-association-test-and-weat">Implicit Association Test and WEAT</h3>

<p>A famous test by Harvard is useful to measure biases in Human: the Implicit Association Test</p>

<blockquote>
  <p>The IAT measures the <strong>strength of associations between concepts</strong> (e.g., black people, gay people) and evaluations (e.g., good, bad) or stereotypes (e.g., athletic, clumsy). The main idea is that making a <strong>response is easier when closely related</strong> items share the same response key.</p>
</blockquote>

<p>Some useful word pairs we will discuss are:</p>

<ul>
  <li>Flowers, insects - pleasant, unpleasant</li>
  <li>European American Names, African American Names - pleasant, unpleasant</li>
  <li>Female names, male names - family, career</li>
  <li>etc.</li>
</ul>

<p>This essentially inspires how we can perform a similar task for word embeddings</p>

<blockquote>
  <p><strong>Word-Embedding Association Test (WEAT)</strong> essentially computes such association of concepts with <strong>cosine similarity</strong> of the embeddings, and was able to replicate the stereotypes found in the IAT tests.</p>
</blockquote>

<p>The details are formalized as follows. Consider:</p>

<ul>
  <li>two sets of <strong>target</strong> words $X$ (e.g., programmer, engineer, scientist; and nurse, teacher, librarian)</li>
  <li>two sets of <strong>attribute</strong> words $Y$ (e.g., man, male; and woman, female).</li>
  <li>The <strong>null hypothesis</strong> is that there is no difference between the two sets of target words in terms of their relative similarity to the two sets of attribute words.</li>
  <li>The <strong>permutation test</strong> measures the (un)likelihood of the null hypothesis by computing the probability that <em>a random permutation of the attribute words would produce the observed (or greater) difference</em> in sample means.</li>
</ul>

<p>Therefore, the test statics of the <strong>differential association of the two sets of target words</strong> (so that it is highest when the two groups have a different association, e.g. $X \to A$ and $Y \to B$) computes:
\(s(X,Y,A,B) = \sum_{x \in X}s(x,A,B) - \sum_{y \in Y}s(y,A,B)\)
where
\(s(w,A,B) = \text{mean}_{a \in A} \cos(\vec{w},\vec{a})-\text{mean}_{b \in B} \cos(\vec{w},\vec{b})\)
essentially measure the association of words to attributes in $A$ v.s. attributes in $B$. So that this is large when the word $\vec{w}$ is <strong>biased</strong> towards a particular group of attributes.</p>

<p>From this we can also formulate the permutation test and the effective size (to reject the null hypothesis), for which we will skip here but can be found on https://www.science.org/doi/10.1126/science.aal4230</p>

<hr />

<p><em>Example of Biases Found in Word Embeddings (such as Word2Vec)</em></p>

<p>Racial Bias: African American names v.s. European American names</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Attributes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204324985.png" alt="image-20220425204324985" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204338074.png" alt="image-20220425204338074" /></td>
    </tr>
  </tbody>
</table>

<p>Gender Bias: Females are more associated with family, men with careers</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Attributes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204532048.png" alt="image-20220425204532048" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204541348.png" alt="image-20220425204541348" /></td>
    </tr>
  </tbody>
</table>

<p>Gender Bias: Female terms more associated with arts, male terms with mathematics</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Attributes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204629557.png" alt="image-20220425204629557" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220425204637821.png" alt="image-20220425204637821" /></td>
    </tr>
  </tbody>
</table>

<hr />

<p>Where does those bias come from? It is basically learnt from <strong>current world statistics</strong> (which is biased). For example</p>

<p><img src="NLP_part2/image-20220425204745622.png" alt="image-20220425204745622" style="zoom:50%;" /></p>

<p>where we see that:</p>

<ul>
  <li>it learns to output a high similarity score for occupations where there are more women</li>
  <li>but we want it to learn those occupations to be gender neutral even if this is the reality</li>
</ul>

<h3 id="effect-of-bias-on-nlp">Effect of Bias on NLP</h3>

<p>As we know that many laguage models are trained on a huge amount of text:</p>

<ul>
  <li>easily pick up biases from old documents/books, for example</li>
  <li>different languages might have different associations/biases</li>
</ul>

<blockquote>
  <p>Sapir-Whorf hypothesis: “real world” is to a large extent <strong>unconsciously built up on the language habits</strong> of the group</p>
</blockquote>

<p>Therefore, together with biases thise produces problems such as:</p>

<ul>
  <li>MT translation:
    <ul>
      <li>Chinese speeches do not distinguish between he/she</li>
      <li>to translate to English, we need he/she</li>
      <li>but we <strong>should not</strong> disambiguate those based on biases!</li>
    </ul>
  </li>
  <li>Dialog
    <ul>
      <li>e.g. implicitly using a pronoun such as “he” when referring to boss?</li>
    </ul>
  </li>
</ul>

<p>Both of which not only relates to biases in word level embeddings, but also perhaps <strong>bias in sentence level embeddings!</strong></p>

<h2 id="bias-in-sentence-embeddings">Bias in Sentence Embeddings</h2>

<p>The idea is the same as WEAT, where we want to know if the same bias exist for sentence embeddings. The only difference is:</p>

<ul>
  <li>we have sentences, hence added context</li>
  <li>the target and the attribute sets are sets of sentences</li>
</ul>

<blockquote>
  <p><strong>SEAT: Sentence Encoder Association Test</strong>. This basically does the same procedure as WEAT where you can:</p>

  <ul>
    <li>form a sentence from words by simple templates where the word has been inserted: “This is a <code class="language-plaintext highlighter-rouge">&lt;word&gt;</code>”.
      <ul>
        <li>this is often used to convert word attributes to sentence attributes.</li>
      </ul>
    </li>
    <li>or if you have sentences to tests as targets, use them directly</li>
  </ul>
</blockquote>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Target</th>
      <th style="text-align: center">Attributes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP_part2/image-20220425210430527.png" alt="image-20220425210430527" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP_part2/image-20220425210435664.png" alt="image-20220425210435664" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>The upshot is that the same biases still exists.</p>

<h3 id="abw-and-double-bind">ABW and Double Bind</h3>

<p>Two traditional biases that were studied are:</p>

<ul>
  <li><strong>ABW</strong>:  “angry black woman” stereotype, basically black women are portrayed as loud, angry and imposing</li>
  <li><strong>Double Bind</strong>: women in professional settings treated with disadvantage v.s. man
    <ul>
      <li>If women succeed in a male dominated job, they are <em>perceived less likeable and more hostile</em> than similarly performing men</li>
      <li>If success not clear, they are perceived <em>less competent and achievement oriented</em> than men</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>ABW Study:</strong></p>

<ul>
  <li>Target: Black and white women names</li>
  <li>Attributes: adjectives presented in Collins and their antonyms</li>
</ul>

<p><strong>Double Bind Study</strong> (likeable):</p>

<ul>
  <li>Targets: male/female names. “Kathy is an engineer with superior technical skills”</li>
  <li>Attributes: likable and non-hostile terms: “the engineer is nice”</li>
</ul>

<p><strong>Double Bind Study</strong> (success):</p>

<ul>
  <li>Targets: “Kathy is an engineer”</li>
  <li>Attributes: competent/achievement-oriented terms: “The engineer is high performing.”</li>
</ul>

<p>Results:</p>

<p><img src="NLP_part2/image-20220425211114721.png" alt="image-20220425211114721" style="zoom:67%;" /></p>

<p>Hence we see that:</p>

<ul>
  <li>
    <p>for word level, you can basically pass in the word itself as a sentence and take that as the mebdding</p>
  </li>
  <li>ELMo, GPT, and BERT seem to have bias when used with sentence level  but not when they are doing on a word level</li>
  <li>however, there are discrepancies as many results does not seem to agree even with same p-values</li>
</ul>

<hr />

<p>The conclusion is that:</p>

<ul>
  <li>SEAT can confirm that bias exists</li>
  <li>Cosine similarity may not be a suitable model of representational similarity in recent models (e.g., BERT)</li>
</ul>

<h2 id="debiasing">Debiasing</h2>

<blockquote>
  <p>Is it possible to <strong>remove the bias</strong> that is clearly present in real life statistics?</p>
</blockquote>

<p>It is still under active research, but we do have some approaches</p>

<ul>
  <li><strong>HARD-DEBIASED:</strong> remove the bias after training by <strong>post-processing</strong></li>
  <li><strong>GN-GLOVE:</strong> use a <strong>loss function</strong> during training that aims to <strong>reduce bias</strong></li>
</ul>

<p>However, the quick conclusion is that even those does remove surface level bias (the word embedding itself may not be biased), it does <strong>not</strong> reduce ==hidden biases== such as “doctor” and “boss” having a similar embedding v.s. “nurse” and “teacher”</p>

<h3 id="de-bias-via-post-processing">De-bias via Post Processing</h3>

<p>Given a word $\vec{w}$, we can define its gender bias (for example) by its projection on the “gender direction”:
\(\vec{w} \cdot (\vec{he}-\vec{she})\)
where $(\vec{he}-\vec{she})$ would be the gender direction. We can compute this in a more robust way by taking</p>

<ol>
  <li>taking 10 pairs of words that surely have gender differences</li>
  <li>take the principle components of the 10 pairs</li>
</ol>

<p>Then, when we have this gender direction</p>

<ol>
  <li>for all words that should not be inherently gendered</li>
  <li>Zero the gender projection for each word</li>
</ol>

<h3 id="de-bias-via-loss-function">De-bias via Loss Function</h3>

<p>Instead of removing/post processing the embedding, consider having loss such that</p>

<blockquote>
  <ul>
    <li>words in different groups to differ in last coordinate (as we know they exist in our data)</li>
    <li>neutral gender words to be orthogonal</li>
  </ul>
</blockquote>

<p>Then essentially:</p>

<ol>
  <li>Train debiased word embeddings from scratch</li>
  <li>Change the loss function for Glove as mentioned above
    <ul>
      <li>so essentially concentrate gender information in last coordinate</li>
    </ul>
  </li>
  <li>Once done, remove the last coordinate</li>
</ol>

<h3 id="hidden-bias-via-clustering">Hidden Bias via Clustering</h3>

<blockquote>
  <p>Essentialy the paper https://www.aclweb.org/anthology/N19-1061.pdf claims that the above does not remove the biases.</p>

  <p>They manage to hide the bias in their <strong>neighborhood relationship</strong>, even if their similarity to gender is removed.</p>

  <ul>
    <li>most word pairs maintain their previous similarity, despite their change in relation to the gender direction</li>
  </ul>
</blockquote>

<p>Consider</p>

<ol>
  <li>taking the most biased words in the vocabulary according to the original bias
    <ul>
      <li>500 male biased, 500 female biased</li>
    </ul>
  </li>
  <li>Cluster into 2 clusters using k-means
    <ul>
      <li>compare the result before de-biasing and after de-biasing</li>
    </ul>
  </li>
</ol>

<p>Results:</p>

<p><img src="NLP_part2/image-20220425165851071.png" alt="image-20220425165851071" style="zoom: 50%;" /></p>

<p>where we see that</p>

<ul>
  <li>
    <p>Bias still manifested by the word being close to socially-marked feminine words</p>
  </li>
  <li>
    <p>i.e. although they are hiding their bias w.r.t gender, but their <strong>neighbor relationship</strong> is still apparent</p>
  </li>
</ul>

<p>Therefore, this introduces a new mechanism for measuring bias: % male/female socially-biased words among the k nearest neighbors of the target word.</p>

<hr />

<p>Another intuitive metric could be:</p>

<ol>
  <li>given 5000 most biased words according to original experiments</li>
  <li>Train an SVM on 1000 random sample, predict gender for remaining 4000</li>
</ol>

<p>So that if words embeddings still have biases, then models such as SVM will not be able to distinguish. However, in reality:</p>

<ul>
  <li>Hard-debiased: 88.8% accuracy vs 98.25% accuracy with non-debiased version</li>
  <li>GN-Glove: 96.53% accuracy vs. 98.65% accuracy with non-debiased version</li>
</ul>

<p>so, again those biases are hidden somewhere in the embeddings.</p>

<blockquote>
  <p>Idea: using GAN to perhaps remove the bias.</p>
</blockquote>

  </div><a class="u-url" href="/lectures/2021@columbia/COMS4705_NLP_part2.html/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
