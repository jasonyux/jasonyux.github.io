<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COMS4705 NLP | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="COMS4705 NLP" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Logistics and Introduction" />
<meta property="og:description" content="Logistics and Introduction" />
<link rel="canonical" href="/lectures/2021@columbia/COMS4705_NLP.html/" />
<meta property="og:url" content="/lectures/2021@columbia/COMS4705_NLP.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-30T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COMS4705 NLP" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-30T00:00:00-04:00","datePublished":"2022-03-30T00:00:00-04:00","description":"Logistics and Introduction","headline":"COMS4705 NLP","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2021@columbia/COMS4705_NLP.html/"},"url":"/lectures/2021@columbia/COMS4705_NLP.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/learning">Blog</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
	<script>
		MathJax = {
		  tex: {
			inlineMath: [['$', '$'], ['\\(', '\\)']],
			displayMath: [['$$', '$$'], ['\\[', '\\]']]
		  }
		};
	</script>
	<script id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
	</script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COMS4705 NLP</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-03-30T00:00:00-04:00" itemprop="datePublished">
        Mar 30, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="logistics-and-introduction">Logistics and Introduction</h1>

<p>Discussion/Q&amp;A:</p>

<ul>
  <li>Piazza. Every questions should be sent there.</li>
</ul>

<p>TA:</p>

<ul>
  <li>
    <p>Professor: 1 hour before class</p>
  </li>
  <li>Yanda Chen: Tue 6-8pm</li>
  <li>Kun Qian: Tue Thur 4:30-5:30pm</li>
  <li>Qingyang Wu: Wed 10am-12pm</li>
</ul>

<p>Grades:</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Homework *3 (All python)</td>
      <td>51%</td>
    </tr>
    <tr>
      <td>Midterm *1</td>
      <td>15%</td>
    </tr>
    <tr>
      <td>Final *1</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>Tendance</td>
      <td>14%</td>
    </tr>
    <tr>
      <td>Piazza (extra credit)</td>
      <td>3%</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>103%</td>
    </tr>
  </tbody>
</table>

<p>Slides:</p>

<ul>
  <li>On the Syllabus</li>
  <li>Readings: https://web.stanford.edu/~jurafsky/slp3/</li>
</ul>

<h2 id="applications-of-nlp">Applications of NLP</h2>

<p>Question Answering:</p>

<ul>
  <li>Won the game “Jeopardy” on Feb 16, 2011</li>
</ul>

<p>Information Extraction</p>

<ul>
  <li>e.g. extraction information from an email, and add the appointment to my calendar</li>
</ul>

<p>Sentiment/Opinion Analysis</p>

<ul>
  <li>whether if it is a positive review or a negative review</li>
</ul>

<p>Machine Translation</p>

<ul>
  <li>
    <p>fully automatic translation between different languages</p>
  </li>
  <li>
    <p>help human translators to translate text</p>

    <p><img src="NLP/image-20220119163324409.png" alt="image-20220119163324409" style="zoom: 80%;" /></p>
  </li>
</ul>

<h2 id="current-progress-in-nlp">Current Progress in NLP</h2>

<p><img src="NLP/image-20220119163418984.png" alt="image-20220119163418984" /></p>

<p>where:</p>

<ul>
  <li>Spam detection would be like a ML binary classification problem</li>
  <li>Part-of-Speech (POS): tag every token with their syntactic meaning</li>
  <li>Sentiment Analysis: though it sounds like a simply classification problem, we may want to be “context aware” due to sarcasm</li>
  <li>Coreference: which person does “he” refers to? (helpful for extracting information)</li>
  <li>Parsing: parse the sentence into a tree, e.g. which part is the central piece, which part is purely for description, e.g. understand the functions of different parts of a text. (helpful for summarization)</li>
</ul>

<h2 id="difficulties-in-nlp">Difficulties in NLP</h2>

<p><img src="NLP/image-20220119170850994.png" alt="image-20220119170850994" /></p>

<p>where:</p>

<ul>
  <li>use of Emoji, internet slangs</li>
  <li>segmentation of a sentence to break them into parts</li>
  <li>idioms: they have deeper/cultural meaning that they superficially mean</li>
  <li>tricky entity names: “Let it Be” is the name of a song. “A bug’s life” is name of a show</li>
</ul>

<h2 id="overall-syllabus">Overall Syllabus</h2>

<ol>
  <li>We will start with theories and statistical NLP
    <ul>
      <li>Viterbi</li>
      <li>N-gram language modeling</li>
      <li>statistical parsing</li>
      <li>Inverted index, tf-id</li>
    </ul>
  </li>
  <li>Programming and Implementations
    <ul>
      <li>using Python</li>
    </ul>
  </li>
</ol>

<h1 id="basic-text-processing">Basic Text Processing</h1>

<p>Before almost any natural language processing of a text, the text has to be normalized. <strong>At least</strong> three tasks are commonly applied as part of any normalization process:</p>

<ol>
  <li>Tokenizing (segmenting) words</li>
  <li>Normalizing word formats</li>
  <li>Segmenting sentences</li>
</ol>

<blockquote>
  <p>Most preprocessing task in NLP involve a set of tasks collectively called <strong>text normalization</strong></p>

  <ul>
    <li><strong>Normalizing</strong> text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or <em>tokenizing</em> words, <em>lemmatization</em> (e.g. sang, sung, and sings are all form sing), <em>stemming</em>, <em>sentence segmentation</em>, and etc.</li>
  </ul>
</blockquote>

<p>Some other nomenclatures:</p>

<ul>
  <li><strong>corpus</strong> (plural corpora), a computer-readable corpora collection of text or speech
    <ul>
      <li>e.g. the Brown corpus is a million-word collection of samples from 500 written English texts from different genres</li>
    </ul>
  </li>
  <li><strong>utterance</strong> is the spoken correlate of a sentence: “I do uh main- mainly business data processing”
    <ul>
      <li>This utterance has two kinds of <strong>disfluencies</strong>. The broken-off word main- is fragment called a <strong>fragment</strong>. Words like <em>uh</em> and <em>um</em> are called <strong>fillers</strong> or filled pauses.</li>
      <li>Should filled pause we consider these to be words? Again, it depends on the application.</li>
    </ul>
  </li>
</ul>

<h2 id="regular-expressions">Regular Expressions</h2>

<p>Some common usages building regular expressions during preprocessing/extracting some words:</p>

<p><strong>Basics:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Pattern</th>
      <th style="text-align: center">Matches</th>
      <th style="text-align: center">Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">[wW]oodchuck</code></td>
      <td style="text-align: center">Woodchuck, woodchuck</td>
      <td style="text-align: center"><strong>Disjunction</strong> <code class="language-plaintext highlighter-rouge">[wW]</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">[1-9]</code></td>
      <td style="text-align: center">any single digit</td>
      <td style="text-align: center"><strong>Ranges</strong>: <code class="language-plaintext highlighter-rouge">[1-9]</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">[^A-Z]</code></td>
      <td style="text-align: center"><strong>Not</strong> an upper case letter</td>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">^</code> only means negation when used with <code class="language-plaintext highlighter-rouge">[]</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">[^Ss]</code>, <code class="language-plaintext highlighter-rouge">[^e^]</code></td>
      <td style="text-align: center"><strong>Not</strong> <code class="language-plaintext highlighter-rouge">S</code> or <code class="language-plaintext highlighter-rouge">s</code>, <code class="language-plaintext highlighter-rouge">e</code> or <code class="language-plaintext highlighter-rouge">^</code></td>
      <td style="text-align: center">e.g. <code class="language-plaintext highlighter-rouge">a^b</code> means actually matching <code class="language-plaintext highlighter-rouge">a^b</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">a|b|c</code></td>
      <td style="text-align: center">same as <code class="language-plaintext highlighter-rouge">[abc]</code></td>
      <td style="text-align: center">More disjunction</td>
    </tr>
  </tbody>
</table>

<p><strong>Regular expressions with <code class="language-plaintext highlighter-rouge">?/*/+/.</code></strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Pattern</th>
      <th style="text-align: center">Matches</th>
      <th style="text-align: center">Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">colou?r</code></td>
      <td style="text-align: center">color, colour</td>
      <td style="text-align: center">previous character would be <strong>optional</strong></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">oo*h</code></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">previous charterer would have 0 or more occurrence</td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">o+h!</code></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">1 or more occurrence</td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">baa+</code></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">beg.n</code></td>
      <td style="text-align: center">begin, began, begun</td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<p><strong>Anchors <code class="language-plaintext highlighter-rouge">^</code> and <code class="language-plaintext highlighter-rouge">$</code></strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Pattern</th>
      <th style="text-align: center">Matches</th>
      <th style="text-align: center">Note</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">^[A-Z]</code></td>
      <td style="text-align: center">Palo</td>
      <td style="text-align: center">the first character must be <code class="language-plaintext highlighter-rouge">[A-Z]</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">^[^A-Z]</code></td>
      <td style="text-align: center">1Hello</td>
      <td style="text-align: center">the first character must NOT be <code class="language-plaintext highlighter-rouge">[A-Z]</code></td>
    </tr>
    <tr>
      <td style="text-align: center"><code class="language-plaintext highlighter-rouge">\.$</code></td>
      <td style="text-align: center">Hello.</td>
      <td style="text-align: center">the last character must be <code class="language-plaintext highlighter-rouge">.</code></td>
    </tr>
  </tbody>
</table>

<p>note that we used escape character as otherwise <code class="language-plaintext highlighter-rouge">.</code> means any symbol.</p>

<p><strong>More operators</strong></p>

<p><img src="NLP/image-20220124204253621.png" alt="image-20220124204253621" /></p>

<hr />

<p><em>For Example</em>:</p>

<p>Finding all instance of the word “<em>the</em>” in a given text, assuming the texts are “clean”:</p>

<p><strong>Solution</strong>:</p>

<p>Using:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">the</code> : misses capitalized <code class="language-plaintext highlighter-rouge">The</code> occurrence</li>
  <li><code class="language-plaintext highlighter-rouge">[tT]he</code>: also matched <code class="language-plaintext highlighter-rouge">other</code></li>
  <li><code class="language-plaintext highlighter-rouge">[^a-zA-Z][tT]he[^a-zA-Z]</code>: works, but would miss the <code class="language-plaintext highlighter-rouge">The</code> at the beginning of document</li>
  <li><code class="language-plaintext highlighter-rouge">(^|[^a-zA-Z])[tT]he([^a-zA-Z]|$)</code>: final version</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The process we just went through was based on fixing two kinds of errors</p>

  <ul>
    <li>Matching strings that we <strong>should not have matched</strong> (there, then, other)
      <ul>
        <li>False positives (Type I)</li>
        <li>==accuracy== or ==precision== (minimizing false positives)</li>
      </ul>
    </li>
    <li><strong>Not matching</strong> things that we should have matched (The)
      <ul>
        <li>False negatives (Type II)</li>
        <li>Increasing ==coverage== or ==recall== (minimizing false negatives)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="simple-models">Simple Models</h3>

<p>We can use Regular Expression as a <strong>simple model</strong>:</p>

<ul>
  <li>whether if certain words exist in a paragraph -&gt; classify <code class="language-plaintext highlighter-rouge">1</code> (e.g. if a positive word exist, positive sentiment)</li>
  <li>Sophisticated sequences of regular expressions are often the <strong>first model</strong> for any text processing text</li>
</ul>

<p>More complicated tasks:</p>

<ul>
  <li>Use regular expression for processing -&gt; use <strong>machine learning models</strong></li>
</ul>

<h2 id="word-tokenization">Word Tokenization</h2>

<p>Every NLP task needs to do ==text tokenization==:</p>

<blockquote>
  <p>Tokenization is essentially <strong>splitting a phrase, sentence, paragraph, or an entire text document into smaller units</strong>, such as individual words or terms. Each of these smaller units are called tokens.</p>

  <ul>
    <li>e.g. “New York” and “rock’ n’ roll” are sometimes treated as large words despite the fact that they contain spaces</li>
    <li><strong>punctuation</strong> will be included/treated as a separate token from words</li>
  </ul>
</blockquote>

<p>Some problems that makes this task invovled:</p>

<ul>
  <li>dealing with URLs</li>
  <li>dealing with numbers such as $555,500.0$, where punctuations are involved</li>
  <li>converting words such as “what’re” to “what are”</li>
  <li>depending on application, tokenize “New York” to a single word</li>
</ul>

<p>In practice, <strong>tokenization runs before any other processing</strong>, and needs to be fast. For example, converting:</p>

<p><img src="NLP/image-20220124210124932.png" alt="image-20220124210124932" /></p>

<p>therefore:</p>

<ul>
  <li>
    <p>often implemented using <strong>deterministic algorithms</strong> based on regular expressions compiled into very efficient finite state automata.</p>
  </li>
  <li>
    <p>programming example:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="s">'That U.S.A. poster-print costs $12.40...'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s">'''(?x) # set flag to allow verbose regexps
... ([A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+(-\w+)* # words with optional internal hyphens
... | \$?\d+(\.\d+)?%? # currency and percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"'?():-_`] # these are separate tokens; includes ], [
... '''</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">nltk</span><span class="p">.</span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
<span class="p">[</span><span class="s">'That'</span><span class="p">,</span> <span class="s">'U.S.A.'</span><span class="p">,</span> <span class="s">'poster-print'</span><span class="p">,</span> <span class="s">'costs'</span><span class="p">,</span> <span class="s">'$12.40'</span><span class="p">,</span> <span class="s">'...'</span><span class="p">]</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Another common task is to compute the <strong>length of a sentence</strong>. While this sounds as just counting the tokenization, depending on application, you might need to think of whether if you should count on <strong>lemma</strong> or <strong>wordform</strong>:
\(\text{Seuss's \textbf{cat} in the hat is different from other \textbf{cats}!}\)
so here notice that:</p>

<ul>
  <li>A <strong>lemma</strong> is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense
    <ul>
      <li>both $\text{cat}, \text{cats}$ would be count as only 1</li>
    </ul>
  </li>
  <li>A <strong>word form</strong> is the full inflected or derived form of the word
    <ul>
      <li>$\text{cat}, \text{cats}$ would count as 2 word forms</li>
    </ul>
  </li>
  <li>i.e. how many “lemma” vs “wordform”</li>
</ul>

<h3 id="types-and-tokens">Types and Tokens</h3>

\[\text{They picnicked by the pool, then lay back on the grass and looked at the stars.}\]

<p>for instance:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Types</strong> are the number of <strong>distinct words</strong> in a corpus; if the set of words in the vocabulary is $V$, the number of types is the word token vocabulary size $</td>
          <td>V</td>
          <td>$.</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>When we speak about the number of words in the language, we are generally referring to word types.</li>
    </ul>
  </li>
  <li><strong>Tokens</strong> are the total number $N$ of running words.
    <ul>
      <li>If we ignore punctuation, the above Brown sentence has 16 tokens and 14 types:</li>
      <li>each type may be exemplified by multiple tokens</li>
    </ul>
  </li>
</ul>

<h3 id="issues">Issues</h3>

<p>Other than some common issues/choices you need to make in tokenization, such as:</p>

<ul>
  <li>whether or not you should expand <code class="language-plaintext highlighter-rouge">what're</code> to <code class="language-plaintext highlighter-rouge">what are</code></li>
  <li>should we remove <code class="language-plaintext highlighter-rouge">-</code> from <code class="language-plaintext highlighter-rouge">Hewlett-Packard</code>?</li>
  <li>what should we do with <code class="language-plaintext highlighter-rouge">PhD.</code>? Should we treat it as the full acronym?</li>
</ul>

<p>Other difficulties arises as well:</p>

<ul>
  <li>
    <p><strong>Issues in Languages</strong>.</p>

    <p>For instance in German, the word:
\(\text{Lebensversicherungsgesellschaftsangesteller} \to \text{life insurance company employee}\)
is basically four words in one:</p>

    <ul>
      <li>this single word contains four things, so we need to use compound splitter to split into four</li>
    </ul>

    <p>In Chinese, the sentence $\text{姚明进入总决赛}$ can be split into:</p>

    <p><img src="NLP/image-20220124211419440.png" alt="image-20220124211419440" /></p>

    <p>because there is <strong>no space</strong>! How do we split them into tokens?</p>

    <ul>
      <li>implemented using Greedy search (Maximum Marching), until they cannot be split further
        <ul>
          <li>this works if you have a dictionary already</li>
        </ul>
      </li>
      <li>therefore, word tokenization is also called <strong>word segmentation</strong> since there are no spaces in Chinese</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em>: Maximum Marching</p>

<p>it turns out that it worked pretty well in most of the cases. For illustration purposes, we demonstrate them in English:</p>

<p><img src="NLP/image-20220124212048751.png" alt="image-20220124212048751" style="zoom: 67%;" /></p>

<p>note that in languages such as Chinese they actually work pretty well</p>

<ul>
  <li>but in general, modern probabilistic segmentation algorithms perform even better (baseline: Maximum Matching)</li>
</ul>

<h3 id="byte-pair-encoding-for-tokenization">Byte-Pair Encoding for Tokenization</h3>

<p>Another popular way of tokenization is to use the data/corpus to <strong>automatically tell us what tokens should be</strong>. NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use these facts to make decisions about a separate test corpus and its language.</p>

<ul>
  <li>however, a problem is, say we had words such as $\text{low}, \text{new}, \text{newer}$, but see $\text{lower}$ in the test corpus. What should we do?</li>
</ul>

<p>To deal with this unknown word problem, modern tokenizers often automatically ==induce== sets of tokens that include tokens smaller than words, called <strong>subwords</strong>.</p>

<ul>
  <li>Subwords can be arbitrary substrings, or they can be meaning-bearing units like the <strong>morphemes</strong>, e.g. $\text{-est, -er}$
    <ul>
      <li>A <strong>morpheme</strong> is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.</li>
    </ul>
  </li>
  <li>e.g. induce $\text{lower}$ from $\text{low}, \text{-er}$</li>
</ul>

<p>In general, this is done in two steps:</p>

<ol>
  <li>a token <strong>learner</strong>: takes a raw training corpus (sometimes roughly pre-separated into words, for example by whitespace) and ==induces a vocabulary== of tokens that will be used</li>
  <li>a token <strong>segmenter</strong>, takes a raw test sentence and ==segments== it into the tokens <strong>using the vocabulary</strong></li>
</ol>

<p>Three algorithms are widely used:</p>

<ul>
  <li>byte-pair encoding (Sennrich et al., 2016),</li>
  <li>unigram language modeling (Kudo, 2018),</li>
  <li>WordPiece (Schuster and Nakajima, 2012);</li>
</ul>

<p>there is also a <code class="language-plaintext highlighter-rouge">SentencePiece </code>library that includes implementations of the first two of the three</p>

<h2 id="normalization-lemmatization-and-stemming">Normalization, Lemmatization, and Stemming</h2>

<blockquote>
  <p>Word <strong>normalization</strong> is the task of putting words/tokens in a standard format, choosing <strong>a single normal form for words</strong> with multiple forms, i.e. implicitly define equivalence classes of terms</p>

  <ul>
    <li>
      <p>e.g. ‘USA’ and ‘US’ or ‘uh-huh’ and ‘uhhuh’, so that searching for “USA” should match result with “US” as well</p>
    </li>
    <li>
      <p>therefore, useful for Information Retrieval: indexed text &amp; query terms must have same form.</p>

      <p>more e.g.</p>

      <ul>
        <li>Enter: window
Search: window, windows</li>
        <li>Enter: windows
Search: Windows, windows, window</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p><strong>Case Folding</strong></p>

<p>One easy kind of normalization would be case folding: Mapping everything to lower case means that $\text{Woodchuck}$ and $\text{woodchuck}$ are represented <strong>identically</strong>/same form.
\(\text{SAIL} \iff \text{sail}\)</p>

\[\text{US} \iff \text{us}\]

<p>for different applications, we need to ==think about whether if this is appropriate==</p>

<ul>
  <li>helpful for <strong>generalization</strong> in many tasks, such as information retrieval and speech recognition (uses tend to use lower case more often)</li>
  <li>For sentiment analysis and other text classification tasks, information extraction, and machine translation, by contrast, <strong>case can be quite helpful</strong> and case folding is generally not done.</li>
</ul>

<h3 id="lemmatization">Lemmatization</h3>

<blockquote>
  <p><strong>Lemmatization</strong>: task of determining that two words have the same <strong>root</strong>, despite their surface differences</p>

  <ul>
    <li>e.g. the words sang, sung, and sings are forms of the verb sing.</li>
    <li>a <em>lemmatizer</em> maps from all of these to sing.</li>
  </ul>

  <p>For <em>morphologically complex languages</em> like Arabic, we often need to deal with lemmatization</p>
</blockquote>

<p>An example when this is useful would be: in web search, someone may type the string “woodchucks” but a useful system might want to also return pages that mention “woodchuck” with no “s”.</p>

<ul>
  <li>i.e. words with same lemma should be considered as the “same”/a match</li>
</ul>

<p>More examples include:
\(\{\text{car, cars, car's, cars'}\} \to \text{car}\)</p>

\[\text{He is reading detective stories} \to \text{He be read detective story}\]

<p>where:</p>

<ul>
  <li>
    <p>another helpful case for this would be doing Machine Translation (MT)</p>
  </li>
  <li>
    <p>how is this done? The most sophisticated methods for lemmatization involve complete ==morphological parsing of the word==.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Morphology</strong> is the study of morpheme the way <strong>words are built up</strong> from smaller meaning-bearing units called <strong>morphemes</strong>.</p>

  <p>Two broad classes of morphemes can be distinguished:</p>

  <ul>
    <li><strong>stems</strong> — the central morpheme of the word, supplying the main meaning</li>
    <li><strong>affix</strong> — adding “additional” meanings of various kinds.</li>
  </ul>

  <p>For example, the word $\text{fox}$ consists of one morpheme (the morpheme fox) and the <strong>word $\text{cats}$ consists of two</strong>: the morpheme $\text{cat}$ and the morpheme $\text{-s}$.</p>
</blockquote>

<h3 id="stemming">Stemming</h3>

<p>Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of <strong>chopping off word-final</strong> ==stemming== affixes. This naive version of morphological analysis is called <strong>stemming</strong>.</p>

<blockquote>
  <p><strong>Stemming</strong> refers to a <strong><em>simpler</em></strong> version of <strong>lemmatization</strong> in which we mainly just strip suffixes from the end of the word</p>
</blockquote>

<p>For instance:
\(\text{automate, automatic, automation} \iff \text{automat}\)
And for paragraphs:</p>

<p><img src="NLP/image-20220124222030375.png" alt="image-20220124222030375" /></p>

<p>sometimes Stemming is not very useful</p>

<ul>
  <li>often used in <strong>information retrieval</strong> to condense/compress information</li>
  <li>one popular algorithm would be the <strong>Porter’s Algorithm</strong></li>
</ul>

<h4 id="porters-algorithm">Porter’s Algorithm</h4>

<p>In short, The algorithm is based on series of rewrite rules run in series, as a <strong>cascade</strong>, in which the output of each pass is fed as input to the next pass:</p>

<p><img src="NLP/image-20220124171350348.png" alt="image-20220124171350348" /></p>

<p>where:</p>

<ul>
  <li>step 1b: removes <code class="language-plaintext highlighter-rouge">ing/ed</code></li>
  <li>again, works pretty well for simple texts</li>
</ul>

<h2 id="sentence-segmentation">Sentence Segmentation</h2>

<p>Another important step in preprocessing (related to <strong>tokenization</strong>) would be separating into sentences. The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points. However:</p>

<ul>
  <li><strong>Question marks and exclamation points</strong> are relatively <strong>unambiguous</strong> markers of sentence boundaries</li>
  <li><strong>Periods</strong>, on the other hand, are more <strong>ambiguous</strong>.
    <ul>
      <li>e.g. abbreviations such as Mr. or Inc., numbers such as $4.3$</li>
    </ul>
  </li>
</ul>

<p>In general, it can be solved using machine learning models or hand-written rules:</p>

<ul>
  <li>Rule-based: Stanford CoreNLP toolkit (Manning et al., 2014),
    <ul>
      <li>for example sentence splitting is rule-based, a deterministic consequence of tokenization</li>
      <li>hand-written rules basically hard-coded rules (<code class="language-plaintext highlighter-rouge">if/elif/...</code>) or related to regular expressions</li>
    </ul>
  </li>
  <li>Machine Learning: decision tree models
    <ol>
      <li>Looks at a “.”</li>
      <li>Decides EndOfSentence/NotEndOfSentence</li>
      <li>Classifiers: hand-written rules, regular expressions, or machine-learning</li>
    </ol>
  </li>
</ul>

<hr />

<p><em>For Example</em>: Machine Learning model for Sentence Segmentation</p>

<p>Often we just use a decision tree for this</p>

<ol>
  <li>
    <p>Construct features:</p>

    <p><img src="NLP/image-20220124171830656.png" alt="image-20220124171830656" style="zoom:50%;" /></p>

    <p>how to choose good features is the key part.</p>
  </li>
  <li>
    <p>Run the Decision Tree. Output could look like:</p>

    <p><img src="NLP/image-20220124172059642.png" alt="image-20220124172059642" /></p>
  </li>
</ol>

<p>Or, we can use other classifiers such as Logistic Regression, SVM, NN, etc.</p>

<h1 id="minimum-edit-distance">Minimum Edit Distance</h1>

<p>Much of natural language processing is concerned with measuring <strong>how similar two strings</strong> are.</p>

<ul>
  <li>e.g. popping up autocompletion</li>
</ul>

<blockquote>
  <p><strong>Edit distance</strong> gives us a way to quantify both of these intuitions about string similarity.</p>

  <ul>
    <li>e.g. $ \text{graffe}$ with $ \text{giraffe}$</li>
  </ul>

  <p><strong>Minimum edit distance</strong> between two strings is defined as the ==minimum number of (weighted) editing operations== (operations like insertion, deletion, substitution) needed to transform one string into another.</p>

  <ul>
    <li>e.g.  $ \text{graffe}$ to $ \text{giraffe}$ would require one</li>
    <li>e.g. $\text{intention}$ and $\text{execution}$, for example, is $5$ (without weighting)
      <ul>
        <li>(delete an $i$, substitute $e$ for $n$, substitute $x$ for $t$, insert $c$, substitute $u$ for $n$)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>But the problem is, how do we <strong>algorithmically do it</strong>. First, we may want to visualize the result:</p>

<p><img src="NLP/image-20220126184007787.png" alt="image-20220126184007787" /></p>

<p>here we are:</p>

<ul>
  <li>given an <strong>alignment</strong>: a correspondence between substrings of the two sequences.
    <ul>
      <li>$I$ aligns with the empty string, $N$ with $E$, and so on</li>
    </ul>
  </li>
  <li>below the aligned string is <strong>operation list</strong> for converting the top string into the bottom string: $d$ for deletion, $s$ for substitution, $i$ for insertion</li>
  <li>therefore, the minimum edit distance is $5$ because this is already the best alignment</li>
</ul>

<p>Additionally:</p>

<ul>
  <li>We can also assign a particular cost or weight to each of these operations. The <strong>Levenshtein distance</strong> between two sequences is the simplest weighting factor in which each of the three operations has a cost of $1$</li>
  <li>An alternative version of his metric in which each insertion or deletion has a cost of $1$ and <strong>substitutions are not allowed.</strong>
    <ul>
      <li>This is equivalent to allowing substitution, but giving each substitution a ==cost of $2$== since any substitution can be represented by one insertion and one deletion</li>
    </ul>
  </li>
</ul>

<h2 id="minimum-edit-distance-algorithm">Minimum Edit Distance Algorithm</h2>

<blockquote>
  <p><em>Intuition</em></p>

  <p>Our final aim is to produce something like this</p>

  <p><img src="NLP/image-20220126190229994.png" alt="image-20220126190229994" /></p>

  <p>Imagine some string (perhaps it is $ \text{exention}$) that is ==in this optimal path== (whatever it is).</p>

  <ul>
    <li>if $ \text{exention} $ is in the optimal operation list, then the optimal sequence must also <strong>include the optimal path</strong> from
$ \text{intention }$ to $ \text{exention}$.</li>
    <li>all the other path we can forget</li>
  </ul>

  <p>Then, iteratively compute it:</p>

  <ol>
    <li>from $ \text{intention}$ to $ \text{*}$ (empty string, base case)</li>
    <li>from $ \text{intention}$ to $ \text{e}$</li>
    <li>from $ \text{intention}$ to $ \text{ex}$</li>
    <li>from $ \text{intention}$ to $ \text{exe}$</li>
    <li>etc</li>
    <li>from $ \text{intention}$ to $ \text{execution}$</li>
  </ol>

  <p>In reality since you can also start from $\text{execution}$, it would be a <strong>2D matrix</strong></p>
</blockquote>

<p>Basically this is solved by <strong>dynamic programming</strong>, which are solutions that apply a table-driven method to solve problems by <strong>combining solutions to sub-problems</strong></p>

<p>Before showing the main algorithm, we use the notation that:</p>

<ul>
  <li>$*$ is an <strong>empty string</strong></li>
  <li>$X,Y$ are inputs of length $n,m$</li>
  <li>$X[1..i],Y[1..j]$ are the first $i$ characters of $X$, and first $j$ characters of $Y$</li>
  <li>we will be filling up a matrix, call it $D$. Our result is basically $D[n,m]$</li>
</ul>

<p>Then, the update rule is simply</p>

<p><img src="NLP/image-20220126191018501.png" alt="image-20220126191018501" /></p>

<p>i.e. choosing the best path <strong>given the previous bests</strong>, which are $D[i-1,j], D[i,j-1], D[i-1,j-1]$.</p>

<ul>
  <li>we used $ \text{sub-cost}$ for substitution cost so that user can specify the weights.</li>
  <li>here, we will use $\text{sub-cost}=2$, $\text{del-cost}=\text{ins-cost}=1$</li>
</ul>

<p>Therefore, the algorithm is:</p>

<p><img src="NLP/image-20220126191416847.png" alt="image-20220126191416847" /></p>

<p>Therefore, it can be easily seen that:</p>

<ul>
  <li>Space Complexity: $O(nm)$</li>
  <li>Time complexity: $O(nm)$</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<ol>
  <li>
    <p>First we initialize</p>

    <p><img src="NLP/image-20220126191709962.png" alt="image-20220126191709962" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>Then, we fill it up row by row</p>

    <p><img src="NLP/image-20220126191552866.png" alt="image-20220126191552866" style="zoom:67%;" /></p>

    <p>which works since for each computation, the 3 inputs $D[i-1,j], D[i,j-1], D[i-1,j-1]$ would have already been computed.</p>

    <ul>
      <li>we could also go column by column, from bottom up</li>
    </ul>

    <p><img src="NLP/image-20220126191745000.png" alt="image-20220126191745000" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>fill up and return the top right</p>

    <p><img src="NLP/image-20220126191918416.png" alt="image-20220126191918416" style="zoom: 67%;" /></p>
  </li>
</ol>

<p>Note that, the optimal path at each stage is outlined here (done by backtracking)</p>

<p><img src="NLP/image-20220126192033839.png" alt="image-20220126192033839" style="zoom: 67%;" /></p>

<p>The value $3$ here would mean the minimal cost to go from $ \text{IN}$ to $ \text{E}$. We can follow the path:</p>

<ol>
  <li>cost $0$ from start</li>
  <li>cost $1$ to go from $I \to *$
    <ul>
      <li><strong>delete</strong> $I$</li>
    </ul>
  </li>
  <li>cost $3$ to go from $ \text{IN} \to \text{E}$
    <ul>
      <li><strong>replace</strong> $N \to E$</li>
    </ul>
  </li>
</ol>

<h2 id="alignment">Alignment</h2>

<p>Recall that an alignment basically is the <strong>optimal path</strong> for edit distance:</p>

<p><img src="NLP/image-20220126184007787.png" alt="image-20220126184007787" /></p>

<p>Therefore, since it is just the optimal path, we only need to store, for each $D[i,j]$. where did the best previous path come from:</p>

<p><img src="NLP/image-20220126193638591.png" alt="image-20220126193638591" style="zoom:67%;" /></p>

<p>since technically a cell $D[i,j]$ could be optimally reached from several path, the <strong>result is NOT unique</strong></p>

<ul>
  <li>for programming, we usually just pick the first in the list</li>
  <li>when done, to <strong>perform the backtrace</strong>, i.e. draw the optimal path from the table, we <strong>need $O(n+m)$</strong></li>
</ul>

<p>The graphical result looks like:</p>

<p><img src="NLP/image-20220126193807284.png" alt="image-20220126193807284" style="zoom: 67%;" /></p>

<hr />

<p><em>For Example</em>:</p>

<p>Consider how this part comes about from our optimal path</p>

<p><img src="NLP/image-20220126194029765.png" alt="image-20220126194029765" /></p>

<h1 id="language-models">Language Models</h1>

<p>The task we are considering is <strong>predicting the next few words someone is going to say</strong>? What word, for example, is likely to follow
\(\text{Please turn your homework ...}\)
In the following sections we will formalize this intuition by introducing models that assign a ==probability to each possible next word==. The same models will also serve to assign a probability to an entire sentence. So that:
\(P(\text{all of a sudden I notice three guys}) &gt; P(\text{guys all I of notice three a sudden})\)
This would be useful in:</p>

<ul>
  <li>
    <p>identify words in noisy, ambiguous input, like <strong>speech recognition</strong>.</p>
  </li>
  <li>For writing tools like <strong>spelling correction</strong> or grammatical error correction
    <ul>
      <li>$ \text{Their are two midterms}$, in which $ \text{There}$ was mistyped as $ \text{Their}$</li>
    </ul>
  </li>
  <li>Assigning probabilities to sequences of words (sentences) is also essential in <strong>machine translation</strong>.</li>
</ul>

<blockquote>
  <p>Models that ==assign probabilities== to sequences of words are called <strong>language model</strong>, or <strong>LMs</strong>.</p>
</blockquote>

<p>In this chapter we introduce the simplest model that assigns probabilties to sentences and sequences of words, the <strong>n-gram</strong>.</p>

<blockquote>
  <p>An <strong>n-gram</strong> is a sequence of $n$ words:</p>

  <ul>
    <li>a $2$-gram (which we’ll call bigram) is a two-word sequence of words like $ \text{please turn}$, or $ \text{turn your}$</li>
    <li>a 3-gram (a trigram) is a three-word sequence of words like $\text{please turn your}$, or $\text{turn your homework}$.</li>
  </ul>

  <p>We will see how ==n-gram models== to <strong>estimate the probability of the last word</strong> of an n-gram given the previous words, and also to assign probabilities to entire sequences.</p>
</blockquote>

<h2 id="n-grams">N-Grams</h2>

<p>Essentially, we would be interested in computing:
\(P(W_1, W_2, ..., W_n)\)
for assigning probability to a given <strong>sequence of words</strong>.</p>

<ul>
  <li>note that this is a ==joint== probability, which means ==order matters==. e,g. $P(X_1 = 1,X_2 =0) \neq P(X_1=0,X_2=1)$</li>
</ul>

<p>For predicting <strong>next</strong> word:
\(P(W_5| W_1, W_2, ..., W_4)\)</p>

<blockquote>
  <p><strong>Language model</strong>, again, needs to be able to compute those probabilities.</p>
</blockquote>

<p>The question is how? If we naively want to estimate:
\(P(X_1, ...,X_n) \approx \frac{\text{Count}(X_1,...X_n)}{\text{All Sentences}}\)
The problem is that combination of $X_1,…,X_n$ might not even be there for $n$ being large.</p>

<ul>
  <li>doesn’t appear in training set (usually just a large corpus) doesn’t mean chance of occurring is $0$</li>
  <li>need Laplace Smoothing, as you shall see soon</li>
</ul>

<p>We know that a joint:
\(\begin{align*}
P(X_1,...,X_n) 
&amp;= P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)...P(X_n|X_1,...X_{n-1})\\
&amp;= P(X_1)P(X_2|X_{1})P(X_3|X_{1:2})...P(X_n|X_{1:n-1})\\
&amp;= \prod_{i=1}^nP(X_i|X_{1:i-1})
\end{align*}\)
But still, we are left we computing for $i = n$, or any large $i$: 
\(P(X_i|X_1, ..., X_{k-1})= \frac{P(X_1,...,X_{k-1},X_k)}{P(X_1,...X_{k-1})}\)
using law of total probability. Therefore:
\(P(X_i|X_1, ..., X_{k-1}) \approx \frac{\text{Count}(X_1,...,X_{k-1},X_k)}{\text{Count}(X_1,...X_{k-1})}\)
So the <strong>same problem persist</strong>.</p>

<p>Yet we can ==approximate== this using <strong>Markov’s Assumption</strong></p>

<blockquote>
  <p><strong>Markov models</strong> are the class of probabilistic models that assume we can predict the probability of some future unit ==without looking too far into the past==.</p>

  <ul>
    <li>
      <p>thus the n-gram (which looks $n-1$ words into the past only)</p>
    </li>
    <li>
      <table>
        <tbody>
          <tr>
            <td>e.g. $n=2$ means we only consider up to $1$ word in the past, i.e. only $P(X_i</td>
            <td>X_{i-1})$</td>
          </tr>
        </tbody>
      </table>

      <ul>
        <li>so that:
\(P(\text{the}|\text{its water is so transparent that})\to P(\text{the}|\text{that})\)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Therefore, the <strong>n-gram model</strong> basically says:
\(\begin{align*}
P(X_1,...,X_m) 
&amp;= \prod_{i=1}^m P(X_i|X_{i-n+1}, ..., X_{i-1})
\end{align*}\)
Then to formally find out estimates for each $P(X_i|X_{i-n}, …, X_{i-1})$, we consider them as parameters and perform a MLE.</p>

<ul>
  <li>
    <p>in a bigram case, where $n=2$, each parameter $P(X_i|X_{i-1})$ can be estimated using MLE to yield:
\(P_{MLE}(X_i | X_{i-1}) = \frac{\text{Count}(X_{i-1},X_{i})}{\text{Count}(X_i)}\)
notice that the demotivator is a <strong>unigram probability</strong></p>
  </li>
  <li>
    <p>In general:
\(P_{MLE}(X_i | X_{i-n+1:i-1}) = \frac{\text{Count}(X_{i-n+1:i-1},X_{i})}{\text{Count}(X_{i-n+1:i-1})}\)</p>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider our corpus being given as three sentences, the start is measured by <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and end <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code></p>

<p><img src="NLP/image-20220126215730731.png" alt="image-20220126215730731" /></p>

<p>Suppose we are using <strong>bigram</strong>. Then we can compute some of the bigrams being</p>

<p><img src="NLP/image-20220126215751859.png" alt="image-20220126215751859" /></p>

<hr />

<p><em>For Example</em>:</p>

<p>Consider using bigram, and recall that:
\(P_{MLE}(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1},w_{i})}{\text{Count}(w_i)}= \frac{\text{Count}(w_{i-1},w_{i})}{\sum \text{Count}(w_i,w_{i-1})}\)
Let the <strong>unigram probabilities</strong> be given:</p>

<p><img src="NLP/image-20220126215919938.png" alt="image-20220126215919938" style="zoom:80%;" /></p>

<p>And the bigram counts have been filled</p>

<p><img src="NLP/image-20220126220043258.png" alt="image-20220126220043258" style="zoom:80%;" /></p>

<p>Then we can compute the <strong>probability of a sentence</strong></p>

<p><img src="NLP/image-20220126220108341.png" alt="image-20220126220108341" /></p>

<hr />

<p><strong>Some practical issues</strong></p>

<ul>
  <li>
    <p>in practice to generate sentences that makes sense, it is more often to use trigram or 4-gram, depending on how much data you have for training</p>
  </li>
  <li>
    <p>We always represent and compute language model probabilities in <strong>log probability</strong>, e.g. instead of $0.000031$, we use $\log (0.000031)$​.</p>

    <ul>
      <li>
        <p>This is because probabilities are (by definition) less than or equal to $1$, the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in <strong>numerical underflow</strong></p>
      </li>
      <li>
        <p>$\log$ is good since it is <strong>monotonously increasing</strong> anyway</p>
      </li>
      <li>
        <p>this could also speed up computation, because we can convert our probability output
\(\log (p_1 \times p_2 \times ... \times p_n) = \log(p_1) + \log(p_2) + ... + \log(p_n)\)
becomes addition</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="evaluating-language-models">Evaluating Language Models</h2>

<p>In general there are two ways to evaluate:</p>

<ul>
  <li>an <strong>intrinsic</strong> evaluation of a language model we need a <strong>test set</strong>
    <ul>
      <li>so measures the quality of a model independent of any application</li>
    </ul>
  </li>
  <li>an <strong>extrinsic</strong> evaluation: evaluate the performance of a language model by to embedding it in an <strong>application</strong> and measure how much the application improves
    <ul>
      <li>the “real metric”, but far too expensive</li>
      <li>e.g. put into auto-corrector, and compare how many misspelled words corrected properly</li>
    </ul>
  </li>
</ul>

<h3 id="perplexity">Perplexity</h3>

<p>In practice we don’t use <strong>raw probability</strong> as our metric for evaluating language model, but a variant called ==perplexity==</p>

<blockquote>
  <p><strong>Perplexity</strong></p>

  <p>Consider a word sequence/sentence of $W_1, …, W_n$
\(PP(W_1, W_2, ..., W_n) \equiv PP(W)\)
Then:
\(\begin{align*}
PP(W)
&amp;= P(W_1,W_2, ..., W_n)^{-1/n}\\
&amp;= \sqrt[n]{\frac{1}{P(W_1,W_2, ..., W_n)}}\\
&amp;= \sqrt[n]{\frac{1}{\prod_{i=1}^nP(W_i|W_{1:i-1})}}
\end{align*}\)
is the exact definition.</p>
</blockquote>

<p>The intuition of what perplexity measures is as follows:</p>

<ul>
  <li>
    <p>in fact:
\(\text{perplexity} = 2^{\text{entropy}}\)
where:</p>

    <ul>
      <li>
        <p>$ \text{entropy}$ is the average number of <strong>bits</strong> that we need to encode the information</p>
      </li>
      <li>
        <p>so $\exp$ of the entropy is the <strong>total amount of all possible information</strong></p>
      </li>
    </ul>
  </li>
  <li>hence, instead of just computing the probability, consider a measure of the “average of choices per word”. The higher it is, the more “choices” you have, then the more random is your language model.</li>
  <li>therefore, <strong>minimizing perplexity</strong> is equivalent to maximizing the test set probability according to the language model</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>In practice, since this sequence $W$ will cross many sentence boundaries, we need to <strong>include the begin- and end-sentence markers <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code></strong> in the probability computation.</p>

  <p>We also need to include the end-of-sentence marker <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> (but not the beginning-of-sentence marker <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code>) in the total count of word tokens $N$.</p>

  <ul>
    <li>if you think about perplexity as the “average of choices per word”, then it makes sense that we do not consider <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> since it must have always started with that</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em></p>

<p>If we use a Bigram model:
\(PP(W)= \sqrt[n]{\frac{1}{\prod_{i=1}^nP(W_i|W_{i-1})}}\)</p>

<hr />

<p><em>For Example</em>: Computing Perplexity</p>

<p>Consider two models that have trained on <strong>digits</strong>, i.e. $0\sim 9$ and know that:</p>

<ul>
  <li>$P(W_i)=1/10$ is constant</li>
  <li>$P(W_i)=1/90$ but $P(W=0)=9/10$</li>
</ul>

<p>Then if we see a text with $N$ words that look like:
\(\text{0 0 0 3 0 0 3 0 ....}\)
then:</p>

<ul>
  <li>
    <p>the perplexity of the first model:</p>

    <p><img src="NLP/image-20220126224432849.png" alt="image-20220126224432849" /></p>

    <p>where we are ignoring the start <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and end <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> for now</p>
  </li>
  <li>
    <p>the perplexity of the second model is lower since it modelled that on average, <strong>the choice of next of next word being $0$ is high</strong>.</p>
  </li>
</ul>

<hr />

<p>We trained unigram, bigram, and trigram grammars on 38 million words (including start-of-sentence tokens) from the Wall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words.</p>

<p>The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars</p>

<p><img src="NLP/image-20220126224549084.png" alt="image-20220126224549084" /></p>

<p>so basically Trigram is doing the best here.</p>

<h2 id="sampling-sentence-from-lm">Sampling Sentence from LM</h2>

<p>One important way to <strong>visualize</strong> what kind of knowledge a language model embodies is to <strong>sample from it</strong>.</p>

<ul>
  <li>Sampling from a distribution means to choose random points according to their likelihood</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>since our distribution basically are $P(w_i</td>
          <td>w_{i-k+1:i-1})$, we treat this as a distribution and <strong>randomly sample from it</strong></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>For example, suppose we have a Bigram Model. Using <strong>Shannon Visualization Method</strong>:</p>

<p><img src="NLP/image-20220131190017653.png" alt="image-20220131190017653" style="zoom:50%;" /></p>

<p>where:</p>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td>consider $P(w</td>
          <td>\text{<s>})$ as a distribution for all token $w$. **Randomly sample from it**</s></td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>in this case, we got $ \text{I}$</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>consider $P(w</td>
          <td>\text{ I})$ as a distribututoin for all token $w$. Randomly sample from it</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>in this case, we got $ \text{want}$</li>
    </ul>
  </li>
  <li>continues until we sampled $\text{&lt;/s&gt;}$</li>
</ol>

<p>Notice that we did not simply <strong>pick the most probable ones</strong>. Because in the end we will be <strong>more likely to generate sentences</strong> that the model thinks have a <strong>high probability</strong> and less likely to generate sentences that the model thinks have a low probability.</p>

<hr />

<p><em>Other Examples</em></p>

<p>Then, if we you this idea to <strong>generate texts from Shakespeare</strong> (by training on Shakespeare corpus)</p>

<p><img src="NLP/image-20220131163935285.png" alt="image-20220131163935285" style="zoom: 33%;" /></p>

<p>where the output is <strong>almost garbage</strong>. This is because:</p>

<ul>
  <li>we are using a vocab of $V=29066$ unique tokens, with a corpus of total size $884647$ tokens.</li>
  <li>Even Shakespeare only produced around $300,000$ bigrams from the all possible $V^2 = 844 * 10^6$ bigrams. This means that $99.96\%$ of the possible bigrams were <strong>never seen in the training corpus</strong> (have zero entries in the table)
    <ul>
      <li>so if it happens that one of the bigrams appeared in test corpus, we obvious won’t be able to get that.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>So, one big problem for simple <strong>language models</strong> is their ability to <strong>generalize</strong> to unseen texts.</p>
</blockquote>

<p>On the other hand, if the test and training corpus are somewhat similar:</p>

<p><img src="NLP/image-20220131164208075.png" alt="image-20220131164208075" style="zoom:33%;" /></p>

<p>then it seems to work.</p>

<ul>
  <li>this is based on <strong>wall street journal</strong> corpus</li>
</ul>

<h2 id="generalization-and-zeros">Generalization and Zeros</h2>

<p>The n-gram model, like many statistical models, has performance <strong>dependent on the training corpus</strong>.</p>

<p>This brings the <strong>following problems of LM</strong>:</p>

<ul>
  <li>One implication of this is that the probabilities often ==encode specific facts== about a given ==training corpus==.
    <ul>
      <li>e.g. if the corpus is about food, then most of the vocabs/structures will be steered towards that</li>
    </ul>
  </li>
  <li>Another implication is that $n$-grams do a better and better job of modeling the training corpus as we <strong>increase the value of $N$</strong>.
    <ul>
      <li>i.e. <strong>higher order $N$-gram</strong> models tends to perform better</li>
      <li>however, this then requires a ==larger training corpus== or a ==larger storage== for the probability matrix, which could go $O(V^N)$ for a corpus with $V$ vocab and $N$-gram is used.
        <ul>
          <li>though this can be optimized as we are having a sparse matrix</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Models are often subject to the problem of <strong>sparsity</strong>. Because any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it.
    <ul>
      <li>i.e. phrases in test dataset is never seen in train dataset. Then obviously this won’t work</li>
      <li>in real life, this happens</li>
    </ul>
  </li>
</ul>

<hr />

<p>For example: <em>Sparsity</em></p>

<p><img src="NLP/image-20220131191756528.png" alt="image-20220131191756528" /></p>

<p>Then:
\(P( \text{offer} | \text{ denied the}) = 0\)
since it was never seen in.</p>

<ul>
  <li>this happens <strong>very often</strong> in real life applications</li>
  <li>notice that this problem we assumed that we have <strong>seen all words</strong>, but haven’t seen the specific phrase (i.e. this specific combination). Another different problem is that you might <strong>not</strong> have seen the <strong>word</strong>!</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>These <strong>zeros</strong> have two very serious consequences:</p>

  <ol>
    <li>First, their presence means we are <strong>underestimating</strong> the probability of all sorts of words that might occur, which will
hurt the performance of any application we want to run on this data.</li>
    <li>Second, if the probability of any word in the test set is 0, the <strong>entire probability of the test set is 0</strong>.
      <ul>
        <li>By definition, <strong>perplexity</strong> is based on the inverse probability of the test set. Thus if some words have zero probability, we can’t compute perplexity at all, since we can’t divide by 0!</li>
      </ul>
    </li>
  </ol>
</blockquote>

<h3 id="unknown-words">Unknown Words</h3>

<p>Before, we showed a problem we assumed that we have <strong>seen all words</strong>, but haven’t seen the specific phrase (i.e. this specific combination). Another different problem is that you might <strong>not</strong> have seen the <strong>word</strong>!</p>

<blockquote>
  <p>In a <strong>closed vocabulary</strong> system the test set can vocabulary only contain words from this lexicon, and there will be no unknown words</p>

  <p>An <strong>open vocabulary</strong> system vocabulary is one in which we model these potential unknown words in the test set by ==adding a==
==pseudo-word== called <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</p>

  <ul>
    <li>The percentage of OOV (out of vocabulary) open words that appear in the test set is called the OOV rate.</li>
  </ul>
</blockquote>

<p>There are two common ways to train the probabilities of the unknown word model <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</p>

<ol>
  <li>The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance:
    <ol>
      <li>Choose a vocabulary (word list) that is fixed <strong>in advance</strong> (without looking at the training set)</li>
      <li><strong>Convert</strong> in the training set any word that is not in this set (<strong>any OOV word</strong>) to the unknown word token <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> in a text normalization step.</li>
      <li>Estimate the ==probabilities for <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> from its counts== just like any other regular word in the training set.</li>
    </ol>
  </li>
  <li>Second alternative, in situations where we don’t have a prior vocabulary in advance, is to create such a vocabulary implicitly
    <ol>
      <li>replace words in the training data by <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> based on their <strong>frequency</strong> (i.e. threshold it).
        <ul>
          <li>e.g. replace by <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> all words that occur fewer than $n$ times in the training set</li>
        </ul>
      </li>
      <li>proceed to train the language model as before, treating <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> like a regular word.</li>
    </ol>
  </li>
</ol>

<p>Then, when testing our model, we use the probability with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> when seeing an unknown word.</p>

<ul>
  <li>i.e. use that when computing probability of seeing a text/perplexity</li>
</ul>

<p>However, The exact choice of <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> model does <strong>have an effect on metrics</strong> like perplexity.</p>

<ul>
  <li>A language model can achieve <strong>low perplexity</strong> by choosing a small vocabulary and ==assigning the unknown word a high probability==.</li>
  <li>For this reason, perplexities should only be compared across language models with the same vocabularies</li>
</ul>

<h2 id="smoothing">Smoothing</h2>

<p>Now, we go back to the problem that <strong>vocabs are seen</strong>, but the specific <strong>phrase/combination</strong> are not seen.</p>

<blockquote>
  <p><em>Intuition</em></p>

  <p>To keep a language model from assigning zero probability to these unseen events, we’ll have to shift a bit of probability mass <strong>from</strong> some more frequent events and <strong>give it to the events we’ve never seen</strong>.</p>

  <ul>
    <li>This modification is called ==smoothing== or ==discounting==</li>
  </ul>
</blockquote>

<p>So for instance:</p>

<p><img src="NLP/image-20220131221619172.png" alt="image-20220131221619172" style="zoom:50%;" /></p>

<p>In general, there are many ways to do smoothing:</p>

<ul>
  <li>Laplace (add-one) smoothing,</li>
  <li>add-k smoothing</li>
  <li>interpolation</li>
  <li>stupid backoff</li>
  <li>Kneser-Ney smoothing</li>
</ul>

<h3 id="laplace-smoothing">Laplace Smoothing</h3>

<p>In reality, one ==simple== way is to use <strong>Laplace Smoothing</strong></p>

<blockquote>
  <p><strong>Laplace Smoothing</strong>: add one to all the n-gram counts, before we normalize them into probabilities..</p>

  <ul>
    <li>does <strong>not</strong> perform well enough to be used smoothing in modern $n$-gram models</li>
    <li>but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful <strong>baseline</strong></li>
  </ul>
</blockquote>

<p>Let us first recall that our <strong>MLE estimate</strong> without smoothing was like (for Bigram)
\(P_{MLE}(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1},w_{i})}{\text{Count}(w_i)}= \frac{\text{Count}(w_{i-1},w_{i})}{\sum \text{Count}(w_i,w_{i-1})}\)
we needed $\sum \text{Count}(w_i,w_{i-1})$ because it is $ \text{Count}(w_i,w_{i-1})$, i.e. the element in our bigram frequency matrix that gets <strong>added 1</strong>.</p>

<ul>
  <li>also notice that we <strong>don’t need gradient descent</strong> type algorithm, as this is a ==probabilistic model==</li>
</ul>

<p>Therefore, the Add-1 estimate is simply:
\(P_{Laplace}(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1},w_{i}) + 1}{\sum( \text{Count}(w_{i-1},w_{i}) + 1)}=\frac{\text{Count}(w_{i-1},w_{i}) + 1}{\text{Count}(w_{i-1}) + V}\)
where notice that:</p>

<ul>
  <li>$V$ is the size of vocabulary, which makes sense since $P$ needs to add up to 1</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>When added smoothing the <strong>bigram frequency matrix</strong> looks like</p>

<p><img src="NLP/image-20220131223032810.png" alt="image-20220131223032810" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>many of the $1$ entries were $0$ before hand.</li>
  <li>Basically all entries are added $1$ to it</li>
</ul>

<p>Now we can <strong>compute the conditional probability</strong></p>

<p><img src="NLP/image-20220131223135921.png" alt="image-20220131223135921" style="zoom:50%;" /></p>

<p>To see the <strong>smoothing effect more easily</strong>, we can reconstruct the <em>count/frequency matrix</em> by treating:
\(P_{ \text{Laplace}}(w_n | w_{n-1}) \equiv \frac{ \text{Count}^*(w_{n-1}w_n)}{\text{Count}(w_{n-1})} = \frac{\text{Count}(w_{i-1},w_{i}) + 1}{\text{Count}(w_{i-1}) + V}\)
Then we can easily get:
\(\text{Count}^*(w_{n-1}w_n) =\frac{(\text{Count}(w_{i-1},w_{i}) + 1 ) * \text{Count}(w_{n-1})}{\text{Count}(w_{i-1}) + V}\)
This means that the frequency becomes:</p>

<p><img src="NLP/image-20220131224038444.png" alt="image-20220131224038444" style="zoom: 67%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>In reality:</p>

  <ul>
    <li>add-1 isn’t used for N-grams. We’ll see better methods</li>
    <li>But add-1 is used to smooth other NLP models
      <ul>
        <li>e.g. for text classification, or other domains where the number of zeros isn’t so huge.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="add-k-smoothing">Add-k Smoothing</h3>

<p>Why add-1? Sometimes you may want to shift <strong>a bit less of the probability</strong> to unseen events, so that you might pick $k=0.5$, or $k=0.05$ for instance to:
\(P_{Laplace}(w_i | w_{i-1}) = \frac{\text{Count}(w_{i-1},w_{i}) + k}{\sum( \text{Count}(w_{i-1},w_{i}) + k)}=\frac{\text{Count}(w_{i-1},w_{i}) + k}{\text{Count}(w_{i-1}) + kV}\)
where:</p>

<ul>
  <li>now we may want to know what $k$ is the most optimal. This can be done, for example, by optimizing on a devset.</li>
</ul>

<h3 id="backoff-and-interpolation">Backoff and Interpolation</h3>

<p>Again, recall that the problem is no examples of a particular trigram (for example), $w_{n-2}w_{n-1}w_{n}$ is seen such that:
\(P(w_n|w_{n-2}w_{n-1}) = 0\)
However, we can maybe just <strong>rely on/estimate its probability by using the bigram probability $P(w_n | w_{n-1})$.</strong></p>

<ul>
  <li>or basically any <em>lower order N-gram model</em></li>
  <li>in other words, sometimes using <strong>less context</strong> is a good thing</li>
</ul>

<p>Therefore, the idea is then:</p>

<blockquote>
  <p>In <strong>backoff</strong>, we use the trigram (e.g.) if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram.</p>

  <ul>
    <li>“back off” to a lower-order n-gram if we have zero evidence</li>
  </ul>

  <p>In <strong>interpolation</strong>, we always ==mix the probability estimates== from all the n-gram estimators</p>

  <ul>
    <li>e.g. produced a weighting and combining the trigram, bigram, and unigram counts</li>
    <li>this usually works better than backoff</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em>: Simple Linear Interpolation</p>

<p><img src="NLP/image-20220131230002807.png" alt="image-20220131230002807" style="zoom:80%;" /></p>

<p>where:</p>

<ul>
  <li>$\sum \lambda_i = 1$, otherwise we can choose the weighting</li>
  <li>but we can do better as choosing which one to weight more by context</li>
</ul>

<hr />

<p><em>For Example</em>: More sophisticated Linear Interpolation</p>

<p>If we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams ==based on this bigram== will be more trustworthy, so we can make the ls for those trigrams higher.</p>

<ul>
  <li>i.e. $\lambda$s differ based on which bigram $w_{n-2}w_{n-1}$ we are conditioning on</li>
</ul>

<p>Then, the parameter $\lambda$ will be huge:</p>

<p><img src="NLP/image-20220131230531023.png" alt="image-20220131230531023" style="zoom:80%;" /></p>

<ul>
  <li>in general, both the simple interpolation and conditional are <strong>learned/optimized from a held-out corpus</strong>.
    <ul>
      <li>A held-out corpus is an additional training corpus, so-called because we hold it out <em>from the training data</em>, that we use to <strong>set hyperparameters</strong> like these l values.</li>
      <li>There are various ways to find this optimal set of $\lambda$ s. One way is to use the EM algorithm, an iterative learning algorithm that
converges on locally optimal $\lambda$s</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>Example: <strong>Katz backoff</strong></em></p>

<p>In the end, we want to produce a <strong>probability distribution</strong>
\(P(w_n | w_{n-N+1:n-1})\)
if we naively just do:
\(P(w_n | w_{n-N+1:n-1}) = \begin{cases}
P(w_n | w_{n-N+1:n-1}), &amp; \text{if Count}(w_{n-N+1:n}) &gt; 0\\
P(w_n | w_{n-N+2:n-1}), &amp; \text{otherwise}
\end{cases}\)
we notice that this <strong>will exceed $1$</strong> since just summing up over $P(w_n | w_{n-N+1:n-1})$ already reaches $1$.</p>

<p>Therefore, we need to:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>discount some probability mass from $P(w_n</td>
          <td>w_{n-N+1:n-1})$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>for lower-order $n$-gram, we need a factor $\alpha$ to distribute it such that everything adds up to 1</li>
</ul>

\[P_{ \text{BO}}(w_n | w_{n-N+1:n-1}) = \begin{cases}
P^*(w_n | w_{n-N+1:n-1}), &amp; \text{if Count}(w_{n-N+1:n}) &gt; 0\\
\alpha ( w_{n-N+1:n-1})\cdot P_{\text{BO}}(w_n | w_{n-N+2:n-1}), &amp; \text{otherwise}
\end{cases}\]

<p>where:</p>

<ul>
  <li>we recursively back off to the Katz probability for the shorter-history ($N-1$)-gram if count is zero.</li>
  <li>$P^*$ is basically discounted $P$ but also sometimes with smoothing. Commonly used smoothing method here is called <strong>Good-Turing</strong>.
    <ul>
      <li>The combined <strong>Good-Turing backoff algorithm</strong> involves quite detailed computation for estimating the Good-Turing smoothing and the $P^*$ and $\alpha$ values</li>
    </ul>
  </li>
</ul>

<h2 id="kneser-ney-smoothing">Kneser-Ney Smoothing</h2>

<p>One of the most commonly used and best performing n-gram smoothing methods is the <strong>interpolated Kneser-Ney algorithm</strong>, which has its root from <strong>absolute discounting</strong></p>

<blockquote>
  <p><strong>Absolute discounting</strong> is basically, say that given a $n$-gram model and count of each $n$-gram, <strong>how much should we discount it</strong> so that it is closest to the test dataset?</p>

  <ul>
    <li>Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm</li>
  </ul>
</blockquote>

<p>For instance, for a Bigram model, it has been found that the following are the training and heldout dataset</p>

<p><img src="NLP/image-20220131233252358.png" alt="image-20220131233252358" style="zoom:80%;" /></p>

<p>notice that:</p>

<ul>
  <li>if we subtract off $0.75$ for Bigrams with count $2 \sim 9$, it seems to be a pretty close estimate</li>
</ul>

<p>Therefore, <strong>absolute discounting for Bigram</strong> models looks like:</p>

<p><img src="NLP/image-20220131233349379.png" alt="image-20220131233349379" /></p>

<p>where:</p>

<ul>
  <li>$d$ is the mount you discounted, e.g. $0.75$</li>
  <li>the second term is the unigram with an <strong>interpolation</strong> a conditional weight $\lambda$.</li>
</ul>

<blockquote>
  <p><strong>Kneser-Ney discounting</strong> (Kneser and Ney, 1995) augments absolute discounting with a more ==sophisticated way== to handle the ==lower-order unigram distribution==.</p>

  <p>Essentially instead of blindly picking the most probable $P(w_i)$ if you don’t know what to do next, pick the $w_i$ that is <strong>likely to appear in a new/novel context</strong></p>
</blockquote>

<p>For instance, consider that you are predicting the next word of:
\(\text{I can’t see without my reading \_\_.}\)
to predict the next word:</p>

<ul>
  <li>if we basically used $P(w_i)$ for interpolation, then words like $ \text{Kong}$ might appear more often as a unigram than $ \text{glasses}$</li>
  <li>basically, if we are only to guess the next word, use the one that <strong>appears more context</strong>/has a wider distribution
    <ul>
      <li>i.e. $ \text{Kong}$ could have only appeared after $ \text{Hong}$, but $ \text{glasses}$ would have appeared ==after many different types of words==</li>
    </ul>
  </li>
</ul>

<p>Therefore, we would be doing, for <strong>Interpolated Kneser-Ney smoothing for bigrams</strong></p>

<p><img src="NLP/image-20220131234213040.png" alt="image-20220131234213040" /></p>

<p>where:</p>

<ul>
  <li>the $ \max$ is there since we might go negative in count if $d$ is large</li>
  <li>$P_\text{CONTINUATION}$ would be our approximate of how “versatile” a word/phrase would be.</li>
</ul>

<p>In the case of <strong>Bigram</strong>, $P_{ \text{CONTINUATION}}$ is formalized as
\(P_{ \text{CONTINUATION}}(w) \propto | \{ v: C(vw) &gt; 0 \}|\)
which is basically the number of word types seen to precede $w$. Then, we need to <strong>normalize it</strong>, hence
\(P_{ \text{CONTINUATION}}(w) = \frac{ | \{ v: C(vw) &gt; 0 \}|}{\sum_{w'} | \{ v: C(vw') &gt; 0 \}|}\)
so that:</p>

<ul>
  <li>words such as $ \text{Kong}$ would have a low continuation probability since it only appears after $ \text{Hong}$.</li>
</ul>

<p>Lastly, a formula for $\lambda$ is also provided</p>

<p><img src="NLP/image-20220131234644462.png" alt="image-20220131234644462" /></p>

<p>where the first term is basically a normalizing constant</p>

<hr />

<p><strong>General recursive formulation</strong>:</p>

<p><img src="NLP/image-20220131234713627.png" alt="image-20220131234713627" /></p>

<p>where the definition of the count $c_{KN}$ depends on whether we are</p>

<ul>
  <li>counting the highest-order n-gram being interpolated (for example trigram if we are interpolating trigram, bigram, and unigram)</li>
  <li>or one of the lower-order n-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram)</li>
</ul>

<p><img src="NLP/image-20220131234820143.png" alt="image-20220131234820143" /></p>

<p>where remember that continuation count is the number of unique single word contexts for $\cdot$.</p>

<h2 id="huge-web-scale-n-grams">Huge Web-Scale N-grams</h2>

<p>Essentially those huge web-scale data comes from the internet, so we often refer to them as <strong>huge web-scale</strong> models as well.</p>

<p>Now, <strong>efficiency considerations</strong> are important when building language models that use such large sets of n-grams</p>

<ul>
  <li>store each string as a <strong>64-bit</strong> hash number</li>
  <li>Probabilities are generally quantized using only 4-8 bits</li>
  <li>n-grams are stored in reverse <strong>tries</strong></li>
</ul>

<p>Other techiniques are used to <strong>reduce the size of our model</strong>:</p>

<blockquote>
  <p><strong>Pruning</strong>, for example only storing n-grams with counts greater than some threshold</p>

  <ul>
    <li>count threshold of 40 used for the Google n-gram release</li>
    <li>use entropy to prune <strong>less-important n-grams</strong></li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Bloom filters</strong>: approximate language models</p>
</blockquote>

<p>All of those are used to improve <strong>efficiency</strong> of our model when exposed to ==large corpus==</p>

<h3 id="stupid-backoff">Stupid Backoff</h3>

<p>Although with these above toolkits it is possible to build web-scale language models using full Kneser-Ney smoothing, Brants et al. (2007) show that with <strong>very large language models</strong> a much simpler algorithm may be sufficient.</p>

<ul>
  <li>this algorithm for computing probability is called <strong>stupid backoff</strong></li>
</ul>

<p>Notice that all modifications of the basic N-gram language model is ==modifying this term==:
\(P(w_i | w_{i-n+1:i-1})\)
Stupid backoff basically ==gives up== the idea of trying to make the above quantity a ==true probability distribution== (i.e. sums up to 1).</p>

<ul>
  <li>no discounting of the higher-order probabilities.</li>
  <li>simply backoff to a lower order n-gram, <strong>weighed</strong> by a fixed (context-independent) weight</li>
</ul>

<p><img src="NLP/image-20220202184949652.png" alt="image-20220202184949652" /></p>

<p>where, since this is no longer a probability distribution, $S$ instead of $P$ is used:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>the upper term is essentially $P(w_i</td>
          <td>w_{i-n+1:i-1})$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>the lower term essentially is a <strong>decaying weight</strong> the lower n-gram you are using</li>
  <li>using $\lambda = 0.4$ seems to work well</li>
</ul>

<h2 id="summary">Summary</h2>

<p>Here we mainly introduced the following techniques/modifications to the basic N-gram model:</p>

<ul>
  <li><strong>smoothing</strong>: takes care of the case when word/phrase is unseen
    <ul>
      <li>Add-1/Laplacian Smoothing is easy</li>
      <li>Kneser-Ney works well</li>
    </ul>
  </li>
  <li><strong>backoff and interpolation</strong>
    <ul>
      <li>use other probabilities of lower-order n-gram to fill up the gap when count is 0</li>
      <li>require discounting to create a probability distribution</li>
      <li>Kneser-Ney Smoothing: discounting + continuation probability</li>
    </ul>
  </li>
  <li><strong>dealing with large corpus</strong>
    <ul>
      <li>efficiency tricks</li>
      <li>stupid backoff: no longer a probability distribution</li>
    </ul>
  </li>
</ul>

<h1 id="spelling-correction">Spelling Correction</h1>

<p>A big task in real life:</p>

<ul>
  <li>Estimates for the frequency of spelling <strong>errors</strong> in human-typed text vary from 1-2% for carefully retyping already printed text to 10-15% for web queries</li>
</ul>

<p>The task idea is two fold:</p>

<ul>
  <li>how do you <strong>detect</strong> spelling errors?</li>
  <li>how do you <strong>correct</strong> spelling errors?
    <ul>
      <li>for correction, some common application uses either <em>autocorrect</em>, <em>suggest a single correction</em>, or <em>suggest a list</em></li>
    </ul>
  </li>
  <li><strong>together</strong>, we call them <strong>spelling correction</strong></li>
</ul>

<p>The task of spelling correction has two broad categories to work with:</p>

<ul>
  <li><strong>non-word spelling correction</strong></li>
  <li><strong>real-word spelling correction</strong></li>
</ul>

<blockquote>
  <p><strong>Non-word spelling correction</strong> is the detection and correction of spelling errors that result in non-words (like graffe for giraffe).</p>

  <p>Then, the task involves:</p>

  <ul>
    <li>Non-word errors are ==detected== by looking for any word <strong>not found in a dictionary</strong>. (modern systems often use enormous dictionaries derived from the web.)</li>
    <li>==corrected== by first generate <strong>candidates</strong>: real words that have <em>a similar letter sequence</em> to the error</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Real word spelling correction</strong> is the task of detecting and correcting spelling errors even if they accidentally result in an <em>actual real word of English</em>.</p>

  <ul>
    <li>due to <strong>typographical errors</strong>, from insertion/deletion/transposition (e.g., there for three)</li>
    <li>due to <strong>cognitive errors</strong>, substituted the wrong spelling of a homophone or near-homophone (e.g., dessert for desert, or piece for peace).</li>
  </ul>

  <p>Then, the task involves:</p>

  <ul>
    <li>==detection== is a much more difficult task, since <strong>any word</strong> in the input text could be an error</li>
    <li>==correction== will use the noisy channel to find <strong>candidates</strong> for <strong>each word $w$ typed by the user</strong>, and rank the correction that is most likely to have been the user’s original intention
      <ul>
        <li>the trick is to <strong>include the word typed by the user</strong> itself in the <strong>candidate set</strong></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>When correcting, both problems will involve the following component:</p>

<ul>
  <li>rank the candidates using a <strong>distance metric</strong> (e.g. edit distance)</li>
  <li>but we also prefer corrections that are <strong>more frequent words</strong>, or more likely to occur in the context of the error.
    <ul>
      <li>this can then be done by <strong>Noisy Channel Model</strong></li>
    </ul>
  </li>
</ul>

<h2 id="noisy-channel">Noisy Channel</h2>

<p>In this section we introduce the noisy channel model and show how to apply it to the task of <strong>detecting and correcting spelling errors</strong>. An image of what we need to do is the following:</p>

<p><img src="NLP/image-20220202192513059.png" alt="image-20220202192513059" style="zoom: 67%;" /></p>

<p>basically the idea is similar to the <strong>probabilistic interpretation of logistic regression</strong>:</p>

<ul>
  <li>
    <p>This channel introduces “<strong>noise</strong>” in the form of substitutions or other changes to the letters, making it hard to recognize the “<strong>true</strong>” word. i.e.
\(w' = w + \epsilon\)
where $w$ is the word we <strong>observe</strong>, $w$ is the <strong>true word</strong>, $\epsilon$ is some random noise we ==want to model==</p>
  </li>
  <li>
    <p>we need to find the word $w$ that generated this misspelled word $w’$.</p>
  </li>
</ul>

<p>Therefore, the idea is clear:</p>

<ol>
  <li>model this <strong>noisy channel</strong> $\epsilon$</li>
  <li>pass every word of the language through our model of the noisy channel</li>
  <li>pick the $w$ that comes <strong>closest to observed word $w’$</strong></li>
</ol>

<p>Then, we essentially consider the user <strong>input $x$</strong> which is misspelled, but $w$ is the real intention so that:
\(x \equiv w' = w + \epsilon\)
We want to find out $\hat{w}$ that is <strong>most probably generated $x$</strong>:
\(\begin{align*}
\hat{w}
&amp;= \arg\max_{w \in V} P(w|x)\\
&amp;= \arg\max_{w \in V} \frac{P(x | w)P(w)}{P(x)}\\
&amp;= \arg\max_{w \in V} P(x | w)P(w)
\end{align*}\)
then</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(x</td>
          <td>w)$ represents the probability a word $w$ <strong>generated misspelled $x$</strong> from this noisy channel</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>this would be approximated from the training corpus (which contains <strong>both a misspelled version and the correct version</strong>)</li>
    </ul>
  </li>
  <li><strong>prior</strong> probability of a hidden word is modeled by $P(w)$. This represents the probability of seeing the word $w$ ==at that location/in place of $x$==. This hints at us to use Language models such as Bigram, Trigram, etc.</li>
</ul>

<p>Now, sometimes instead of going over the <strong>full dictionary $V$</strong>, we can</p>

<ol>
  <li><strong>generating a list of candidate</strong> words from $x$, the user input (e.g. misspelled)</li>
  <li>use them instead of $V$</li>
</ol>

<p>Hence the <strong>noisy channel model</strong> is:</p>

<p><img src="NLP/image-20220202194340960.png" alt="image-20220202194340960" /></p>

<p>Then the overall algorithm is this:</p>

<p><img src="NLP/image-20220202195102285.png" alt="image-20220202195102285" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>the most important yet haven’t discussed part is how to estimate $P(x</td>
          <td>w)$, nor $P(w)$ which potentially could depend on many factors such as word context, keyboard position, etc</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>prior in general is computed using <strong>language model</strong>, such as unigram to trigram or 4-gram.</li>
    </ul>
  </li>
  <li>Analysis of spelling error data has shown that the <strong>majority of spelling errors consist of a single-letter change</strong> and so we often make the simplifying ==assumption== that these candidates have an edit distance of $1$ from the error word</li>
  <li>we use extended edit distance so that in addition to insertions, deletions, and substitutions, we’ll add a fourth type of edit, <strong>transpositions</strong>. This version is also called <strong>Damerau-Levenshtein edit distance</strong></li>
</ul>

<p>Now, we discuss how to estimate the two key probability. We will take the example of correcting the misspelled word “$ \text{acress}$”</p>

<hr />

<table>
  <tbody>
    <tr>
      <td>**Estimating $P(x</td>
      <td>w)$**: Simple Model</td>
    </tr>
  </tbody>
</table>

<p>A <strong>perfect</strong> model of the probability that a word will be mistyped would condition on all sorts of factors: who the typist was, whether the typist was lefthanded or right-handed, and so on. However, it turns out that we can do pretty well by just looking at the <strong>local context</strong>.</p>

<table>
  <tbody>
    <tr>
      <td>==A simple model== might estimate, for example, $P( \text{acress}</td>
      <td>\text{across})$ just using the number of times that <strong>the letter $e$ was substituted for the letter $o$</strong> in some large corpus of errors.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>to compute the probability for each edit we’ll need a <strong>confusion matrix</strong> that contains counts of errors.</li>
  <li>In general, a confusion matrix lists the matrix number of times one thing was confused with another.</li>
</ul>

<p>Then, since we have four operations, we get <strong>four confusion matrix</strong>, each for:</p>

<p><img src="NLP/image-20220202202359773.png" alt="image-20220202202359773" style="zoom:80%;" /></p>

<p>An example for substitution confusion matrix is:</p>

<p><img src="NLP/image-20220131173445849.png" alt="image-20220131173445849" style="zoom:50%;" /></p>

<p>(technically the above is $ \text{sub}(y,x)$ by our above definition, but the order is something you can decide by yourself)</p>

<ul>
  <li>
    <p>this type of data would be drawn from a <strong>list of misspellings</strong> like the following</p>

    <p><img src="NLP/image-20220202202619100.png" alt="image-20220202202619100" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>There are lists available on Wikipedia and from Roger Mitton (http://www.dcs.bbk.ac.uk/~ROGER/corpora.html) and Peter Norvig (http://norvig.com/ngrams/).</p>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>Finally, with those counts, we can estimate $P(x</td>
      <td>w)$ as follows:</td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220202202920494.png" alt="image-20220202202920494" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>$w_i$ is the $i$th character of the correct word $w$, $x_i$ is the $i$th character of the misspelled word</li>
  <li>notice that we are assuming again a <strong>single character/operation error</strong>, as our candidates are only 1 edit-distance away</li>
  <li>notice that we are ==not caring about the context== in this part!</li>
</ul>

<p>Example computations look like</p>

<p><img src="NLP/image-20220202203132853.png" alt="image-20220202203132853" style="zoom:50%;" /></p>

<table>
  <tbody>
    <tr>
      <td>where make sure $x</td>
      <td>w$ and $P(x</td>
      <td>w)$ makes sense.</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>**Estimating $P(x</td>
      <td>w)$**: Iterative model</td>
    </tr>
  </tbody>
</table>

<p>An alternative approach used by Kernighan et al. (1990) is to compute the confusion matrices by iteratively using this very spelling error correction algorithm itself.</p>

<ol>
  <li>The iterative algorithm first initializes the matrices with equal values
    <ul>
      <li>e.g. any character is equally likely to be deleted, etc.</li>
    </ul>
  </li>
  <li>spelling error correction algorithm is run on a set of spelling errors. Given the typos and corrections, <strong>update the confusion matrix</strong></li>
  <li>repeat step 1</li>
</ol>

<p>Basically an EM algorithm. Then, once we have the confusion matrix, use the same equation</p>

<p><img src="NLP/image-20220202202920494.png" alt="image-20220202202920494" style="zoom:67%;" /></p>

<p>and the rest is the same.</p>

<hr />

<p><strong>Estimating $P(w)$</strong>: Unigram</p>

<p>Now, recall that our model is:
\(\begin{align*}
\hat{w}
&amp;= \arg\max_{w \in V} P(w|x)\\
&amp;= \arg\max_{w \in V} \frac{P(x | w)P(w)}{P(x)}\\
&amp;= \arg\max_{w \in V} P(x | w)P(w)
\end{align*}\)
so we still need $P(w)$.</p>

<p>If we choose to compute $P(w)$ as a <strong>unigram</strong>, then:</p>

<p><img src="NLP/image-20220202203357036.png" alt="image-20220202203357036" style="zoom:50%;" /></p>

<p>It turns out $ \text{across}$ has a high value!</p>

<p>Unfortunately, this is the wrong choice as the writer’s intention becomes clear from the context: 
\(\text{ . . was called a “stellar and versatile \textbf{acress} whose combination of sass and glamour has defined her. . . }\)
The surrounding words make it clear that $ \text{actress }$ was the intended word. We are <strong>missing context</strong> data from unigram + simple model for $P(x|w)$ based on purely alphabets.</p>

<hr />

<p><strong>Estimating $P(w)$</strong>: Bigram</p>

<p>Now, with bigram, we can compute $P(\text{actress})$ and $P(\text{across})$ as:</p>

<p><img src="NLP/image-20220202203838501.png" alt="image-20220202203838501" style="zoom:80%;" /></p>

<p>which then can be estimated using <strong>bigram</strong> because we know the following:</p>

<p><img src="NLP/image-20220202203901397.png" alt="image-20220202203901397" style="zoom:80%;" /></p>

<p>Finally:</p>

<p><img src="NLP/image-20220202203911300.png" alt="image-20220202203911300" style="zoom:80%;" /></p>

<table>
  <tbody>
    <tr>
      <td>which combining the the $P(x</td>
      <td>w)$ prediction, we gave out the <strong>correct result</strong> that $ \text{actress}$ is more likely.</td>
    </tr>
  </tbody>
</table>

<h2 id="real-word-spelling-errors">Real-Word Spelling Errors</h2>

<p>Recall that another class of problem is that we might have <strong>mistyped but those words exist</strong>.</p>

<ul>
  <li>The noisy channel approach can also be applied to detect and correct those errors as well! But we need a small tweak.</li>
  <li>A number of studies suggest that between 25% and 40% of spelling errors are valid English words</li>
</ul>

<p>For instance:
\(\text{This used to belong to \textbf{thew} queen. They are leaving in about fifteen \textbf{minuets} to go to her house.}\)
How do we use noisy channel that seems to be based on <strong>detecting error from misspelling</strong>?</p>

<hr />

<p>Let’s begin with a version of the noisy channel model first proposed by Mays et al. (1991) to deal with these real-word spelling errors.</p>

<blockquote>
  <p><em>Idea</em>:</p>

  <ol>
    <li>
      <p>takes the input sentence $X = {x1 , x_2 , …, x_n }$</p>
    </li>
    <li>
      <p>Generates a large set of ==candidate correction sentences== $C(X)$</p>

      <ol>
        <li>
          <p>start by generating a set of ==candidate words== $C(x_i)$ for each input word $x_i$, e.g. within edit distance 1</p>
        </li>
        <li>
          <p>generate possible sentences from those words. For instance, if the input is $\text{two of thew}$:</p>

          <p><img src="NLP/image-20220202161842395.png" alt="image-20220202161842395" style="zoom: 33%;" /></p>

          <p>where we already see <strong>a lot of possibility</strong>. To simplify, we ==assume== that every sentence has only one error. Then we have:</p>

          <p><img src="NLP/image-20220202213018817.png" alt="image-20220202213018817" /></p>
        </li>
      </ol>
    </li>
    <li>
      <p>picks the sentence with the highest language model probability
\(\hat{W} = \arg\max_{W \in C(X)} P(W|X)= \arg \max_{W \in C(X)} P(X|W)P(W)\)
where $W$ we know are generated by us.</p>
    </li>
  </ol>
</blockquote>

<p>Now, the problem is we need to estimate:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(X</td>
          <td>W)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$P(W)$ - this can be done using a <strong>language model</strong>, essentially like we are generating sentences from it. (e.g. use a Trigram)</li>
</ul>

<p>Now, the problem is $P(X|W)$. Since we ==assume only one word error==, then we can assume:
\(P(X|W) = P(x_1, ..., x_n | w_1, ..., w_n) = \prod_{i=1}^n P(x_i|w_i)\)
where we assumed each $x_i|w_i$ pair are uncorrelated to each other. Then, if correct, i.e. $x_i = w_i$, we assumed $\alpha$ (as shown below). Hence:
\(P(X|W) = \prod_{i=1}^n P(x_i|w_i) = \alpha^{n-1}P(x_k|w_k)\)
where:</p>

<ul>
  <li>we assumed there is only one error, and the error is on $x_k$.</li>
</ul>

<p>since context information/positional information is encoded into $P(W)$. Then, we can use the simple model that</p>

<p><img src="NLP/image-20220202213308252.png" alt="image-20220202213308252" style="zoom:80%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>essentially $P(w</td>
          <td>w)$, i.e. typed correctly, is probability $\alpha$. Often this is set to $\alpha = 0.95$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>alternatively we can make the distribution <strong>proportional to the edit probability</strong> from the more sophisticated channel model. i.e. we use $P(x</td>
          <td>w)$ estimation by look at the confusion matrices.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Lastly, since constants can be <strong>dropped during $\arg\max_W$</strong>:
\(\hat{W} =\arg \max_{W \in C(X)} P(X|W)P(W) = \text{arg\,max}_{W \in C(X), k\in[1,n]} P(x_k|w_k)P(W)\)
for the single wrong word being at position $k$.</p>

<hr />

<table>
  <tbody>
    <tr>
      <td>For example, if we used the simple estimation for $P(x</td>
      <td>w)$, then $P(X</td>
      <td>W)$ essentially is:</td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220202213655859.png" alt="image-20220202213655859" style="zoom:67%;" /></p>

<p>where here we only looked at the case when we want to correct $ \text{thew}$ in the sentence, which again is a <strong>real word</strong>.</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>finally, don’t forget to <strong>multiply by $P(W)$</strong> to get your final estimate for $P(W</td>
          <td>X)$!</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="noisy-channel-model-the-state-of-the-art">Noisy Channel Model: The State of the Art</h2>

<p>State of the art implementations of noisy channel spelling correction make a number of extensions to the simple models we presented above. Here we go over a few</p>

<ul>
  <li>
    <p>rather than make the assumption that the input sentence has only a single error, modern systems go through the input one word at a time, using the noisy channel to make a decision for that word.</p>
  </li>
  <li>
    <p>we could suffer from <strong>overcorrecting</strong>, replacing correct but rare words with more frequent words</p>

    <ul>
      <li>Google solves this by using a <strong>blacklist</strong>, forbidding certain tokens (like numbers, punctuation, and single letter words) from being changed</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>Instead of just choosing a candidate correction if it has a higher probability $P(w</td>
              <td>x)$ than the word itself, these more careful systems choose to <strong>suggest</strong> a correction $w$ over keeping the non-correction $x$ only if the ==difference in probabilities is sufficiently great==.</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <p>improve the performance of the noisy channel model by changing <strong>how the prior and the likelihood are combined</strong></p>

    <ul>
      <li>we might trust one of the two models more, therefore we use a <strong>weighted combination</strong>
\(\hat{w} = \arg\max_{w \in V} P(x|w)P(w)^\lambda\)
for some constant $\lambda$</li>
    </ul>
  </li>
  <li>For specific confusion sets, such as <em>peace</em>/<em>piece</em>, <em>affect</em>/<em>effect</em>, <em>weather</em>/<em>whether</em>, we can train <strong>supervised classifiers</strong> to draw on many features of the context and make a choice between the two candidates
    <ul>
      <li>achieve very high accuracy for these specific set</li>
      <li>but it requires some handcrafted feature extraction/work</li>
    </ul>
  </li>
  <li>Since many documents are typed, error by nearby keys include more weights
    <ul>
      <li>for instance, used in weighing <strong>edit distance</strong></li>
    </ul>
  </li>
</ul>

<h1 id="neural-networks-in-nlp">Neural Networks in NLP</h1>

<p>Basically the same content as <strong>ML</strong>: review <strong>Perceptron</strong>. This include:</p>

<ul>
  <li>how the does the ==update rule== in a <strong>single perceptron work</strong>?</li>
  <li>perceptron as a <strong>linear classifier</strong></li>
  <li>(maybe checkout SVM)</li>
</ul>

<p>In <strong>DL</strong>, review the chapter of Neural Network and <strong>Backpropagation</strong>.</p>

<ul>
  <li>feed-forward network</li>
  <li>activation functions</li>
  <li>backpropagation
    <ul>
      <li>==this will be tested on Midterm==</li>
    </ul>
  </li>
</ul>

<h2 id="architecture-for-neural-language-model">Architecture for Neural Language Model</h2>

<p>Now, let’s talk about the <strong>architecture</strong> for training a neural language model</p>

<h2 id="comments-on-modeltraining">Comments on Model/Training</h2>

<p>Comments on Training</p>

<ul>
  <li>Many <strong>epochs</strong> (thousands) <strong>may</strong> be required, hours or days of training for large networks.</li>
  <li>To avoid <strong>local-minima</strong> problems, run several trials starting with different random weights (<strong>random restarts</strong>).
    <ul>
      <li>Take results of trial with lowest training set error.</li>
      <li>Build a committee of results from multiple trials (possibly weighting votes by training set accuracy).</li>
    </ul>
  </li>
  <li>Keep a <strong>hold-out validation set</strong> and test accuracy on it after every epoch.
    <ul>
      <li>Stop training when overfitting. Do callbacks/epoch saves</li>
    </ul>
  </li>
</ul>

<p>Comments on Model/Architecture choice:</p>

<ul>
  <li>Trained hidden units can be seen as <strong>newly constructed features</strong> that make the target concept linearly separable in
the transformed space. (e.g. for CNNs, edges, textures, etc.)
    <ul>
      <li>However, the hidden layer can also become a distributed representation of the input in which each individual unit is
<strong>not easily interpretable</strong> as a meaningful feature.</li>
    </ul>
  </li>
  <li>Too few hidden units prevents the network from adequately fitting the data, but too many hidden units can result in over-fitting.
    <ul>
      <li>Use <strong>internal cross-validation</strong> to empirically determine an optimal number of hidden units.</li>
    </ul>
  </li>
</ul>

<h1 id="rnn-and-lstm">RNN and LSTM</h1>

<p>For <strong>RNN</strong>:</p>

<ul>
  <li>review DL notes, on the section of Sequence Models.</li>
</ul>

<blockquote>
  <p><em>Recall</em></p>

  <p>The basic problem of RNN (e.g. Deep RNN or stacked RNN) is that <strong>backpropagation</strong> involves matrices multiple times. This will cause the problem of <strong>vanishing/exploding gradient</strong></p>

  <ul>
    <li>difficult to train a RNN</li>
  </ul>
</blockquote>

<p>Some ideas to solve this problem:</p>

<ul>
  <li>LTSM</li>
  <li>GRU</li>
</ul>

<p>Both of which will also be able to <strong>model long term dependencies</strong></p>

<h1 id="transformers">Transformers</h1>

<p>Essentially you have:</p>

<p><img src="NLP/image-20220207170751180.png" alt="image-20220207170751180" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>many <strong>stacked layers of encoder</strong> and <strong>stacked layers of decoder</strong>
    <ul>
      <li>an encoder essentially is a <strong>transformer block</strong></li>
    </ul>
  </li>
  <li>input to the decoder only comes form the last layer</li>
</ul>

<p>Within each encoder, essentially you will have:</p>

<p><img src="NLP/image-20220207170815940.png" alt="image-20220207170815940" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>feed forward layer basically is a <strong>NN</strong> that feeds forward (layers of linear + nonlinear activation)</li>
</ul>

<p>Checkout the <strong>Transformer</strong> section in the DL notes.</p>

<h2 id="transformer-models">Transformer Models</h2>

<p>Essentially different models use different number of <strong>stacks for encoder/decoder/attention</strong></p>

<ul>
  <li>Models are larger and larger</li>
  <li>
    <p>More computation resources are required</p>
  </li>
  <li>Models example: GPT, BERT, Roberta, T-5</li>
</ul>

<h1 id="vector-semantics-and-embeddings">Vector Semantics and Embeddings</h1>

<p>This chapter essentially deals with <strong>how we represent texts</strong> as vectors.</p>

<p>In more detail, we will use the <strong>distributional hypothesis</strong> that “words that occur in similar contexts tend to have similar meanings.”  to instantiate this linguistic hypothesis, we will see this done by <strong>learning representations</strong> of the meaning of words, called <strong>embeddings</strong>. We will cover:</p>

<ul>
  <li>static embeddings (e.g. SGNS Embedding from Word2Vec)</li>
  <li>contextualized embeddings (e.g. BERT)</li>
</ul>

<p>All of those are done by <strong>self-supervised ways</strong> to learn representations of the input.</p>

<h2 id="lexical-semantics">Lexical Semantics</h2>

<p>The idea is that we want to compute <strong>similarity between words</strong>, i.e. a <strong>vector representation</strong> that achieves this.</p>

<blockquote>
  <p><strong>Lexical Semantics</strong> is the study of word meanings. It includes the study of how words structure their meaning, how they act in grammar and compositionality, and the relationships between the distinct senses</p>
</blockquote>

<p>First of all, recall some terms commonly used in NLP include:</p>

<ul>
  <li><strong>lemma</strong>, basically the “infinitive” form of words, such as “sing”</li>
  <li><strong>wordforms</strong>, the specific form of words, such as “sings, sang”</li>
  <li><strong>word sense</strong>: aspect of meaning of a word (e.g. a word could have many meanings)</li>
</ul>

<p>Some of the important aspects we need to answer for <strong>representing words</strong> include:</p>

<ol>
  <li>
    <p><strong>Word Similarity</strong>: being able to measure similarity between two words is an important component of tasks like question answering, paraphrasing, and summarization</p>

    <ul>
      <li>e.g. synonyms would have high similarity</li>
      <li>but also words such as “cat/dog” would have high similarity</li>
    </ul>
  </li>
  <li>
    <p><strong>Word Relatedness</strong>: the meaning of two words can be related in the sense of <em>association</em></p>

    <ul>
      <li>e.g. the word <em>coffee</em> and the word <em>cup</em> are very related yet have different meanings</li>
      <li>One common kind of relatedness between words is if they belong to the same <strong>semantic field</strong>
        <ul>
          <li>For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Semantic Frames and Roles</strong>: a set of words that denote perspectives or participants in a particular type of event</p>

    <ul>
      <li>e.g. understanding a sentence like <em>Sam bought the book from Ling</em> could be paraphrased as <em>Ling sold the book to Sam</em>, and that <em>Sam</em> has the role of the <em>buyer</em> in the frame and <em>Ling</em> the <em>seller</em></li>
    </ul>
  </li>
  <li>
    <p><strong>Connotation</strong>: aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. Hence very related to <strong>sentiment analysis</strong> of texts!</p>

    <p>Early work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:</p>

    <ul>
      <li><strong>valence</strong>: the pleasantness of the stimulus</li>
      <li><strong>arousal</strong>: the intensity of emotion provoked by the stimulus</li>
      <li><strong>dominance</strong>: the degree of control exerted by the stimulus</li>
    </ul>

    <p>Then, we can represent a word by a 3D vector, each component a score for each of the above. With this, we introduce the revolutionary idea of <strong>word embedding</strong> - represent words as vectors!</p>
  </li>
</ol>

<h2 id="vector-semantics">Vector Semantics</h2>

<p>Vectors semantics is the standard way to represent word meaning in NLP, essentially helping us to build models that takes in texts.</p>

<p>In short, we will consider the merging the idea of:</p>

<ul>
  <li>representing a word as a <strong>vector</strong> (e.g. in 3D space)</li>
  <li><strong>define the meaning</strong> of a word by its ==distribution in language use==, meaning its neighboring words or grammatical environments</li>
</ul>

<p>Therefore, our task is to represent a word as a point in a <strong>multidimensional semantic space</strong> that is <strong>derived</strong> (in ways we’ll see) from the ==distributions of embeddings word neighbors==.</p>

<blockquote>
  <p>Vectors for representing words are called <strong>embeddings</strong> (although the term is sometimes more strictly applied only to <strong>dense</strong> vectors like word2vec (Section 6.8), rather than sparse TF-IDF or PPMI vectors</p>
</blockquote>

<p>In this chapter we will mainly discuss <strong>two commonly used models</strong></p>

<ol>
  <li><strong>TF-IDF</strong>, which is usually used as the baseline, consider the meaning of a word is defined by a simple function of the <strong>counts of nearby word</strong>
    <ul>
      <li>hence, resulting in sparse vectors</li>
    </ul>
  </li>
  <li><strong>Word2Vec</strong>, which is technically a family of algorithms that construct short, <strong>dense</strong> vectors that have useful <strong>semantic properties</strong></li>
</ol>

<p>(We’ll also introduce the <strong>cosine</strong>, the standard way to use embeddings to compute <strong>semantic similarity</strong>, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading)</p>

<h2 id="words-and-vectors">Words and Vectors</h2>

<p>Vector or <strong>distributional models of meaning</strong> (mentioned above) are generally based on a ==co-occurrence matrix,== a way of representing how often words co-occur. Two popular matrices related to this is:</p>

<ul>
  <li>term-document matrix</li>
  <li>term-term matrix</li>
</ul>

<h3 id="vectors-and-documents">Vectors and Documents</h3>

<blockquote>
  <p>In <strong>Term-document matrix</strong>, each <em>row</em> represents a <em>word</em> in the vocabulary and each <em>column</em> represents a <em>document</em> from some collection of documents</p>

  <ul>
    <li>e.g. a document could be a book</li>
  </ul>

  <p>Then, the value would be <strong>number of times</strong> a particular word (defined by the row) occurs in a particular document (defined by the column).</p>
</blockquote>

<p><em>For example</em>, term frequency would look like:</p>

<p><img src="NLP/image-20220209162739783.png" alt="image-20220209162739783" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>notice that <strong>each row</strong> could be treated as a vector for the <strong>word</strong>!</li>
  <li>similarly (but maybe not that useful for now), a <strong>document</strong> can then be represented as a <strong>column vector</strong>
    <ul>
      <li>interestingly, this idea was introduced as as means of <em>finding similar documents for the task of document information retrieval</em>, i.e. having similar column vectors</li>
    </ul>
  </li>
</ul>

<p>This can be useful to determine <strong>similarity between word</strong> if their <strong>vector representation is similar</strong></p>

<p><img src="NLP/image-20220209162905547.png" alt="image-20220209162905547" style="zoom: 50%;" /></p>

<p>for instance, “fool” and “clown” would be more similar than others.</p>

<ul>
  <li>a problem with this is that we need <strong>lots of documents</strong> to represent more accurate information</li>
  <li>vocabulary sizes are generally in the tens of thousands, and the number of documents can be enormous (think about all the pages on the web). Hence the size of the matrix will also be enormous!</li>
</ul>

<h3 id="term-context-matrix">Term-Context Matrix</h3>

<p>An <strong>alternative</strong> to using the <strong>term-document matrix</strong> to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or <strong>term-context matrix</strong></p>

<blockquote>
  <p>In <strong>term-context matrix</strong>: columns are labeled by words rather matrix than documents.</p>

  <ul>
    <li>
      <table>
        <tbody>
          <tr>
            <td>This matrix is thus of dimensionality $</td>
            <td>V</td>
            <td>\times</td>
            <td>V</td>
            <td>$</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>each cell records the number of times the row (target) word and the column (context) word <strong>co-occur in some context</strong> in some training corpus
      <ul>
        <li>e.g. the context could be the document, in which case the cell represents the number of times the two words appear in the same document.</li>
        <li>It is most common, however, to use smaller contexts, generally a <strong>window around the word</strong>, for example of 4 words to the left and 4 words to the right</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>For instance, consider the following training corpus:</p>

<p><img src="NLP/image-20220209234231289.png" alt="image-20220209234231289" style="zoom:50%;" /></p>

<p>we want to compute the term-context matrix for the <strong>target word</strong> highlighted in bold.</p>

<ul>
  <li>where we are taking a window size of $\pm 7$ for context word.</li>
</ul>

<p>Then, if we take <strong>every occurrence</strong> of each word (say strawberry) and count the context words around it, we get a word-word co-occurrence matrix:</p>

<p><img src="NLP/image-20220209234351416.png" alt="image-20220209234351416" style="zoom:50%;" /></p>

<p>note that:</p>

<ul>
  <li>here, the words <em>cherry</em> and <em>strawberry</em> are more similar to each other, and the same goes for <em>digital</em> and <em>information</em></li>
  <li>therefore, if the vector for a word is close to each other, we would expect <strong>the row vector to look similar</strong> (the column vector would be a context vector, which we can choose to ignore in this case)</li>
  <li>in general, the <strong>shorter the window size</strong> captures <strong>more syntactic representation</strong>, and the <strong>longer the window</strong>, the more <strong>semantic representation</strong> (e.g. $L=4\sim 10$)</li>
</ul>

<p>However, this is problematic in reality because if we have <strong>vocabulary size of $50,000$</strong>, then this means that the matrix will have dimension $\mathbb{R}^{50,000 \times 50,000}$, and it will be ==sparse==!</p>

<h2 id="tf-idf">TF-IDF</h2>

<p>The co-occurrence matrices above represent each cell by frequencies, either of words with documents (Fig. 6.5), or words with other word. But <strong>raw frequency</strong> does not work directly:</p>

<ul>
  <li>want to know what kinds of contexts are shared by <em>cherry</em> and <em>strawberry</em> but not by <em>digital</em> and <em>information</em>. But there are words that occur frequently with all sorts of words and <strong>aren’t informative about any particular word</strong> (e.g <code class="language-plaintext highlighter-rouge">the</code>, <code class="language-plaintext highlighter-rouge">good</code>, etc)</li>
  <li>essentially, words that occur <strong>too frequent</strong> are sometimes unimportant, e.g. <em>good</em></li>
</ul>

<p>Therefore, the solution is to somehow add <strong>weighting</strong> (essentially the idea of TF-IDF and PPMI as well)</p>

<blockquote>
  <p><strong>TF-IDF</strong> essentially consist of two components:</p>

  <ul>
    <li><strong>term frequency</strong> in log, essentially captures the frequency of words occurring in a document (i.e. the word-document matrix)</li>
    <li><strong>inverse document frequency</strong>, essentially giving a <strong>higher weight</strong> to words that occur <strong>only in a few documents</strong> (i.e. they would carry important discriminative meanings)</li>
  </ul>
</blockquote>

<p>The <strong>term frequency</strong> in a document $d$ is computed by:
\(\text{tf}(t,d) = \log_{10}(\text{count}(t,d) + 1)\)
where:</p>

<ul>
  <li>we added $1$ so that we won’t do $\log 0$, which is negative infinity</li>
</ul>

<p>The <strong>document frequency</strong>  of a term $t$ is the number of documents it occurs in.</p>

<ul>
  <li>note that this is different from <strong>collection frequency</strong> of a term, which is the total number of times the word appears in any document of the whole collection</li>
</ul>

<p>For example: consider in the collection of Shakespeare’s 37 plays the two words <em>Romeo</em> and <em>action</em></p>

<p><img src="NLP/image-20220210001201947.png" alt="image-20220210001201947" style="zoom:50%;" /></p>

<p>Therefore, we want to <strong>emphasize discriminative</strong> words like Romeo via the <strong>inverse document frequency</strong> or IDF term weight:
\(\text{idf}(t) = \log_{10}\left( \frac{N}{\text{df}(t) } \right)\)
where:</p>

<ul>
  <li>apparently $\text{df(t)}$​ is the <strong>number of documents</strong> in which term t occurs and $N$ is the total number of documents in the collection</li>
  <li>so essentially, the fewer documents in which a term occurs, the higher this weight.</li>
  <li>
    <p>notice that we don’t need $+1$ here because the minimum document frequency of a word in your corpus would be $1$</p>
  </li>
  <li>again, because the number could be large, we use a $\log$.</li>
</ul>

<p>Here are some IDF values for some words in the Shakespeare corpus</p>

<p><img src="NLP/image-20220210001906935.png" alt="image-20220210001906935" style="zoom: 50%;" /></p>

<p>so we have:</p>

<ul>
  <li>extremely informative words which occur in only one play like <em>Romeo</em></li>
  <li>those that occur in a few like <em>salad</em> or <em>Falstaff</em></li>
  <li>those which are very common like <em>fool</em></li>
  <li>so common as to be completely non-discriminative since they occur in all 37 plays like <em>good</em> or <em>sweet</em></li>
</ul>

<p>Finally, the TF-IDF basically then does:
\(\text{TF-IDF}(t,d) = \text{tf}(t,d) \times \text{idf}(t)\)
An example would be:</p>

<p><img src="NLP/image-20220210002036715.png" alt="image-20220210002036715" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>essentially it is <strong>still a term-document matrix</strong>, but it is <strong>weighted by IDF</strong>.</li>
  <li>notice that because $\text{idf(good)}=0$​, the row vector for <em>good</em> becomes all zero: this word appears in every document, the tf-idf weighting leads it to be ignored (as it is not very informative anymore)</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>this is by far the most common weighting metric used when we are considering <strong>relationships</strong> of words to documents</li>
    <li>however, not generally used for <strong>word-word similarity</strong>, which PPMI would be better at</li>
  </ul>
</blockquote>

<h2 id="ppmi">PPMI</h2>

<p>An alternative for TF-IDF would be PPMI (positive pointwise mutual information), which is essentially weightings for <strong>term-term-matrices</strong>.</p>

<blockquote>
  <p><em>Intuition</em>:</p>

  <p>PPMI considers the best way to weigh the <strong>association between two words</strong> is to ask how much <strong>more often</strong> the two words <strong>co-occur</strong> in our corpus than we would have a priori <strong>expected them to appear by chance</strong></p>

  <ul>
    <li>i.e. like a ratio of them appearing by chance v.s. actually appearing together in our corpus</li>
  </ul>
</blockquote>

<p>Therefore, this is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:
\(I(x,y) = \log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)\)
where essentially:</p>

<ul>
  <li>probability of them occurring <strong>jointly together</strong> over them occurring <strong>independently</strong></li>
</ul>

<p>Hence, the <strong>pointwise mutual information</strong> between a target word $w$ and a context word $c$ (Church and Hanks 1989, Church and Hanks 1990) is then defined as:
\(\text{PMI}(w,c) = \log_2\left(\frac{P(w,c)}{P(w)P(c)}\right)\)
where notice that:</p>

<ul>
  <li>
    <p>since we are back with <strong>word-context matrix</strong>, we consider target words $w$ and context $c$</p>
  </li>
  <li>each probability would then be estimated by MLE, which basically are counts</li>
  <li>the denominator computes the probability that the two words to co-occur assuming they each <strong>occurred independently</strong>, the numerator of will be them <strong>actually co-occurring</strong></li>
  <li>therefore, it is a useful tool whenever we need to find words that are <strong>strongly associated</strong></li>
  <li>PMI values ranges from negative to positive infinity</li>
</ul>

<p>Now, in reality, the problem with PMI is that <strong>negative PMI values</strong> (which imply things are co-occurring less often than we would expect by chance) tend to be <strong>unreliable</strong> unless our corpora are <strong>enormous</strong>.</p>

<ul>
  <li>To distinguish whether two words whose individual probability is each $10^{-6}$ occur together less often than chance, we would need to be certain that the probability of the two occurring together is significantly different than $10^{-12}$, and this kind of granularity would require an enormous corpus.</li>
</ul>

<p>Therefore, we often turn to the <strong>Positive PMI</strong> (PPMI), which <strong>replaces all negative PMI values</strong> with zero:
\(\text{PPMI}(w,c) = \max\left( \log_2 \frac{P(w,c)}{P(w)P(c)},0 \right)\)
More formally, for our <strong>word-context matrix $F$</strong>, if we have $W$ rows (words) and $C$ columns (contexts), and each element is represented by $f_{ij}$, we can <strong>estimate the probability</strong> by:</p>

<p><img src="NLP/image-20220210005339711.png" alt="image-20220210005339711" style="zoom: 67%;" /></p>

<p>Therefore, the PPMI can be computed as:
\(\text{PPMI}(w,c) = \max\left( \log_2 \frac{p_{ij}}{p_{i*}p_{*j}},0 \right)\)</p>

<hr />

<p><em>For Example</em></p>

<p>Consider the following given statistics:</p>

<p><img src="NLP/image-20220210005520660.png" alt="image-20220210005520660" style="zoom: 67%;" /></p>

<p>we could compute $\text{PPMI(information,data)}$, assuming we pretended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as follows:</p>

<p><img src="NLP/image-20220210005645612.png" alt="image-20220210005645612" style="zoom:67%;" /></p>

<p>Alternatively, notice that essentially the above probabilities are joint and their marginal. So, we can compute the <strong>joint $P(w,c)$</strong> with their marginal shown on the right/bottom:</p>

<p><img src="NLP/image-20220210005924934.png" alt="image-20220210005924934" style="zoom: 67%;" /></p>

<p>where you see the three values are exactly what we used before for PPMI.</p>

<p>Lastly, doing it for all pairs results in:</p>

<p><img src="NLP/image-20220210010050305.png" alt="image-20220210010050305" style="zoom: 67%;" /></p>

<p>so we see that:</p>

<ul>
  <li><em>cherry</em> and <em>strawberry</em> are highly associated with both <em>pie</em> and <em>sugar</em></li>
  <li><em>data</em> is mildly associated with <em>information</em></li>
</ul>

<h3 id="weighted-pmi">Weighted PMI</h3>

<p>PMI is <strong>biased toward infrequent events</strong></p>

<ul>
  <li>Very rare words have very high PMI values, as $P(c)$ could be low</li>
</ul>

<p>Two solutions:</p>

<ul>
  <li>Give rare words slightly higher probabilities (covered here)</li>
  <li>Use add-one smoothing (covered next)</li>
</ul>

<p>Therefore, now we consider:
\(\text{PPMI}(w,c) = \max\left( \log_2 \frac{P(w,c)}{P(w)P_\alpha(c)},0 \right)\)
for:
\(P_\alpha (c) = \frac{\text{Count}(c)^\alpha}{\sum_c \text{Count}(c)^\alpha}\)
for some $\alpha &lt; 1$, so that $P(c)$ is larger for rare words.</p>

<ul>
  <li>empirically using $\alpha = 0.75$ works pretty well</li>
</ul>

<h3 id="laplace-smoothing-1">Laplace Smoothing</h3>

<p>Another way to get rid of the problem is to do <strong>add-k smoothing</strong>:</p>

<ul>
  <li>Before computing PMI, a small constant $k$ (values of 0.1-3 are common) is added to each of the counts:</li>
</ul>

<p>For instance</p>

<p><img src="NLP/image-20220209165806308.png" alt="image-20220209165806308" style="zoom: 50%;" /></p>

<p>Then, the rest of the computation is the same:</p>

<p><img src="NLP/image-20220209170027477.png" alt="image-20220209170027477" style="zoom:33%;" /></p>

<h2 id="cosine-distance">Cosine Distance</h2>

<p>Now, after all the above of embedding, we return to the task of measuring properties such as <strong>similarities</strong> between two <strong>embeddings</strong>:</p>

<ul>
  <li>To measure similarity between two target words $v$ and $w$, we need a metric that takes two vectors and spit out some values</li>
  <li>By far the most common similarity metric is the <strong>cosine of the angle</strong> between the vectors (i.e. normalized dot product)</li>
</ul>

<blockquote>
  <p>Why can we <strong>not use raw dot product</strong>?</p>

  <ul>
    <li>the raw dot product favors <strong>long vectors</strong>, or with <strong>high values in each dimension</strong>, as it is just a sum.</li>
    <li>We do not want that because <strong>more frequent words</strong> have longer vectors, since they tend to <strong>co-occur with more words</strong> and have <strong>higher</strong> co-occurrence values with each of them</li>
    <li>we’d like a similarity metric that tells us how similar two words are <strong>regardless of their frequency</strong></li>
  </ul>
</blockquote>

<p>Therefore, we would like to be able to <strong>normalize them</strong>. Then, essentially we get a <strong>cosine of the angle</strong>:</p>

<p>\(\frac{a \cdot b}{|a||b|} = \cos(\theta_{ab})\)
where:</p>

<ul>
  <li>we can use the $v,w$ vector being the <strong>PPMI vector</strong> for word $v$, or <strong>TF-IDF vectors,</strong> or whatever we want</li>
</ul>

<hr />

<p><em>For Example</em>: Raw Frequency Count</p>

<p>If we have a raw frequency count word-context matrix, such that each word embedding is essentially a <strong>row vector</strong>:</p>

<p><img src="NLP/image-20220210173041179.png" alt="image-20220210173041179" style="zoom:67%;" /></p>

<p>Then, we can compute, for instance the similarity between <em>cheery</em> and <em>information</em> v.s. <em>digital</em> and <em>information</em></p>

<p><img src="NLP/image-20220210173126201.png" alt="image-20220210173126201" style="zoom:67%;" /></p>

<p>where the result here makes sense because <em>information</em> is indeed more similar to <em>digital</em> then <em>cherry</em>!</p>

<hr />

<p><em>For Example</em>: Document Similarity</p>

<p>One way to measure similarity between documents is to use the document vector (if you have), and consider the cosine similarity between them. However, here we introduce another idea:</p>

<ul>
  <li>
    <p>for $k$ words in a document, consider the embedding for each word $w_i$</p>
  </li>
  <li>
    <p>then, compute the <strong>centroid</strong> to be representing the document:
\(d = \frac{w_1 + w_2 + ... + w_k}{k}\)</p>
  </li>
  <li>
    <p>compute $\cos(d_1,d_2)$ of the angles to measure similarly.</p>
  </li>
</ul>

<h3 id="other-similarity-measures">Other Similarity Measures</h3>

<p>Besides cosine similarly, some other common ones are shown below:</p>

<p><img src="NLP/image-20220210173709980.png" alt="image-20220210173709980" style="zoom:50%;" /></p>

<h2 id="using-syntax">Using Syntax</h2>

<p>Before, we cared more about <strong>semantics</strong> as a measure of similarity. Now, we consider words are <strong>similar</strong> if they have similar <strong>syntactic</strong> contexts.</p>

<ul>
  <li>for example, we can <strong>count syntax occurance</strong> instance of just word occurance: 
\(\text{M(“cell”,”absorb”) = count(subj(cell,absorb)) +
count(obj(cell,absorb)) + ...}\)</li>
</ul>

<p>Then, we can also construct a <strong>frequency matrix</strong> where the values represent <strong>count syntax occurrence</strong>. With that, we can then compute quantities such as PMI:</p>

<p><img src="NLP/image-20220210173833488.png" alt="image-20220210173833488" style="zoom: 50%;" /></p>

<p>where in this example, we see that:</p>

<ul>
  <li><em>drink tea</em> and <em>drink liquid</em> is <strong>more associated (syntactically) to each other</strong> than <em>drink it</em>, for instance</li>
</ul>

<h2 id="word-embeddings">Word Embeddings</h2>

<p>We see that the previous examples:</p>

<ul>
  <li>PPMI and context vectors are all <strong>sparse vectors</strong></li>
  <li>we want to save space/encode information by a ==dense vector== - which is often called the ==word embedding==</li>
</ul>

<p>Some common techniques to obtain a dense representation would be:</p>

<ol>
  <li>SVD/PCA/Factor Analysis of the PPMI matrix, or TF-IDF embedding matrix
    <ul>
      <li>since in SVD we can choose to only include the top <strong>$k$ eigenvalues</strong>, we can essentially reduce dimension to $k$</li>
    </ul>
  </li>
  <li>Neural Language Model, embedding using ML
    <ul>
      <li>e.g. skip-gram, using Machine Learning models to train/output <strong>embedding matrix</strong></li>
    </ul>
  </li>
  <li>Brown Clustering</li>
</ol>

<h3 id="svd">SVD</h3>

<p>==will not be on the exam==</p>

<blockquote>
  <p><strong>Goal</strong>:</p>

  <p>Approximate an N-dimensional <strong>co-occurrence matrix</strong> or whatever matrix you like using <strong>fewer dimensions</strong>.</p>
</blockquote>

<p><em>Recall that</em>:</p>

<p>Let $A \in \mathbb{R}^{n \times p}$. Then:
\(A = USV^T\)
is the singular value decomposition where:</p>

<ul>
  <li>$U^TU=I \in \mathbb{R}^{n \times n}$, and $V^TV=I \in \mathbb{R}^{p \times p}$</li>
  <li>The <strong>eigenvectors</strong> of $A^TA$ make up the columns of $V$ , the <strong>eigenvectors</strong> of $AA^T$ make up the columns of $U$</li>
  <li>singular values in $S$ are <strong>square roots</strong> of <strong>eigenvalues</strong> from $AA^T$ or $A^TA$</li>
</ul>

<hr />

<p>To begin with, consider the matrix we want to truncate is $X$:</p>

<p><img src="NLP/image-20220210175311644.png" alt="image-20220210175311644" style="zoom: 50%;" /></p>

<table>
  <tbody>
    <tr>
      <td>Then, since we want to <strong>truncate the dimension</strong>, we would like to have embedding matrix of size $</td>
      <td>V</td>
      <td>\times k$, hence we consider taking <strong>only the $k$ singular values</strong></td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220209172324672.png" alt="image-20220209172324672" style="zoom: 50%;" /></p>

<p>Then we can treat:</p>

<ul>
  <li>$W$ matrix for the <strong>embedding</strong> matrix, $C$ be the <strong>context</strong> matrix, both of which now has only vectors of dimension $k$</li>
  <li>the reason why we take $W$ which is essentially the matrix $U$ in $A=USV^T$ is related to the <strong>PCA</strong> understanding covered next</li>
</ul>

<h3 id="pca">PCA</h3>

<p>The PCA viewpoint requires that one compute the <strong>eigenvalues</strong> and <strong>eigenvectors</strong> of the <strong>covariance matrix</strong>, which is the product:
\(\frac{1}{n-1} XX^T = \frac{1}{n-1}WDW^T\)
where recall that covariance matrices are <strong>symmetric</strong>.</p>

<ul>
  <li>
    <p>$X$ is the matrix where we want to decompose</p>
  </li>
  <li>
    <p>then, we will take the <strong>principle axis in $D$</strong> to embed data</p>
  </li>
</ul>

<p>Its relation to SVD can be seen easily if we consider:
\(X= USV^T\)
then reconstructing the covariance:
\(\frac{1}{n-1}XX^T = \frac{1}{n-1}(USV^T)(USV^T) = \frac{1}{n-1}US^2U^T\)
where it is obvious that:</p>

<ul>
  <li>$U \to W$ is the correspondence between SVD and PCA, also related to why we are taking $W$ to be the word matrix in the above section.</li>
</ul>

<h3 id="skip-gram-embedding">Skip-Gram Embedding</h3>

<p>Some advantage using ML related algorithms for embedding</p>

<ul>
  <li>much <strong>faster</strong> to train than methods such as <strong>SVD</strong></li>
  <li>better performance</li>
  <li>(skip-gram algorithm is one word2vec of two algorithms in a software package called word2vec)</li>
</ul>

<blockquote>
  <p><em>Intuition</em></p>

  <p>We’re going to train a simple <strong>neural network</strong> with a <strong>single hidden layer</strong> to perform a certain task:</p>

  <ul>
    <li>but then we’re not actually going to use that neural network for the task we trained it on!</li>
    <li>Instead, the goal is actually just to ==learn the weights of the hidden layer== - we’ll see that these ==weights== are actually the “<strong>word embeddings</strong>” that we’re trying to learn.</li>
  </ul>

  <p>Additionally:</p>

  <ul>
    <li>
      <p>this can be done in <strong>self-supervision</strong>, which avoids the need for any sort of hand-labeled supervision signal</p>
    </li>
    <li>
      <p>this is <strong>static</strong>, in that it learns one <strong>fixed embedding for each word</strong> in the embeddings vocabulary.</p>
      <ul>
        <li>In the Transformer chapter in DL notes, we will introduce methods for learning <strong>dynamic contextual embeddings</strong> like the popular family of BERT representations, in which the vector for each word is <strong>different in different contexts</strong></li>
      </ul>
    </li>
  </ul>

  <p>This model then can be extended to include negative samplings, which is covered next.</p>
</blockquote>

<table>
  <tbody>
    <tr>
      <td>The goal of our simple neural model is this (e.g. say out vocabulary size is $</td>
      <td>V</td>
      <td>$)</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>given an input target word (e.g. <em>Soviet</em>)
    <ul>
      <li>also perhaps specify a window size (e.g. $L=5$)</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>==input== will be a <strong>one-hot encoded vector</strong> of dimension $</td>
              <td>V</td>
              <td>$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>output probability for each word $w_i \in V$ to be <strong>next to the input word</strong>, within the specified window
    <ul>
      <li>e.g. output probabilities are going to be much higher for words like “<em>Union</em>” and “<em>Russia</em>” than for unrelated words like “<em>watermelon</em>” and “<em>kangaroo</em>”.</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>==output== will **also be vector of dimension $</td>
              <td>V</td>
              <td>$<strong>, but values are the **probability that word will be inside the window</strong></td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>We’ll run the model once per context word within the window defined by $L$</li>
</ol>

<table>
  <tbody>
    <tr>
      <td>Then, our architecture this the follows, say that $</td>
      <td>V</td>
      <td>=10,000$:</td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220214190726243.png" alt="image-20220214190726243" style="zoom: 67%;" /></p>

<p>where we have:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$W \in \mathbb{R}^{</td>
          <td>V</td>
          <td>\times d}$ so that the first layer is $W^T x_t = e_t$ for the $t$-th word</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>$C \in \mathbb{R}^{d \times |V|}$ so that the output layer does:
\(o = \text{Softmax}(C^T e_t) \in \mathbb{R}^{|V|\times 1}\)
is essentially the probability for each word in the vocabulary as a neighbor to $w_t$</p>
  </li>
  <li>
    <p>finally, since we need to output probability, we take <strong>SoftMax</strong> as the activation layer in the end</p>
  </li>
  <li>
    <p>graphically, the two matrices loko like:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">$W$</th>
          <th style="text-align: center">$C$</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220214191348056.png" alt="image-20220214191348056" style="zoom: 67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220214191337883.png" alt="image-20220214191337883" style="zoom: 67%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where we can see that:</p>

    <ul>
      <li>the $e_i = W^T x_i$ for one-hot vector $x_i$ shows that the $i$-th <strong>column of $W$</strong> is the ==word/input embedding for word $w_i$==</li>
      <li>then $o[k] \propto C^Te_i[k]$ for an embedded target word $w_i$ means the <strong>$k$-th row of $C$</strong> is the ==context embedding of $w_k$ relative to $w_i$ being the target==</li>
    </ul>
  </li>
</ul>

<p>Therefore, ==interpretation== of the output would be:
\(p(w_k | w_t) = \frac{\exp(c_k \cdot e_t)}{\sum_{k \in |V|} \exp(c_k \cdot e_t)}\)
is like a <strong>bigram probability</strong>, but here it signified the probability of word $w_k$ being <strong>within the window of $w_t$</strong>.</p>

<p>Finally, we need to define a loss. Our ==objective== is to maximize the probability of the <strong>entire sentence</strong>.
\(\begin{align*}
&amp;\quad\arg\max_\theta \prod_{t=1}^T P(w_{t-L},...,w_{t-1},w_{t+1},...,w_{t+L})\\
&amp;=\arg\max_\theta \prod_{t=1}^T \prod_{-L \le j \le L, j \neq 0} P(w_{t+j}|w_t;\theta)\\
&amp;=\arg\min_\theta -\sum_{t=1}^T \sum_{-L \le j \le L, j \neq 0} \log P(w_{t+j}|w_t;\theta)
\end{align*}\)
note that:</p>

<ul>
  <li>
    <p>$T$ is the length of the entire sequence/sentence, $L$ is the window size</p>
  </li>
  <li>
    <p>essentially this is <strong>negative loss likelihood</strong>, which is the same as ==cross entropy loss==, which recall is basically the loss if you used $q(x)$ to approximate $p(x)$:
  \(H(p,q) = -\sum_x p(x)\log q(x)\)</p>
  </li>
</ul>

<p>Then, we abasically obtained two <strong>embedding matrix $W,C$</strong>:</p>

<ul>
  <li>either just use $W$ as our embedding matrix</li>
  <li>use $W+C^T$</li>
  <li>use $[W,C^T]$ by concatenating</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>It turns out that this simple skipgram is very <strong>related to PPMI matrix</strong></p>

  <ul>
    <li>$WC^T \in \mathbb{R}^{|V|\times |V|}$ can be shown as:
\(WC^T = M_{PPMI}- \log k\)
so that each element in $WC^T$ is related to <strong>association between input $i$ and output word $j$</strong>.</li>
  </ul>
</blockquote>

<hr />

<p><strong>Training</strong>:</p>

<p>Training data comes by feeding it <strong>word pairs</strong> found in our training documents</p>

<p><img src="NLP/image-20220214182056311.png" alt="image-20220214182056311" style="zoom: 67%;" /></p>

<p>then use gradient descent to minimize the loss. In this example a window size of $L=2$ is used.</p>

<h3 id="sgns-embeddings">SGNS Embeddings</h3>

<p>In this section we introduce one method for computing embeddings: <strong>skip-gram SGNS with negative sampling</strong>, sometimes called SGNS.</p>

<ul>
  <li>the previous version <strong>only included positive samples</strong></li>
</ul>

<blockquote>
  <p><em>Intuition</em></p>

  <p>The intuition of ==word2vec== is to train a <strong>classifier</strong> on a binary prediction task: given a word $w$, <strong>how likely is it to show up near another word</strong> $w_i$, e.g. apricot?</p>

  <ul>
    <li>essentially training a logistic regression classifier</li>
    <li>however, now we also try to include <strong>negative samples</strong> to make our model/embedding more robust</li>
  </ul>

  <p>Then, ==while we are learning the likelihood==, we would have ==needed/learnt some representation $\vec{w}$ for a word $w$==, which will be our ==embedding==!</p>
</blockquote>

<p>Therefore, the model is simple:</p>

<ol>
  <li>Treat the target word $w$ and a <strong>neighboring</strong> context word as <strong>positive</strong> examples.</li>
  <li>Randomly sample other words in the lexicon to get negative samples.</li>
  <li>Use <strong>logistic regression</strong> to train a classifier to distinguish those two cases.</li>
  <li>Use the <strong>learned weights</strong> $W$ as the <strong>embeddings</strong>, commonly represented as $E$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>This means that the embedding for word $w_i$ will be similar to $w_j$ if they are <strong>physically close</strong> to each other.</p>

  <ul>
    <li>i.e. if phrases like “bitter sweet” will create problems in the embedding! (physically close but their meanings are different)</li>
  </ul>
</blockquote>

<h4 id="sgns-classifier">SGNS Classifier</h4>

<blockquote>
  <p><strong>Goal of Classifier</strong>:</p>

  <p>Given a tuple $w,c$ being target word $w$ paired with a candidate word $c$, what is the <strong>probability</strong> that $c$ is a <strong>context word</strong> (i.e. physically next to it)?</p>

  <p>To present such <strong>probability</strong>, we will use:
\(P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\)
for $\vec{w},\vec{c}$ being the <strong>embedding of word $w,c$</strong>.</p>

  <ul>
    <li>so on our way to learn such classifier, we would have learnt $\vec{w},\vec{c}$ which will be used for embedding</li>
  </ul>
</blockquote>

<p>Consider we want to find out the <strong>embedding of the word $\text{apricot}$</strong>, and we have the following data:</p>

<p><img src="NLP/image-20220208214715503.png" alt="image-20220208214715503" /></p>

<p>where:</p>

<ul>
  <li>
    <p>let us take a window size of $2$, so that we view $[c_1, …, c_4]$ above as <strong>real context word</strong> for $ \text{apricot}$.</p>
  </li>
  <li>
    <p>our goal is to have a logistic regression such that:
\(P(+|w,c)\)
is <strong>high</strong> if $c$ is a <strong>real context word</strong>, and that:
\(P(-|w,c)  = 1-P(+|w,c)\)
is <strong>high</strong> if $c$ is <strong>not a context word</strong>.</p>
  </li>
</ul>

<p>Then, the question is how do we model such probability? We assumed that <strong>words next to each other</strong> should have ==similar embeddings==. This means that:
\(\text{Similarity}(w,c) \approx \vec{w} \cdot \vec{c}\)
for $\vec{w},\vec{c}$ being the embeddings for the word $w,c$. Then, to map this to <strong>probability that $c$ is a real context word for $w$ as</strong>:
\(P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\)
is high if high dot product = similar = next to each other. Then similarly, probability that $c$ is not a context word as:
\(P(-|w,c) = \sigma(-\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(+\vec{w}\cdot \vec{c})}\)
for $c$ being <strong>negative samples</strong> (not context words).</p>

<hr />

<p>Now, this means that we can also assign “<strong>similarity score</strong>” between a target word and a <strong>context window</strong>:
\(P(+|w, c_{1:L}) = \prod_{i=1}^L \sigma(\vec{c}_i \cdot \vec{w})\)
where:</p>

<ul>
  <li>we assumed <strong>all contexts words are independent</strong></li>
  <li>we can also compute the log probability to make it a sum</li>
</ul>

<hr />

<p>Now, we can think of what are are learning graphically:</p>

<p><img src="NLP/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>where essentially:</p>

<ul>
  <li>Embedding matrix $E$ contains two matrices, $W$ for word embedding and $C$ for context embedding
    <ul>
      <li>i.e. for the $i$-th word (in the dictionary), its word embedding will be the $i$-th column of $W$, and similarly for context embedding</li>
      <li>i.e. every word will have <strong>two embeddings</strong>, one in $W$ and another in $C$. In reality people either only take $W$ or take $W+C$</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>if your vocabulary size is $</td>
              <td>V</td>
              <td>$, and you want an embedding of dimension $d$, then $W,C \in \mathbb{R}^{d \times</td>
              <td>V</td>
              <td>}$ so that you can fetch the embedding from a one-hot vector.</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>we have <strong>two embeddings</strong> because a word $w$ could be treated as a target, but sometimes it might also be picked as a context $c$, in which we update the embedding separately.</li>
</ul>

<h4 id="learning-the-embedding">Learning the Embedding</h4>

<p>Our target is to learn the matrix:</p>

<p><img src="NLP/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>Let us begin with an example text, where our target word currently is $w=\text{apricot}$.</p>

<p><img src="NLP/image-20220208221658252.png" alt="image-20220208221658252" /></p>

<p>Then taking a window size of $2$, we also want to have <strong>negative samples</strong> (in fact, more negative samples than positive ones per target word so that we are called <strong>SGNS</strong>):</p>

<p><img src="NLP/image-20220208221831637.png" alt="image-20220208221831637" /></p>

<p>where here:</p>

<ul>
  <li>
    <p>each of the training sample $(w, c_{pos})$ comes with $k=2$ negative samples, for $k$ being tunable</p>
  </li>
  <li>
    <p>the negative samples are sampled <strong>randomly by</strong>:
\(\text{Prob of sampling $w$}= P_\alpha(w) = \frac{ \text{Count}(w)^\alpha }{\sum_{w'} \text{Count}(w')^\alpha}\)
where we usually take $\alpha = 0.75$ so that <strong>rare words have a  better chance</strong></p>

    <p>e.g. if $P(a)=0.99,P(b)=0.01$, doing the power would give:</p>

    <p><img src="NLP/image-20220208222147320.png" alt="image-20220208222147320" style="zoom:80%;" /></p>
  </li>
</ul>

<p>Then, <strong>given a positive pair and the $k$ negative pairs</strong>, our ==loss function to minimize would be==
\(L_{CE} = -\log{\left[ P(+|w,c_{pos}) \cdot \prod_{i=1}^k P(- |w,c_{neg})\right]} = - \left[ \log \sigma(c_{pos} \cdot w) + \sum_{i=1}^k  \log \sigma(-c_{neg} \cdot w) \right]\)
which we want to <strong>minimize</strong> by <strong>updating $\vec{w} \in W$</strong> for the target word and $\vec{c} \in C$ for the context word.</p>

<p>Therefore, we need to take the derivatives:</p>

<p><img src="NLP/image-20220208222630846.png" alt="image-20220208222630846" style="zoom:80%;" /></p>

<p>Then the update equations</p>

<p><img src="NLP/image-20220208222649078.png" alt="image-20220208222649078" style="zoom:80%;" /></p>

<p>So graphically, we are doing:</p>

<p><img src="NLP/image-20220208222800901.png" alt="image-20220208222800901" style="zoom:80%;" /></p>

<p>notice that we update target word embedding in $W$ but context words in $C$.</p>

<p>Then that is it! You now have leant <strong>two separate embeddings</strong> for each word $i$:</p>

<ul>
  <li>$\vec{w}_i = W[i]$ is the embedding <strong>target embedding</strong> for word $i$</li>
  <li>$\vec{c}_i = C[i]$ is the embedding <strong>context embedding</strong> for word $i$</li>
</ul>

<p>(then it is common to either just add them together as $\vec{e}_i = \vec{w}_i+\vec{c}_i$ as the embedding for word $i$, or just take $\vec{e}_i = \vec{w}_i$.)</p>

<h3 id="cbow-embedding">CBOW Embedding</h3>

<p>Essentially the reverse of Skip-Gram embedding</p>

<blockquote>
  <p><em>Intuition</em>:</p>

  <p>Given $2L$ context words, we need to <strong>predict the missing target word $w_t$</strong>. (we should be able to recover meaning from neighbors!)</p>

  <ul>
    <li>then, while we train our model to do it, we can learn the context embedding $W,W’$ similar to the skip-gram model</li>
    <li>hints at the idea that <strong>words physically close = words in same context</strong> is related to <strong>words being similar</strong></li>
  </ul>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Skip Gram</th>
      <th style="text-align: center">CBoW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220214195156731.png" alt="image-20220214195156731" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220214195149883.png" alt="image-20220214195149883" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where now, our idea becomes:</p>

<ol>
  <li>
    <p>given our hot vectors for context words $x_{t-L},…,x_{t=1},x_{t+1}, …, x_{t+L}$</p>
  </li>
  <li>
    <p>get the embedding for contexts $e_{t-L} = W^T x_{t-L}$, and etc for all the $2L$ vectors</p>
  </li>
  <li>
    <p>take an <strong>average</strong> of the $2L$ embedded vectors:
\(h_t = \frac{1}{2L}(e_{t-L}+ ... + e_{t-1} + e_{t+1}+...+e_{t+L})\)</p>
  </li>
  <li>
    <p>generates the output $z=W’^Th_t$</p>
  </li>
  <li>
    <p>convert the output score to probability $\hat{y} = \text{Softmax}(z)$</p>
  </li>
</ol>

<p>Then, since the correct <strong>label we already know</strong>, let it be $y$. Then the loss is again cross entropy for <strong>each sample:</strong>
\(H(\hat{y},y) = -\sum_{i=1}^{|V|}y_i \log(\hat{y}_i)\)
where:</p>

<ul>
  <li>$\hat{y},y$ essentially represents <strong>probability distributions</strong>, and the above is <strong>loss for a single sample</strong></li>
</ul>

<p>Once done, $W,W’$ would be the embedding matrix for words and context words again.</p>

<h3 id="other-embeddings">Other Embeddings</h3>

<p>Some problem with SGNS Embedding (Word2Vec) is:</p>

<ul>
  <li>what if we want to know the embedding of an <strong>unknown word</strong> (i.e. unseen in the training corpus)?</li>
  <li><strong>sparsity</strong>: in languages with rich morphology, where some of the <em>many forms for each noun and verb may only occur rarely</em></li>
</ul>

<p>There are many other kinds of word embeddings that could deal with those problems. Here we briefly cover two:</p>

<ul>
  <li>
    <p><strong>Fasttext</strong>: Fasttext deals with these problems by using <strong>subword</strong> models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols <code class="language-plaintext highlighter-rouge">&lt;</code> and <code class="language-plaintext highlighter-rouge">&gt;</code> added to each word</p>

    <p>For example, with $n = 3$ the word where would be represented by the sequence <code class="language-plaintext highlighter-rouge">&lt;where&gt;</code> plus the character n-grams:
\(\text{&lt;wh, whe, her, ere, re&gt;}\)
then a skipgram embedding is learned for each constituent n-gram, and the word <code class="language-plaintext highlighter-rouge">where</code> is represented by the <strong>sum of all of the embeddings of its constituent n-grams</strong>. Therefore, ==Unknown words== can then be presented only by the sum of the constituent n-grams!</p>
  </li>
  <li>
    <p><strong>GloVe</strong>: GloVe model is based on capturing <strong>global corpus statistics</strong>. GloVe is based on ratios of probabilities from the word-word cooccurrence matrix, combining the intuitions of count-based models like PPMI while also capturing the linear structures used by methods like word2vec.</p>
  </li>
  <li>
    <p>and many more</p>
  </li>
</ul>

<h3 id="properties-of-embedding">Properties of Embedding</h3>

<p>The embeddings learnt from machine learning related methods have the advantage of <strong>pair-wise similarity</strong> as well:</p>

<p><img src="NLP/image-20220214164550354.png" alt="image-20220214164550354" style="zoom: 67%;" /></p>

<h1 id="sequence-labeling-for-pos-and-named-entities">Sequence Labeling for POS and Named Entities</h1>

<p>Parts of speech (also known as POS) and named entities are useful clues to <strong>sentence structure and meaning</strong>.</p>

<ul>
  <li>
    <p>Knowing whether a word is a <strong>noun or a verb</strong> tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of ==parsing==</p>
  </li>
  <li>
    <p>Knowing if a <strong>named entity</strong> like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like ==question answering==, ==stance detection==, or ==information extraction==:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Information Extraction</th>
          <th style="text-align: center">Semantic Role Labeling</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220214211513858.png" alt="image-20220214211513858" style="zoom: 50%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220214211510110.png" alt="image-20220214211510110" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>In this section, we will also introduce the</p>

<ul>
  <li>task of <strong>part-of-speech tagging</strong>, taking a sequence of words and assigning each word a part of speech like <code class="language-plaintext highlighter-rouge">NOUN </code>or <code class="language-plaintext highlighter-rouge">VERB</code></li>
  <li>task of <strong>named entity recognition (NER)</strong>, assigning words or phrases tags like <code class="language-plaintext highlighter-rouge">PERSON</code>, <code class="language-plaintext highlighter-rouge">LOCATION</code>, or <code class="language-plaintext highlighter-rouge">ORGANIZATION</code></li>
</ul>

<blockquote>
  <p><strong>In general</strong></p>

  <p>Such tasks in which we assign, to each word $x_i$ in an input word sequence, a label $y_i$, so that the output sequence $Y$ has the same length as the input sequence $X$ are called ==sequence labeling tasks.==</p>
</blockquote>

<p>This section covers models including:</p>

<ul>
  <li><strong>generative</strong>: the Hidden Markov Model (HMM)</li>
  <li><strong>discriminative</strong>: Conditional Random Field (CRF), and using NN/Transformers
    <ul>
      <li>ML based methods today outperform HMM</li>
    </ul>
  </li>
</ul>

<h2 id="english-pos">English POS</h2>

<p>Until now we have been using part-of-speech terms like <code class="language-plaintext highlighter-rouge">noun </code>and <code class="language-plaintext highlighter-rouge">verb </code>rather freely. In this section we give more <strong>complete definitions</strong>.</p>

<p>In general, Part of Speech falls into <strong>two categories</strong>: open class and closed class.</p>

<blockquote>
  <p><strong>Closed classes</strong> are those with relatively fixed membership, such as prepositions—new prepositions are rarely coined.</p>

  <p><strong>Open classes</strong> have new words within the class continually being created or borrowed.</p>

  <ul>
    <li>e.g. <code class="language-plaintext highlighter-rouge">nouns</code> have new nouns and <code class="language-plaintext highlighter-rouge">verbs </code>like <em>iPhone</em> or <em>to fax</em></li>
    <li>Four major open classes occur in the languages of the world: <code class="language-plaintext highlighter-rouge">nouns </code>(including proper nouns), <code class="language-plaintext highlighter-rouge">verbs</code>, <code class="language-plaintext highlighter-rouge">adjectives</code>, and <code class="language-plaintext highlighter-rouge">adverbs</code>, as well as the smaller open class of <code class="language-plaintext highlighter-rouge">interjections</code>. English has all five, although not every language does</li>
  </ul>
</blockquote>

<p>Now, there are also <strong>many POS tags</strong> used today. Two common ones are:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Universal Dependencies POS Tags</th>
      <th style="text-align: center">Penn Treebank POS Tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220214205049594.png" alt="image-20220214205049594" style="zoom:80%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220214205135570.png" alt="image-20220214205135570" /></td>
    </tr>
  </tbody>
</table>

<p>where you will see some small subdivisions such as:</p>

<ul>
  <li><strong>Common nouns</strong> include concrete terms like <em>cat</em> and <em>mango</em>, abstractions like <em>algorithm</em>, verb-like terms like <em>pacing</em>
    <ul>
      <li>Many languages, including English, divide common nouns into count nouns and mass noun mass nouns.</li>
      <li><strong>Count nouns</strong> can occur in the singular and plural (<em>goat</em>/<em>goats</em>, <em>relationship</em>/<em>relationships</em>) and can be counted (one <em>goat</em>, two <em>goats</em>).</li>
      <li><strong>Mass nouns</strong> are used when something is conceptualized as a homogeneous group. So <em>snow</em>, <em>salt</em>, and <em>communism</em> are not counted</li>
    </ul>
  </li>
  <li><strong>Proper nouns</strong>, like <em>Regina</em>, <em>Colorado</em>, and <em>IBM</em>, are names of specific persons or entities</li>
  <li>Adverbs can also be subdivided:
    <ul>
      <li><strong>Directional adverbs</strong> or locative adverbs (<em>home</em>, <em>here</em>, <em>downhill</em>) specify the direction or location of some action;</li>
      <li><strong>degree adverbs</strong> (<em>extremely</em>, <em>very**,</em> <em>somewhat</em>) specify the extent of some action, process, or manner property</li>
    </ul>
  </li>
  <li>etc.</li>
</ul>

<blockquote>
  <p>Thrax’s set of <strong>eight parts of speech</strong> (results from liguistics) is the stem of many part of speech system today</p>

  <ul>
    <li>noun, verb, pronoun, preposition, adverb, conjunction, participle, and article</li>
  </ul>
</blockquote>

<p>An example of tagging would include (blue tagged with universal, red tagged with Penn Treebank):</p>

<p><img src="NLP/image-20220214205734029.png" alt="image-20220214205734029" style="zoom:67%;" /></p>

<h2 id="pos-tagging">POS Tagging</h2>

<p>Part-of-speech tagging is the process of <strong>assigning a part-of-speech tag $y_i$ to each word $x_i$</strong> in tagging a text.</p>

<blockquote>
  <p><em>Recall</em></p>

  <p>POS tagging would be useful for subsequent syntactic <strong>parsing</strong> and <strong>word sense disambiguation</strong> (i.e. knowing the correct meaning of the word in different contexts).</p>
</blockquote>

<p>Tagging is a ==disambiguation== task, because words are ambiguous—have more than one possible part-of-speech:</p>

<ul>
  <li><em>book</em> can be a <code class="language-plaintext highlighter-rouge">verb</code> (<em>book</em> that flight) or a <code class="language-plaintext highlighter-rouge">noun </code>(hand me that <em>book</em>).</li>
</ul>

<blockquote>
  <p>The <strong>goal</strong> of POS-tagging is to <strong>resolve these ambiguities</strong>, choosing the <strong>proper tag for the context</strong></p>
</blockquote>

<p>It turns out that the <strong>accuracy</strong> of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high.</p>

<ul>
  <li>One study found accuracies over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu and Dredze, 2019).</li>
  <li>Accuracies on various English treebanks are also 97% (no matter the algorithm; HMMs, CRFs, BERT perform similarly</li>
</ul>

<p>But sometimes, it is the 3% that matters. But anyway here we want to know <strong>how it is done</strong>.</p>

<hr />

<p>First, let us take a look at the task. In general:</p>

<ul>
  <li>most word types (85-86%) are <strong>unambiguous</strong> (<em>Janet</em> is always NNP, <em>hesitantly</em> is always RB)</li>
  <li><strong>ambiguous words</strong>, though accounting for only 14-15% of the vocabulary, are very common
    <ul>
      <li>55-67% of word tokens in running text are ambiguous</li>
    </ul>
  </li>
</ul>

<p>For instance, the word <em>back</em> can have 6 different POS in different contexts:</p>

<p><img src="NLP/image-20220214210940242.png" alt="image-20220214210940242" style="zoom:67%;" /></p>

<p>Nonetheless, many words are easy to disambiguate, because their <strong>different tags aren’t equally likely.</strong> For example, <em>a</em> can be a determiner or the letter <em>a</em>, but the determiner sense is much more likely.</p>

<p>Therefore, a <strong>baseline for POS tagging</strong> would be:</p>

<blockquote>
  <p><strong>Most Frequent Class Baseline</strong>: given an ambiguous word (i.e. can have many tags), <strong>always</strong> assign it the tag which is <strong>most frequent</strong> in the training corpus.</p>

  <ul>
    <li>this algorithm actually already have 92% accuracy</li>
  </ul>
</blockquote>

<p>Some techniques applied today to beat the baseline include:</p>

<p>==Rule-Based==: Human crafted rules based on lexical and other linguistic knowledge.</p>

<ul>
  <li>e.g. token after a verb will be a noun</li>
</ul>

<p>==Learning-Based==: Trained on human annotated corpora like the Penn Treebank.</p>

<ul>
  <li>
    <p><strong>Statistical models</strong>: Hidden Markov Model (HMM), Maximum Entropy Markov Model (MEMM), Conditional Random Field (CRF)</p>
  </li>
  <li>
    <p>Rule learning: Transformation Based Learning (TBL)</p>
  </li>
  <li>
    <p><strong>Neural networks</strong>: Recurrent networks like Long Short Term Memory (LSTMs)</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Preprocessing, <strong>tokenization</strong> (e.g. we don’t tag punctuations) is assumed to have taken place <strong>before inputting to POS algorithms</strong>. This stage should separates and/or disambiguates punctuation, including detecting sentence boundaries.</p>
</blockquote>

<h3 id="problems-with-sequence-labeling">Problems with Sequence Labeling</h3>

<p>Some tasks that is faced with sequence labeling in general:</p>

<ul>
  <li>Not easy to integrate information from category of tokens on both sides.
    <ul>
      <li>potential solution: <strong>bidirectional connections</strong></li>
    </ul>
  </li>
  <li>Difficult to propagate uncertainty between decisions and “collectively” determine the most likely joint assignment of categories to all of the tokens in a sequence.
    <ul>
      <li>potential solution: <strong>beam search</strong> in GNN, or <strong>probabilistic sequence models</strong> such as HMM</li>
    </ul>
  </li>
</ul>

<h2 id="tagging-with-hmm">Tagging with HMM</h2>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a <strong>probability distribution over possible sequences of labels</strong> and chooses the best label sequence.</p>
</blockquote>

<p>Essentially it is a model based on <strong>finite state machine</strong>.</p>

<h3 id="markov-chain">Markov Chain</h3>

<p>A Markov chain makes a very strong assumption that if we want to predict the <strong>future</strong> in the sequence, all that matters is the <strong>current state</strong>.</p>

<p>Consider a sequence of state variables $q_1, …, q_i$:
\(\text{Markov Assumption}: P(q_i=a|q_1,...,q_{i-1}) = p(q_i=a|q_{i-1})\)
(basically the assumption used in <strong>bigram models</strong>)</p>

<p>It is then common to represent the states and probabilities as a <strong>directed graph</strong>:</p>

<p><img src="NLP/image-20220214215527052.png" alt="image-20220214215527052" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>The <strong>states</strong> are represented as <strong>nodes</strong> in the graph, and the <strong>transitions</strong>, with their probabilities, as <strong>edges</strong>.</li>
  <li>A <strong>start distribution $\pi$</strong> is required; setting $\pi = [0.1; 0.7; 0.2]$ for (a) would mean a probability $0.7$ of starting in state 2 (cold), probability $0.1$ of starting in state 1 (hot), etc.</li>
  <li>Figure 8.8b shows a Markov chain for assigning a <strong>probability to a sequence of words</strong> $w_1,…,w_t$
    <ul>
      <li>e.g. sequence <code class="language-plaintext highlighter-rouge">hot hot hot</code> would have a probability of $0.1 \times 0.6 \times 0.6$</li>
      <li>therefore, a Markov chain is useful when we need to compute a ==probability for a sequence of observable events==</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Therefore, key components in a Markov Chain include:</p>

  <ul>
    <li>$Q=q_1,…,q_N$ being a set of $N$ <strong>states</strong></li>
    <li>$A=a_{11},a_{12},…,a_{NN}$ being a <strong>transition probability matrix</strong> $A$
      <ul>
        <li>each element $a_{ij}$ means probability of moving from state $i$ to state $j$</li>
        <li>$\sum_{j=1}^Na_{ij}=1,\forall i$ so that the total probability of transitioning from state $i$ sums up to $1$</li>
      </ul>
    </li>
    <li>$\pi = \pi_1, …, \pi_N$​ being an <strong>initial probability distribution</strong> over states.
      <ul>
        <li>$\pi_i$ is the probability that the Markov chain will start in state $i$.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="hidden-markov-model">Hidden Markov Model</h3>

<p>The reason why we now want to consider <strong>hidden Markova model</strong> is that in reality, our <strong>observation are not states $q_i$</strong>. In fact, we observe words ($o_i$), but we will be interested in the ==sequence of states $q_i$ that most likely generated the sequence of words/observations==.</p>

<ul>
  <li>
    <p>then, essentially each state $q_i$ will be associated with a tag, then we would output a <strong>sequence of tags</strong> and our job is done</p>
  </li>
  <li>
    <p>our goal would be to do:
\(\hat{t}_{1:n} = \arg\max_{t_1,...,t_n} P(t_1,...,t_n|w_1,...,w_n)\)
then we need to find away to calculate this.</p>
  </li>
</ul>

<p>Some components of hidden Markova model include:</p>

<blockquote>
  <p>Therefore, key components in a Hidden Markov Chain include:</p>

  <ul>
    <li>$Q=q_1,…,q_N$ being a set of $N$ <strong>states</strong>
      <ul>
        <li>this will correspond to tags $t_i$ we need</li>
      </ul>
    </li>
    <li>$A=a_{11},a_{12},…,a_{NN}$ being a <strong>transition probability matrix</strong> $A$</li>
    <li>$O=o_1,o_2,…o_T$: a sequence of $T$ observations
      <ul>
        <li>this will correspond to the word drawn from our vocabulary</li>
      </ul>
    </li>
    <li>$B=b_i(o_t)$: <strong>emission probabilities,</strong> each expressing the probability of an observation $o_t$ being generated from a state $q_i$
      <ul>
        <li>essentially, from a tag $t_i$, what is the probability a word $w_t$ will be generated</li>
      </ul>
    </li>
    <li>$\pi = \pi_1, …, \pi_N$​ being an <strong>initial probability distribution</strong> over states.
      <ul>
        <li>$\pi_i$ is the probability that the Markov chain will start in state $i$.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>(alternatively, you can also setup $q_0$ being the <strong>fixed start state</strong>, which has probabilities to other states being the $\pi$ specified above.)</p>

<p>First, we talk about how to <strong>construct a HMM graph and their values</strong> from some given <strong>tagged training corpus</strong>.</p>

<ul>
  <li>
    <p>the transition $a_{ij}$ signifies the <strong>transition probability</strong>:
\(a_{ij}\to P(t_i |t_{i-1}) = \frac{\text{Count}(t_{i-1},t_i)}{\text{Count}(t_i)}\)
for instance, in WSJ corpus <code class="language-plaintext highlighter-rouge">MD </code>occurs 13124 times of which it is followed by <code class="language-plaintext highlighter-rouge">VB </code>10471, for an MLE estimate then gives:
\(P(VB|MD) = \frac{10471}{13124}=0.8\)</p>
  </li>
  <li>
    <p>the <strong>emission probability</strong> $b_i(o_t)$​ represent the probability, given a tag (say <code class="language-plaintext highlighter-rouge">MD</code>), that it will be associated with a given word (say <em>will</em>).
\(b_i(o_t) \to P(w_i|t_i) = \frac{\text{Count}(t_{i},w_i)}{\text{Count}(t_i)}\)
For instance, of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times:
\(P(will|MD) = \frac{4046}{13124} = 0.31\)</p>
  </li>
</ul>

<p>Together, we have essentially <strong>two probabilities $A,B$</strong> to look at. Graphically:</p>

<p><img src="NLP/image-20220214222145077.png" alt="image-20220214222145077" /></p>

<p>where:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>the $A$ probability are the $a_{ij}\to P(t_i</td>
          <td>t_{i-1})$, and the $B$ probability $P(w_i</td>
          <td>t_i)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>therefore, the total <strong>number of parameters</strong> with $\lambda = {A,B}$</li>
  <li>you might want to do <strong>smoothing techniques</strong> here as well, as you are doing counts</li>
  <li>so essentially the hidden state $q_i=t_i$ is <strong>what we are interested in</strong>, and we want to find out the ==best sequence of hidden state==, hence tagging, when ==given a sequence of observations==</li>
</ul>

<p>Formalizing this mathematically, given an observation of $n$ words:
\(\hat{t}_{1:n} = \arg\max_{t_1,...,t_n} P(t_1,...,t_n|w_1,...,w_n)\)
Then, we can simplify this by:
\(\begin{align*}
\hat{t}_{1:n} 
&amp;= \arg\max_{t_1,...,t_n} P(t_1,...,t_n|w_1,...,w_n)\\
&amp;= \arg\max_{t_1,...,t_n} P(w_1,...,w_n|t_1,...,t_n)P(t_1,...,t_n)\\
&amp;\approx \arg\max_{t_1,...,t_n} \prod_{i=1}^nP(w_i|t_i) P(t_1,...,t_n)\\
&amp;\approx \arg\max_{t_1,...,t_n} \prod_{i=1}^n P(w_i|t_i) \prod_{j=1}^n  P(t_j|t_{j-1})\\
&amp;= \arg\max_{t_1,...,t_n} \prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})\\
\end{align*}\)
where:</p>

<ul>
  <li>
    <p>the third equality ==assumed== independence of neighboring words and tags
\(P(t_1,...,t_n|w_1,...,w_n) \approx \prod_{i=1}^nP(w_i|t_i)\)</p>
  </li>
  <li>
    <p>the fourth equality ==assumed== bigram for tags
\(P(t_1,...,t_n) \approx \prod_{i=1}^n P(t_i|t_{i-1})\)</p>
  </li>
  <li>
    <p>the second last equality <strong>is not a nested product</strong>, hence we can simply rewrite it as a single product</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>now, notice that $P(w_i</td>
          <td>t_i)$ and $P(t_i</td>
          <td>t_{i-1})$ are ==known probabilities== (emission and transition). So we can actually compute this task!</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Before we talk about an algorithm that can <strong>solve this optimization problem</strong>, let us talk about the advantage of the this model as a <strong>generative model</strong>.</p>

<hr />

<p><em>For Example</em>: HMM Generation</p>

<p>Using this model, we can also ==generate a sequence of $T$ observations== (i.e. words)</p>

<ol>
  <li>Start from some initial state $q_0$, specified by the $\pi$ distribution</li>
  <li>For $t=1…T$
    <ol>
      <li>transit to another state $q_{t}$ from $q_{t-1}$ based on <strong>transition distribution $a_{ij}$</strong></li>
      <li>pick an observation $o_t$ (i.e. pick a word) based on the <strong>emission distribution</strong> $b_i(o_t)$</li>
    </ol>
  </li>
</ol>

<hr />

<p>Additionally, this model can also perform the <strong>following tasks</strong>:</p>

<ul>
  <li>
    <p><strong>Observation Likelihood:</strong> To classify and order sequences.</p>

    <ul>
      <li>provide a probability score for a given sequence</li>
      <li>simply traverse the states and compute the probability</li>
    </ul>

    <p>For example, given two sequences $O_1={o_1^1,o_2^1,…,o_n^1},O_2={o_1^2,o_2^2,…,o_n^2}$</p>

    <p><img src="NLP/image-20220216162822681.png" alt="image-20220216162822681" style="zoom:33%;" /></p>

    <p>our HMM model could emit the probaility for each of them, and we can take the highest as our prediction. (similar to bigram, but more robust as we are also having syntax information)</p>
  </li>
  <li>
    <p><strong>Most likely state sequence (Decoding):</strong> To tag each token in a sequence with a label.</p>

    <ul>
      <li>the optimization task mentioned before
\(\hat{t}_{1:n} = \arg\max_{t_1,...,t_n} P(t_1,...,t_n|w_1,...,w_n)\)</li>
    </ul>

    <p>essentially spit out a <strong>sequence of hidden states</strong> (in this case, tags)</p>
  </li>
</ul>

<h3 id="observation-likelihood">Observation Likelihood</h3>

<p>Our first problem is to compute the <strong>likelihood of a particular observation</strong> sequence $O={o_1,…,o_n}$</p>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td><strong>Computing Likelihood</strong>: Given an HMM $\lambda = (A,B)$, i.e. the probabilities, and an observation sequence $O$, ==determine the likelihood $P(O</td>
        <td>\lambda) \equiv P_\lambda(O)$.==</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>this can then be used for classifying which sequence has a higher likelihood, for instance</li>
  </ul>
</blockquote>

<hr />

<p><em>Recall</em></p>

<p>For Markov Chain/Bigram model, we would simply say:
\(P(O|\lambda)=P_\lambda(O) = \prod_{i=1}^N P(o_i|o_{i-1})\)
however, in a HMM, emitting $o_i$ means we need to ==first know $q_i$==, which we don’t know.</p>

<ul>
  <li>i.e. we don’t know the hidden sequence that generates the sequence $O$</li>
</ul>

<hr />

<p>To make the idea more concrete, consider the following HMM that computes <strong>how many ice-cream I will eat on each day</strong>, given a sequence of days:</p>

<p><img src="NLP/image-20220216193412037.png" alt="image-20220216193412037" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the number of ice-cream I eat is the <strong>observed event $o_i$</strong></p>
  </li>
  <li>
    <p>but the number of ice-cream I eat <strong>depends on the hidden state $q_i$</strong>, which is the <em>weather on a given day</em></p>
  </li>
</ul>

<p>Suppose we are given the observed sequence $O={o_1=3,o_2=1,o_3=3}$. The question is: ==what is the probability of this happening, i.e. $P_\lambda(O)$==?</p>

<p>Since we only know $P(o_i|q_i)$, i.e. $P(1 | \text{hot})=0.2$, we do know the following:
\(P(O|Q) = P(o_1,...,o_T|q_1,...,q_t) = \prod_{i=1}^T P(o_i | q_i)\)
which is true due to <strong>markov’s assumption</strong>.</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>e.g. $P(3,1,3</td>
          <td>\text{ hot,hot,cold}) = P(3</td>
          <td>\text{hot})P(1</td>
          <td>\text{hot})P(3</td>
          <td>\text{cold})$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Then, we know that:
\(P(O) = \sum_Q P(O|Q)P(Q)\)
where $\sum_Q$ means summing over ==all possible sequences==.</p>

<p>Therefore, notice that we now only need $P(Q)$:
\(P(Q)=P(q_1,...,q_t) = \prod_{i=1}^T P(q_i|q_{i-1})\)
Therefore, combining we get the <strong>probability of observing a sequence $O={o_1,…,o_T}$</strong>
\(P(O)=P(o_1,...,o_t) = \sum_Q P(O|Q)P(Q) =\sum_Q \left(  \prod_{i=1}^T P(o_i | q_i)P(q_i|q_{i-1}) \right)\)
summing over all possible sequences $Q={q_1,…,q_T}$</p>

<ul>
  <li>
    <p>for example, in the ice-cream case, we would have <strong>$2^3=8$ possible cases</strong> for our observation being length $T=3$:
\(P(3,1,3) = P(3,1,3, \text{cold, cold, cold})+P(3,1,3, \text{cold, cold, hot})+...\)</p>
  </li>
  <li>
    <p>for an HMM with $N$ hidden states and an observation sequence of $T$ observations, <strong>there are $N^T$ possible observations.</strong> In practice this would be a pain to compute directly, so we have the following ==forward algorithm==.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Forward Algorithm</strong> is a kind of dynamic programming algorithm that can compltet this task in $O(N^2T)$ by implicitly folding each of these paths into a single <strong>forward trellis</strong></p>
</blockquote>

<table>
  <tbody>
    <tr>
      <td>Essentially, realize that computing $P(3,1</td>
      <td>\text{ hot hot})$ and $P(3,1</td>
      <td>\text{ hot cold})$ results in:</td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220216200606139.png" alt="image-20220216200606139" style="zoom:67%;" /></p>

<p>where the key idea is to realize that they are <strong>sharing the same path</strong>:</p>

<ul>
  <li>therefore, the key insight to speed up computation is to realize that each cell at $t=2$ ==only depends on the previous cells== $t=1$.</li>
</ul>

<p>Let us represent:</p>

<ul>
  <li>
    <p>$\alpha_{t}(j)$ being the probability of <strong>being in state $j$ after the first $t$ observations</strong> = ==emitting $o_t$ at $q_t=j$== This means that:
\(\alpha_{t}(j) = P_\lambda(o_1,...,o_t,q_t = j)\)
e.g. probability of being in $q_2 = \text{hot}$ after seeing the first $1$ observation $o_1=3$. Hence graphically:</p>

    <p><img src="NLP/image-20220216200948999.png" alt="image-20220216200948999" /></p>
  </li>
  <li>
    <p>Therefore, the we can compute it by:
\(\alpha_t(j) = \sum_{i=1}^N \underbrace{\alpha_{t-1}(i)a_{ij}}_{\text{get to state $q_j$}}\quad \underbrace{b_j(o_t)}_{\text{emit $o_t$ at $q_t$}}\)
whic should be pretty clear form the graph, so that:</p>

    <ul>
      <li>$\alpha_{t-1}(i)$ is <strong>previous forward path probability</strong> from the previous time step</li>
      <li>$a_{ij}$ is the <strong>transitioni probability</strong> to state $q_t=j$, i.e. the current focus</li>
      <li>$b_j(o_t)$ is the <strong>state observation likelihood/emission probability</strong> to emit $o_t$ when you are at state $q_t=j$
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>i.e. emitting $P(o_t=3</td>
                  <td>q_t = \text{hot})$</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em>. Given the following HMM, and $O={3,1,3}$</p>

<p><img src="NLP/image-20220216193412037.png" alt="image-20220216193412037" style="zoom: 67%;" /></p>

<p>what would be $\alpha_2(2)$?</p>

<p><em>Solution</em>: Essentially this means $t=2,j=2$, that we are emitting $o_2=1$ at state $q_t=2=\text{hot}$. Hence:
\(\alpha_2(a)=\alpha_{1}(1)\cdot P(H|C)\cdot P(1|H) + \alpha_{1}(2)\cdot P(H|H)\cdot P(1|H)\)
 The answer is essentially</p>

<p><img src="NLP/image-20220216201909089.png" alt="image-20220216201909089" style="zoom:80%;" /></p>

<hr />

<p>With that, the algorithm basically does:</p>

<p>Given a sequence of observation $o_1,…,o_t$</p>

<ol>
  <li>
    <p><strong>Initialization</strong>:
\(\alpha_1(j)=P(o_1,q_1=j) = \pi_j b_j(o_1),\quad 1 \le j \le N\)
i.e. probability of emitting $o_1$ at each state</p>

    <p><img src="NLP/image-20220216203453892.png" alt="image-20220216203453892" style="zoom:50%;" /></p>
  </li>
  <li>
    <p><strong>Recursion</strong>. Iterate for $t=2…T$:
\(\alpha_t(j) = \sum_{i=1}^N \alpha_{t-1}(i)a_{ij}b_j(o_t),\quad 1 \le j \le N\)
graphically:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">$t=2$</th>
          <th style="text-align: center">$t=3$</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220216203622723.png" alt="image-20220216203622723" /></td>
          <td style="text-align: center"><img src="NLP/image-20220216203714068.png" alt="image-20220216203714068" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>Termination</strong>: Sum over the last cells:
\(P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)\)
graphically:</p>

    <p><img src="NLP/image-20220216203747644.png" alt="image-20220216203747644" style="zoom:50%;" /></p>
  </li>
</ol>

<p>Then the actual algorithm pseudo-code:</p>

<p><img src="NLP/image-20220216203846265.png" alt="image-20220216203846265" style="zoom:80%;" /></p>

<p>which you can see the main loops goes to $T\cdot N \cdot N$, hence we have $O(TN^2)$</p>

<h3 id="decoding-the-viterbi-algorithm">Decoding: The Viterbi Algorithm</h3>

<p>Now, we consider <strong>performing POS Tagging</strong> for a given observation $O$. This means essentially we need to compute:
\(\hat{t}_{1:n} = \arg\max_{t_1,...,t_n} P(t_1,...,t_n|w_1,...,w_n)\)
which can be generalized to the task of <strong>sequence labeling</strong>/==most likely sequence==</p>

<blockquote>
  <p><strong>Decoding</strong>: Given as input an HMM $\lambda = (A,B)$ and a sequence of observations $O = o_1,…,o_T$ , find the ==most probable sequence of states $Q=q_1,…,q_T$ .==</p>
</blockquote>

<p>Consider again the example of number of ice-cream eaten per day:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>give a sequence $3,1,3$, what is the best likely weather sequence? e.g. $P(H,H,H</td>
          <td>3,1,3)=?$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>For <strong>each possible sequence</strong>, e.g. $(HHH, HHC, HCH)$, we could run the forward algorithm and compute the likelihood of the observation sequence given that hidden state sequence. This is brute force and takes an ==exponentially large number of state sequences==.</li>
</ul>

<p>Then, the idea is as follows:</p>

<ol>
  <li>
    <p>at each time step $t$, for each state $q_t=j,1 \le j \le N$, we can store a <strong>best path</strong> that arrived there:</p>

    <p><img src="NLP/image-20220216210631958.png" alt="image-20220216210631958" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>Then, once we have computed the probability and <strong>propagated to the end</strong>, we can <strong>backtrace</strong> and get the optimal path</p>
  </li>
</ol>

<p>Therefore, the viterbi algorithm essentially does:</p>

<ol>
  <li>
    <p>consider at each time step $t$, we want to know:
\(v_{t}(j) = \max_{q_1,...,q_{t-1}}P_\lambda(o_1,...,o_t,q_t = j)\)
This can be computed from <strong>iterating over the previous best scores $v_{t-1}$</strong>, taking the best, and <strong>save a “checkpoint/back pointer”</strong>.
\(v_t(j) = \max_{q_1,...,q_{t-1}}\,\, \underbrace{v_{t-1}(i)a_{ij}}_{\text{get to state $q_j$}}\quad \underbrace{b_j(o_t)}_{\text{emit $o_t$ at $q_t$}}\)
note that</p>

    <ul>
      <li>the only difference of this with the <strong>forward algorithm</strong> is that it was 1. taking the sum; 2. no need to store a back pointer/checkpoint</li>
      <li>The reason is that while the forward algorithm needs to produce an observation likelihood, the Viterbi algorithm must produce a probability <strong>and</strong> also the most likely state sequence.</li>
    </ul>

    <p>When we have the back pointers, it look like:</p>

    <p><img src="NLP/image-20220216211645021.png" alt="image-20220216211645021" style="zoom: 67%;" /></p>

    <p>which we only need to backtrack one step per node.</p>
  </li>
  <li>
    <p>Then, all yuo need to do is to <strong>backtracing</strong> the best path to the beginning using the <strong>saved back pointers</strong></p>
  </li>
</ol>

<p>So the algorithm looks like Given a sequence of observation $o_1,…,o_t$</p>

<ol>
  <li>
    <p><strong>Initialization</strong>:
\(\begin{align*}
v_1(j) &amp;= \pi_j b_j(o_1),&amp;1 \le j \le N\\
bt_1(j) &amp;= 0, &amp; 1 \le j \le N
\end{align*}\)
where $bt_1(j)$ means the back pointer of state $q_1=j$ is $q_0$,</p>
  </li>
  <li>
    <p><strong>Recursion</strong>. Iterate for $t=2…T$:
\(\begin{align*}
\alpha_t(j) &amp;= \max_{i=1...N}\,\,v_{t-1}(i)a_{ij}b_j(o_t),\quad 1 \le j \le N\\
bt_t(j)&amp; = \arg\max_{i=1...N}\,\, v_{t-1}(i)a_{ij}b_j(o_t),\quad 1 \le j \le N
\end{align*}\)
so in addition to moving forward, we are saving the best back pointer of $q_t=j$ being $q_{t-1}=i$</p>
  </li>
  <li>
    <p><strong>Termination</strong>: The best score by <strong>comparing</strong> over the last cells:
\(\begin{align*}
\text{Best Score}: \quad &amp;P^* = \max_{i=1...N} v_T(i)\\
\text{Start of Backtrace}: \quad &amp;q_T^* = \arg\max_{i=1...N} v_T(i)
\end{align*}\)</p>
  </li>
</ol>

<p>Hence the algorithm pseudo-code looks like:</p>

<p><img src="NLP/image-20220216212501011.png" alt="image-20220216212501011" style="zoom: 80%;" /></p>

<p>which is again $O(N^2T)$ as it is almost the same as the forward algorithm.</p>

<h2 id="unsupervised-hmm">Unsupervised HMM</h2>

<p><em>Recall that</em></p>

<ul>
  <li>for supervised learning, there is a problem that we may not have those tagged dataset/becomes useless if we want to do it in an different language</li>
  <li>it turns out that we can train a HMM even in an <strong>unsupervised setting</strong>.</li>
</ul>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>Given an observation sequence, $O={o_1, o_2, …, 0_n}$, what set of parameters, $λ=(A,B)$, for a given model <strong>maximizes the probability that this data was generated</strong> from this model $P(O</td>
        <td>\lambda)=P_\lambda(O)$?</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>but now, data ara <strong>unlabelled</strong> so we cannot use counts.</li>
  </ul>
</blockquote>

<hr />

<p><em>Reminder: EM algorithm</em></p>

<p>The idea of EM algorithm is basically as follows:</p>

<ol>
  <li>computing an <strong>initial</strong> estimate for the probabilities</li>
  <li>using those estimates to computing a <strong>better estimate</strong></li>
  <li>repeat</li>
</ol>

<p>In more details:</p>

<ol>
  <li>
    <p>Initialize the parameters arbitrarily</p>
  </li>
  <li>
    <p>Given the current setting of parameters find the <strong>best (soft) assignment</strong> of data samples to the clusters (==Expectation-step==)</p>

    <ul>
      <li>
        <p>this <em>soft partition</em> basically means, we assign <strong>proportion</strong> of the data to <strong>each Gaussian mountain</strong></p>

        <p>e.g. if we have four mountains represented by <code class="language-plaintext highlighter-rouge">X</code> in the below figure, each has a $P_i$ estimate of the data $\vec{x}$ in the circle,</p>

        <p><img src="../../Fall/Machine_Learning/Verma/ML/image-20211130173706229.png" alt="image-20211130173706229" style="zoom:50%;" /></p>

        <p>then, e.g. mountain 3 will get the proportion
\(\frac{P_1}{P_1+P_2+P_3+P_4}\)</p>
      </li>
      <li>
        <p>in the Lloyd’s Algorithm for the K-means, the idea is similar in that we assign the data point <strong>completely (hard)</strong> to the closest cluster (closest to the mean)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Update all the parameters $\theta$ with respect to the current (soft) assignment</strong> that maximizes the likelihood (==Maximization-step==)</p>

    <ul>
      <li>same idea as Lloyd’s algorithm</li>
      <li>this is now basically ==MLE==</li>
    </ul>
  </li>
  <li>
    <p>Repeat until no more progress is made.</p>
  </li>
</ol>

<p>An example of what it looked like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Arbitrary Initialization</th>
      <th style="text-align: center">E-Step, Soft Assignment</th>
      <th style="text-align: center">M-Step, Optimize</th>
      <th style="text-align: center">After 5 Rounds</th>
      <th style="text-align: center">After 20 rounds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220216213626101.png" alt="image-20220216213626101" /></td>
      <td style="text-align: center"><img src="NLP/image-20220216213632341.png" alt="image-20220216213632341" /></td>
      <td style="text-align: center">!<img src="NLP/image-20220216213639649.png" alt="image-20220216213639649" /></td>
      <td style="text-align: center"><img src="NLP/image-20220216213645910.png" alt="image-20220216213645910" /></td>
      <td style="text-align: center"><img src="NLP/image-20220216213652320.png" alt="image-20220216213652320" /></td>
    </tr>
  </tbody>
</table>

<hr />

<blockquote>
  <p>The standard algorithm for HMM training is the <strong>forward-backward</strong>, or <strong>Baum Welch algorithm</strong> (Baum, 1972), a special case of the Expectation-Maximization or <strong>EM algorithm</strong></p>
</blockquote>

<p>A brief sketch of the algorithm is provided below.</p>

<p>Assume an HMM with $N$ states.</p>

<ol>
  <li>Randomly set its parameters $\lambda = (A,B)$
    <ul>
      <li>(making sure they represent legal distributions)</li>
    </ul>
  </li>
  <li>Do until converge (i.e. $\lambda$ no longer changes):
    <ul>
      <li>E Step: Use the forward/backward procedure to determine the probability of various possible state sequences for generating the training data</li>
      <li>M Step: Use these probability estimates to re-estimate values for all of the parameters $λ$</li>
    </ul>
  </li>
</ol>

<p>Note that in general, for EM type algorithms:</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Each iteration</strong> changes the parameters in a way that is <strong>guaranteed to increase t</strong>he likelihood of the data: $P(O</td>
          <td>\lambda )=P_\lambda (O)$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Converges to a local maximum.</li>
</ul>

<h3 id="semi-supervised-hmm">Semi-Supervised HMM</h3>

<blockquote>
  <p><strong>EM Algorithm</strong> can be exploited to deal with:</p>

  <ul>
    <li>a mix of labeled and unlabeled data</li>
    <li>even purely supervised data</li>
  </ul>
</blockquote>

<p>When you have a mix of labeled and unlabeled data.</p>

<ol>
  <li>
    <p>Use supervised learning on labeled data to initialize the parameters (instead of initializing them randomly).</p>
  </li>
  <li>
    <p>Essentially <strong>soft label for unlabelled</strong> data, but <strong>hard assignments for labelled data</strong>.</p>

    <p><img src="NLP/image-20220216221236167.png" alt="image-20220216221236167" style="zoom:67%;" /></p>
  </li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Use of additional unlabeled data improves on supervised learning when amount of <strong>labeled data is very small</strong> and amount of unlabeled data is large.</p>

  <ul>
    <li>otherwise, if unlabelled data is small, it <strong>could degrade performance</strong> when there is sufficient when unsupervised learning tends to create labels that are ==incompatible with the labelled/desired ones==.</li>
  </ul>
</blockquote>

<p>When you have supervised data <strong>and you want to use EM</strong></p>

<ol>
  <li>Use supervised learning on labeled data to initialize the parameters (instead of initializing them randomly).</li>
  <li>Use known labels for supervised data instead of predicting soft labels for these examples during retraining iterations.</li>
</ol>

<h1 id="constituency-grammar-and-parsing">Constituency Grammar and Parsing</h1>

<p>Essentially we discuss how <strong>syntax</strong>/grammar works, by exploring the idea of <strong>context-free grammar</strong> which is the backbone of many formal models of the syntax of natural languages.</p>

<blockquote>
  <p>Syntactic <strong>constituency</strong> is the idea that groups of words can behave as single units, or constituents.</p>

  <ul>
    <li>e.g. a noun phrase <code class="language-plaintext highlighter-rouge">NP</code>, a sequence of words surrounding at least one noun</li>
  </ul>
</blockquote>

<p>A more specific example would be:</p>

<p><img src="NLP/image-20220221182530243.png" alt="image-20220221182530243" style="zoom: 50%;" /></p>

<h2 id="context-free-grammar">Context Free Grammar</h2>

<p>The most widely used formal system for <strong>modeling constituent structure</strong> in English and other natural languages is the <strong>Context-Free Grammar</strong>, CFG. Essentially:</p>

<ul>
  <li>A context-free grammar consists of <strong>a set of rules or productions</strong>, each of which expresses the ways that symbols of the language can be grouped and ordered together, and <strong>a lexicon of words and symbols</strong></li>
</ul>

<p>Goal: The problem of mapping from <strong>a string of words</strong> to its <strong>parse tree</strong> is called <strong>syntactic parsing</strong></p>

<blockquote>
  <p><strong>Definition of Context Free Grammar:</strong></p>

  <p>A context-free grammar is a tuple $(N, \Sigma, R, S)$, where</p>

  <ul>
    <li>
      <p>$N$ is a finite set of <strong>non-terminal symbols</strong>/single variables</p>

      <ul>
        <li>it is exactly because it only allow a single variable, it is context free because it only depends on that variable without context</li>
      </ul>
    </li>
    <li>
      <p>$\Sigma$ is a set of <strong>terminal symbols</strong>/an alphabet (finite). This is <strong>disjoint from $N$</strong>.</p>
    </li>
    <li>
      <p>$R$ is a finite set of <strong>rules</strong> of the form:
\(A \to w;\,\,A\in N;\,\,w\in(V\cup\Sigma)^*\)
so basically, you can <strong>have more than one variables</strong>, and <strong>each variable points to either a string or a string+another variable</strong>.</p>

      <p>In general, for generating an <strong>non-empty language</strong>, you need to have <strong><em>at least one rule that points to a symbol only</em></strong>.</p>
    </li>
    <li>
      <p>$S \in N$ is the <strong>start</strong> variable</p>
    </li>
  </ul>
</blockquote>

<p>We will use the following notation for CFG in this case:</p>

<ul>
  <li>Capital letters such as $A,B,S$ represents <strong>non-terminals</strong></li>
  <li>Lower-case roman letters such as $u,v,w$ are <strong>strings of terminals</strong></li>
  <li>$S$ is the <strong>start symbol</strong></li>
  <li>Lower-case letters such as $\alpha, \beta, \gamma$ are <strong>strings from $(\Sigma \cup N)^*$</strong></li>
</ul>

<blockquote>
  <p><em>Recap from CS Theory</em></p>

  <ul>
    <li>
      <p>If two string $\alpha, \gamma \in (N \cup \Sigma)^*$, and there is a rule $A \to \beta \in R$, then we can write:
\(\alpha A \gamma \Rightarrow \alpha \beta \gamma\)
which reads: $\alpha A\gamma$ yields $\alpha \beta \gamma$.</p>
    </li>
    <li>
      <p>If two string $\alpha_1, \alpha_m \in (N \cup \Sigma)^<em>$, we say <strong>that $\alpha_1 \Rightarrow^* \alpha_m$ if</strong> some $\alpha_2, …,\alpha_{m-1} \in (N \cup \Sigma)^</em>$ we have:
\(\alpha_1 \Rightarrow \alpha_2 \Rightarrow \alpha_3 \Rightarrow ... \Rightarrow \alpha_m\)
where:</p>

      <ul>
        <li>$\alpha_1,\alpha_2, …,\alpha_m$ are just strings that you can reach from its previous string.</li>
      </ul>

      <p>This basically means that <strong>$\alpha_1$ generates to $\alpha_m$</strong> if there is an arbitrary number of step that can make we <strong>start with $u$ and end in $v$.</strong></p>

      <ul>
        <li>e.g. $S \Rightarrow^* 0011$ in the example above</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Notice that:</p>

<ul>
  <li>when you <strong>specify rules, you use $\to$,</strong> when you <strong>show derivations, you use $\Rightarrow$</strong></li>
</ul>

<p>And correspondingly:</p>

<blockquote>
  <p><strong>Context Free Language Def:</strong></p>

  <p>A language $L$ is a context-free language if there <strong>exists a context free grammar</strong> $G$ such that $L(G)=L$.</p>

  <ul>
    <li>i.e. set of strings composed of terminal symbols that can be derived from the designated start symbol $S$</li>
  </ul>
</blockquote>

<h3 id="syntax-and-cfg">Syntax and CFG</h3>

<p>Now, we can illustrate some examples <strong>how this relates to syntax</strong> in reality.</p>

<ul>
  <li><strong>terminal $\Sigma$</strong>: The symbols that correspond to words in the language (“the”, “nightclub”) are called terminal symbol</li>
  <li><strong>nonterminal $N$</strong>: The symbols that express abstractions over these terminals are called non-terminals</li>
  <li><strong>start symbol $S$</strong>: can be thought of as a sentence node</li>
</ul>

<p>For instance, the ATIS English corpus provides the following CFG definitions for <strong>English</strong></p>

<p><img src="NLP/image-20220221185338372.png" alt="image-20220221185338372" style="zoom:67%;" /></p>

<p>where we can see all <strong>rules/productions</strong> that are possible, and that:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">PP</code> is a prepositional phrase</li>
  <li>notice that the terminals cannot be on the LHS of the rule, whereas non-terminals such as <code class="language-plaintext highlighter-rouge">VP</code> can be on both LHS and RHS of a grammar rule.</li>
  <li>Lexicons are basically rules that generate <strong>terminals/words</strong></li>
</ul>

<p>And we can use CFG in two ways:</p>

<ul>
  <li>
    <p><strong>generating</strong> sentences/==derivation==.</p>

    <p><img src="NLP/image-20220221190302536.png" alt="image-20220221190302536" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>as a device for <strong>assigning</strong> a structure to a given sentence (i.e. output a parse tree)</p>

    <p><img src="NLP/image-20220221190318504.png" alt="image-20220221190318504" style="zoom: 50%;" /></p>

    <p>where sometimes you are asked to</p>

    <ul>
      <li>
        <p>return a parse tree for the string</p>
      </li>
      <li>
        <p>or return all possible parse trees for the string</p>
      </li>
    </ul>
  </li>
</ul>

<p>Both can be represented as a <strong>parse tree</strong>. For instance:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parse Tree</th>
      <th style="text-align: center">Bracketed Notation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220221185745736.png" alt="image-20220221185745736" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220221185833457.png" alt="image-20220221185833457" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<h2 id="grammar-equivalence-and-normal-form">Grammar Equivalence and Normal Form</h2>

<p>In fact, it is possible to have <strong>two distinct context-free grammars</strong> generate the <strong>same language</strong>.</p>

<blockquote>
  <p>Two grammars are <strong>strongly equivalent</strong> if they generate the same set of strings <strong>and</strong> if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols).</p>

  <p>Two grammars are <strong>weakly equivalent</strong> if they generate the same set of strings but do not assign the same phrase structure to each sentence.</p>
</blockquote>

<p>Additionally, a single sentence can have multiple parse trees, which means <strong>ambiguity in structure</strong>, and this will be covered in a another section.</p>

<p>It is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a common context-free grammar form used is <strong>Chomsky normal form (CNF)</strong> (Chomsky, 1963)</p>

<blockquote>
  <p><strong>Chomsky normal form (CNF)</strong></p>

  <p>A CFG is in Chomsky normal form it is</p>

  <ul>
    <li>$\epsilon$-free, i.e. no empty string</li>
    <li>each production is either of the form $A\to BC$ or $A\to a$. That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol</li>
  </ul>

  <p>Therefore, CNF parse trees are <strong>binary branching</strong>.</p>
</blockquote>

<p>For instance, you can convert $A \to BCD$ to CNF by:
\(\begin{cases}
A \to BX\\
X \to CD
\end{cases}\)
where you added a non-terminal $X$.</p>

<h2 id="partial-parsing">Partial Parsing</h2>

<p>Before going right into finding a parse tree given a sentence, we first consider some other <strong>simpler yet related task</strong>.</p>

<ul>
  <li>Many language processing tasks do not require complex, complete parse trees for all inputs. For these tasks, a <strong>partial parse</strong>, or shallow parse, of input sentences may be sufficient.</li>
  <li>one kind of partial parsing is <strong>chunking</strong></li>
</ul>

<blockquote>
  <p><strong>Chunking</strong> is a process of extracting <em>flat</em> phrases from unstructured text.</p>

  <ul>
    <li>e.g. identifying phrases such as “South Africa” as a <em>single phrase</em> instead of ‘South’ and ‘Africa’ as two tokens</li>
    <li>there is <strong>no hierarchical structure</strong> in the result, hence less informative than tree parsing</li>
  </ul>
</blockquote>

<p>Since chunked texts lack a hierarchical structure, a simple <strong>bracketing notation</strong> is sufficient to denote the location and the type of the chunks in a given example:
\(\text{$[_{NP}$ The morning flight] $[_{PP}$ from] $[_{NP}$ Denver] $[_{VP}$ has arrived.]}\)
this makes clear that chunking invovles two tasks:</p>

<ul>
  <li>
    <p><strong>segmenting</strong> (finding the non-overlapping extents of the chunks)</p>
  </li>
  <li>
    <p><strong>labeling</strong> (assigning the correct tag to the discovered chunks).</p>
  </li>
  <li>
    <p>in different applications, some input words may not be part of any chunk. For example, if you consider base $NP$ chunking:
\(\text{$[_{NP}$ The morning flight] from $[_{NP}$ Denver] has arrived.}\)</p>
  </li>
</ul>

<blockquote>
  <p>In most approaches, <strong>base phrases</strong> include the <strong>headword</strong> of the phrase, along with any <strong>pre-head material</strong> within the constituent, while crucially excluding any <strong>post-head</strong> material.</p>

  <p>For instance:
\(\text{$[_{NP}$ a flight] $[_{PP}$ from] $[_{NP}$ Indianapolis]$[_{PP}$ to]$[_{NP}$ Houston]}\)
has:</p>

  <ul>
    <li>$[_{NP} \text{ a flight}]$ for <em>flight</em> being the headwords of the <code class="language-plaintext highlighter-rouge">NP</code>, and <em>a</em> being the pre-head.</li>
  </ul>
</blockquote>

<h3 id="chunking-algorithms">Chunking Algorithms</h3>

<p>Chunking is generally done via <strong>supervised learning</strong>, training a <strong>BIO sequence labeler</strong>. In BIO tagging, we have</p>

<ul>
  <li>a tag for the beginning (<code class="language-plaintext highlighter-rouge">B</code>)</li>
  <li>inside (<code class="language-plaintext highlighter-rouge">I</code>) of each chunk type,</li>
  <li>tokens outside (<code class="language-plaintext highlighter-rouge">O</code>) any chunk</li>
</ul>

<p>For instance:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">All</th>
      <th style="text-align: center">Base NP</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220221200618115.png" alt="image-20220221200618115" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220221200625067.png" alt="image-20220221200625067" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>for all base phrase, we don’t need <code class="language-plaintext highlighter-rouge">O</code> tags</li>
  <li>for base <code class="language-plaintext highlighter-rouge">NP </code>only, we need <code class="language-plaintext highlighter-rouge">O</code> tag to show the boundaries</li>
</ul>

<p>Then, given a training set, any sequence model can be used to chunk: CRF, RNN, Transformer, etc</p>

<hr />

<p>To <strong>evaluate our performance</strong>, we are strict in that:
\(\begin{align*}
\text{Precision}&amp;=P=\frac{\text{Number of correct chunks found}}{\text{Total number of chunks found}}\\
\text{Recall}&amp;=R=\frac{\text{Number of correct chunks found}}{\text{Total number of chunks}}
\end{align*}\)</p>

<ul>
  <li>note that if one part of the phrase is labeled as wrong, the <strong>entire chunk is wrong</strong></li>
</ul>

<p>Take harmonic mean to produce a single evaluation metric called <strong>F measure.</strong>:
\(F_\beta = \frac{(\beta^2 + 1)P\cdot R}{\beta^2 P + R}\)
and mostly commonly $F_1$ is used:
\(F_1 = \frac{2PR}{P+2} = \frac{1}{2[(1/P) + (1/R)]}\)</p>

<ul>
  <li>current best system for NP chunking: F1=96%</li>
  <li>most other models also have around F1=92-94%</li>
</ul>

<h2 id="brute-force-parsing">Brute Force Parsing</h2>

<p>Our task of parsing a give sentence is essentially:</p>

<p><img src="NLP/image-20220221201827629.png" alt="image-20220221201827629" style="zoom: 50%;" /></p>

<blockquote>
  <p><em>Intuition</em></p>

  <p>This means we need to search <strong>space of derivations</strong> for one that derives the given string.</p>

  <ul>
    <li>
      <p><strong>Top-Down Parsing</strong>: Start searching space of derivations <strong>from the start symbol.</strong></p>
    </li>
    <li>
      <p><strong>Bottom-up Parsing</strong>: Start search space of reverse deviations <strong>from the terminal symbols</strong> in the string.</p>
    </li>
  </ul>
</blockquote>

<h3 id="top-down-parsing">Top-Down Parsing</h3>

<p>Essentially trial and errors, with backtracking once an error is made</p>

<ul>
  <li>if a mistake is made, <strong>backtrack</strong> one step</li>
  <li>if no more tries are left, <strong>backtrack</strong> one step</li>
</ul>

<p>Consider parsing the sentence $\text{book that flight}$:</p>

<ol>
  <li>
    <p>Start with top and try to fit <em>book</em>:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Try <code class="language-plaintext highlighter-rouge">NP -&gt; Pronoun</code></th>
          <th style="text-align: center"><code class="language-plaintext highlighter-rouge">NP -&gt; ProperNoun</code></th>
          <th style="text-align: center"><code class="language-plaintext highlighter-rouge">Aux</code></th>
          <th style="text-align: center">Success</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220221202005862.png" alt="image-20220221202005862" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202043999.png" alt="image-20220221202043999" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202055609.png" alt="image-20220221202055609" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202415708.png" alt="image-20220221202415708" style="zoom:67%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Try fitting <em>that</em>:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">No more options/production</th>
          <th style="text-align: center">Backtrack One Step</th>
          <th style="text-align: center">Failed</th>
          <th style="text-align: center">Success</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220221202316095.png" alt="image-20220221202316095" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202354464.png" alt="image-20220221202354464" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202448182.png" alt="image-20220221202448182" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202536027.png" alt="image-20220221202536027" style="zoom:67%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>notice that you <strong>only backtracked</strong> to fix previous problems <strong>when you ran out of current options.</strong></p>
  </li>
  <li>
    <p>Finally:</p>

    <p><img src="NLP/image-20220221164426930.png" alt="image-20220221164426930" style="zoom:50%;" /></p>
  </li>
</ol>

<h3 id="bottom-up-parsing">Bottom-Up Parsing</h3>

<p>The same idea, but now you <strong>start with bottom/tokens</strong></p>

<ol>
  <li>
    <p><strong>Start</strong> with <em>book</em>, and try to fit <em>that</em> and <em>flight</em></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Failed</th>
          <th style="text-align: center"> </th>
          <th style="text-align: center"> </th>
          <th style="text-align: center">Backtrack, no more options</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220221202819359.png" alt="image-20220221202819359" style="zoom: 80%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221202948073.png" alt="image-20220221202948073" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221203032448.png" alt="image-20220221203032448" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221203102330.png" alt="image-20220221203102330" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Try other options with <em>book</em> by backtracking this</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Failed</th>
          <th style="text-align: center"> </th>
          <th style="text-align: center">Success</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220221203220418.png" alt="image-20220221203220418" style="zoom: 50%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221203244530.png" alt="image-20220221203244530" style="zoom: 50%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221203314276.png" alt="image-20220221203314276" style="zoom: 50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>notice that we cached/saved what has been working on the right branch as we <strong>backtrack the “deepest” step one at a time</strong></p>
  </li>
</ol>

<h3 id="top-down-bottom-up-comparison">Top-down Bottom-Up Comparison</h3>

<ul>
  <li>
    <p><strong>Top down</strong> never explores options that will not lead to a full parse, but can explore many options that never connect to the actual sentence.</p>
  </li>
  <li>
    <p><strong>Bottom up</strong> never explores options that do not connect to the actual sentence but can explore options that can never lead to a full parse.</p>
  </li>
  <li>
    <p>Relative amounts of wasted search depend on how much the grammar branches in each direction.</p>
  </li>
</ul>

<p>Therefore, the idea is to use <strong>dynamic programming</strong> to save those wasted computations (though still $O(n^3)$ for $n$ is the length of input string)</p>

<ul>
  <li><strong>CKY Algorithm</strong> - based on bottom-up parsing</li>
  <li><strong>Earley Parser</strong> - based on top-down parsing</li>
  <li><strong>Chart Parsers</strong> - combine top-down and bottom-up search.</li>
</ul>

<h2 id="cky-algorithm">CKY Algorithm</h2>

<p>The CKY algorithm ==requires== grammars to first be in Chomsky Normal Form (CNF).</p>

<ul>
  <li>productions must have either exactly 2 non-terminal symbols on the RHS or 1 terminal symbol (lexicon rules).</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: converting to CNF form.</p>

  <p>In general, we have three cases to deal with:</p>

  <ul>
    <li>
      <p>Rules with a <strong>single non-terminal on the right</strong> are called unit productions.</p>

      <p>if $A \Rightarrow^* B$ by a chain of one or more unit productions and $B \to \gamma$ is a non-unit production in our grammar, then we add $A \to \gamma$ <strong>for each such rule</strong> int he grammar and discard all the intervening unit productions.</p>
    </li>
    <li>
      <p>Rules with <strong>right-hand sides longer than 2</strong> are normalized through the introduction of ==new non-terminals==</p>

      <p>This is simple, if you have $A \to BC\gamma$, then:
\(A \to X\gamma\\
X \to BC\)
would work.</p>
    </li>
  </ul>
</blockquote>

<p>A full example looks like:</p>

<p><img src="NLP/image-20220221204556585.png" alt="image-20220221204556585" style="zoom: 50%;" /></p>

<p>as an exercise you can try to reproduce the red highlighted parts.</p>

<h3 id="cky-recognition">CKY Recognition</h3>

<p>The CKY algorithm can do two things/have two versions:</p>

<ul>
  <li><strong>recognizer</strong>: ell us whether a valid parse exists for a given sentence</li>
  <li><strong>parser</strong>: in addition to recognizer, provide all possible parse trees</li>
</ul>

<p>Here, we start with the simple base model of recognizer. In essence, the algorithm considers a <strong>two dimensional matrix</strong></p>

<ul>
  <li>
    <p>our index start with $0$</p>
  </li>
  <li>each cell $[i,j]$ means the <strong>set of non-terminals</strong> that represent <strong>all the constituents that span positions $i$ through $j$ o</strong>f the input</li>
  <li>since it is a CNF form, we can think of a node $[i,j]$ having branched to a <strong>node below it $[i+<em>,j]$** and a **node left of it $[i,j-</em>]$</strong>. This works because each non-terminal production has <strong>exactly two children</strong>.</li>
</ul>

<p>An example of recognizing the sentence $\text{Book the flight through Houston}$ looks like:</p>

<p><img src="NLP/image-20220221211559902.png" alt="image-20220221211559902" style="zoom: 80%;" /></p>

<p>notice that:</p>

<ul>
  <li>
    <p>the cell that represents the entire input resides in position $[0,n]$ in the matrix</p>
  </li>
  <li>the right figure demonstrates the <strong>order the non-terminals are filled</strong>. This order guarantees that at the point where we are filling any cell $[i, j]$, the <strong>cells containing the parts</strong> that could contribute to this entry (i.e. the cells to the left and the cells below) <strong>have already been filled</strong>.</li>
  <li>The <strong>superdiagonal row</strong> (lowest diagonal) in the matrix contains the parts of speech for <strong>each word in the input.</strong> The subsequent diagonals above that superdiagonal contain <strong>constituents with increasing length</strong></li>
  <li>finally, all we need to check is if there is a $S$ in the $[0,n]$-th cell of the matrix. If there is, then there <strong>exists</strong> a valid parsing, otherwise there isn’t.</li>
</ul>

<p>In specific, each cell $[i,j]$ is filled by considering all the following possibilities:</p>

<p><img src="NLP/image-20220221212351220.png" alt="image-20220221212351220" style="zoom:67%;" /></p>

<p>where for instance:</p>

<ul>
  <li>the cell $[i,j+2]$ and $[i+2,j]$ represents parsing for combining <strong>first 2 blocks starting from $i$</strong>, and the <strong>3rd block to current block</strong> respectively.</li>
</ul>

<p>Hence, the algorithm looks like:</p>

<p><img src="NLP/image-20220221213305457.png" alt="image-20220221213305457" style="zoom:67%;" /></p>

<p>where it is important to note that the highlighted loop is essentially trying the combined pairs in the above figure (13.6).</p>

<ul>
  <li>The algorithm given in Fig. 13.5 is a recognizer, not a parser.</li>
</ul>

<h3 id="cky-parser">CKY Parser</h3>

<p>Though the above algorithm is not a parser, it can be <strong>easily modified into a parser</strong> by doing the following:</p>

<ol>
  <li>
    <p>we need to store, for each non-terminal, <strong>pointers to the table entries from which it was derived</strong></p>

    <p>For example:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">No Match, Next</th>
          <th style="text-align: center">Works, Save Pointer</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="NLP/image-20220221214015066.png" alt="image-20220221214015066" style="zoom:67%;" /></td>
          <td style="text-align: center"><img src="NLP/image-20220221214034779.png" alt="image-20220221214034779" style="zoom:67%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>permit <strong>multiple versions of the same non-terminal</strong> to be entered into the table (e.g. we can have multiple possible parse trees)</p>

    <p>For example:</p>

    <p><img src="NLP/image-20220221214139022.png" alt="image-20220221214139022" style="zoom: 80%;" /></p>
  </li>
</ol>

<p>Then, returning</p>

<ul>
  <li>an arbitrary single parse consists of choosing an $S$ from cell $[0,n]$, i.e. the ==last cell on top right==.</li>
  <li>then recursively retrieving its component constituents from the table</li>
</ul>

<p>would give us <strong>all possible parse trees</strong>.</p>

<blockquote>
  <p>The problem now is it doesn’t tell us <strong>which parse is the correct one</strong>! That is, it doesn’t disambiguate among the possible parses.</p>

  <ul>
    <li>the intuition to fix this would be to use a simple <strong>neural network</strong> extension to provide <strong>scores</strong> to each parse tree</li>
    <li>or use PCFG, i.e. each <strong>rule will have an associated probability</strong>, hence parse tree has a score/probability!</li>
  </ul>
</blockquote>

<hr />

<p>Lastly, we consider the complexity.</p>

<ul>
  <li>we need to iterate over $O(n^2)$ cells</li>
  <li>for each cell, look at all possible splits/combinations. There are $O(n)$ of them</li>
</ul>

<p>Hence, we have:
\(O(n^3)\)
as the time complexity.</p>

<h2 id="problems-with-cfg">Problems with CFG</h2>

<blockquote>
  <p>Is CFG practical to represent rules under the language we speak?</p>
</blockquote>

<ul>
  <li>In reality, those are not complete as there could be so many <strong>more specific grammar rules</strong> within a single language</li>
  <li>additionally, the <strong>conversion to CNF will complicate</strong> any syntax-driven approach to semantic analysis</li>
</ul>

<p><em>For Example</em>: Word Agreement:</p>

<ul>
  <li>
    <p>we need subject-verb agreement such as:
\(\text{I am cold.} \quad vs \quad \text{*I are cold}\)
both of which will be parsed by the CKY, as both <em>am</em> and <em>are</em> are verb.</p>

    <p>To fix issues like this, we need <strong>separate production rules for each combination</strong></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">S -&gt; NP1stPersonSing VP1stPersonSing</code></li>
      <li><code class="language-plaintext highlighter-rouge">S → NP2ndPersonSing VP2ndPersonSing</code></li>
      <li><code class="language-plaintext highlighter-rouge">NP1stPersonSing → …</code></li>
      <li><code class="language-plaintext highlighter-rouge">VP1stPersonSing → …</code></li>
      <li>etc</li>
    </ul>
  </li>
  <li>
    <p>pronoun agreement:
\(\text{I gave him the book.} \quad vs \quad \text{*I gave he the book}\)
again, we need more rules that what we have to correctly deal with them.</p>
  </li>
</ul>

<hr />

<p><em>For Example</em>: Subcategorization</p>

<p>Specific verbs have specific rules for words after them. For example:
\(\text{John wants to buy a car.} \quad vs \quad \text{*John wants buy a car.}\)
Again, more specific rules are needed for different tokens, this cannot be characterized by our current CFG.</p>

<hr />

<p>In short, even if the CKY runtime will be $O(N^3)$, it depends on having prepared the grammar, which <strong>could be exponential in size</strong>. Hence this could be a bottleneck one has to deal with.</p>

<h2 id="statistical-parsing">Statistical Parsing</h2>

<p>A PCFG is a probabilistic version of a CFG where each production has a probability. By adding a probability to production rules, <strong>one can an approach to resolving syntactic ambiguity</strong>, i.e. it produces a <strong>score for each parse tree</strong>.</p>

<blockquote>
  <p><strong>Definition of Probabalistic Context Free Grammar:</strong></p>

  <p>A context-free grammar is a tuple $(N, \Sigma, R, S)$, where</p>

  <ul>
    <li>
      <p>$N$ is a finite set of <strong>non-terminal symbols</strong>/single variables</p>

      <ul>
        <li>it is exactly because it only allow a single variable, it is context free because it only depends on that variable without context</li>
      </ul>
    </li>
    <li>
      <p>$\Sigma$ is a set of <strong>terminal symbols</strong>/an alphabet (finite). This is <strong>disjoint from $N$</strong>.</p>
    </li>
    <li>
      <p>$R$ is a finite set of ==probabalistic rules== of the form:
\(A \to \beta[p],\, p\equiv p(A \to \beta |A); \quad \,A\in N;\,\,\beta \in(V\cup\Sigma)^*\)
which is probabalistic, meaning the <strong>probability of expanding to $\beta$</strong> when <strong>given the non-terminal $A$</strong>. Therefore necessarily we need:
\(\sum_\beta p(A \to \beta| A)=1\)</p>
    </li>
    <li>
      <p>$S \in N$ is the <strong>start</strong> variable</p>
    </li>
  </ul>
</blockquote>

<p>So a probabilistic context-free grammar is also defined by four parameters, with a <strong>slight augmentation to each of the rules in $R$</strong>.</p>

<p><em>For Example</em></p>

<p><img src="NLP/image-20220223223442662.png" alt="image-20220223223442662" style="zoom:67%;" /></p>

<table>
  <tbody>
    <tr>
      <td>and notice that, for instance $\sum_\beta p(S \to \beta</td>
      <td>S)=1$ is the first three rows of the table.</td>
    </tr>
  </tbody>
</table>

<p>This would be useful because we can define the ==probability of a particular parse tree $T$== of a setnence $S$ being:
\(P(T,S)=P(T)=\prod_{i=1}^nP(\text{RHS}_i|\text{LHS}_i)\)
where each rule $i$ is being expressed as $\text{LHS}_i \to \text{RHS}_i$.</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(T,S)=P(S</td>
          <td>T)P(T)=P(T)$ since given the tree $P(S</td>
          <td>T)=1$ includes all words of the sentence.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>so all we need to do it to <strong>multiply probabilities of each of the rules</strong> used in the derivation, to get a score/probability of a parse tree $P(T)$!.</li>
</ul>

<p>An example would be:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parse Tree</th>
      <th style="text-align: center">Probability CFG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220223224338903.png" alt="image-20220223224338903" /></td>
      <td style="text-align: center"><img src="NLP/image-20220223224343499.png" alt="image-20220223224343499" /></td>
    </tr>
  </tbody>
</table>

<p>And as an exercise, make sure the following is correct:</p>

<p><img src="NLP/image-20220223224426107.png" alt="image-20220223224426107" style="zoom:67%;" /></p>

<p>Then, with the single capability of providing a score to a parse tree, we can do:</p>

<ul>
  <li><strong>Observation likelihood</strong>: To classify and order sentences.</li>
  <li><strong>Most likely derivation</strong>: To determine the most likely parse tree for a sentence.</li>
  <li>(Finding the probability for the rules) Maximum likelihood training: To train a PCFG to fit empirical training data.</li>
</ul>

<h3 id="pcfg-most-likely-derivation">PCFG Most Likely Derivation</h3>

<p>With the ability to assign a score to a tree $T$, we can basically do <strong>disambiguation</strong> by picking the most probable:
\(\hat{T}(S) = \arg\max_T P(T|S)=\arg\max_T P(T,S)=\arg\max_T P(T)\)
since $P(S)$ is a constant, and $P(T,S)=P(T)$. Baically we just choose the parse with highest probability.</p>

<p>Then, the algorithms for computing the most likely parse are <strong>simple extensions</strong> of the standard algorithms for parsing, i.e. <strong>augmenting CKY algorithm with probabilities</strong>.</p>

<p>Therefore, the idea is basically:</p>

<ol>
  <li>
    <p>Convert the rules to CNF, so that right-hand side of each rule must expand to either two non-terminals or to a single terminal. This is non-trivial as it involves also <strong>converting probabilities</strong> while breaking into CNF.</p>
  </li>
  <li>
    <p>For probabilistic CKY, it’s slightly simpler to think of the each cell in the matrix $M[i,j,A]$ as constituting a ==third dimension== of maximum length $V$: corresponds to <strong>each non-terminal that can be placed in this cell</strong>, e.g. $A_1 \to BC$ being a cell in the third dimension, then next $A_2 \to DE$, etc.</p>

    <p>The value of the cell is then a <strong>probability</strong> for that nonterminal/constituent (then of course many of those cells in the third dimension will have value $0$ since most rules cannot produce the constituent $S[i:j]$).</p>
  </li>
  <li>
    <p>Lastly, the entire CKY algorithm is the <strong>same, except at each step we update the probability</strong></p>
  </li>
</ol>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Computation Matrix</th>
      <th style="text-align: center">PCFG</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="NLP/image-20220223230424007.png" alt="image-20220223230424007" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="NLP/image-20220223230433792.png" alt="image-20220223230433792" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>And the algorithm is simply:</p>

<p><img src="NLP/image-20220223233120282.png" alt="image-20220223233120282" style="zoom:67%;" /></p>

<p>as compared to the original CKY, which is basically <strong>concatenating</strong> each rule $A_i$ at the cell $T[i,j]$.</p>

<ul>
  <li>and again, since in the end we need to spit out a tree, we also need to keep a <strong>backtrack matrix</strong> which is the last step in the innermost loop of the algorithm.</li>
</ul>

<p>For a better visualization, at cell $T[1,4]$, the will do:</p>

<p><img src="NLP/image-20220223233727351.png" alt="image-20220223233727351" style="zoom: 67%;" /></p>

<p>for every possible rule $A_i \to …$, and each cell $T[1,4,A_i]$ records the best arrow combination for rule $A_i$ (e.g. which of the two purple is better for rule $A_1$ is recorded at $T[1,4,A_1]$, and so on for $A_2$…)</p>

<hr />

<p><em>For Example</em>: Convert CFG Probability</p>

<p><img src="NLP/image-20220223231013212.png" alt="image-20220223231013212" style="zoom:67%;" /></p>

<p>Notice how the red boxes is converted to the red boxes on the right, while the yellow highlighted part obeyed that:
\(\sum_\beta P(A\to \beta |A)=1\)</p>

<h3 id="pcfg-observation-likelihood">PCFG Observation Likelihood</h3>

<p>If we can quantify the probability of a parse tree, we can also quantify the probability of a sentence. Hence we can directly <strong>assign scores to a sentence!</strong></p>

<blockquote>
  <p>The probability of an ambiguous sentence is the ==sum of the probabilities of all the parse trees== for the sentence</p>
</blockquote>

\[P(S) = \sum_{T} P(T,S) = \sum_T P(T)\]

<p>Therefore, since in this case we <strong>only needs to spit out a score</strong>, we do:</p>

<ol>
  <li>for each possible rule $A_i \to XY$, we want to combine all the probability of all possible parse instead of only taking the max</li>
  <li>do CKY while keeping the sum for each cell in the third dimension</li>
  <li>return the sum of the third dimension on the last cell $\text{sum}_i(T[0,N,A_i])$ to spit out the final score</li>
</ol>

<p>Therefore, the algorithm is:</p>

<p><img src="NLP/image-20220223234622363.png" alt="image-20220223234622363" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>we don’t need to only store the max, we are summing over all possible ones for each parse $A_i$</li>
  <li>we <strong>don’t need to store backtrack matrix</strong> because we are only spitting out a score</li>
</ul>

<h3 id="pcfg-learning">PCFG Learning</h3>

<p>Lastly, we talk about how to <strong>learn those probabilities in the CFG rule</strong> in the first place. In general, there are two approaches:</p>

<ul>
  <li>
    <p>when given some <strong>labelled</strong> training corpus (i.e. Treebank corpus, where parse trees are given), we just need to do <strong>counts</strong>. This is as simple as:
\(P(A \to \beta |A) = \frac{\text{Count}(A \to \beta)}{\text{Count}(A)}\)
again, since it is counts, we will need <strong>smoothing techniques</strong>.</p>
  </li>
  <li>
    <p>if we don’t have a treebank but we do have a (non-probabilistic) parser, we can <strong>generate the counts and probabilities</strong> we need by</p>

    <ol>
      <li>
        <p>parsing a corpus of sentences with some nonprobabalistic parser to <strong>generate all possible parses</strong> (given the CNF productions)</p>

        <ul>
          <li>of if <strong>only the set of non-terminals</strong> are provided, first generate all possible CFG rules and treat them as given</li>
        </ul>
      </li>
      <li>
        <p>count for each parse of a sentence and <strong>weight each of these counts by the probability</strong> of the parse it appears in. But we <em>dont’ have the probability to begin with</em>! Hence the idea is then to use a <strong>EM like algorithm</strong> at this step.</p>

        <p>So we <strong>begin with a parser with equal rule probabilities</strong>.</p>
      </li>
      <li>
        <p>Use <strong>EM to iteratively train the probabilities</strong> of these productions to locally maximize the likelihood of the data.</p>

        <ol>
          <li>parse the sentence, compute a probability for each parse</li>
          <li>use these probabilities to weight the counts, <strong>re-estimate the rule probabilities</strong></li>
          <li>repeat until convergence</li>
        </ol>
      </li>
    </ol>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong>: If you don’t even have the set of non-terminals to start with, i.e. without the set of non-terminal fixed, the learning of CFG itself might be <strong>exponential</strong> as you need to try every possible non-terminals + every possible productions.</p>
</blockquote>

<h2 id="problems-with-pcfg">Problems with PCFG</h2>

<p>While probabilistic context-free grammars are a natural extension to context-free grammars, they have <strong>two main problems as probability estimators</strong>:</p>

<ul>
  <li><strong>Poor independence assumptions</strong>: CFG rules impose an independence assumption on probabilities (i.e. Markov) that leads to poor modeling of structural dependencies <em>across the parse tree</em>
    <ul>
      <li>Recall that in a CFG the expansion of a non-terminal is independent of the context</li>
    </ul>
  </li>
  <li><strong>Lack of lexical conditioning</strong>: CFG rules don’t model syntactic facts about specific words, leading to problems with subcategorization ambiguities, preposition attachment, and coordinate structure ambiguities.
    <ul>
      <li>basically the problem in section <a href="#Problems with CFG">Problems with CFG</a> comes back here.</li>
    </ul>
  </li>
</ul>

<p>Because of these problems, probabilistic constituent parsing models use some augmented version of PCFGs, or modify the Treebank-based grammar in some way.</p>

<h3 id="pcfg-independence-assumptions">PCFG Independence Assumptions</h3>

<p>Recall that in a CFG the expansion of a non-terminal is independent of the context, but in English the choice of how a node expands can after all depend on the location of the node in the parse tree.</p>

<p>For example, in English it turns out that <code class="language-plaintext highlighter-rouge">NPs </code> that are syntactic subjects are far more likely to be <strong>pronouns</strong>, and <code class="language-plaintext highlighter-rouge">NPs </code>that are syntactic objects are far more likely to be <strong>non-pronominal</strong>:</p>

<p><img src="NLP/image-20220228210117439.png" alt="image-20220228210117439" style="zoom: 67%;" /></p>

<p>and there is no way to represent this in CFG as it is context free, so all we can do is:
\(\text{NP} \to \text{DT  NN}, \quad 0.28 \\
\text{NP} \to \text{PN}, \quad 0.25\)
instead of something like “$\text{NP} \to \text{DT  NN}= 0.91 $ if it is in subject position”.</p>

<h3 id="pcfg-lexical-dependencies-problem">PCFG Lexical Dependencies Problem</h3>

<p>A second class of problems with PCFGs is their lack of sensitivity to the words in the parse tree.</p>

<p>For example, consider the following problem of <strong><code class="language-plaintext highlighter-rouge">PP</code> attachment</strong>: should it be to <code class="language-plaintext highlighter-rouge">VP</code> or <code class="language-plaintext highlighter-rouge">NP</code>? We have the following rules:
\(VP \to VBD\,\,NP\,\,PP\)
and
\(VP \to VBD \,\, NP\\
NP \to NP \,\, PP\)
It turns out that in some specific cases, the <strong>correct answer is with <code class="language-plaintext highlighter-rouge">VP</code></strong>, but in other cases, the correct answer is with <strong>NP</strong>:</p>

<p><img src="NLP/image-20220228211200751.png" alt="image-20220228211200751" style="zoom: 50%;" /></p>

<p>And with correct answer being with NP is when you analyze the sentence “<em>fishermen caught tons of herring</em>”, so that <em>of</em> is attached to <em>tons</em>, which is <code class="language-plaintext highlighter-rouge">NP</code> instead of attaching to <code class="language-plaintext highlighter-rouge">caught</code>.</p>

<p>Depending on how these probabilities are set, a PCFG will always either prefer NP attachment or VP attachment, hence <strong>missing one of the two correct parsing</strong>.</p>

<blockquote>
  <p>What information in the input sentence lets us know that when we need NP attachment while the other needs VP attachment? These preferences come from the <strong>identities of the verbs, nouns, and prepositions.</strong></p>

  <ul>
    <li>affinity between the verb <code class="language-plaintext highlighter-rouge">dumped </code>and the preposition <code class="language-plaintext highlighter-rouge">into </code>is greater, hence VP is preferred</li>
    <li>affinity between the noun <code class="language-plaintext highlighter-rouge">ton</code> and <code class="language-plaintext highlighter-rouge">of</code>is greater, hence NP is preferred</li>
  </ul>
</blockquote>

<p>Thus, to get the correct parse for these kinds of examples, we need a model that somehow augments the PCFG probabilities to deal with these lexical dependency <strong>statistics for different verbs and prepositions</strong> in addition to what we have. (this will be done by using productions <strong>specialized to specific words</strong> by including their head-word in their LHS non-terminals)</p>

<h2 id="pcfg-with-splitting-non-terminals">PCFG with Splitting Non-terminals</h2>

<p>Recall that the problem is CFG is context-free: what if <code class="language-plaintext highlighter-rouge">NPs </code> in <strong>subject</strong> position tend to be <strong>pronouns</strong>, whereas <code class="language-plaintext highlighter-rouge">NPs </code> in <strong>object</strong> position tend to have full lexical (<strong>non-pronominal</strong>) form.</p>

<blockquote>
  <p>One idea would be to split the NP non-terminal into two versions: one for subjects, one for objects: $NP_{\text{subject}}$ and $NP_{\text{object}}$. Then we can have:
\(NP_{\text{subject}} \to PR\\
NP_{\text{object}} \to PR\)
with two probability.</p>
</blockquote>

<p>One way to implement this would be to do <strong>parent annotation</strong></p>

<p><img src="NLP/image-20220228214223666.png" alt="image-20220228214223666" style="zoom: 50%;" /></p>

<p>where, except for pre-terminal nodes, each non-terminal is labelled by <code class="language-plaintext highlighter-rouge">X^Y</code> where <code class="language-plaintext highlighter-rouge">Y</code> is the parent.</p>

<ul>
  <li>In addition to splitting these phrasal nodes, we can also improve a PCFG by splitting the pre-terminal part-of-speech nodes.</li>
</ul>

<p>However, node-splitting is not without problems; it ==increases the size of the grammar== and hence reduces the amount of training data available for each grammar rule, leading to ==overfitting==. Thus, it is important to split to just the correct level of granularity for a particular training set.</p>

<h2 id="pcfg-with-lexicalization">PCFG with Lexicalization</h2>

<p>Recall that another problem is to capture the fact that <strong>which production rule is better</strong> (disambiguation) also depends on the specific <strong>tokens/words/phrase</strong>, e.g. whether if <code class="language-plaintext highlighter-rouge">PP</code> should follow <code class="language-plaintext highlighter-rouge">NP</code> or <code class="language-plaintext highlighter-rouge">VP</code>.</p>

<p>Instead of using the above idea of <strong>splitting non-terminals = adding more syntax rules</strong>, here we add/allow for ==lexicalized rules==.</p>

<ul>
  <li>The resulting family of lexicalized parsers includes the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 1997).</li>
</ul>

<blockquote>
  <p>Here the idea is that, since syntactic constituents could be associated with a <strong>lexical head</strong>, and we defined a lexicalized grammar in which each <strong>non-terminal</strong> in the <strong>grammar tree is annotated with its lexical head</strong>,</p>

  <ul>
    <li>lexical head: one word which represents the crux of the underlying constituent</li>
  </ul>
</blockquote>

<p>For instance, instead of $VP \to VBD \,\, NP\,\,PP$ we have:
\(VP(\text{dumped}) \to VBD(\text{dumped}) \,\, NP(\text{sacks})\,\,PP(\text{into})\)
for the following tree:</p>

<p><img src="NLP/image-20220228215843081.png" alt="image-20220228215843081" style="zoom: 67%;" /></p>

<p>which also made a further extension to <strong>include the head word tag</strong>, e.g.
\(VP(\text{dumped}, VBD) \to VBD(\text{dumped}, VBD) \,\, NP(\text{sacks},NNS)\,\,PP(\text{into},P)\)
notice that the rules here have to be split into two sets:</p>

<ul>
  <li>lexical rules will be <strong>deterministic</strong>, since $p[DT(\text{a},DT) \to \text{a}] = 1$</li>
  <li>internal rules with be <strong>probabilistic</strong> as usual. Those are the parameters we need to train.</li>
</ul>

<p>Then, to train it, we can imagine this as a simple context-free grammar with <strong>many copies of each rule</strong>, one copy for each possible headword/head tag for each constituent. For instance, the rule:
\(VP(\text{dumped}, VBD) \to VBD(\text{dumped}, VBD) \,\, NP(\text{sacks},NNS)\,\,PP(\text{into},P)\)
<em>should be estimated</em> by:
\(\frac{\text{Count}[VP(\text{dumped}, VBD) \to VBD(\text{dumped}, VBD) \,\, NP(\text{sacks},NNS)\,\,PP(\text{into},P)]}{\text{Count}[VP(\text{dumped}, VBD) ]}\)
but obviously this is <strong>too specific</strong>. The idea of lexicalized parsing is to make some further <strong>independence assumptions</strong> to break down each rule so that we would estimate the probability, and an example would be the <strong>Collins Parser</strong>.</p>

<blockquote>
  <p>Although this can make <strong>disambiguation</strong> between parses work even better, accurately estimating parameters (i.e. probability of each of those production rules) on such a large number of very specialized productions could ==require enormous amounts of treebank data==.</p>
</blockquote>

<h3 id="simplified-collins-parser">Simplified Collins Parser</h3>

<p>In short, there we consider estimating the probability using some <strong>generative rules</strong>. This new generative story is that given the left-hand side:</p>

<ol>
  <li>We also add a special <code class="language-plaintext highlighter-rouge">STOP </code> non-terminal at the left and right edges of the rule;</li>
  <li>we first generate the <strong>head</strong> of the rule</li>
  <li>generate dependents on the left side of the head until we’ve generated on the left side of the head <code class="language-plaintext highlighter-rouge">STOP</code></li>
  <li>move to the right side of the head and start generating dependents there until we generate <code class="language-plaintext highlighter-rouge">STOP</code>.</li>
</ol>

<p>Therefore, consider estimating:
\(P[VP(\text{dumped}, VBD) \to \text{STOP}\,\,\,\, VBD(\text{dumped}, VBD) \,\, NP(\text{sacks},NNS)\,\,PP(\text{into},P)\,\,\,\, \text{STOP}]\)
Then the generative procedure is:</p>

<p><img src="NLP/image-20220228222206905.png" alt="image-20220228222206905" style="zoom: 67%;" /></p>

<p>where notice that</p>

<ul>
  <li>except from the first node, all nodes are generated <strong>conditioning</strong> on $LHS,H$.</li>
  <li>being left of the tree or right of the tree also matters!</li>
</ul>

<p>Then, we can estimate the probability by a product:</p>

<p><img src="NLP/image-20220228222314839.png" alt="image-20220228222314839" style="zoom: 67%;" /></p>

<table>
  <tbody>
    <tr>
      <td>and each smaller probability can be estimated using counts, for example $P_R[NP(\text{sacks})</td>
      <td>VP,VBD, \text{dumped}]$ will be estimated by:</td>
    </tr>
  </tbody>
</table>

<p><img src="NLP/image-20220228222545521.png" alt="image-20220228222545521" style="zoom: 80%;" /></p>

<p>which is much less specific.</p>

<h2 id="evaluating-parsers">Evaluating Parsers</h2>

<p>The standard tool for <strong>evaluating parsers that assign a single parse tree</strong> to a sentence is the <strong>PARSEVAL</strong> metrics</p>

<blockquote>
  <p>The PARSEVAL metric measures how much the <strong>constituents</strong> in the hypothesis parse tree (returned by your parser) look like the constituents in a hand-labeled, reference parse (given label).</p>
</blockquote>

<blockquote>
  <p>Constituent in a hypothesis parse $C_h$ of a sentence $s$ is labeled <strong>correct</strong> if there is a constituent in the reference parse $C_r$ with the same <strong>starting point</strong>, <strong>ending point</strong>, and <strong>non-terminal symbol</strong></p>
</blockquote>

<p>For instance:</p>

<p><img src="NLP/image-20220228170603417.png" alt="image-20220228170603417" style="zoom: 67%;" /></p>

<p>so in short a constituent basically corresponds to the <strong>word/leaves it entails + the label itself</strong>. Then, we can measure <strong>precision and recall</strong></p>

<p><img src="NLP/image-20220228224205589.png" alt="image-20220228224205589" style="zoom:67%;" /></p>

<p>This is essentially related to the evaluation metric used in <a href="#Chunking Algorithms">Chunking Algorithms</a>. Therefore, the in this example will have:
\(\text{Recall} = 10/12\\
\text{Precision} = 10/12\)
And normally we report the $F_1$ score being:
\(F_1 = \frac{2P\cdot R}{P+R}\)
Results of current state-of-the-art systems on the English Penn WSJ treebank are $91\%-92\%$ labeled $F_1$.</p>

<hr />

<p>We may also additionally use a <em>new metric</em>, <strong>crossing brackets</strong>, for each sentence $s$.</p>

<blockquote>
  <p><strong>cross-brackets</strong>: the number of constituents for which the reference parse has a bracketing such as $((A B) C)$ but the hypothesis parse has a bracketing such as $(A (B C))$.</p>
</blockquote>

<h2 id="human-parsing">Human Parsing</h2>

<p>Another idea for building a parser would be to think about <strong>how human parses</strong> a sentence.</p>

<blockquote>
  <p>Psycholinguistic studies show that words that are <strong>more probable given the preceding</strong> lexical and syntactic <strong>context are read faster.</strong> For instance:
\(\text{John put the dog inthe pen with a \textbf{lock}}.\)
where the last word should come pretty fast.</p>
</blockquote>

<p>Then, modeling these effects requires an ==incremental statistical parser== that incorporates one word at a time into a continuously growing parse tree.</p>

<p>Some additional problems that we need to consider due to “human effects” are:</p>

<ul>
  <li>
    <p><strong>garden path sentences</strong>: people are confused by sentences that seem to have a particular syntactic structure but then <em>suddenly violate this structure</em>: a grammatically correct sentence that starts in such a way that a reader’s most likely interpretation will be incorrect. For instance:
\(\text{The old man the boat.}\)
When we read <em>old</em> followed by <em>man</em> they assume that the phrase <em>the old man</em> is to be interpreted as <em>determiner</em> – <em>adjective</em> – <em>noun</em>. When readers encounter another <em>the</em> following the supposed noun <em>man</em> (rather than the expected verb, as in e.g. <em>The old man washed the boat</em>),[<a href="https://en.wikipedia.org/wiki/Garden-path_sentence#cite_note-4">a]</a> they are <strong>forced to re-analyze the sentence.</strong></p>

    <p>Incremental computational parsers can try to predict and explain the problems encountered parsing such sentences.</p>
  </li>
  <li>
    <p><strong>nested expressions</strong>: nested expressions are hard for humans to process beyond 1 or 2 levels of nesting, for instance:
\(\text{The rat the cat the dog the boy owned bit chased died.}\)
to solve this, one way is to use <strong>center embedding</strong>, which is computing the equivalent “tail embedded” (tail recursive) versions are easier to understand:
\(\text{The boy owned a dog that bit a cat that chased a rat that died.}\)</p>
  </li>
</ul>

<h2 id="other-grammars">Other Grammars</h2>

<p>Other constituent parsers that use an <strong>alternative to CFG grammar</strong>:</p>

<ul>
  <li>Mildly Context-Sensitive Grammars:
    <ul>
      <li>parses that use Combinatory Categorial Grammar (CCG)</li>
      <li>parsers that use Tree Adjoining Grammar (TAG)</li>
    </ul>
  </li>
  <li>Unification Grammars: in order to handle word-agreements better, each constituent has a list of features such as number, person, gender, etc, and the constraint is that combining two constituents mean <strong>their features must unify as well</strong>
    <ul>
      <li>Expressive grammars and parses (HPSG)</li>
    </ul>
  </li>
</ul>

<h1 id="midterm">Midterm</h1>

<p>Material up to and including the PCFG section.</p>

<p>Four big questions in midterm, on:</p>

<ul>
  <li>LM, Spell Correction</li>
  <li>Neural Networks, concepts and some basic optimization methods</li>
  <li>POS tagging (HMMs)</li>
  <li>Syntactic Parsing, Statistical Parsing</li>
</ul>

<p>Can bring cheat sheet. a double paged A4 paper.</p>

<p>Can bring a calculator.</p>

<h1 id="dependency-parsing">Dependency Parsing</h1>

<p>The focus of the two previous chapters has been on context-free grammars and <strong>constituent-based</strong> representations. Here we present another important family of grammar formalisms called ==dependency grammars==.</p>

<blockquote>
  <p>Here, syntactic structure of a sentence is described solely in terms of <strong>directed binary grammatical relations</strong> between the words, as in the following dependency parse:</p>

  <p><img src="NLP/image-20220302183610409.png" alt="image-20220302183610409" style="zoom: 80%;" /></p>

  <p>Relations among the words are illustrated above the sentence with <strong>directed, labeled arcs</strong> from heads to dependents.</p>
</blockquote>

<p>A major advantage of dependency grammars is their ability to deal with languages that are morphologically rich and have a relatively <strong>free word order</strong>. Though you can of course convert from a constituency parse tree to dependency tree, so they are related in some way.</p>

<p>For example:</p>

<p><img src="NLP/image-20220302184449124.png" alt="image-20220302184449124" style="zoom: 67%;" /></p>

<p>notice that:</p>

<ul>
  <li>the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the <strong>internal/real structure of the dependency pars</strong>e consists solely of <strong>directed relations between lexical items</strong> in the sentence.</li>
  <li>so you can convert a phrase structure parse to a dependency tree by
    <ul>
      <li>take each non-head child of a node</li>
      <li>make its parent to be the node with the head</li>
    </ul>
  </li>
</ul>

<h1 id="neural-shift-reduce-parser">Neural Shift-Reduce Parser</h1>

<p>Deterministically builds a parse incrementally, bottom up, and left to right, without backtracking.</p>

<h2 id="shift-reduce-parsing">Shift Reduce Parsing</h2>

<p>==TODO==</p>

<p>When each verb is encountered in the stack,</p>

<p><img src="NLP/image-20220302170518958.png" alt="image-20220302170518958" style="zoom:33%;" /></p>

<h2 id="neural-dependency-parser">Neural Dependency Parser</h2>

<p>Train a neural net to choose the best shift-reduce parser action to take at each step.</p>

<ul>
  <li>e.g. whether if you shoud left reduce or right reduce</li>
</ul>

<p><img src="NLP/image-20220302171055831.png" alt="image-20220302171055831" style="zoom:33%;" /></p>

<p>Use  features (words, POS tags, arc labels) extracted from the current stack, buffer, and arcs as context.</p>

  </div><a class="u-url" href="/lectures/2021@columbia/COMS4705_NLP.html/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
