<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COMS4771 Machine Learning | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="COMS4771 Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Logistics" />
<meta property="og:description" content="Logistics" />
<link rel="canonical" href="/lectures/2021@columbia/COMS4771_ML.html/" />
<meta property="og:url" content="/lectures/2021@columbia/COMS4771_ML.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-09T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COMS4771 Machine Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-09T00:00:00+00:00","datePublished":"2021-12-09T00:00:00+00:00","description":"Logistics","headline":"COMS4771 Machine Learning","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2021@columbia/COMS4771_ML.html/"},"url":"/lectures/2021@columbia/COMS4771_ML.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
				<a class="page-link" href="/learning">Blog</a>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
  <script>
    MathJax = {
      // 
      loader: {
        load: ['[tex]/ams', '[tex]/textmacros', '[tex]/boldsymbol']
      },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']}
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COMS4771 Machine Learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-12-09T00:00:00+00:00" itemprop="datePublished">
        Dec 9, 2021
      </time></p>
  </header>

  <div class="section-nav" id="toc-all">
    <button type="button" id="toc-close" class="toc_collapsible hidden" title="collapse">
      <span><strong>Table of Contents</strong></span>
    </button>
    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" mirror-in-rtl="true" fill="#000000" style="width: 18px;" id="toc-reopen" class="toc_collapsible">
      <g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <circle fill="#494c4e" cx="2" cy="2" r="2"></circle> <circle fill="#494c4e" cx="2" cy="8" r="2"></circle> <circle fill="#494c4e" cx="2" cy="20" r="2"></circle> <circle fill="#494c4e" cx="2" cy="14" r="2"></circle> <path fill="#494c4e" d="M23.002 3H7.998C7.448 3 7 2.55 7 2.002v-.004c0-.55.45-.998.998-.998H23c.55 0 1 .45 1 .998V2c0 .55-.45 1-.998 1zM23.002 9H7.998C7.448 9 7 8.55 7 8.002v-.004c0-.55.45-.998.998-.998H23c.55 0 1 .45 1 .998V8c0 .55-.45 1-.998 1zM23.002 15H7.998c-.55 0-.998-.45-.998-.998V14c0-.55.45-1 .998-1H23c.55 0 1 .45 1 .998V14c0 .55-.45 1-.998 1zM23.002 21H7.998c-.55 0-.998-.45-.998-.998V20c0-.55.45-1 .998-1H23c.55 0 1 .45 1 .998V20c0 .55-.45 1-.998 1z"></path> </g>
    </svg>
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#logistics">Logistics</a></li>
<li class="toc-entry toc-h1"><a href="#basics-and-mle">Basics and MLE</a>
<ul>
<li class="toc-entry toc-h2"><a href="#statistical-modelling-approach">Statistical Modelling Approach</a></li>
<li class="toc-entry toc-h2"><a href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#multivariate-gaussian">Multivariate Gaussian</a></li>
<li class="toc-entry toc-h3"><a href="#mle-to-classification">MLE to Classification</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#naïve-bayes-classifier">Naïve Bayes Classifier</a></li>
<li class="toc-entry toc-h2"><a href="#evaluating-quality-of-classifier">Evaluating Quality of Classifier</a>
<ul>
<li class="toc-entry toc-h3"><a href="#bias">Bias</a></li>
<li class="toc-entry toc-h3"><a href="#consistency">Consistency</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#nearest-neighbors-and-decision-trees">Nearest Neighbors and Decision Trees</a>
<ul>
<li class="toc-entry toc-h2"><a href="#nearest-neighbor-classifier">Nearest Neighbor Classifier</a>
<ul>
<li class="toc-entry toc-h3"><a href="#computing-distance">Computing Distance</a></li>
<li class="toc-entry toc-h3"><a href="#computing-similarity">Computing Similarity</a></li>
<li class="toc-entry toc-h3"><a href="#computing-using-domain-expertise">Computing Using Domain Expertise</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#generative-vs-discriminative-approach">Generative vs Discriminative Approach</a></li>
<li class="toc-entry toc-h2"><a href="#k-nn-optimality">k-NN Optimality</a>
<ul>
<li class="toc-entry toc-h3"><a href="#issues-with-k-nn-classification">Issues with k-NN Classification</a></li>
<li class="toc-entry toc-h3"><a href="#speed-issue-with-k-nn">Speed Issue with k-NN</a></li>
<li class="toc-entry toc-h3"><a href="#measurement-space-issue-with-k-nn">Measurement Space Issue with k-NN</a></li>
<li class="toc-entry toc-h3"><a href="#space-issues-with-k-nn">Space Issues with k-NN</a></li>
<li class="toc-entry toc-h3"><a href="#k-nn-summary">k-NN Summary</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#decision-tree-tree-based-classification">Decision Tree (Tree Based Classification)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#measuring-label-uncertainty">Measuring Label Uncertainty</a></li>
<li class="toc-entry toc-h3"><a href="#problems-with-decision-tree">Problems with Decision Tree</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#overfitting">Overfitting</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#perceptron-and-kernelization">Perceptron and Kernelization</a>
<ul>
<li class="toc-entry toc-h2"><a href="#linear-decision-boundary">Linear Decision Boundary</a>
<ul>
<li class="toc-entry toc-h3"><a href="#dealing-with-w_0">Dealing with $w_0$</a></li>
<li class="toc-entry toc-h3"><a href="#linear-classifier">Linear Classifier</a>
<ul>
<li class="toc-entry toc-h4"><a href="#learning-the-weights">Learning the Weights</a></li>
<li class="toc-entry toc-h4"><a href="#perceptron-algorithm">Perceptron Algorithm</a></li>
<li class="toc-entry toc-h4"><a href="#perceptron-algorithm-guarantees">Perceptron Algorithm Guarantees</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#non-linear-classifier">Non-Linear Classifier</a>
<ul>
<li class="toc-entry toc-h3"><a href="#generalizing-linear-classification">Generalizing Linear Classification</a>
<ul>
<li class="toc-entry toc-h4"><a href="#transformation-for-quadratic-boundaries">Transformation for Quadratic Boundaries</a></li>
<li class="toc-entry toc-h4"><a href="#theorem-for-linear-separability">Theorem for Linear Separability</a></li>
<li class="toc-entry toc-h4"><a href="#kernel-trick">Kernel Trick</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#using-kernel-trick-in-perceptron">Using Kernel Trick in Perceptron</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#support-vector-machine">Support Vector Machine</a>
<ul>
<li class="toc-entry toc-h2"><a href="#perceptron-and-linear-separability">Perceptron and Linear Separability</a></li>
<li class="toc-entry toc-h2"><a href="#svm-formulation">SVM Formulation</a></li>
<li class="toc-entry toc-h2"><a href="#slacked-svm">Slacked SVM</a></li>
<li class="toc-entry toc-h2"><a href="#finding-minimization">Finding Minimization</a>
<ul>
<li class="toc-entry toc-h3"><a href="#constrained-optimization">Constrained Optimization</a></li>
<li class="toc-entry toc-h3"><a href="#lagrange-penalty-method">Lagrange (Penalty) Method</a>
<ul>
<li class="toc-entry toc-h4"><a href="#convexity">Convexity</a></li>
<li class="toc-entry toc-h4"><a href="#convex-optimization">Convex Optimization</a></li>
<li class="toc-entry toc-h4"><a href="#weakstrong-duality-theorem">Weak/Strong Duality Theorem</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#optimizing-svm">Optimizing SVM</a>
<ul>
<li class="toc-entry toc-h4"><a href="#solving-svm-dual-problem">Solving SVM Dual Problem</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#regression">Regression</a>
<ul>
<li class="toc-entry toc-h2"><a href="#parametric-vs-non-parametric-regression">Parametric vs Non-Parametric Regression</a></li>
<li class="toc-entry toc-h2"><a href="#linear-regression">Linear Regression</a>
<ul>
<li class="toc-entry toc-h3"><a href="#solving-linear-regression">Solving Linear Regression</a>
<ul>
<li class="toc-entry toc-h4"><a href="#geometric-view-of-linear-regression">Geometric View of Linear Regression</a></li>
<li class="toc-entry toc-h4"><a href="#statistical-view-of-linear-regression">Statistical View of Linear Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#logistic-regression">Logistic Regression</a>
<ul>
<li class="toc-entry toc-h3"><a href="#statistical-interpretation">Statistical Interpretation</a></li>
<li class="toc-entry toc-h3"><a href="#solving-logistic-regression">Solving Logistic Regression</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#variations-of-linear-regression">Variations of Linear Regression</a>
<ul>
<li class="toc-entry toc-h3"><a href="#ridge-regression">Ridge Regression</a></li>
<li class="toc-entry toc-h3"><a href="#lasso-regression">Lasso Regression</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#regression-optimality">Regression Optimality</a></li>
<li class="toc-entry toc-h2"><a href="#non-parametric-regression">Non-Parametric Regression</a>
<ul>
<li class="toc-entry toc-h3"><a href="#kernel-regression">Kernel Regression</a></li>
<li class="toc-entry toc-h3"><a href="#consistency-theorem">Consistency Theorem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#statistical-learning-theory">Statistical Learning Theory</a>
<ul>
<li class="toc-entry toc-h2"><a href="#pac-learning">PAC Learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#simple-pac-learning-erm-algorithm">Simple PAC Learning: ERM Algorithm</a></li>
<li class="toc-entry toc-h3"><a href="#vc-theory">VC Theory</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#no-free-lunch">No Free Lunch</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#unsupervised-learning">Unsupervised Learning</a>
<ul>
<li class="toc-entry toc-h2"><a href="#clustering">Clustering</a>
<ul>
<li class="toc-entry toc-h3"><a href="#k-means">$k$-means</a>
<ul>
<li class="toc-entry toc-h4"><a href="#lloyds-method">Lloyd’s method</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#hierachical-clustering">Hierachical Clustering</a></li>
<li class="toc-entry toc-h3"><a href="#clustering-via-probabilities-mixture-modelling">Clustering via Probabilities Mixture Modelling</a>
<ul>
<li class="toc-entry toc-h4"><a href="#gaussian-mixture-modelling-gmm">Gaussian Mixture Modelling (GMM)</a></li>
<li class="toc-entry toc-h4"><a href="#learning-gmm-parameters---em-algorithm">Learning GMM Parameters - EM Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#dimensionality-reduction">Dimensionality Reduction</a>
<ul>
<li class="toc-entry toc-h3"><a href="#principal-components-analysis-pca">Principal Components Analysis (PCA)</a>
<ul>
<li class="toc-entry toc-h4"><a href="#eigen-decomposition">Eigen Decomposition</a></li>
<li class="toc-entry toc-h4"><a href="#normalizing-data-in-pca">Normalizing Data in PCA</a></li>
<li class="toc-entry toc-h4"><a href="#maximum-covariance-interpretation">Maximum Covariance Interpretation</a></li>
<li class="toc-entry toc-h4"><a href="#generalization-to-k1">Generalization to $k&gt;1$</a></li>
<li class="toc-entry toc-h4"><a href="#examples-of-pca">Examples of PCA</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#future-classes">Future Classes</a></li>
</ul>
  </div>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="logistics">Logistics</h1>

<p>Homework:</p>

<ul>
  <li>Must type your homework (no handwritten homework).</li>
  <li>Please include your name and UNI.</li>
</ul>

<p>Exams:</p>

<ul>
  <li>Exam1 will cover all the material discussed before Exam1</li>
  <li>Exam2 will be comprehensive and will cover all the course material covered during the semester.</li>
  <li>The exams are mainly conceptual, on mathematical models. So there will be no coding questions.</li>
</ul>

<p>Resources:</p>

<ul>
  <li>http://www.cs.columbia.edu/~verma/classes/ml/index.html</li>
</ul>

<h1 id="basics-and-mle">Basics and MLE</h1>

<p>Quick recap on notation.</p>

<p>Consider the input of:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210909090814464 (Small).png" alt="image-20210909090814464" /></p>

<p>then basically what we will do is:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210909090859737.png" alt="image-20210909090859737" style="zoom: 80%;" /></p>

<p>where we</p>

<ul>
  <li>use $d$ to represent that the <strong>dimensionality</strong> of the input measurement, since $d$=dimension</li>
  <li>in this case, we just converted an e.g. $10 \times 10$ sized image to vector with $d=100$</li>
</ul>

<p>Then, we need some <strong>function</strong> $f$ that can map that input to the output space $\mathcal{Y}$</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210909091235311.png" alt="" /></p>

<p>where here:</p>

<ul>
  <li>output space is has only a discrete set of $10$ values</li>
</ul>

<p>Then obviously, the aim of our task is to <mark>figure our $f$</mark>.</p>

<blockquote>
  <p>In summary, for Supervised Learning, we need:</p>

  <ul>
    <li><strong>Data</strong>: $(\vec{x}_1, y_1), (\vec{x}_2, y_2), …, (\vec{x}_m, y_m) \in \mathcal{X} \times \mathcal{Y}$</li>
    <li><strong>Assumption</strong>: there exists some optimal function $f^* : \mathcal{X} \to \mathcal{Y}$ such that $f^*(\vec{x}_i)=y_i$ for <strong>most</strong> $i$</li>
    <li><strong>Learning Task</strong>: given $n$ examples from the data, find approximation $\hat{f} \approx f^*$ (as close as possible)</li>
    <li><strong>Goal</strong>: find $\hat{f}$ that gives the mostly correct prediction in <mark>unseen examples</mark></li>
  </ul>

  <p>For Unsupervised Learning, the difference is that:</p>

  <ul>
    <li><strong>Data</strong>: $\vec{x}_1, \vec{x}_2, …, \vec{x}_m \in \mathcal{X}$</li>
    <li><strong>Learning Task</strong>: discover the structure/pattern given $n$ examples from the data</li>
  </ul>
</blockquote>

<p>Graphically, we basically do the following:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912150545581.png" alt="image-20210912150545581" style="zoom:80%;" /></p>

<p>The key is for models to <strong>generalize well</strong>, instead of just memorizing the training data. This is basically the hardest part.</p>

<h2 id="statistical-modelling-approach">Statistical Modelling Approach</h2>

<p>Consider we are given a picture, as shown above as well:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210909090814464.png" alt="image-20210909090814464" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>Heuristics</strong>:</p>

  <ul>
    <li>let this data be $\vec{x}_1$. Then idea is that the <mark>probability of $(\vec{x}_1, y_1=5)$ should be a lot more likely</mark> than $\vec{x}_1, y_1=1$, for example. Therefore, we can consider some non-uniform distribution of $(\mathcal{X} \times \mathcal{Y}) \sim D$.</li>
    <li>Then, we basically assume that we have $(\vec{x}_1, y_1), (\vec{x}_2, y_2), …$ <mark>drawn IID (Independently and identically distributed) from $\mathcal{D}$.</mark>
      <ul>
        <li><em>independent</em>: drawing data $(\vec{x}_2, y_2)$ does not depend on the value of $(\vec{x}_1, y_1)$</li>
        <li><em>identically distributed</em> : data are drawn from the same distribution $\mathcal{D}$</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Therefore, basically we are doing:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912152307709.png" alt="image-20210912152307709" style="zoom:67%;" /></p>

<p>Then, to select $\hat{f} \in \mathcal{F}$, basically there are two approaches:</p>

<ul>
  <li>Maximum Likelihood Estimation - MLE</li>
  <li>Maximum A Posteriori - MAP</li>
  <li>Optimization of some custom loss criterion</li>
</ul>

<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>

<p>Given some data, we consider our <strong>model</strong>/distribution being defined by some parameters $\theta$, such that:</p>

\[\mathcal{P} = \{ p_\theta | \theta \in \Theta \}\]

<p>for instance:</p>

<ul>
  <li>if you assumed a Gaussian distribution, then $\Theta \in \mathbb{R}^2$ would include any possible combination of $(\mu, \sigma^2)$</li>
  <li>so each model is defined by a $p_\theta$, and the set of all possible models is $\mathcal{P}$</li>
  <li>$p_\theta = p_\theta(\vec{x}_i)$, basically takes in our data and spits out the <strong>probability of generating this input</strong></li>
</ul>

<p>If each model $p_\theta$ is a <mark>probability model</mark>, then we can find $\theta$ that best fits the data by using the <mark>maximum likelihood estimation</mark>.</p>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <ul>
    <li>suppose we have models $p_1, p_2, …$, <strong>given</strong> my data being $\vec{x}_1, \vec{x}_2, …$, what it the most probable model/distribution that generated my data?</li>
  </ul>
</blockquote>

<p>So we define the <strong>likelihood</strong> to be:</p>

\[\mathcal{L}(\theta | X) := P(X|\theta) = P(\vec{x}_1, ..., \vec{x}_n | \theta)\]

<p>where this means that:</p>

<ul>
  <li>consider we are looking at some model with $\theta$, what is the <strong>probability that this model $p_\theta$ generated the data set $X$</strong>?</li>
</ul>

<p>Now, <mark>using the IID assumption</mark>, we can expand this into:</p>

\[P(\vec{x}_1, ..., \vec{x}_n | \theta) = \prod_{i=1}^n P(\vec{x}_i | \theta) = \prod_{i=1}^n p_\theta(\vec{x}_i)\]

<p>The interpretation is simple: How probable(or how likely) is the data $X$ given the model $p_\theta$?</p>

<blockquote>
  <p><mark>MLE</mark></p>

  <ul>
    <li>
      <p>Therefore, MLE is about:</p>

\[\arg \max_\theta \mathcal{L}(\theta | X) =  \arg \max_\theta \prod_{i=1}^n p_\theta (\vec{x}_i)\]
    </li>
  </ul>

</blockquote>

<hr />

<p><em>Example: Fitting the best Gaussian</em></p>

<p>Consider the case where we have some height data:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912154928174.png" alt="image-20210912154928174" style="zoom:67%;" /></p>

<p>For now, we <strong>ignore the labelling</strong>.</p>

<ul>
  <li>a better way would be to think that all those are having the same label, so we are learning currently $P[X=\vec{x}\vert Y=\text{some label}]$</li>
</ul>

<p>And suppose that we know <mark>this data is generated from a Gaussian</mark>, so then we know:</p>

\[p_\theta (x) = p_{\{\mu, \sigma^2\}}(x)L=\frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2})\]

<p>So our <strong>model space</strong> is the set of Gaussians, some of which looks like:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912155136689.png" alt="image-20210912155136689" style="zoom:80%;" /></p>

<p>Now, we just need to do the MLE:</p>

\[\begin{align*}
\arg \max_\theta \mathcal{L}(\theta | X) 
&amp;=  \arg \max_\theta \prod_{i=1}^n p_\theta (\vec{x}_i)\\
&amp;= \arg \max_\theta \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2})
\end{align*}\]

<p>while this may be painful to do, but we have some <mark>tricks</mark>:</p>

<ol>
  <li>
    <p>Using $\log$ instead, since logarithmic functions are <strong>monotonic</strong>, hence:</p>

\[\arg \max_\theta \,\,\mathcal{L}(\theta | X) = \arg \max_\theta \,\,
\log \mathcal{L}(\theta | X)\]
  </li>
  <li>
    <p>To find the maximum, we basically analyze the “stationary points” of the $\mathcal{L}$ by taking <mark>derivatives</mark></p>

    <ul>
      <li>for high order parameter, we need to use partial derivatives</li>
    </ul>
  </li>
</ol>

<p>Hence we consider:</p>

\[\begin{align*}
l(\theta)
&amp;= \log L(\theta) \\
&amp;= \log \prod_{i=1}^m \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(x_i - \mu)^2)}{2 \sigma^2} \right)}\\
&amp;= \sum_{i=1}^m \left( \log\frac{1}{\sqrt{2\pi} \sigma} - \frac{(x_i - \mu)^2}{2 \sigma^2}  \right)\\
&amp;= m \log\frac{1}{\sqrt{2\pi} \sigma} -\sum_{i=1}^m\left( \frac{(x_i - \mu)^2}{2 \sigma^2}  \right)\\
&amp;= m \log\frac{1}{\sqrt{2\pi} \sigma} -\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^m (x_i - \mu)^2
\end{align*}\]

<p>notice here the difference between the other note:</p>

<ul>
  <li>we had $(y_i - \theta^T x_i)^2$ instead of $(x_i - \mu)^2$, since we are currently ignoring the label.</li>
</ul>

<p>Now, computing the maximum:</p>

\[\arg \max_{\mu, \sigma^2} \left( m \log\frac{1}{\sqrt{2\pi} \sigma} -\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^m (x_i - \mu)^2 \right)\]

<p>First we compute $\mu$ by partial derivatives:</p>

\[0 = \nabla_\mu \left( m \log\frac{1}{\sqrt{2\pi} \sigma} -\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^m (x_i - \mu)^2 \right)\]

<p>Solving this yields:</p>

\[\mu_{MLE} = \frac{1}{n}\sum_{i=1}^m x_i\]

<p>for $m$ samples. This is basically the sample mean.</p>

<p>Similarly, maximizing $\sigma^2$:</p>

\[\sigma^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2\]

<p>Therefore, we have now found our best, MLE, model with Gaussian distribution $p_\theta = p_{{\mu, \sigma^2}}$.</p>

<ul>
  <li>in other words, MLE is just figuring out the <strong>best parameter for the assumed distribution $\mathcal{D}$  for the dataset $X$</strong></li>
</ul>

<hr />

<p>Now, for other examples, other models usually used are:</p>

<ul>
  <li>Bernoulli model (coin tosses)</li>
  <li>Multinomial model (dice rolls)</li>
  <li>Poisson model (rare counting events)</li>
  <li>Multivariate Gaussian Model - most often the case</li>
  <li>Multivariate version of other scale valued models (basically the first three)</li>
</ul>

<h3 id="multivariate-gaussian">Multivariate Gaussian</h3>

<p>Basically, we have:</p>

\[p(x;\mu, \Sigma) = \frac{1}{(2\pi)^{d} \det(\Sigma)} \exp\left( -\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu) \right)\]

<p>where:</p>

<ul>
  <li>
    <p>$\mu$ is a vector here in $\mu \in \mathbb{R}^d$</p>
  </li>
  <li>
    <p>$\Sigma \in \mathbb{R}^{d \times d}$ is the variance is now a covariance matrix, which is <strong>symmetric and positive semi-definite</strong></p>
  </li>
</ul>

<h3 id="mle-to-classification">MLE to Classification</h3>

<p>For labelled data, suppose labels are $y_1, y_2, … \in \mathcal{Y}$.</p>

<p>Then, our <mark>ending function</mark> should be (this is justified soon):</p>

\[\hat{f}(\vec{x}) = \arg \max_{y \in \mathcal{Y}} P(y \in Y | \vec{x})\]

<p>basically, if we have $(y_1=0, y_2=1)$, then we are deciding if $P(y=0\vert \vec{x})$ is larger or $P(y=1\vert \vec{x})$ is larger.</p>

<p>We can simplify this using <strong>Bayes Rule</strong>:</p>

\[\begin{align*}
\hat{f}(\vec{x}) 
&amp;= \arg \max_{y \in \mathcal{Y}} P(Y=y | X=\vec{x}) \\
&amp;= \arg \max_{y \in \mathcal{Y}} \frac{P(\vec{x} | y)P(y)}{P(\vec{x})}\\
&amp;= \arg \max_{y \in \mathcal{Y}} P(X=\vec{x} | Y=y)\cdot P(y)
\end{align*}\]

<p>where:</p>

<ul>
  <li>the third equality comes from the fact that we are looking for $\arg \max_{y \in \mathcal{Y}}$ which is independent of $P(\vec{x})$.</li>
  <li>the probability $P(y)$ is also called the <strong>class prior</strong>.
    <ul>
      <li>The probability of something happening <mark>regardless of your "features/condition"</mark>.</li>
    </ul>
  </li>
  <li>the probability $P(\vec{x}\vert y)$ is also called the <strong>class conditional/probability model</strong>.
    <ul>
      <li>Given a $y$, e.g. being fraud email, how is the feature $\vec{x}$ is distributed over it, e.g. each feature is distributed as a Gaussian</li>
      <li>once we decide on the above, we can plugin an input $\vec{x}^{(1)}$ and compute the probability</li>
    </ul>
  </li>
</ul>

<p>Now, we want to <mark>use MLE</mark> to find out the quantity $P(\vec{x}\vert y)$ and $P(y)$.</p>

<blockquote>
  <p><strong>Side Note</strong></p>

  <ul>
    <li>
      <p>In statistics, you might see priori and posterior estimation like the follows, though the meaning of them can be applied as the ones above.</p>

      <p>Given that the data samples $X=x_1,x_2,…,x_m$ are <em>distributed in some distribution</em> determined by $\mathcal{D}(\theta)$, and that the <strong>parameters $\theta$</strong> itself is distributed by $\theta \sim \mathcal{D}(\rho)$, then:</p>

\[P(\theta|X) = \frac{P(\theta)P(X|\theta)}{P(X)}\]

      <p>where, interpreting it as the following order makes sense:</p>

      <ol>
        <li>$P(X\vert \theta)$ indicates that, if we <em>fixed some parameter $\theta$</em>, then the probability of seeing the data $X$</li>
        <li>$P(\theta)$ indicates the probability of <em>seeing the above fixed $\theta$</em></li>
      </ol>

      <p>and the second one (with $P(\theta)$) is called the <strong>a priori estimation</strong>, and the <strong>posterior</strong> estimation is our resulting $P(\theta\vert X)$.</p>
    </li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the case of distinguishing between $\text{male}$ and  $\text{female}$, based on features such as height and weight.</p>

<p>Given the dataset of $X$.</p>

<p>Then, what we know already would be:</p>

<ul>
  <li>$P(Y = \text{male})$ = fraction of training data labelled as male
    <ul>
      <li>(<mark>note that the TRUE probability is not this, of course</mark>, this is just an estimate of what the true probability is)</li>
    </ul>
  </li>
  <li>$P(Y = \text{female})$ = fraction of training data labelled as female</li>
</ul>

<p>Finally, to learn the class conditions:</p>

<ul>
  <li>$P(X\vert Y=\text{male}) = p_{\theta_\text{male}}(X)$</li>
  <li>$P(X\vert Y=\text{female})= p_{\theta_\text{female}}(X)$</li>
</ul>

<p>This we can do MLE to find out $\theta_{\text{male}}$ and $\theta_{\text{female}}$.</p>

<ul>
  <li><mark>if it is normally distributed</mark>, then we can find the parameter = mean + variance matrix easily</li>
  <li>then we just use MLE to figure out $\mu, \sigma$ for the normal distribution</li>
</ul>

<p>Graphically, the fits might look like:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912170127324.png" alt="image-20210912170127324" style="zoom: 67%;" /></p>

<p>In a 3D fashion:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210912170201565.png" alt="image-20210912170201565" style="zoom: 80%;" /></p>

<p>And basically this is our first <strong>predictor</strong>!</p>

<hr />

<p>Now, coming back to the claim that:</p>

\[\hat{f}(\vec{x}) = \arg \max_{y \in \mathcal{Y}} P(y \in Y | \vec{x})\]

<p>First, we <mark>define</mark> the <strong>true accuracy</strong> of a classifier what have made $f$ to be:</p>

\[P_{\vec{x},y}[f(\vec{x}) = y] = \mathbb{E}_{(\vec{x},y)}[1\{ f(\vec{x}) = y \}]\]

<p>note that the notation here is precisely:</p>

<ul>
  <li>
    <p>$\mathbb{E}_{\text{random variable}}[\text{event}]$. So here, we are consider the randomness of $\vec{x},y$ joinly.</p>
  </li>
  <li>
    <p>here, we are assuming this is computed over the <mark>true population $(\vec{x},y) \sim \mathcal{D}$</mark>, hence equality.</p>
  </li>
  <li>
    <p>recall that the expected value would be calculated by integrating samples from</p>

\[\mathbb{E}_{(\vec{x},y)}[1\{ f(\vec{x}) = y \}] = \int_{(\vec{x},y)}1\{ f(\vec{x}) = y \}\cdot d\mu_{(x,y)}=  \int_{(\vec{x},y)}1\{ f(\vec{x}) = y \}\cdot P(\vec{x},y)dV\]

    <p>so we are integrating over all possible pairs of $(\vec{x},y)$ with a volume element $P(\vec{x},y)dV$ <strong>weighted by the probability</strong> of each pair, since the probability of each pair of $(\vec{x},y)$ happening is <em>not equal</em>.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Theorem: Optimal Bayes Classifier</strong></p>

  <p>Given a space of label $\mathcal{Y}$, consider</p>

\[\begin{align*}
\hat{f}(\vec{x}) &amp;= \arg \max_{y \in \mathcal{Y}} P(y | \vec{x}), \quad \text{Bayes Classifier} \\
g(\vec{x}) &amp;= \mathcal{X} \to \{0,1\}, \quad \text{Any Classifier}
\end{align*}\]

  <p>It can be proven that:</p>

\[P_{\vec{x},y}[g(\vec{x}) = y]  \le P_{\vec{x},y}[f(\vec{x}) = y]\]

  <p>i.e. the Bayes Classifier it the <strong>optimal</strong> if model are evaluated using the above definition of <strong>accuracy</strong>.</p>
</blockquote>

<p>However, note that being optimal does not mean being 100% correct. Suppose we have:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Optimal Bayes Accuracy = $1-\int \min_{y \in Y}P[ y\vert x]P[x]dx$</th>
      <th style="text-align: center">Optimal Bayes Accuracy = $1$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921171127631.png" alt="image-20210921171127631" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921171135242.png" alt="image-20210921171135242" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong><em>Proof:</em></strong></p>

<p>Consider any classifier $h$, and for simplicity, assume that the space of labels is ${0,1} = \mathcal{Y}$.</p>

<p>Now, consider <strong>some given $\vec{x}$</strong> and its label $y$:</p>

<p>First, we observe that:</p>

\[\begin{align*}
P[h(\vec{x})=y|X=\vec{x}]
&amp;= P[h(\vec{x})=1, Y=1|X=\vec{x}] + P[h(\vec{x})=0, Y=0|X=\vec{x}] \\
&amp;= 1\{ h(\vec{x})=1 \} \cdot P[Y=1 | X = \vec{x}] + 1\{h(\vec{x} = 0)\} \cdot P[Y=0 | X = \vec{x}] \\
&amp;= 1\{ h(\vec{x})=1 \} \cdot \eta(\vec{x}) + 1\{ h(\vec{x})=0 \} \cdot (1-\eta(\vec{x}))
\end{align*}\]

<p>where we have:</p>

<ul>
  <li>the second equality comes from the fact that we have <strong>fixed $\vec{x}$</strong>, i.e. we have no randomness in $\vec{x}$</li>
  <li>defined $\eta(\vec{x}) \equiv P[Y=1 \vert  X = \vec{x}]$</li>
  <li>though we fixed $\vec{x}$, the <strong>randomness in $y$ remained</strong> (hence the $P$ everywhere) since both $\vec{x},y$ are sampled from some distribution.</li>
</ul>

<p>Therefore, if we consider:</p>

\[\begin{align*}
P[f(\vec{x}) = y | X=\vec{x}] &amp;- P[g(\vec{x})=y|X=\vec{x}] \\
&amp;= \eta(\vec{x})\cdot [1\{ f(\vec{x})=1 \} - 1\{ g(\vec{x})=1 \}] + (1-\eta(\vec{x}))\cdot [1\{ f(\vec{x})=1 \} - 1\{ g(\vec{x})=1 \}] \\
&amp;= (2\eta(\vec{x})-1) [1\{ f(\vec{x})=1 \} - 1\{ g(\vec{x})=1 \}] 
\end{align*}\]

<p>We want to show that this is $\ge 0$. Recall that we have defined:</p>

<ul>
  <li>$\hat{f}(\vec{x}) = \arg \max_{y \in \mathcal{Y}} P(y \in Y \vert  \vec{x})$</li>
</ul>

<p>So we have <strong>two cases</strong>:</p>

<ul>
  <li>if $f(\vec{x})$ and $g(\vec{x})$ gave the same prediction, then it is $0$. Satisfies $\ge 0$</li>
  <li>if $f(\vec{x})$ is different with $g(\vec{x})$, then either:
    <ul>
      <li>$f(\vec{x})=1,  [1{ f(\vec{x})=1 } - 1{ g(\vec{x})=1 }]  &gt; 0$. Then since $f$ is defined by $\arg \max$, it means that $P[Y=1 \vert  X=\vec{x}]\ge P[Y=0 \vert  X=\vec{x}]$. Therefore, $P[Y=1 \vert  X=\vec{x}] \ge 1/2$. So $(2\eta(\vec{x})-1)  \ge 0$.</li>
      <li>$f(\vec{x})=0,  [1{ f(\vec{x})=1 } - 1{ g(\vec{x})=1 }]  &lt; 0$. By a similar argument, $(2\eta(\vec{x})-1)  \le 0$.</li>
      <li>In either case, the product $(2\eta(\vec{x})-1) [1{ f(\vec{x})=1 } - 1{ g(\vec{x})=1 }]  \ge 0$</li>
    </ul>
  </li>
</ul>

<p>Therefore, this means:</p>

\[P[f(\vec{x}) = y | X=\vec{x}] - P[g(\vec{x})=y|X=\vec{x}]  \ge 0\]

<p>Integrating over $X$ to remove the condition that we fixed a $X=\vec{x}$, we then complete the proof.</p>

\[P[f(\vec{x})=y] = \int_{\vec{x}}P[f(\vec{x}) = y | X=\vec{x}]\cdot P[X=\vec{x}]d\vec{x}\]

<hr />

<blockquote>
  <p><strong>Note</strong></p>

  <p>We see that the Bayes Classifier is “optimal” for the given criterion of accuracy. But this still <strong>imposes three main problems</strong>:</p>

  <ol>
    <li>What if the data is extremely unbalanced, such that $99\%$ data is positive? Then a function $f(x)=1$ could achieve $99\%$ accuracy given the definition we had.</li>
    <li>This is optimal if we get the <strong>true value of</strong> $P[X=\vec{x}\vert Y=y], P[Y=y]$, but we can only <strong>estimate</strong> it given our data.</li>
    <li>If we don’t know some information of which kind of distribution should be used for $P[X=\vec{x}\vert Y=y]$, we then need to do a <strong>non-parametric</strong> density estimation, which would require a <strong>large amount of data</strong> and the convergent <strong>rate</strong> would be $1/\sqrt{n}^d$, where $d$ is the number of features you have and $n$ the number of sample data.
      <ul>
        <li>this is really bad if we have a lot of features</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>Next, we will see one example of <em>estimating</em> the quantity:</p>

\[\hat{f}(\vec{x})= \arg \max_{y \in \mathcal{Y}} P[X=\vec{x} | Y=y]\cdot P[Y=y]\]

<p>note that</p>

<ul>
  <li>one known way to estimate $P[X=\vec{x} \vert  Y=y]$ would be using MLE, basically fitting the most likely distribution</li>
  <li>we will soon see another way of doing it using some additional assumption (i.e. Navies Bayes)</li>
  <li>other possibilities for $P[X=\vec{x} \vert  Y=y]$ could be to use Neural Networks, DL models etc.</li>
</ul>

<p>Basically, even if we know this is the <mark>optimal solution/framework</mark> to work under, we need to be able to compute the quantities such as $P[X=\vec{x} \vert  Y=y]$. But since we don’t know the true population, all we are doing are the estimates.</p>

<blockquote>
  <p><strong>In summary</strong></p>

  <ul>
    <li>There are many ways to estimate the <strong>class conditional</strong> $P[X=\vec{x} \vert  Y=y]$, unless you know something specific to the task, we don’t know what is the right way</li>
    <li>Probability density estimation of $P[X=\vec{x} \vert  Y=y]$ will degrade as data <strong>dimension increases</strong>, i.e. the convergent rate $1/\sqrt{n}^d$ requires <em>more data</em> (more $n$).
      <ul>
        <li>note that this assumes that we are imposing a <em>probability density over our training data</em> in our model. We will see that in cases such as <strong>Nearest Neighbor</strong>, we might need to worry about this.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="naïve-bayes-classifier">Naïve Bayes Classifier</h2>

<p>If we <mark>assume</mark> that <strong>individual features</strong> are <mark>independent given the class label.</mark> Then we have the Naïve Bayes:</p>

\[\begin{align*}
\hat{f}(\vec{x}) 
&amp;= \arg \max_{y \in \mathcal{Y}} P[X=\vec{x} | Y=y]\cdot P[Y=y]\\
&amp;= \arg \max_{y \in \mathcal{Y}} \prod_{j=1}^d P[X=\vec{x}^{(j)} | Y=y]\cdot P[Y=y]
\end{align*}\]

<p>where here, we are flipping the notation in our other note, such that:</p>

<ul>
  <li>superscript $x^{(j)}$ means the $j$-th feature</li>
  <li>subscript $x_i$ means the i-th sample data point</li>
</ul>

<blockquote>
  <p><strong>Advantage:</strong></p>

  <ul>
    <li>
      <p>Quick for coding and computation</p>
    </li>
    <li>
      <p>Maybe <em>fewer samples</em> is needed to figure out the distribution of a particular feature</p>

      <ul>
        <li>
          <p>e.g. if they are <em>dependent</em>, then you need to figure out if it is a positively correlated, negatively correlated, not correlated, etc. So you need to have <em>more samples</em> to know what is happening</p>
        </li>
        <li>
          <p>e.g. if you are doing a <mark>non-parametric fit</mark>, you are now estimating $d$ densities, each of which would be $1$-dimensional. This means that you reduced the data needed:</p>

\[\frac{1}{\sqrt{n}^d} \to d\cdot \frac{1}{\sqrt{n}}\]
        </li>
      </ul>
    </li>
  </ul>

  <p><strong>Disadvantage:</strong></p>

  <ul>
    <li>
      <p>The assumption often does not hold, as they might be correlation between features that <mark>matters for classification</mark>. This will result in giving bad estimates.</p>

      <p>For instance, consider:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921172020242.png" alt="image-20210921172020242" style="zoom: 50%;" /></p>

      <p>where for <strong>each green-labeled class</strong>, the correlation is difference <mark>but projection is the same</mark>. So assuming independence might cause some trouble. However, it might <em>not matter</em> for classification since the threshold value is clear/correct even if we used independence.</p>

      <ul>
        <li>at this level, whether if the correlation matters would be tested using <em>trial and error</em>.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="evaluating-quality-of-classifier">Evaluating Quality of Classifier</h2>

<p>Given a classifier $f$, we essentially need to compute the <strong>true accuracy</strong>:</p>

\[P_{\vec{x},y}[f(\vec{x}) = y] = \mathbb{E}_{(\vec{x},y)}[1\{ f(\vec{x}) = y \}]\]

<p>to decide which is a better classifier (this is one and the most common way of doing it), using accuracy.</p>

<p>However, since we don’t know the true population, we can only do an <mark>estimate</mark> using:</p>

\[\frac{1}{n}\sum_{i=1}^n 1\{ f(\vec{x}_i) = y_i \}\]

<p>using the <mark>test data</mark>.</p>

<ul>
  <li>if we have used the <strong>train data</strong>, then the approximation no longer holds because the IID could be falsified. In other words, the data would have been contaminated since we have already seen it. (e.g. think of a classifier being a lookup table)</li>
</ul>

<h3 id="bias">Bias</h3>

<p>The idea of bias has also been formalized in the other note, basically we want a low bias to be:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210916182422737.png" alt="image-20210916182422737" style="zoom:50%;" /></p>

<p>the idea can be <strong>formalized</strong> by the following.</p>

<blockquote>
  <p><strong>Bias</strong></p>

  <ul>
    <li>The <strong>idea</strong> is that, for each time you sample <em>some set of data points from $\mathcal{D}$</em>, your model $\hat{\theta}$ would have trained and split out the learnt parameter. If on average, those parameters are close to the actual parameter $\theta$, then it is <strong>unbiased</strong> (see bottom left figure as well). Otherwise it is biased.</li>
  </ul>
</blockquote>

<p>An <mark>unbiased estimator would have:</mark></p>

\[\mathbb{E}_{\vec{x}\sim \mathcal{D}}[\hat{\theta}(\vec{x})] = \lang \hat{\theta}(\vec{x}) \rang = \theta\]

<p>where the equation means:</p>

<ul>
  <li>$\hat{\theta}$ is your <strong>estimator/learning model</strong> that spits out the learnt parameters given some data point</li>
  <li>$\theta$ is the <strong>true/correct parameter</strong> for the distribution $\mathcal{D}$, which is unknown (you want to approximate)</li>
  <li>the first equality is because expected value is the same as mean</li>
</ul>

<p>Notice that again, it is defined upon the <mark>true population</mark>.</p>

<hr />

<p><em>Example</em>:</p>

<p>Suppose your data $X$ is generated from $\vec{x} \sim B(p)$ being a Bernoulli distribution, so $\theta=p$ is the true paramter.</p>

<p>Suppose your estimator then <strong>corrected assumed a Bernoulli distribution</strong> by giving $\hat{\theta} \to \mathbb{R}$, but stupidly:</p>

\[\hat{\theta}_{\vec{x}\in X}(\vec{x}) = \vec{x}_1\]

<p>which basically:</p>

<ul>
  <li>spits out the <em>first data point</em> in a training set</li>
</ul>

<p>Then this is <em>actually an unbiased estimator</em> because:</p>

\[\mathbb{E}_{\vec{x} \sim \mathcal{D}}[\hat{\theta}(\vec{x})] = \mathbb{E}_{\vec{x} \sim \mathcal{D}}[\vec{x}] = p = \theta\]

<p>since it is a <em>Bernoulli distribution</em>.</p>

<h3 id="consistency">Consistency</h3>

<p>The other idea is <strong>consistency</strong>.</p>

<blockquote>
  <p><strong>Consistency</strong></p>

  <ul>
    <li>The idea is, that if we increase the number of samples, a <strong>consistent estimator $\hat{\theta}$</strong> should have spitted out the <strong>correct parameter</strong> as our number of samples approaches infinity.</li>
  </ul>
</blockquote>

<p>We say a model is consistent if:</p>

\[\lim_{n \to \infty} \hat{\theta}_n(\vec{x}) = \theta\]

<p>where the equations is similar to the above:</p>

<ul>
  <li>$\hat{\theta}_n$ means how many sample data $n$ is used in the training process</li>
</ul>

<hr />

<p><em>Example</em></p>

<p>The same example as above, if we use the estimator</p>

\[\hat{\theta}_{\vec{x}\in X}(\vec{x}) = \vec{x}_1\]

<p>for data generated from a Bernoulli $\vec{x} \sim B(p)$, this would be an <strong>inconsistent estimator</strong>, since:</p>

\[\lim_{n \to \infty} \hat{\theta}_{n}(\vec{x}) = \vec{x}_1 \neq p\]

<p>In general, bias usually has nothing to do with consistency, and vice versa.</p>

<h1 id="nearest-neighbors-and-decision-trees">Nearest Neighbors and Decision Trees</h1>

<p>Consider back the data we had before:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210913085640033.png" alt="image-20210913085640033" style="zoom:67%;" /></p>

<p>instead of trying to fit a probability density for each class of data, one <strong>intuitive way</strong> would be looking at its neighbors. If its <mark>closest</mark> neighbor is a $\text{male}$, then it is probably also a $\text{male}$</p>

<h2 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h2>

<p>First, we need to define what we mean by <mark>distance between two data points</mark>. There are in fact many ways to do it:</p>

<ul>
  <li>Actually compute some sort of distance (smaller the distance, closer the examples)</li>
  <li>Compute some sort of similarity (higher the similarity, closer the examples)</li>
  <li>Can use domain expertise to measure closeness</li>
</ul>

<p>Once we know how to compute the distance, we are basically done. We just assign the <strong>same label to its closest neighbor</strong>.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>Since we are assigning the label to the same label as its “closest neighbor”, we are very <strong>vulnerable to noise</strong>. However, we can reduce this noise by taking the <strong>majority of $k$-nearest neighbors</strong>.</li>
  </ul>
</blockquote>

<h3 id="computing-distance">Computing Distance</h3>

<p>Normally we will have $\mathcal{X} = \mathbb{R}^d$, then a few natural ways to computation would be:</p>

<p><strong>Euclidean Distance</strong> between to data $\vec{x}_1, \vec{x}_2$:</p>

\[\begin{align*}
\rho(\vec{x}_1, \vec{x}_2)
&amp;= \left[(x_1^{(1)}-x_2^{(1)})+...+(x_1^{(d)}-x_2^{(d)})\right]^{1/2} \\
&amp;= \left[(\vec{x}_1 - \vec{x}_2)^T(\vec{x}_1 - \vec{x}_2)\right]^{1/2} \\
&amp;= \|\vec{x}_1 - \vec{x}_2\|_2
\end{align*}\]

<p>Other normed distance would include the form:</p>

\[\rho(\vec{x}_1, \vec{x}_2)
= \left[(x_1^{(1)}-x_2^{(1)})^{p}+...+(x_1^{(d)}-x_2^{(d)})^p\right]^{1/p} 
= \|\vec{x}_1 - \vec{x}_2\|_p\]

<p>where if we have:</p>

<ul>
  <li>$p=2$ then it is Euclidean Distance</li>
  <li>$p=1$ then it is <strong>Manhattan Distance</strong> (i.e. we are basically adding up the differences)</li>
  <li>$p=0$ then we are count up the <em>number</em> of differences</li>
  <li>$p=\infty$ then we are finding the <em>maximum</em> distance</li>
</ul>

<p>the last two needs to be computed by taking a limit.</p>

<h3 id="computing-similarity">Computing Similarity</h3>

<p>Some typical ways would involve the inverse of a distance:</p>

\[\rho(\vec{x}_1, \vec{x}_2) = \frac{1}{1+\| \vec{x}_1 - \vec{x}_2 \|_2}\]

<p>Another one would be the <strong>cosine similarly</strong>, which depends on the <strong>angle $\ang$ between two vector</strong>:</p>

\[\rho(\vec{x}_1, \vec{x}_2) = \cos(\ang (\vec{x}_1, \vec{x}_2)) = \frac{\vec{x}_1 \cdot \vec{x}_2}{\| \vec{x}_1\|_\mathrm{2}  \,\, \| \vec{x}_2\|_\mathrm{2}}\]

<h3 id="computing-using-domain-expertise">Computing Using Domain Expertise</h3>

<p>For things related to genome/mutation, we might consider the <strong>edit distance</strong>:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210913092440161.png" alt="image-20210913092440161" style="zoom:80%;" /></p>

<p>where here:</p>

<ul>
  <li>since we only need two edits to make them the same, $\rho(x_1, x_2)=2$.</li>
  <li>this is quite useful for genome related research.</li>
</ul>

<p>Another example would be the <strong>distance between</strong> rankings of webpages:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210913092349788.png" alt="image-20210913092349788" style="zoom: 80%;" /></p>

<p>where here:</p>

<ul>
  <li>bubble sort distance: the number of swaps needed to make one ranking order the same as another.</li>
  <li>so in this case, it will be $\rho(x_1, x_2)=1$ since we only needed one swap</li>
</ul>

<h2 id="generative-vs-discriminative-approach">Generative vs Discriminative Approach</h2>

<p>We have basically covered both approaches already.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center"><strong>Generative</strong></th>
      <th style="text-align: center">Discriminative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Idea</td>
      <td style="text-align: center">we are trying to <em>model the sample population</em> by giving some distribution</td>
      <td style="text-align: center">we are classifying directly</td>
    </tr>
    <tr>
      <td>Example</td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210913092736425.png" alt="image-20210913092736425" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210913092853664.png" alt="image-20210913092853664" style="zoom:80%;" /></td>
    </tr>
    <tr>
      <td>Advantages</td>
      <td style="text-align: center">A probability model gives <strong>interpretation</strong> of how data gets generated from population</td>
      <td style="text-align: center">Typically better classification <strong>accuracies</strong> since we are doing it directly</td>
    </tr>
    <tr>
      <td> </td>
      <td style="text-align: center">MLE estimation of the <mark>true probability parameter $\rho$</mark> such that true distribution is $D(\rho)$ converges with rate $\vert \rho-\hat{p}_n\vert \le 1/\sqrt[\leftroot{-3}\uproot{3}d]{n}$  where $\hat{p}_n$ is the <mark>estimated parameters</mark> based on the $n$ training samples</td>
      <td style="text-align: center">The rate of convergence to optimal bayes classifier is shown in <a href="https://cseweb.ucsd.edu/~dasgupta/papers/nn-rates.pdf">this paper</a></td>
    </tr>
    <tr>
      <td>Disadvantages</td>
      <td style="text-align: center">Need to pick/assume a <strong>probability model</strong>, and is doing more work then required to do classification so <strong>prone to errors</strong>!</td>
      <td style="text-align: center">Gives <strong>no understanding</strong> of the population</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Though it seems that generative approach of assuming a distribution is usually risky, there is an approach of having an <strong>non-parametric estimator</strong>. Yet the problem is that computing that non-parametric distribution takes <strong>too much samples points</strong> then we can usually supply.</li>
</ul>

<hr />

<p><em>For instance</em>:</p>

<p>Suppose we are looking at the students who scored 100 on a test (i.e. label $y=100$), and we want to find distribution of the GPA (i.e. a feature) with respect to that label $y=100$.</p>

<p>We might then have the following data.</p>

<table>
  <thead>
    <tr>
      <th>GPA</th>
      <th>Number of 100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2.0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2.1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>3.0</td>
      <td>10</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>4.0</td>
      <td>12</td>
    </tr>
  </tbody>
</table>

<p>Graphically, we can then <strong>fit a distribution</strong> by using the data as it is, i.e. <strong>the histogram itself</strong></p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210916190416978.png" alt="image-20210916190416978" style="zoom: 50%;" /></p>

<p>which can be made into a function by basically have a bunch of constant functions.</p>

<p>Now, at this point, it probably make sense that the <strong>rate for which this estimator $\hat{\theta}$ approaches correct result</strong> would be:</p>

\[|\theta - \hat{\theta}_n| \le \frac{1}{\sqrt{n}}\]

<p>But what if we have now <strong>two features?</strong> Then we need the number of data points <em>squared</em> to make sure <strong>all pair of values have some histogram</strong> in the $\mathbb{R}^2$. So in general, as the <mark>dimension $d$ of data increases</mark>:</p>

\[|\theta - \hat{\theta}_n| \le \frac{1}{\sqrt{n}^d}\]

<p>which means that if we want the difference to be <em>smaller than $1/2$</em>, we need $n \ge 2^d$ data, <strong>which is growing exponentially</strong> as we collect more features.</p>

<h2 id="k-nn-optimality">k-NN Optimality</h2>

<p>If we are using the $k$-th nearest neighbor, this has some really <strong>good advantages</strong>.</p>

<blockquote>
  <p><strong>Theorem</strong>:</p>

  <ul>
    <li>For fixed $k$, as and (number of samples) $n \to \infty$ , $k$-NN classifier error <strong>converges to no more than twice</strong> Bayes classifier error (which is the optimal one).</li>
  </ul>
</blockquote>

<p>This will be proved below.</p>

<blockquote>
  <p><strong>Theorem</strong></p>

  <ul>
    <li>If $k\to \infty$ and $n \to \infty$, but $n$ is growing faster than $k$ such that $k/n\to0$, then $k$-NN classifier <strong>converges to Bayes classifier</strong>.</li>
  </ul>
</blockquote>

<p>This is really technical, so it will not be covered here.</p>

<p><strong>Notice</strong> that</p>

<ul>
  <li>this is “much better” than the MLE with fitting model and estimating $P[X=\vec{x} \vert  Y=y]\cdot P[Y=y]$. In that case, we are assuming we know which distribution the data comes from, but if we made a mistake on that, then $P[X=\vec{x} \vert  Y=y]$ could be <strong>not even close</strong> to the optimal Bayes classifier.</li>
</ul>

<hr />

<p><em>Proof Sketch: k-NN with fixed k</em></p>

<p>First consider with $k=1$. Then consider that we are <strong>given a fixed data point $x_{\text{test}}=x_t$</strong>, we want to evaluate the error of a <strong>given classifier</strong> $f$.</p>

<p><mark>Heuristic</mark>: What should be the <em>probability of error in this case?</em> Suppose the actual label for $x_t$ is a <strong>given</strong> $y_t$, then obviously:</p>

\[\text{Error}=1\{ f(x_t) \neq y_t \}\]

<p>But since we are <em>not given the $y_t$</em> but only $x_t$, we need to consider the <mark>true distribution of $y_t$ given $x_t$</mark>, which gives the <mark>probability of error</mark> being:</p>

\[\text{Probability of Error}=\mathbb{E}_{y_t}[1\{ f(x_t) \neq y_t \}]\]

<p>where basically it is averaging over the <strong>distribution</strong> of $y_t\vert X=x_t \sim D$.</p>

<p>Now, we need to consider some <mark>1-NN classifier $f_n$</mark> given <mark>some arbitrary</mark> training dataset $D_n={X_n, Y_n}$ of size $n$. Let the nearest neighbor of a dataset $\mathcal{D}_n$ be $x_n$, and the label of that being $y_n$.</p>

<ul>
  <li>note that the classifier $f_n$ depends on size $n$ since the nearest point could change</li>
</ul>

<p>Then the probability of error $P[e]$ for a given $x_t$ for test <strong>assumed drawn IID</strong> would be:</p>

\[\begin{align*}
\lim_{n \to \infty} &amp;P_{y_t, D_n}[e|x_t]\\
&amp;= \lim_{n \to \infty} \int P_{y_t, Y_n}[e|x_tX_n]P[X_n|x_t]dX_n \\
&amp;= \lim_{n \to \infty} \int P_{y_t, y_n}[e|x_t,x_n]P[x_n|x_t]dx_n \\
&amp;= \lim_{n \to \infty} \int \left[1-\sum_{y \in \mathcal{Y}}P[y_t=y,y_n=y|x_t,x_n] \right] P[x_n|x_t] dx_n\\
&amp;= \lim_{n \to \infty} \int \left[1-\sum_{y \in \mathcal{Y}}P[y_t=y|x_t]P[y_n=y|x_n] \right] P[x_n|x_t] dx_n\\
&amp;= 1- \sum_{y \in \mathcal{Y}}P[y_t=y|x_t]^2
\end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>$P_{y_t, D_n}[e\vert x_t]$ means that $y_t, \mathcal{D}_n$ will be randomly drawn/are random variables.</p>
  </li>
  <li>
    <p>the first line of equality comes from the fact that:</p>

\[P[A] = \sum_B P[A,B] = \sum_B P[A|B]P[B]\]

    <p>and it it is conditioned, then simply add the condition:</p>

\[P[A|C] = \sum_B P[A,B|C] = \sum_B P[A|BC]P[B|C]\]
  </li>
  <li>
    <p>the second equality comes from the fact that a 1-NN classifier <strong>only depends on 1 nearest neighbor $x_n$</strong>. Therefore, it is <em>indepedent of all the others</em>:</p>

\[P[e|X_n] = P[e|x_n]\]
  </li>
  <li>
    <p>the third equality comes from evaluation the error. Notice that we are computing over the <mark>true distribution of $\mathcal{Y}$</mark></p>
  </li>
  <li>
    <p>the fourth equality comes from that fact that $x_t$ and $x_n$ are drawn <strong>independently</strong></p>
  </li>
  <li>
    <p>the last equality assumes a <mark>reasonable sample space</mark> such that as $n \to \infty$, $x_n \to x_t$ (basically getting really close).</p>

    <ul>
      <li>
        <p>in more detailed, this is what happens:</p>

\[\begin{align*}
\lim_{n\to\infty}&amp;\int\left[1-\sum_{y \in \mathcal{Y}}P[y_t=y|x_t]P[y_n=y|x_n]\right]P[x_n|x_t]dx_n \\
&amp;= \lim_{n\to\infty}\int P[x_n|x_t]dx_n - \lim_{n\to\infty}\int\left[\sum_{y \in \mathcal{Y}}P[y_t=y|x_t]P[y_n=y|x_n]P[x_n|x_t]\right]dx_n\\
&amp;= 1 - \lim_{n\to\infty}\int\left[\sum_{y \in \mathcal{Y}}P[y_t=y|x_t]P[y_n=y|x_n]P[x_n|x_t]\right]dx_n \\
&amp;= 1 - P[y_t=y|x_t]\lim_{n\to\infty}\int\left[\sum_{y \in \gamma}P[y_n=y|x_n]P[x_n|x_t]\right]dx_n \\
&amp;= 1 - P[y_t=y|x_t]\lim_{n\to\infty}\sum_{y \in \mathcal{Y}}\left[\int P[y_n=y|x_n]P[x_n|x_t]dx_n\right] \\
&amp;= 1 - P[y_t=y|x_t]\lim_{n\to\infty}\sum_{y \in \mathcal{Y}}P[y_n=y|x_t] \\
&amp;= 1 - \sum_{y \in \mathcal{Y}}P[y_t=y|x_t]^2
\end{align*}\]
      </li>
      <li>
        <p>the meaning here is that we are sampling <em>twice</em> the $y$ value given that we got $X=x_t$.</p>
      </li>
    </ul>
  </li>
</ul>

<p>So now, we get that:</p>

\[\lim_{n \to \infty} P_{y_t, D_n}[e|x_t] = 1- \sum_{y \in \mathcal{Y}}P[y_t=y|x_t]^2\]

<p>Now suppose we are comparing this with a <strong>Bayes Classifier</strong> ($\arg \max_{y \in \mathcal{Y}} P(X=\vec{x} \vert  Y=y)\cdot P(y)$), which will assign some label $y^*$ to the data $x_t$. Then hence:</p>

\[\begin{align*}
1- \sum_{y \in \mathcal{Y}}P[y_t=y|x_t]^2 
&amp;\le 1 - P^2[y_t=y^*|x_t]\\
&amp;\le 2(1 - P[y_t=y^*|x_t])\\
&amp;= 2P^*[e|x_t]
\end{align*}\]

<p>where:</p>

<ul>
  <li>$P^*[e\vert x_t]$ is the error of Bayes Classifier</li>
  <li>the second inequality comes from the fact that $0 \le P[y_t=y^*\vert x_t] \le 1$</li>
</ul>

<p>then we just need to integrate over $x_t$ (i.e. $P[x_t]dx_t$)for removing the conditional.</p>

<hr />

<h3 id="issues-with-k-nn-classification">Issues with k-NN Classification</h3>

<p>In general, three main problems:</p>

<ul>
  <li>Finding the $k$ closest neighbor <strong>takes time</strong>! (we need to compute this for every single input $x_t$)</li>
  <li>Most times the ‘closeness’ in raw measurement spaceis not good!</li>
  <li>Need to <strong>keep all the training data around</strong> during test time!
    <ul>
      <li>as compared with MLE, which only keeps the computed parameter</li>
    </ul>
  </li>
</ul>

<h3 id="speed-issue-with-k-nn">Speed Issue with k-NN</h3>

<p>Finding the $k$ closest neighbor <strong>takes time</strong>! If we are given a $x_t$ for prediction, then we need</p>

\[O(nd)\]

<p>where $n$ is the number of training data and $d$ is dimension</p>

<p><strong><em>Heuristic</em></strong></p>

<p>To do it faster, we could do a sorting. H</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921090602537.png" alt="image-20210921090602537" style="zoom: 80%;" /></p>

<p>where the hit is that we need $O(n\log n)$ for preprocessing.</p>

<p>But obviously we cannot compute all the <em>distances</em> and order them, since that takes $O(nd)$. Instead, we can <strong>compute the threshold</strong> via some binary search:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921091331850.png" alt="image-20210921091331850" style="zoom:67%;" /></p>

<p>first, we order the data points. Then, we</p>

<ol>
  <li>find the first <mark>median</mark> $T_0$, and see if $x_t$ is larger or smaller than that</li>
  <li>suppose it is larger. Then we <strong>look in the right region only</strong></li>
  <li>then we compute the median of the left region of $T_0$, which is $T_{1,2}$, and we compare that to $x_t$</li>
  <li>repeat 1-3 until we just have some constant number of training sample $c$ left in a region</li>
  <li>then compute within that $c$ samples and find the closet neighbor</li>
</ol>

<p>So basically the search looks like:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921091017319.png" alt="image-20210921091017319" style="zoom:80%;" /></p>

<p>where we see the <mark>problem</mark> is that:</p>

<ul>
  <li>if the threshold was $T_{1,2}$ and $x_t$ is larger than that, we would have <strong>returned the rightest point</strong>. Yet the actual nearest neighbor is the one almost on the $T_{1,2}$.</li>
  <li>so we are <mark>approximating</mark> the closest neighbor (which actually does <em>not</em> reduce the accuracy)</li>
</ul>

<p><strong><em>High Dimensional Data</em></strong></p>

<p>The the idea is that:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921093753125.png" alt="image-20210921093753125" style="zoom:80%;" /></p>

<p>given some data points (green):</p>

<ol>
  <li>
    <p>pick a <em>coordinate to work on</em> (the optimal way is to pick the axis with <mark>largest variance</mark>)</p>

    <ul>
      <li>
        <p>e.g. you should pick the green “axis” to split</p>

        <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210921094325321.png" alt="image-20210921094325321" style="zoom:67%;" /></p>
      </li>
    </ul>
  </li>
  <li>
    <p>split the data <em>in that coordinate</em> by finding the median</p>
  </li>
  <li>
    <p>compare the the component of $x_t$ in that coordinate</p>
  </li>
  <li>
    <p>repeat until the region(cell) has some constant number of training sample $c$ left in a region</p>

    <ul>
      <li>so $c$ is a fixed, tunable parameter</li>
    </ul>
  </li>
  <li>
    <p>find the closet neighbor among those $c$ data points.</p>
  </li>
</ol>

<p>This process above is also called the <strong>$k$-d trees</strong>.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>this split is sensible if the <strong>features</strong> are sensible/all equally useful.</li>
  </ul>
</blockquote>

<p>Other ways to find neighbor fast would be:</p>

<p><strong>Tree-based methods</strong>: (recursive)</p>

<ul>
  <li>k-d trees
    <ul>
      <li>by default it splits on feature axis, but you can do a PCA and split on those eigenvectors. This is another method.</li>
    </ul>
  </li>
  <li>Cover trees</li>
  <li>Navigation nets</li>
  <li>Ball trees</li>
  <li>Spill trees</li>
  <li>…</li>
</ul>

<p><strong>Compression-based methods</strong>:</p>

<ul>
  <li>Locality Sensitive Hashing (LSH)</li>
  <li>Vector Quantization (VQ) methods</li>
  <li>Clustering methods</li>
  <li>…</li>
</ul>

<h3 id="measurement-space-issue-with-k-nn">Measurement Space Issue with k-NN</h3>

<p>Often times we don’t know what measurements are helpful for classification a priori. So we <strong>need to know</strong> <em>which features</em> should be more important/useless.</p>

<p>Recall the old task: learn a classifier to distinguish $\text{males}$ from $\text{females}$</p>

<p>But say we don’t know which measurements would be helpful, so we measure a whole bunch:</p>

<ul>
  <li>height</li>
  <li>weight</li>
  <li>blood type</li>
  <li>eye color</li>
  <li>Income</li>
  <li>Number of friends</li>
  <li>Blood sugar level</li>
  <li>…</li>
</ul>

<p><mark>Observation</mark>:</p>

<ul>
  <li>Feature measurements not-relevant (noisy) for the classification task simply <mark>distorts</mark> NN distance computations</li>
  <li>Even highly correlated relevant measurements (signal) <mark>distorts</mark> the distance comparisons</li>
</ul>

<hr />

<p>On idea to solve this would be:</p>

<ol>
  <li>
    <p><mark>Re-weight</mark> the contribution of each feature to the distance computation</p>

\[\begin{align*}
\rho (\vec{x}_1, \vec{x}_2; \vec{w})
&amp;= \left[w_1 \cdot (\vec{x}_1^{(1)}-\vec{x}_2^{(1)})^2+\cdots + w_d \cdot (\vec{x}_1^{(d)}-\vec{x}_2^{(d)})^2\right]^{1/2}\\
&amp;=\left[ (\vec{x}_1 - \vec{x}_2)^TW (\vec{x}_1 - \vec{x}_2)\right]^{1/2}
\end{align*}\]

    <p>and that $W$ would be a diagonal matrix with $w_i$ terms, assuming we want to use <strong>L2 Euclidean distance</strong>.</p>

    <ul>
      <li>
        <p>essentially, we want to have some <em>linear transformation</em> space</p>
      </li>
      <li>
        <p>in general, even if we include <em>mixing</em> such that $\vec{x} \to L\vec{x}$ where $L$ is non-symmetric nor diagonal. Then, it can be shown that for L2 Euclidian distance, we can show that $W=L^TL$, meaning that $W$ is <strong>symmetric and positive semi definite</strong>.</p>
      </li>
    </ul>
  </li>
</ol>

<p>Our goal is to have $\rho (\vec{x}_1, \vec{x}_2; \vec{w})$:</p>

<ul>
  <li>data samples from <strong>same</strong> class yield small values</li>
  <li>data samples from <strong>different</strong> class yield large values</li>
</ul>

<p>Then we can create <strong>two sets</strong>:</p>

<ul>
  <li>Similar set $S={ (\vec{x}_i,\vec{x}_j)\vert y_i = y_j }$</li>
  <li>Different set  $D={ (\vec{x}_i,\vec{x}_j)\vert y_i \neq y_j }$</li>
</ul>

<p>And we can <mark>minimize the cost function $\Psi$</mark></p>

\[\Psi(\vec{w}):=\lambda \sum_{(\vec{x}_i,\vec{x}_j) \in S}\rho(\vec{x}_i,\vec{x}_j;\vec{w}) -(1- \lambda) \sum_{(\vec{x}_i,\vec{x}_j) \in D}\rho(\vec{x}_i,\vec{x}_j;\vec{w})\]

<p>where:</p>

<ul>
  <li>$\lambda\in (0,1)$ is a <strong>hyper-parameter</strong> that you can <em>pick according to your preference</em>, which side do you place emphasis on
    <ul>
      <li>it might be useful if you picked $\lambda$ relative to the <em>size of $S$ and $D$</em>.</li>
    </ul>
  </li>
  <li>basically we want to minimize $\sum_{(\vec{x}<em>i,\vec{x}_j) \in S}\rho(\vec{x}_i,\vec{x}_j;\vec{w})$ and maximize $\sum</em>{(\vec{x}_i,\vec{x}_j) \in D}\rho(\vec{x}_i,\vec{x}_j;\vec{w})$.</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>we cannot pick values such as $\lambda =1$, because then we will get $\vec{w}=\vec{0}$ since we know the distance is non-negative. A similar argument is for $\lambda = 0$</li>
    <li>since $\Psi(\vec{w})$ is <em>linear in $W$</em>, essentially it is doing a <strong>linear transformation</strong> of the input space.</li>
  </ul>
</blockquote>

<h3 id="space-issues-with-k-nn">Space Issues with k-NN</h3>

<p>We need to keep all the data in our trained model. Is there some ways to save some space?</p>

<p>Suppose we are given a red data sample:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210922204158134.png" alt="image-20210922204158134" style="zoom:67%;" /></p>

<p>then we can just <strong>assign a label to each region/cell</strong>, by giving the <em>majority label</em> to the region. Then we <em>do not need to keep all the data</em>.</p>

<ul>
  <li>again, it is about <em>approximation</em>.</li>
</ul>

<p>So in the end, it looks like this:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210922204255362.png" alt="image-20210922204255362" style="zoom: 67%;" /></p>

<p>Then the space requirement is reduced to:</p>

\[\text{\# cells} = \min \{n, \approx \frac{1}{r^d}\}\]

<p>for cells of radius $r$.</p>

<h3 id="k-nn-summary">k-NN Summary</h3>

<ul>
  <li>A simple and intuitiveway to do classification</li>
  <li>Don’t need to deal with probability modeling</li>
  <li>Care needs to be taken to select the distance metric</li>
  <li>Can improve the basic speed and space requirements for NN</li>
</ul>

<h2 id="decision-tree-tree-based-classification">Decision Tree (Tree Based Classification)</h2>

<p>$k$-d tree construction was optimizing for:</p>

<ul>
  <li><strong>come up with the nearest neighbor</strong> in a faster and more space efficient way</li>
  <li>but what if <em>two cells</em> are assigned with the same label? Isn’t that a waste of time/space since we could have just kept one label?</li>
</ul>

<p>For instance:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210922210240178.png" alt="image-20210922210240178" style="zoom:67%;" /></p>

<p>where we see the splitting in the right is basically useless.</p>

<p>Therefore, $k$-d tree construction was  <mark>NOT</mark> optimizing for <strong>for classification accuracy</strong> directly.</p>

<hr />

<p><strong>Heuristic</strong></p>

<ul>
  <li>Rather than selecting arbitrary feature and splitting at the median (our k-NN model), select the feature and threshold that <strong>maximally reduces label uncertainty</strong> within that cell!</li>
</ul>

<p>For instance:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210922210916025.png" alt="image-20210922210916025" style="zoom: 67%;" /></p>

<p>where:</p>

<ol>
  <li>pick a coordinate and split such that the <em>purity of label</em> would be highest (improved)</li>
  <li>repeat until some given condition is met (e.g. each leaf/cell contains at least 5 data points)</li>
</ol>

<p>In the end, you just get a bunch of <strong>thresholds</strong>, which then becomes a <mark>tree</mark> for input data to be classified.</p>

<hr />

<p>Therefore, now we need to work out how to measure <em>uncertainty/impurity</em> of labels.</p>

<h3 id="measuring-label-uncertainty">Measuring Label Uncertainty</h3>

<p>Let $p_y$ be the <strong>fraction of training labelled $y$ in a region $C$</strong>.</p>

<p>Some criteria for <strong>impurity/uncertainty</strong> within a region $C$ would be:</p>

<ul>
  <li>
    <p>classification error:</p>

\[u(C):= 1 - \max_y p_y\]
  </li>
  <li>
    <p>Entropy</p>

\[u(C):= \sum_{y \in \mathcal{Y}} p_y \log_2 \frac{1}{p_y}\]

    <p>the idea behind this is that if an event is <em>unbiased (e.g. coin)</em>, then $p_y=0.5$ and you will get entropy $1$. If the coin is <em>perfectly biased</em>, $p_H=1,p_T=0$, you will get no disorder so that entropy is $0$.</p>
  </li>
  <li>
    <p>Gini Index (from economics):</p>

\[u(C):=1- \sum_{y \in \mathcal{Y}}p_y^2\]
  </li>
</ul>

<p>Then the idea is to find $C$ such that $u(C)$ is <mark>minimized</mark>. So we need to find a <strong>feature $F$ and the threshold $T$</strong> that maximally reduces the  uncertainty of labels:</p>

\[\arg\max_{F,T} [u(C) - (p_L \cdot u(C_L) + p_R \cdot u(C_R))]\]

<p>where:</p>

<ul>
  <li>$u(C)$ was the <em>parent cell’s uncertainty</em></li>
  <li>$p_L$ is the fraction of the parent cell data in the left cell</li>
  <li>$p_R$ is the fraction of the parent cell data in the right cell</li>
  <li>so $p_L \cdot u(C_L) + p_R \cdot u(C_R)$ is the uncertainty after a split into left and right region</li>
  <li>we want to <strong>maximize the reduction</strong> in uncertainty <strong>after a split</strong>, hence the minus sign</li>
</ul>

<h3 id="problems-with-decision-tree">Problems with Decision Tree</h3>

<p>Tree complexity is highly dependent on data geometry in the feature space:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923085354437.png" alt="image-20210923085354437" style="zoom:50%;" /></p>

<p>where we have basically only rotated the data for 45 degrees.</p>

<p>The problem for a complex classifier is that <mark>the more complex your classifier</mark>, the <mark>poorer it generalizes</mark></p>

<ul>
  <li>complexity here basically would be the number of nodes/thresholds</li>
</ul>

<p>On the other hand, this is useful for:</p>

<ul>
  <li><strong>interpretation</strong>. They are more interpretable, explaining why you gave a particular exmaple.</li>
</ul>

<h2 id="overfitting">Overfitting</h2>

<p>This is the <strong>empirical observation</strong>.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923090344140.png" alt="image-20210923090344140" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>model complexity would be measured by the <strong>number of parameters</strong> in the estimator</li>
  <li><mark>overfitting</mark> occurs when you are fitting to the noise (to the right)</li>
  <li><mark>underfitting</mark> occurs when your model is too simple (to the left)</li>
</ul>

<p>note that the gap between the test error and training error depends on the task/data.</p>

<h1 id="perceptron-and-kernelization">Perceptron and Kernelization</h1>

<p>Another way to see the past models is that:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MLE</th>
      <th style="text-align: center">k-NN</th>
      <th style="text-align: center">Decision Tree</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923094227168.png" alt="image-20210923094227168" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923094243517.png" alt="image-20210923094243517" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923094237125.png" alt="image-20210923094237125" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially, we are figuring out a <strong>boundary</strong> that separates the classification.</p>

<hr />

<p><strong>Heuristic</strong>:</p>

<p>Why don’t we find that <strong>boundary directly</strong>?</p>

<ul>
  <li>linear decision boundary</li>
  <li>non-linear decision boundary</li>
</ul>

<hr />

<h2 id="linear-decision-boundary">Linear Decision Boundary</h2>

<p>Now, given some data, we basically want to come up with:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923095049580.png" alt="image-20210923095049580" style="zoom:67%;" /></p>

<p>We can describe the plane by (in lower dimension, a line):</p>

\[w^T \vec{x} = w^T\vec{a} = -w_0,\quad \text{or}\quad  g(\vec{x}) = w^T\vec{x}+w_0 = 0\]

<p>for $\vec{w}$ being the normal vector $\vec{w} \in \mathbb{R}^d$, and $\vec{a}$ being a vector in the plane, $w_0$ being a constant.</p>

<ul>
  <li>
    <p>for example, in $d=1$, we have:</p>

\[g(x) = w_1 x + w_0\]

    <p>so we have $d+1$ parameters</p>
  </li>
</ul>

<p>Hence, our <mark>classifier $f(x)$</mark> from the decision boundary would be:</p>

\[f(x):= 
\begin{cases}
+1 &amp; \text{if } g(x) \ge 0\\
-1 &amp; \text{if } g(x) &lt; 0
\end{cases} \quad = \text{sign}(\vec{w}^T\vec{x}+w_0)\]

<p>which works because:</p>

<ul>
  <li>
    <p>$\vec{w}^T\vec{x}=\vert w\vert \vert x\vert \cos\theta$, and suppose we have $\vec{w}$ looking like this:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923163950963.png" alt="image-20210923163950963" style="zoom:50%;" /></p>

    <p>so we see that all vectors <strong>below</strong> that plane will have $g(x) &lt; 0$ due to $\cos \theta &lt; 0$</p>
  </li>
</ul>

<h3 id="dealing-with-w_0">Dealing with $w_0$</h3>

<blockquote>
  <p><strong>Trick</strong></p>

  <ul>
    <li>
      <p>one trick that we will use would be to rename:</p>

\[g(\vec{x}) = \vec{w}^T\vec{x}+w_0 = \vec{w}^{'T}\vec{x}^{'}\]

      <p>where we have:</p>

\[\begin{cases}
\vec{w}' = \begin{bmatrix}
w_0\\
w_1\\
\vdots\\
x_d
\end{bmatrix} \\
\vec{x}' = \begin{bmatrix}
1\\
x_1\\
\vdots\\
x_d
\end{bmatrix} \\
\end{cases}\]

      <p>and in fact, $w_0$ is also referred as the <mark>bias</mark>, since it is shifting things.</p>
    </li>
  </ul>
</blockquote>

<p>So essentially, we lifted the dimension up:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original Data</th>
      <th style="text-align: center">Lifted Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923165856283.png" alt="image-20210923165856283" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923165954191.png" alt="image-20210923165954191" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>one advantage of this lifted plane is that:</p>

<ul>
  <li>the lifted plane must have <strong>gone through the origin</strong>, since $g(\vec{x}) = \vec{w}^{‘T}\vec{x}^{‘}=0$ is <strong>homogenous</strong>.</li>
</ul>

<h3 id="linear-classifier">Linear Classifier</h3>

<p>Therefore, essentially we are having a model/classifier that does:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923170837443.png" alt="image-20210923170837443" style="zoom:80%;" /></p>

<p>where:</p>

<ol>
  <li>takes a linear combination/weighted sum of the <em>features</em> of a given data $\vec{x}$, i.e. compute $g(\vec{x})$</li>
  <li>pass the result $g(\vec{x})$ to $\text{sign}$ function (nonlinear) to make a classification</li>
</ol>

<p>So our aim is to find out the <strong>optimal value for $\vec{w}$</strong>!</p>

<hr />

<p><strong>Heuristic</strong></p>

<p>Essentially, our final predictor is $f=f(g(\vec{x}))$, which in turns depend on $\vec{w}$. The <em>last step</em> of our optimization should be taking the computing the minimization of $\text{error}(f)$ , which means we need to <strong>take derivatives</strong>. It is <em>not good</em> if $f$ is a <mark>not differentiable</mark> function.</p>

<p>Hence, a reasonable step to do would be to approximate $f$ as a <strong>continuous function</strong> first, and then the rest should work.</p>

<hr />

<p>Therefore, we usually use an alternative:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923173429871.png" alt="image-20210923173429871" style="zoom:80%;" /></p>

<blockquote>
  <p><strong>Note: Neural Network</strong></p>

  <ul>
    <li>
      <p>the structure shown above can be interpreted as a <em>unit of neuron</em> (which biologically is triggered based on some activation energy threshold):</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923173852559.png" alt="image-20210923173852559" style="zoom:50%;" /></p>

      <p>where in this case, we get “triggered” if $f(g(\vec{x})) &gt; 0$ for example.</p>
    </li>
    <li>
      <p>then, essentially we combine a <em>network of neurons</em> to get neuron network:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210923173825080.png" alt="image-20210923173825080" style="zoom: 50%;" /></p>

      <p>this is the basic architecture behind a neural network, where nonlinearity comes from the <em>nonlinear functions at the nodes</em>.</p>

      <p>The amazing fact here is that:</p>

      <ul>
        <li><mark>this network can approximate any smooth function!</mark></li>
        <li>hence it is very useful</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Now, we go back to the single perceptron/neuron case. Essentially, a single linear function $f=\sigma(w^Tx)$, and we need to find $w$.</p>

<h4 id="learning-the-weights">Learning the Weights</h4>

<p>Given some labeled training data, bias included, $(\vec{x}_1,y_1),(\vec{x}_2,y_2),…,(\vec{x}_n,y_n)$, we want to find the <em>optimal $w$</em> such that the <mark>training error is minimized.</mark></p>

\[\arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n 1\{ \text{sign}(\vec{w}^T \vec{x}) \neq y_i \}\]

<p>where we are using $\text{sign}$ instead of $\sigma$ to be more exact now. And if you think about this, we <em>cannot compute the derivatives</em> of this. In fact, minimizing this is actually <strong>NP-hard</strong> or even approximate.</p>

<ul>
  <li>
    <p>For instance, if you used something like:</p>

\[\arg\max \sum y_i\cdot (\sigma(\vec{w}^T \vec{x}_i)-1)\]

    <p>the problem is that then, (it can be proven) that there exist some dataset where the <em>distance</em> between $\vec{w}^{<em>}$ and the $\vec{w}$ will be very large. Essentially, this *approximation is NOT</em> an approximation since the difference between the optimal solution is unbounded.</p>
  </li>
</ul>

<hr />

<p><em>For Instance</em></p>

<p>Consider that the data looks like this:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927204005994.png" alt="image-20210927204005994" /></p>

<p>since we <em>cannot take derivatives</em> of the above eq.53, we basically have to try it out. And this would them be NP-hard.</p>

<ul>
  <li>maybe we should try to change its representation to some other linearly separable parts <em>first</em>?</li>
</ul>

<hr />

<p>So the idea is to consider some <mark>assumptions</mark> that might simplify the problem: suppose the <mark>training data IS linearly separable</mark>. Then what can we do?</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927204334284.png" alt="image-20210927204334284" style="zoom: 50%;" /></p>

<p>such that there is a linear decision boundary which can <strong>perfectly</strong> separate the training data.</p>

<ul>
  <li>and here, we might want to make the <strong>margin $\gamma$</strong> as <strong>large as possible</strong></li>
</ul>

<p>Then, <mark>under this assumption</mark>, we can then can find <strong>$\vec{w}$ such that $y_i (\vec{w} \cdot \vec{x}_i) \ge 0$</strong> for all $i$. Then, we can do a constraint optimization</p>

<ul>
  <li>
    <p>we use $\ge$ instead of $&gt;$ because we are assuming $\gamma \ge 0$ instead of $\gamma &gt; 0$.</p>
  </li>
  <li>
    <p>this is doable, so something like:</p>

\[\DeclareMathOperator*{\argmin}{arg\,min}
\argmin_{w\text{ s.t. }y_i (\vec{w} \cdot \vec{x}_i) \ge 0, \forall i}||w||^2\]

    <p>(this is not the only function you can minimize, just an example) Yet, there is a much easier way -&gt; Perceptron Algorithm</p>
  </li>
</ul>

<h4 id="perceptron-algorithm">Perceptron Algorithm</h4>

<p>The idea is <em>basically do a “gradient descent”</em>. The algorithm is as follows:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927205947846.png" alt="image-20210927205947846" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>basically, if on $t=k$ step you found an error of prediction, you update $\vec{w} := \vec{w}+y \vec{x}$</li>
</ul>

<p>Yet some proofs are not clear <em>yet</em>:</p>

<ol>
  <li>is this guaranteed to terminate?</li>
  <li>can you actually find the correct $\vec{w}$?</li>
</ol>

<p><em>For Example</em></p>

<p>Consider you started with:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Started with $t-1$, got an error</th>
      <th style="text-align: center">Update</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927210337851.png" alt="image-20210927210337851" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927210442456.png" alt="image-20210927210442456" style="zoom: 50%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center">error: marked negative but should be positive</td>
      <td style="text-align: center">though it is not guaranteed you fixed it in one update</td>
    </tr>
  </tbody>
</table>

<p>The other case is</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Started with $t-1$, got an error</th>
      <th style="text-align: center">Update</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927210837510.png" alt="image-20210927210837510" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927210854658.png" alt="image-20210927210854658" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<h4 id="perceptron-algorithm-guarantees">Perceptron Algorithm Guarantees</h4>

<blockquote>
  <p><strong>Theorem</strong> (Perceptron mistake bound)</p>

  <ul>
    <li>
      <p>Assume there is a unit length $\vec{w}^*$ that separate the training sample $S$ with margin $\gamma$.</p>

      <p>Let $R = \max_{\vec{x}\in S}\vert \vert \vec{x}\vert \vert$ be the radius of your data, for your sample being $S$.</p>

      <p>Then we claim that the perceptron algorithm will make <mark>at most</mark>:</p>

\[T:= \left(\frac{R}{\gamma}\right)^2 \text{ mistakes}\]

      <p>So that the algorithm will <strong>terminate</strong> in $T$ rounds, since you only can have $T$ mistakes.</p>
    </li>
  </ul>

</blockquote>

<p><em>Proof</em></p>

<p>The key quantity here we think about is: <mark>how far away is $\vec{w}^{(t)}$ from $\vec{w}^*$</mark>?</p>

<ul>
  <li>essentially we want to know the <em>angle between the two vector</em>, which then is related to $\vec{w}^{(t)}\cdot \vec{w}^*$.</li>
</ul>

<p>Suppose the perceptron algorithm makes a mistake in iteration $t$, then (remember we start with $w^{0}$ at $t=1$), then it means a <strong>data</strong> $\vec{x}$ is classified <strong>wrongly</strong>. Hence:</p>

\[\begin{align*}
\vec{w}^{(t)} \cdot \vec{w}^*
&amp;= (\vec{w}^{(t-1)} + y\vec{x}) \cdot \vec{w}^* \\
&amp;\ge \vec{w}^{(t-1)}\cdot \vec{w}^* + \gamma
\end{align*}\]

<p>because $y\vec{x} \cdot \vec{w}^<em>$ must have been *identified correctly</em>, and that at least $\gamma$ away by definition of $\vec{w}^*$.</p>

<p>Additionally:</p>

\[\begin{align*}
||w^{(t)}||^2
&amp;= || \vec{w}^{(t-1)} + y\vec{x} ||^2 \\
&amp;= ||\vec{w}^{(t-1)}||^2 + 2y(\vec{w}^{(t-1)}\cdot \vec{x}) + ||y\vec{x}||^2 \\
&amp;\le ||\vec{w}^{(t-1)}||^2 + R^2
\end{align*}\]

<p>since $y(\vec{w}^{(t-1)}\cdot \vec{x})$ is classified <em>wrongly</em>, so that this must be negative.</p>

<p>Then, since we know that for all iterations $t$, we get:</p>

\[\begin{cases}
\vec{w}^{(t)} \cdot \vec{w}^* \ge \vec{w}^{(t-1)}\cdot \vec{w}^* + \gamma \\
||w^{(t)}||^2 \le ||\vec{w}^{(t-1)}||^2 + R^2
\end{cases}\]

<p>from above. Hence, <strong>if we made $T$ mistakes/rounds of update</strong>, then</p>

\[\begin{cases}
\vec{w}^{(T)} \cdot \vec{w}^* \ge T\gamma \\
||w^{(T)}||||\vec{w}^*|| \le R\sqrt{T}
\end{cases}\]

<p>because:</p>

<ul>
  <li>the <em>first inequality comes from</em> considering:
    <ol>
      <li>made a mistake on $t=1$, then $\vec{w}^{(1)} \cdot \vec{w}^* \ge 0 + \gamma$</li>
      <li>made a mistake on $t=2$, then $\vec{w}^{(2)} \cdot \vec{w}^* \ge \vec{w}^{(t-1)}\cdot \vec{w}^* + \gamma \ge 2\gamma$ by substituting from above</li>
      <li>etc.</li>
    </ol>
  </li>
  <li>the second inequality comes from a similar idea, so that $\vert \vert \vec{w}^{(T)}\vert \vert ^2 \le TR^2$, and since $\vert \vert \vec{w}^*\vert \vert =1$, then the inequality is obvious</li>
</ul>

<p>Finally, connecting the two inequality:</p>

\[T\gamma \le \vec{w}^{(T)} \cdot \vec{w}^* \le ||w^{(T)}||||\vec{w}^*|| \le R\sqrt{T}\]

<p>Hence we get that:</p>

\[T \le \left(\frac{R}{\gamma} \right)^2\]

<h2 id="non-linear-classifier">Non-Linear Classifier</h2>

<p>One obvious problem with a linear classifier is that if you have the following data:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210930145036701.png" alt="image-20210930145036701" style="zoom:50%;" /></p>

<p>where technically, we have:</p>

<ul>
  <li>${0,1}^2 = X$, such that the cardinality $\vert X\vert =4$.</li>
  <li>$y=\text{XOR}(x^{(1)}, x^{(2)})$ for $\vec{x} \in X$.</li>
</ul>

<p>so we see that <strong>no linear classifier can classify the above data perfectly correctly</strong>.</p>

<ul>
  <li>
    <p>but note that an optimal Bayes classifier would have been able to classify it perfectly. (Anyway Bayes classifier is nonlinear)</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210930150504805.png" alt="image-20210930150504805" style="zoom:50%;" /></p>

    <p>where the boundary even for a 1-D Gaussian <em>can be nonlinear</em>.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>The idea of a linear classifier is that the <mark>decision boundary</mark> induced by a linear classifier is LINEAR (AFFINE) in the <strong>untransformed input space</strong>.</li>
  </ul>
</blockquote>

<p>So often the data is <em>not linearly separable</em>, then we need some <mark>tricks</mark>.</p>

<ul>
  <li>(Basically applying nonlinear transformation to an input space, so that the data in the end is linearly separable in that space)</li>
</ul>

<h3 id="generalizing-linear-classification">Generalizing Linear Classification</h3>

<p>Consider the following data:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210930150815651.png" alt="image-20210930150815651" style="zoom:80%;" /></p>

<p>where the data <mark>is separable</mark>, but <mark>not linearly separable</mark>.</p>

<p>In this case, <strong>suppose we knew beforehand</strong> the separation boundary is a perfect circle:</p>

\[g(\vec{x}) = w_1 x_1^2 + w_2x_2^2 + w_0 = 0\]

<p>which is basically an equation of a circle, parametrized by $\vec{w}$. so basically:</p>

<ul>
  <li>
    <p>$w_1=w_1=1$ and $w_0 = -r^2$ for $r$ being the radius of a <em>unit circle IN THIS CASE</em>.</p>
  </li>
  <li>
    <p>in general, it should then look like:</p>

\[g(\vec{x}) = w_1 x_1^2 + w_2x_2^2 + w_3x_1x_2 + w_4x_1+w_5x_2+w_0 =0\]

    <p>for ellipses as well.</p>
  </li>
</ul>

<p>But we can consider:</p>

\[\begin{align*}
 w_1 x_1^2 + w_2x_2^2 + w_0 = w_1 \chi_1 + w_2 \chi_2 + w_0
\end{align*}\]

<p>such that we have <strong>linear/affine</strong> in $\chi$ space.</p>

<blockquote>
  <p><em>Reminder: Linear Function</em></p>

  <ul>
    <li>
      <p>linearity test in high dimension input:</p>

\[f(c_1a + c_2b) = c_1f(a) + c_2f(b)\]

      <p>then $f:\mathbb{R}^d\to \mathbb{R}$.</p>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>
      <p>After we have applied a <mark>feature transformation</mark> $\phi(x_1 , x_2)\mapsto (x_1^2 , x_2^2)$, then $g$ becomes <mark>linear in $\phi$-feature space</mark>. (i.e. we don’t look at $X$ now, we look at data in $\phi(X)$ instead).</p>

      <ul>
        <li>Then, once in $\phi(X)$ it is linearly separable, we just use Perceptron and we are done.</li>
        <li>feature transformation is sometimes also called the <strong>Kernel Transformation</strong></li>
      </ul>
    </li>
    <li>
      <p>graphically, this is what happened:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210930152612906.png" alt="image-20210930152612906" style="zoom: 80%;" /></p>
    </li>
    <li>
      <p>yet the problem is that we don’t know <mark>what should be $\phi$</mark> beforehand.</p>

      <ul>
        <li>This is why we have soon some kernel tricks which would compute an <em>infinite power of polynomial instead</em>.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h4 id="transformation-for-quadratic-boundaries">Transformation for Quadratic Boundaries</h4>

<p>Now, we continue exploring some the <strong>general form</strong> for <mark>quadratic boundaries</mark>. (so only up to power of 2)</p>

<p>Suppose we have for only two features, $\vec{x} \in R^2$:</p>

\[g(\vec{x})= w_1 x_1^2 + w_2x_2^2 + w_3x_1x_2 + w_4x_1+w_5x_2+w_0  = \sum_{p+q\le 2} w_{p,q}x_1^px_2^q\]

<p>with:</p>

<ul>
  <li>
    <p>$x_i$ being the $i$-th feature of the data point $\vec{x}$</p>
  </li>
  <li><strong>doing feature transformation</strong> of (two features) $\phi(x_1,x_2) \to (x_1^2, x_2^2, x_1x_2, x_1,x_2, 1)$</li>
  <li>so basically $x_1^px_2^q$ are the new $\chi$ dimension</li>
</ul>

<p>Then, if we consider data of <strong>$d$ features</strong>, we basically have:</p>

\[g(\vec{x}) = \sum_{i,j}^d\sum_{p+q\le 2} w_{p,q}x_i^px_j^q\]

<p>where:</p>

<ul>
  <li>$x_i$ being the $i$-th feature of the data point $\vec{x}$</li>
  <li><strong>doing feature transformation</strong> of ($d$ features) $\phi(x_1, …,x_d) \mapsto (x_1^2,x_2^2,…,x_d^2,x_1x_2,…x_{d-1}x_d,x_1,…,x_d)$
    <ul>
      <li>so you approximately needs $d^p$, where $p$ is the degree of polynomial you want to go up to.</li>
    </ul>
  </li>
  <li>captures all <mark>pairwise interactions between each feature</mark>. Note that only <em>pairwise</em> though.</li>
</ul>

<p>So the problem is that we see, even for <em>only using quadratics</em>, we have too many terms -&gt; too many parameters to fit.</p>

<h4 id="theorem-for-linear-separability">Theorem for Linear Separability</h4>

<blockquote>
  <p><strong>Theorem</strong></p>

  <p>Given $n$ <mark>distinct points</mark> $S=\vec{x}_1,…,\vec{x}_n$</p>

  <p>There <mark>exists</mark> a <mark>feature transformation</mark> (can be anything, but only looks at feature, not labels) such that for <em>any labelling</em> of $\mathcal{S}$ is linearly separable <mark>in that transformed space</mark>!</p>
</blockquote>

<hr />

<p><em>Proof</em></p>

<p>Given $n$ distinct points, we basically want to show a transformation such that we can <strong>find a $\vec{w}$ in that space</strong> that perfectly separates the data.</p>

<p>Consider the transformation, for the $i$-th given data point $\vec{x}_i$</p>

\[\phi(\vec{x}_i) = \begin{bmatrix}
0\\
\vdots\\
1\\
\vdots\\
0
\end{bmatrix}=\text{only i-th component is 1}\]

<p>Then, we need $\vec{w}\cdot \phi(\vec{x}_i)$ to gives us a boundary. This can be easily achieved by:</p>

\[\vec{w}=\begin{bmatrix}
y_1\\
\vdots\\
y_n
\end{bmatrix}\]

<p>where we see that:</p>

<ul>
  <li><strong>only</strong> when we are <strong>fitting $\vec{w}$ we look at our labels</strong></li>
  <li>this formation gives $\vec{w}\cdot \phi(\vec{x}_i)=y_i$.</li>
</ul>

<p>So if we know that $y_i \in {-1,+1}$, then we can have the classifier:</p>

\[\text{sign}(\vec{w}\cdot \phi(\vec{x}_i))\]

<p>which is a linear classifier in $\phi(\vec{x})$ space.</p>

<blockquote>
  <p><strong>Intuitively</strong></p>

  <p>If each data point is in its own dimension, then I can <em>build a separating plane</em> by considering in the $\phi(\vec{x})$ space:</p>

  <ol>
    <li>build a plane that classifies the <em>first point</em> $\vec{x}_1$ correctly</li>
    <li><strong>rotate</strong> the plane <strong>but fix its intersection on $\hat{x}_1$</strong> so that $\vec{x}_2$ can now be classified correctly</li>
    <li><strong>rotate</strong> the plane <strong>but fix its intersection on $\hat{x}_1, \hat{x}_2$</strong> so that $\vec{x}_3$ can now be classified correctly</li>
    <li>repeat until $\vec{x}_n$ is also classified correctly</li>
  </ol>

  <p>And this would work because all data points are now “independent in a new dimension”.</p>
</blockquote>

<hr />

<p>Though this finishes the proof,</p>

<ul>
  <li>but <mark>that transformation cannot be generalized</mark> to test data points. So we need to find some reasonable transformations.</li>
  <li>the <mark>computational complexity</mark> is high (in $\Omega(d’)$ where $d’$ is the new dimension)</li>
  <li>also you have a high chance of overfitting (<mark>model complexity</mark>)</li>
</ul>

<h4 id="kernel-trick">Kernel Trick</h4>

<p>First we discuss how to deal with computation complexity, where even writing down the vector in high dimension will take much time.</p>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <p>Recall that the classifier is computing $\text{sign}(\vec{w}\cdot \vec{x})$. And in the end, you <strong>will</strong> see that learning $\vec{w}$ <em>can be written as</em>:</p>

\[\vec{w}=\sum_k\alpha_ky_x\vec{x}_k\]

  <ul>
    <li>see section <a href="#Using Kernel Trick in Perceptron">Using Kernel Trick in Perceptron</a></li>
  </ul>

  <p>Then computing $f(\vec{x})=\text{sign}(\vec{w}\cdot \vec{x})$ will only be computing:</p>

\[f(\vec{x})=\text{sign}(\vec{w}\cdot \vec{x}) = \text{sign}\left(\vec{x}\cdot \sum_{k=1}^n\alpha_ky_x\vec{x}_k\right) =  \text{sign}\left(\sum_{k=1}^n\alpha_ky_x(\vec{x}_k\cdot \vec{x})\right)\]

  <p>So we <mark>technically just need to make sure $\vec{x}_i \cdot \vec{x}_j$ is fast</mark>. And this is doable even if $\vec{x}\to \phi(\vec{x})$ will be <strong>large in dimension</strong> in the new/transformed dimension, because some kernel transformation can <mark>do $\phi(\vec{x}_i)\cdot \phi(\vec{x}_j)$ fast.</mark></p>
</blockquote>

<p>An illustration with an example is the fastest.</p>

<p>Consider doing a simple case of mapping to polynomial of degree $2$.</p>

<ul>
  <li>so we are transforming from input with $d$ dimension to $d’\approx d^2$ dimension.</li>
</ul>

<blockquote>
  <p><strong>Note that</strong>:</p>

  <ul>
    <li>
      <p>in general, if we are doing polynomial with power of $p$, then we consider <strong>each term</strong> being:</p>

\[(\_,\_,\_,\_,...,\_)\quad \text{with $p$ blanks}\]

      <p>for instance if $p=3$, with $\vec{x}=[a,b,c,d]^T$, then one term could be:</p>

\[a*c^2\to (a,c,c)\]

      <p>so approximately we have the <mark>new dimension being $d'=d^p$</mark>.</p>
    </li>
  </ul>
</blockquote>

<p>Additionally, suppose our polynomial looks like:</p>

\[\vec{x} \mapsto (x_1^2,...,x_d^2,\sqrt{2}x_1x_2,...,\sqrt{2}x_{d-1}x_d,...,\sqrt{2}x_d,1) = \phi(\vec{x})\]

<p>Then obviously <mark>computing $\phi(\vec{x}_i) \cdot \phi(\vec{x}_j)$</mark> directly will take $O(d^2)$.</p>

<ul>
  <li>the fact that it needs to be $\sqrt{2}$ will not affect classification in the end, since the weight is controlled by $\vec{w}$ anyway.</li>
</ul>

<p>But we know that the above is <strong>equivalent</strong> to:</p>

\[\phi(\vec{x}_i)\cdot \phi(\vec{x}_j) = (1+\vec{x}_i\cdot \vec{x}_j)^2\]

<p>which is <mark>only $O(d)$.</mark> So for some <mark>specific transformation $\phi$</mark>, we can make computation efficient.</p>

<hr />

<p><em>Proof</em>:</p>

<p>For a 2D case above, let us take $x_1 = (a_1,a_2)$ and $x_2=(b_1, b_2)$. Then the transformation does:</p>

\[\phi(x_1)=\begin{bmatrix}
a_1^2\\
a_2^2\\
\sqrt{2}a_1a_2\\
\sqrt{2}a_1\\
\sqrt{2}a_2\\
1
\end{bmatrix}, \quad \phi(x_2)=\begin{bmatrix}
b_1^2\\
b_2^2\\
\sqrt{2}b_1b_2\\
\sqrt{2}b_1\\
\sqrt{2}b_2\\
1
\end{bmatrix}\]

<p>Before computing $\phi(x_1)\cdot \phi(x_2)$ explicitly, let’s look at the trick:</p>

\[(1+x_1\cdot x_2)^2 =\left[ 1+(a_1b_1 + a_2b_2) \right]^2 =1+2a_1b_1+2a_2b_2+2a_1b_1a_2b_2+a_1^2b_1^2+a_2^2b_2^2\]

<p>which is exactly $\phi(x_1)\cdot \phi(x_2)$</p>

<hr />

<blockquote>
  <p><strong>RBF (Radial Basis Function)</strong></p>

  <p>So we have seen one kernel trick, and it turns out that this kernel transformation <mark>gets up to infinite dimension</mark>.</p>

\[\vec{x} \mapsto \left(\exp(-||\vec{x}-\alpha||^2)\right)_{\alpha \in \mathbb{R}^d},\quad \vec{x}\in \mathbb{R}^d\]

  <p>this transformed vector even needs <mark>infinite space</mark> to write down. But the dot products $\phi(\vec{x}_i),\phi(\vec{x}_j)$ is:</p>

\[\phi(\vec{x}_i)\cdot \phi(\vec{x}_j)=\exp(-||\vec{x}_i - \vec{x}_j||^2)\]

  <p>which again <mark>becomes $O(d)$</mark></p>
</blockquote>

<p>To understand this, we need to consider <em>another way of visualizing a vector</em>.</p>

<p>First, we can write a vector to be:</p>

\[\vec{x} = \begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_d
\end{bmatrix} = (x_i)_{i =1,2,...,d}\]

<p>but this only works for a <strong>countably infinite dimension</strong>. To represent an <mark>uncountably infinite dimension</mark>, consider:</p>

\[\vec{x}=(x_i)_{i \in \mathbb{R}} = (x_\alpha)_{\alpha \in \mathbb{R}}\]

<p>And if we stack the <em>components vertically</em>, then essentially <mark>$\vec{x}$ is a function</mark>.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211005185501058.png" alt="image-20211005185501058" style="zoom: 50%;" /></p>

<p>where each entry continuously maps to a value. So a $f\to \mathbb{R}$ is like a “1-D vector”.</p>

<ul>
  <li>
    <p>e.g., for a one dimensional data $x=2$, can transform this into infinite dimension (i.e. a “1-D vector”) with:</p>

\[x=2\to\phi(2) =\left(\exp(-(2-\alpha)^2)\right)_{\alpha \in \mathbb{R}}=f(\alpha)=e^{(-(2-\alpha)^2)}\]
  </li>
</ul>

<p>Therefore, the <mark>dot product becomes integral for functions</mark>. For a 1-D case, basically you get:</p>

\[\phi(x_1)\cdot \phi(x_2)= \int_{i\in \mathbb{R}}\phi(x_1)_i\phi(x_2)_idi\]

<ul>
  <li>then for input of $d=2$, we then need to integrate over a surface, so $i\in \mathbb{R}^2$. And this generalizes to $\mathbb{R}^d$</li>
</ul>

<p>Now, since we know $\phi$ basically is a Gaussian, the actual integral becomes for input of dimension $d$:</p>

\[\phi(\vec{x}_i)\cdot \phi(\vec{x}_j)= \int_{k\in \mathbb{R}^d}\phi(\vec{x}_i)_k\phi(\vec{x}_j)_kd\vec{k} = \exp(-||\vec{x}_i-\vec{x}_j||^2)\]

<p>where $\phi(\vec{x}_i)$ as a “vector” has infinite dimensions.</p>

<h3 id="using-kernel-trick-in-perceptron">Using Kernel Trick in Perceptron</h3>

<p>Now we need to somehow <strong>use the trick in our perceptron algorithm</strong>. Recall that our algorithm is:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20210927205947846.png" alt="image-20210927205947846" style="zoom: 67%;" /></p>

<ul>
  <li>if we want to apply kernel directly by replacing in $\vec{x} \to \phi(\vec{x})$, we will be stuck at learning $\vec{w}$ because it has $…+y\vec{x}$, which we cannot compute with $\vec{x} \to \phi(\vec{x})$</li>
</ul>

<p>However, we can <mark>rewrite the algorithm</mark>. Since each time we we <mark>update $\vec{w}$</mark>, we are <mark>adding a sample data</mark> point $\vec{x}_i$. This means that:</p>

\[\vec{w}^{(t)}:=\vec{w}^{(t-1)}+y_k\vec{x}_k,\text{ when $\vec{x}_k$ is wrong}\iff \vec{w}=\sum_k\alpha_ky_x\vec{x}_k\]

<p>where:</p>

<ul>
  <li>$\alpha_k$ is the number of times that we made a mistake on $\vec{x}_k$ (i.e. number of times we added $\vec{x}_k$ as a contribution)</li>
</ul>

<p>Therefore:</p>

<ul>
  <li>
    <p>The training algorithm becomes:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211005191123678.png" alt="image-20211005191123678" style="zoom: 50%;" /></p>

    <p>where basically you fitted all the $\alpha_i$.</p>
  </li>
  <li>
    <p>The testing/prediction functions becomes:</p>

\[f(\vec{x})=\text{sign}(\vec{w}\cdot \vec{x}) = \text{sign}\left(\vec{x}\cdot \sum_{k=1}^n\alpha_ky_x\vec{x}_k\right) =  \text{sign}\left(\sum_{k=1}^n\alpha_ky_x(\vec{x}_k\cdot \vec{x})\right)\]
  </li>
</ul>

<p>which in <mark>both cases are only taking dot products $\vec{x}_i\cdot \vec{x}_j$</mark>. So we can <mark>replace them with $\phi(\vec{x}_i)\cdot \phi(\vec{x}_j)$</mark>.</p>

<blockquote>
  <p><strong>Kernel For Perceptron</strong></p>

  <ul>
    <li>
      <p>During the training phase:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211005191410187.png" alt="image-20211005191410187" style="zoom: 50%;" /></p>

      <p>where  $\vec{\alpha} \in \mathbb{R}^n$, and that</p>

      <ul>
        <li>
          <p>basically we start with assuming no mistake. Then, when mistake is made on the $i$-th data point, we update $\alpha_i$.</p>
        </li>
        <li>
          <p>we essentially never compute <em>individually</em> $\phi(\vec{x}_i)$, we <mark>only compute the dot product</mark>.</p>
        </li>
      </ul>
    </li>
    <li>
      <p>During the testing/prediction phase:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211005191517894.png" alt="image-20211005191517894" style="zoom: 50%;" /></p>

      <p>so basically in the <mark>transformed space</mark>:</p>

\[\vec{w} = \sum_{i=1}^n \alpha_i y_i \phi(\vec{x}_i)\]

      <p>(though we never compute it)</p>
    </li>
  </ul>
</blockquote>

<p>So in the end, we can encapsulate $K(\vec{x}_i,\vec{x}_j)$ that performs <em>some kernelization</em> $\phi(\vec{x}_i)\cdot\phi(\vec{x}_j)$ of your choice. But there are still some <mark>rules</mark> that we need to follow for <mark>our choice of $K$:</mark></p>

<ol>
  <li>For a kernel function $K(x,z)$, there better exists $\phi$ such that $\phi(x)^T\phi(z) = K(x,z)$
    <ul>
      <li>but technically you don’t need to know what is $\phi(x)$. We just need to spit out some number.</li>
    </ul>
  </li>
  <li>if $x\cdot z \ge x\cdot y$, then $K(x,z) \ge K(x,y)$.</li>
  <li>$K(x,x) = \phi(x)^T\phi(x) \ge 0$​</li>
  <li>Kernel Matrix $K$​, where $K_{ij}=K(x^{(i)},x^{(j)})=K(x^{(j)}, x^{(i)})=K_{ji}$ is <strong>symmetric</strong></li>
  <li>Kernel Matrix $K$​​ is positive semidefinite (the <em>other direction is also true!!, but not proven here</em>)</li>
  <li>Mercer Kernel Theorem</li>
</ol>

<blockquote>
  <p><strong>Disadvantages</strong> (of being in an <mark>kernel</mark>/Hilbert space)</p>

  <ul>
    <li>The classifier needs to <strong>hold all the data points</strong> around $\vec{x} \in \mathcal{D}$</li>
    <li>The classifier, when predicting needs to <strong>iterate through</strong> all training data points.</li>
  </ul>
</blockquote>

<p>Though in reality, there will be some approximation made.</p>

<ul>
  <li>for instance, suppose we have training $x_1, x_2$, and test $x$. If $x_1\cdot x_2$ is large, then it is <em>likely</em> that $\phi(x_1) \approx\phi(x_2)$. So we don’t need to compute both $\alpha_1\phi(x_1)\cdot \phi(x)+\alpha_2\phi(x_2)\cdot \phi(x)$ but only a single time.</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>$R = \max_{\vec{x}\in S}\vert \vert \vec{x}\vert \vert$ becomes $R = \max_{\vec{x}\in S}\vert \vert \phi(\vec{x})\vert \vert$, where recall:</p>

\[||\phi(x)||^2 = \int_{-\infty}^{\infty}\phi^2(x)_idi\]

      <p>and this is <mark>finite</mark> (due to Cauchy Schwartz Inequality). Therefore, $R$ is still finite and we can still apply the <em>theorem such that</em>:</p>

\[T \le \left(\frac{R}{\gamma} \right)^2\]
    </li>
  </ul>

</blockquote>

<h1 id="support-vector-machine">Support Vector Machine</h1>

<h2 id="perceptron-and-linear-separability">Perceptron and Linear Separability</h2>

<p>Say there is a linear decision boundary which can perfectly separate the training data.</p>

<p>Now, if we use perceptron:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211007152925216.png" alt="image-20211007152925216" /></p>

<p>where <mark>any of the above line</mark> might be returned from our perceptron algorithm</p>

<ul>
  <li>e.g. due to the order of us iterating through the data point</li>
  <li>yet in reality, you might <strong>want</strong> to use the <em>middle line with largest margin $\gamma$</em>, so that your <mark>testing phase</mark> would perform better (this is the meaning of defining a $\gamma$ anyway)</li>
</ul>

<blockquote>
  <p><strong>Motivation</strong></p>

  <ul>
    <li>returns a linear classifier that is <mark>stable</mark> solution by giving a <strong>maximum margin</strong> solution (which is not considered in the perceptron algorithm).
      <ul>
        <li>stable: no matter which order of training data ($x_1, x_2,x_3$ vs $x_3,x_1,x_2$) you have, you give the same solution.</li>
      </ul>
    </li>
    <li>It is <strong>kernelizable</strong>, so gives an implicit way of yielding non-linear classification.
      <ul>
        <li>maximum margin in the kernel space $\iff$ maximum margin in the original space</li>
      </ul>
    </li>
    <li>Slight modification to the problem provides a way to deal with non-separable cases (in the original input space)</li>
  </ul>
</blockquote>

<h2 id="svm-formulation">SVM Formulation</h2>

<p>Again, let us start with a simple case and move forward.</p>

<p>Say the training data is <strong>linearly separable</strong> by some margin (but the linear separator does not necessarily passes through the origin).</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211007154109084.png" alt="image-20211007154109084" /></p>

<p>so that our decision boundary can be:</p>

\[g(\vec{x}) = \vec{w}\cdot \vec{x} - b =0 \to f(\vec{x}) = \text{sign}(\vec{w}\cdot\vec{x}-b)\]

<blockquote>
  <p><strong>Heuristics</strong></p>

  <ul>
    <li>We can try finding <mark>two parallel hyperplanes</mark> that <strong>correctly classify all the points</strong>, and <mark>maximize the distance</mark> between them!
      <ul>
        <li>then, you just need to return the <mark>average of the two parallel hyperplanes</mark>.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Consider that you get <mark>some</mark> boundary $\vec{w} \cdot \vec{x}-b=0$ ($\vec{w}$ is not necessarily the best here).</p>

<p>Then we get two planes, being some distance $c$ away such that:</p>

\[\begin{cases}
\vec{w}\cdot \vec{x} - b = +c\\
\vec{w}\cdot \vec{x} - b = -c
\end{cases}\]

<p>but we could divide both side by $c$, such that we get a <strong>simpler form (i.e. less parameter to worry about)</strong>:</p>

\[\begin{cases}
\vec{w}\cdot \vec{x} - b = +1\\
\vec{w}\cdot \vec{x} - b = -1
\end{cases}\]

<p>Graphically:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211007154706417.png" alt="image-20211007154706417" /></p>

<p>Then we claim that the distance between the two plane is:</p>

\[d=\frac{2}{||\vec{w}||}\]

<hr />

<p><em>Proof</em></p>

<p>First, we can shift the entire space such that $b=0$. Then essentially, we consider:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211007163302087.png" alt="image-20211007163302087" style="zoom: 25%;" /></p>

<p>Then obviously the red distance is:</p>

\[\vec{x}\cdot \frac{\vec{w}}{||\vec{w}||} = \frac{1}{||\vec{w}||}\]

<p>since the two planes are equidistance apart, hence the distance between the two planes is just multiplied by 2.</p>

<hr />

<p>Then, <mark>since we have two planes</mark>, the correct classification means:</p>

\[\begin{cases}
\vec{w} \cdot \vec{x}_i - b \ge +1,\quad \text{if $y_i=+1$}\\
\vec{w} \cdot \vec{x}_i - b \le -1,\quad \text{if $y_i=-1$}
\end{cases}\]

<p>So together, this can be summarized as:</p>

\[y_i(\vec{w} \cdot \vec{x}_i - b) \ge +1,\quad \forall i\]

<ul>
  <li>so basically if we have $n$ data points, this is $n$ <mark>constraint</mark> equations.</li>
</ul>

<p>Therefore, our optimization problem is:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012145324750.png" alt="image-20211012145324750" /></p>

<p>But we would want convexity in the objective function. Therefore, we consider the reciprocal and we get:</p>

<blockquote>
  <p><strong>SVM Standard (Primal) Form</strong></p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012145735972.png" alt="image-20211012145735972" /></p>

  <p>where we have converted maximization to minimization of reciprocal.</p>

  <ul>
    <li>note that $(1/2)\vert \vert w\vert \vert ^2$ is convex.</li>
  </ul>
</blockquote>

<p>Then, we are left with 3 problems:</p>

<ol>
  <li>what if the data is not linearly separable (in the raw input space)</li>
  <li>how to actually perform the minimization</li>
  <li>can we make it kernalizable</li>
</ol>

<h2 id="slacked-svm">Slacked SVM</h2>

<p>We address the first question of how to manage data points that are slightly off:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012150348253.png" alt="image-20211012150348253" /></p>

<p>which made it not linearly separable.</p>

<p>But consider some way to <em>account for the error</em>.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012150555956.png" alt="image-20211012150555956" style="zoom: 50%;" /></p>

<p>where we denote the error of those points being:</p>

<ul>
  <li>$\xi_i$ and $\xi_{i’}$ are the error which is the <mark>distance $&gt;0$ to the correct (stringent) hyperplane</mark></li>
  <li>$\xi = 0$ if the data is <strong>correctly</strong> classified</li>
</ul>

<p>So now the idea is to optimize over <mark>$\vec{w}$ AND $\xi_i$ simultaneously</mark>. Then we can just associated each training data with a slack/error $x_i \to \xi_i$  such that</p>

<p>where:</p>

<ul>
  <li>
    <p>the <strong>constraint</strong>:</p>

\[y_i(\vec{w}\cdot \vec{x}_i -b) \ge 1 - \xi_i\]

    <p>represents us <strong>slacking the constraint</strong> (due to non-separability)</p>

    <ul>
      <li>this is why $\xi_i$ is the <strong>distance</strong>. Because if we have some optimal $\vec{w},b$, then eventually $\xi_i$ will be the shortest distance to the correct classification hyperplane.</li>
    </ul>
  </li>
  <li>
    <p>the <strong>objective</strong> of</p>

\[\frac{1}{2}||w||^2 + C \sum_{i=1}^n\xi_i\]

    <p>means I want to <strong>minimize the slack, if given</strong>.</p>

    <p>However, also notice that we have attached the $C$ variable, because:</p>

    <ul>
      <li>if we want to <em>just</em> maximize the margin, then minimizing $\frac{1}{2}\vert \vert w\vert \vert ^2$ (do not overfit) would cause $\sum_{i=1}^n\xi_i$ will shoot up</li>
      <li>if we want to <em>just</em> minimize the error, then our <em>margin will become small</em>. This means that $\frac{1}{2}\vert \vert w\vert \vert ^2$ will be large.</li>
    </ul>

    <p>Therefore, $C$ is like a hyperparameter telling you which one would you emphasize on.</p>
  </li>
  <li>
    <p>The output is to <strong>simultaneously give a combination of $\vec{w}, b, \xi_i$</strong>, (if you know one variable, you know everything. But you do NOT know any on the first hand. $\xi_i$ is not the distance prior to the optimization problem).</p>

    <p>So the output would be operating on $\vec{w},b,\xi_i$, so dimension $\mathbb{R}^{d+1+n}$</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>Sometimes you will see the problem be phrased as</p>

\[(1-\lambda)\frac{1}{2}||w||^2 + \lambda \sum_{i=1}^n\xi_i,\quad \lambda \in[0,1]\]

      <p>which is <strong>equivalent</strong> to the one with $C$ defined above.</p>
    </li>
  </ul>
</blockquote>

<h2 id="finding-minimization">Finding Minimization</h2>

<p>Our goal now becomes trying to <strong>solve this problem</strong>.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019181557716.png" alt="image-20211019181557716" style="zoom:50%;" /></p>

<p>note that:</p>

<ul>
  <li>doing minimization is not the problem, but the <mark>$n$ constraint</mark> are annoying.</li>
</ul>

<p>First, let us consider <mark>visualization</mark> of minimization and functions.</p>

<p>Consider some function $f(y,z) \to \mathbb{R}$. We can visualize this as:</p>

<p><img src="https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/27178/versions/6/previews/html/gaPeaksExample_01.png" alt="peaks Minimization with Genetic Algorithm" style="zoom: 80%;" /></p>

<ul>
  <li>the input plane/plane we operate on is <strong>always y-z plane</strong>. The vertical dimension of x is an <mark>imagined axis</mark> so that we can visualize output.</li>
  <li>the idea is that for a function $f:\mathbb{R}^d \to \mathbb{R}$, we are having base plane (input being $\mathbb{R}^d$), and the <mark>output is "imagined to be" perpendicular</mark> to the base space.</li>
</ul>

<p>Therefore, the function</p>

\[f(\vec{w},b,\xi_i)=\frac{1}{2}||\vec{w}||^2 + C \sum_{i=1}^n\xi_i\]

<p>is basically attach a “vertical number” to the input space of $\mathbb{R}^{d+1+n}$.</p>

<p>And the constraint of:</p>

\[y_i(\vec{w}_i \cdot \vec{x}_i- b) \ge 1 - \xi_i,\quad \xi_i \ge 0\]

<p>is basically <strong>constraining the space of $\mathbb{R}^{d+1+n}$</strong> that output can be.</p>

<p>Graphically, it looks like:</p>

<p><img src="https://ecdn.teacherspayteachers.com/thumbitem/Constrained-Optimization-3537361-1513084511/original-3537361-1.jpg" alt="Constrained Optimization by BUSINESS TEACHING MATERIALS STORE | TpT" style="zoom: 80%;" /></p>

<p>where:</p>

<ul>
  <li>the curves a like constraints, where <strong>output</strong> in y-z plane should <strong>only lie within that region.</strong></li>
</ul>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>SVM is basically a problem a <strong>constraint optimization problem</strong>.
      <ul>
        <li>you cannot do simple gradient descent, because the constraint might not longer be satisfied after you moved.</li>
      </ul>
    </li>
    <li>the visualization techniques above will help you understand how to solve for the constraint optimization.</li>
  </ul>
</blockquote>

<h3 id="constrained-optimization">Constrained Optimization</h3>

<p>Consider a constraint optimization of:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012155030945.png" alt="image-20211012155030945" /></p>

<p>where basically we are only looking at:</p>

<ul>
  <li>a finite number of a constraints</li>
  <li>equality constraint can also be made into an inequality constraint. E.g. $x^2=5$ could be transformed to $x^2-5 \ge 0$ and $x^2-5 \le 0$</li>
  <li>and we assume that <mark>the problem is feasible</mark>.</li>
</ul>

<p>Some common ways of doing it would be</p>

<p><strong>Projection Methods</strong></p>

<ol>
  <li>start with a feasible solution $x_0$ (not necessarily minimized)</li>
  <li>find $x_1$ that has slightly lower objective value</li>
  <li>if $x_1$ violates the constraints, <mark>project back</mark> to the constraints.
    <ul>
      <li>where project back to the constraints is the hard part</li>
    </ul>
  </li>
  <li>iterate, stop when you cannot find a lower value in step 2</li>
</ol>

<p><strong>Penalty Methods</strong></p>

<ol>
  <li>
    <p>Use a <mark>penalty function</mark> to incorporate the <mark>constraints into the objective</mark></p>

    <ul>
      <li>
        <p>so that you won’t even step into the forbidden region even if you are only looking at the objective function. For instance, the <em>penalty could be infinite</em> once you entered the forbidden region. So you would avoid that.</p>
      </li>
      <li>
        <p>hard to find a working penalty function</p>
      </li>
    </ul>
  </li>
</ol>

<h3 id="lagrange-penalty-method">Lagrange (Penalty) Method</h3>

<p>Consider the augmented function</p>

\[L(\vec{x}, \vec{\lambda}) := f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})\]

<p>and recall that :</p>

<ul>
  <li>our aim was to minimize $f(\vec{x})$ <mark>such that $g_i(\vec{x}) \le 0$ is satisfied</mark></li>
  <li>$\vec{x}$ is the original variable, called primal variable as well</li>
  <li>$\lambda_i$ will be some new variable, called <strong>Lagrange/Dual Variables</strong>.</li>
</ul>

<p><strong>Observation</strong></p>

<ol>
  <li>
    <p>For <mark>any feasible $x$</mark> and all $\lambda_i \ge 0$, then since $g_i(\vec{x}) \le 0$, then obviously:</p>

\[L(\vec{x},\vec{\lambda}) \le f(\vec{x}) \to \max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) \le f(\vec{x})\]
  </li>
  <li>
    <p>So the optimal value to the constraint problem is:</p>

\[p^* := \min_{\vec{x}}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda})\]

    <p>and note that now we have:</p>

    <ul>
      <li>$\max_{\lambda_i \ge 0}$ which is much easier than satisfying entire functions $g_{i}(\vec{x})\le 0$.</li>
      <li>once  $\max_{\lambda_i \ge 0}$ this is done, $\min_{\vec{x}}$ has <mark>no constraint on $\vec{x}$</mark> anymore</li>
    </ul>

    <p>we now show that this $p^*$ actual is the <mark>same as our original goal</mark>.</p>
  </li>
</ol>

<blockquote>
  <p><strong>Proof</strong></p>

  <ol>
    <li>
      <p>Consider that we landed a $p^*$ that has a $\vec{x}$ being <strong>infeasible</strong>. Then:</p>

      <ul>
        <li>it means at least one of the $g_i(\vec{x})&gt; 0$ (since $\vec{x}$ is infeasible)</li>
        <li>Therefore, $\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda})$ would end up with <mark>one</mark> $\lambda_i \to \infty$ and $\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) \to \infty$</li>
        <li>therefore, such a $p^*$ cannot be computed, which is a contradiction.</li>
      </ul>
    </li>
    <li>
      <p>Consider that we landed a $p^*$ that has a $\vec{x}$ being <strong>feasible</strong>. Then:</p>

      <ul>
        <li>
          <p>this means all $g_i(\vec{x})\le 0$.</p>
        </li>
        <li>
          <p>Therefore, $\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda})$ would end up <mark>with all</mark> $\lambda_i = 0$, or $g_i(x)=0$ already. Then, this becomes the same as:</p>

\[\min_\vec{x}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) \to \min_{\vec{x}}f(\vec{x})\]

          <p>which is the <strong>same</strong> as the original task.</p>
        </li>
      </ul>
    </li>
  </ol>

  <p>Therefore, <mark>this $p^*$ problem is the same as what we wanted to compute</mark>, namely:</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211012155030945.png" alt="image-20211012155030945" /></p>
</blockquote>

<p>The problem is that the $\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda})$ is difficult to compute on the first hand.</p>

<p>Therefore, we introduce some other thought. Consider</p>

\[\min_{\vec{x}}L(\vec{x},\vec{\lambda}) = \min_{\vec{x}} f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})\]

<p>Then, we observe that:</p>

\[p^* =\min_{\vec{x}}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) \ge \min_{\vec{x}}L(\vec{x},\vec{\lambda})\]

<p>i.e. minimum of a maxi zed function is $\ge$ just minimum of a function. Then, we can define the problem:</p>

\[\min_{\vec{x}}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) \ge \max_{\lambda_{i}\ge 0}\min_{\vec{x}}L(\vec{x},\vec{\lambda}) := d^*\]

<p>so we get that the <strong>dual problem of $d^*$</strong> defined as:</p>

\[d:=\max_{\lambda_{i}\ge 0}\min_{\vec{x}}L(\vec{x},\vec{\lambda}) \le p^*\]

<p>Technically, $d^<em>$ is dual problem, and <mark>in some cases</mark>, $d^</em> = p^*$. Then this is good because:</p>

<ul>
  <li>$\min_{\vec{x}}L(\vec{x},\vec{\lambda})$ is unconstraint optimization, and we can compute derivatives</li>
  <li>in fact, $h(\lambda) \equiv \min_{\vec{x}} L(\vec{x},\vec{\lambda})$. Then we can show that <mark>every dual problem is concave</mark>. This justifies the final maximization procedure.</li>
</ul>

<hr />

<p><em>Proof</em></p>

<p>Since $L(\vec{x},\vec{\lambda})=f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})$ is a <strong>linear function in $\lambda$</strong> for a particular $\vec{x}$. So graphically:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021163423941.png" alt="image-20211021163423941" style="zoom: 50%;" /></p>

<p>so we can then draw many $L$ functions for different $\vec{x}$, which will be linear in $\lambda$. Then:</p>

<ul>
  <li>$h(\lambda) = \min_{\vec{x}} L(\vec{x},\vec{\lambda})$ basically says for each $\lambda$, pick the smallest $\vec{x}_i$. This basically results in the red highlighted line, which is concave.</li>
</ul>

<p>The general observation is that $h(\lambda)$ is a pointwise minimum of linear functions $L$ in $\lambda$.</p>

<ul>
  <li><strong>pointwise minimum of linear functions is concave</strong>, (pointwise maximum of linear functions is convex)</li>
  <li>so the dual of a problem is concave, and a dual of a dual will be convex.</li>
</ul>

<hr />

<p>Therefore, now we have defined two (different yet related) problems</p>

<blockquote>
  <p><strong>Primal Problem</strong></p>

\[p^* = \min_\vec{x}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda})\]

  <p>which is the exact problem we needed to solve, but difficult to solve.</p>
</blockquote>

<blockquote>
  <p><strong>Dual Problem</strong></p>

\[d^* = \max_{\lambda_i \ge 0}\min_\vec{x} L(\vec{x},\vec{\lambda})\]

  <p>which is different from what we need to solve, but <mark>much easier to solve.</mark> And we know that $d^* \le p^*$</p>
</blockquote>

<p>However, the dual problem is useful because under <em>certain conditions</em>, $p^<em>=d^</em>$.</p>

<blockquote>
  <p><strong>Duality Gap</strong></p>

\[p^* - d^*:=\text{Duality Gap}\]

  <p>and our aim is to achieve equality/duality gap is zero in some cases, which is useful since the <strong>dual problem is easy to solve</strong>.</p>
</blockquote>

<p>Then, first we need to consider <mark>when is the duality gap zero</mark>. To do that, we need to first understand convexity.</p>

<h4 id="convexity">Convexity</h4>

<p>The following concepts would be helpful before we go into show <mark>when $p^* = d^*$</mark>.</p>

<hr />

<p><strong>Convexity of Sets</strong></p>

<p>A set $S \subset \mathbb{R}^d$ is called convex $\iff$ for any two points $x,x’ \in S$ and any $\beta \in [0,1]$, the following holds:</p>

\[\beta \vec{x} + (1-\beta)\vec{x}' \in S\]

<p>which means any point on the line segment $\beta \vec{x} + (1-\beta)\vec{x}’$ is also in the set.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019165410787.png" alt="image-20211019165410787" style="zoom:80%;" /></p>

<p>where:</p>

<ul>
  <li>in other words, pick any two point in the set, draw a line, and that line needs to be inside the set.</li>
  <li>notice that the convex set does <em>not need to be closed</em>. For example $S:=x\in \mathbb{R},x\in(0,1)$ is an open convex set.</li>
</ul>

<hr />

<p><strong>Convex of Functions</strong></p>

<p>A real values function $f:\mathbb{R}^d\to \mathbb{R}$ is called a convex function</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211014153325612.png" alt="image-20211014153325612" /></p>

<p>where:</p>

<ul>
  <li>
    <p>basically for any two values $f(\vec{x}_1),f(\vec{x}_2)$, in the graph, draw a line, and all values $f(\vec{z}),\vec{z}\in[\vec{x},\vec{x_2}]$ is below the line.</p>
  </li>
  <li>
    <p>formally:</p>

\[f(\beta \vec{x}+(1-\beta)\vec{x}) \le \beta f(\vec{x}) + (1-\beta)f(\vec{x})\]

    <p>for <em>any point $\vec{x} \in \mathbb{R}^d$</em>, the above needs to hold. On the graph:</p>

    <ul>
      <li>$\beta \vec{x}+(1-\beta)\vec{x}$ is the input $\vec{z} \in [\vec{x},\vec{x}’]$</li>
      <li>$f(\vec{x}) + (1-\beta)f(\vec{x})$ is the line connecting the two dots.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>By the definitions of convex (and concave similarly), a linear function would be <strong>both convex and concave</strong>.</li>
    <li>Gaussian function is neither convex nor concave.</li>
  </ul>
</blockquote>

<h4 id="convex-optimization">Convex Optimization</h4>

<blockquote>
  <p><strong>Convex Optimization</strong></p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211014155124853.png" alt="image-20211014155124853" /></p>

  <p>this problem is a convex optimization <mark>IFF both holds</mark>:</p>

  <ul>
    <li>the <strong>feasible</strong> region of output (due to the constraint) is a <strong>convex set</strong></li>
    <li>the <strong>objective</strong> function $f(\vec{x})$ is a <strong>convex function</strong></li>
  </ul>

  <p>Then a optimization solution for convex problems can be computed efficiently! (Which is what we are doing for SVM).</p>
</blockquote>

<p>Note that:</p>

<ul>
  <li>
    <p>if the <em>function</em> $g_i(\vec{x})$ is convex, it does <mark>not necessarily mean</mark> out feasible region is also convex (e.g. draw two quadratic, and the region outside is not a convex set)</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019165830633.png" alt="image-20211019165830633" style="zoom:50%;" /></p>
  </li>
</ul>

<hr />

<p><strong>Useful Properties of Convex Optimization</strong></p>

<ol>
  <li>
    <p>Every local optima is a <strong>global optima</strong></p>

    <ul>
      <li>but note that a local optima <mark>might not exist</mark>. For instance $y=e^x,y=x$ both has no local/global optima</li>
      <li>also, there could be <mark>multiple local/global optima</mark></li>
    </ul>

    <blockquote>
      <p><em>Example Convex Problems</em></p>

      <ul>
        <li>Linear programs: objective function is linear (affine), and constraints are also linear (affine)
          <ul>
            <li>so that the feasible region is a convex set (because the feasible region is always a polygon = convex set)</li>
          </ul>
        </li>
        <li>Quadratic program: objective function is quadratic, and constraints are linear (affine)
          <ul>
            <li>if constraints are quadratic, then the feasible region might not be a convex set.</li>
          </ul>
        </li>
        <li>Conic Program: where constraints are a conic shaped region</li>
        <li>Other common solvers include: <code class="language-plaintext highlighter-rouge">CVX</code>, <code class="language-plaintext highlighter-rouge">SeDuMi</code>, <code class="language-plaintext highlighter-rouge">C-SALSA</code>,</li>
      </ul>
    </blockquote>
  </li>
  <li>
    <p>We can use a simple <strong>‘descend-type’ algorithm</strong> for finding the minima</p>
  </li>
</ol>

<h4 id="weakstrong-duality-theorem">Weak/Strong Duality Theorem</h4>

<p>The weak version is proven before which <em>always holds</em>:</p>

<blockquote>
  <p><strong>Weak Lagrange Duality</strong></p>

\[d^* \le p^*\]

  <p>which we proved before already.</p>
</blockquote>

<p>However, the more useful theorem is:</p>

<blockquote>
  <p><strong>Strong Lagrange Duality</strong></p>

  <p>If we know that, for a feasible point $x^*$</p>

  <ul>
    <li>$f$ is <strong>convex</strong></li>
    <li>$g_i(x^<em>) &lt; 0$; or $g_i(x^</em>)\le 0$ when $g$ is affine</li>
  </ul>

  <p>This is also called the Slater’s condition. Then:</p>

\[d^* = p^*\]

</blockquote>

<p>Note:</p>

<ul>
  <li>Slater’s condition is a <mark>sufficient</mark> condition (fulfillment of requirement “guarantees” that the outcome), but not a <em>necessary condition</em> (i.e. there might be other causes such that $d^<em>=p^</em>$)</li>
  <li>(the Karush-Kuhn-Tucker (KKT) condition would be the necessary condition)</li>
</ul>

<h3 id="optimizing-svm">Optimizing SVM</h3>

<p>So we recall that the objective is</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019151647950.png" alt="image-20211019151647950" /></p>

<p>observe that:</p>

<ol>
  <li>the objective function is a <strong>convex function</strong> <mark>in the space of $\vec{w},b$</mark> (i.e. a bowl shape extended in some direction)
    <ul>
      <li>an easy way to check is to see the <em>second derivative is <strong>positive definite</strong></em></li>
    </ul>
  </li>
  <li>the constraint functions are <strong>affine</strong> <mark>in $\vec{w},b$</mark>, which gives a polytope feasible region -&gt; convex set.</li>
</ol>

<p>Therefore, this means:</p>

<ul>
  <li>
    <p>we can solve this primal problem <mark>directly</mark> using a quadratic program (done)</p>
  </li>
  <li>
    <p>this satisfies the Slater’s condition, then $d^<em>=p^</em>$, so we might be able to <mark>optimize if solving for $d^*$</mark></p>
  </li>
</ul>

<h4 id="solving-svm-dual-problem">Solving SVM Dual Problem</h4>

<p>If we solve the dual problem, you will see it has some <strong>additional properties</strong> for our SVM, In this setting, let us define, by naming $\vec{x}\to \vec{w},b$ and $\lambda \to \alpha$, and substituting in our $g$ constraints:</p>

\[L(\vec{x}, \vec{\lambda}) \to L(\vec{w},b,\vec{\alpha}) = \frac{1}{2}||\vec{w}||^2 + \sum_{i=1}^n \alpha_i(1-y_i(\vec{w}\cdot\vec{x}-b))\]

<p>We know that the primal problem which we need to solve is the same as the dual problem, so:</p>

\[p^* = \min_{\vec{w},b}\max_{\alpha_i \ge 0} L(\vec{w},b,\vec{\alpha})=d^*=\max_{\alpha_i \ge 0}\min_{\vec{w},b} L(\vec{w},b,\vec{\alpha})\]

<p>since we are interested in the dual problem, we can actually solve it by computing the derivatives.</p>

<ol>
  <li>
    <p>First, we can compute $\min_{\vec{w},b} L(\vec{w},b,\vec{\alpha})$:</p>

\[\begin{align*}
\frac{\partial L}{\partial \vec{w}} &amp;= \vec{w} - \sum_{i=1}^n \alpha_i y_i \vec{x}_i = 0\\
\frac{\partial L}{\partial b} &amp;=\sum_{i=1}^n \alpha_i y_i  = 0
\end{align*}\]

    <p>so we get:</p>

\[\begin{align*}
\vec{w} &amp;= \sum_{i=1}^n \alpha_i y_i \vec{x}_i\\
0 &amp;= \sum_{i=1}^n \alpha_i y_i 
\end{align*}\]

    <p>(some trick for taking vector derivatives is that, if you are taking a derivative WRT a $n\times m$ matrix, the result better be also $n\times m$, i.e. the same dimension)</p>

    <blockquote>
      <p><em>Support Vectors</em></p>

      <p>Notice that we see:
\(\vec{w}=\sum_{i=1}^n \alpha_i y_i \vec{x}_i\)
this means that:</p>

      <ul>
        <li>
          <p>looks similar to the perceptron algorithm, though $\alpha_i$ has a different meaning here (and is different)</p>
        </li>
        <li>
          <p>whatever your result is from SVM, your <mark>boundary MUST</mark> be a linear combination of the data points</p>
        </li>
        <li>
          <p>since $\alpha_i \ge 0$, the <mark>only participating vectors</mark> are $x_i \to \alpha_i &gt; 0$. These vectors are called the <strong>support vectors</strong>. Now, notice that the primal problem (which is the same as dual problem now) we setup was:
\(\min_\vec{x}\max_{\lambda_i \ge 0} L(\vec{x},\vec{\lambda}) = \max_{\lambda_i \ge 0}\min_{\vec{x}} f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})\)
where $g_i(x)=1-y_i(\vec{w}\cdot\vec{x}-b)\le 0$, and $\lambda = \alpha$ in our setup. Since we needed to maximize it, then obviously:</p>

          <ul>
            <li>if $g_i(x) = 0$, then $\lambda_i$ can take any value. This means $1=y_i(\vec{w}\cdot\vec{x}-b)$. We are <mark>ON THE BOUNDARY</mark>.</li>
            <li>if $g_i(x) &lt; 0$, then $\lambda_i=0$ since we need to maximize it in the end. This means $1 &lt; y_i(\vec{w}\cdot\vec{x}-b)$. We are away from the boundary.</li>
          </ul>

          <p>Hence, the <strong>support vectors</strong> with $\alpha_i &gt; 0$ must be the <strong>on either of the two boundaries.</strong> Graphically:</p>

          <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019172757429.png" alt="image-20211019172757429" style="zoom:67%;" /></p>

          <p>only the green boxed points will appear in $\vec{w}$.</p>

          <ul>
            <li>
              <p>basically, your boundary will extend in each dimension until touching one of those points</p>
            </li>
            <li>
              <p>As dimension increases, then you will have more support vectors participating, e.g. if you have $\mathbb{R}^d$ as data input, you will have at least $d$ support vectors.</p>
            </li>
          </ul>
        </li>
      </ul>
    </blockquote>
  </li>
  <li>
    <p>Now, we now the formula for $\vec{w}$ and the constraint of $0 = \sum_{i=1}^n \alpha_i y_i$, substituting into our Lagrangian we get:</p>

\[\min_{\vec{w},b} L(\vec{w},b,\vec{\alpha}) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\lang \vec{x}_i, \vec{x}_j\rang\]

    <p>then your entire dual problem becomes:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019173344639.png" alt="image-20211019173344639" /></p>

    <p>and notice that we can have a kernel injected into the highlighted part.</p>
  </li>
  <li>
    <p>Therefore, we finish and we get our dual form of the SVM:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019154639227.png" alt="image-20211019154639227" /></p>

    <p>notice that:</p>

    <ul>
      <li>this objective function is <strong>concave</strong>, so a maxima exists. (because the $0.5\sum …$ is convex, but there is a negative sign, which made it concave)
        <ul>
          <li>in fact, <mark>every dual problem</mark> is <mark>concave</mark>.</li>
        </ul>
      </li>
      <li>
        <p>data points are in the form of $\lang x_i , x_j\rang$, which becomes <mark>kernelizable</mark>. So we can actually substitute in a kernel function and optimize over it.</p>
      </li>
      <li>
        <p>the constraint now is comes from $\partial L / \partial b = 0$</p>
      </li>
      <li>
        <p>the optimization is now over $\alpha_i$, so you take gradient over $\alpha_i$ (do a gradient ascent)</p>
      </li>
      <li>
        <p>then the output optimizes over the space of $\vec{\alpha}$</p>
      </li>
      <li>once we get $\vec{\alpha}$, we then solve:</li>
    </ul>

\[\vec{w}=\sum_{i=1}^n \alpha_i y_i \vec{x}_i\]

    <p>since we also know that support vectors with $\alpha_i &gt; 0$ touches the boundary, we can compute $b$ from</p>

\[y_i(\vec{w} \cdot \vec{x}_i - b) = +1,\quad \text{$x_i$ on the boundary}\]

    <p>which recall basically was out setup:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019173834736.png" alt="image-20211019173834736" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>To <mark>solve</mark> this, notice that the objective is no longer convex (it will be convex only in the feasible region). The constraint is essentially $\vec{\alpha}\cdot \vec{y}=0$. Therefore, we can solve this by:</p>

    <ol>
      <li>use gradient descent for our objective function, computing gradient of $\alpha_i$</li>
      <li>if outside of the constraint, <strong>project</strong> back (which is easy to do since $\vec{\alpha}\cdot \vec{y}=0$ just defined a plane of valid $\vec{\alpha}$, so you just project back to that plane)</li>
      <li>repeat until you don’t move in step 1</li>
    </ol>
  </li>
</ol>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>
      <p>The dual form of the SVM problem is (equivalent to the primal form in this case):</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019154639227.png" alt="image-20211019154639227" /></p>

      <p>then needs to <strong>optimize in $\vec{\alpha}$ space</strong> (which is the <strong>number of data points you have</strong>). So this is fast if the dimensionality for data point is high</p>

      <ul>
        <li>with an additional advantage that you can use a kernel</li>
        <li>and gives you understanding where the support vectors come from</li>
        <li>to solve this, basically it is a <em>projection optimization</em>, where we need to make sure basically $\vec{\alpha}\cdot \vec{y}=0$ is satisfied.</li>
      </ul>
    </li>
    <li>
      <p>The primal form of the SVM problem is:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211019161334591.png" alt="image-20211019161334591" /></p>

      <p>this operates over $\vec{w},b$ which is in the dimension of your input features.</p>

      <ul>
        <li>notice that this is also solvable directly a Quadratic program since objective is convex quadratic and constraint is linear (affine).</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h1 id="regression">Regression</h1>

<p>The idea is to learn some more <strong>sophisticated output space</strong>, other than a discrete set of labels (which we have covered so far).</p>

<p>For example:</p>

<ul>
  <li>
    <p>$\text{PM}_{2.5}$ (pollutant) particulate matter exposure estimate, which outputs some real number</p>
  </li>
  <li>
    <p>pose estimation in CV:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Input</th>
          <th style="text-align: center">Output</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021150706071.png" alt="image-20211021150706071" /></td>
          <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021150716981.png" alt="image-20211021150716981" /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>Regression would be a tool to deal with output space being $\mathbb{R}$:</p>

\[f:X \to \mathbb{R}\]

<hr />

<p><em>For Example</em>: Next eruption time of old faithful geyser.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021151250295.png" alt="image-20211021151250295" /></p>

<p>In the end we expect something like</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input (current eruption time)</th>
      <th style="text-align: center">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021151319246.png" alt="image-20211021151319246" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021151416996.png" alt="image-20211021151416996" /></td>
    </tr>
  </tbody>
</table>

<p>where the vertical axis is the next eruption time, horizontal axis being the current eruption time</p>

<ul>
  <li>notice that here we are <mark>combining the output space with the input space</mark>. (In classification, it was color coded.)</li>
</ul>

<p>Let us for now assume that input space is one dimensional, $X \in \mathbb{R}$. We ant to estimate $\hat{y}$, and compare that against the actual observation $y$. Our <mark>general aim is to minimize the "error"/loss $\hat{y}-y$</mark>.</p>

<ul>
  <li>in the discrete case, our “error” is defined as $\hat{y}\neq y$.</li>
</ul>

<p>So here we can define <em>several possible loss</em></p>

\[L(\hat{y},y):=\begin{cases}
|\hat{y}-y|, &amp; \text{Absolute Error}\\
(\hat{y}-y)^2, &amp; \text{Squared Error}\\
...
\end{cases}\]

<p>Next, we need to <mark>assume a form of our predictor</mark> $f$, in this case, a linear predictor:</p>

\[\hat{y} =\hat{f}(\vec{x}) = \vec{w} \cdot \vec{x} + w_0\]

<p>being our predictor. Then, we just need to minimize loss and obtain the $\vec{w},w_0$ parameters.</p>

\[\min_{f\in F}\mathbb{E}_{x,y}[L(f(x),y)] = \min_{\vec{w},w_0} \mathbb{E}_{x,y}[L(f(x),y)]\]

<p>so basically we want the <mark>average loss</mark> to be <mark>minimized</mark> over all the data points $(\vec{x},y)$</p>

<h2 id="parametric-vs-non-parametric-regression">Parametric vs Non-Parametric Regression</h2>

<p>Before going into how do to regression by minimizing loss, we talk about some common types of regressions used first.</p>

<blockquote>
  <p><strong>Parametric Regression</strong></p>

  <p>If we assumed a <em>particular functional form</em> of the regressor $\hat{f}(x)$</p>

  <ul>
    <li>e.g. it is a polynomial function, then you are doing a parametric regression</li>
    <li>e.g. a specific neural network also is parametric. Though it can approximate any function, it still has some structural form.</li>
  </ul>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021153049493.png" alt="image-20211021153049493" /></p>

  <p>The goal here is to <strong>learn parameters</strong> which yield the minimum loss.</p>
</blockquote>

<p>The <strong>disadvantage</strong> of this would be:</p>

<ul>
  <li>you might easily underfit</li>
</ul>

<blockquote>
  <p><strong>Non-parametric Regression</strong></p>

  <p>If we didn’t assume a <em>particular functional form</em> of regressor $\hat{f}(x)$.</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021153110239.png" alt="image-20211021153110239" /></p>

  <p>The goal here is to <strong>learn the predictor directly</strong> from the input data.</p>
</blockquote>

<p>The <strong>disadvantage</strong> of this would be:</p>

<ul>
  <li>you might easily overfit</li>
  <li>at test time, you need to keep your training data around</li>
</ul>

<h2 id="linear-regression">Linear Regression</h2>

<p>Here we discuss a <strong>linear predictor $\hat{f}$</strong>:</p>

\[\hat{f}(\vec{x}):= \vec{w}\cdot \vec{x}\]

<p>where $w_0$ is absorbed via lifting, and find some optimal $\vec{w}$.</p>

<p><strong>Ideally</strong>, we want to minimize this over the entire population</p>

\[\min_{\vec{w}} \mathbb{E}_{x,y}[L(f(\vec{x}),y)]\]

<p><strong>Practically</strong>, we <em>only have training data</em>, so we do:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^nL(\vec{w}\cdot \vec{x}_i,y_i) = \arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2\]

<p>where here, we basically used loss function $L$ to be an <strong>ordinary least square</strong>.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>This squared quantity means that I am penalizing <em>more</em> the further you are away. This might or might not be suitable depending on the application.</li>
    <li>In other words, you are preferring <mark>many small mistakes</mark> than few large mistakes.</li>
  </ul>
</blockquote>

<p>Graphically, we are basically making the <mark>loss</mark> to be the <mark>vertical distance</mark>:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021155124932.png" alt="image-20211021155124932" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the plane is defined by $\vec{w} \cdot \vec{x} = 0$, and the red points are the labels.</p>
  </li>
  <li>label $y$ is perpendicular to the input space, but attached to it as we are looking at both together</li>
  <li>we are minimizing the vertical distance (quantity $(\vec{w}\cdot \vec{x}_i - y_i)$) squared. Because our predicted output is technically $(\vec{x}_i, \vec{w}\cdot \vec{x}_i)$.
    <ul>
      <li>we are <mark>not minimizing</mark> the <mark>distance to the orthogonal projection</mark>. if you are minimizing the orthogonal distance, then its like PCA.</li>
    </ul>
  </li>
</ul>

<h3 id="solving-linear-regression">Solving Linear Regression</h3>

<p>Basically we want to sovle the problem:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^nL(\vec{w}\cdot \vec{x}_i,y_i) = \arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2\]

<p>we could either solve it using derivative, or, a simpler approach using <strong>linear algebra presented</strong> below.</p>

<p>Consider using linear algrebra:</p>

\[\begin{align*}
\arg \min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n (\vec{w}\cdot \vec{x}-y_i)^2
&amp;= \arg\min_{\vec{w}} \left\| 
\begin{bmatrix}
-x_1-\\
-x_2-\\
\vdots\\
-x_n-
\end{bmatrix}\begin{bmatrix}
w_1\\
w_2\\
\vdots\\
w_n
\end{bmatrix}-\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix}
\right\|^2\\
&amp;= \arg\min_{\vec{w}} ||X\vec{w}-\vec{y}||_2^2
\end{align*}\]

<p>where</p>

<ul>
  <li>note we choose to represent data vectors in row, because this is the convention for statistics. Mathematically doing it in a column vector is “nicer”.</li>
</ul>

<p>Now we take the <mark>derivative</mark> w.r.t. $\vec{w}$ to minimize it:</p>

\[\frac{d}{d\vec{w}}||X\vec{w}-\vec{y}||^2 = 2X^T(X\vec{w}-\vec{y}) = 0\]

<p>which basically uses chain rule and matches the dimension.</p>

<ul>
  <li>
    <p>(Alternatively, you can brute force it by considering $A \equiv X\vec{w}-\vec{y}$) then basically you are doing:</p>

\[\begin{align*}
\frac{d}{d\vec{w}} A^TA
&amp;= \frac{d}{d\vec{w}} (X\vec{w}-\vec{y})^T(X\vec{w}-y)\\
&amp;= \frac{d}{d\vec{w}}\text{expand it}\\
&amp;= 2X^T(X\vec{w}-\vec{y})
\end{align*}\]
  </li>
</ul>

<p>Therefore, the solution for <strong>ordinary least square $\vec{w}_{\text{ols}}$</strong> is analytically:</p>

\[\begin{align*}
X^TX\vec{w}
&amp;= X^T \vec{y}\\
\vec{w}_{\text{ols}} 
&amp;= (X^TX)^{\dagger}X^T\vec{y}
\end{align*}\]

<p>note that</p>

<ul>
  <li>we are taking the <mark>pseudo inverse</mark> of $X^TX$, because $X^TX$ might <mark>not be invertible</mark> (if invertible, the solution is <strong>unique and stable</strong>).</li>
</ul>

<p>The advantage of this is that you no longer needs to do some <strong>iterative computation</strong> of gradient descent on the data, we can compute in one shot the solution.</p>

<blockquote>
  <p><em>Reminder: Pseudo Inverse</em></p>

  <p>Some properties of pseudo inverse $X^\dagger$:</p>

  <ol>
    <li>if the original matrix $X$ is <strong>full rank</strong>, then $X^\dagger = X^{-1}$</li>
  </ol>

  <p>Computing Moore-Penrose Pseudo Inverse $M=X^TX$:</p>

  <ol>
    <li>
      <p>Since this is a square symmetric matrix, then we know</p>

      <ul>
        <li>$M=V\Lambda V^T$</li>
        <li>the eigenvectors can be made orthonormal such that $VV^T = I$, i.e. $V^T = V^{-1}$</li>
        <li>since $M=X^TX$, then all eigenvalues are non-negative</li>
      </ul>
    </li>
    <li>
      <p>Then, consider an <em>attempt to compute inverse</em></p>

\[\begin{align*}
M^{-1}
&amp;= (V\Lambda V^T)^{-1}\\
&amp;= (V^T)^{-1} \Lambda^{-1}V^{-1}\\
&amp;= V \Lambda^{-1} V^T
\end{align*}\]

      <p>where the last equality used the fact that $V^T = V^{-1}$ for orthonormal matrix. However, some of the eigenvalues may be $0$. Therefore:</p>

\[\Lambda^\dagger = \begin{bmatrix}
1/\lambda_1 &amp; 0 &amp; \dots \\
0 &amp; \ddots &amp; \dots\\
0 &amp; \dots &amp; 1 / \lambda_k
\end{bmatrix},\quad \text{for }\lambda_k &gt; 0\]

      <p>in other words, you <mark>drop all the $\lambda_i,v_i$ if $\lambda_i=0$</mark>.</p>
    </li>
    <li>
      <p>Therefore, another way is to write as:</p>

\[M^\dagger = \sum \frac{1}{\lambda_i} v_i v_i^T,\quad \lambda_k &gt; 0\]
    </li>
  </ol>

</blockquote>

<h4 id="geometric-view-of-linear-regression">Geometric View of Linear Regression</h4>

<p>Now there are two geometric ways to understand the result:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2 \to \vec{w}_{\text{ols}} 
= (X^TX)^{\dagger}X^T\vec{y}\]

<hr />

<p><strong>Row Space Interpretation</strong></p>

<p>Basically covered before, we are considering:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2\]

<p>which is in the row space $\mathbb{R}^d$ of our data matrix. Then we are simply computing the vertical distance defined by some $\vec{w}$:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211021155124932.png" alt="image-20211021155124932" /></p>

<hr />

<p><strong>Column Space Interpretation</strong></p>

<p>Now, if we consider the <strong>column space</strong> of your data, basically those highlighted in orange:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211029131500291.png" alt="image-20211029131500291" style="zoom:50%;" /></p>

<p>Let us denote those vectors as $\ddot{x}_1,…,\ddot{x}_d \in \mathbb{R}^n$. Each vector here represents the “<mark>realization of that feature across your $n$ data points</mark>”. Then, we can rewrite the same problem as:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2 = \arg\min_{\vec{w}}\frac{1}{n}\left\|\vec{y} - \sum_{i=1}^d w_i\ddot{x}_i\right\|^2\]

<ul>
  <li>
    <p>this is trivial if we recall that:</p>

\[\arg\min_{\vec{w}} \frac{1}{n} \sum_{i=1}^n(\vec{w}\cdot \vec{x}_i - y_i)^2 = \arg\min_{\vec{w}} ||X\vec{w}-\vec{y}||_2^2\]

    <p>so essentially $X\vec{w}$ is linear combination of columns of $X$.</p>
  </li>
  <li>
    <p>This task now becomes: how to <mark>combine each column together</mark>, such that the <mark>result is closest to the label vector $\vec{y}$</mark></p>

    <ul>
      <li>so basically how <mark>important</mark> is <strong>each feature column in your training data</strong>. In other words, this is telling you the feature importance. e.g. if $w_i=0$, that means feature $i$ is <em>useless</em> for my prediction.</li>
    </ul>
  </li>
</ul>

<p>Now, let the solution of the problem be $\hat{y} = X\vec{w}_{ols}$. Then, notice that:</p>

\[\hat{y} = X\vec{w}_{ols} = \sum_{i=1}^d \vec{w}_{\mathrm{ols},i}\ddot{x}\]

<p>this means that:</p>

<ul>
  <li>
    <p>$\hat{y}$ is the <mark>orthogonal projection of $\vec{y}$</mark> into $\text{span}{\ddot{x}_1 , \ddot{x}_2,…\ddot{x}_d}$. This is because that we know:</p>

\[w_{\mathrm{ols}}=\arg\min_{\vec{w}}\frac{1}{n}\left\|\vec{y} - \sum_{i=1}^d w_i\ddot{x}_i\right\|^2\]

    <p>and we know that $\hat{y}$ lives in the $\text{span}{\ddot{x}_1 , \ddot{x}_2,…\ddot{x}_d}$, so it must be that the minimum is by doing the <em>orthogonal projection</em>.</p>
  </li>
</ul>

<p>Graphically, this is what is happening to $\vec{y}$ and $\hat{y}$:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211029132919121.png" alt="image-20211029132919121" style="zoom:50%;" /></p>

<p>where basically:</p>

<ul>
  <li>
    <p>$\hat{y}$ must be in the space of $\text{span}{\ddot{x}_1 , \ddot{x}_2,…\ddot{x}_d}$</p>
  </li>
  <li>
    <p>$\vec{y}$ may or <mark>may not</mark> be in the $\text{span}{\ddot{x}_1 , \ddot{x}_2,…\ddot{x}_d}$</p>
  </li>
  <li>
    <p>the residual is basically:</p>

\[\frac{1}{n}\left\|\vec{y} - \sum_{i=1}^d w_i\ddot{x}_i\right\|^2 := \text{residual}\]

    <p>which will only be $0$ if $\vec{y}$ is in the space of $\hat{y}$.</p>
  </li>
</ul>

<p>Last but not least, we want to know what is that projection $\Pi$, which is essentially:</p>

\[\hat{y} = X\vec{w}_{ols} = X(X^TX)^\dagger X^T \vec{y}\]

<p>which means <mark>$X(X^TX)^\dagger X^T$ is the projection matrix</mark> into the space of $\text{span}{\ddot{x}_1 , \ddot{x}_2,…\ddot{x}_d}$.</p>

<ul>
  <li>note that this orthogonal projection is <em>still</em> different from the PCA, because now we are in the dimension of $n$, i,e, the column space of data points $X$ instead of the row space $\mathbb{R}^d$.</li>
</ul>

<h4 id="statistical-view-of-linear-regression">Statistical View of Linear Regression</h4>

<p>Let’s <mark>assume</mark> that data $(x_i, y_i)$ is <mark>generated</mark> from the following process. Basically a “generative view” of the problem.</p>

<p>Consider the generation process be:</p>

<ol>
  <li>
    <p>A sample $x_i$ is drawn independently from some distribution $\mathcal{D}_X$:</p>

\[x_i \sim \mathcal{D}_X\]
  </li>
  <li>
    <p>Some intermediate $y_{\text{clean}}$ is computed by some <strong>fixed (for all $x_i$)</strong> but unknown $w$:</p>

\[y_{\text{clean}_i}:= w \cdot x_i\]
  </li>
  <li>
    <p>Then you have some noise $\epsilon_i \sim N(0, \sigma^2)$ that disrupts that $y_{\text{clean}}$ to get your <mark>actual "label"</mark>:</p>

\[y_i := w \cdot x_i + \epsilon_i\]

    <p><em>Pictorially</em></p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Model:</th>
          <th style="text-align: center">Actual Data:</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211029151347255.png" alt="image-20211029151347255" style="zoom:50%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211029151403143.png" alt="image-20211029151403143" style="zoom:50%;" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Then what you can <mark>only see (as training data) it the final result</mark>:</p>

\[S:=(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\]
  </li>
</ol>

<blockquote>
  <p>Now, the <strong>goal</strong> is to figure out what is $w$ from some <em>noised</em> data. So we want to:</p>

  <ul>
    <li>determine $w$, from the gaussian noise distribution using <mark>MLE</mark>! Because we basically just have a parameter $\theta = w$, we have some data essentially from some distribution.</li>
  </ul>
</blockquote>

<p>Observe that what happens is:</p>

\[y_i = \vec{w}\cdot \vec{x}_i + N(0,\sigma^2) = N(\vec{w}\cdot \vec{x}_i, \sigma^2)\]

<p>note that</p>

<ul>
  <li>
    <p>each $y_i$ label is dependent on $x_i$, makes sense!</p>
  </li>
  <li>
    <p>the only unknown is $\vec{w}$, since we don’t care about $\sigma$.</p>
  </li>
  <li>
    <p>if $\sigma \to \sigma_i$ is different for each data point, then this problem is not solvable as you have now two degrees of freedom.</p>
  </li>
  <li>
    <p>last but not least, this means that:</p>

\[y_i | x_i; w \sim \mathcal{N}(\vec{w}^T \vec{x}_i, \sigma^2)\]
  </li>
</ul>

<p>So to estimate $\vec{w}$, just use MLE and maximize log likelihood, given $S=(x_1,y_1), (x_2,y_2),…(x_n,y_n)$</p>

\[\arg \max_{\vec{w}}\log \mathcal{L}(w|S)\]

<p>Then basically using the IID assumption, we can simplify:</p>

\[\begin{align*}
L(w|S) 
=P(y|x;w)
&amp;= \prod_{i=1}^n P(y_i|x_i;w) \\
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y_i - w^T x_i)^2}{2 \sigma^2} \right)}
\end{align*}\]

<p>where $P(y\vert x;w)$ means a parameter $w$ is already fixed.</p>

<p>Finally, we just consider the log to convert the product to a sum:</p>

\[\begin{align*}
l(w|S)
&amp;= \log L(w|S) \\
&amp;= \log \prod_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} \exp{\left( - \frac{(y_i - w^T x_i)^2}{2 \sigma^2} \right)}\\
&amp;= \sum_{i=1}^n \left( \log\frac{1}{\sqrt{2\pi} \sigma} - \frac{(y_i - w^T x_i)^2}{2 \sigma^2}  \right)\\
&amp;= n \log\frac{1}{\sqrt{2\pi} \sigma} -\sum_{i=1}^n\left( \frac{(y_i - w^T x_i)^2}{2 \sigma^2}  \right)\\
&amp;= n \log\frac{1}{\sqrt{2\pi} \sigma} -\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^n (y_i - w^T x_i)^2
\end{align*}\]

<p>where now we can basically thrown away the term with $\sigma$, since eventually we are <strong>only maximizing over $\vec{w}$.</strong> Therefore, we basically ends up with:</p>

\[\arg\max_{\vec{w}} \mathcal{L}(w|S) = \arg\max_{\vec{w}}\sum_{i=1}^n -(\vec{w}\cdot \vec{x}_i - y_i)^2 = \arg\min_{\vec{w}} \sum_{i=1}^n (\vec{w}\cdot \vec{x}_i-y_i)^2\]

<p>which is <mark>exactly the Ordinary Least Square</mark> problem.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>note that this only works if</p>

  <ul>
    <li>
      <p>the noise is <em>not data dependent</em>. However, we can still interpret the what we are doing if $\sigma \to \sigma_i$. Basically this gives:</p>

\[\arg\min_{\vec{w}} \sum_{i=1}^n \frac{(\vec{w}\cdot \vec{x}_i-y_i)^2}{2\sigma^2_i}\]

      <p>then each $\sigma_i$ is <strong>weighing how confident</strong> my $\hat{y}_i=\vec{w}\cdot \vec{x}_i$ is. In other words, if $\sigma_i$ is large for some $x_i$, then it means even if the “discrepancy” $(\hat{y}_i - y_i)^2$ may be large, but if $\vert \hat{y}_i - y_i\vert$ is still a <mark>within $\sqrt{2} \sigma$ from the "center of mass"</mark> of the distribution, then I am <strong>quite confident</strong> about it.</p>
    </li>
    <li>
      <p>the noise is distributed as $\epsilon \sim N(0,\sigma^2)$, and the data is generated in the way mentioned above.</p>
    </li>
  </ul>
</blockquote>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Can we use a regressor to do <strong>classification</strong>?</p>

<blockquote>
  <p><em>Recall</em>:</p>

  <ul>
    <li>
      <p>one big difference in visualizing regresion and classicaition is that, for visualizing <strong>output/label</strong>:</p>

      <ul>
        <li>
          <p>linear regression <mark>tacks the label onto the same space</mark> as input</p>

          <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104144946621.png" alt="image-20211104144946621" style="zoom: 50%;" /></p>
        </li>
        <li>
          <p>classification used <mark>color code</mark> for the label, but space is still $X$.</p>

          <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104144812504.png" alt="image-20211104144812504" style="zoom: 50%;" /></p>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>In this sense, consider a classification problem looks like</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104144812504.png" alt="image-20211104144812504" style="zoom: 50%;" /></p>

<p>but we could <mark>manually tack on a output value</mark>, such that:</p>

<ul>
  <li>$Y=1$ for blue dots</li>
  <li>$Y=-1$ for red dots</li>
</ul>

<p>So that we could so some regression. Graphically we have:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104144946621.png" alt="image-20211104144946621" style="zoom: 50%;" /></p>

<p>But the problem here is that, our linear regressor would look like the <strong>line</strong> above:</p>

<ul>
  <li>for large $X$, we technically predicted it correctly to be $\hat{Y} &gt; 0$, but we are still being <strong>penalized</strong> by $\hat{Y}-Y$ getting larger as $X$ gets larger.</li>
  <li>essentially, we are fitting a “stepwise-ish” function (since we are doing classification) to a linear relationship</li>
</ul>

<p>A better model would be to consider a <mark>sigmoid function</mark> whose shape is much closer</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104145812551.png" alt="image-20211104145812551" style="zoom: 50%;" /></p>

<p>where the sigmoid function here:</p>

\[\sigma(z) = \frac{1}{1+e^{-z}} =\frac{1}{1+e^{-\vec{w} \cdot \vec{x}}}\]

<p>so basically:</p>

<ul>
  <li>the controlling parameter is $w$, which tells you how <strong>fast</strong> the ramp goes up</li>
  <li>notice that the logistic regression $\sigma : \mathbb{R} \to [0,1]$, but <mark>our data is $X \in \mathbb{R}^d$</mark>. Therefore, we basically did $z = \vec{w}\cdot \vec{x}$.</li>
</ul>

<p>Also notice that technically, ${1}/(1+e^{-z}) \to [0,1]$, but since we want to get $[-1, 1]$, we could do either of the <mark>two things</mark> to make it classification:</p>

<ol>
  <li>
    <p>shifting the function to the shape:</p>

\[f(x) = \text{sign}(2\sigma (w\cdot x) - 1)\]
  </li>
  <li>
    <p>Or just thresholding it, for example at $0.5$.</p>

\[f(x) = \text{sign}(\sigma (w \cdot x)-0.5)\]
  </li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>thought this looks like a nonlinear function, but this is in the space of $Y$ <mark>attached</mark> to $X$, i.e. $\mathbb{R}^{d+1}$. Recall that a linear classifier means the <strong>boundary in the $X$ space is affine</strong>, which is correct here.</p>

      <table>
        <thead>
          <tr>
            <th style="text-align: center">Visualization (threshold $Y&gt;0$)</th>
            <th style="text-align: center">Boundary Induced</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104174115507.png" alt="image-20211104174115507" /></td>
            <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104174158702.png" alt="image-20211104174158702" /></td>
          </tr>
        </tbody>
      </table>

      <p>notice the boundary induces is <mark>in the input space, which is linear</mark>.</p>
    </li>
    <li>
      <p>therefore, for a “higher dimensional sigmoid function”, this is still a linear boundary in the input space</p>

      <p><img src="https://i.stack.imgur.com/Pjuqe.png" alt="Fitting 3d sigmoid to data - Stack Overflow" style="zoom:50%;" /></p>

      <p>in this case, if I again put a threshold of $Z &gt; 0.5$, we get a sloped line in the input 2D Space.</p>
    </li>
    <li>
      <p>additionally, if we have a multi-class problem, we can simply tweak the binary classification as:</p>

      <ol>
        <li>For each class $c_i$, consider the <strong>binary</strong> problem of class $c_i$ vs <em>all the other class</em>. Therefore, if you have $n$ classes, you get $n$ binary classification.</li>
        <li>For each binary classification, we can compute some “probability” (see interpretation section) of $f(\vec{x}) = \sigma (\vec{w}\cdot \vec{x})$.</li>
        <li>Whatever class $c_i$ has the highest “probability” is my final prediction.</li>
      </ol>
    </li>
  </ul>
</blockquote>

<h3 id="statistical-interpretation">Statistical Interpretation</h3>

<p>Consider for some event which occurs at probability $P$. The <strong>odds</strong> of the event happening is:</p>

\[\text{odds}(P) = \frac{P}{1-P} = \frac{\text{probabilit of occuring}}{\text{probabilit of not occuring}}\]

<p>which if we consider this is <mark>asymmetric</mark></p>

<ul>
  <li>if $p=0.9$, then odds is $9$. But if $p=0.1$, odds is $0.11…$. Seems unrelated</li>
</ul>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <ul>
    <li>Given <mark>some data point $x$</mark>, what is the <mark>odds that $\text{Y=1}$</mark>? To find that out, we need to think about what is the probability of “success”, i.e. getting $Y=1$ for some $x$. Hence, something like $P(x)$.</li>
  </ul>
</blockquote>

<p>Since we should not apply some linear model to the Odds function, we can try <mark>consider the $\log$ of the odds</mark>, which is symmetric:</p>

\[\log (\text{odds}(P)) := \text{logit}(P):= \log \left( \frac{P}{1-P} \right)\]

<p>the advantage is:</p>

<ul>
  <li>
    <p>this means: <mark>what is the log of the chance/odd that some event is "successful"</mark>, i.e. how likely to have $Y=1$</p>
  </li>
  <li>
    <p>having some symmetry here makes much <strong>more sense</strong> to model $\text{logit}$ function as a linear model.??? <mark>TODO</mark></p>
  </li>
  <li>
    <p>graphically:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104190209696.png" alt="image-20211104190209696" style="zoom: 67%;" /></p>
  </li>
</ul>

<p>Now, consider our modelling being:</p>

\[\text{logit}(P(x)) := w \cdot x\]

<p>notice that $\vec{w}\cdot \vec{x} \in \mathbb{R}$, and then if we want to figure out $P(x)$:</p>

\[\begin{align*}
\text{logit}(P(x))
= \log \left( \frac{P(x)}{1-P(x)} \right)
&amp;= w \cdot x\\
\frac{P(x)}{1-P(x)}
&amp;= w \cdot x\\
P(x)
&amp;= \frac{e^{w \cdot x}}{1+e^{w \cdot x}}\\
P(x)
&amp;= \frac{1}{1+e^{-w \cdot x}}
\end{align*}\]

<p>which is basically our <strong>logistic regression</strong>.</p>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <ul>
    <li>
      <p>Basically, the idea is that, <mark>given some data $x$</mark>, we are modelling $X \sim \text{Bern}(p)$. And we basically come up with:</p>

\[p= \frac{1}{1+e^{-w \cdot x}} := \text{Probability of getting $Y=1$}\]

      <p>And we got here by modelling <mark>(log of) the odds of having $Y=1$</mark> for some data $x$ <mark>by a linear approximation $w \cdot x$</mark>.</p>

      <ul>
        <li>assuming that this is Bernoulli, that the probability of “failure” of $Y=0$ is $1-P(x)$, for $P(x)$ is the probability of “success”.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Advantage of Logistic Regression for Classification</strong></p>

  <ul>
    <li>the obvious advantage is that now, given some $x$, we can spit out <mark>how confidence/probable</mark> we are in our <mark>classification</mark>.</li>
  </ul>
</blockquote>

<h3 id="solving-logistic-regression">Solving Logistic Regression</h3>

<p>Now basically our only task left is to find out the <mark>optimum $w$</mark>, given our model being:</p>

\[1\left\{ \frac{1}{1+e^{-w \cdot x}} &gt; 0.5 \right\} = 1\left\{ P(x) &gt; 0.5 \right\}\]

<p>where basically just taking the threshold at $0.5$.</p>

<p>Now,  basically we just do MLE to find out what is the <strong>best $w$</strong>. Given IID samples $S=(x_1, y_1),…,(x_n, y_n)$ then:</p>

\[\begin{align*}
\mathcal{L}(w|S)
&amp;= p(S|w)\\
&amp;= \prod_{i=1}^n p[(x_i, y_i)|w]\\
&amp;= \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}
\end{align*}\]

<p>notice that:</p>

<ul>
  <li>
    <p>since $y_i \in {0,1}$</p>

\[p[(x_i, y_i)|w] = p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\]

    <p>which basically means we are modelling a <strong>Bernoulli distribution</strong>:</p>

    <ul>
      <li>if we get $(x_i,1)$, the probability of this happening is $p(x_i)$</li>
      <li>if we get $(x_i,0)$, the probability of this happening is $1-p(x_i)$</li>
    </ul>

    <p>(and this $p(x_i)=\sigma(w\cdot x_i)$).</p>
  </li>
</ul>

<p>Finally, we take the log likelihood:</p>

\[\begin{align*}
\log \mathcal{L}(w|S)
&amp;= \sum_{i=1}^n y_i \log p(x_i) + (1-y_i)\log (1-p(x_i))\\
&amp;= \sum_{i=1}^n \log (1-p(x_i)) + \sum_{i=1}^n y_u \log \frac{p(x_i)}{1-p(x_i)}\\
&amp;= \sum_{i=1}^n - \log (1+e^{w \cdot x_i}) + \sum_{i=1}^n y_i w \cdot x_i
\end{align*}\]

<p>where this is as far as we can go, then we just do a gradient ascent to maximize the likelihood</p>

<ul>
  <li>
    <p>the last step basically substituted the probability estimation.</p>
  </li>
  <li>
    <p>sadly, the solution by taking derivative has <strong>no closed form solution</strong>. So basically we just use <strong>gradient ascent</strong> to find the solution.</p>
  </li>
</ul>

<h2 id="variations-of-linear-regression">Variations of Linear Regression</h2>

<p>Back to the old problem of fitting a line through the data, where for OLS basically considers:</p>

\[\min ||X\vec{w} - \vec{y}||^2\]

<p>But what if we have some <strong>prior knowledge on the task</strong>?</p>

<ul>
  <li>for instance, suppose $y$ is the height for a person, and $X$ is the 1000+ genes that person has. Suppose you know <em>beforehand</em> than <em>only 10 genes</em> should matter/dictates.</li>
</ul>

<p>These idea introduces us to some popular variants of linear regressions</p>

<ul>
  <li><strong>Lasso Regression</strong> for a sparse $w$</li>
  <li><strong>Ridge Regression</strong> for a simple $w$ (avoid overfitting)</li>
</ul>

<h3 id="ridge-regression">Ridge Regression</h3>

<p>Consider now we have the <mark>objective</mark> being</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104154726308.png" alt="image-20211104154726308" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the upshot is that it will prefer <strong>feature contribution being distributed evenly</strong>, in the case of the sum of the contribution being the same.</p>

    <ul>
      <li>For instance, $w=[0.5,0.5]^T$ would be preferred over $w=[1,0]^T$.</li>
    </ul>

    <p>Therefore, this is most suitable when a data set contains <mark>a higher number of features than the number of observations</mark>, so that we <mark>prevent overfitting.</mark></p>
  </li>
  <li>
    <p>so again, we have <strong>competing</strong> terms of reconstruction error and regularization (preventing overfitting of features)</p>
  </li>
  <li>
    <p>notice that here we are doing a <mark>L-2 regularization</mark>, since $\vert \vert \vec{w}\vert \vert _2^2$ is L-2 distance.</p>
  </li>
</ul>

<p>The solution for the ridge regression can be solved exactly:</p>

\[\vec{w}_{ridge}=(X^TX + \lambda I)^{-1}X^T \vec{y}\]

<p>which basically comes from taking the derivative of the objective and setting it to zero, and note that:</p>

<ul>
  <li>this matrix $X^TX + \lambda I$ is exactly <mark>invertible</mark> since it is now <strong>positive definite</strong> (because we added some positive number to diagonal)</li>
  <li>since $X^TX + \lambda I$ is invertible, this always result in a <mark>unique solution</mark>.</li>
</ul>

<p>But also notice that this objective is <mark>similar</mark> to the following optimization problem</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104155643938.png" alt="image-20211104155643938" style="zoom: 50%;" /></p>

<p>which is <strong>similar</strong> but not the same as the original task, but:</p>

<ul>
  <li>it is similar if you take the constraint and put it to the objective for a Lagrangian.</li>
  <li>gives us some idea what is going on <strong>geometrically</strong></li>
</ul>

<p>Then, the problem basically looks like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Objective Function</th>
      <th style="text-align: center">Contour Projection into $w$ Space</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104202925337.png" alt="image-20211104202925337" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211104155959797.png" alt="image-20211104155959797" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>since basically:</p>

<ul>
  <li>
    <p>the objective (left figure) is a quadratic function in $w$, so in the space of $z=\vert \vert X\vec{w}-\vec{y}\vert \vert ^2$ basically is a bowl.</p>
  </li>
  <li>
    <p>Now, since we want to find the minimum $w$, we consider <strong>projecting contour lines to the $w$ space</strong>, we get ellipses (right figure)</p>
  </li>
  <li>
    <p>the constraint of $\vert \vert w\vert \vert ^2 \le B$ gives you the blue ball</p>
  </li>
  <li>
    <p>therefore the $\vec{w}_{\text{ridge}}$ <em>in this case</em> is touching the circumference of the circle (gradient vector parallel to normal vector), which is <mark>unique</mark>, and this problem is also a <strong>convex optimization</strong>.</p>

    <ul>
      <li>
        <p>however, consider using L-0 regularization, then you <em>could get multiple solutions $\vec{w}$</em>. e.g.</p>

        <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109150532372.png" alt="image-20211109150532372" style="zoom:50%;" /></p>

        <p>where the “plus” like shape is the L-0 constraint. Since the feasible set is not a convex set, it is <strong>not a convex optimization</strong>. The <mark>closest convex approximation</mark> is the L-1 constraint, which gives us the Lasso’s Regression.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="lasso-regression">Lasso Regression</h3>

<p>Now, consider basically the L-0 convex approximation problem, which is Lasso’s regression:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109151353105.png" alt="image-20211109151353105" style="zoom:50%;" /></p>

<p>Geographically, we are looking at:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Actual Aim (Sparsity)</th>
      <th style="text-align: center">Lasso’s Approximation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109151557750.png" alt="image-20211109151557750" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109151050867.png" alt="image-20211109151050867" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where recall that the closest convex approximation of a L-0 region is the L-1 norm (i.e. by simply filling up the non-convex region), then:</p>

<ul>
  <li><strong>sparsity</strong> is the achieved on the left figure, as we will get <strong>either $\vec{w} = [a,0]^T$ or $\vec{w} = [0,a]^T$</strong>. In <em>this case</em> we get the former</li>
  <li>however, since the L-0 problem is not a convex problem, we used L-1 norm instead and attempts to <strong>approximate/encourage sparsity</strong>.
    <ul>
      <li>in this case, if the concentric ellipses happens to align itself such that $\vec{w}_{lasso}$ is at the corner, then we have achieved perfect sparsity.</li>
    </ul>
  </li>
</ul>

<p>Also, sadly there is <strong>no closed form solution</strong>:</p>

\[\vec{w}_{\text{lasso}} =?\]

<p>yet we can solve this using <strong>iterative methods</strong> such as gradient descent, just taking derivatives of the <mark>objective function</mark>.</p>

<p>The equivalent problem in <strong>optimization/Lagrange</strong> is:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109152425438.png" alt="image-20211109152425438" style="zoom:67%;" /></p>

<p>which is a convex optimization problem.</p>

<ul>
  <li>don’t forget that in the end, your predictor is still <em>linear</em> which is basically $\hat{y} = X\vec{w}$.</li>
</ul>

<blockquote>
  <p><strong>Elastic Net</strong></p>

  <p>Technically you can also combine the two optimization to get:</p>

\[\min ||X\vec{w}-\vec{y}||^2 + \lambda ||\vec{w}||_1 + \gamma ||\vec{w}||_2^2\]

</blockquote>

<h2 id="regression-optimality">Regression Optimality</h2>

<p>Given some data, can we find some <strong>optimal estimator</strong> (not necessarily linear), that basically <mark>parallels the concept of a Bayes' classifier</mark> in the discrete case?</p>

<p>Recall that in the discrete case, the Bayes was simply taking the most possible estimate:</p>

\[f^*(x) = \arg \max_{y \in Y} P[Y=y|X=x]\]

<p>notice that it like a function that is taking the best value <strong>point-wisely</strong> for each $x$. Therefore, the parallel in <mark>continuous case</mark> is:</p>

\[f^*(x) := \mathbb{E}_{y|x}[Y|X=x]\]

<p>where basically:</p>

<ul>
  <li>
    <p>for each $x$, the randomness is only in <strong>its “label”</strong>, so we are averaging over $y\vert x \sim \mathcal{D}$ of the <strong>true population</strong>.</p>
  </li>
  <li>
    <p>graphically, consider a “thin slice” at some $X=x$, then you want to see how $y$ value is distributed:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109154545105.png" alt="image-20211109154545105" style="zoom: 67%;" /></p>

    <p>which makes sense since in the end we can <em>only return one value/label</em> for a given $x$.</p>
  </li>
  <li>
    <p>However, why is <mark>taking the mean</mark> giving us the <mark>best performance</mark>? What <mark>performance metric</mark> are we using?</p>
  </li>
</ul>

<blockquote>
  <p><strong>Theorem</strong>:</p>

  <p>For any regression estimate $g(x)$:</p>

\[\mathbb{E}_{(x,y)}[ |f^*(x) - y|^2 ] \le \mathbb{E}_{(x,y)}[ |g(x) - y|^2 ]\]

  <p>so basically:</p>

  <ul>
    <li>our “error” is a <strong>L-2 error</strong> of $\vert f^<em>(x)-y\vert ^2$. Therefore, the optimal $f^</em>$ defined above is also the <strong>optimal $L_2$ regressor</strong>.</li>
    <li>so the <mark>performance metric</mark> is actually $\vert f^*(x)-y\vert ^2$.</li>
  </ul>
</blockquote>

<p><em>Proof</em></p>

<p>Bsaically the idea is to compute the <strong>minimum of</strong>:</p>

\[\arg\min_{g(x)}\mathbb{E}[ |g(x)-y|^2 ]\]

<p>Then basically:</p>

\[\begin{align*}
\mathbb{E}[ |g(x)-y|^2 ]
&amp;= \mathbb{E}[ |g(x)-f^*(x) + f^*(x)-y|^2 ]\\
&amp;= \mathbb{E}[ |g(x)-f^*(x)|^2] + \mathbb{E}[|f^*(x)-y|^2 ]
\end{align*}\]

<p>which works because the <strong>cross term is basically zero</strong>:</p>

\[\begin{align*}
2\mathbb{E}_{x,y}[ (g(x)-f^*(x))(f^*(x)-y) ]
&amp;= 2\mathbb{E}_{x}[ \mathbb{E}_{y|x} [(g(x)-f^*(x))(f^*(x)-y)  | X=x  ] ]\\
&amp;= 2\mathbb{E}_{x}[(g(x)-f^*(x)) \mathbb{E}_{y|x} [(f^*(x)-y)  | X=x  ] ]\\
&amp;= 2\mathbb{E}_{x}[(g(x)-f^*(x))(f^*(x) - f^*(x))]\\
&amp;= 0
\end{align*}\]

<p>notice that the</p>

<ul>
  <li>first equality used the identity that $\mathbb{E}<em>{x,y} = \mathbb{E}_x[ \mathbb{E}</em>{y\vert x} ]$</li>
  <li>the second equality used the fact that when we conditioned on $X=x$, then $(g(x)-f^*(x))$ is a constant</li>
  <li>the third equality used the definition that $f^*(x) := \mathbb{E}_{y\vert x}[Y\vert X=x]$</li>
</ul>

<p>Therefore, now if we compute the minimum:</p>

\[\begin{align*}
\arg\min_g \mathbb{E}[ |g(x)-y|^2 ]
&amp;= \arg\min_g \left\{ \left( \int_x |g(x) - f^*(x)|^2 \mu \,dx \right)  + \mathbb{E}[|f^*(x)-y|^2] \right\} \\
&amp;=  \arg\min_g \left\{ \left( \int_x |g(x) - f^*(x)|^2 \mu \,dx \right) \right\} \\
&amp;= f^*(x)
\end{align*}\]

<p>where:</p>

<ul>
  <li>the first equality bascially comes from the result above, and we explicitly computed the first expected value</li>
  <li>the second equality comes from the fact that we are minimizing over $g(x)$</li>
</ul>

<p>Therefore, under this <mark>specific performance metric</mark>, $f^*(x) := \mathbb{E}_{y\vert x}[Y\vert X=x]$is the best we can do.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Again, this is the optimal over the <strong>true population</strong>, since we are having access to $\mathbb{E}_{y\vert x}[Y\vert X=x]$, which means we need to see <mark>all possible $y$</mark>, and eventually this needs to be done for <mark>all possible $x$</mark> as well.</p>

  <ul>
    <li>this, in a sense, is approximated/answered in the non-parametric regression</li>
  </ul>
</blockquote>

<h2 id="non-parametric-regression">Non-Parametric Regression</h2>

<p>Consider you are given some <strong>training data</strong>, and your task now is to predict $x_0$:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109163054175.png" alt="image-20211109163054175" style="zoom: 80%;" /></p>

<p>where notice that we are facing the problem of:</p>

<ul>
  <li>if you <strong>only look at the slice of $x=x_0$</strong>, we might have no data of $y$ at all!</li>
  <li>therefore, the idea is to <mark>look at slice of the neighborhood around $x_0$</mark></li>
</ul>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <p>To approximate/practically do the optimal regression problem, we could do:</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109163330074.png" alt="image-20211109163330074" style="zoom: 50%;" /></p>

  <p>Graphically:</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109163347087.png" alt="image-20211109163347087" style="zoom:67%;" /></p>

  <p>where here we are taking a <strong>bandwidth</strong> (tunable), so that in general:</p>

  <ul>
    <li>if you have few data, you might want to have a large bandwidth</li>
    <li>if you have lots of data, you can use a small bandwidth</li>
  </ul>
</blockquote>

<p>So the idea is to look at the neighbor, but we can even do better by <strong>weighting the neighbor</strong> by how far it is from the data point. This introduces us the idea of non-parametric regression - <mark>Kernel Regression</mark></p>

<h3 id="kernel-regression">Kernel Regression</h3>

<p>Consider that we have taken a slice in the example above. If we simply compute:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109163330074.png" alt="image-20211109163330074" style="zoom: 50%;" /></p>

<p>then it is the same as considering, <mark>within that slice</mark>, we take a <mark>uniform weighting</mark>:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109164029716.png" alt="image-20211109164029716" style="zoom:80%;" /></p>

<p>In general, the idea of kernel regression is that, for <mark>given $x$ and within the slice/neighborhood</mark>:</p>

\[\hat{y} = \hat{f}_n(x):= \sum_{i=1}^n w(x,x_i)y_i\]

<p>so basically we are weighting <strong>each contribution by $w(x,x_i)$</strong>:</p>

\[w(x,x_i) = \frac{K_h(x,x_i)}{\sum_{j=1}^n K_h(x,x_j)}\]

<p>where the above is now clear if we consider some example kernels:</p>

\[K_h(x,x') = 
\begin{cases}
1\{ ||x-x'|| \le h \}, &amp; \text{Box Kernel}\\
e^{-||x-x'||^2/h}, &amp; \text{Gassian Kernel}\\
[1-(1/h)||x-x'||]_+, &amp; \text{Triangle Kernel}
\end{cases}\]

<p>where the <strong>bandwidth parameter is $h$</strong>.</p>

<ul>
  <li>
    <p>since the Kernel is not normalized, we are normalizing them in the $w(x,x_i)$ function.</p>
  </li>
  <li>
    <p>e.g. if you take the Box Kernel, then you just get the predictor mentioned at the beginning of the section with uniform weighting.</p>
  </li>
</ul>

<h3 id="consistency-theorem">Consistency Theorem</h3>

<p>Recall that the best possible regression we can make is $f^*(x) := \mathbb{E}_{y\vert x}[Y\vert X=x]$. Here, we want to investigate <strong>how close kernel regression is to the best regression</strong>.</p>

<blockquote>
  <p><strong>Theorem</strong></p>

  <p>As $n\to \infty$, $h \to 0$ but slower than $n$, so that $nh \to \infty$ (is we shrink too fast, then there is no data in the bandwidth slice). Then:</p>

\[\mathbb{E}_{\vec{x},y}[|\hat{f}_{n,h}(x) - f^*(x)|^2] \to 0\]

  <p>for the kernel regression, so basically where:</p>

\[\hat{f}_n(x):= \sum_{i=1}^n w(x,x_i)y_i = \sum_{i=1}^n \frac{K_h(x,x_i)}{\sum_{j=1}^n K_h(x,x_j)} y_i\]

  <p>for most (reasonable) of the localization kernels. (for the kernels introduced above, it works)</p>
</blockquote>

<p><em>Proof Sketch</em></p>

<p>Basically we start with manipulating:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211109170143720.png" alt="image-20211109170143720" style="zoom:67%;" /></p>

<p>where the term:</p>

<ul>
  <li>$\mathbb{E}[\hat{f}_{h,n}(x)]-f^*(x)$ is basically measuring the <strong>bias</strong>, how far off am I form the optimal if I average myself</li>
  <li>$\mathbb{E}[\hat{f}_{h,n}(x) - \mathbb{E}[f^<em>(x)]]$ is measuring, on average, how far off is $\hat{f}$ from the central point of $f^</em>$. which is the <strong>variance</strong></li>
</ul>

<p>Eventually, after some calculations you will get the two terms being:</p>

\[\begin{align*}
\text{sqrd. bias} &amp;\approx c_1 h^2 \\
\text{variance} &amp;\approx c_2 \frac{1}{nh^d}
\end{align*}\]

<p>Then, since we can choose any bandwidth, consider <mark>$h \approx n^{-1/(2+d)}$</mark>, then we get:</p>

\[\mathbb{E}[|\hat{f}_{h,n}(x) - f^*(x)|^2] \approx n^{-2 / (2+d)} \to 0, \quad \text{as } n \to \infty\]

<h1 id="statistical-learning-theory">Statistical Learning Theory</h1>

<p>How do we formalize the concept that “our machine has <mark>learned</mark> something” (e.g. what is a face vs what is an apple.) So the idea is that we want to define what what <strong>learning means</strong>.</p>

<ul>
  <li>in specific, we are using a statisical approach, so that is why it is called “statistical learning theory”</li>
</ul>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <p>The basic process of learning should invovle:</p>

  <ul>
    <li>Observe a phenomenon</li>
    <li>Construct a <strong>model</strong> from observations</li>
    <li>Use that <strong>model</strong> to make decisions / predictions</li>
  </ul>
</blockquote>

<p>To formalize the above mathematically:</p>

<ul>
  <li>
    <p><strong>Observing</strong> the phenomenon of interest:</p>

    <ol>
      <li>There is an input space $X$ and an output space $Y$</li>
      <li>There is some distribution $\mathcal{D}$ where you get those data, but the model <mark>does not know what is the distribution</mark></li>
      <li>The learner <strong>observes</strong> $m$ examples $(x_1, y_1),…,(x_m, y_m)$ drawn from $\mathcal{D}$
        <ul>
          <li>note that at this stage, techinically this <em>does not have to be IID</em>. But for the scope of this course, IID is the assumption.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Constructing</strong> a model</p>

    <ol>
      <li>
        <p>Let $\mathcal{F}$ be a collection of models (functions), where each $f :X \to Y$ predicts $y$ given some $x$</p>

        <ul>
          <li>so each $f \in \mathcal{F}$ is a predictor/function/hypothesis (i.e. on this $x_0$, I hypothesize it should be $y_0$)</li>
        </ul>
      </li>
      <li>
        <p>From $m$ observations, <strong>select</strong> a model $f_m \in \mathcal{F}$ which predicts <mark>well</mark></p>

        <ul>
          <li>
            <p>where $f_m$ means that the choice is dependent on the $m$ data we provided.</p>
          </li>
          <li>
            <p>our metric of measuring performance is:</p>

\[\mathrm{err}(f) := \mathbb{P}_{(x,y)\sim \mathcal{D}}[f(x) \neq y]\]

            <p>which again is just the <mark>generalization error</mark> defined on the <strong>entire/true population</strong>.</p>
          </li>
        </ul>

        <p>Then, we say that we are predicting well if:</p>

\[\mathrm{err}(f_m) - \mathrm{err}(f^*) \le \epsilon\]

        <p>for some tolerance level $\epsilon &gt; 0$ of our choice, and that:</p>

        <ul>
          <li>$f^<em>$ is the optimal predictor in the class $\mathcal{F}$, such that $f^</em> =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$. Therefore, if the optimal Bayes is <em>not</em> in $\mathcal{F}$, then $f^*$ is obviously not the optimal Bayes.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>Now the problem is we <mark>cannot compute $\mathrm{err}(f)$</mark> since this is dependent on the true distribution. So can we find some way to approximate it in the practical world?</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>most of the machine learning we did before is on how to <strong>select</strong> a model. e.g. SVM is selecting from a range of linear classifer that gives the largest margin.</li>
  </ul>
</blockquote>

<h2 id="pac-learning">PAC Learning</h2>

<p>This is the <strong>stringest</strong> definition of learning we have, but its name is a bit unfortunate: Probably Approximately Correct - PAC.</p>

<blockquote>
  <p><strong>PAC Learning Definition</strong></p>

  <p>For <mark>all</mark> tolerance levels $\epsilon &gt; 0$, and all confidence level $\delta &gt; 0$, if there is exist some model selection algorithm $A$ that <strong>selects $f_m^A\in \mathcal{F}$ from $m$ observations,</strong> i.e  $A: (x_i,y_i)_{i=1}^m \to f_m^A$, that has the property:</p>

  <blockquote>
    <p>(for <mark>all $\epsilon &gt;0, \delta&gt;0$</mark>), with probably at least $1-\delta$ over <mark>some</mark> draw of the sample with size $m$:</p>
  </blockquote>

\[\mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon\]

  <blockquote>
    <p>for the learned/selected predictor $f_m^A$ using your algorithm.</p>
  </blockquote>

  <ul>
    <li>the parameter $\delta$ is necessary, because you might <em>accidentally be given some “bad samples”</em>, so you cannot achieve the $\epsilon$ bound (e.g. very biased data). Hence this is necessary due to the fact that your sample drawn is <strong>random</strong>.</li>
    <li>the parameter $m$ is tunable. The aim is to find some $m$ that makes it work.</li>
    <li>note that this error is the <mark>generalization</mark> error, not training/testing error.</li>
  </ul>

  <p>If the above holds for $A: (x_i,y_i)_{i=1}^m \to f_m^A$, then the <mark>model class $\mathcal{F}$</mark> is PAC-learnable.</p>

  <ul>
    <li>
      <p>Since what PAC can make sure is that you are $1-\epsilon$ correct for $1-\delta$ of the time, this is probably, approximately correct.</p>

      <p>Graphically:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118165156991.png" alt="image-20211118165156991" style="zoom:67%;" /></p>

      <p>so basically $1-\delta$ of the time my $f_m^A$ is within the good range of $(\mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon)$.</p>
    </li>
  </ul>
</blockquote>

<p>Some reality constraints that we should also consider here is:</p>

<ul>
  <li>time complexity</li>
  <li>space complexity (dependent on your input and on your algorithm)</li>
  <li>sample complexity (i.e. how many samples $m$ do you need to get PAC-learnable)</li>
</ul>

<blockquote>
  <p><strong>Efficiently PAC-Learnable</strong></p>

  <p>If a model class $\mathcal{F}$ is learnable, and if the data needed $m$ is <strong>polynomial in $1/\epsilon$ and $1/ \delta$</strong>, then we call the <strong>model class $\mathcal{F}$</strong> being Efficiently PAC-learnable.</p>
</blockquote>

<p>A popular algorithm is the ERM algorithm, that basically works <strong>on some given sample</strong>:</p>

\[f_m^{\mathrm{ERM}} := \arg\min_{f \in \mathcal{F}} \frac{1}{m} \sum_{i=1}^m 1\{ f(x_i) \neq y_i \}\]

<p>the key thing here is that:</p>

<ul>
  <li>
    <p>this is <mark>an algorithm $A$</mark>, that operates on some input data of size $m$ and spits our a predictor.</p>
  </li>
  <li>
    <p>this is defined on <em>some sample</em>, which may either the <mark>training sample or the test sample</mark> (or whatever sample)</p>
  </li>
</ul>

<h3 id="simple-pac-learning-erm-algorithm">Simple PAC Learning: ERM Algorithm</h3>

<p>Now we want to consider the “performance” of <strong>ERM model</strong>, see under what $m$ do we have PAC Learnable condition.</p>

<blockquote>
  <p><strong>Theorem (finite $\mathcal{F}$)</strong></p>

  <p>Let us have some sampled data $(x_1,y_1), …, (x_m,y_m)$ of size $m$ <strong>IID</strong> from some <em>unknown $\mathcal{D}$</em>. Pick any tolerance level $\epsilon &gt; 0$ and confidence level $\delta &gt; 0$, we can let:</p>

\[m \ge  \frac{2}{\epsilon^2} \ln \frac{|\mathcal{F}|}{\delta}\]

  <p>for some constant $C$ which will be shown later.</p>

  <p>Then with probability at least $1-\delta$, we will <strong>achieve</strong>:</p>

\[\mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon\]

  <p>Essentially $\mathcal{F}$ will be <strong>efficiently PAC Learnable</strong> for using the ERM algorithm.</p>

  <ul>
    <li>recall that $f^* =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$, so it is basically <mark>as good as the best on in the class</mark>, not necessarily the optimal Bayes</li>
  </ul>
</blockquote>

<p>Note that here, we are talking about finite $\mathcal{F}$</p>

<ul>
  <li>
    <p>this is “simple” PAC learnable because we have a <strong>discrete/finite</strong> number of $f \in \mathcal{F}$ .</p>
  </li>
  <li>so linear classifier and neural networks are both not accounted for, since they have infinite hypothesis size</li>
  <li>but later on we will generalize this to infinite hypothesis class size</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>This theorem is also called the <strong>Occam’s Razor Theorem</strong>, its principle is basically that the “all things being equal (<mark>i.e. same performance</mark>), simplest explanation is the best explanation”, because here we are considering the <strong>smaller the $m$, the better</strong>, which means:</p>

\[\text{smaller } \ln(|\mathcal{F}|)\]

  <p>since $\ln (\vert \mathcal{F}\vert )$ is like a representation of how many “bits” you need to differentiate all the predictors in your class:</p>

  <ul>
    <li>simplicity = representation succinctness = less complicated model</li>
  </ul>
</blockquote>

<p>In other words, a less complicated model class is preferred, e.g. linear classifier <strong>preferred</strong> over neural network, if the performance (e.g. generalization error) is the same.</p>

<hr />

<p><em>Proof:</em></p>

<p>The idea is that since $\mathrm{err}(f)$ is not computable, <strong>we want to relate this to $\mathrm{err}_m(f)$</strong>, then we can do some computation. First defined that:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211111154338054.png" alt="image-20211111154338054" style="zoom: 50%;" /></p>

<p>where be careful:</p>

<ul>
  <li>sample error could be test data, could be train data, etc. All it matters is that it is sampled from $\mathcal{D}$.</li>
</ul>

<p>Then, the trick is to compare the <em>generalization</em> error of:</p>

\[\begin{align*}
\mathrm{err}(f_M^{\mathrm{ERM}}) - \mathrm{err}(f^*)
&amp;= (\mathrm{err}(f_M^{\mathrm{ERM}}) - \mathrm{err}_m(f_M^{\mathrm{ERM}})) +\\
&amp;\quad + (\mathrm{err}_m(f_M^{\mathrm{ERM}}) - \mathrm{err}_m(f^*) )\\
&amp;\quad + (\mathrm{err}_m(f^*) - \mathrm{err}(f^*))\\
&amp;\le 2 \max_{f \in \mathcal{F}} | \mathrm{err}(f) - \mathrm{err}_m(f) |
\end{align*}\]

<p>where:</p>

<ul>
  <li>the first and third term is bounded by the the <strong>“worst $f \in \mathcal{F}$”</strong></li>
  <li>the second term disappears because $\mathrm{err}_m(f_m^{\mathrm{ERM}})$ is the lowest possible sample error you can get, so that part is less than $0$.</li>
  <li>note that in the end, $\mathrm{err}_m(f)$ is typically what you compute practically as test error.</li>
</ul>

<p>The result $2 \max_{f \in \mathcal{F}} \vert  \mathrm{err}(f) - \mathrm{err}_m(f) \vert$ is also called the “<strong>uniform deviations of expectation of a random variable to the sample</strong>”, because essentially:</p>

<ul>
  <li>for some fixed $f$, $\mathrm{err}_m(f)$ is basically expectation (computing $\mathrm{err}_m$) of a random variable due to sampled data with size $m$ (which is unknown/random)</li>
  <li>since this is then performed over $\max_{f\in \mathcal{F}}$, this is then computed “uniformly” for all $f \in \mathcal{F}$/all those different random variables.</li>
</ul>

<p>Now, to proceed and compute $2 \max_{f \in \mathcal{F}} \vert  \mathrm{err}(f) - \mathrm{err}_m(f) \vert$, consider an <strong>arbitrary, fixed classifier</strong> $f\in \mathcal{F}$ and a sample $(x_i,y_i)$, then we have a random variable:</p>

\[Z_i^f := 1\{f(x_i) \neq y_i \}\]

<p>basically whether if we got it correct, for some random sample. Notice then:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211116151521819.png" alt="image-20211116151521819" style="zoom:50%;" /></p>

<p>where the generalization error comes from the fact that:</p>

<ul>
  <li>$Z_1^f$ is still a <mark>random draw from some fixed distribution</mark>, so the generalization is $\mathbb{E}[Z_1^f]$.</li>
</ul>

<p>Since $Z \in {0,1}$ basically is a Bernoulli random variable, the following Lemma will actually be useful:</p>

<blockquote>
  <p><strong>Lemma (Chernoff-Hoeffding bound ‘63)</strong></p>

  <p>Let $Z_1, Z_2, …, Z_m$ be $m$ Bernoulli random variables drawn IID from $\text{Bern}(p)$, for any tolerance level $\epsilon &gt;0$:</p>

\[P_{Z_i}\left[ \left| \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right| &gt;\epsilon\right] \le 2e^{-2\epsilon^2m}\]

  <p>where note that:</p>

  <ul>
    <li>
      <p>$\mathbb{E}[Z_i] = p$ for the Bernoulli random variable</p>
    </li>
    <li>
      <p>again, the randomness is in the sampling for $Z_i$ for some fixed distribution $\text{Bern}(p)$</p>
    </li>
    <li>
      <p>this basically says that the chance that your sample average is <strong>away</strong> from the truth is <strong>bounded</strong>. Graphiaclly, this is trying to do:</p>

      <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211116173606809.png" alt="image-20211116173606809" style="zoom:50%;" /></p>

      <p>so that basically the chances of being farther off than $\epsilon$ is bounded as we increase $m$.</p>
    </li>
  </ul>
</blockquote>

<p>Then, we want to evaluate the <mark>how probable it is to have picked a bad classifier $f \in \mathcal{F}$</mark>, i.e. sample average is far off from true average $p=\mathbb{E}[Z_1]$:</p>

\[P_{x_i, y_i}\left[\exists f \in \mathcal{F} , \, \left|  \ \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right| &gt;\epsilon \right]\]

<p>Then, using the <strong>union bound theorem</strong>:</p>

\[P(\exists f \text{ bad}) \le P(f_1 \text{ bad}) + P(f_2 \text{ bad}) + ... P(f_k \text{ bad})\]

<p>Therefore we have:</p>

\[\begin{align*}
&amp;P_{x_i, y_i}\left[\exists f \in \mathcal{F} , \, \left|  \ \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right| &gt;\epsilon \right]\\
&amp;\le \sum_{f \in \mathbb{F}} P_{(x_i,y_i)}\left[\, \left|  \ \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right| &gt;\epsilon \right]\\
&amp;\le 2|\mathcal{F}|e^{-2\epsilon^2 m} \\
&amp;\le \delta
\end{align*}\]

<p>where on the second inequality we used the Chernoff-Hoeffding bound. But here what it says is that:</p>

<ul>
  <li>for probability les than $\delta$ we are doing to pick a <strong>bad classifier</strong></li>
  <li>alternatively, <strong>ALL</strong> classifiers are good (smaller than $\epsilon$ away) with probability $1-\delta$</li>
</ul>

<p>Now, recall that we need to to make sure our $f$ is good for $1 - \delta$ amount of time, when we just need to solve for $m$ in:</p>

\[\begin{align*}
2|F| e^{-2 \epsilon^2 m} &amp;\le \delta\\
m &amp;\ge \frac{1}{2 \epsilon^2} \ln \frac{2|\mathcal{F}|}{\delta}
\end{align*}\]

<p>so that if $m$ <mark>satisfies this condition</mark>, then we <mark>WILL get good classifiers (error smaller than $\epsilon$ away) for $1-\delta$ of the time</mark> such that:</p>

\[\left|  \ \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right|=| \mathrm{err}_m(f) - \mathrm{err}(f) | \le \epsilon\]

<p>Hence, putting it back into our equation in the beginning, we realize that:</p>

\[\begin{align*}
\mathrm{err}(f_M^{\mathrm{ERM}}) - \mathrm{err}(f^*)
&amp;\le 2 \max_{f \in \mathcal{F}} | \mathrm{err}(f) - \mathrm{err}_m(f) | = 2 \epsilon
\end{align*}\]

<p>Hence, to force it back to $\epsilon$, we need to <mark>substitute $\epsilon \to \epsilon/2$</mark> for their $m$ so that we get:</p>

\[m \ge \frac{2}{\epsilon^2} \ln \frac{2|\mathcal{F}|}{\delta}\]

<p>being the necessary condition, so we finish with the following:</p>

\[\begin{align*}
\mathrm{err}(f_M^{\mathrm{ERM}}) - \mathrm{err}(f^*)
&amp;\le 2 \max_{f \in \mathcal{F}} | \mathrm{err}(f) - \mathrm{err}_m(f) | = \epsilon
\end{align*}\]

<p>which happens for $1-\delta$ of the time.</p>

<hr />

<p><em>Proof: Chernoff-Hoeffding bound</em> (Optional)</p>

<p>First, Markov’s inequality states that for $X \ge 0$:</p>

\[P(X \ge c) \le \frac{\mathbb{E}[X]}{c}\]

<p>this is kind of easy to see because: observe that since $X$ is a non-negative random variable:,</p>

<ul>
  <li>then $c \cdot 1{ X \ge c } \le X$</li>
  <li>take the expected value on both sides.</li>
</ul>

<p>Then, we can construct $X$ to be $\vert X-\mathbb{E}[X]\vert$, so that:</p>

\[\begin{align*}
P[|X-\mathbb{E}[X]| \ge c]
&amp;= P[(X-\mathbb{E}[X])^2 \ge c^2]\\
&amp;\le \frac{\mathbb{E}[(X-\mathbb{E}[X])^2]}{c^2}\\
&amp;= \frac{\text{Var}[X]}{c^2}
\end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>the second inequality comes from Markov’s inequality</p>
  </li>
  <li>
    <p>now this is true for <mark>any distribution/random variable</mark> .</p>
  </li>
  <li>
    <p>notice that this can be applied for any power $(X-\mathbb{E}[X])^n$ for some even $n$.</p>
  </li>
</ul>

<p>Then, Chernoff’s bounding method basically takes a $X\to e^{tX}$ for any $t &gt; 0$, so we obtain a sharper bound. Let $X$ be a random variable (not necessarily non-negative), for some constant $c$:</p>

\[\begin{align*}
P[X \ge c]
&amp;= P[e^{tX} \ge e^{tc}]\\
&amp;\le \frac{\mathbb{E}[e^{tX}]}{e^{tc}}
\end{align*}\]

<p>notice this is sharper because it tapers off at speed $\propto e^{-c}$. This is called the Chernoff’s inequality.</p>

<p>Finally, we apply this to our calculation:</p>

\[\begin{align*}
P_{Z_i}\left[ \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1] &gt;\epsilon\right]
&amp;= P\left[ \sum_{i=1}^m Z_i - m\mathbb{E}[Z_1] &gt;m\epsilon\right]  \\
&amp;= P\left[  \sum_{i=1}^m Y_i &gt;m\epsilon\right]\\
&amp;\le \frac{\mathbb{E}[e^{t\sum Y_i}]}{e^{tm\epsilon}}\\
&amp;= \frac{1}{e^{tm\epsilon}} \prod_{i=1}^m \mathbb{E}[e^{tY_i}]\\
&amp;\le e^{(t^2m / 8) - tm\epsilon} \\ 
&amp;\le e^{-2\epsilon^2m}
\end{align*}\]

<p>where the second last inequality comes from the fact that (proof omitted) $\mathbb{E}[e^{tY_1}] \le e^{t^2 / 8}$; the last inequality comes from using $t=4\epsilon$. Notice that we <mark>didn't take the absolute value</mark>. Therefore, to obtain the result we had, this is bounded by 2 times:</p>

\[P_{Z_i}\left[ \left| \frac{1}{m} \sum_{i=1}^m Z_i - \mathbb{E}[Z_1]\right| &gt;\epsilon\right] \le 2e^{-2\epsilon^2m}\]

<h3 id="vc-theory">VC Theory</h3>

<p>One problem of using Simple PAC Learning is that we assumed the hypothesis class is finite in size. However, for most if not all the algorithms we used, the hypothesis size is infinite!</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118145705540.png" alt="image-20211118145705540" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>Aim</strong>: need find another way to capture the richness of $\mathcal{F}$.</p>
</blockquote>

<p>(Note that all the concepts below are for <mark>classification</mark>. They can be extended to regression.)</p>

<blockquote>
  <p><strong>Definition (Vapnik-Chervonenkis or VC dimension):</strong></p>

  <p>We say that a model class $\mathcal{F}$ as VC dimension $d$, if $d$ is the <strong>largest set</strong> of points $x_1, …, x_d$ such that for all possible labelings of $x_1, …, x_d$, (i.e. there are in total $2^d$ labellings if is binary) there exists some $f\in \mathcal{F}$ that achieves that labelling.</p>

  <ul>
    <li>
      <p>the fact that some configuration of $x_1, …, x_d$ with any label can be classified is also called <mark>shattering</mark></p>
    </li>
    <li>
      <p>Note that VC dimension $d$ is dependent on <em>some set of points</em> AND <em>some model</em>. As long as there is one configuration that works, it is ok.</p>
    </li>
    <li>
      <p>intuively, this is a measure of <strong>richness/complexity</strong> of the hypothesis class, since, e.g. if you have a polynomial of degree 5, then for any $d=5$ data you give me, I can achieve the labelling. So I <em>at least have VC=$5$</em> (unless $d=5$ is the larges set I can find).</p>
      <ul>
        <li>another way to see this is that, for those $d$ points, you can have <mark>any noise to the label</mark>, and the model will fit to that label (is bad). So it hints at how many data you need to not overfit.</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p><em>For Example:</em> $\mathcal{F}$ = linear classifier in $\mathcal{R}^2$</p>

<p>For $d=1, 2$, it is trivial. For $d=3$, I can cime up with the following confiiguration such that they are separable</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118151418357.png" alt="image-20211118151418357" style="zoom: 67%;" /></p>

<p>note that:</p>

<ul>
  <li>
    <p>we just need to provide <mark>one configuration</mark>. It does not need to hold for all configurations of the $d=3$ Points.</p>
  </li>
  <li>
    <p>for $d=4$, no configuration is achievable. This you can show using the convex hull of four points. To show this, consider <em>ANY</em> configuration of four points, then you can either have:</p>

    <ul>
      <li>
        <p>four points form <strong>four corners</strong> (their convex hull) -&gt; the case of XOR is not separable</p>
      </li>
      <li>
        <p>four points form <strong>three corners</strong> (their convex hull) -&gt; the case below  I can just flip the sign of the point in the middle and it is not separable</p>

        <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118172939664.png" alt="image-20211118172939664" style="zoom:67%;" /></p>

        <p>(notice that <mark>linear separability</mark> = classify data into <mark>convex sets</mark>)</p>
      </li>
      <li>
        <p>two corners -&gt; more than three points being colliear</p>
      </li>
    </ul>

    <p>note that if three points are collinear (on the same line), it is obviously not separable.</p>
  </li>
</ul>

<hr />

<p><em>For Example:</em> $\mathcal{F}$ = square (not rotatable) classifier in $\mathbb{R}^2$</p>

<p>It turns out that the VC dimension is $d=4$. So</p>

\[\text{VC}(\mathcal{F})=4\]

<p>How to prove that for <em>any configuration of $d=5$</em> we cannot shatter it? Consider any configuration (including labels) of five points:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118165811083.png" alt="image-20211118165811083" style="zoom: 67%;" /></p>

<p>Basically:</p>

<ul>
  <li>each edge of the rectanagle will be determined by at least 1 point</li>
  <li>then I have one point left. By construction it is either inside or on the line of the rectangle.</li>
  <li>then, I can always find a label for that point, such that the 5 points is not shatter-able.</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>Often (but not always!) VC dimension is proportional to the degrees-of-freedom or the number of independent parameters in. (i.e. you can find a $\mathcal{F}$ that has a finite number of parameters but the dimension is infinite.)
      <ul>
        <li>in fact, for linear classifier, VC dimension is your space’s dimension + 1</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Now you can compute PAC-learnability</p>

<blockquote>
  <p><strong>Theroem (Vapnik-Chervonenkis):</strong></p>

  <p>Let us have some sampled data $(x_1,y_1), …, (x_m,y_m)$ of size $m$ <strong>IID</strong> from some <em>unknown $\mathcal{D}$</em>. Pick any tolerance level $\epsilon &gt; 0$ and confidence level $\delta &gt; 0$, we can let:</p>

\[m \ge C \cdot \frac{\text{VC}(\mathcal{F})\ln(1 / \delta)}{\epsilon^2}\]

  <p>for some constant $C$, then with <strong>probability at least $1-\delta$</strong>:</p>

\[\mathrm{err}(f_m^{\text{ERM}}) - \mathrm{err}(f^*) \le \epsilon\]

  <p>Basically $\mathcal{F}$ is <strong>efficiently PAC-Learnable</strong>.</p>
</blockquote>

<p>The power of VC dimension also lies in being able to answer the question:</p>

<ul>
  <li>what if I give data $m &lt; \frac{\text{VC}(\mathcal{F})\ln(1 / \delta)}{\epsilon^2}$? Is it possible to get away and still get reasonably good result?</li>
</ul>

<p>In fact, the VC dimension can also show the <strong>low bound on the number of samples</strong>:</p>

<blockquote>
  <p><strong>Theorem</strong></p>

  <p>Let $\mathcal{A}$ be any model select algorithm that given $m$ samples, returns a model from $\mathcal{F}$, i.e. $A :(x_i, y_i)_{i=1}^m \to f_m^A$.</p>

  <p>Then, for all tolerance levels $0 &lt; \epsilon &lt; 1$, and all confidence levle $0 &lt; \delta &lt; 1/4$, there <mark>exists a distribution</mark> $\mathcal{D}$ for your data such that, if:</p>

\[m \le C \frac{\text{VC}(\mathcal{F})}{\epsilon^2}\]

  <p>My algorithm will return often give bad models:</p>

\[P_{(x_i, y_i)}\left[ \mathrm{err}(f_M^{\mathrm{ERM}}) - \mathrm{err}(f^*) &gt; \epsilon \right] &gt; \delta\]

  <p>Basically:</p>

  <ul>
    <li>if you give me less data than the bound, then, <strong>for more than $\delta$ amount of the time</strong>, your model can be arbitrarily bad in generalization error (for all $\epsilon$)</li>
    <li>note that this fails for <mark>some distribution of data $\mathcal{D}$</mark>. So you <em>might</em> be fine if your dataset is not that distribution $\mathcal{D}$.</li>
  </ul>
</blockquote>

<p>Notice that:</p>

<ul>
  <li>VC dimension of a model class fully characterizes its learning ability!</li>
  <li>Results are agnostic to the underlying distribution (it fails for <em>some distribution</em>, but maybe not for all distribution)</li>
</ul>

<h2 id="no-free-lunch">No Free Lunch</h2>

<p>ERM Algorithm + PAC Learning sounds very powerful, such that you can “solve any question”. However, be careful the PAC-Learnable is comparing against $f^* =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$, which <mark>could be itself pretty bad</mark>.</p>

<blockquote>
  <p><strong>Thereon</strong> (no free lunch)</p>

  <p>Pick any sample size $m$, any algorithm $\mathcal{A}$ and any $\epsilon &gt; 0$. There exists a distribution $\mathcal{D}$ such that:</p>

\[\mathrm{err}(f^A_m) &gt; (1/2) - \epsilon\]

  <p>while Bayes optimal error is (basically perfect):</p>

\[\min_f\mathrm{err}(f) = 0\]

  <p>basically whatever your algorithm returns (e.g. ERM algorithm), I can <strong>always find a distirbution of data</strong> such that your algorithm returned garbage/close to random guessing.</p>

  <p>Note that there is <mark>no conflict with PAC theorem</mark> because:</p>

  <ul>
    <li>you can pick some $\mathcal{D}$ such that the entire $\mathcal{F}$ is bad, i.e. your $f^* =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$ is bad such that $\mathrm{err}(f^*)\approx 1/2$. However, notice that $f$ which is the optimal bayes can be good and that it could be $f \notin \mathcal{F}$. So there is no conflict.</li>
  </ul>
</blockquote>

<h1 id="unsupervised-learning">Unsupervised Learning</h1>

<p>Now consider the problem of:</p>

<ul>
  <li>Data: $\vec{x}_1, \vec{x}_2, …\in \mathcal{X}$
    <ul>
      <li>basically we have no labels</li>
    </ul>
  </li>
  <li>Assumption: there is an underlying structure in</li>
  <li><strong>Learning task</strong>: discover the <strong>structure</strong> given n examples from the data
    <ul>
      <li>(e.g. clustering: partition the data into meaningful structures)</li>
      <li>(e.g. dimensionality reduction: find a low-dimensional representation to suppress noise)</li>
    </ul>
  </li>
  <li>Goal: come up with the summary of the data using the discovered structure</li>
</ul>

<hr />

<p><em>For Example</em>: Handwritten digits revisited</p>

<p>Consider the digits but you <em>don’t have the labels</em></p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118155222305.png" alt="image-20211118155222305" style="zoom:50%;" /></p>

<p>since you have no idea what labels those input have, what you can do is that:</p>

<ol>
  <li>
    <p>perform dimensionality reduction to 2-D. (we haven’t talked about how to do that yet)</p>

    <p>Suppose you did that and the output looks like:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118155452135.png" alt="image-20211118155452135" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>We see that there are some clusters. How do we mathematically/quantitiatvely cluster them?</p>

    <p>This data exploration step is pretty useful, as maybe your NN gives only 99% accuracy, and the reason behind is that ther is a smooth transition for your digit <code class="language-plaintext highlighter-rouge">4-&gt;9</code>, i.e. there appears no clear boundary.</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211118155740270.png" alt="image-20211118155740270" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>Now you can perhaps either do some clustering, or you this knowledge to do some other stuff</p>
  </li>
</ol>

<h2 id="clustering">Clustering</h2>

<p>Suppose you know in advance that we have $k$ groups in our data $\vec{x}_1, …, \vec{x}_n$.</p>

<blockquote>
  <p>Our <mark>goal</mark> is to partition our data into $k$ groups such that there is some “common features” shared</p>

  <ul>
    <li>the definition of “common feature” <strong>decides what clustering algorithm</strong> we will use</li>
    <li>in some literature, this clustering is also called “unsupervised classification” or “quantization”</li>
  </ul>
</blockquote>

<p>Here, we split the task into two parts:</p>

<ol>
  <li>given a $k$, find the optimal partition</li>
  <li>find a $k$</li>
</ol>

<h3 id="k-means">$k$-means</h3>

<p>Let there be $\vec{c}_1, …, \vec{c}_k$ being a set of <mark>representative</mark> points such that data $\vec{x}_1, …, \vec{x}_n$ is close to some representative.</p>

<blockquote>
  <p><strong>Heuristics</strong>:</p>

  <p>Then our goal here is to find the $k$ points such that the <strong>total</strong> distance between the data point <em>in the group</em> to the representative point is minimized.</p>

  <p>For instance, we want the result to look like this:</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123145559010.png" alt="image-20211123145559010" /></p>

</blockquote>

<p>Therefore, essentially we want to <mark>optimize the following</mark>:</p>

\[\min_{c_1, ..., c_k} \left[ \sum_{i=1}^n \min_{j=1,...,k}||\vec{x}_i - \vec{c}_j||^2 \right]\]

<p>where:</p>

<ol>
  <li>the part $\min_{j=1,…,k}\vert \vert \vec{x}_i - \vec{c}_j\vert \vert ^2$ says: for every data point in the dataset, find me its <strong>closest representative point</strong></li>
  <li>take the “cost” to be the distance of that data point to the representative</li>
  <li>compute for each data point and sum over all of them, for some given $c_1, …, c_k$</li>
  <li>minimize over the choice of $c_1, …, c_k$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>this <mark>will</mark> give you $k$ <mark>distinct</mark> representative points if $n &gt; k$ and we have $n$ distinct data points.</p>
    </li>
    <li>
      <p>we cannot use gradient descent for $c_i$ because there is a $\min$ term inside that derivatives can’t deal with</p>
    </li>
    <li>
      <p>this problem is <strong>NP hard</strong>, so there are no polynomial time algorithm for this if $d\ge 2$</p>

      <ul>
        <li>
          <p>for $d=1$ there is a dynamic programming algorithm that can do this fast</p>
        </li>
        <li>
          <p>for $k=1$, this can also be done fast because the optimization basically becomes $\vec{c}_1 = \vec{\mu}$</p>

\[\min_{c_1} \left[ \sum_{i=1}^n ||\vec{x}_i - \vec{c}_j||^2 \right]\]

          <p>notice that the $\min$ inside disappeared because we just have one representative.</p>
        </li>
        <li>
          <p>but we do have some good <strong>approximation algorithms</strong> for $k\ge 2, d\ge 2$ .</p>
        </li>
      </ul>
    </li>
    <li>
      <p>K-means will always produce <mark>convex clusters</mark> (due to the defined objective above), thus it can <mark>only work if clusters can be linearly separated</mark>.</p>

      <p>E.g.</p>

      <table>
        <thead>
          <tr>
            <th style="text-align: center">Given Data</th>
            <th style="text-align: center">K-means Output</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="text-align: center"><img src="https://i.stack.imgur.com/fP5Zz.png" alt="Matlab kmeans clustering for non linearly separable data - Stack Overflow" style="zoom:50%;" /></td>
            <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211203161033264.png" alt="image-20211203161033264" style="zoom:50%;" /></td>
          </tr>
        </tbody>
      </table>

      <p>where the $c_1, c_2, c_3$ will be the three crosses on the right figure</p>
    </li>
  </ul>
</blockquote>

<h4 id="lloyds-method">Lloyd’s method</h4>

<blockquote>
  <p><strong>Heuristics for Approximation</strong></p>

  <ol>
    <li>Suppose we are given some center/representative, then we also know the partition/groups</li>
    <li>once we know the partition, we can compute a <strong>center within the partition</strong> (becomes a $k=1$ problem locally)
      <ul>
        <li>this can be done fast</li>
      </ul>
    </li>
    <li>repeat this process to improve the mean/center (see below)</li>
  </ol>
</blockquote>

<p><strong>Algorithm (K-means Algorithm):</strong></p>

<ol>
  <li>Initialize cluster centers (say randomly)</li>
  <li>Repeat till no more changes occur
    <ol>
      <li>Assign data to its closest center, i.e. create a partition (assume centers are fixed)</li>
      <li>Find the optimal centers  (assuming the data partition is fixed)</li>
    </ol>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Step 1-2.1</th>
      <th style="text-align: center">Step 2.2</th>
      <th style="text-align: center">Step 2.1 (Repeat)</th>
      <th style="text-align: center">Step 2.2 (Repeat)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123170507229.png" alt="image-20211123170507229" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123170533583.png" alt="image-20211123170533583" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123170549611.png" alt="image-20211123170549611" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123170603370.png" alt="image-20211123170603370" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>this Lloyds’ method is so popular that it is now used <strong>synonymously known as the k-means algorithm</strong></p>
    </li>
    <li>
      <p>however, since it is an approximation, there are cases when this <strong>doesn’t work</strong>.</p>

      <ul>
        <li>
          <p>i.e. the approximation <mark>can</mark> be arbitrarily bad, such that the cost difference against the optimal solution basically can be made arbitrarily bad. The <strong>cost</strong> function would be</p>

\[\text{cost}(\{c_j\}) = \sum_{i=1}^n \min_{j=1,...,k}||\vec{x}_i - \vec{c}_j||^2\]

          <p>so that if we take the output of Lloyd $C_{\mathrm{lloyd}}={c_j \text{ from lloyd’s algorithm}}$ and compare it with the optimal $C_{\mathrm{optimal}}$, then we get:</p>

\[\frac{\text{cost}(C_{\mathrm{lloyd}})}{\text{cost}(C_{\mathrm{opt}})} \text{ is unbounded}\]
        </li>
      </ul>
    </li>
    <li>
      <p>Performance quality heavily dependent on the initialization!</p>
    </li>
    <li>
      <p>The whole purpose of $\vec{c}_j$ was <mark>only</mark> to <strong>compute the new partition for $\vec{x}_i$</strong>, which is</p>

\[\min_{c_1,...,c_j} \left[ \sum_{i=1}^n ||\vec{x}_i - \vec{c}_j||^2 \right]\]

      <p>so if we can do <mark>this step using dot products</mark>, we can <mark>kernelize this</mark> (see HW4 last question)</p>
    </li>
  </ul>
</blockquote>

<h3 id="hierachical-clustering">Hierachical Clustering</h3>

<p>Now, the <strong>second</strong> problem is how should be <strong>decide what $k$ is</strong>?</p>

<blockquote>
  <p><strong>Solution</strong>: You don’t, since you might have no idea how many clusters there are. Then you basically can <mark>encode clustering for all values of $k$</mark>, and the user choose later which one works the best!</p>

  <ul>
    <li>this gives hierarchical clustering</li>
  </ul>
</blockquote>

<hr />

<p>First, let us consider an example of <em>output of hierarchical clustering</em>.</p>

<p>Let us have many animal data, and each animal has some feature representing their genetics.</p>

<p>Then the <strong>output</strong> will look like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Output</th>
      <th style="text-align: center">Your Choice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123171620702.png" alt="image-20211123171620702" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123152921985.png" alt="image-20211123152921985" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where basically <strong>we can choose</strong> where to cut it:</p>

<ul>
  <li>
    <p>then $k$ is the numebr of branches that resulted from our cut</p>
  </li>
  <li>
    <p>the length of the branch is telling you the <strong>amount of “effort” needed to merge two groups</strong>.</p>

    <p>For example, suppose I want to merge group purple and green:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123171903610.png" alt="image-20211123171903610" style="zoom:50%;" /></p>

    <p>then the cost to do that merge is $35$ of the yellow highlighted part. Another way to see this is consider how many cuts you can make to distinguish the two groups:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211123172228043.png" alt="image-20211123172228043" style="zoom:50%;" /></p>

    <p>Therefore, the idea of $35$ being the cost of merging makes sense because:</p>

    <ul>
      <li>
        <p>if I lifted $35$, then any cut in the yellow area will now give me purple and green as the same group</p>
      </li>
      <li>
        <p>this also says. the longer the branch is, the more <mark>"saliently different"</mark> it is from other group as you can <mark>cut many times and still result in different branches</mark></p>
      </li>
    </ul>
  </li>
</ul>

<hr />

<p>In general, there are <strong>two approaches</strong> to do the above diagram</p>

<p><strong>Algorithm: Top Down (divisive):</strong></p>

<ol>
  <li>Partition data into groups (say, by k-means, with $k=2$)
    <ul>
      <li>technically $k$ here is customizable</li>
    </ul>
  </li>
  <li>Recurse on each part</li>
  <li>Stop when cannot partition data anymore (ie <strong>single points left</strong>)</li>
</ol>

<p><strong>Algorithm: Bottom Up (agglomerative)</strong></p>

<ol>
  <li>
    <p>Start by each data sample as its own cluster (so initial number of clusters is n)</p>
  </li>
  <li>
    <p>Repeatedly merge “closest” pair of clusters</p>

    <ul>
      <li>
        <p>e.g. distiance between two cluster could be defined as “sum of distances between each pair of points in the two clusters”</p>
      </li>
      <li>
        <p>in specific, the common metrics here are: single linkage, complete linkage, and average linkage</p>

        <ul>
          <li>
            <p><strong>single linkage</strong>: take the distance between <em>two closest point</em> in the two cluster:</p>

\[\text{dist}(c_1, c_2) = \min_{x_i \in c_1, x_j \in c_2} ||x_i - x_j||^2\]
          </li>
        </ul>
      </li>
      <li>
        <p><strong>complete linkage</strong>: take the distance between <em>two farthest point</em> in the two cluster</p>
      </li>
      <li>
        <p><strong>average linkage</strong>: averaging over all pair distances, so basically our space becomes a cartesian product</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Stop when only one cluster is left</p>

    <ul>
      <li>so basically $n$ iterations, since each iteration reduces $1$ cluster</li>
      <li>(basically drawing the tree from bottom up)</li>
    </ul>
  </li>
</ol>

<h3 id="clustering-via-probabilities-mixture-modelling">Clustering via Probabilities Mixture Modelling</h3>

<p>This is another clustering algorithm that is also popular, but it is a generative model.</p>

<blockquote>
  <p>Basically the same problem but considers how data are <strong>generated</strong>.</p>
</blockquote>

<p>Given that we have $\vec{x}_1, …, \vec{x}_n \in \mathbb{R}^d$ being the data and the number of intended clusters is $k$.</p>

<p>We first <strong>assume</strong> there is a joint distribution $(X,C)$ over the space $\mathbb{R}^d \times [k]$ for $[k] = {1,2,…k}$.</p>

<ul>
  <li>basically <strong>imagine every possible cluster for every data point</strong>, and it has a certain probability of being optimal</li>
  <li>e.g. $(x_{20}, 3)$ could happen with some probability</li>
</ul>

<p>But remember here we don’t have the “label”, i.e. we don’t know $C$. So here we <strong>assume the generation process</strong> is:</p>

<ol>
  <li>
    <p>Basically let the probability of being in cluster $i$ be $\pi_i$, then let the <strong>cluster be distributed by</strong></p>

\[C \sim \begin{bmatrix}
\pi_1\\
\vdots\\
\pi_k
\end{bmatrix}\]

    <p>where $\pi_i$ is the probability to get cluster $i$.</p>
  </li>
  <li>
    <p>To generate a <strong>single data point $x_i$</strong>, row the “dice $C$ which has $k$ faces”, and whatever it outputs will be the <mark>cluster that $x_i$ belongs to</mark> (i.e. we first <strong>generate the cluster</strong>)</p>

    <ul>
      <li>in the end, after we generated cluster information for all data, this is basically a <strong>multinomial distribution for $C$</strong>, so you basically have $P(C_1=c_1, …, C_k=c_k)$ for $c_i$ being the number of data belonging to the $i$-th cluster.</li>
    </ul>

\[P(C_1=c_1, ..., C_k=c_k) = \frac{n!}{c_1!...c_k!}\pi_1^{c_1}...\pi_k^{c_k}\]

    <p>e.g. a trinomial with $\pi_1=0.45, \pi_2=0.25, \pi_3=0.3$ with 20 rolls $n=20$) looks like:
 <img src="https://miro.medium.com/max/892/1*iIN4QSMr7wMlNXyBKSetjA.png" alt="" /></p>
  </li>
  <li>
    <p>Then, once you determined that the data is inside cluster $i$, you <strong>generate the data point</strong></p>

\[X|(C=i) \sim \mathcal{N} (\vec{\mu}_i, \Sigma_i )\]

    <p>being a multivariate normal.</p>

    <ul>
      <li>i.e. basically now we zoom into each cluster individually. This is very similar to the MLE for classification where we modelled $P(X\vert Y=y_i)$. However, here you don’t have $y_i$ beforehand, so you don’t know $P(y_i)$</li>
    </ul>
  </li>
</ol>

<p>Therefore, this general <strong>parameters</strong> to estimate are:</p>

\[\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, ..., \pi_k, \vec{\mu}_k, \Sigma_k)\]

<p>Then we <mark>want to do MLE</mark>, but we don’t have data about $C$.</p>

<ul>
  <li>modelling assumption is $(X,C)=(x_1, c_1),…,(x_n, c_n)$ drawn IID from $\mathbb{R}\times [k]$,</li>
  <li>but we don’t have the “label” information! So need to <mark>model</mark> the joint <mark>only with $x_1, ..., x_n$</mark>.
    <ul>
      <li>this is also called <strong>latent variable modelling</strong> where variable $c_i$ is latent/hidden from us</li>
      <li>if data is complete, we just do MLE</li>
    </ul>
  </li>
</ul>

<p>The solution to this is to do a <strong>Gaussian Mixture Model</strong></p>

<h4 id="gaussian-mixture-modelling-gmm">Gaussian Mixture Modelling (GMM)</h4>

<p>Given that we have $\vec{x}_1, …, \vec{x}_n \in \mathbb{R}^d$ being the data and the number of intended clusters is $k$.</p>

<p>We first <strong>assume</strong> there is a joint distribution $(X,C)$ over the space $\mathbb{R}^d \times [k]$ for $[k] = {1,2,…k}$.</p>

<p>Our assumption on distributions are as follows:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130164011283.png" alt="image-20211130164011283" /></p>

<p>where:</p>

<ul>
  <li>the reason why it is called a Gaussian Model is clear, because $X\vert C=i$ is a Gaussian.</li>
  <li>we will show what the mixture part is about</li>
</ul>

<p>Then, consider $P[\vec{x}\vert \theta]$ is the probability of us getting some particular $\vec{x}$ data point</p>

\[\begin{align*}
P[\vec{x}|\theta]
&amp;= \sum_{i=1}^k P[\vec{x},c_i|\theta]\\
&amp;=  \sum_{i=1}^k P[c_i|\theta]P[\vec{x}|c_i, \theta]\\
&amp;= \sum_{i=1}^k \pi_i \frac{1}{\sqrt{(2\pi)^{d} \det(\Sigma_i)}} \exp\left( -\frac{1}{2}(\vec{x}-\vec{\mu}_i)^T \Sigma^{-1}_i(\vec{x}-\vec{\mu}_i) \right)
\end{align*}\]

<p>where:</p>

<ul>
  <li>the first step is just using Law of Total Probability</li>
  <li>
    <p>the last step basically is because by construction $P[c_i\vert \theta]=\pi_i$ and $P[\vec{x}\vert c_i,\theta]=N(\vec{x};\mu_i, \Sigma_i)$</p>
  </li>
  <li>now the <strong>Gaussian Mixture Model</strong> is clear because the probability is a <mark>sum/mixture of many gaussians</mark>.</li>
</ul>

<p>Graphically:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130164903632.png" alt="image-20211130164903632" /></p>

<p>where:</p>

<ul>
  <li>always remember that <mark>we don't have $C$ information</mark>, therefore our data is still in $\mathbb{R}^2$</li>
  <li>the model $P[\vec{x}\vert \theta]$ for each data point contains a <strong>weighted contribution from each of the three Gaussian mountain</strong></li>
</ul>

<h4 id="learning-gmm-parameters---em-algorithm">Learning GMM Parameters - EM Algorithm</h4>

<p>Now we have for each data a probability given by $P[\vec{x}\vert \theta]$, we simply want to find the best $\theta$.</p>

<p><strong>Approach 1: MLE</strong></p>

\[\begin{align*}
\theta_{\mathrm{MLE}}
&amp;= \arg\max_\theta \sum_{i=1}^n \ln P[\vec{x}|\theta]\\
&amp;= \arg\max_\theta \sum_{i=1}^n \ln \left[  \sum_{i=1}^k \pi_i \frac{1}{\sqrt{(2\pi)^{d} \det(\Sigma_i)}} \exp\left( -\frac{1}{2}(\vec{x}-\vec{\mu}_i)^T \Sigma^{-1}_i(\vec{x}-\vec{\mu}_i) \right) \right]
\end{align*}\]

<p>where basically since the data is IDD:</p>

<ul>
  <li>the first step comes from taking the log likelihood to convert $\Pi \to \sum$.</li>
  <li>but we <mark>cannot simplify further</mark> because $\ln \sum …$ is a pain to deal with.
    <ul>
      <li>also note that this is <strong>not a convex optimization problem</strong> as this function is not convex</li>
    </ul>
  </li>
  <li>the bigger problem with MLE directly is that the estimate for $\theta$ will <mark>yield degenerate $\theta$</mark></li>
</ul>

<p>This basically does not work due to the fact that we are summing up the probability for each data point, and we can configure the Gaussian to be such that the following happens</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">What we want</th>
      <th style="text-align: center">MLE Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130171116968.png" alt="image-20211130171116968" style="zoom: 80%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130171127563.png" alt="image-20211130171127563" style="zoom: 80%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>the MLE output is not what we wanted! (is <strong>degenerate</strong>)</li>
  <li>recall that for our MLE with fitting <strong>one Gaussian</strong>, it <strong>works</strong> because all data will be part of that Gaussian (there is no escape). So if we fit a narrow Gaussian, data points slightly off center will have a small prob. But now, here we fit two Gaussian. Then the optimal will be:
    <ol>
      <li>fit one Gaussian to all the data in the MLE manner. So <em>all data has some non-minuscule probability</em></li>
      <li>fit the other Gaussian to a <em>single data point</em>, which can be configured to <em>shoot up as high as possible</em>
        <ul>
          <li>basically this Gaussian can be tweaked such that our $P[X\vert \theta]$ will be large.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>In fact, MLE of <strong>any</strong> mixture models will have this problem.</li>
  </ul>
</blockquote>

<hr />

<p><strong>Approach 2: <mark>EM Algorithm</mark></strong></p>

<p>The idea is as follows. Even though the MLE which gives a <strong>global maximum</strong> is not desired, <strong>local maximum</strong> seems to be desirable:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Start</th>
      <th style="text-align: center">Local Maximum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130172638578.png" alt="image-20211130172638578" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130172650913.png" alt="image-20211130172650913" /></td>
    </tr>
  </tbody>
</table>

<p>where we reached a <strong>local</strong> maximum because shifting the Gaussian (e.g. using gradient ascent) any further reduces the probability.</p>

<p>Exactly how it works is as follows (pseudo):</p>

<ol>
  <li>
    <p>Initialize the parameters arbitrarily</p>
  </li>
  <li>
    <p>Given the current setting of parameters find the <strong>best (soft) assignment</strong> of data samples to the clusters (<mark>Expectation-step</mark>)</p>

    <ul>
      <li>
        <p>this <em>soft partition</em> basically means, we assign <strong>proportion</strong> of the data to <strong>each Gaussian mountain</strong></p>

        <p>e.g. if we have four mountains represented by <code class="language-plaintext highlighter-rouge">X</code> in the below figure, each has a $P_i$ estimate of the data $\vec{x}$ in the circle,</p>

        <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130173706229.png" alt="image-20211130173706229" style="zoom:50%;" /></p>

        <p>then, e.g. mountain 3 will get the proportion</p>

\[\frac{P_1}{P_1+P_2+P_3+P_4}\]
      </li>
      <li>
        <p>in the Lloyd’s Algorithm for the K-means, the idea is similar in that we assign the data point <strong>completely (hard)</strong> to the closest cluster (closest to the mean)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Update all the parameters $\theta$ with respect to the current (soft) assignment that maximizes the likelihood (<mark>Maximization-step</mark>)</p>

    <ul>
      <li>same idea as Lloyd’s algorithm</li>
      <li>this is now basically <mark>MLE</mark></li>
    </ul>
  </li>
  <li>
    <p>Repeat until no more progress is made.</p>
  </li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>This algorithm works in general for <strong>any “missing data” model</strong>, such that if your data generation needs data $x_1, …,x_m$, with but you only have $x_1, …, x_l$ for $l &lt; m$, then you can apply this Expectation-Maximization Algorithm to:</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130174701638.png" alt="image-20211130174701638" style="zoom:50%;" /></p>

  <ol>
    <li>fix a random $\theta$ setting</li>
    <li>generate/fill in the missing information $x_3,…, x_m$ based on $\theta$</li>
    <li>adjust $\theta$ since now you have complete information</li>
    <li>repeat</li>
  </ol>
</blockquote>

<p>More detailed step for EM Algorithm:</p>

<ol>
  <li>
    <p>Initialize $\theta = (\pi_1, \vec{\mu}_1, \Sigma_1, …, \pi_k, \vec{\mu}_k, \Sigma_k)$ arbitrarily</p>
  </li>
  <li>
    <p><strong>Expectation-Step</strong>: for each $i \in {1,…,n}$ data and $j \in {1,…,k}$ cluster, compute the assignment/weight $w_j^{(i)}$ of the data $x_i$ to cluster $j$ as:</p>

\[\begin{align*}
w_j^{(i)}
&amp;= \frac{P[\vec{x}_i ,c_j|\theta]}{\sum_{j'=1}^kP[\vec{x}_i ,c_{j'}|\theta]}\\
&amp;= \frac{P[c_j|\theta]P[\vec{x}_i | c_j, \theta]}{\sum_{j'=1}^kP[c_{j'}|\theta]P[\vec{x}_i | c_{j'}, \theta]}\\
&amp;= \frac{\pi_j \sqrt{\det(\Sigma_j^{-1})} \exp\left( -\frac{1}{2}(\vec{x}-\vec{\mu}_j)^T \Sigma^{-1}_j(\vec{x}-\vec{\mu}_j) \right)}{\sum_{j'=1}^k\pi_{j'} \sqrt{\det(\Sigma_{j'}^{-1})} \exp\left( -\frac{1}{2}(\vec{x}-\vec{\mu}_{j'})^T \Sigma^{-1}_{j'}(\vec{x}-\vec{\mu}_{j'}) \right)}
\end{align*}\]

    <p>which is basically weight to each mountain</p>
  </li>
  <li>
    <p><strong>Maximization-Step</strong>: maximize the log-likelihood of the parameters</p>

    <ul>
      <li>
        <p>First, for the $j$-th class, we have $n_j$ <strong>effective data points</strong> (since each data point gives a portion) assigned to it</p>

\[n_j = \sum_{i=1}^n w_j^{(i)}\]
      </li>
      <li>
        <p>then, the optimal parameters are basically</p>

\[\begin{align*}
\pi_j &amp;= \frac{n_j}{n}\\
\vec{\mu}_j &amp;= \frac{1}{n_j} \sum_{i=1}^n  w_j^{(i)} \vec{x}_i\\
\Sigma_j &amp;= \frac{1}{n_j}\sum_{i=1}^n  w_j^{(i)} (\vec{x}_i - \vec{\mu}_j)(\vec{x}_i - \vec{\mu}_j)^T
\end{align*}\]

        <p>(<strong>derivation</strong> of those formula are basically done by maximizing:</p>

\[P[X|\theta] \to \sum_{i=1}^n \ln \left[  \sum_{i=1}^k \pi_i \frac{1}{\sqrt{(2\pi)^{d} \det(\Sigma_i)}} \exp\left( -\frac{1}{2}(\vec{x}-\vec{\mu}_i)^T \Sigma^{-1}_i(\vec{x}-\vec{\mu}_i) \right) \right]\]

        <p>e.g. to find out $\vec{\mu}_j$, take the above derivative w.r.t. $\vec{\mu}_j$ and solve by setting it to $\vec{0}$)</p>
      </li>
      <li>
        <p>Basically, this maximazation step is doing MLE. If this cannot be solved in closed form, the you just do gradient descent/ascent here.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Repeat until no improvement</p>
  </li>
</ol>

<p><em>Example: Running EM</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Arbitrary Initialization</th>
      <th style="text-align: center">E-Step, Soft Assignment</th>
      <th style="text-align: center">M-Step, Optimize</th>
      <th style="text-align: center">After 5 Rounds</th>
      <th style="text-align: center">After 20 rounds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130181706424.png" alt="image-20211130181706424" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130181720784.png" alt="image-20211130181720784" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130181750457.png" alt="image-20211130181750457" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130181919338.png" alt="image-20211130181919338" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211130181928926.png" alt="image-20211130181928926" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>EM algorithm will always terminate</p>
    </li>
    <li>
      <p>However, this can be arbitrarily bad as well (e.g. if I initilaize the Gaussian near the degenerate solutions, then the maximization step is basically worsening the degenerate solution)</p>
    </li>
    <li>
      <p>However, it is useful <strong>in some cases</strong> (i.e. some specific configuation of the data and some specific initialization), such that the output of EM algorith will be <strong>approximation of</strong>:</p>

\[\min_{c_1, ..., c_k} \left[ \sum_{i=1}^n \min_{j=1,...,k}||\vec{x}_i - \vec{c}_j||^2 \right]\]

      <p>which is good.</p>
    </li>
  </ul>
</blockquote>

<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>

<p>Again, consider some given data, which perhaps has $\mathbb{R}^d$ dimension:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211202152417187.png" alt="image-20211202152417187" style="zoom:50%;" /></p>

<p>e.g. each data sample has $27\times 27$ features, we want to be able to <strong>reduce the dimension</strong> such that each data point <em>looks like a 2-D data point</em>:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211202152535488.png" alt="image-20211202152535488" style="zoom: 50%;" /></p>

<p>where this is useful as:</p>

<ul>
  <li>it is <strong>suppressing the “noise” (hopefully)</strong> of our data - speeds of computation and perhaps performance
    <ul>
      <li>there is no guarantee that what we remove is useful information or not. So you will lose some data (obviously), but hopefully they are the useless ones</li>
    </ul>
  </li>
  <li>
    <p>in this case, we want to do dimensionality reduction:</p>

\[f:\mathbb{R}^{784} \to \mathbb{R}^2\]

    <p>which basically is doing a <mark>mapping from high dimension to low dimension</mark></p>
  </li>
</ul>

<blockquote>
  <p>Some previously seen useful transformations:</p>

  <ul>
    <li>z-scoring</li>
    <li>kernel transformation (higher dimension)</li>
  </ul>

  <p>Unseen:</p>

  <ul>
    <li>t-SNE (nonlinear dimensional reduction), used for the digit data above.</li>
  </ul>
</blockquote>

<h3 id="principal-components-analysis-pca">Principal Components Analysis (PCA)</h3>

<p><strong>Data</strong>: given data $\vec{x}_1,…, \vec{x}_n \in \mathbb{R}^d$</p>

<p><strong>Goal</strong>: find the best <mark>linear</mark> transformation $\phi : \mathbb{R}^d \to \mathbb{R}^k, k &lt; d$ that best maintains reconstruction accuracy.</p>

<ul>
  <li>basically it means minimizing the discrepancy if I <em>recreate the data</em></li>
  <li>equivalently, we want to <mark>minimze aggregate residual error</mark> (so we need to define this objective)</li>
</ul>

<blockquote>
  <p><em>Recall: Linear Mapping</em></p>

  <p>A linear mapping $\phi$ simply means:</p>

\[\phi(c\vec{x}+d\vec{y}) = cf(\vec{x}) + df(\vec{y})\]

  <p>so basically $\phi$ is a mapping but it is linear.</p>

  <ul>
    <li>e.g. in the case of $\phi:\mathbb{R}^{784} \to \mathbb{R}^2$ for the digits, a <mark>linear mapping then it means $\phi(\vec{x})=M\vec{x}$ for $M \in \mathbb{R}^{2 \times 784}$</mark>. So we are just dealing with <strong>transformation matrices</strong>!</li>
    <li>One property of linear transformation is, that if $\vec{x}_j$ is close to $\vec{x}_i$, then in the transformed space, $\phi(\vec{x}_j)$ is still close to $\phi(\vec{x}_i)$. i.e. Neighboorhood  will be “preserved” in lower dimension.</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Heuristics</strong></p>

  <p>Consider that we are given some blue data points, e.g. in 3D. We know that given any plane, the <strong>minimum error you can get is to do orthogonal projection</strong>. Now, the question is <mark>which plane gives the smallest "error"</mark> (if on each we do orthogonal projection):</p>

  <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211202223740693.png" alt="image-20211202223740693" style="zoom:80%;" /></p>

  <p>so essentially we want to minimize the sum of the <strong>blue distances</strong> to find the <mark>best plane for projection</mark>.</p>

  <ul>
    <li>so notice that now we are <strong>looking at the orthogonal distance</strong> (as compared to the regression case)</li>
  </ul>
</blockquote>

<p>Therefore, in order we define our <mark>objective</mark> as follows. Let us find $\Pi^k:\mathbb{R}^d \to \mathbb{R}^d$ (instead of directly $\phi : \mathbb{R}^d \to \mathbb{R}^k, k &lt; d$ because we need to do subtraction with $\mathbb{R}^d$), such that:</p>

\[\min_{\Pi^k} \frac{1}{n} \sum_{i=1}^n || \vec{x}_i - \Pi_k (\vec{x}_i) ||^2\]

<p>so basically this is to <mark>minimize the height residuals</mark>.</p>

<ul>
  <li>note that we use $\Pi^k:\mathbb{R}^d \to \mathbb{R}^d$, which <strong>projects</strong> blue points into red points but in the <mark>same dimensional representation of $\mathbb{R}^{d}$</mark>. This is also called a <strong>k-dimensional orthogonal linear projector</strong>
    <ul>
      <li>you will see that eventually this $\Pi^k$ will contain our transformation matrix $\phi$ mentioned above</li>
    </ul>
  </li>
  <li>essentially we want to find the representation of the best plane in $\mathbb{R}^d$</li>
</ul>

<p>Now, our task is to <strong>find $\Pi^k$</strong> for now. We know that:</p>

<ul>
  <li>A $k$ dimensional <mark>subspace</mark> can be represented by any $k$ set of <strong>orthonormal</strong> vectors <mark>$\vec{q}_1, ..., \vec{q}_k \in \mathbb{R}^d$</mark> (basically I remove some unit vectors so I am in $\mathbb{R}^d$ but a subspace)</li>
  <li>then <strong>projection</strong> of <mark>any $\vec{x}\in \mathbb{R}^d$</mark> into that <strong>subspace</strong> is just <mark>projecting into the $\text{span}\{\vec{q}_1, ..., \vec{q}_k\}$</mark></li>
</ul>

<p>Therefore, such a projection into the <strong>$k$ dimensional subspace of $d$ dimension</strong> is:</p>

\[\sum_{i=1}^k (\vec{q}_i \cdot \vec{x})\vec{q}_i = \left( \sum_{i=1}^k \vec{q}_i\vec{q}_i^T\right)\vec{x} = \Pi^k\vec{x}\]

<p>where basically we defined:</p>

\[\Pi_k\equiv  \sum_{i=1}^k \vec{q}_i\vec{q}_i^T \in \mathbb{R}^{d \times d}\]

<p>notice that</p>

<ul>
  <li>
    <p>we are adding $k$ rank $1$ matrices, where each $\vec{q}_i\vec{q}^T$ is a rank 1 matrix. The result will be exactly a rank $k$ matrice since $\vec{q}_i$ are orthonormal.</p>
  </li>
  <li>
    <p>$\vec{q}_i \in \mathbb{R}^d$ since we are doing the subspace!</p>
    <ul>
      <li>but then our task is to represent data in space $k$! You can visualize that by <strong>imagining yourself residing in the $\text{span}{\vec{q}_1, …, \vec{q}_k}$,</strong> then each data point is the $k$ dimension</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>Example:</em> $k=1$ case</p>

<p>Then our task becomes finding $\Pi_k = \vec{q}\vec{q}^T$, which is the same as finding $\vec{q}$ that works.</p>

<p>Firstly, our objective becomes</p>

\[\min_{||\vec{q}||=1} \frac{1}{n} \sum_{i=1}^n || \vec{x}_i - (\vec{q}\vec{q}^T) \vec{x}_i ||^2\]

<p>Open the product up</p>

\[\begin{align*}
\frac{1}{n} \sum_{i=1}^n || \vec{x}_i - (\vec{q}\vec{q}^T) \vec{x}_i ||^2
&amp;= \left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i^T\vec{x}_i \right) - 2\vec{q}^T\left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i\vec{x}_i^T \right)\vec{q} + \vec{q}^T\left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i\vec{x}_i^T \right)\vec{q}\\
&amp;= \left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i^T\vec{x}_i \right) - \vec{q}^T\left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i\vec{x}_i^T \right)\vec{q}
\end{align*}\]

<p>note that:</p>

<ul>
  <li>
    <p>the first equality comes from the fact that $\vec{a}^T\vec{b}$  is a scalar and can be moved around.</p>

\[\begin{align*}
x_i^T q q^T x_i 
&amp;= q^T x_i x_i^T q
\end{align*}\]

    <p>then we can extract the $\vec{q}$ to be on the outside. Also, since $\vec{q}$ are orthonormal vectors, the term with:</p>

\[\vec{q}\vec{q}^T\vec{q}\vec{q}^T = \vec{q}\vec{q}^T\]

    <p>since $\vec{q}^T\vec{q} = 1$</p>
  </li>
  <li>
    <p>notice that <strong>only the last term depends</strong> on $\vec{q}$, so if we do $\max_{\vert \vert \vec{q}\vert \vert }$, the first term can be dropped.</p>
  </li>
</ul>

<p>Therefore, the task becomes:</p>

\[\begin{align*}
\min_{||\vec{q}||=1} - \vec{q}^T\left( \frac{1}{n} \sum_{i=1}^n \vec{x}_i^T\vec{x}_i \right)\vec{q}
&amp;= \max_{||\vec{q}||=1} \vec{q}^T \left( \frac{1}{n} X^TX \right)\vec{q} \\
&amp;= \max_{||\vec{q}||=1} \vec{q}^T M\vec{q}
\end{align*}\]

<p>but notice that we are stacking data points such that $X \in \mathbb{R}^{n \times d}$</p>

<ul>
  <li>
    <p>the conversion from vectors to matrix works by:</p>

\[\begin{align*}
\vec{q}^T
\begin{bmatrix}
\vline &amp; \vline &amp; \vline\\
x_1 &amp; ... &amp; x_n\\
\vline &amp; \vline &amp; \vline
\end{bmatrix}
\begin{bmatrix}
\rule[2.2pt]{1.2em}{0.4pt}  &amp; x_1 &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\rule[2.2pt]{1.2em}{0.4pt}  &amp; ... &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\rule[2.2pt]{1.2em}{0.4pt}  &amp; x_n &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\end{bmatrix}\vec{q}
= \begin{bmatrix}
\vec{q}^T\vec{x}_1 &amp;...&amp; \vec{q}^T\vec{x}_n
\end{bmatrix}
\begin{bmatrix}
\vec{q}^T\vec{x}_1\\ 
\vdots\\
\vec{q}^T\vec{x}_n
\end{bmatrix}
= \sum_{i=1}^n (\vec{q}^T\vec{x}_i)^2
\end{align*}\]

    <p>which is the same as:</p>

\[\vec{q}^T\left( \sum_{i=1}^n \vec{x}_i\vec{x}_i^T \right)\vec{q} =  \sum_{i=1}^n (\vec{q}^T\vec{x}_i)^2\]
  </li>
  <li>$X^TX$ is close to the covariance of the data (only if centered at the mean)</li>
  <li>$M=X^TX$ is basically a matrix of solely composed of our data, and it is <mark>diagonalizable since it is symmetric</mark></li>
  <li>and lastly, notice <mark>that $M \in \mathbb{R}^{d \times d}$ for $\vec{x}_i \in \mathbb{R}^d$</mark></li>
</ul>

<p>Now, it turns out that the <strong>maximal solution</strong> happens <mark>when $\vec{q}$ is the eigenvector of $M$ that has the largest eigenvalue</mark> (many resources have the <a href="https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav">proof</a>. Below is one that is intuitively)</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Difference between doing this in $k=1$ and doing a <strong>“linear regression”</strong> to fit a line (if you treat $x_d = y$)</p>

  <ul>
    <li>
      <p>if $\vec{x} \in \mathbb{R}^d$, then your surface of linear regression will be $f:\mathbb{R}^d \to \mathbb{R}$ is a <strong>surface in $\mathbb{R}^{d+1}$</strong>. Though our PCA is giving also $\phi: \mathbb{R}^d \to \mathbb{R}$, it is a <strong>subspace/plane through origin</strong> in $\mathbb{R}^d$</p>
    </li>
    <li>
      <p>the output of linear regression of drawing a line through $\vec{x}$ data points will be the vector $\vec{w} \in \mathbb{R}^{d+1]}$ (lifted) that is perpendicular to the linear surface (going through origin), such that:</p>

\[\vec{w} \cdot \vec{x} = 0\]
    </li>
    <li>
      <p>the “error” criteria for PCA and Linear Regression is different, the former wants to minimize over the orthogonal distance, the latter the vertical distance</p>
    </li>
  </ul>
</blockquote>

<hr />

<p><em>Proof</em></p>

<p>Since our matrix $M$ is diagonalizable, we can write it as:</p>

\[M = V \Lambda V^T\]

<p>where $V$ are composed of <strong>orthonormal eigenvectors</strong>. Then, since they actually look like:</p>

\[V \Lambda V^T = \begin{bmatrix}
\vline &amp; \vline &amp; \vline\\
v_1 &amp; ... &amp; v_n\\
\vline &amp; \vline &amp; \vline
\end{bmatrix}
\begin{bmatrix}
\lambda_1 &amp; 0&amp; \dots\\
0&amp; \ddots &amp;  0&amp;\\
\vdots &amp; 0 &amp; \lambda_n
\end{bmatrix}
\begin{bmatrix}
\rule[2.2pt]{1.2em}{0.4pt}  &amp; v_1 &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\rule[2.2pt]{1.2em}{0.4pt}  &amp; ... &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\rule[2.2pt]{1.2em}{0.4pt}  &amp; v_n &amp; \rule[2.2pt]{1.2em}{0.4pt}\\
\end{bmatrix}\]

<p>Then, if you consider doing:</p>

\[q^T V \Lambda V^Tq, \quad ||q||=1\]

<p>so we can write, for $\tilde{q} = V^{\top}q$</p>

\[q^{\top}V\Lambda V^{\top}q = \tilde{q}^{\top}\Lambda \tilde{q} = \sum_{i=1}^n \lambda_i \tilde{q}_i^2.\]

<p>Therefore it follows that:</p>

\[\min \lambda(M) \sum_{i=1}^n \tilde{q}_i^2 \leq \sum_{i=1}^n \lambda_i \tilde{q}_i^2 \leq \max \lambda(M)\sum_{i=1}^n \tilde{q}_i^2\]

<p>but realize that since $\vert \vert q\vert \vert =1$ was a constarint and $V$ is unitary, we know $\tilde{q}\cdot \tilde{q}=1$:</p>

\[\min \lambda(M) \leq q^{\top}Mq \leq \max \lambda(M).\]

<p>so we the best $q$ is the one that yields $\max \lambda(M)$.</p>

<h4 id="eigen-decomposition">Eigen Decomposition</h4>

<p>Continuing on the example of $k=1$. We have basically just shown that the largest value occurs at the largest $\lambda_1$. Therefore, consider the eigen decomposition of $M=X^TX$:</p>

\[Mv = \lambda v\]

<p>Then, for that/any eigenvector $v$:</p>

\[\begin{align*}
\lambda = \frac{v^T M v}{v^T v}
&amp;= \bar{v}^T M \bar{v}, \quad \text{where }\bar{v} \equiv \frac{v}{||v||}
\end{align*}\]

<p>then to maximize $\lambda$, we just have the that corresponding $\bar{v}$.</p>

<ul>
  <li>basically just solve for the orthonormal vector for the largest eigenvalue $\lambda$.</li>
</ul>

<blockquote>
  <p><em>Remember:</em></p>

  <ul>
    <li>$M = X^TX \in \mathbb{R}^{d \times d}$ for $\vec{x}_i \in \mathbb{R}^{d}$. Therefore, eigenvectors will also be $\vec{v}\in \mathbb{R}^d$ and there will be $d$ of them.</li>
  </ul>
</blockquote>

<h4 id="normalizing-data-in-pca">Normalizing Data in PCA</h4>

<p>Consider some data $\vec{x} \in \mathbb{R}^2$ that looks like:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209162317800.png" alt="image-20211209162317800" style="zoom: 67%;" /></p>

<p>But notice that to be a <strong>subspace of $\mathbb{R}^2$</strong>, we need  $\vec{q}$ to <mark>go through the origin</mark>. Therefore:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Wrong</th>
      <th style="text-align: center">Possible</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209162614926.png" alt="image-20211209162614926" /></td>
      <td style="text-align: center"><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209162907669.png" alt="image-20211209162907669" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>since we know $\vec{q}\in \mathbb{R}^d$, it must <strong>originate from the origin</strong> and the subspace/line must therefore pass through the origin, the figure on the left cannot be the output of PCA.</p>
  </li>
  <li>
    <p>then, if we consider the figure on the right, the <strong>correct PCA</strong> result would be line $A$, because it <strong>minimizes the residual</strong> (recall our objective is)</p>

\[\min_{||\vec{q}||=1} \frac{1}{n} \sum_{i=1}^n || \vec{x}_i - (\vec{q}\vec{q}^T) \vec{x}_i ||^2\]

    <p>for the case of $k=1$.</p>
  </li>
  <li>
    <p>therefore, for <mark>off-mean</mark> data, you will get <mark>undesirable result</mark>.</p>
  </li>
</ul>

<p>In reality, many packets secretly <strong>subtracts off the mean automatically</strong>:</p>

\[X^TX \to  (X - \mathbb{E}[X])^T(X - \mathbb{E}[X])\]

<p>so that the results will look more desirable.</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209163942415.png" alt="image-20211209163942415" style="zoom: 33%;" /></p>

<blockquote>
  <p><strong>Therefore</strong>, to get sensible results, we are <strong>always going to assume</strong> that $X$ has been subtracted out from the mean.</p>

  <ul>
    <li>the structure $(X - \mathbb{E}[X])^T(X - \mathbb{E}[X])$ indicating that we are computing <strong>covariance</strong>!</li>
  </ul>
</blockquote>

<h4 id="maximum-covariance-interpretation">Maximum Covariance Interpretation</h4>

<p>Recall that, for $\vec{q}$ only (wanting to have $k=1$) we have</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209151837556.png" alt="image-20211209151837556" style="zoom:50%;" /></p>

<p>This is because, consider the variance of the <strong>projected data</strong> in the <mark>$\vec{q}$ subspace</mark> (a line) is:</p>

\[\text{projected data in $\vec{q}$ direction: }\quad \vec{q}^T \vec{x}_1, ..., \vec{q}^T \vec{x}_n\]

<p>Then, if we compute the variance of those data points:</p>

\[\begin{align*}
\text{Var}[X_q] 
&amp;= \mathbb{E}[X_q^2] - \mathbb{E}[X_q]^2\\
&amp;= \frac{1}{n} \sum_{i=1}^n (\vec{q}^T \vec{x}_i)^2 - 0 \\
&amp;= \frac{1}{n} \sum_{i=1}^n (\vec{q}^T \vec{x}_i)^T(\vec{q}^T \vec{x}_i) \\
&amp;= \vec{q}^T \left( \frac{1}{n}X^TX \right)\vec{q}
\end{align*}\]

<p>therefore $\vec{q}^T \left( \frac{1}{n}X^TX \right)\vec{q}$ is <strong>exactly our objective</strong>!</p>

<ul>
  <li>therefore, what we computed as <mark>obejctive</mark> is the <mark>same as computing the variance in the direction of $\vec{q}$</mark></li>
  <li>the second equality comes from the idea that, if the global mean is zero, the projected mean is zero</li>
</ul>

<p>Therefore, the objective of doing (again for $k=1$)</p>

\[\max_{||\vec{q}||=1} \vec{q}^T \left( \frac{1}{n} X^TX \right)\vec{q}\]

<p>is the same as finding $\vec{q}$ such that the projected data $X_q$ will have the <strong>largest variance</strong>.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>since we are maximizing variance, this makes PCA very <strong>sensitive</strong> to both <mark>outliers</mark> and <mark>units</mark>
      <ul>
        <li>obviously since it maximizes variance, or minimize the residual, <strong>outlies have big impacts</strong>!</li>
        <li>PCA treats <strong>all features equally</strong>. So if one of the feature, say, measures height, happens to have the unit of $\text{km}$. Then, all data will have “small distances” between each other, which will distort PCA output.</li>
      </ul>
    </li>
    <li>there are robust versions of PCA</li>
  </ul>
</blockquote>

<h4 id="generalization-to-k1">Generalization to $k&gt;1$</h4>

<p>Then, since we know that for $k=1$, the problem</p>

\[\max_{||\vec{q}||=1} \vec{q}^T \left( \frac{1}{n} X^TX \right)\vec{q}\]

<p>is the same as finding a **single $\vec{q} \in \mathbb{R}^d$ ** such that projected data gets maximum variance, then if $k&gt;1$, the problem is</p>

\[\arg\min_{Q\in \mathbb{R}^{d \times k},Q^TQ=I}\frac{1}{n} \sum_{i=1}^n || \vec{x}_i - QQ^T \vec{x}_i ||^2 = \arg\min_{Q\in \mathbb{R}^{d \times k},Q^TQ=I}\tr \left( Q^T \left(\frac{1}{n}X^TX\right) Q \right)\]

<p>But we can just <strong>iteratively treat it as $k$ 1-D problem</strong>:</p>

<ol>
  <li>assume $k=1$, find the $\vec{q}=\vec{q}_1$ (which is the eigenvector with <strong>largest</strong> $\lambda_1$ as well) that maximizes variance</li>
  <li>take the <mark>orthogonal subspace</mark> of $\vec{q}_1$, and repeat step 1 for $k$ times</li>
</ol>

<p>Therefore, you in the end get $k$ orthogonal eigenvectors, which means the problem is <mark>the same as</mark>:</p>

\[\text{Find Top $k$ eigenvectors of the matrix $X^TX$}\]

<h4 id="examples-of-pca">Examples of PCA</h4>

<p>Consider PCA of handwritten digits, each data $\vec{x}_i \in \mathbb{R}^{784}$ (which can be reshaped to $27 \times 27$), into some dimension $k$:</p>

<ol>
  <li>
    <p>subtract off the mean for each image:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209171040914.png" alt="image-20211209171040914" style="zoom:80%;" /></p>

    <p>where</p>

    <ul>
      <li>since each data point is a vector in $\mathbb{R}^{784}$, we can compute that mean, and reshape it back to $27 \times 27$</li>
    </ul>
  </li>
  <li>
    <p>find eigenvectors of data $XX^T \in \mathbb{R}^{725 \times 724}$, which means we get $\vec{q}_i$ will be in $\mathbb{R}^{784}$. Also, there will be $784$ of them:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209171245180.png" alt="image-20211209171245180" style="zoom:80%;" /></p>

    <p>where here we presented 4 of them:</p>

    <ul>
      <li>the <strong>purple</strong> points are when elements of eigenvectors are <strong>positive</strong>, and the <strong>green</strong> parts are when elements of eigenvectors are <strong>negative</strong> (different color because we need to show them as “purple scale” which goes $0 \sim 255$)</li>
    </ul>
  </li>
  <li>
    <p>If we want $k$ dimension, pick only $k$ of the largest eigenvectors</p>
  </li>
</ol>

<p>To interpret what <strong>eigenvectors in images mean</strong>, consider we <mark>write an image $\vec{x}_i \in \mathbb{R}^{784}$ as linear combination of eigenvectors $\vec{q}_i \in \mathbb{R}^{784}$</mark>:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209171645986.png" alt="image-20211209171645986" style="zoom:80%;" /></p>

<p>essentially now:</p>

<ul>
  <li>
    <p>each $w_i = \vec{x}_i \cdot \vec{q}_i$ is the component/weighting of each image in the $\vec{q}_i$ direction</p>
  </li>
  <li>any new data can be decomposed in a space $\text{span}{\vec{q}_1, …,\vec{q}_k}$ for PCA</li>
  <li>perfect reconstruction if we take $k=n$, which basically chooses all $784$ eigenvectors we computed</li>
  <li>now perhaps negative value for a RGB makes sense, as it means if we superimpose it onto another, it will subtract off from the RGB “purple scale”</li>
</ul>

<p>Some examples of picking $k$ values are:</p>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209171937637.png" alt="image-20211209171937637" style="zoom:80%;" /></p>

<p>notice that:</p>

<ul>
  <li>
    <p>if $k=0$, it is the same as just picking the mean as we are computing this:</p>

    <p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209172319854.png" alt="image-20211209172319854" style="zoom:50%;" /></p>

    <p>where each data point is a <em>point in $\mathbb{R}^{784}$</em>, and our PCA will be just the mean.</p>

\[\min_{\Pi^0} \frac{1}{n} \sum_{i=1}^n || \vec{x}_i - \Pi_0 (\vec{x}_i) ||^2 = \min_{\vec{\mu}} \frac{1}{n} \sum_{i=1}^n || \vec{x}_i - \vec{\mu}||^2\]
  </li>
</ul>

<h1 id="future-classes">Future Classes</h1>

<ul>
  <li>Unsupervised Learning</li>
  <li>Deep Learning (heavy load, submit papers)</li>
</ul>

<p><img src="/lectures/images/2021-12-09-COMS4771_ML/image-20211209154724672.png" alt="image-20211209154724672" /></p>


  </div><a class="u-url" href="/lectures/2021@columbia/COMS4771_ML.html/" hidden></a>
  <script src="/lectures/assets/js/my_navigation.js"></script>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
