<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COMS4995 Deep Learning part1 | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="COMS4995 Deep Learning part1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Logistics and Introduction" />
<meta property="og:description" content="Logistics and Introduction" />
<link rel="canonical" href="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/" />
<meta property="og:url" content="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-12-08T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COMS4995 Deep Learning part1" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-08T00:00:00-05:00","datePublished":"2022-12-08T00:00:00-05:00","description":"Logistics and Introduction","headline":"COMS4995 Deep Learning part1","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/"},"url":"/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/learning">Blog</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
  <script>
    MathJax = {
      // 
      loader: {
        load: ['[tex]/ams', '[tex]/textmacros', '[tex]/boldsymbol']
      },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']}
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COMS4995 Deep Learning part1</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-12-08T00:00:00-05:00" itemprop="datePublished">
        Dec 8, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="logistics-and-introduction">Logistics and Introduction</h1>

<p>Office hours</p>

<ul>
  <li>Lecturer, Iddo Drori (idrori@cs.columbia.edu), Tuesday 2:30pm, Zoom (Links to an external site.)</li>
  <li>CA, Anusha Misra, Wednesday 3:30-4:30pm, Zoom (Links to an external site.)</li>
  <li>CA, Vaibhav Goyal, Friday 3-4pm, Zoom (Links to an external site.)</li>
  <li>CA, Chaewon Park (cp3227@columbia.edu), Thursday 3:30-4:30PM, Zoom (Links to an external site.)</li>
  <li>CA, Vibhas Naik (vn2302@columbia.edu), Monday 11AM-12PM, Zoom</li>
</ul>

<p>Grades:</p>

<ul>
  <li>9 Exercises (30%, 3% each, <strong>individual</strong>, quizzes on Canvas)
    <ul>
      <li>quizzes will timed in some of the live lectures. So attend lectures!</li>
    </ul>
  </li>
  <li>Competition (30%, in pairs)</li>
  <li>Projects (40%, in teams of 3 students)</li>
</ul>

<p><strong>Projects Timeline</strong></p>

<ul>
  <li><em>Feb 18</em>: Form Teams and Signup</li>
  <li><em>Feb 25</em>: Select project</li>
  <li><em>Mar 10-11</em>: Project Kick-off and proposal meetings</li>
  <li><em>Mar 31 - Apr 1</em>: Milestone Meetings</li>
  <li><em>Apr 21-11</em>: Final Project Meetings</li>
  <li><em>Apr 28</em>: Project poster session</li>
</ul>

<h2 id="human-brain-deep-learning">Human Brain Deep Learning</h2>

<p>In comparison of sizes of number of neurons:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133414667.png" alt="image-20220118133414667" style="zoom: 67%;" /></p>

<p>where the left hand size are the deep learning models and the right hand size the human brain</p>

<ul>
  <li>however, it is also said that humans are “generalized”, where machines are “specialized”</li>
</ul>

<p>What happens</p>

<p>| <img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133924987.png" alt="image-20220118133924987" /> | <img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118133935376.png" alt="image-20220118133935376" /> |
| ———————————————————— | ———————————————————— |</p>

<ul>
  <li>Type 2 process, your pupil will dilate (slow)</li>
</ul>

<p>Then, in AlphaGo, as well as other models, essentially it is doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134044859.png" alt="image-20220118134044859" style="zoom:50%;" /></p>

<p>where the</p>

<ul>
  <li>neural networks DNN are doing Type 1 processes</li>
  <li>tree search doing Type 2. (e.g. Monte Carlo Tree Search)</li>
</ul>

<h2 id="intro-transformers">Intro: Transformers</h2>

<p><a href="https://arxiv.org/abs/1706.03762">The paper</a> ‘Attention Is All You Need’ describes transformers and what is called a <strong>sequence-to-sequence architecture</strong>. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the  sequence of words in a sentence, into another sequence. (Well, this  might not surprise you considering the name.)</p>

<p>Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A brief sketch of how it works would be:</p>

<ol>
  <li>Input (e.g. in English) passes through an encoder</li>
  <li>Encoder takes the input sequence and maps it into a higher dimensional space (imagine translating it to some imaginary language $A$)</li>
  <li>Decoder takes in the sequence in the imaginary language $A$ and turns it into an output sequence (e.g. French)</li>
</ol>

<p>Initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.</p>

<ul>
  <li>A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.</li>
</ul>

<p>More details:</p>

<ul>
  <li>
    <p>https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</p>
  </li>
  <li>
    <p>https://towardsdatascience.com/transformers-141e32e69591</p>
  </li>
</ul>

<h3 id="example-application">Example Application</h3>

<p>Consider the task of trying to use AI to solve math problems (e.g. written in English)</p>

<p>It turns out that using language models, it doesn’t work if you want to do those math questions. However, if you turn math questions into programs, then it worked!</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134738566.png" alt="image-20220118134738566" style="zoom: 33%;" /></p>

<p>Then, some example output of this using Codex would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118134913140.png" alt="image-20220118134913140" /></p>

<p>notice that questions will need to be able to <strong>rephrased/transformed</strong> so that it is clearly a <strong>programming task</strong>, before putting into learning models.</p>

<ul>
  <li>this also means a full automation would be difficult</li>
</ul>

<h2 id="dl-timeline">DL Timeline</h2>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118135630742.png" alt="image-20220118135630742" /></p>

<h2 id="supervised-deep-learning-example">Supervised Deep Learning Example</h2>

<p>Consider the following data, and we want to use a linear model to separate the data</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118135900287.png" alt="image-20220118135900287" style="zoom:50%;" /></p>

<p>notice that by default, a linear transformer does not work. Hence we need to consider $x_1x_2$ as a feature</p>

<ul>
  <li>the idea is that we may want to consider ==feature extraction/processing== before putting them into the network</li>
</ul>

<p>However, what if we are given the following data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118140203431.png" alt="image-20220118140203431" /></p>

<p>where there doesn’t seem to be a clear/easy solution if we stick with a linear classifier even with some single layer feature transformation. As a result, in this case you will have to use a neural network:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118140229608.png" alt="image-20220118140229608" /></p>

<p>the upshot is that we really need to consider <strong>extracting features and use linear classifiers</strong> before using deep neural network, which is necessary only in some cases like the one above.</p>

<h2 id="representations-sharing-weights">Representations Sharing Weights</h2>

<p>Some nowadays popular NN architectures are:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220118145207752.png" alt="image-20220118145207752" /></p>

<p>where notice that one common feature that made them successful is to <strong>share weights $W$ between layers/neurons</strong>:</p>

<ul>
  <li>
    <p>CNN share $W$ through space</p>
  </li>
  <li>
    <p>RNN share $W$ through time</p>
  </li>
  <li>
    <p>GNN share $W$ across neighborhoods</p>
  </li>
</ul>

<h1 id="forward-and-back-propagation">Forward and Back Propagation</h1>

<p>The basics of all the NN is a Neuron/Perceptron</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120131919835.png" alt="image-20220120131919835" style="zoom:50%;" /></p>

<p>where:</p>

<ol>
  <li>input is a single vector</li>
  <li>the $\Sigma$ represents we are summing the components of $\vec{x}$ ($w_0$ is a scalar representing bias)</li>
  <li>then the scalar is passed into an activation function $f$, often non-linear</li>
</ol>

<p>Now, remember that our aim is to ==minimize loss== with a given training label:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132007260.png" alt="image-20220120132007260" style="zoom:50%;" /></p>

<p>where $\mathcal{L}$ is a loss function of our choice:</p>

<ul>
  <li>this is summed over all the training $m$ samples</li>
  <li>$w_0$ will represent the bias, often/later absorbed into $W$</li>
  <li>our objective is to minimize this $J$</li>
</ul>

<blockquote>
  <p><em>Note</em></p>

  <p>Remember that for any Learning task</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120203244526.png" alt="image-20220120203244526" style="zoom:80%;" /></p>

  <p>Before the step of feature extraction and selection, you may want to pay careful attention on how to do <strong>feature extraction/selection</strong> to improve your model.</p>
</blockquote>

<h2 id="neural-networks">Neural Networks</h2>

<p>The simplest component is a Neuron/Perceptron, whose mathematical model was basically finding:</p>

\[g(\vec{x}) = w^T\vec{x}+w_0 = 0\]

<p>and doing the following for classification:</p>

\[f(x):= 
\begin{cases}
+1 &amp; \text{if } g(x) \ge 0\\
-1 &amp; \text{if } g(x) &lt; 0
\end{cases} \quad = \text{sign}(\vec{w}^T\vec{x}+w_0)\]

<p>However, since the output is $\text{sign}$ function which is not differentiable, in Neural Network we will use $\sigma$ sigmoid instead. This also means that, instead of using Perceptron Algorithm to learn, we can use <strong>backpropagation</strong> (basically a taking derivatives using chain rule backwards).</p>

<p>That said, a ==Neural Network== basically involves connecting a number of <strong>neurons</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205520527.png" alt="image-20220120205520527" style="zoom: 33%;" /></p>

<p>where since at $\Sigma$ we are just doing $\vec{w}^T \vec{x}$, which is a linear combination, we used the $\Sigma$ symbol.</p>

<p>Then, combining those neurons in a <strong>fully connected network</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132436067.png" alt="image-20220120132436067" style="zoom:67%;" /></p>

<p>Conceptually, since each neuron/layer $l$ basically does two things, from the book:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204213314.png" alt="image-20220120204213314" /></p>

<p>Hence each layer can be expressed as:</p>

\[z^l = W^l \vec{a}^{l-1},\quad \vec{a}^l = f(z^l)\]

<p>where we assumed a single data point input (i.e. a vector instead of a matrix), but note that:</p>

<ul>
  <li>
    <p>$Z^l$ is the linear transformation, $a^l$ is the $\sigma$ applied to each element if activation is $\sigma$.</p>

    <p>Therefore, since we are just doing a bunch of linear transformation (stretchy) and passing to a sigmoid (squash):</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Start</th>
          <th> </th>
          <th> </th>
          <th> </th>
          <th> </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204442993.png" alt="image-20220120204442993" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204456452.png" alt="image-20220120204456452" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120204624982.png" alt="image-20220120204624982" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205207727.png" alt="image-20220120205207727" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120205247722.png" alt="image-20220120205247722" /></td>
        </tr>
      </tbody>
    </table>

    <p>where:</p>

    <ul>
      <li>at the start, the red will be labeled as $+1$, and blue $-1$ in a space perpendicular to the graph</li>
      <li>the last one looks separable.</li>
    </ul>
  </li>
  <li>
    <p>This can be easily gerealized when you have $n$ data points, so that:</p>

\[Z^l = (W^{l})^T A^{l-1},\quad A^l = f(Z^l)\]

    <p>basically you have now <strong>matrices</strong> as input and output, weights are still the same. To be more precise, a layer then looks like</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120210731448.png" alt="image-20220120210731448" style="zoom:50%;" /></p>

    <p>so that you are doing $z^l = (W^{l})^T a^{l-1}+W_0$, if again, we have a single data point input of $m$ dimension. Yet often we would have absorbed the $W_0$ by lifting, so that we have:</p>

\[\vec{a} = \begin{bmatrix}
a_1\\
\vdots\\
a_m\\
1
\end{bmatrix} \in \mathbb{R}^{m+1}, \quad W^T \leftarrow [W^T, W_0] \in \mathbb{R}^{n \times (m+1)}\]
  </li>
</ul>

<p>In general, if there are $n_l$ neurons in the $l$-th layer, then at the $l$-th layer:</p>

\[A^{l-1} = \text{input} \in \mathbb{R}^{(n_{l-1}+1)) \times m},\quad  (W^l)^T \in \mathbb{R}^{n_l \times (n_{l-1} + 1)}\]

<p>where $n_0 = m$ is the dimension of the data, and we absorbed in the bias.</p>

<ul>
  <li>a quick check is to make sure that $Z^l = (W^l)^T A^{l-1}$ works</li>
  <li>a visualization is that data points are now aligned <strong>vertically</strong> in the matrix</li>
</ul>

<h3 id="neuron-layer">Neuron Layer</h3>

<p>To make it easier to digest <strong>mathematically</strong>, we can think of ==each layer as a single operation==. Graphically, we convert:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132436067.png" alt="image-20220120132436067" style="zoom: 50%;" /></p>

<p>To this, simply three “neurons” (excluding the input):</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120212259363.png" alt="image-20220120212259363" style="zoom: 33%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>$F(x) = f(W^Tx)$ is a shorthand for notating the entire linear and nonlinear operation. This would be <em>nonlinear</em>.</p>
  </li>
  <li>
    <p>therefore, each layer $l$ basically just does:</p>

\[F^l(A^{l-1}) = A^l\]

    <p>outputting $A^l$. Hence we can do a “Markov chain” view of the NN as just doing:</p>

\[F(x) = F^3 (F^2 (F^1(x)))\]
  </li>
  <li>
    <p>if $f$ are identities, then the output is just a linear transformation since $F$ would be <em>linear</em>.</p>
  </li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the following NN. For brevity, I only cover the first layer:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120212538132.png" alt="image-20220120212538132" /></p>

<p>First we do the pre-activation $z^1$ for the input of a single data point of dimension $3$:</p>

\[z^1 = (W^1)^T a^{0} = \begin{bmatrix}
w_{11} &amp; w_{21} &amp; w_{31}\\
w_{12} &amp; w_{22} &amp; w_{32} \\
\end{bmatrix}\begin{bmatrix}
x_{1}\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
z_{1}^1\\
z_2^1\\
\end{bmatrix}\]

<p>where bias would be ignored for now (otherwise there will be one more column of $[b_1, b_2^T$ for $(W^1)^T$ and a row of $1$ for $a^0$. Then the activation does:</p>

\[a^1 =  \begin{bmatrix}
f(z_{1}^1)\\
f(z_{2}^1)
\end{bmatrix}= \begin{bmatrix}
a_1^1\\
a_2^1
\end{bmatrix}\]

<p>which will be input of the second layer. Eventually:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120134137821.png" alt="image-20220120134137821" style="zoom: 50%;" /></p>

<p>the last part does not have an activation since we are doing a regression instead of classification.</p>

<h3 id="activation-functions">Activation Functions</h3>

<p>The activation function always does a mapping from $f: \mathbb{R} \to \mathbb{R}$. Hence they are applied element-wise.</p>

<p>Common examples of activation functions include:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120213419561.png" alt="image-20220120213419561" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>a difference between Swish and ReLU is that Switch has a defined derivative at $z=0$</p>
  </li>
  <li>
    <p>In the case of ReLU, consider an input space of dimension $2$. Now fold it along an axis, returning two separately smooth planes intersecting at a “fold”. At this point, one of them is flat, and the other is angled at $45$ degrees. Since the input dimension is $2$, we will be folding again at the y-axis.</p>

    <ul>
      <li>By repeating this to create many ‘folds’, we can use the ReLU function to generate an “n-fold hyperplane” from the smooth 2D input plane.</li>
    </ul>

    <p>Therefore, combined with linear transformation, the folds will be slanted:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120220614045.png" alt="image-20220120220614045" style="zoom:67%;" /></p>

    <p>in the end we just have a sharded space from many folds.</p>
  </li>
  <li>
    <p>Swish: notice that $\lim_{\beta \to \infty}$ Swish becomes ReLU</p>
  </li>
</ul>

<p>Their derivatives are important to know since we will use them when taking derivatives during backpropagation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120132903520.png" alt="image-20220120132903520" style="zoom:50%;" /></p>

<p>where notice that</p>

<ul>
  <li>Sigmoid: $\sigma’$ depends on $\sigma$ means we would have ==already computed this== in the forward pass. This will save computational effort!</li>
  <li>ReLU: the simplest derivative among all. Saves computational effort.</li>
</ul>

<p>Last but not least, for multiclass classification, often the <strong>last layer</strong> uses SoftMax, which maps $\mathbb{R}^d \to \mathbb{R}^d$. This is often used <strong>only for the last layer if we are doing multiclass</strong>:</p>

\[g^L(z^L)_i = \frac{e^{z_i^L}}{\sum_{j=1}^d e^{z_{j}^L}}\]

<p>where:</p>

<ul>
  <li>
    <p>$z_i^L$ basically is the pre-activatoin on the last layer $l$</p>
  </li>
  <li>
    <p>this is a generalizatoin of logistic regression because $\sum_i^d g(z)_i = 1$, i.e. elements sum up to 1.</p>
  </li>
  <li>
    <p>also pretty easy to implemnet:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>yet since they add up to 1, they can also be seen as a generalization of logistic regressoin.</p>

<h3 id="loss-functions">Loss Functions</h3>

<p>Now we have several choice of $f$, our final objective of minimize is:</p>

\[\frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)\]

<p>for $\hat{y}^i$ is basically the $F^3 (F^2 (F^1(\vec{x}^i)))$, for example. Therefore, a couple of different choices for $\mathcal{L}$ function:</p>

<ul>
  <li>
    <p>Mean Squared Error</p>

\[\mathcal{L}(y^i, \hat{y}^i) = (y^i - \hat{y}^i)^2\]
  </li>
  <li>
    <p>Other power $p$ error:</p>

\[\mathcal{L}(y^i, \hat{y}^i) = |y^i - \hat{y}^i|^p\]

    <p>Graphically:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120214540633.png" alt="image-20220120214540633" style="zoom: 80%;" /></p>
  </li>
  <li>
    <p>Logistic Regression Loss (Cross-Entropy Loss)</p>

\[\mathcal{L}(y^i, \hat{y}^i) = -y^i \log(\hat{y}^i) - (1-y^i) \log (1-\hat{y}^i)\]

    <p>If this loss is used, the objective $J$ is convex in $W$.</p>
  </li>
  <li>
    <p>There is also a Softmax version/multiclass version of the logistic loss. Checkout the book for more info.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>In general, the loss function is <strong>not</strong> convex with respect to $W$, therefore solving:</p>

\[\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, \hat{y}^i)=\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W))\]

  <p>does not guarantee a global minimum. We therefore <strong>use gradient descent</strong> to find a <strong>local minimum</strong>.</p>
</blockquote>

<h3 id="regularization">Regularization</h3>

<p>What happens if we add regularization, such that we consider:</p>

\[\min_W \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^i, F(x^i, W)) + R(W)\]

<p>In general, this will more or less force values inside $W$ to be small.</p>

<p>Now, recall that $W$ basically does the linear part/composes the pre-activation before putting into $f$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120215157181.png" alt="image-20220120215157181" style="zoom:50%;" /></p>

<p>So we if have $f$ being a function such as sigmoid, it means that pre-activation would be most at the blue part:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120215433180.png" alt="image-20220120215433180" style="zoom: 80%;" /></p>

<p>where notice that:</p>

<ul>
  <li>adding regularization will likely shrink the $W$, which means that $Z^l$ would be small. Hence, $A^l=f^l(Z^l)$  would likely be ==linear== (the green part)!</li>
  <li>This means our activation being less complex, i.e. $f$ becomes almost a <strong>linear</strong> operation. Intuitively, the more we regularize, the less complicated our model</li>
</ul>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120134756312.png" alt="image-20220120134756312" style="zoom: 80%;" /></p>

<p>where if we use the previous example:</p>

<ul>
  <li>
    <p>e.g. $L=3$ since we have 3 layers</p>
  </li>
  <li>
    <p>$T$ is the number of iterations we want to perform (e.g. number of epochs if we are doing over the entire batch)</p>
  </li>
  <li>
    <p>the ==initialization must be randomized==, so that the updates for new $W$ will not be identical/same (when doing back prop)</p>

    <ul>
      <li>
        <p>If we set all the weights to be the same, then all the the neurons in the same layer <strong>performs the same calculation</strong>, there by making the whole deep net useless. If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron.</p>

        <p>E.g.</p>

\[z^1 = (W^1)^T a^{0} = \begin{bmatrix}
w_{11} &amp; w_{21} &amp; w_{31}\\
w_{12} &amp; w_{22} &amp; w_{32} \\
\end{bmatrix}\begin{bmatrix}
x_{1}\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
z_{1}^1\\
z_2^1\\
\end{bmatrix}\]

        <p>will have $z_1^1 = z_2^1$ and updates within the same layer will be identical.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>we are picking only one sample from the set because we will be doing <strong>stochastic gradient descent</strong> with the backprop algorithm. This is commonly used when datasets are large so we don’t want to do the entire dataset per step.</p>
  </li>
</ul>

<h2 id="back-propagation">Back Propagation</h2>

<p>The aim of this algorithm is, for each $W^l$ at layer $l$ (assumed bias is absorbed):</p>

\[W^l := W^l - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}\]

<p>basically doing a gradient descent to minimize our loss:</p>

<ul>
  <li>$\alpha$ would be the learning step size, which we can tune as a parameter</li>
</ul>

<p>Now, to compute the derivative, instead of <strong>doing it in a forward pass</strong> so that we need to do:</p>

<ol>
  <li>compute $\partial \mathcal{L}/\partial \mathcal{W^1}$</li>
  <li>then compute $\partial \mathcal{L}/\partial \mathcal{W^2}$</li>
  <li>then compute $\partial \mathcal{L}/\partial \mathcal{W^3}$</li>
</ol>

<p>if we have 3 layers. However, it turns out we can do ==achieve all the calculations== if we do it in ==a backward pass==:</p>

<ol>
  <li>
    <p>Notice that:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{W^l}} = \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]

    <p>since $Z^l = (W^l)^TA^{l-1}$. notice that $A^{l-1}$ is already computed in the forward pass. Graphically, if $l=2$, we are here:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120225335839.png" alt="image-20220120225335839" /></p>

    <p>note that we basically are doing derivative of a scalar w.r.t. a vector, so ==Jacobians== would be the brute force way:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120231135345.png" alt="image-20220120231135345" /></p>

    <p>yet the point is that we can save much effort using chain rule.</p>

    <ul>
      <li>if you have a $f:\mathbb{R}^n \to \mathbb{R}^m$, then the Jacobian will be dimension $\mathbb{R}^{m \times n}$. You can image each row doing $[df_i/dx_1,…,df_i/dx_n]$.</li>
    </ul>
  </li>
  <li>
    <p>Then, we need:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
= \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}\]

    <p>since we know $A^l=f(Z^l)$. If we have sigmoid, then we know $\sigma’(x) = \sigma(x)\cdot(1- \sigma(x))$.</p>

    <ul>
      <li>
        <p>Hence each element such as</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l_{11}}} 
= \frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{A^l}}{\partial \mathcal{Z^l_{11}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{\sigma(Z^l_{11})}}{\partial \mathcal{Z^l_{11}}}
=\sigma(Z_{11}^l)\cdot(1-\sigma(Z_{11}^l))\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}
=A_{11}^l\cdot(1-A_{11}^l)\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

        <p>so then computing for the derivative for the entire matrix, means just doing the above for each element</p>
      </li>
      <li>
        <p>so the ==one thing we actually have to compute== would be:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

        <p>though we know $\partial Z^l / \partial A^{l-1}=W^l$, we don’t know the loss function. However, it turns out we only needed to compute this for ==one (the last) layer== (see next step)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Lastly, the real shortcut in back propagation is that:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
=W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

    <p>hence, the ==other thing we have to compute is==:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

    <p>and that from this formula, knowing $\partial L / \partial Z^{l}$ means we can compute $\partial L / \partial A^{l-1}$ of the <strong>previous layer</strong>!</p>
  </li>
</ol>

<blockquote>
  <p><strong>In summary</strong></p>

  <p>To compute</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]

  <p>we needed to know</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}} 
=\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\frac{\partial \mathcal{f(Z^l)}}{\partial \mathcal{Z^l}}\]

  <ul>
    <li>notice that this computation is <strong>local</strong>: it only involves stuff from layer $l$</li>
  </ul>

  <p>which requires</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^l}}\]

  <p>which can be done in an efficient way once we are done with a layer:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{A^{l-1}}}
=\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\frac{\partial \mathcal{Z^l}}{\partial \mathcal{A^{l-1}}}
=W^l\frac{\partial \mathcal{L}}{\partial \mathcal{Z^{l}}}\]

</blockquote>

<p>Therefore, the back propagation algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232037290.png" alt="image-20220120232037290" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>we have an “extra” step here because this assumes that the <em>biases are not absorbed</em></p>
  </li>
  <li>the three main steps are outlined in the summary mentioned above. Due to dependency, they are done in reverse order</li>
  <li>the purple line shows the efficiency, that $\partial L / \partial A^{l}$ can be  computed from previous result with layer $l+1$. The only one what requires computation is the first iteration/last layer.</li>
  <li>this is basically the entire model! i.e. back propagation does the <strong>training/update</strong> of the parameters</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>In the first iteration, if we don’t care about losses yet, we need to figure out the second step on $\partial L / \partial Z^{l}$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232507028.png" alt="image-20220120232507028" style="zoom: 67%;" /></p>

<p>which we are doing element-wise for clarity. This can be easily generalized and placed into a vector:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232742923.png" alt="image-20220120232742923" style="zoom:67%;" /></p>

<ul>
  <li>
    <p>then we can compute easily:</p>

\[\frac{\partial \mathcal{L}}{\partial \mathcal{W^l}}= \frac{\partial \mathcal{L}}{\partial \mathcal{Z^l}}(A^{l-1})^T\]
  </li>
</ul>

<p>In the second iteration, we notice that we can reuse the results above:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120232935679.png" alt="image-20220120232935679" style="zoom:67%;" /></p>

<ul>
  <li>which then can compute the $$</li>
</ul>

<p>Lastly:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120233035309.png" alt="image-20220120233035309" style="zoom:67%;" /></p>

<p>which again reused the result from its higher up layer.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>If we take derivatives in the forward pass, we get only derivative of one variable. If we do it in the backward pass, we do it once and get ==all the derivatives== with little effort</p>

  <p>A big advantage of back propagation is also utilizing the fact that we are doing <strong>local</strong> computation (and passing on the result)</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120140038638.png" alt="image-20220120140038638" style="zoom:50%;" /></p>

  <p>which means backpropagation is a special case of <strong>differential programming</strong>, which can be optimized.</p>
</blockquote>

<h2 id="weight-initialization">Weight Initialization</h2>

<p>Basically a uniform distribution for weights and zeros for bias</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127141900682.png" style="zoom:50%;" /></p>

<p>Program</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="c1"># glorot init
</span>    <span class="n">epa</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="s">"W"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="s">"b"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer_dimensions</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.01</span>
</code></pre></div></div>

<h2 id="problems-with-nn">Problems with NN</h2>

<p>Recall that our objective is to minimize:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220120233655595.png" alt="image-20220120233655595" style="zoom: 50%;" /></p>

<p>This implies the following problem:</p>

<ol>
  <li>What if $n$ is large? Since our loss sums over all $n$ data points, it would take a long time to compute
    <ul>
      <li><strong>Solution</strong>: Stochastic Gradient Descent</li>
    </ul>
  </li>
  <li>Computing derivatives/doing gradient descent of large network takes time
    <ul>
      <li><strong>Solution</strong>: Backpropagation</li>
    </ul>
  </li>
  <li>For each time step/update, the gradient would be <em>perpendicular</em> to the previous one, forming a slow zig-zag pattern (slow to converge).
    <ul>
      <li><strong>Solution</strong>: Adaptive gradient Descent</li>
    </ul>
  </li>
</ol>

<h2 id="example-implementation">Example Implementation</h2>

<p>Consider the implementing the following architecture</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/neural_network_practice.png" style="zoom: 23%;" /></p>

<p>Then we would have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Implement the forward pass
</span><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W1'</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>  <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>

    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W2'</span><span class="p">].</span><span class="n">T</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>    
    <span class="c1"># Z1 -&gt; output of the hidden layer before applying activation
</span>    <span class="c1"># H -&gt; output of the  hidden layer after applying activation
</span>    <span class="c1"># Z2 -&gt; output of the final layer before applying activation
</span>    <span class="c1"># Y -&gt; output of the final layer after applying activation
</span>    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Z1</span>
</code></pre></div></div>

<p>And backward:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implement the backward pass
# Y_T are the ground truth labels
</span><span class="k">def</span> <span class="nf">back_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y_T</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">N_points</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># forward propagation
</span>    <span class="n">Y</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Z1</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N_points</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_T</span><span class="p">))</span>
    
    <span class="c1"># back propagation
</span>    <span class="n">dLdY</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">N_points</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">Y_T</span><span class="p">)</span>
    <span class="c1"># dLdZ2 = dLdA2 * dA2dZ2 = dLdA2 * sig(Z2)*[1-sig(1-Z2)] # broadcast multiply
</span>    <span class="n">dLdZ2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dLdY</span><span class="p">,</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">))))</span> 
    <span class="c1"># dLW2 = dLdA2 * dA2dZ2 * dZ2dW2 = dLdZ2 * dZ2dW2 = dLdZ2 * A1 # matrix multiply
</span>    <span class="n">dLdW2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dLdZ2</span><span class="p">)</span> 

     <span class="c1"># dLb2 = dLdA2 * dA2dZ2 * dZ2db2 = dLdZ2 * dZ2db2 = dLdZ2 * 1
</span>    <span class="n">dLdb2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ2</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_points</span><span class="p">))</span>
    
    <span class="c1"># dLdA1 = dLdA2 * dA2dZ2 * dZ2dA1 = dLdZ2 * dZ2dA1 = dLdZ2 * W2
</span>    <span class="n">dLdA1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ2</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span> 
    <span class="c1"># dLdZ1 = dLdA1 * dA1dZ1 = dLdA1 * sig(A1) * [1-sig(A1)] # broadcast multiply
</span>    <span class="n">dLdZ1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dLdA1</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">H</span><span class="p">)))</span>
     <span class="c1"># dLW1 = dLdA1 * dA2dZ1 * dZ2dW1 = dLdZ1 * dZ2dW1 = dLdZ1 * A0 # matrix multiply
</span>    <span class="n">dLdW1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dLdZ1</span><span class="p">)</span> 

    <span class="n">dLdb1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dLdZ1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N_points</span><span class="p">))</span>
    
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'W1'</span><span class="p">:</span> <span class="n">dLdW1</span><span class="p">,</span>
        <span class="s">'b1'</span><span class="p">:</span> <span class="n">dLdb1</span><span class="p">,</span>
        <span class="s">'W2'</span><span class="p">:</span> <span class="n">dLdW2</span><span class="p">,</span>
        <span class="s">'b2'</span><span class="p">:</span> <span class="n">dLdb2</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">L</span>
</code></pre></div></div>

<h1 id="optimization">Optimization</h1>

<p>In practice, it is important to have the some understanding of the optimizer we will use (e.g. how to gradient descent), because they perform differently in different scenarios.</p>

<p>The ones we use in practice the ==different algorithms to find minima== can be separated into the following three classes:</p>

<ul>
  <li>First order methods.
    <ul>
      <li>Gradient Descent (Stochastic, Mini-Batch, Adaptive)</li>
      <li>Momentum Related (Adagrad, Adam, Hypergradient Descent)</li>
    </ul>
  </li>
  <li>Second order methods
    <ul>
      <li>Newton’s Method (<em>generally faster than gradient descent</em> because it uses Hessian/is higher order)</li>
      <li>Quasi Newton’s Method (SR1 update, DFP, BFGS)</li>
    </ul>
  </li>
  <li>Evolution Strategies
    <ul>
      <li>Cross-Entropy Method (uses cluster of initial points as initial conditions, then descent as a group)</li>
      <li>Distributed Evolution Strategies, Neural Evolution Strategies</li>
      <li>Covariance Matrix Adaptation</li>
    </ul>
  </li>
</ul>

<h2 id="overview">Overview</h2>

<p>Again, our goal of learning this is to understand, given an optimization problem of finding best $\theta^*$:</p>

\[\theta^* = \arg\min_\theta J(\theta)\]

<p>where $J$ would be the total loss we are dealing with.</p>

<ul>
  <li>
    <p>an example for $J$ would be</p>

\[J(\theta)  = \left( \frac{1}{n} \sum_{i=1}^n \mathcal{L}(y^{(i)}, h(x^{(i)}; \theta)) \right) + \lambda R(\theta)\]

    <p>where we included a regularization term $R(\theta)$ here as well.</p>
  </li>
</ul>

<p>First, let us recall that the basic gradient descent algorithm generally looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125132905495.png" alt="image-20220125132905495" /></p>

<p>where $\eta$ would be tunable:</p>

<ul>
  <li>its aim is to obvious find the $\theta^*$, but it might get <strong>stuck at local minima</strong>
    <ul>
      <li>in that case, we need to add some noise, or use stochastic gradient descent</li>
    </ul>
  </li>
  <li>for large NN, finding $dJ/dW$ takes effort.
    <ul>
      <li>this is optimized with back-propagation algorithm</li>
    </ul>
  </li>
  <li>but obviously this is not the only way to do it, as you shall see soon</li>
</ul>

<blockquote>
  <p><strong>Convex Optimization</strong></p>

  <p>It would be so little pain if $J$ is convex w.r.t. $W$, so that any <strong>local minimum</strong> is also a <strong>global minimum</strong>.</p>

  <ul>
    <li>but often in NN, $J$ is not a convex function of $W$, so we do have the problem of stopping at local minimas.</li>
  </ul>

  <p>For a problem to be a convex optimization (with constraints):</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20211014155124853.png" alt="image-20211014155124853" /></p>

  <p>this problem is a convex optimization ==IFF both holds==:</p>

  <ul>
    <li>the <strong>feasible</strong> region of output (due to the constraint) is a <strong>convex set</strong></li>
    <li>the <strong>objective</strong> function $f(\vec{x})$ is a <strong>convex function</strong></li>
  </ul>

</blockquote>

<p>But in reality, this is what are we are facing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133004782.png" alt="image-20220125133004782" style="zoom: 50%;" /></p>

<p>where recall that:</p>

<ul>
  <li>Linear programs: objective function is linear (affine), and constraints are also linear (affine)
    <ul>
      <li>so that the feasible region is a convex set (because the feasible region is always a polygon = convex set)</li>
    </ul>
  </li>
  <li>Quadratic program: objective function is quadratic, and constraints are linear (affine)
    <ul>
      <li>if constraints are quadratic, then the feasible region might not be a convex set.</li>
    </ul>
  </li>
  <li>Conic Program: where constraints are a conic shaped region</li>
  <li>Other common solvers include: <code class="language-plaintext highlighter-rouge">CVX</code>, <code class="language-plaintext highlighter-rouge">SeDuMi</code>, <code class="language-plaintext highlighter-rouge">C-SALSA</code>,</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133237058.png" alt="image-20220125133237058" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>LHS shows starting with different initial points yields different result, i.e. some ended up at local minimas</li>
  <li>RHS shows starting from different initial points yields the same result, i.e. global minimum.</li>
</ul>

<h2 id="derivative-and-gradient">Derivative and Gradient</h2>

<p>Most of the cases we will be dealing with $f(\vec{x})$ where $\vec{x}$ is multi-dimensional. For instance $L(W)$ with loss being dependent on weights. Then, an obvious usage of this would be in gradient descent:</p>

\[w_{t+1} = w_t - \alpha_t \nabla f(w_t)\]

<p>for us using $\alpha_t$ because it can be changing (e.g. in adaptive methods)</p>

<blockquote>
  <p><em>Recall</em></p>

\[\nabla f(\vec{x}) = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} , ..., \frac{\partial f}{\partial x_n}  )\]

  <p>being a $n$ dimensional vector:</p>

  <ul>
    <li>imagine graphing $f(\vec{x})$ in a $\mathbb{R}^{n+1}$ since $\vec{x}\in \mathbb{R}^n$</li>
    <li>then $\nabla f$ points at direction of steepest ascent</li>
  </ul>
</blockquote>

<p>Another useful quantity would be the second derivative:</p>

<blockquote>
  <p><strong>Hessian</strong></p>

  <p>Again, for a scalar function with vector input:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133408736.png" alt="image-20220125133408736" style="zoom: 33%;" /></p>

  <p>which is useful because:</p>

  <ul>
    <li>$\vec{x}^*$ is a local minimum if $H\equiv \nabla^2f$ is <strong>positive semi-definite</strong>
      <ul>
        <li>in the case of $x\in \mathbb{R}$, we know that $f’‘(x) \ge 0$ means minima. In the case of vector input space, you have $n$-directions to look at. If each direction satisfies $Hx = ax$ for $a \ge 0$, then obviously it is “concave up”, and that $Hx = ax$ for all $x$ means $H$ contains only non-negative eigenvalues -&gt; positive semidefinite</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Sometimes, we may want to use <strong>numerical calculations of derivatives</strong> to make sure our formula put in practice is correct:</p>

\[f'(x) \approx \frac{f(x+\epsilon) - f(x - \epsilon)}{2 \epsilon}\]

<p>for small $\epsilon$.</p>

<ul>
  <li>
    <p>notice we are all using $x$ as the input variable. It might be useful in context if we think of $x \to \vec{w}$ being the weights that we need to optimize on.</p>
  </li>
  <li>
    <p>an example program would be</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133621511.png" alt="image-20220125133621511" /></p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>To compute gradient using:</p>

  <ul>
    <li>analytic equation: exact, fast
      <ul>
        <li>e.g. for NN, we derive the derivatives and used the formula</li>
      </ul>
    </li>
    <li>numerical equation: slow, contains error
      <ul>
        <li>useful for debugging, .e.g if backprop is implemented correctly</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="first-order-methods">First Order Methods</h2>

<p>Now, we talk about first order methods: <strong>using only first order derivative</strong> $g_t \equiv \nabla f(x_t)$ for weight (remember we generalized weights $w \to x$ any input) at iteration $t$.</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>The easiest and direct use of $\nabla f(x_t)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133718470.png" alt="image-20220125133718470" style="zoom: 33%;" /></p>

<p>where note that:</p>

<ul>
  <li>
    <p>you could add an early-stopping criteria at the end</p>
  </li>
  <li>
    <p>the tunable learning rate $\alpha_t$ is critical:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Too Big</th>
          <th style="text-align: center">Too Small</th>
          <th>Just Right</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133933698.png" alt="image-20220125133933698" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125133949095.png" alt="image-20220125133949095" /></td>
          <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134009975.png" alt="image-20220125134009975" /></td>
        </tr>
      </tbody>
    </table>

    <p>but as you might have guessed, step size $\alpha_t$ could be updated automatically in some other methods</p>
  </li>
  <li>
    <p>However, at saddle points, it may cause the <strong>update</strong> be too small. Hence often a noise term will be added</p>

\[x_{t+1} = x_t - \alpha_tg_t + \epsilon_t\]

    <p>for $\epsilon_t \sim N(0, \sigma)$ hoping that it goes out of local minima/saddle points. (related: <a href="#Vanishing/Exploding Gradient">Vanishing/Exploding Gradient</a>)</p>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider logistic loss:</p>

\[J(x;\theta) = -\frac{1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]\]

<p>Then, compute the gradient:</p>

\[\frac{\partial J(\theta)}{\partial \theta_j}  = 
\frac{\partial}{\partial \theta_j} \,\frac{-1}{m}\sum_{i=1}^m 
\left[ y^{(i)}\log\left(h_\theta \left(x^{(i)}\right)\right) +
(1 -y^{(i)})\log\left(1-h_\theta \left(x^{(i)}\right)\right)\right]\]

<p>Carefully computing the derivative yields:</p>

\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m\left[h_\theta\left(x^{(i)}\right)-y^{(i)}\right]\,x_j^{(i)}\]

<p>Then you can update $\theta_j$ using this.</p>

<hr />

<p><strong>Problems with Gradient Descent</strong></p>

<ol>
  <li>
    <p>The above takes an entire training set for computing the loss. Takes time.</p>

    <ul>
      <li>use mini-batch or stochastic</li>
    </ul>
  </li>
  <li>
    <p>Computing derivative w.r.t weights $\theta$ takes effort if $\theta$ is high dimensional</p>

    <ul>
      <li>use backpropagation</li>
    </ul>
  </li>
  <li>
    <p>What step-size should we use? We may overshoot if too large of a stepsize.</p>

    <ul>
      <li>adaptive learning rate</li>
    </ul>
  </li>
  <li>
    <p>Gradient descent typically spend too much time in regions that is relatively flat as gradient is small</p>

    <ul>
      <li>
        <p>e.g.</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Normal Region</th>
              <th style="text-align: center">Flat Region</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134255982.png" alt="image-20220125134255982" style="zoom:50%;" /></td>
              <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125134221684.png" alt="image-20220125134221684" style="zoom:50%;" /></td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>
        <p>Use <a href="#Adaptive Gradient Descent">Adaptive Gradient Descent</a></p>
      </li>
    </ul>
  </li>
</ol>

<h4 id="adaptive-step-size">Adaptive Step Size</h4>

<p>There are certain options we can choose from:</p>

<ul>
  <li>
    <p><strong>decrease learning rate as training progresses</strong> (learn less in the future -&gt; prevent overfitting)</p>

    <ul>
      <li>
        <p>this can be done using either a decay factor that gets smaller over time:</p>

\[\alpha_t = \alpha_0 \frac{1}{1+t\beta}\]

        <p>for $\beta$ being small</p>
      </li>
      <li>
        <p>simply exponential decay:</p>

\[\alpha_t = \alpha_0 \gamma^t\]

        <p>for some $\gamma &lt; 1$ but close to $1$, or</p>

\[\alpha_t = \alpha_0 \exp(-\beta t)\]
      </li>
    </ul>
  </li>
  <li>
    <p><strong>line searches</strong>: given some $\min_x f(x)$, and suppose we are currently at $x_t$ being our <strong>current best guess</strong>. We know the current gradient is $g_t = \nabla f(x)\vert _{x_t} = \nabla f(x_t)$. We consider some step size $\alpha_t$ we <strong>might take</strong>:</p>

\[\phi(\alpha_t) \equiv f(x_t + \alpha_t g_t)\]

    <p>and we want to <strong>approximately minimize $\phi(\alpha_t)$</strong> to output a $\alpha_t$ to use. Basically we are ==sliding along the tangent line of the current point and see how far we should slide==</p>

    <ul>
      <li>
        <p><strong>backtrack line search</strong></p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125222301156.png" alt="image-20220125222301156" /></p>

        <p>where $t^*$ is our desired $\alpha_t$</p>

        <p>Graphically:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125225435921.png" alt="image-20220125225435921" style="zoom: 67%;" /></p>
      </li>
      <li>
        <p><strong>exact line search</strong></p>

        <p>Solve the following exactly:</p>

\[\min_{\alpha_t} f(x_t + \alpha_t \hat{g}_t)\]

        <p>where $\hat{g}_t = g_t  /\vert g_t\vert$. So this means we want:</p>

\[\nabla f(x_t + \alpha_t \hat{g}_t) \cdot \hat{g}_t = 0\]

        <p>and we need to solve this for $\alpha_t$</p>

        <ul>
          <li>
            <p>one property of this result is that, since we know:</p>

\[\quad \hat{g}_{t+1} = \nabla f(x_t + \alpha_t \hat{g}_t)\]

            <p>So we see that</p>

\[\hat{g}_{t+1} \cdot \hat{g}_t = 0\]

            <p>meaning <strong>consecutive runs gives perpendicular gradient direction</strong>. This makes sense since we are taking the <em>optimal step size</em>, i.e. we have walked the farthest along that direction.</p>
          </li>
          <li>
            <p>Finding $\alpha_t$ is computationally expensive as we need to solve for it, so it is rarely used</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>adaptive line search</strong>: skipped</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h4>

<p>A common technique within gradient descent is to split your dataset into $n$ sets of size $k$, and train each set as <strong>one step for updating the gradient</strong>.</p>

<ul>
  <li>so that we don’t spend time computing the loss function on the entire data</li>
  <li>if you want, you can also parallelize this computation</li>
</ul>

<p>This is useful because, for a sample of size $k$, the <strong>sample mean</strong> follows the central limit theorem:</p>

\[\hat{\mu} \sim N(\mu, \sigma^2/n)\]

<p>which can be easily seen because:</p>

<ul>
  <li>
    <p>$\text{Var}[X_i] = \sigma^2$, and using linearity:</p>

\[\text{Var}\left[\frac{1}{n}\sum X_i\right] = \frac{1}{n^2} \text{Var}\left[\sum X_i\right] = \frac{1}{n^2} \sum\text{Var}\left[ X_i\right] = \sigma^2 / n\]
  </li>
  <li>
    <p>This is good because **standard deviation of $\hat{\mu} \propto 1/\sqrt{n}$ **</p>

    <p>So if using 100 samples vs 10000 samples means:</p>

    <ul>
      <li>faster computation for factor of 100</li>
      <li>but only more error of factor of 10</li>
    </ul>
  </li>
</ul>

<h4 id="stochastic-gradient-descent">Stochastic Gradient Descent</h4>

<p>Basically equivalent of Mini-batch of size $1$</p>

<ul>
  <li>i.e. each update involves <strong>taking 1 random sample</strong></li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125225520623.png" alt="image-20220125225520623" style="zoom: 67%;" /></p>

<p>Some common nomenclatures:</p>

<ul>
  <li><strong>Epoch</strong>: a pass through the entire data</li>
</ul>

<p>Some <strong>properties</strong>:</p>

<ul>
  <li>unbiased estimators of the true gradient</li>
  <li>
    <p>early steps often converge fast towards the minimum</p>
  </li>
  <li>very noisy -&gt; but increases the chance of getting a global minima
    <ul>
      <li>e.g. at saddle points, it may cause the step be too small. But this is <strong>already noisy</strong>, so no problem.</li>
    </ul>
  </li>
</ul>

<h3 id="adaptive-gradient-descent">Adaptive Gradient Descent</h3>

<p>Either we use normal gradient descent, or gradient descent with optimized steps, we faced the problem of taking too long to converge in <strong>flat regions</strong>.</p>

<p>An overview would be that it uses <strong>gradients from previous steps</strong> to compute current gradient.</p>

<ul>
  <li>want to achieve faster convergence by move faster in dimension with low curvature, and slower in dimension with oscillations</li>
  <li>
    <p>the more official documentation:  <strong>AdaGrad</strong> for short, is an <strong>extension of the gradient</strong> descent optimization algorithm that allows the step size in each dimension used by the optimization algorithm to be automatically adapted ==based on the gradients seen for the variable==</p>
  </li>
  <li>however, some critics of this would say that it yields different result with gradient descent</li>
</ul>

<p>Examples with adaptive gradients include:</p>

<ul>
  <li>Momentum</li>
  <li>AdaGrad</li>
  <li>Adam</li>
</ul>

<h4 id="momentum">Momentum</h4>

<p>The basic idea is that the momentum vector <strong>accumulates gradients from previous iterations</strong> for computing the current gradient.</p>

<p><strong>Arithmetically weighted moving average</strong></p>

\[a_t = \frac{na_t +  (n-1)a_{t-1}) + ... + a_{t-n+1}}{n+(n-1)+ ... + 1}\]

<p>for basically imagining $a_t \to g_t$ is the gradient</p>

<ul>
  <li>$n$ is the <strong>weight</strong> which we can specify</li>
  <li>basically this is in a ==weighted moving average== the weights decrease ==arithmetically==, normalized by the sum of weights</li>
</ul>

<p>In the end, we see <strong>accumulation of gradients</strong> because:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232040667.png" alt="image-20220125232040667" style="zoom: 80%;" /></p>

<p>where notice that:</p>

<ul>
  <li>$s_t = \sum_{i=t-n+1}^ta_i$ is the accumulation of past gradients, since $a_t \to g_t$</li>
</ul>

<hr />

<p>Alternatively, there is also an expoentially weighted version</p>

<p><strong>Exponentially Weighted Moving Average</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232352096.png" alt="image-20220125232352096" style="zoom: 80%;" /></p>

<p>where here:</p>

<ul>
  <li>
    <p>again basically $a_t \to g_t$</p>
  </li>
  <li>
    <p>the parameter is actually $(1-\alpha) = \beta$ for convenience, and we want $\beta \in [0,1)$</p>
  </li>
  <li>
    <p>in an algorithm:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" /></p>

    <p>where basically $x$ would be our weights.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>A first problem with momentum is that the step sizes may not decrease once we have reached close to the minimum that may cause oscillations, which can be remedied by using Nesterov momentum (Dozat 2016) that replaces the gradient with the gradient after computing momentum (Dozat 2016):</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232704982.png" alt="image-20220125232704982" style="zoom:80%;" /></p>
</blockquote>

<p>Graphically</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125233001622.png" alt="image-20220125233001622" /></p>

<h4 id="adagrad">AdaGrad</h4>

<p>This deals with the case that we <strong>didn’t talk about what to do with $\alpha_t$</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125232534974.png" alt="image-20220125232534974" style="zoom:80%;" /></p>

<p>So one variation, AdaGrad, ==adapts the learning rate== to the parameters, i.e. $\alpha_t$ is different for <strong>each $\theta_i$</strong>/parameter:</p>

<ul>
  <li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features</li>
  <li>larger updates (i.e. high learning rates) for parameters associated with infrequent features</li>
</ul>

<p>For this reason, it is well-suited for dealing with sparse data, and suitable for SGD.</p>

<p>Instead of using $\alpha_t$ for all parameters at current time, use</p>

\[\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha_t}{\sqrt{s_{t,i} + \epsilon}} g_{t,i}\]

<p>and that $s_{t,i}$ is a weighted sum of gradients of $\theta_i$ up to time $t$:</p>

\[s_{t,i} = \beta s_{t-1,i} + g_{t,i}^2\]

<p>for $g_{t,i}$ is the gradient for the $\theta_i$.</p>

<p><strong>Problem</strong></p>

<p>This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is <strong>no longer able to acquire additional knowledge</strong>:</p>

\[\lim_{s_{t,i} \to \infty} \frac{1}{\sqrt{s_{t,i} + \epsilon}} = 0\]

<p>This is then solved by:</p>

<ul>
  <li>
    <p><strong>Adadelta</strong></p>

    <p>An exponential decaying average of square updates without a learning rate, replacing</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234218229.png" alt="image-20220125234218229" style="zoom:80%;" /></p>
  </li>
  <li>
    <p><strong>RMSProp</strong></p>

    <p>Adagrad using a weighted moving average, replacing:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234149326.png" alt="image-20220125234149326" style="zoom: 80%;" /></p>
  </li>
</ul>

<h4 id="adam">Adam</h4>

<p>Adaptive moment estimation, or Adam (Kingma &amp; Ba 2014), combines the best of both momentum updates and Adagrad-based methods as shown in Algorithm 6.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234407385.png" alt="image-20220125234407385" style="zoom:67%;" /></p>

<p>where basically:</p>

<ul>
  <li>uses momentum in the red part</li>
  <li>uses AdaGrad like adaptive learning rate on the yellow part</li>
  <li>since it combined two models, we have <strong>two parameters to specify</strong>. Typically $\beta_1 = 0.9, \beta_2 = 0.99$</li>
</ul>

<p>Several improvements upon Adam include:</p>

<ul>
  <li>
    <p><strong>NAdam</strong> (Dozat 2016) is Adam with Nesterov momentum</p>
  </li>
  <li>
    <p><strong>Yogi</strong> (Zaheer, Reddi, Sachan, Kale &amp; Kumar 2018) is Adam with an improvement to the second momentum term which is re-written as:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234659018.png" alt="image-20220125234659018" style="zoom:80%;" /></p>
  </li>
  <li>
    <p><strong>AMSGrad</strong> (Reddi, Kale &amp; Kumar 2018) is Adam with the following improvement</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234724476.png" alt="image-20220125234724476" style="zoom:80%;" /></p>
  </li>
</ul>

<h3 id="hyper-gradient-descent">Hyper-gradient Descent</h3>

<p>Hypergradient descent (Baydin, Cornish, Rubio, Schmidt &amp; Wood 2018) performs ==gradient descent on the learning rate== within gradient descent.</p>

<ul>
  <li>may be applied to any adaptive stochastic gradient descent method</li>
</ul>

<p>The basic idea is to consider $\partial f(x_t)/ \partial \alpha$, for $x \to w$</p>

\[\frac{\partial f(w_t)}{\partial \alpha} = \frac{\partial f(w_t)}{\partial w_t} \frac{\partial w_t}{\partial \alpha}\]

<p>we know that $w_t =  w_{t-1} - \alpha g_{t-1}$:</p>

\[\frac{\partial f(w_t)}{\partial \alpha} = g_t \cdot \frac{\partial }{\partial \alpha} ( w_{t-1} - \alpha g_{t-1})\]

<p>where:</p>

<ul>
  <li>The schedule may lower the learning rate when the network gets stuck in a local minimum, and increase the learning rate when the network is progressing well.</li>
</ul>

<p>Algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125234826019.png" alt="image-20220125234826019" style="zoom:80%;" /></p>

<h3 id="vanishingexploding-gradient">Vanishing/Exploding Gradient</h3>

<p>These problem is encountered when training artificial <strong>neural networks</strong> with ==gradient-based learning methods== and ==backpropagation==. In such methods:</p>

<ul>
  <li>
    <p><strong>vanishing gradient:</strong> during each iteration of training each of the neural network’s weights receives an update proportional to the partial derivative of the error function with respect to the current weight.</p>

\[W^l := W^l - \alpha \frac{\partial L}{\partial W^l}\]

    <p>The problem is that in some cases, the <strong>gradient will be vanishingly small</strong>, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.</p>
  </li>
  <li>
    <p>When <strong>activation functions</strong> are used whose derivatives can take on larger values, one risks encountering the related <strong>exploding gradient problem</strong></p>
  </li>
</ul>

<p>One example of the problem cause for vanishing gradient</p>

<ul>
  <li>traditional <strong>activation</strong> functions such as the hyperbolic tangent function have gradients in the range $(0,1]$, is very small</li>
  <li>Since <strong>backpropagation</strong> computes gradients by the chain rule. This has the effect of ==multiplying $n$ of these small numbers== to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially</li>
</ul>

<h2 id="second-order-methods">Second Order Methods</h2>

<p>First order methods are easier to implement and understand, but they are <strong>less efficient than second order methods</strong>.</p>

<ul>
  <li>Second order methods use the first and ==second derivatives== of a univariate function or the gradient and Hessian of a multivariate
function to compute the step direction</li>
  <li>Second order methods approximate the objective function using a <strong>quadratic</strong> which results in faster convergence
    <ul>
      <li>imagine basically 2nd order methods -&gt; parabola -&gt; go down a bowl with a bowl (2nd order); as compared to with a ruler (1st order method)</li>
    </ul>
  </li>
  <li>but a problem that they need to overcome is how to deal with computing/storing <strong>Hessian</strong> matrix, which could be large</li>
</ul>

<h3 id="newtons-method">Newton’s Method</h3>

<p>Basically we know that we can find the <strong>root of an equation</strong> using newton’s method:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125235632561.png" alt="image-20220125235632561" style="zoom: 67%;" /></p>

<p>so we basically guessed $x_{t+1}$ to be the root by <strong>fitting a line</strong>:</p>

\[x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}\]

<p>which basically does:</p>

<ul>
  <li>the number of steps to move being $\Delta x = f(x_t)/{f’(x_t)}$</li>
</ul>

<p>Then, since our goal is to solve (in 1-D case):</p>

\[\min f(x) \to f'(x) = 0\]

<p>So basically we consider finding root for $f’(x)$:</p>

\[x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}\]

<p>This results in</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125235919309.png" alt="image-20220125235919309" style="zoom: 67%;" /></p>

<p>where the blue line is the “imagined function” using Newton’s Method</p>

<ul>
  <li>notice it is a <strong>quadratic</strong></li>
  <li>therefore, it goes down the “bowl” <strong>faster than first order methods</strong> as mentioned before</li>
</ul>

<hr />

<p>The same formula can be derived using Taylor’s methods as well</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000201474.png" alt="image-20220126000201474" style="zoom: 67%;" /></p>

<p>However, this is <strong>useful</strong> because it guides on how to deal with ==vector input functions $f(\vec{x})$== which we need to deal with:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000337166.png" alt="image-20220126000337166" style="zoom: 67%;" /></p>

<p>where the last step is basically our new update rule.</p>

<ul>
  <li>
    <p>note that this $H^{-1}$ basically <strong>takes place of the $\alpha_t$</strong> we had in first order methods</p>
  </li>
  <li>
    <p>therefore the algorithm is:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000628969.png" alt="image-20220126000628969" style="zoom:67%;" /></p>
  </li>
</ul>

<p>However, some problem resides:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126000538635.png" alt="image-20220126000538635" style="zoom:80%;" /></p>

<p>notice that computing and inverting Hessian takes lots of computation, and storing Hessian takes space!</p>

<h3 id="quasi-newton-methods">Quasi-Newton Methods</h3>

<p>Quasi-Newton methods, which provide an iterative <strong>approximation</strong> to the inverse Hessian $H^{-1}$, so that it may:</p>

<ul>
  <li>avoid computing the second derivatives</li>
  <li>avoid inverting the Hessian</li>
  <li>may also avoid storing the Hessian matrix.</li>
</ul>

<p>The idea is to start thinking exactly what we need to approximate. Our goal is anyway the <strong>iterative update</strong>:</p>

\[x^+ = x - B^{-1}\nabla f(x)\]

<p>so our goal is to ==approximate $B^{-1} \approx \nabla^2 f$==. The task is therefore find some <strong>conditions to calculate $B$</strong></p>

<p>By definition of second derivative, we know that:</p>

\[\nabla f(x^k+s^k) - \nabla f(x^k) \approx B^k s^k\]

<p>where:</p>

<ul>
  <li>$s^k$ is the <strong>step size</strong> at iteration $k$</li>
  <li>$B^k$ is our approximation of Hessian/second derivative at step $k$</li>
</ul>

<p>Now, since it will be an approximation, we want to impose some constraints to make the approximation good:</p>

<ol>
  <li>
    <p>Second equation <strong>for next $B$</strong> should hold eaxctly:</p>

\[\nabla f(x^{k+1}) - \nabla f(x^k) =  B^{k+1}s^k\]

    <p>where $x^{k+1} = x^k+s^k$. This will be then represented as:</p>

\[B^{k+1}s^k = y^k\]

    <p>for $\nabla f(x^{k+1}) - \nabla f(x^k)  \equiv y^k$, or even more simply:</p>

\[B^{+}s = y\]
  </li>
  <li>
    <p>We also want the following <strong>desirable properties</strong></p>

    <ul>
      <li>$B^+$ is symmetric, as Hessians are symmetric</li>
      <li>$B^+$ should be close to $B$, which is the previous approximation</li>
      <li>$B,B^+$ being positive definite</li>
    </ul>
  </li>
</ol>

<p>Now, we explore some <strong>approximations for $B^+$</strong> that attempts to satisfy the above constraint.</p>

<h4 id="sr1-update">SR1 Update</h4>

<p>This is the simpliest update procedure, such that $B^+$ can be close to $B$, and it will be symmetric:</p>

\[B^+ = B + a u u^T\]

<p>for some $a,u$ we will solve soon. Notice that if we let this be our <strong>update rule for $B$</strong> (first iteration just initialize $B=I$), and we have the ==enforcement that secant equation should hold==:</p>

\[y=B^+s = Bs + a u u^Ts=Bs + (au^Ts)u\]

<p>for $s, u, y$ all being vectors. Notice that this means:</p>

\[y - Bs = (au^Ts)u\]

<p>where:</p>

<ul>
  <li>both sides of the equation are <strong>vectors</strong>! This means that $u$ is a scalar multiple of $y-Bs$.</li>
</ul>

<p>So we can ==solve for $u,a$==, and obtain the solution and <strong>plug back into our update rule for $B^+$</strong>:</p>

\[B^+ = B + \frac{(y-Bs)(y-Bs)^T}{(y-Bs)^Ts}\]

<p>where at iteration $k$, we already know $B\equiv B^k, s \equiv s^k$ and $y$, so we can compute $B^+$ at iteration $k$.</p>

<hr />

<p><em>Just to be clear</em>, using the above formula our <strong>descent algorithm</strong> would be:</p>

<p><strong>At iteration $k$</strong></p>

<ol>
  <li>compute $(B^{k})^{-1} \nabla f(x^k)$</li>
  <li>do the descent $x^{k+1} = x^k - \alpha_k (B^{k})^{-1} \nabla f(x^k)$ for some tunable parameter $\alpha_k$</li>
  <li>prepare $B^{k+1}$ using the above formula.</li>
</ol>

<hr />

<p>Now, while this technically <strong>computes the approximation</strong>, we can make the algorithm even better by <strong>directly computing $H = B^{-1}$</strong> and its updates using the above formula for $B^+$.</p>

<p>Using the following theorem:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220130161458549.png" alt="image-20220130161458549" /></p>

<p>We can show that $(B^+)^{-1}$ <strong>can be computed directly</strong></p>

\[(B^+)^{-1} = H^+ = H + \frac{(s-Hy)(s-Hy)^T}{(s-Hy)^Ty}\]

<p>then we just use $H$ and $H^+$ all the time instead of $B,B^+$ in the above algorithm.</p>

<h4 id="other-approximations">Other Approximations</h4>

<p>The <strong>David-Fletcher-Powell (DFP)</strong> correction is defined by</p>

\[H^+ = H+ \frac{ss^T}{y^Ts} - \frac{(Hy)(Hy)^T}{y^T(Hy)}\]

<p>The <strong>Broyden-Fletcher-Goldfarb-Shannon</strong> (BFGS) is defined by:</p>

\[H^+ = H+ \frac{2(Hy)s^T}{y^T(Hy)} - \left( 1 + \frac{y^T s^T}{y^T(Hy)} \right)\frac{(Hy)(Hy)^T}{y^T(Hy)}\]

<p>In summary, they all attempt to approximate the real Hessian, which is expensive in computation.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>These methods are similar to each other in that they all begin by initializing the inverse Hessian to the identity matrix and then iteratively update the inverse Hessian. These three update rules differ from each other in that their <strong>convergence properties improve upon one another</strong>.</p>
</blockquote>

<h2 id="evolution-strategies">Evolution Strategies</h2>

<p>In contrast to gradient descent methods which advance a single point towards a local minimum, evolution strategies update
a probability distribution, from which multiple points are sampled, lending itself to a highly efficient distributed computation</p>

<blockquote>
  <p><strong>Useful resource</strong></p>

  <ul>
    <li>https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html#simple-gaussian-evolution-strategies</li>
  </ul>
</blockquote>

<p>Intuition: Instead of updating a single initial point and go downhill, use <strong>a distribution of points</strong> to go downhill</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220125140715802.png" alt="image-20220125140715802" /></p>

<h3 id="simple-gaussian-evolution-strategies">Simple Gaussian Evolution Strategies</h3>

<p>at each iteration, we sample from distribution and update that distribution</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126002014142.png" alt="image-20220126002014142" /></p>

<h3 id="covariance-matrix-adaptation">Covariance Matrix Adaptation</h3>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/CMA-ES-algorithm.png" alt="CMA-ES Algorithm" style="zoom: 33%;" /></p>

<p>Example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220126002131825.png" alt="image-20220126002131825" style="zoom: 67%;" /></p>

<h3 id="natural-evolution-strategies">Natural Evolution Strategies</h3>

<p><img src="https://lilianweng.github.io/lil-log/assets/images/NES-algorithm.png" alt="NES" style="zoom: 33%;" /></p>

<h1 id="regularization-1">Regularization</h1>

<p>Regularization is a technique that helps prevent over-fitting by <strong>penalizing the complexity</strong> of the network. Often, we want our model to achieve <strong>low bias and low variance</strong> for ==test set==:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131208903.png" alt="image-20220127131208903" style="zoom: 50%;" /></p>

<p>where</p>

<ul>
  <li>our final aim is to have the model generalize to <strong>unseen data</strong>.</li>
</ul>

<blockquote>
  <p>Why <strong>overfitting</strong> happens? In general it is because your ==training dataset is not representative of all the trends in the population==, so that you could fit too much to the training data and <strong>miss the real “trends”</strong> in the population.</p>

  <ul>
    <li>hence, it cannot generalize to test sets well</li>
  </ul>
</blockquote>

<hr />

<p><em>Recall that</em></p>

<p>Bias and variance are basically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127164739968.png" alt="image-20220127164739968" style="zoom:50%;" /></p>

<p>and intuitively:</p>

<ul>
  <li><strong>bias</strong>: error introduced by approximating a complicated true model by a simpler model</li>
  <li><strong>variance</strong>: amount by which our approximation/model would change for different training sets</li>
</ul>

<p>e.g. an unbiased estimator would have:</p>

\[\mathbb{E}_{\vec{x}\sim \mathcal{D}}[\hat{\theta}(\vec{x})] = \lang \hat{\theta}(\vec{x}) \rang = \theta\]

<p>which is different from consistency:</p>

\[\lim_{n \to \infty} \hat{\theta}_n(\vec{x}) = \theta\]

<p>In reality, NN does better than traditional ML models such as SVM by being more complicated:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131541371.png" alt="image-20220127131541371" style="zoom: 67%;" /></p>

<hr />

<p>Main <strong>regularization techniques</strong> here include:</p>

<ol>
  <li>add a penalty term of the weights $W$ directly to the loss function</li>
  <li>use dropouts, which randomly “disables” some neuron during training</li>
  <li>augment the data</li>
</ol>

<h2 id="generalization">Generalization</h2>

<p>Training data is a sample from a population and we would like our neural network model to ==generalize well to unseen test data== drawn from the same population.</p>

<p>Specifically, the <strong>definition of generalization error</strong> would be the difference between the <strong>empirical loss</strong> and <strong>expected loss</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127171220853.png" alt="image-20220127171220853" style="zoom: 67%;" /></p>

<p>where in Machine Learning, you would have seen something like this:</p>

\[\mathrm{err}(f) := \mathbb{P}_{(x,y)\sim \mathcal{D}}[f(x) \neq y]\]

<p>here:</p>

<ul>
  <li>
    <p>$f$ is a given model to test, and $\text{err}(f)$ is the generalization error.</p>
  </li>
  <li>
    <p>so it is technically equivalent to the red highlighted box only. Yet the whole expression resembles the PAC learning criterion:</p>

\[\mathrm{err}(f_m^A) - \mathrm{err}(f^*) \le \epsilon\]

    <p>where $f^<em>$ is the optimal predictor in the class $\mathcal{F}$, such that $f^</em> =\arg\min_{f \in \mathcal{F}}\mathrm{err}(f)$.</p>

    <ul>
      <li>however, notice that it is <strong>not</strong>, because here we are computing $\text{err}$ which is “generalization error” for both, instead of computing sample error.</li>
    </ul>
  </li>
</ul>

<p>The difference between the two is <strong>important</strong>:</p>

<ul>
  <li>suppose $G(f(X,W))  = 0$ for our model $f$. Then it means our empirical loss is as good as the expected loss. This only implies that our model has done the “best it could”.</li>
  <li>but suppose $\mathrm{err}(f)=0$, this means that our model is performing perfectly on the population, which I think is a stronger statement than the above.</li>
</ul>

<blockquote>
  <p>The key idea is that, if <strong>generalization error is low</strong>, then our model is not overfitting (doing the same performance for both train and test dataset)</p>

  <ul>
    <li>though this metric is ==not computable== since we don’t have the population, there are various ways to “estimate” it, like doing Cross Validation with many folds. (<a href="#Cross Validation">Cross Validation</a>)</li>
  </ul>

  <p>However, it is important to remember that <strong>generalization error</strong> depends on ==both variance and bias== (and noise) of the model/dataset:</p>

\[\mathbb{E}[y-\hat{f}(x)]^2 =  \text{Var}[\hat{f}] + ( \text{Bias}(\hat{f}))^2 + \text{Var}[\epsilon]\]

  <p>where $\text{Var}[\epsilon]$ is the variance of the <strong>noise of the data</strong>, i.e. $y = W^Tx + \epsilon$ if you think about regression.</p>

  <ul>
    <li>therefore, this is why we want to ==reduce bias and variance==!</li>
  </ul>
</blockquote>

<p>In practice, we see things like this:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127131908115.png" alt="image-20220127131908115" style="zoom:67%;" /></p>

<p>where usually:</p>

<ul>
  <li>Adding more training data $(X, Y)$ increases the generalization accuracy until a limit, i.e. the best our model can do anyway $\neq$ the optimal bayes</li>
  <li>Unless we have sufficient data, a very complex neural network may <strong>fit the training data very well at the expense of a poor fit to the test data</strong>, resulting in a large gap between the training error and test error, which is over-fitting, as shown above</li>
</ul>

<hr />

<p>However, in Deep Learning, recent results have shown:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127132009196.png" alt="image-20220127132009196" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>for DL it seems that we are double descending, so we may want to “overfit”</li>
</ul>

<h3 id="cross-validation">Cross Validation</h3>

<blockquote>
  <p><strong>Cross validation</strong> allows us to estimate the <strong>mean and variance of the generalization error</strong> using our limited data.</p>

  <ul>
    <li>mean generalization error is the average of the generalization error over all $k$ models and is a good ==indicator for how well a model performs on unseen data==</li>
  </ul>
</blockquote>

<p>In short, the idea is simple. We randomly split the data <strong>into $k$ folds</strong></p>

<ol>
  <li>take the $i$-th fold to be testing, and the rest, $k-1$ folds being training data</li>
  <li>learn a model $f_i$</li>
  <li>compute generalization error of $f_i$</li>
  <li>repeat 1-3 for $k$ times, but with a <strong>new $i$</strong></li>
</ol>

<p>This is useful because now we can compute the <strong>mean and variance of generalization error</strong> using the $k$ different models we trained.</p>

<ul>
  <li>i.e. think of the evaluation metric being applied on your <em>model choice/architecture</em>, hence the need of mean/variance so it doesn’t depend much on “which data is chosen to be training”</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>We can also use cross validation to select features for building a model. We can build many models with <strong>different subsets of features</strong> and then compute their mean and variance of the generalization error to determine ==which subset performs best==.</p>
</blockquote>

<h2 id="regularization-methods">Regularization Methods</h2>

<p>Our general goal is to <strong>reduce both bias and variance</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127190032359.png" alt="image-20220127190032359" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>“better optimization” means using a better optimizer, as covered in the previous chapter</li>
  <li>here, our focus is on the bottom right part: <strong>regularize</strong></li>
</ul>

<p>Some main methods we will discuss</p>

<ol>
  <li>add a penalty term of the weights $W$ directly to the loss function, usually using the <strong>norm</strong></li>
  <li>use <strong>dropouts</strong>, which randomly “disables” some neuron during training</li>
  <li><strong>augment</strong> the data</li>
</ol>

<blockquote>
  <p>Different methods have different specific effects technically, though the overall effect is that they reduce overfitting.</p>
</blockquote>

<h3 id="vector-norms">Vector Norms</h3>

<p>We define vector norms before discussing regularization using different norms.</p>

<p>For all vectors $x$, $y$ and scalars $\alpha$ all vector norms <strong>must satisfy</strong></p>

<ol>
  <li>$\vert \vert x\vert \vert  \ge 0$ and $\vert \vert x\vert \vert =0$ iff $x = 0$</li>
  <li>$\vert \vert x+y\vert \vert  \le \vert \vert x\vert \vert  + \vert \vert y\vert \vert$</li>
  <li>$\vert \vert \alpha x\vert \vert  = \vert \alpha\vert  \,\vert \vert x\vert \vert$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Under this definition:</p>

  <ul>
    <li>$L_p$ for $p &lt; 1$ will have non-convex shapes</li>
    <li>$L_0$ does not count as a norm</li>
  </ul>
</blockquote>

<p>The general equation is simply:</p>

\[||x||_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}\]

<p>Some most common norms include:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127191601903.png" alt="image-20220127191601903" /></p>

<h3 id="regularized-loss-functions">Regularized Loss Functions</h3>

<p>One way to regularize it to use <strong>regularized loss function</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127132745260.png" alt="image-20220127132745260" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>$\lambda$ would be tunable.
    <ul>
      <li>If $\lambda$ = 0, then no regularization occurs.</li>
      <li>If $\lambda$ = 1, then all weights are penalized equally.</li>
      <li>A value between $0$ and $1$ gives us a tradeoff between fitting complex models and fitting simple models</li>
    </ul>
  </li>
  <li>Notice that here $R(w)$ refers to regularizing the <strong>entire weight of the network $W$</strong>.
    <ul>
      <li>if you want to have different regularization for different layers, you need to do $R_1(W^1)+R_2(W^2)$ in the final loss term.</li>
    </ul>
  </li>
  <li>Common types of regularization are $L_1$ and $L_2$
    <ul>
      <li>$L_1$ regularization is a penalty on the sum of absolute weights which <strong>promote sparsity</strong></li>
      <li>$L_2$ regularization is a penalty on the sum of the squares of the weights which prefer <strong>feature contribution being distributed evenly</strong></li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>If we are using SGD, and we added a $L_2$ regularization would cause our <strong>gradient update rules to change</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127133353173.png" alt="image-20220127133353173" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>the extra term is due to regularization. so if $\lambda \to 0$, we get back to regular SGD</li>
  <li>also confirms that $W_t$ in general are smaller if we have regularization</li>
</ul>

<h4 id="ridge-and-lasso-regression">Ridge and Lasso Regression</h4>

<p>Recall that in machine learning, the following is <strong>Ridge Regression</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192337546.png" alt="image-20220127192337546" style="zoom: 67%;" /></p>

<p>The solution for the ridge ==regression== can be solved exactly:</p>

\[\vec{w}_{ridge}=(X^TX + \lambda I)^{-1}X^T \vec{y}\]

<p>which basically comes from taking the derivative of the objective and setting it to zero, and note that:</p>

<ul>
  <li>this matrix $X^TX + \lambda I$ is exactly ==invertible== since it is now <strong>positive definite</strong> (because we added some positive number to diagonal)</li>
  <li>since $X^TX + \lambda I$ is invertible, this always result in a ==unique solution==.</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>analytic solution doesn’t exist for ==DL==, since our prediction is no longer:</p>

\[\hat{y} = XI\beta\]

      <p>which is for simple linear regression.</p>

      <p>But in DL, we have a NN with many nonlinear functions nested like:</p>

\[\hat{y} = f_3(W^3 \, f_{2}(W^2\,f_{1}(W^1X)))\]

      <p>where each layer $f$ are the activation functions for each layer. The solution of this is no longer analytic.</p>
    </li>
  </ul>
</blockquote>

<p>Yet, since this problem can be converted to the <strong>constraint optimization problem</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192522192.png" alt="image-20220127192522192" style="zoom: 67%;" /></p>

<p>using <strong>Lagrange Method</strong>, then, the problem basically looks like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Objective Function</th>
      <th style="text-align: center">Contour Projection into $w$ Space</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192856939.png" alt="image-20220127192856939" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192905141.png" alt="image-20220127192905141" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><em>Recall</em>: Lagrange Penalty Method</p>

  <p>Consider the problem of:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192658890.png" alt="image-20220127192658890" /></p>

  <p>This problem will be the same as minimizing the augmented function</p>

\[L(\vec{x}, \vec{\lambda}) := f(\vec{x}) + \sum_{i=1}^n \lambda_i g_i(\vec{x})\]

  <p>and recall that :</p>

  <ul>
    <li>our aim was to minimize $f(\vec{x})$ ==such that $g_i(\vec{x}) \le 0$ is satisfied==</li>
    <li>$\vec{x}$ is the original variable, called primal variable as well</li>
    <li>$\lambda_i$ will be some new variable, called <strong>Lagrange/Dual Variables</strong>.</li>
  </ul>
</blockquote>

<hr />

<p>Similarly, if we use $L_1$ norm, then we have <strong>Lasso’s Regression</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127192950886.png" alt="image-20220127192950886" style="zoom:50%;" /></p>

<p>Geographically, we are looking at:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Actual Aim (Sparsity)</th>
      <th style="text-align: center">Lasso’s Approximation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193020129.png" alt="image-20220127193020129" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193026129.png" alt="image-20220127193026129" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Sadly,  there is <strong>no closed form solution</strong> even for simple regression in this case.</p>

<h3 id="dropout-regularization">Dropout Regularization</h3>

<p>The idea is that we <strong>randomly dropout neurons</strong> by setting their activations to be $0$.</p>

<ul>
  <li>
    <p>this will reduce cause the training to be less accurate, but makes it more robust in overfitting as those neurons “won’t fit all the time” to the data</p>
  </li>
  <li>
    <p>this is done in training only. When testing, we don’t drop them out</p>
  </li>
</ul>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Fully connected neural network</th>
      <th style="text-align: center">Dropout regularization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193519465.png" alt="image-20220127193519465" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193527248.png" alt="image-20220127193527248" /></td>
    </tr>
  </tbody>
</table>

<p>Since activations are randomly set to zero during training, we basically <strong>implement it by adding a layer before after activation</strong></p>

\[a_j^l := a_j^l I_{j}^l\]

<p>where $I_j^l$ is like a <strong>mask, deciding whether if it will be dropped</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127193726263.png" alt="image-20220127193726263" style="zoom: 67%;" /></p>

<p>Intuitively:</p>

<ul>
  <li>over iterations, some neurons will be dropped -&gt; less overfitting on those neurons. In some other cases, those neurons will need to stand in for others.</li>
  <li>
    <p>overall, we want to keep the <strong>same magnitudes of “neurons”</strong> for that layer even if we dropped out, hence $1/(1-p_l)$ scale up, so that they ==stand in== for the dropped out neurons</p>
  </li>
  <li>$p_l$ is a hyper-parameter. In some framework we can set it, in some other like <code class="language-plaintext highlighter-rouge">keras</code> it is ==automatically tuned==</li>
</ul>

<p>To implement it in code, we use a mask:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
    <span class="c1"># a mask
</span>    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">&gt;</span> <span class="n">prob</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span>
    <span class="n">M</span> <span class="o">/=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prob</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">*=</span> <span class="n">M</span> <span class="c1"># applying the mask
</span>    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">M</span>
</code></pre></div></div>

<p>note that:</p>

<ul>
  <li>forward propagation: apply and <em>store the mask</em></li>
  <li>backward propagation: <em>load the mask</em> and apply derivatives
    <ul>
      <li>since $a_j^l := a_j^l I_{j}^l$, then backpropagation equation needs to be updated as well</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Examples</em></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127134329966.png" alt="image-20220127134329966" style="zoom:67%;" /></p>

<h4 id="least-square-dropout">Least Square Dropout</h4>

<p>Dropout is actually not completely new</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127194559956.png" alt="image-20220127194559956" style="zoom:67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>the solution is <strong>exact</strong></li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>again, analytic solution doesn’t exist for DL, since our prediction is not a simple linear regression but concatenating a bunch of nonlinear operations as well</li>
  </ul>
</blockquote>

<h4 id="least-squares-with-noise-input">Least Squares with Noise Input</h4>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127194543146.png" alt="image-20220127194543146" style="zoom:67%;" /></p>

<h3 id="data-augmentation">Data Augmentation</h3>

<blockquote>
  <p><strong>Data augmentation</strong> is the process of ==generating new data points by transforming existing ones==.</p>

  <ul>
    <li>For example, if a dataset has a lot of images of cars, data augmentation might generate new images by rotating them or changing their color. Then, it is used to <strong>train a neural network</strong></li>
  </ul>
</blockquote>

<p>Data augmentation may be used to <strong>reduce overfitting</strong>. Overfitting occurs when a model is too closely tailored to the training data and does not generalize well to new data. Data augmentation can be used to generate new training data points that are similar to the existing training data points, but are not identical copies.</p>

<ul>
  <li>This helps the model <strong>avoid overfitting</strong> and generalize better to new data.</li>
</ul>

<p>The general idea here is that we augment the training data by replacing each example pair with a <strong>set of pairs</strong></p>

\[(x_i, y_i) \to  \{(x_i^{*^b} , y_i)\}_{b=1}^B\]

<p>by, transformations including</p>

<ul>
  <li>e.g. rotation, reflection, translation, shearing, crop, color transformation, and added noise.</li>
</ul>

<h4 id="input-normalization">Input Normalization</h4>

<p>This is simply to normalize the input <strong>in the beginning</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127142128055.png" alt="image-20220127142128055" /></p>

<p>Then perform:</p>

\[\hat{x} = \frac{x- \mu}{\sigma}\]

<h4 id="batch-normalization">Batch Normalization</h4>

<p>Batch normalization basically <strong>standardizes the inputs</strong> to a layer <strong>for each mini-batch</strong>. You can think of this as doing normalization for each layer, for each batch</p>

<ul>
  <li>advantage: avoid exploding/vanishing gradients if the inputs are small!</li>
  <li>usually not only normalizing the input, but also for each <strong>layer</strong> over and over again.</li>
</ul>

<p>So basically, <strong>for input of next layer</strong>:</p>

\[Z = g(\text{BN}(WA))\]

<p>where $\text{BN}$ is doing batch normalization. In essence:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127195826143.png" alt="image-20220127195826143" style="zoom: 80%;" /></p>

<p>note that:</p>

<ul>
  <li>this means you <strong>backpropagation equation</strong>/derivatives needs also to include that term.</li>
</ul>

<h2 id="uncertainty-in-dnns">Uncertainty in DNNs</h2>

<p>Sometimes we also want to measure the <strong>confidence about the outputs</strong> of a neural network.</p>

<ul>
  <li>for instance, confidence in our learnt paramotor $\theta$</li>
</ul>

<p>In theory, we want ask the question: <strong>What is the distribution over weights $\theta$ given the data?</strong> (from which we know the confidence of our current learnt parameter)</p>

\[p(\theta |x,y) = \frac{p(y|x,\theta) p(\theta)}{p(y|x)}\]

<p>where $p(\theta)$ would be the <strong>prior</strong>, and $p(\theta\vert x,y)$ would be the <strong>posterior</strong> since we see the data.</p>

<ul>
  <li>this is not possible to compute since we don’t know them.</li>
</ul>

<p>In practice, we can roughly compute <strong>confidence of current prediction</strong> by “dropout masks”</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127135347957.png" alt="image-20220127135347957" style="zoom: 40%;" /></p>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127135432128.png" alt="image-20220127135432128" /></p>

<p>so we are not only <strong>outputting predictions</strong>, but also <strong>outputting confidence of our predictions</strong></p>

<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>

<p>The major aim of this section is to discuss <strong>models that solves ML problems with images</strong></p>

<p>A brief overview of what we will discuss:</p>

<ul>
  <li>CNN is basically a “preprocessing neural network” that replaces Linear part from $W^lA^{l-1}$ to <strong>convolution with kernel</strong> (which is also linear)</li>
  <li>problems with deeper CNN layers causes vanishing gradients, hence models such as Residual NN and DenseNet are introduced</li>
</ul>

<p>On a high level, we should know that treating images means our input vector would be <strong>large in size</strong>, with $n$ dimension (after flattening) means that, if we use vanilla model, we need $O(n n_{1})$ matrix for the first layer with $n$ neurons!</p>

<p>Then, CNN aims to</p>

<ul>
  <li>deal with this storage/computation problem by using <strong>sparse matrix (kernel)</strong>, which are essentially matrices with ==repeated elements==.</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201170511507.png" alt="image-20220201170511507" style="zoom:80%;" /></p>

<p>since the output is size $3$, it means we basically have <strong>3 neurons</strong> essentially having ==the same weight== ($k_1, k_2, k_3$). Therefore, we also call this <strong>sharing weights across space</strong>.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201142122782.png" alt="image-20220201142122782" style="zoom: 50%;" /></p>

<p>notice that in this case, our number of parameters to learn is $O(3)=O(1)$ is <strong>constant</strong>!</p>

<ul>
  <li>
    <p>another problem of CNN is to encode <strong>spatial and local information</strong>, which would be otherwise lost if we <strong>directly flatten it</strong> and pass it onto a normal NN.</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201133600948.png" alt="image-20220201133600948" style="zoom: 33%;" /></p>

    <p>where you will see the aim of kernels would be that they <strong>captures features such as edges/texture/objects</strong>, which obviously has spatial relationships in the image.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>The fact that weights learnt in CNN preprocessing part of the architecture are constant can be thought of as the <strong>result of the constraints we placed on those weights</strong>: we need them to be kernels, hence we need <em>symmetry, sparsity, and the particular output shape</em> as shown above.</p>
    </li>
    <li>
      <p>After using the kernel, CNN architecture then would add a bias and an activation, all of which would assemble the actions taken in <strong>one layer</strong>.</p>

      <ul>
        <li>essentially the linear $W^l A^{l-1}$ is replaced by convolution</li>
        <li>each filter in a layer is of effectively $O(1)$ in size, but we can learn <strong>multiple such filters</strong>. Finally it may look like this</li>
      </ul>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201171758990.png" alt="image-20220201171758990" /></p>

      <p>where notice that:</p>

      <ul>
        <li>the output after <strong>several such layers</strong> will be <strong>piped to a normal NN</strong>, for instance, for the final specific classification tasks.</li>
        <li>subsampling are basically techniques such as <strong>pooling</strong>, which will be covered later. The aim is to <strong>reduce the dimension (width $\times$ height)</strong>.</li>
        <li>To reduce the number of channels, you can use $f$ number of $1\times 1\times c$ filters, which can reduce to $f$ channels.
          <ul>
            <li>make sense since doing $1\times 1\times c$ convolution is telling how to sum the pixel on each channel into $1$ single pixel.</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="convolution">Convolution</h2>

<p>Basically it is nothing than doing:</p>

<ul>
  <li>elementwise multiplication between a patch of a matrix and a filter/kernel</li>
  <li>summing them up</li>
</ul>

<p>Therefore, convolutions in any dimension can be represented as a <strong>matrix vector multiplication</strong>:</p>

\[k * x = Kx_{\text{flatten}}\]

<p>where:</p>

<ul>
  <li>$K$ is the kernel, and $x_{\text{flatten}}$ is the <strong>flattened</strong> version of the image $x$.</li>
  <li>the exact shape of $K$ would be interesting. Think about how you would realize the above equation.</li>
</ul>

<h3 id="one-dimensional-convolution">One-Dimensional Convolution</h3>

<p>The idea is simple, if we are given a <strong>1D vector</strong> and a <strong>1D kernel</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201172613937.png" alt="image-20220201172613937" style="zoom:67%;" /></p>

<p>so essentially it is a <strong>locally weighted sum</strong>. To think about how we <strong>represent this in linear algrebra</strong>, consider that we have a $1 \times 5$ input with size $3$ kernel:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201172859269.png" alt="image-20220201172859269" /></p>

<p>verify that the above works.</p>

<ul>
  <li>notice that the output is of dimension $3$. This <strong>must be the case</strong> because there are only $3$ unique positions to place the size $3$ filter inside the size $5$ input vector.</li>
  <li>this matrix is also called Koeplitz matrix</li>
</ul>

<p>Therefore, we can reason this as:</p>

\[k * x = Kx = \sum_{i=1}^3 k_iS_i x\]

<p>where $S_i$ are the matrices where only “diagonal” entries are ones, otherwise zeros.</p>

<p>Now, one problem is that we noticed the <strong>output size is smaller</strong>, which can be bad in some cases. We can fix this by adding padding to the edges:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201173324819.png" alt="image-20220201173324819" /></p>

<p>however:</p>

<ul>
  <li>one problem with <strong>zero padding</strong> is that it introduces <strong>discontinuities at boundaries</strong></li>
  <li>another technique is to pad with <strong>reflection</strong>, i.e. replacing the top $0 \to x_1$, and bottom $0 \to x_5$.</li>
</ul>

<hr />

<p><em>For Example</em>: Stride with size 2</p>

<p>The above all assumed a stride with size 1. We can perform the task with stride 2 by doing</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174111406.png" alt="image-20220201174111406" /></p>

<p>this could be useful as the <strong>output size is decreased</strong> by a factor of $2$.</p>

<hr />

<p><em>For Example</em>: A Simple Single Conv Layer</p>

<p>A typical layer looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209143859413.png" alt="image-20220209143859413" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ol>
  <li>pass the 1D image vector to filter (<strong>replacing the linear part</strong> to $Kx$)
    <ul>
      <li>optionally you would then also add the <strong>bias</strong> to the output $Kx + b$</li>
      <li>notice that this linear operation has a <strong>very sparse matrix</strong>, $K$</li>
    </ul>
  </li>
  <li>shortened version of the vector then goes through <strong>activation</strong></li>
</ol>

<p>This particular setup in the end can detect <strong>any block of lonely $1$</strong> in the input.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The fact that we are applying a kernel <strong>everywhere the same</strong> is so that it preserves the property that images are <strong>translational invariant</strong>.</p>
</blockquote>

<hr />

<p><em>Other Filters</em></p>

<p>Sharpening:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174625087.png" alt="image-20220201174625087" style="zoom:50%;" /></p>

<p>more filters are omitted.</p>

<h3 id="multi-dimensional-convolution">Multi-Dimensional Convolution</h3>

<p>Since our images are usually <strong>2D</strong> if grey scale, so we extend the convolution to a 2D kernel.</p>

<ul>
  <li>the pattern you will see is easily generalizable to 3D inputs as well.</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174800911.png" alt="image-20220201174800911" style="zoom: 67%;" /></p>

<p>Then, if we want to <strong>add paddings</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201174906311.png" alt="image-20220201174906311" style="zoom:67%;" /></p>

<p>But more importantly, we can <strong>put this in a matrix vector multiplication as well</strong>:</p>

<ul>
  <li>flattening 2D matrix to $[x_{11}, x_{12}, …, x_{nm}]^T$.</li>
  <li>the shape of kernel would be <strong>repeatedly assembling 1D filters</strong></li>
</ul>

<p>Consider the following operatoin:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175143906.png" alt="image-20220201175143906" style="zoom:67%;" /></p>

<p>Can be done by:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175349503.png" alt="image-20220201175349503" /></p>

<p>which is basically:</p>

<ul>
  <li>the lowest diagonal is the <strong>1D Toeplitz matrix for the first row of $k$</strong></li>
  <li>the second lowest diagonal is the <strong>1D Toeplitz matrix for the second row of $k$</strong></li>
  <li>etc.</li>
  <li>finally, the <strong>output is a vector</strong>, which can be interpreted as a flattened 2D image</li>
</ul>

<blockquote>
  <p>Therefore, convolution with 3D images using 3D kernels, basically is equivalent of matrix-vector multiplication with:</p>

  <ul>
    <li>flattened image to 1D</li>
    <li>repeatedly assembling <strong>2D Toeplitz matrix</strong> for the “$i$-th place” to form a 2D matrix.</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em></p>

<p>To find the lonely one:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201135357568.png" alt="image-20220201135357568" style="zoom:33%;" /></p>

<hr />

<h4 id="three-dimensional-convolution">Three Dimensional Convolution</h4>

<p>The technique of expanding 2D Toeplitz matrix for 3D convolution basically does the following for convolution:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182111825.png" alt="image-20220201182111825" style="zoom:80%;" /></p>

<p>which basically <strong>outputs a single 2D matrix</strong>.</p>

<ul>
  <li>makes sense that the <strong>number of channels</strong> in both kernel and input lines up, as in the end we just do a element-wise multiplication and sum up.</li>
</ul>

<p>However, <strong>another way</strong> would be to do ==two dimensional convolution on each channel==</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182357125.png" alt="image-20220201182357125" style="zoom:80%;" /></p>

<p>notice that:</p>

<ul>
  <li>this means we would have 3 filters (may be the same), each is a <strong>2D matrix/kernel/filter</strong></li>
  <li>the number of channels for both kernel and their respective input is $1$.</li>
  <li><strong>outputs 3 channels instead of $1$</strong>, as compared to the previous case</li>
</ul>

<h3 id="properties-of-convolution">Properties of Convolution</h3>

<p>There are several nice properties of convolution that are handy for <strong>optimizing computation complexity</strong>.</p>

<p>First, the most obvious ones are due to <strong>convolution</strong> are essentially matrix/vector multiplication as they are from linear algebra</p>

<ul>
  <li><strong>Commutative</strong>: $f*g = g * f$</li>
  <li><strong>Associative</strong>: $f<em>(g</em>h) = (f*g) * h$</li>
  <li><strong>Distributive</strong>: $f*(g + h) = f * g + f * h$</li>
  <li><strong>Differentiation</strong>: $\frac{d}{dx} (f * g) = \frac{df}{dx}* g = f * \frac{dg}{dx}$</li>
</ul>

<h4 id="separable-kernels">Separable Kernels</h4>

<p>Some kernels would be separable like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201135511859.png" alt="image-20220201135511859" style="zoom: 50%;" /></p>

<p>Then, we can use the <strong>property that</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201175857580.png" alt="image-20220201175857580" style="zoom: 80%;" /></p>

<p>where this is <strong>very useful because</strong>, if the image is size $n \times n$, and separable kernel $k \times k$</p>

<ul>
  <li>directly convovling needs $O(n^2 k^2)$, since each of the $\approx n^2$ output pixel needs $k^2$ computation.</li>
  <li>if we do it with <strong>two simpler convolutions</strong>, then $O(2n^2k)=O(n^2k)$ which is better</li>
</ul>

<h4 id="composition">Composition</h4>

<p>Since we know convolutions is basically matrix-vector multiplication: Repeated convolutions with a small kernel are equivalent to a single convolution with a large kernel</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201180550990.png" alt="image-20220201180550990" style="zoom:80%;" /></p>

<p>where this is useful because it is more efficient.</p>

<h2 id="convolutional-layers">Convolutional Layers</h2>

<p>Now, we discuss what happens in convolutional layers in a NN such as the following</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201171758990.png" alt="image-20220201171758990" /></p>

<p>Notice that we know:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182741514.png" alt="image-20220201182741514" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>convolution of $n \times n \times 3$ with a <strong>single kernel $k \times k \times 3$</strong> produces $n \times n$ (if we have padding)</p>
  </li>
  <li>
    <p>if prepare $4$ different $k \times k \times 3$ kernel and we do this ==separately for $4$ times==, we get $n \times n \times 4$ output</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201182937454.png" alt="image-20220201182937454" style="zoom: 67%;" /></p>
  </li>
</ul>

<p>Therefore, since in general <strong>a single layer would have many different filters</strong>, if we have $f$ filters, we would produce $n \times n \times f$ as our output size.</p>

<ul>
  <li>however, before we put this through activation, notice that having $n \times n \times f$ is pretty large. (in practice the performance gain is not too huge, so it doesn’t matter if we put it before or after activation)</li>
</ul>

<p>So, before activation, we would use <strong>pooling techniques</strong> to reduce the dimension.</p>

<h3 id="pooling">Pooling</h3>

<p>Pooling is an operation that reduces the dimensionality of the input. Some simple and common ones are</p>

<blockquote>
  <p><strong>Max pooling</strong> takes the maximum over image ==patches==.</p>

  <ul>
    <li>
      <p>for example over $2 \times 2$ grids of neighboring pixels $m =\max{x_1, x_2, x_3, x_4}$, hence <strong>reducing dimensionality in half</strong> in each spatial dimension as shown in Figure 5.18.</p>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201183419024.png" alt="image-20220201183419024" style="zoom:67%;" /></p>
    </li>
    <li>
      <p>notice that this <strong>does not reduce the number of channels!</strong></p>
    </li>
  </ul>
</blockquote>

<p>To reduce the number of channels, it is essentially saying that <strong>how do we want to sum pixels in different channel</strong>? Therefore, it makes sense that we can use a <strong>one-dimensional convolution</strong> to solve this</p>

<blockquote>
  <p><strong>One dimensional convolution</strong> with $f$ filters also allows reducing the number of channels to $f$ as shown in Figure 5.19.</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201183701494.png" alt="image-20220201183701494" style="zoom:80%;" /></p>

  <ul>
    <li>which makes sense as if $f=1$, essentially you summed over all channels, collapsing all channels to $1$ channel.</li>
  </ul>
</blockquote>

<h3 id="simple-convolutional-layer">Simple Convolutional Layer</h3>

<p>Putting everything above together, a typical convolutional layer involves <strong>three operations</strong>:</p>

<ul>
  <li>convolution with kernel (linear)</li>
  <li>pooling</li>
  <li>activation (nonlinear)</li>
</ul>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201184433823.png" alt="image-20220201184433823" style="zoom: 80%;" /></p>

<p>where, In this example, the input is a $28 \times 28$ grayscale image and the output is one of ten classes, such as the digits $0-9$</p>

<ul>
  <li>The first convolutional layer consists of $32$ filters, such as $5 \times 5$ filters, which are applied to the image with padding which yields a $28 \times 28 \times 32$ volume.</li>
  <li>Next, a non-linear function, such as the ReLU, is applied pointwise to each element in the volume</li>
  <li>The first convolution layer of the network shown above is followed by a $2 \times 2$ max pooling operation which reduces dimensionality in half in each spatial dimension, to $14 \times 14 \times 32$.</li>
  <li>then, go from $14 \times 14 \times 32$ to $14 \times 14 \times 64$, you would have <strong>64 filters</strong> of size $5 \times 5 \times 32$, for example</li>
</ul>

<h2 id="architectures">Architectures</h2>

<p>Now we talked about some of the <strong>modern architectures</strong> that builds up on the basic CNN we discussed before.</p>

<ul>
  <li>in fact, now as <strong>Vision Transformers</strong> are out, processing with images have now been mostly done using that as Transformers itself is quite a generic model</li>
</ul>

<h3 id="cnn">CNN</h3>

<p>A basic example of CNN is shown in this example</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203202818491.png" alt="image-20220203202818491" style="zoom: 67%;" /></p>

<ul>
  <li>A deeper network of <strong>eight layers may resemble the cortical visual pathways</strong> in the brain (Cichy, Khosla, Pantazis, Torralba &amp; Oliva 2016).</li>
  <li>note that we are doing activation <strong>then pooling here</strong>
    <ul>
      <li>max-pooling and monotonely increasing non-linearities commute. This means that $\text{MaxPool(Relu(x)) = Relu(MaxPool(x))}$ for any input.</li>
      <li>So the result is the same in that case. Technically it is better to first subsample through max-pooling and then apply the non-linearity (if it is costly, such as the sigmoid). <strong>In practice it is often done the other way round</strong> - it doesn’t seem to change much in performance.</li>
    </ul>
  </li>
  <li>the second Convolution Layer comes from applying 64 filters of size $k \times k \times 32$. Usually this can be <strong>abbreviated</strong> to say applying $k \times k$ dimension filters.</li>
  <li>Many early implementations of CNN architectures were <strong>handcrafted for specific image classification tasks</strong>. These include
    <ul>
      <li>LeNet (LeCun, Kavukcuoglu &amp; Farabet 2010),</li>
      <li>AlexNet (Krizhevsky, Sutskever &amp; Hinton 2012)</li>
      <li>VGGNet (Simonyan &amp; Zisserman 2014),</li>
      <li>GoogLeNet (Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich et al. 2015)</li>
      <li>Inception (Szegedy, Vanhoucke, Io↵e, Shlens &amp; Wojna 2016).</li>
    </ul>
  </li>
</ul>

<p>(but now, vision transformers are of big focus due to its <strong>generality</strong>)</p>

<h3 id="resnet">ResNet</h3>

<p>The <strong>deep residual neural network</strong> (ResNet) architecture (He, Zhang, Ren &amp; Sun 2016a), (He, Zhang, Ren &amp; Sun 2016b), introduced ==skip connections== between consecutive layers as shown below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Architecture</th>
      <th style="text-align: center">Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201190948331.png" alt="image-20220201190948331" style="zoom:80%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191036841.png" alt="image-20220201191036841" style="zoom:80%;" /></td>
    </tr>
  </tbody>
</table>

<p>The idea is simple, the <strong>pre-activation of a layer</strong> $z^l$ now has a <strong>residual term from previous layer</strong></p>

\[z^{l+1} = f(W^l,a^l)+a^l;\quad a^{l+1} = g(z^{l+1})\]

<p>instead of $z^{l+1} = f(W^l,a^l)$.</p>

<ul>
  <li>adding those skip connections/residual terms allow training deeper neural networks by <strong>avoiding vanishing gradients</strong>. The ResNet architecture enables training very deep neural networks with hundreds of layers.</li>
  <li>Adding a new layer to a neural network with a skip connection <strong>does not reduce its representation power</strong>. Adding a residual layer results in the network being able to represent all the functions that the network was able to represent before adding the layer plus additional functions, thus <strong>increasing the space of functions</strong>.</li>
</ul>

<p>For instance, a three layer network composition <strong>originally</strong> would look like</p>

\[F(x)=f(f(f(x)))\]

<p>Now it becomes, if each layer has a residual:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191454941.png" alt="image-20220201191454941" /></p>

<h3 id="densenet">DenseNet</h3>

<p>A DenseNet (Huang, Liu, van der Maaten &amp; Weinberger 2017) layer <strong>concatenates</strong> the input $x$ and output $f(x)$ of each layer to form the next layer $[f(x), x]$.</p>

<ul>
  <li>because it is concatenating it, the input in the first layer also <strong>directly appears in input to any further layers</strong></li>
  <li>in the ResNet, input in the first layer <strong>indirectly appears</strong> as they are absorbed in, such as  $f(f(x)+x)$</li>
</ul>

<p>Therefore, graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201191751076.png" alt="image-20220201191751076" style="zoom:80%;" /></p>

<p>And the formula composition for three layers look like</p>

\[F(x) =  f(f([f(x),x]),[f(x),x]), f([f(x),x]),[f(x),x]\]

<h2 id="understanding-cnns">Understanding CNNs</h2>

<p>Consider the model ImageNet, which has the following architecture</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192419470.png" alt="image-20220201192419470" style="zoom:80%;" /></p>

<p>When trained on a large dataset, since we also <strong>gradient descent to learn kernel/filters</strong> in CNN, we can look at the <strong>learnt kernels</strong> for different layers.</p>

<ul>
  <li>
    <p>In the end the kernel is just a <strong>matrix with some constraints</strong></p>
  </li>
  <li>
    <p>therefore, we can impose those constraints only and let <strong>back propagation</strong> to learn those weights</p>
  </li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192533577.png" alt="image-20220201192533577" style="zoom: 67%;" /></p>

<p>which, interestingly, coincides with many of the handcrafted ones we had before</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201140058415.png" alt="image-20220201140058415" style="zoom:33%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>now we learn filters, but we need to specify the architecture</li>
    <li>this is now superseded with vision transformer, which learns <strong>both the architecture</strong> and the kernel</li>
  </ul>
</blockquote>

<p>However, we are interested in knowing ==what patterns do each layer learn==. How do we do that?</p>

<h3 id="input-maximizing-activation">Input Maximizing Activation</h3>

<p>Consider transferring the above to an <strong>optimization problem</strong>: given trained network with weights $W$, ==find input $x$ which maximizes activation==.</p>

\[\arg\max_x a^l_{i}(W, x)\]

<p>which we can find by <strong>gradient ascent</strong>.</p>

<ul>
  <li>
    <p>e.g. given some kernel, doing gradient ascent gives:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201192921549.png" alt="image-20220201192921549" style="zoom: 67%;" /></p>

    <p>where the first steps are basically initializing with <strong>random noise</strong></p>
  </li>
</ul>

<p>Applying this technique to multiple layers, and we find that</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193034483.png" alt="image-20220201193034483" /></p>

<p>so basically:</p>

<ul>
  <li>first layers learn the edges</li>
  <li>then textures</li>
  <li>then objects</li>
</ul>

<p>Alternatively, you can also use this technique to find out <strong>what patch of images</strong> this kernel is bad at:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193141397.png" alt="image-20220201193141397" /></p>

<h3 id="transfer-learning">Transfer Learning</h3>

<p>The fact that those CNN learn fundamental concepts such as edges and textures means we can do <strong>transfer learning</strong></p>

<p>Task 1: learn to recognize animals given many (10M) examples which are not horses</p>

<p>Task 2: learn to recognize horses given a few (100) examples</p>

<ul>
  <li>Keep layers from task 1, re-train on <strong>last layer</strong></li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220201193339131.png" alt="image-20220201193339131" style="zoom:50%;" /></p>

<h1 id="sequence-models">Sequence Models</h1>

<p>Applications using sequence models include machine translation, protein structure prediction, DNA sequence analysis, and etc. All of which needs some <strong>representation that remembers previous data/state</strong>.</p>

<p>An overview of what will be discussed</p>

<ul>
  <li><strong>Sequence models</strong></li>
  <li><strong>Recurrent neural networks (RNNs)</strong>: when unrolled, basically modelling Finite State Machine</li>
  <li><strong>Backpropagation through time</strong>: Updating the shared/same weights across states</li>
  <li><strong>GRU and LSTM</strong>: fixing vanishing/exploding gradient in RNN</li>
  <li>Word embeddings, beam search, encoder-decoder attention</li>
  <li>Transformers</li>
</ul>

<p>In general, to reduce computational complexity through this deep network, <strong>weights are shared/same across time</strong>. i.e. shared weights when unrolled.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203131557649.png" alt="image-20220203131557649" style="zoom: 50%;" /></p>

<p>Another interesting thing to know is the <strong>timeline</strong> of model development w.r.t. sequence models:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203131802058.png" alt="image-20220203131802058" style="zoom:33%;" /></p>

<p>where:</p>

<ul>
  <li>for simple deep models like RNN, there were problems of vanishing gradients. So LSTM and GRU are essentially variants to solve this problem in RNN.</li>
</ul>

<h2 id="natural-language-models">Natural Language Models</h2>

<p>Representing language requires a natural language model which may be a <strong>probability distribution over strings</strong></p>

<p>Some basic methods for <strong>treating input text data</strong> include</p>

<ul>
  <li>Bag of words + Classifier</li>
  <li>Feature Vector + Classifier</li>
  <li>Markov Model</li>
</ul>

<h3 id="bag-of-words">Bag of Words</h3>

<blockquote>
  <p><strong>Bag of words</strong> are text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but <strong>keeping multiplicity</strong>/count.</p>
</blockquote>

<blockquote>
  <p><strong>Term Frequency</strong>: In a bag of words we count how prevalent each term $x$ is in a single document $d$ which is the term frequency $TF(x, d)$.</p>

  <ul>
    <li>we assume words are commonly normalized to lowercase, stemmed by removing their suffixes, and common stopwords (such as a, an, the, etc.) are removed.</li>
  </ul>
</blockquote>

<p>However, sometimes we also want to get more weight/focus for words that are rare. Therefore, we may want to consider the entropy of the word using <strong>inverse document frequency</strong>:</p>

\[IDF(x) = 1 + \log( \frac{ \text{total number of documents}}{ \text{number of documents containing $x$} })\]

<blockquote>
  <p><strong>TF-IDF</strong>: Then we can <em>boost</em> the frequency of rare words by considering the product of term frequency and inverse document frequency:</p>

\[TFIDF(x,d) = TF(x,d) \times IDF(x)\]

</blockquote>

<p>Finally, we use some <strong>classifier models</strong>, e.g. ML or DL models for classification using the mentioned above features.</p>

<p><strong>Problem</strong></p>

<ul>
  <li>
    <p>however, such a representation does not preserve order information</p>

\[\text{Alice sent a message to Bob} \quad \text{v.s.} \quad \text{Bob sent a message to Alice}\]

    <p>would have the same score/vector representation</p>
  </li>
</ul>

<h3 id="feature-vector">Feature Vector</h3>

<p>In contrast to a bag of words, using a feature vector to represent a sentence <strong>preserves order information</strong>. However, the problem is that sentences that have the <strong>same meaning</strong> could have a different word order:</p>

\[\text{Alice sent a message on Sunday} \quad \text{v.s.} \quad \text{On Sunday Alice sent a message}\]

<p>would need <strong>different feature vectors</strong> even if same information.</p>

<ul>
  <li>hence, there will be a lot of redundancy</li>
</ul>

<h3 id="n-gram-model">N-Gram Model</h3>

<p>Here we basically model <strong>probability distribution of n-grams</strong></p>

<ul>
  <li>
    <p>A Markov model is a 2-gram or bi-gram model where:</p>

\[P(x_ n | x_1 ,....,x_{n-1}) \approx p(x_n | x_{n-1})\]
  </li>
  <li>
    <p>but we want <strong>long term dependencies</strong></p>

    <ul>
      <li>using large/high-order n-gram model requires <strong>large corpus</strong></li>
      <li>hence not effective at capturing long term dependencies</li>
    </ul>
  </li>
</ul>

<h2 id="rnn">RNN</h2>

<blockquote>
  <p><strong>Recurrent Neural Network</strong> both maintain word order and model long term dependencies by sharing parameters across time.</p>

  <ul>
    <li>also allows for inputs and outputs of different length</li>
    <li>model both forward and backward sequence dependencies
      <ul>
        <li>using bidirectional RNN</li>
      </ul>
    </li>
    <li>but generally <em>difficult to train</em>: backpropagation causes gradients to explode/vanish
      <ul>
        <li>hence need LSTM or GRU</li>
      </ul>
    </li>
  </ul>

  <p>Its behavior basically is:</p>

  <ol>
    <li>process the input sequence one word at a time</li>
    <li>attempting to predict the next word from the current word and the previous hidden state $h_{t-1}$.
      <ul>
        <li>RNNs don’t have the limited context problem that n-gram models have, since the hidden state can in principle <strong>represent information about all of the preceding words</strong> all the way back to the beginning of the sequence</li>
      </ul>
    </li>
    <li>output $y_t=f(Vh_t)$ at time $t$ where $V$ will be shared and $f$ is an activation function of your choice.
      <ul>
        <li>exactly when it outputs can be tuned/changed in algorithm, which leads to different architectures such as many-to-one (outputting $y$ only at the last time step)</li>
      </ul>
    </li>
  </ol>
</blockquote>

<p>The basic foundation of a RNN is a <strong>finite state machine</strong></p>

<h3 id="state-machine">State Machine</h3>

<p>In a state machine, we have:</p>

<ul>
  <li>$S$: possible states</li>
  <li>$X$: possible inputs</li>
  <li>$f: S \times X \to S$: <strong>transitions</strong></li>
  <li>$Y$: possible outputs</li>
  <li>
    <p>$g: S \to Y$: mapping from state to outputs</p>
  </li>
  <li>$S_0$ initial state</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203205139875.png" alt="image-20220203205139875" style="zoom:50%;" /></p>

<p>The key idea is that <strong>a new state</strong> comes from <strong>both the previous state and an input</strong></p>

\[s_t = f(s_{t-1},x_t)\]

<p>This idea will be the ==same in RNN==.</p>

<p>Then, for a <strong>sequence of inputs $x_t$</strong>, the output $y_t$ would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203205331091.png" alt="image-20220203205331091" style="zoom:67%;" /></p>

<p>Essentially depends on all previous state/input.</p>

<ul>
  <li><strong>Recurrent neural networks are state machines</strong> with specific definitions of transition function $f$ and mapping $g$, in which the states, inputs, and outputs are vectors.</li>
</ul>

<h3 id="recurrent-neural-network">Recurrent Neural Network</h3>

<p>Given some sequence of data: $x_1, …, x_n$, we consider some <strong>hidden state $h_1, …, h_n$</strong> that will be used to form <strong>output</strong> $y_1, …, y_t$. This is done by <strong>sharing weights $U,W,V$</strong> across time:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></p>

<p>where essentially we see that RNN is modelling <strong>state transitions and outputting stuff</strong>, which is like a FSM</p>

<ul>
  <li>
    <p>hidden state $h_t$ at time $t$ is computed by:</p>

\[h_t = g(Wh_{t-1} + Ux_t)\]

    <p>which is a <strong>nonlinear function</strong> on <strong>previous state and current input</strong>. ($x_t, h_{t-1}$ would be vectors)</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203132556812.png" alt="image-20220203132556812" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>the output then is:</p>

\[y_t = V h_t\]

    <p>which <strong>can be followed by an activation</strong>, in that case we say:</p>

\[y_t = f(z_t) = f(Vh_t)\]

    <p>where $f$ would be a nonlinear function such as sigmoid.</p>
  </li>
  <li>notice that all weights $V,W,U$ <strong>are the same</strong>!</li>
  <li>this means that we only need to update it once for each matrix!</li>
</ul>

<p>Additionally, since to get to the new hidden state $h_t$ we need $Wh_{t-1} + Ux_t$, this can be computed in one shot by:</p>

\[[W,U] \begin{bmatrix}
h_{t-1}\\
x_t
\end{bmatrix} = Wh_{t-1} + Ux_t\]

<p>Then the algorithm looks as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207183223337.png" alt="image-20220207183223337" style="zoom:67%;" /></p>

<p>which essentially is when we have one layer.</p>

<ul>
  <li>As with feedforward networks (NN), we’ll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks.</li>
  <li>more details on backpropagation through time is shown in later sections.</li>
</ul>

<h3 id="rnn-architecture">RNN Architecture</h3>

<p>Some common architectures used in RNN can be visualized as:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">One to Many</th>
      <th style="text-align: center">Many to One</th>
      <th style="text-align: center">Many to Many</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133619748.png" alt="image-20220203133619748" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203212927731.png" alt="image-20220203212927731" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133652920.png" alt="image-20220203133652920" /></td>
    </tr>
  </tbody>
</table>

<p>where in the end you need to <strong>specify size of output</strong>. Neural networks are always trained with fixed length of input and output.</p>

<ul>
  <li><strong>one to many</strong>: e.g. image to caption</li>
  <li><strong>many to one</strong>: caption to image/stock price prediction/sentiment classification</li>
  <li><strong>many to many</strong>: machine translation, video action classification
    <ul>
      <li>e.g. this can be done by “hardcoding” in the algorithm such that you ask the model to only output $y_t=f(Vh_t)$ if $t \in [T-2,T-1,T]$, for example.</li>
    </ul>
  </li>
</ul>

<p>There are many choices of architectures you can use:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203133936167.png" alt="image-20220203133936167" style="zoom:33%;" /></p>

<p>which would be useful for <strong>audio to text</strong></p>

<h3 id="loss-function">Loss Function</h3>

<p>To complete our definition of the RNN architecture requires incorporating a loss function, with which we can <strong>improve our model by gradient descending on the shared weigths</strong>.</p>

<p>The simple idea is that <strong>each output can be compared to the label</strong>, so we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>we have covered how states are computed, but the others:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" /></p>

    <p>so essentially $\hat{y} = \text{softmax}(Vh_t)$</p>
  </li>
</ul>

<p>Then, the <strong>total loss is just the sum over the characters</strong>:</p>

\[\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i) = \sum_{t=1}^{l_i}\mathcal{L}_{\text{character}}(\hat{y}_t^i, y_t^i)\]

<p>note that the $i$th input $x^i$ is a <strong>sequence of characters</strong>.</p>

<ul>
  <li>
    <p>the sequence length $l_i$ has <strong>nothing</strong> to do with dimension of the <strong>$i$th input $x_t^i$ or $y_t^i$ at time $t$</strong>, which are basically feature/output vectors for <strong>each character</strong> along the sequence of length $l_i$.</p>
  </li>
  <li>
    <p>then, since we could have $m$ sequences/sentences in the entire dataset, we would have:</p>

\[\mathcal{L}_{\text{total}}(\hat{y}_t, y_t) = \sum_{i=1}^{m}\mathcal{L}_{\text{sequence}}(\hat{y}^i, y^i)\]

    <p>this means that together it will be a double sum.</p>
  </li>
</ul>

<h3 id="deep-rnn">Deep RNN</h3>

<p>We can <strong>stack multiple hidden layers and connecting them</strong> as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203220235023.png" alt="image-20220203220235023" style="zoom: 50%;" /></p>

<p>where essentially:</p>

<ul>
  <li>the output of the lowest layer is treated as the “current state/input” of the second last layer.</li>
  <li>
    <p>the weights $U^l,W^l$ will be <strong>shared within each layer $l$</strong>, but there is still a <strong>single $V$</strong> for output</p>
  </li>
  <li>
    <p>therefore, the new state transition equation at layer $l$ becomes:</p>

\[h_t^l = g(W^lh_{t-1}^l + U^lh_{t}^{l-1})\]

    <p>where $h_{t}^{l-1}$ is the state of previous layer at that time $t$, treated as an “input/current state”.</p>
  </li>
</ul>

<p>Additionally, we can build <strong>dependencies by</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203220609110.png" alt="image-20220203220609110" style="zoom:67%;" /></p>

<p>where then we need to specify a weight for connection  $o_{t-1} \to h_t$, which is certainly doable.</p>

<p>Last but not least, an important variant is the <strong>bidirectional RNN</strong>, which you shall see in the next section.</p>

<h3 id="bidirectional-rnn">Bidirectional RNN</h3>

<p>Basically, the idea is that we not only want to remember <strong>forward information</strong>, but also <strong>backward information</strong> (context in both direction). Therefore, we consider each state $h_t$ being <strong>duplicated into two</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 50%;" /></p>

<p>such that</p>

<ul>
  <li>
    <p>then you have two weights $W, \bar{W}$, where the former is used to flow between $h_{t-1} \to h_t$, the latter $\bar{h}_{t+1} \to \bar{h}_t$:</p>

\[\begin{align*}
h_t &amp;= g(Wh_{t-1} + Ux_t)\\
\bar{h}_t &amp;= g(\bar{W}\bar{h}_{t+1} + Ux_t)\
\end{align*}\]
  </li>
  <li>
    <p>then output basically depends on <strong>both state information</strong> by concatenating them</p>

\[o_t = V\begin{bmatrix}
h_t\\
\bar{h}_t
\end{bmatrix}\]
  </li>
</ul>

<p>Then we can stack those as well</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203134545172.png" alt="image-20220203134545172" style="zoom: 80%;" /></p>

<h3 id="backpropagation-through-time">Backpropagation Through Time</h3>

<p>Having defined the RNN architectures and loss function, our goal is to train the RNN by <strong>finding derivatives and descending.</strong> Essentially we will use <strong>backpropagation</strong> again as it is a deep network.</p>

<p>Then general form of the gradient of a sequence loss $\mathcal{L}_{ \text{sequence}}$ on some parameter $\theta$ is:</p>

\[\frac{d\mathcal{L}_{ \text{sequence}}(\hat{y} , y)}{d\theta} = \sum_{t=1}^{l_i}\frac{d\mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{d\theta} = \sum_{t=1}^{l_i}\sum_{t}\frac{\partial \mathcal{L}_{ \text{character}}(\hat{y}_t , y_t)}{\partial h_t}\frac{\partial h_t}{\partial \theta}\]

<p>you will see that all derivatives from then on <strong>will have dependence on $t$</strong>, which is why we call it backpropagation through time.</p>

<p>Recall that in a simple RNN:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Label</th>
      <th style="text-align: center">Diagram</th>
      <th style="text-align: center">Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214354503.png" alt="image-20220203214354503" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203214314704.png" alt="image-20220203214314704" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Then first computing the gradient for $V$ at time $t=3$</p>

\[\frac{\partial L_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial V} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial o_3}\frac{\partial o_3}{\partial V}\]

<p>notice that it ==does not depend on data from previous time==, since $o_t = Vh_t$ only depends on current time.</p>

<ul>
  <li>therefore, updating $V$ at each iteration at time $t$ is simple</li>
</ul>

<p>Now, if we consider updating $W$:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W}\]

<p>but then we know that $h_3  = f(Wh_2 + Ux_3)$ which <strong>depends on previous time</strong>! Hence in this case we would have:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3}\frac{\partial h_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \frac{\partial h_3}{\partial h_i} \frac{\partial h_i}{\partial W}\]

<p>but then, for instance:</p>

\[\frac{\partial h_3}{\partial h_1} = \frac{\partial h_3}{\partial h_2} \frac{\partial h_2}{\partial h_1}\]

<p>We can generalize the above even further to:</p>

\[\frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial \hat{y}_3}\frac{\partial  \hat{y}_3}{\partial h_3} \sum_{i=1}^3 \left( \prod_{j=i+1}^3 \frac{\partial h_3}{\partial h_{j-1}} \right) \frac{\partial h_i}{\partial W}\]

<p>if we take the activation function to be $\tanh$, then the part in parenthesis above is basically $\prod W^T \, \text{diag}(\tanh’(h_{t-1}))$, which means that as $t » 3$, we would have backpropagation <strong>rasing $W^T$ to a high power</strong>.</p>

<ul>
  <li>
    <p>hence, RNN could suffer vanishing/explode gradients if the eigenvalues are less than one or greater than one, respectively</p>
  </li>
  <li>
    <p>graphically, where $E = L$ that we used above:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203135108631.png" alt="image-20220203135108631" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>remember that we <strong>only update once in the end for $W$</strong>, because we only have <strong>one shared $W$</strong>! (same for $U,V$ if we are using a single layer RNN)</p>
  </li>
</ul>

<p>Finally, the algorithm is then summarized here:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203230211497.png" alt="image-20220203230211497" style="zoom:50%;" /></p>

<p>note that this is only the backpropagation part (second pass). The full algorithm of RNN would have a two-pass algorithm</p>

<ol>
  <li>In the first pass, we perform forward inference, computing $h_t$ , $y_t$ , accumulating the loss at each step in time, <strong>saving the value of the hidden layer</strong> at each step for use at the next time step.</li>
  <li>In the second phase, we process the sequence in reverse, computing the required <strong>gradients</strong> as we go, computing and saving the error term for use in the hidden layer for each step <strong>backward</strong> in time</li>
</ol>

<h3 id="rnn-as-language-models">RNN as Language Models</h3>

<p>This is essentially an example of applying RNN in real life. This idea of how you <strong>treat input/output</strong> as probabilities is used in other models introduced next as well.</p>

<p>Now, consider the task of <strong>predicting next word</strong> again. Here we have:</p>

<ul>
  <li>input as <strong>text/sentences</strong></li>
  <li>output <strong>probability</strong> that each word $w_i \in V$ will be the next word</li>
</ul>

<p>Then, we can have the following in each RNN layer:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207185449002.png" alt="image-20220207185449002" /></p>

<p>where:</p>

<ul>
  <li>
    <p>$E$ is an <strong>embedding matrix</strong>, so $E x_t$ is embedding of current word $t$.</p>
  </li>
  <li>
    <p>the probability that a particular word $w_i$ in the vocabulary is the next word is represented by $y_t[i]$, the $i$th component of $y_t$$.</p>

\[y_t[i] = P(w_{w+1}=w_i| w_1, ..., w_t)\]

    <p>where you can imagine $w_i \in V$ can be indexed easily</p>
  </li>
</ul>

<p>Then, while <strong>training</strong>, you would have a correct distribution $y^<em>_t$, which is essentially a <strong>one-hot encoded vector</strong> for the correct word $w^</em>_t$. Since this can be treated as a probability distribution, and our prediction $y_t$ is also a probability distribution:</p>

\[L_\text{Cross Entropy} = \sum_t L_{\text{CE}}(y_t, y^*_t) = \sum_t - \log y_t[w^*_{t+1}]\]

<p>where essentially $y_t[w^<em>_{t+1}]$ is the probability that our model predicts $w^</em>_{t+1}$ to be the next word correctly.</p>

<p>Graphically, this is what happens when we are training:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207190334237.png" alt="image-20220207190334237" style="zoom: 80%;" /></p>

<p>where it is important to note that the loss is on the <strong>probability for next word</strong>.</p>

<h2 id="gated-recurrent-unit">Gated Recurrent Unit</h2>

<p>One solution for vanishing/exploding gradient would be to use <strong>GRU</strong> instead of an RNN architecture for transition.</p>

<p>In RNN, we had the following structure</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Structure</th>
      <th style="text-align: center">Encapsulation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203211204415.png" alt="image-20220203211204415" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203231239039.png" alt="image-20220203231239039" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>But now, what GRU does is essentially doing <strong>more complicated thing for transition</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203135552047.png" alt="image-20220203135552047" style="zoom:33%;" /></p>

<p>where:</p>

<ul>
  <li>there are many version of GRU circuits, many of which perform equally well on certain dataset</li>
  <li>the final best design was essentially discovered using a grid search over all possible gates. So there is kind of no theoretical reason why.</li>
</ul>

<p>Schematically, GRU does the following:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">GRU Schematic</th>
      <th style="text-align: center">Equations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203232453595.png" alt="image-20220203232453595" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203231843814.png" alt="image-20220203231843814" /></td>
    </tr>
  </tbody>
</table>

<p>where it emphasized that we need to be skilled at <strong>translating between a graphical representation to equations</strong></p>

<ul>
  <li>the shared weights are now $W_r, W_z, W$ and $U_r, U_z,U$ (and of course $V$ for output)</li>
  <li>the main changes are that we <strong>added three more gates/components</strong> before deciding what is $h_t$</li>
  <li>it is also important to notice the <strong>plus operation</strong> as the last step for state transition. This plus instead of multiply essentially solves the exploding gradient problem</li>
  <li><strong>inputs</strong> of functions/state have arrows pointing in</li>
  <li><strong>weights</strong> are labelled on the arrow</li>
  <li><strong>nonlinear functions</strong> are not shown but applied <strong>if an operator is not on the graph</strong>
    <ul>
      <li>e.g. $h_t$ does not have a nonlinear function because it is specified we have a $+$ operation</li>
    </ul>
  </li>
  <li>the rest of the architecture is the same as RNN</li>
</ul>

<hr />

<p><em>An analogy of the input/state/gates</em></p>

<p>Consider the example:</p>

<ul>
  <li>state $h_{t-1}$ is the cloth we wear yesterday</li>
  <li>$x_t$ is the weather/input on day $t$/today</li>
  <li>$\bar{h}_t$ is the candidate clothes we <em>prepared/predicted</em> to wear</li>
  <li>$h_t$ is the actual clothes we wear on day $t$/today</li>
</ul>

<p>Then, essentially those additional gates (the update and reset gates) determine ==to what extent== we ==take into account these factors==:</p>

<ul>
  <li>do we <strong>ignore</strong> the weather $x_t$ completely,</li>
  <li>do we <strong>forget</strong> what we wore yesterday $h_{t-1}$</li>
  <li>and do we take into account our candidate clothes we prepared $\bar{h}_t$, and to <strong>which extent.</strong></li>
</ul>

<p>In short, the effect of those can be overviewed as below:</p>

<h3 id="update-gate">Update Gate</h3>

<p>The update gate basically does the following:</p>

\[z_t = \sigma (W_z h_{t-1} + U_z x_t)\]

<p>which is <strong>between 0 and 1</strong>, then is used in $h_t$ as:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234102881.png" alt="image-20220203234102881" style="zoom:80%;" /></p>

<p>Examples of what $z_t$ does is shown below</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">output is the new candidate</th>
      <th style="text-align: center">output is the previous hidden state</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233102030.png" alt="image-20220203233102030" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233355981.png" alt="image-20220203233355981" /></td>
    </tr>
  </tbody>
</table>

<h3 id="reset-gate">Reset Gate</h3>

<p>The reset gate again is nonlinear:</p>

\[r_t = \sigma (W_r h_{t-1} + U_r x_t)\]

<p>This is used by the candidate activation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234223392.png" alt="image-20220203234223392" style="zoom: 80%;" /></p>

<p>The effects:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Candidate forgetting the previous hidden state</th>
      <th style="text-align: center">Candidate does same as RNN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233449807.png" alt="image-20220203233449807" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203233500860.png" alt="image-20220203233500860" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="function">Function</h3>

<p>The last possible combination is:</p>

<ul>
  <li>$z_t = r_t = 0$, then hidden state is only dependent ton current state as $h_t = \bar{h}_t = \phi(Ux_t)$</li>
  <li>$z_t =0, r_t = 1$, then we get back RNN because $h_t = \bar{h}<em>t = \phi(Wh</em>{t-1} + Ux_t)$</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Output hidden state is only dependent on the current state</th>
      <th style="text-align: center">Reduced to RNN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234523035.png" alt="image-20220203234523035" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203234531917.png" alt="image-20220203234531917" /></td>
    </tr>
  </tbody>
</table>

<h2 id="long-short-term-memory">Long Short-Term Memory</h2>

<p>Long short-term memory (LSTM) (Hochreiter &amp; Schmidhuber 1997) was introduced two decades before the GRU (Cho et al. 2014).</p>

<p>The LSTM is easy to train, and includes an <strong>additional input and output compared with the RNN and GRU</strong>. At each time step $t$:</p>

<ul>
  <li>receives as input the current state $x_t$, the hidden state $h_{t-1}$, and <strong>memory cell</strong> $c_{t-1}$ of the previous time step</li>
  <li>outputs the hidden state $h_t$ and memory cell $c_t$</li>
</ul>

<p>An encapsulation would look like this</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000418648.png" alt="image-20220204000418648" style="zoom:50%;" /></p>

<p>It is then combined such that we have</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000434163.png" alt="image-20220204000434163" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>similarly, you can also combine to have a <strong>bidirectional LSTM</strong> by:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Bidirectional RNN</th>
          <th style="text-align: center">Bidirectional LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220203221058252.png" alt="image-20220203221058252" style="zoom: 33%;" /></td>
          <td style="text-align: center"><img src="http://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f7bdb849dafe17c952bfd88b879e01f74cf59d78/4-Figure3-1.png" alt="Bidirectional LSTM (BiLSTM) Training Task - GM-RKB" style="zoom: 50%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>where essentially <strong>each LSTM cell at time $t$ is duplicated</strong> (they also have a separate weights like in bidirectional RNN) into a forward direction and a backward direction.</p>
  </li>
</ul>

<p>Each unit of LSTM look like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">LSTM Schematic</th>
      <th style="text-align: center">Another View</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000548449.png" alt="image-20220204000548449" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207190734060.png" alt="image-20220207190734060" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the highlighted part is clear, same as RNN.</p>

<ul>
  <li>
    <p>so we have an additional five components:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000633720.png" alt="image-20220204000633720" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>so now shared weights are $W_f, W_i, W, W_0$ and $U_f, U_i, U, U_0$</p>
  </li>
  <li>
    <p>outputs will be $c_t, h_t$, which will be inputs in the next LSTM unit.</p>
  </li>
</ul>

<p>Alike GRU, those three gate essentially regulates how much information can get through</p>

<h3 id="forget-gate">Forget Gate</h3>

<p>The forget gate has the following equation</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204000921911.png" alt="image-20220204000921911" style="zoom: 67%;" /></p>

<ul>
  <li>or somtimes, we can simply this as $f_t = \sigma(W_f \cdot [h_{t-1},x_t]^T )$ for $W_f \equiv [W_f, U_f]$ being concatenated</li>
</ul>

<p>Since it is sigmoid, we can investigate the extreme values</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Memory Cell Ignore Previous Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001008540.png" alt="image-20220204001008540" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="input-gate">Input Gate</h3>

<p>The input gate has the function</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001140743.png" alt="image-20220204001140743" style="zoom:67%;" /></p>

<p>which again is <strong>used by the memory cell</strong></p>

<ul>
  <li>controls <strong>how much $\bar{c}_t$ will be included</strong> in the new cell state $c_t$</li>
</ul>

<p>For instance, since $\sigma \in [0,1]$</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">new candidate memory $\bar{c}_t$ is ignored</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001242584.png" alt="image-20220204001242584" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="memory-cell">Memory Cell</h3>

<p>The memory cell has equation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001326157.png" alt="image-20220204001326157" style="zoom:67%;" /></p>

<p>which basically is updated very iteration to store some new memory, essentially <strong>candidate memory</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001418530.png" alt="image-20220204001418530" style="zoom: 67%;" /></p>

<p>which then stores <strong>information about previous state and input</strong></p>

<ul>
  <li>then, when we are outputing $h_t$, it will <strong>read from memory cell $c_t$</strong></li>
</ul>

<h3 id="output-gate">Output Gate</h3>

<p>The output gate has equation:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001608239.png" alt="image-20220204001608239" style="zoom:67%;" /></p>

<p>which is <strong>essentially RNN</strong>, highlighted in green:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207163227530.png" alt="image-20220207163227530" style="zoom:33%;" /></p>

<p>but notice that the next state $h_t$ also <strong>included memory</strong>:</p>

\[h_t = o_t \cdot \phi(c_t)\]

<p>which is a point-wise multiplication.</p>

<ul>
  <li>function $\phi$ is a nonlinear function, such as $\tanh$</li>
  <li>so essentially $h_t$ depends on <strong>output $o_t$ and cell memory $c_t$</strong>, where $o_t$ is essentially the RNN cell</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Recall that the output will essentially comes out from $h_t$, essentially:</p>

\[y_t = f(z_t) = f(Vh_t)\]

<p>where $f$ is an activation function if we are donig classification.</p>

<p>Then, some of the usages for LSTM would look like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207164111464.png" alt="image-20220207164111464" style="zoom: 50%;" /></p>

<p>where your output at time $t$ essentialy comes out from $h_t$.</p>

<hr />

<h2 id="gru-vs-lstm">GRU vs LSTM</h2>

<p><strong>Similarities</strong> between the two:</p>

<ul>
  <li>both units avoid repeated multiplications which cause vanishing or exploding gradients by a similarly positioned ==addition==</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204001848344.png" alt="image-20220204001848344" style="zoom: 67%;" /></p>

<ul>
  <li>
    <p>update gate $z_t$​ controls the amount of the new candidate to pass in the GRU; whereas the input gate controls the amount of the new candidate memory to pass in the LSTM</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">GRU</th>
          <th style="text-align: center">LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002239125.png" alt="image-20220204002239125" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002134935.png" alt="image-20220204002134935" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Comparing the GRU reset gate controlling the candidate hidden state as highlighted in Figure 6.45 with the LSTM input gate controlling the candidate memory cell as highlighted in Figure 6.46 shows the <strong>modulation of the candidate</strong> in both units.</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">GRU</th>
          <th style="text-align: center">LSTM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002408761.png" alt="image-20220204002408761" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220204002416921.png" alt="image-20220204002416921" /></td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>Differences</strong></p>

<ul>
  <li>as GRU has fewer gates, they have a <strong>fewer numbe of parameters and trains faster</strong> than LSTM</li>
</ul>

<p>Lastly, an overview of NN, RNN and LSTM as “neuron”:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207191037059.png" alt="image-20220207191037059" /></p>

<h2 id="sequence-to-sequence">Sequence to Sequence</h2>

<p>You can essentially use LSTM/RNN/GRU blocks as follows;</p>

<p><img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>each component LSTM used here can be changed to GRU or RNN</p>
  </li>
  <li>
    <p>the <strong>encoder</strong> takes in the input sequence $(x_1, …, x_s)$ and output a <strong>context vector</strong> $z=f(Vh_t)$</p>

    <ul>
      <li>this is a <strong>single vector</strong>, which is treated as hidden state $h_0$ in the decoder</li>
    </ul>
  </li>
  <li>
    <p>the <strong>decoder</strong> also consists of LSTM/GRU that takes $h_0=z$ as the first hidden state, and generates output <strong>sequence $(y_1,…,y_t)$</strong></p>

    <ul>
      <li>
        <p>therefore, the entire model is doing:</p>

\[(y_1,...,y_t) = \text{decoder}(\text{encoder}(x_1, ..., x_s))\]
      </li>
    </ul>
  </li>
  <li>
    <p>this is often used for <strong>machine translation</strong>, for instance</p>
  </li>
</ul>

<h2 id="adding-attention">Adding Attention</h2>

<blockquote>
  <p>Below introduces the idea of attention as essentially a <strong>weighted sum over inputs</strong>, and talks about <strong>encoder-decoder attention</strong>. For reference, checkout <a href="#Self-Attention">Self-Attention</a> which is relevant but different.</p>
</blockquote>

<p>For many applications, it helps to <strong>add “attention”</strong> to RNNs, so that we can <strong>focus on certain part of the input sequence</strong></p>

<ul>
  <li>
    <p>Allows network to learn to ==attend to different parts of the input== at different time steps, shifting its attention to focus on different aspects during its processing.</p>
  </li>
  <li>Used in image captioning to focus on <em>different parts of an image</em> when <em>generating different parts of the output sentence</em>.</li>
  <li>In MT, allows focusing attention on <em>different parts of the source sentence</em> when <em>generating different parts of the translation</em>.</li>
</ul>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207170115608.png" alt="image-20220207170115608" /></p>

<p>where we see that:</p>

<ul>
  <li>the underlined text is our <strong>query</strong></li>
  <li>the image is our <strong>key</strong> (the database of data we have)</li>
  <li>then we want to output the highlighted part</li>
</ul>

<p>To see why this is useful, consider the task of doing <strong>machine translation</strong>, so we are using a Seq2Seq Model which consists of an encoder and a decoder as shown before:</p>

<p><img src="https://www.researchgate.net/profile/Tryambak-Gangopadhyay/publication/340443252/figure/fig1/AS:876840973520898@1586066587656/Encoder-decoder-model-using-stacked-LSTMs-for-encoding-and-one-LSTM-layer-for-decoding.ppm" alt="Encoder-decoder model using stacked LSTMs for encoding and one LSTM... |  Download Scientific Diagram" style="zoom: 67%;" /></p>

<p>Now, Seq2seq models <strong>incorporating attention</strong> can:</p>

<ol>
  <li>the <strong>decoder</strong> receives as input the <strong>encoder encoder output sequence</strong> $c_i=c_i(o_1, …,o_t)$
    <ul>
      <li>where $o_t= V\begin{bmatrix}h_t\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$</li>
    </ul>
  </li>
  <li>different parts of the output sequence <strong>pay attention</strong> to <strong>different parts of the input sequence</strong></li>
</ol>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 60%;" /></p>

<p>where essentially:</p>

<ul>
  <li>
    <p>for each input to <strong>decoder</strong> hidden state $\tilde{h}_t$, a context vector is used which is <strong>attention</strong> w.r.t step $t=1$ and of <strong>all input sequences $x_1, …, x_t$</strong></p>
  </li>
  <li>
    <p>specifically, in this <strong>bidirectional RNN</strong> as encoder, the context vector is computed as:</p>

\[c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T\]

    <p>which is a <strong>weighted sum</strong> over <strong>all hidden states in the encoder</strong> (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:</p>

\[\alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j\]

    <p>which basically represents the <strong>amount of attention</strong> output $o_i$ will give to input word $x_j$, for some score function. (checkout <a href="#Self-Attention">Self-Attention</a>).</p>
  </li>
  <li>
    <p>This context vector, $c_i$, is generated anew with <strong>each decoding step $i$</strong> and takes all of the encoder hidden states into account in its derivation</p>
  </li>
  <li>
    <p>this is also called ==encoder-decoder attention==, which is different from <a href="#Self-Attention">Self-Attention</a>, and covered more in detail in <a href="#Attention">Attention</a></p>
  </li>
</ul>

<h2 id="sgns-embeddings">SGNS Embeddings</h2>

<p>Another common problem in language data is <strong>how do we represent texts/words (tokens)</strong>.</p>

<ul>
  <li>one-hot encoding
    <ul>
      <li>problem: every two words have <strong>the same distance</strong>. i.e. losing relationship between them</li>
      <li>problem: sparse vector.</li>
    </ul>
  </li>
  <li><strong>feature embedding</strong> for each word
    <ul>
      <li>we want to somehow learn word embedding from large <strong>unsupervised</strong> text corpus.</li>
    </ul>
  </li>
</ul>

<p>In this section we introduce one method for computing embeddings: <strong>skip-gram SGNS with negative sampling</strong>, sometimes called SGNS.</p>

<ul>
  <li>The skip-gram algorithm is one word2vec of two algorithms in a software package called word2vec</li>
</ul>

<blockquote>
  <p><em>Intuition</em></p>

  <p>The intuition of ==word2vec== is to train a classifier on a <strong>binary</strong> prediction task: given a word $w$, <strong>how likely is it to show up near another word</strong> $w_i$, e.g. apricot?</p>

  <ul>
    <li>this can be done in <strong>self-supervision</strong>, which avoids the need for any sort of hand-labeled supervision signal</li>
    <li>essentially training a logistic regression classifier</li>
    <li>this is <strong>static</strong>, in that it learns one <strong>fixed embedding for each word</strong> in the embeddings vocabulary.
      <ul>
        <li>In the <a href="#Transformer">Transformer</a>  chapter we will introduce methods for learning <strong>dynamic contextual embeddings</strong> like the popular family of BERT representations, in which the vector for each word is <strong>different in different contexts</strong></li>
      </ul>
    </li>
  </ul>

  <p>Then, ==while we are learning the likelihood==, we would have ==needed/learnt some representation $\vec{w}$ for a word $w$==, which will be our embedding!</p>
</blockquote>

<p>Therefore, the model is simple:</p>

<ol>
  <li>Treat the target word $w$ and a <strong>neighboring</strong> context word as <strong>positive</strong> examples.</li>
  <li>Randomly sample other words in the lexicon to get negative samples.</li>
  <li>Use <strong>logistic regression</strong> to train a classifier to distinguish those two cases.</li>
  <li>Use the <strong>learned weights</strong> $W$ as the <strong>embeddings</strong>, commonly represented as $E$</li>
</ol>

<blockquote>
  <p><strong>Note</strong></p>

  <p>This means that the embedding for word $w_i$ will be similar to $w_j$ if they are <strong>physically close</strong> to each other.</p>

  <ul>
    <li>i.e. if phrases like “bitter sweet” will create problems in the embedding! (physically close but their meanings are different)</li>
  </ul>
</blockquote>

<h3 id="sgns-classifier">SGNS Classifier</h3>

<blockquote>
  <p><strong>Goal of Classifier</strong>:</p>

  <p>Given a tuple $w,c$ being target word $w$ paired with a candidate word $c$, what is the <strong>probability</strong> that $c$ is a <strong>context word</strong> (i.e. physically next to it)?</p>

  <p>To present such <strong>probability</strong>, we will use:</p>

\[P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\]

  <p>for $\vec{w},\vec{c}$ being the <strong>embedding of word $w,c$</strong>.</p>

  <ul>
    <li>so on our way to learn such classifier, we would have learnt $\vec{w},\vec{c}$ which will be used for embedding</li>
  </ul>
</blockquote>

<p>Consider we want to find out the <strong>embedding of the word $\text{apricot}$</strong>, and we have the following data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208214715503.png" alt="image-20220208214715503" /></p>

<p>where:</p>

<ul>
  <li>
    <p>let us take a window size of $2$, so that we view $[c_1, …, c_4]$ above as <strong>real context word</strong> for $\text{apricot}$.</p>
  </li>
  <li>
    <p>our goal is to have a logistic regression such that:</p>

\[P(+|w,c)\]

    <p>is <strong>high</strong> if $c$ is a <strong>real context word</strong>, and that:</p>

\[P(-|w,c)  = 1-P(+|w,c)\]

    <p>is <strong>high</strong> if $c$ is <strong>not a context word</strong>.</p>
  </li>
</ul>

<p>Then, the question is how do we model such probability? We assumed that <strong>words next to each other</strong> should have ==similar embeddings==. This means that:</p>

\[\text{Similarity}(w,c) \approx \vec{w} \cdot \vec{c}\]

<p>for $\vec{w},\vec{c}$ being the embeddings for the word $w,c$. Then, to map this to <strong>probability that $c$ is a real context word for $w$ as</strong>:</p>

\[P(+|w,c) = \sigma(\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(-\vec{w}\cdot \vec{c})}\]

<p>is high if high dot product = similar = next to each other. Then similarly, probability that $c$ is not a context word as:</p>

\[P(-|w,c) = \sigma(-\vec{w}\cdot \vec{c}) =\frac{1}{1 + \exp(+\vec{w}\cdot \vec{c})}\]

<p>for $c$ being <strong>negative samples</strong> (not context words).</p>

<hr />

<p>Now, this means that we can also assign “<strong>similarity score</strong>” between a target word and a <strong>context window</strong>:</p>

\[P(+|w, c_{1:L}) = \prod_{i=1}^L \sigma(\vec{c}_i \cdot \vec{w})\]

<p>where:</p>

<ul>
  <li>we assumed <strong>all contexts words are independent</strong></li>
  <li>we can also compute the log probability to make it a sum</li>
</ul>

<hr />

<p>Now, we can think of what are are learning graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>where essentially:</p>

<ul>
  <li>Embedding matrix $E$ contains two matrices, $W$ for word embedding and $C$ for context embedding
    <ul>
      <li>i.e. for the $i$-th word (in the dictionary), its word embedding will be the $i$-th column of $W$, and similarly for context embedding</li>
      <li>i.e. every word will have <strong>two embeddings</strong>, one in $W$ and another in $C$. In reality people either only take $W$ or take $W+C$</li>
      <li>if your vocabulary size is $\vert V\vert$, and you want an embedding of dimension $d$, then $W,C \in \mathbb{R}^{d \times \vert V\vert }$ so that you can fetch the embedding from a one-hot vector.</li>
    </ul>
  </li>
  <li>we have <strong>two embeddings</strong> because a word $w$ could be treated as a target, but sometimes it might also be picked as a context $c$, in which we update the embedding separately.</li>
</ul>

<h3 id="learning-the-embedding">Learning the Embedding</h3>

<p>Our target is to learn the matrix:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208220831910.png" alt="image-20220208220831910" /></p>

<p>Let us begin with an example text, where our target word currently is $w=\text{apricot}$.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208221658252.png" alt="image-20220208221658252" /></p>

<p>Then taking a window size of $2$, we also want to have <strong>negative samples</strong> (in fact, more negative samples than positive ones per target word so that we are called <strong>SGNS</strong>):</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208221831637.png" alt="image-20220208221831637" /></p>

<p>where here:</p>

<ul>
  <li>
    <p>each of the training sample $(w, c_{pos})$ comes with $k=2$ negative samples, for $k$ being tunable</p>
  </li>
  <li>
    <p>the negative samples are sampled <strong>randomly by</strong>:</p>

\[\text{Prob of sampling $w$}= P_\alpha(w) = \frac{ \text{Count}(w)^\alpha }{\sum_{w'} \text{Count}(w')^\alpha}\]

    <p>where we usually take $\alpha = 0.75$ so that <strong>rare words have a  better chance</strong></p>

    <p>e.g. if $P(a)=0.99,P(b)=0.01$, doing the power would give:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222147320.png" alt="image-20220208222147320" style="zoom:80%;" /></p>
  </li>
</ul>

<p>Then, <strong>given a positive pair and the $k$ negative pairs</strong>, our ==loss function to minimize would be==</p>

\[L_{CE} = -\log{\left[ P(+|w,c_{pos}) \cdot \prod_{i=1}^k P(- |w,c_{neg})\right]} = - \left[ \log \sigma(c_{pos} \cdot w) + \sum_{i=1}^k  \log \sigma(-c_{neg} \cdot w) \right]\]

<p>which we want to <strong>minimize</strong> by <strong>updating $\vec{w} \in W$</strong> for the target word and $\vec{c} \in C$ for the context word.</p>

<p>Therefore, we need to take the derivatives:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222630846.png" alt="image-20220208222630846" style="zoom:80%;" /></p>

<p>Then the update equations</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222649078.png" alt="image-20220208222649078" style="zoom:80%;" /></p>

<p>So graphically, we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220208222800901.png" alt="image-20220208222800901" style="zoom:80%;" /></p>

<p>notice that we update target word embedding in $W$ but context words in $C$.</p>

<p>Then that is it! You now have leant <strong>two separate embeddings</strong> for each word $i$:</p>

<ul>
  <li>$\vec{w}_i = W[i]$ is the embedding <strong>target embedding</strong> for word $i$</li>
  <li>$\vec{c}_i = C[i]$ is the embedding <strong>context embedding</strong> for word $i$</li>
</ul>

<p>(then it is common to either just add them together as $\vec{e}_i = \vec{w}_i+\vec{c}_i$ as the embedding for word $i$, or just take $\vec{e}_i = \vec{w}_i$.)</p>

<h3 id="other-embeddings">Other Embeddings</h3>

<p>Some problem with SGNS Embedding (Word2Vec) is:</p>

<ul>
  <li>what if we want to know the embedding of an <strong>unknown word</strong> (i.e. unseen in the training corpus)?</li>
  <li><strong>sparsity</strong>: in languages with rich morphology, where some of the <em>many forms for each noun and verb may only occur rarely</em></li>
</ul>

<p>There are many other kinds of word embeddings that could deal with those problems. Here we briefly cover two:</p>

<ul>
  <li>
    <p><strong>Fasttext</strong>: Fasttext deals with these problems by using <strong>subword</strong> models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols <code class="language-plaintext highlighter-rouge">&lt;</code> and <code class="language-plaintext highlighter-rouge">&gt;</code> added to each word</p>

    <p>For example, with $n = 3$ the word where would be represented by the sequence <code class="language-plaintext highlighter-rouge">&lt;where&gt;</code> plus the character n-grams:</p>

\[\text{&lt;wh, whe, her, ere, re&gt;}\]

    <p>then a skipgram embedding is learned for each constituent n-gram, and the word <code class="language-plaintext highlighter-rouge">where</code> is represented by the <strong>sum of all of the embeddings of its constituent n-grams</strong>. Therefore, ==Unknown words== can then be presented only by the sum of the constituent n-grams!</p>
  </li>
  <li>
    <p><strong>GloVe</strong>: GloVe model is based on capturing <strong>global corpus statistics</strong>. GloVe is based on ratios of probabilities from the word-word cooccurrence matrix, combining the intuitions of count-based models like PPMI while also capturing the linear structures used by methods like word2vec.</p>
  </li>
  <li>
    <p>and many more</p>
  </li>
</ul>

<h1 id="graph-neural-network">Graph Neural Network</h1>

<p>In this chapter we describe graph neural networks, applied to networks or general graphs sharing weights across neighborhoods as shown below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211223146024.png" alt="image-20220211223146024" style="zoom: 50%;" /></p>

<p>Essentially graphs are everywhere, and we deal with data that can be represented as a graph by the following idea:</p>

<ul>
  <li>Each <strong>node</strong> in a network may have an <strong>associated feature vector</strong> that represents its attributes</li>
  <li>The edge information would be <strong>also encoded</strong> in the <strong>feature vector</strong> of the node
    <ul>
      <li>essentially, a node will <strong>aggregate information from neighbors</strong> to achieve this</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>In the end, a GNN basically attempts to <strong>encode much information about the graph</strong> into <strong>each node of the graph</strong>, resulting in some complicated <strong>embedded vector $h_i^l$ for each node $i$.</strong></p>

  <p>Then, this information is used to do downstream tasks such as node classification.</p>
</blockquote>

<p>Then, essentially we have a list of feature vectors representing the graph. With that, some <strong>common tasks</strong> include:</p>

<ol>
  <li><strong>Node prediction</strong>: Predicting a property of a graph node.</li>
  <li><strong>Link prediction</strong>: Predicting a property of a graph edge.
    <ul>
      <li>For example, in a social network we can predict whether two people will become friends</li>
    </ul>
  </li>
  <li><strong>Graph or sub-graph prediction</strong>: Predicting a property of the entire graph or a sub-graph.
    <ul>
      <li>For example, given a graph representation of a protein we can predict its function as an enzyme or not</li>
    </ul>
  </li>
</ol>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211222822911.png" alt="image-20220211222822911" style="zoom:50%;" /></p>

<h2 id="definitions">Definitions</h2>

<p>A graph $G = (V, E)$ contains a set of $n$ vertices (or nodes) $V$ and set of $m$ edges $E$ between vertices. The edges of the graph can either be undirected or directed.</p>

<p>Two basic graph representations are an <strong>adjacency matrix</strong> and <strong>adjacency list</strong>.</p>

<blockquote>
  <p>An <strong>adjacency matrix</strong> $A$ of dimensions $n \times n$ is defined such that:</p>

\[A_{i,j} = \begin{cases}
1, &amp; \text{if there is an edge between vertex $i$ and $j$}\\
0,&amp; \text{otherwise}
\end{cases}\]

  <p>where if edges have weights, then replace $1\to w$</p>
</blockquote>

<p>For example:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Adjacency Matrix</th>
      <th style="text-align: center">Graph</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211230849534.png" alt="image-20220211230849534" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211230856499.png" alt="image-20220211230856499" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>An <strong>adjacency list</strong> is a list of edges for each node.</p>
</blockquote>

<p><strong>Though</strong> adjacency matrix and list encode the same information:</p>

<ul>
  <li>different <strong>permutations of the node labels</strong> result in different adjacency matrices. In contrast, an adjacency list of the edges in the graph is invariant to node permutations.</li>
  <li><strong>storing</strong> an adjacency matrix takes $O(n^2)$ memory for $n$ being the number of nodes; whereas for adjaceny matrix it only takes $O(m)$ for $m$ is the number of edges in the graph. Since most graphs are sparse, this makes it even more appropriate to use adjacency list instead.</li>
</ul>

<blockquote>
  <p>The <strong>degree</strong> of a node $d_i$ represents the number of edges incident to that node (i.e. number of connections that it has to other nodes in the network)</p>

  <ul>
    <li>
      <p>average degree of a graph is the average degree over all its node:</p>

\[\frac{1}{n}\sum_{i=1}^n d_i\]

      <p>which is $2m/n$ for an undirected graph and $m/n$ for a directed graph</p>
    </li>
  </ul>

  <p>You can also represent the degree matrix $D$ being a diagonal matrix (so it can be used for calculating Laplacian):</p>

\[D_{i,i} = \text{degree}(v_i) = \sum_{j=1}^n A_{i,j}\]

</blockquote>

<p>For example, the degree matrix for the above graph:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211231752362.png" alt="image-20220211231752362" style="zoom:50%;" /></p>

<blockquote>
  <p>for directed graphs the <strong>indegree</strong> of a node is the number of edges leading into that node and its <strong>outdegree</strong>, the number of edges leading away from it.</p>
</blockquote>

<blockquote>
  <p>The <strong>graph Laplacian matrix</strong> $L$ is the difference between the degree matrix and adjacency matrix $L = D − A$.</p>

  <ul>
    <li>
      <p>for functions, Laplacian measures the <em>divergence</em>:</p>

      <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211232324624.png" alt="image-20220211232324624" style="zoom:67%;" /></p>
    </li>
    <li>
      <p>for more reference to see how it works: https://mbernste.github.io/posts/laplacian_matrix/</p>
    </li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211232434295.png" alt="image-20220211232434295" style="zoom:50%;" /></p>

<p>notice that:</p>

<ul>
  <li>The adjacency matrix and the degree matrix are symmetric and therefore the Laplacian matrix is symmetric</li>
</ul>

<p>Some other related matrix would be:</p>

<ul>
  <li>
    <p><strong>graph symmetric normalized Laplacian</strong></p>

\[\hat{L}=D^{-1/2} L D^{-1/2}=I-D^{-1/2} A D^{-1/2}\]
  </li>
  <li>
    <p><strong>random walk normalized Laplacian matrix</strong>:</p>

\[L_r=D^{-1}L = I-D^{-1}A\]
  </li>
</ul>

<blockquote>
  <p>A Laplacian matrix $L$ of a graph with $n$ nodes ha<strong>s $n$ eigenvectors with eigenvalues which are non-negative</strong> since the Laplacian matrix $L$ has non-negative eigenvalues.</p>

  <p>The number of <strong>zero eigenvalues</strong> of the Laplacian matrix of a graph is the <strong>number of its connected components</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Sub-graph</strong> of a graph is <strong>a subset of edges</strong> and <strong>all their nodes</strong> in the graph.</p>
</blockquote>

<blockquote>
  <p>A <strong>walk</strong> is a sequence of vertices and edges of a graph i.e. if we traverse a graph then we get a walk.</p>

  <ul>
    <li>a walk can be open or closed (i.e. end same as start)</li>
    <li>vertices and Edges can be <strong>repeated</strong> in a walk</li>
  </ul>
</blockquote>

<p>An example of a walk would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211233110467.png" alt="image-20220211233110467" style="zoom: 50%;" /></p>

<p>here <code class="language-plaintext highlighter-rouge">1-&gt;2-&gt;3-&gt;4-&gt;2-&gt;1-&gt;3</code> is a walk</p>

<blockquote>
  <p><strong>Trail</strong> is an open walk in which <strong>no edge is repeated</strong>.</p>

  <ul>
    <li>vertex can be <strong>repeated</strong></li>
  </ul>
</blockquote>

<p>For example:</p>

<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-4.png" alt="img" style="zoom: 33%;" /></p>

<p>here Here <code class="language-plaintext highlighter-rouge">1-&gt;3-&gt;8-&gt;6-&gt;3-&gt;2</code> is trail</p>

<blockquote>
  <p><strong>Path</strong> is a trail in which neither vertices nor edges are repeated</p>
</blockquote>

<blockquote>
  <p>The matrix $A^k$ from an adjacency matrix contains $A_{i,j}$ being the <strong>number of walks of length $k$</strong> in the graph between the node in row $i$ and the node in column $j$.</p>
</blockquote>

<h2 id="problem-using-graphs">Problem using Graphs</h2>

<p>Polynomial</p>

<ul>
  <li>
    <p><strong>Minimum Spanning Tree (MST)</strong>: For an undirected graph, produce an acyclic tree that is the subset of the graph that spans all of the Vertices (Spanning), and it needs to have a minimum sum in terms of the edges included (minimum)</p>

    <ul>
      <li>Greedy algorithms with time complexity $O(\vert E\vert \log\vert V\vert )$: <strong>Boruvka</strong>, <strong>Prim</strong>, <strong>Kruskal</strong></li>
      <li>similar to Dijkstra, basically a graph without cycles</li>
    </ul>
  </li>
  <li>
    <p><strong>Single-Source Shortest Paths (SSP)</strong></p>

    <ul>
      <li>
        <p>For SSP with nonnegative weights: <strong>Dijkstra’s</strong> algorithm. Complexity $O(\vert V\vert \log\vert V\vert  + \vert E\vert )$ using a heap</p>

        <ul>
          <li>
            <p>essentially the greedy step is that we set the node that is marked with <strong>smallest tentative distance</strong> as the current node/completed.</p>

            <p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Dijkstra_Animation.gif/220px-Dijkstra_Animation.gif" alt="Dijkstra Animation.gif" /></p>
          </li>
        </ul>
      </li>
      <li>
        <p>For general SSP: <strong>Bellman-Ford</strong> runs in $O(\vert V\vert \vert E\vert )$</p>
      </li>
    </ul>
  </li>
</ul>

<p>NP-hard</p>

<ul>
  <li><strong>Traveling Salesman Problem (TSP)</strong>
    <ul>
      <li>e.g. find shortest tour visiting each city once and returns to start.</li>
    </ul>
  </li>
  <li><strong>Vehicle Routing Problem (VRP)</strong></li>
</ul>

<blockquote>
  <p><em>Recall</em></p>

  <p><strong>Dijkstra’s Algorithm</strong>:</p>

  <ol>
    <li>initialization: marks everything as unvisited, and set each $D_v$ field to max. Then set the source $D_v$ to 0 and visited</li>
    <li>vistit each reachable vertices and update the field
      <ul>
        <li>update $D_v$ only when it is smaller</li>
        <li>but not update <em>visited</em>, yet</li>
      </ul>
    </li>
    <li>then we take the greedy step of marking the <strong>unvisited vertex with current smallest $D_v$</strong> to be visited</li>
    <li>continues with that vertex</li>
  </ol>

  <p><strong>Prim’s Algorithm</strong>
this is pretty much the same as Dijkstra’s Algorithm, but since we are constructing a tree, the difference is</p>

  <ul>
    <li><em>known</em> means whether if we have <strong>included that vertex into the tree</strong></li>
    <li>$D_v$ means the current smallest distance we currently know to bring that vertex into the graph (not cumulative)</li>
    <li>$P_v$ vertex that achieves the shortest distance in the $D_v$ field</li>
  </ul>
</blockquote>

<h2 id="node-embeddings">Node Embeddings</h2>

<blockquote>
  <p>Graph embeddings basically means finding “<strong>latent vector representation</strong>” of graphs which captures the <strong>topology</strong> (in very basic sense) of the graph. We can make this “vector representation” rich by also considering the vertex-vertex relationships, edge-information etc.</p>

  <ul>
    <li>the following assumes that ==each vertex/node has an associated feature vector==</li>
    <li>so in a sense it is “node embedding”</li>
  </ul>
</blockquote>

<p>There are roughly two levels of embeddings in the graph (of-course we can anytime define more levels by logically dividing the whole graph into subgraphs of various sizes):</p>

<ul>
  <li><strong>Vertex Embeddings</strong> - Here you find latent vector representation of <em>every vertex</em> in the given graph. You can then compare the different vertices by plotting these vectors in the space and interestingly “similar” vertices are plotted closer to each other than the ones which are dissimilar or less related. This is the same work that is done in “DeepWalk” by Perozzi.</li>
  <li><strong>Graph Embeddings</strong> - Here you find the latent vector representation of the whole graph itself. For example, you have a group of chemical compounds for which you want to check which compounds are similar to each other, how many type of compounds are there in the group (clusters) etc. You can use these vectors and plot them in space and find all the above information. This is the work that is done in “Deep Graph Kernels” by Yanardag.</li>
</ul>

<p><img src="https://snap-stanford.github.io/cs224w-notes/assets/img/node_embeddings.png?style=centerme" alt="node embeddings" style="zoom: 20%;" /></p>

<p>For example, we may optimize for the similarity between nodes $i$ and $j$, such that their similarity $s(i, j)$ is maintained after the embedding $f(i)^T f(j)$.</p>

<h3 id="shallow-embedding">Shallow Embedding</h3>

<p>“Shallow” encoding is the simplest embedding approach, it means it is just an embedding-lookup :</p>

\[f(v_i) =We_i\]

<p>for $e_i \in \mathbb{I}^{n}$ is essentially a <strong>one-hot encoded</strong> vector, and $W \in \mathbb{R}^{d \times n}$ if there are $n$ nodes.</p>

<ul>
  <li>This results in a problem with shallow embeddings, which is that they do not share weights, i.e. does not scale with the number of nodes</li>
</ul>

<h3 id="node-similarity">Node Similarity</h3>

<p>One key idea of embedding is that <strong>similarity of nodes</strong> is preserved in the embedding space. To formally use that as objective for learning the embedding (e.g. shallow encoding), we need to define <strong>similarity</strong></p>

<ul>
  <li>essentially different similarity metric captures <strong>different properties of a graph</strong>, results in <strong>different loss functions</strong> hence <strong>different embedding algorithm</strong></li>
</ul>

<h4 id="adjacency-based-similarity">Adjacency-based Similarity</h4>

<blockquote>
  <p><strong>Similarity</strong> between nodes $i$ and $j$ is the weight on the edge between them $s(i, j) = A_{i,j}$ where $A$ is the weighted adjacency matrix.</p>

  <ul>
    <li>this is a bit “bad” because non-neighbors will have weight $0$ as it is adjacency matrix</li>
  </ul>
</blockquote>

<p>Them, we can define <strong>loss</strong> to find the <strong>embedding matrix $W$</strong>:</p>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}||^2\]

<p>over all pairs of nodes in the graph</p>

<h4 id="multi-hop-similarity">Multi-hop Similarity</h4>

<blockquote>
  <p>Instead of only considering immediate neighbor in $A$, we can consider $k$-hop neighbors by using $A^k$ to be the <strong>adjacency matrix</strong>.</p>

  <ul>
    <li>an improvement over adjacency based similarity</li>
  </ul>
</blockquote>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-A_{i,j}^k||^2\]

<h4 id="overlap-similarity">Overlap Similarity</h4>

<p>Another measure of similarity is the overlap between node neighborhoods as shown in Figure 7.9.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212002606862.png" alt="image-20220212002606862" style="zoom: 67%;" /></p>

<p>Suppose nodes $i$ and $j$ share common nodes. We can then minimize the loss function <strong>measuring the overlap</strong> between neighborhood:</p>

\[\mathcal{L} = \sum_{(i,j) \in V \times V}||f(i)^Tf(j)-S_{i,j}||^2\]

<p>where:</p>

<ul>
  <li>$S_{i,j}$ could be Jaccard overlap or Adamic-Adar score</li>
</ul>

<h3 id="random-walk-embeddings">Random Walk Embeddings</h3>

<p>Essentially an alternative of shallow embedding.</p>

<blockquote>
  <p>A <strong>random</strong> walk in a graph begins with a node $i \in V$ and repeatedly walks to one of its neighbors $v\in N(i)$ with probability $1
/d(i)$for$t$steps until reaching and ending node$j$ on the graph.</p>

  <p>In essence:</p>

  <ol>
    <li>given a graph and a starting point, we select a neighbor of it at random</li>
    <li>move to this neighbor; then we select a neighbor of this point at random</li>
    <li>move to it, etc. until $t$ steps are gone</li>
  </ol>

  <p>So $\text{similarity}(u,v)$ is defined as the probability that $u$ and $v$ <strong>co-occur on a random walk</strong> over a network.</p>
</blockquote>

<p>Formally:</p>

\[f(i)^Tf(j) \propto P(\text{$i$ and $j$ co-occur on the random walk}) = p(i|j)\]

<p>So basically we want to learn node embedding such that <strong>nearby nodes are close together in the network</strong>. Hence we want to maximize the likelihood of random walk co-occurrences, we compute loss function as:</p>

\[\mathcal{L} = \sum_{i \in V}\sum_{j \in N(i)} - \log p(j|f(i))\]

<p>for</p>

\[P(j|f(i)) = \frac{\exp(f(i)^T f(j))}{\sum_{j \in V} \exp(f(i)^Tf(j))}\]

<p>Then basically we want to find $W$ for $f(i)=We_i$ such that the loss can be minimized.</p>

<h2 id="graph-embedding">Graph Embedding</h2>

<p>We may also want to embed an entire graph $G$ or subgraph in some applications</p>

<p><img src="https://snap-stanford.github.io/cs224w-notes/assets/img/graph_embedding.png?style=centerme" alt="GraphE" style="zoom:50%;" /></p>

<p>There are several ideas to accomplish graph embedding:</p>

<ol>
  <li>
    <p>The simple idea (Duvenaud et al., 2016) is to run a standard graph embedding technique on the (sub)graph GG, then just sum (or average) the node embeddings in the (sub)graph GG.</p>

    <ul>
      <li>i.e. taking the sum of the embeddings of the nodes in the sub-graph $\sum_{i \ni S} f(i)$</li>
    </ul>
  </li>
  <li>
    <p>Introducing a “virtual node” to <strong>represent</strong> the (sub)graph and run a standard graph embedding technique</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212000807810.png" alt="image-20220212000807810" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>We can also use <strong>anonymous walk embeddings</strong>. In order to learn graph embeddings, we could enumerate all possible anonymous walks aiai of ll steps and record their counts and represent the graph as a probability distribution over these walks. To read more about anonymous walk embeddings, refer to <em>Ivanov et al., Anonymous Walk Embeddings (2018)</em>.</p>
  </li>
</ol>

<h2 id="neighborhood-aggregation">Neighborhood Aggregation</h2>

<p>Once embedding in done, we finally <strong>start with GNN</strong>.</p>

<p>We consider graph neural networks (GNNs) which ==take into account neighbors of each node== (i.e. their embeddings)</p>

<ul>
  <li>if you naively think of concatenating feature vector $f(i)$ into things such as adjacency matrix. Then this will be problematic as: what if you wanted to add a node to the graph? Then the graph size changed -&gt; adj matrix size change -&gt; new architecture.</li>
  <li>basically, the number of parameters is <strong>linear in the size of the graph</strong>, the network is dependent on the order of the nodes and does not accommodate dynamic graphs</li>
</ul>

<blockquote>
  <p><strong>Desired Properties</strong></p>

  <ul>
    <li>Invariant to node ordering</li>
    <li>Locality, operations depend on neighbors of a given node</li>
    <li>Number of parameters independent of graph size</li>
    <li>Model independent of graph structure</li>
    <li>Able to transfer across graphs</li>
  </ul>
</blockquote>

<p>The problem above can be <strong>solved</strong> by:</p>

<ul>
  <li>
    <p>aggregating information from neighboring nodes in a BFS manner</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005141979.png" alt="image-20220212005141979" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>aggregating information in a chain, in a DFS manner</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005155734.png" alt="image-20220212005155734" style="zoom:50%;" /></p>
  </li>
</ul>

<p>Here we will discuss the first architecture, which can be visualized as:</p>

<ul>
  <li>for each node in the graph in turn,</li>
  <li>pick up the graph from that node as the root allowing all other nodes to dangle</li>
  <li>building a computation graph where that node is the root</li>
  <li>propagate and transform information from its neighbors, its neighbors’ neighbors etc, as shown below</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212005458064.png" alt="image-20220212005458064" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the information to collect from each vertex would be their <strong>node embedding</strong></p>
  </li>
  <li>
    <p>each grey box would contain <strong>weights</strong> for them to “transform”</p>
  </li>
</ul>

<p>Most graph neural networks are based on aggregating information into each node from it’s neighboring nodes in a layer in the above manner:</p>

\[h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}\]

<p>so that essentially:</p>

<ul>
  <li>$h_i^l$ is the feature representation of node $i$ <strong>at layer $l$</strong>
    <ul>
      <li>e.g $h_i^0$ is the 0 layer aggregation so $h_i^0 = f(i)$ is the raw embedding</li>
      <li>the feature vector $h_i^{i-1}$ of the previous <strong>layer embedding</strong></li>
    </ul>
  </li>
  <li>essentially we are doing for each $l$:
    <ul>
      <li>aggregate embedding from <strong>neighbors</strong></li>
      <li>combining with your <strong>previous embedding</strong></li>
    </ul>
  </li>
</ul>

<p>Next, we consider each node in turn, and generate a computation graph for each node where that node is the root. Finally, we will <strong>share the aggregation parameters</strong> across all nodes, for every layer of neighbors, as shown in Figure below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212012049523.png" alt="image-20220212012049523" style="zoom: 80%;" /></p>

<p>where the grey boxes are the aggregation parameter/weights.</p>

<p>Formalizing with weights:</p>

\[h_{i}^l = \sigma\left(B^l h_{i}^{l-1}+W^l \sum_{j \in N(i)}\frac{h_j^{l-1}}{|N(i)|} \right)\]

<p>where:</p>

<ul>
  <li>
    <p>this is essentially a <strong>recursive formula</strong>, the base case is:</p>

\[h_{i}^1 = \sigma\left(B^1 h_{i}^{0}+W^1 \sum_{j \in N(i)}\frac{h_j^{0}}{|N(i)|} \right)=\sigma\left(B^1 x_{i}+W^1 \sum_{j \in N(i)}\frac{x_j}{|N(i)|} \right)\]

    <p>for the $0$-th layer embedding is the same as the <strong>raw node embedding $x_i=f(i)$</strong>. Notice that ==embedding of the node itself== is included in this operation (along with its neighbors)!</p>
  </li>
  <li>
    <p>so technically each layer has two weights: <strong>$B^l$ is a matrix of weights for self-embedding</strong> and <strong>$W^l$ for neighbor embedding</strong>.</p>
  </li>
</ul>

<p>With this, we can now perform tasks such as <strong>node classification</strong>!</p>

<h3 id="supervised-node-classification">Supervised Node Classification</h3>

<p>For the task of node classification, given $m$ labeled nodes $i$ with labels $y_i$ we train a GNN by minimizing the objective:</p>

\[\mathcal{J}=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(y^i,\hat{y}^i)\]

<p>where the prediction $\hat{y}^i$ will be some neural network output based on the <strong>layered embedding $h_i^l$</strong> (at the last layer) we discussed before.</p>

<h2 id="gnn-architecture">GNN Architecture</h2>

<p>Essentially each architecture varies by <strong>how they perform aggregation</strong>:</p>

\[h_i^l = \text{combine}^l \{ h_{i}^{l-1}, \text{aggregate}^l\{h_{j}^{l-1},j\in N(i)\} \}\]

<h3 id="graph-convolution-network">Graph Convolution Network</h3>

<p>A graph convolution network (GCN) (Kipf &amp; Welling 2017) has a similar formulation using a single matrix for both the neighborhood and self-embeddings:\</p>

\[\begin{align*}
h_{i}^l
&amp;= \sigma\left( \frac{1}{\hat{d}_i}W^l h_{i}^{l-1} + \sum_{j \in N(i)}\frac{\hat{A}_{i,j}}{\sqrt{\hat{d}_j \hat{d}_i}}W^lh_{j}^{l-1} \right)
\end{align*}\]

<p>where:</p>

<ul>
  <li>where $\hat{A}= A+I$ is the adjacency matrix including <strong>self loops</strong>, $\hat{d}_i$ is the degree in the graph with <strong>self loops</strong>, and $\sigma$ a non-linear activation function</li>
</ul>

<h3 id="grated-graph-neural-networks">Grated Graph Neural Networks</h3>

<p>This is the second architecture mentioned in this section, which is similar to DFS: instead of sharing weights across neighborhoods (i.e. horizontally), <strong>weights are shared across all the layers in each computation graph (vertically)</strong>.</p>

<p>In gated graph neural networks (Li, Tarlow, Brockschmidt &amp; Zemel 2016) nodes aggregate messages from neighbors using a neural network, and similar to RNNs parameter sharing is across layers:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220212014256440.png" alt="image-20220212014256440" style="zoom: 67%;" /></p>

<h1 id="transformers">Transformers</h1>

<p>While the addition of gates allows LSTMs/GRUs to handle more <strong>distant information</strong> than RNNs, they don’t completely solve the underlying problem:</p>

<ul>
  <li>passing information through an <strong>extended series of recurrent connections</strong> leads to information <strong>loss</strong>
    <ul>
      <li>i.e. “too distant” information are still lost</li>
    </ul>
  </li>
  <li>Moreover, the inherently sequential nature of recurrent networks makes it <strong>hard to do computation in parallel.</strong></li>
</ul>

<blockquote>
  <p>These considerations led to the transformers development of <strong>transformers</strong> – an approach to sequence processing that ==eliminates recurrent connections== and returns to architectures reminiscent of the fully connected networks</p>

  <ul>
    <li>Transformers map sequences of input vectors $(x_1, …,x_n)$ to sequences of output vectors $y_1,…,y_n$ of the <strong>same length.</strong>
      <ul>
        <li>if our input is a sequence of tokens, then we can image each token represented as vector by $x_i=  Ew_i$ for an embedding matrix (e.g. see <a href="#SGNS Embeddings">SGNS Embeddings</a>)</li>
      </ul>
    </li>
    <li>Transformers are made up of stacks of transformer blocks, which are <strong>multilayer</strong> networks made by combining
      <ul>
        <li>simple linear layers</li>
        <li>feedforward networks (i.e. NN)</li>
        <li>self-attention layers/multihead attention (key innovation)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="self-attention">Self-Attention</h2>

<blockquote>
  <p><strong>Self-attention</strong> allows a network to directly ==extract== and use information from ==arbitrarily large contexts== without the need to pass it through intermediate recurrent connections as in RNNs.</p>

  <ul>
    <li>this means when processing each item in the input for $y_i$, the model <strong>has access to all of the inputs up to and including</strong> the one under consideration</li>
    <li>moreover, the computation performed for each item is <strong>independent of other computations</strong>. This allows for ==parallel computation==!</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>Heuristics</strong>:</p>

  <p>Given some data points you already know, e.g. $x_i, y_i$ being the <strong>keys</strong> and <strong>values</strong></p>

  <ul>
    <li>
      <p>then, you are <strong>given a query</strong> $x$, which you want to know the result $y$. The idea is to output:</p>

\[y = \sum_{i=1}^n a(x,x_i)y_i\]

      <p>so essentially the output for a query $x$ will <strong>be weighted average</strong> for the $x_i$ (keys) that we already know. Since we can say that <strong>$x_i$ near $x$ would be more important</strong> to consider:</p>

\[\alpha (x,x_i) = \frac{k(x,x_i)}{\sum_{j}k(x,x_j)}\]

      <p>for example $k$ kernel could be a <strong>gaussian kernel</strong>.</p>
    </li>
  </ul>

  <p>Hence, attention is essentially $\alpha(x,x_i)$, a measure of <strong>how relevant keys are to a query</strong></p>
</blockquote>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>inputs and outputs of the same length</li>
  <li>when processing each item in the input for $y_i$, the model <strong>has access to all of the inputs up to and including</strong> the one under consideration</li>
</ul>

<p>Then, a <strong>simple attention model</strong> would be to consider $y_i$ being some <strong>weighted version</strong>:</p>

\[y_i = \sum_{j \le i} \alpha_{ij} x_j\]

<p>where $\alpha_{ij}$ aims to capture the <strong>similarity/relevance</strong> of input $x_j$ for the <strong>current input $x_i$</strong>:</p>

\[\alpha_{ij} = \text{softmax}( \text{score}(x_i, x_j) ) = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j \le i\]

<p>where</p>

<ul>
  <li>
    <p>the key aim is to have $\alpha_{ij}$ compare <strong>an item of interest</strong> (i.e. $x_i$) to a <strong>collection of other items</strong> (i.e. $x_j, \forall j \le i$) in a way that ==reveals their relevance== in the current context</p>
  </li>
  <li>
    <p>a simple example would be relevance = similarity:</p>

\[\text{score}(x_i, x_i) = x_i \cdot x_j\]
  </li>
</ul>

<blockquote>
  <p><strong>Intuition</strong></p>

  <p>Though the above is simplified version it represents the core of an attention-based approach:</p>

  <ul>
    <li>a set of ==comparisons== to <strong>relevant</strong> items in some context</li>
    <li>normalization of those scores to provide a probability distribution</li>
    <li>the output of self-attention $y$ is a <strong>weighted sum</strong> of the inputs using the above distribution</li>
  </ul>
</blockquote>

<p>Transformers allow us to create a more sophisticated way of representing <strong>how words can contribute</strong> to the representation of longer inputs. In essence, we will have <strong>three input embeddings</strong>:</p>

<ul>
  <li><strong>query</strong>: current focus of attention $q_i = W^Q x_i$</li>
  <li><strong>key</strong>: the role of preceding input $k_j=W^Kx_j$, which will be compared against current focus $q_i$</li>
  <li><strong>value</strong>: used to compute the output for the current focus $v_j = W^V x_j$</li>
</ul>

<p>All the intermediate values are of dimension $d$, which means $W^Q,W^K, W^V$ <strong>all will be $\mathbb{R}^{d\times d}$</strong></p>

<ul>
  <li>this will be changed when we have a multi-headed attention, because technically we <strong>only needed $W^Q, W^K$ to be in the same dimension</strong>. (as we need dot products)</li>
</ul>

<p>These <strong>embedded inputs</strong> are then used for:</p>

\[\begin{align*}
\text{score}(x_i, x_j) &amp;= q_i \cdot k_j\\
\alpha_{ij} &amp;= \text{softmax}( \text{score}(x_i, x_j) )\\
y_i &amp;= \sum_{j \le i} \alpha_{ij} v_j
\end{align*}\]

<p>where:</p>

<ul>
  <li>the second step is the same as in our simple model</li>
  <li>now $q_i \cdot k_j$ measures the <strong>relevance</strong> between a <strong>query and key</strong> (e.g. a SQL search <em>query</em>, and the data <em>keys</em> you have in your table)</li>
  <li>finally, the weighted sum of $v_j = W^V x_j$ consists of the focus $x_i$ itself outputs the result <strong>value</strong></li>
</ul>

<p>Graphically, for computing $y_3$ in our previous example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207203739212.png" alt="image-20220207203739212" style="zoom: 50%;" /></p>

<p>note that:</p>

<ul>
  <li>
    <p>in reality, the dot product for score could be <strong>very large</strong>, so that having $\text{softmax}$ later on rasing it to some power will cause overflow. Hence we usually do:</p>

\[\text{score}(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}\]

    <p>for $d_k$ being the <strong>dimensionality of the query/key vector</strong>.</p>
  </li>
</ul>

<p>Finally, we can convert everything to a <strong>single matrix-matrix multiplication</strong> as each $y_i$ is independent:</p>

\[Q = XW^Q; K = XW^K; V=XW^V\]

<p>where we are packing the input embeddings of the $N$ tokens/words of the input sequence into a single matrix $X \in \mathbb{R}^{N \times d}$.</p>

<ul>
  <li>i.e. each <strong>row vector</strong> is the embedding of a token</li>
  <li>then $Q,K,V \in \mathbb{R}^{N\times d}$</li>
</ul>

<p>Then the final output can be done in a <strong>single shot of $Y \in \mathbb{R}^{N \times d}$</strong> as:</p>

\[Y = \text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

<p>where:</p>

<ul>
  <li>
    <p>this can be parallelized and is fast as it is just a matrix-matrix multiplication</p>
  </li>
  <li>
    <p>==but for langue modelling in guess next word==, the part $QK^T$ compute the <strong>full matrix</strong>:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207204740770.png" alt="image-20220207204740770" /></p>

    <p>but we obviously want $q_1 \cdot k_2$ to be not there (so that $\text{Softmax}$ of this gives $0$). Hence we will need <strong>upper-triangular values to be set to $-\infty$</strong>. (otherwise we are “seeing the next word” when we want to predict the next word)</p>
  </li>
  <li>
    <p>this makes it clear that the computation is $O(N^2)$ for $N$ being the length of text you are inputting. Hence, generally you want to <strong>avoid</strong> putting in long texts such as Wikipedia pages/novels.</p>
  </li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Essentially what happens is that each output $y_t$ will take into <strong>account the entire input sequence $x_i$</strong>, but have <strong>weights</strong> on more relevant ones to the key (i.e. $x$) to spit out $y_t$ as <strong>a weighted average of the values</strong> (i.e. $y_i$).</p>

<ul>
  <li>
    <p>think of this as output $y$ being a weighted average of:</p>

\[y = \sum_i \alpha(x,x_i)\cdot y_i\]

    <p>for $x$ being the query, $y$ being what we want, and $x_i,y_i$ are known inputs to be keys and values.</p>
  </li>
</ul>

<p>A concrete example would be. consider computing $y_{t=1}$ from three inputs of dimension $4$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210164535973.png" alt="image-20220210164535973" style="zoom: 50%;" /></p>

<p>Then, essentially this is what happens:</p>

<ol>
  <li>
    <p>Compute key, value, query representation of all input (from some embedding matrix $W^Q,W^K,W^V$)</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210164631592.png" alt="image-20220210164631592" style="zoom: 50%;" /></p>

    <p>so essentially query $q$ is the final question we are interested in</p>
  </li>
  <li>
    <p>Calculate the <strong>attention score</strong> for $y_{t=1}$, essentially meaning how important each data key $k_1,…,k_3$ is relevant to the query</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165117963.png" alt="image-20220210165117963" style="zoom: 50%;" /></p>

    <p>which basically is doing a SoftMax as mentioned before</p>
  </li>
  <li>
    <p>Then, you use the attention score to <strong>scale the values</strong> (i.e. doing $\alpha(x,x_i)y_i$):</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165421872.png" alt="image-20220210165421872" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>Finally, you sum up the weighted values to spit out $y_{t=1}$:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210165351646.png" alt="image-20220210165351646" style="zoom: 50%;" /></p>

    <p>(note that technically value would be of dimension $4$, so that the final output has the same dimension as input)</p>
  </li>
</ol>

<blockquote>
  <p><strong>Take Away Message</strong></p>

  <p>Like RNN architectures that remembers information of a sequence, self-attention does it even <strong>better</strong> by <strong>taking the entire input sequence into consideration</strong> at each time for output.</p>

  <ul>
    <li>this means parallelization of code</li>
    <li>i.e. <strong>each output</strong> (e.g. token) will have attended to (with weights) the <strong>entire input sequence</strong></li>
  </ul>
</blockquote>

<h2 id="transformer-blocks">Transformer Blocks</h2>

<p>The core of a transformer composes of <strong>transformer blocks</strong>, which essentially consists of:</p>

<ul>
  <li><strong>self-attention</strong> layer (e.g. a multihead attention layer = multiple self-attention layer)</li>
  <li><strong>normalization</strong> layer</li>
  <li><strong>feedforward</strong> layer</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Single Self-Attention Layer</th>
      <th style="text-align: center">Multihead Self-Attention</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209004304005.png" alt="image-20220209004304005" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209004414819.png" alt="image-20220209004414819" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where here:</p>

<ul>
  <li>we will mainly address the transformer block with single self-attention first.</li>
</ul>

<p>From the left figure, we can summarize what we are doing as:</p>

\[\begin{align*}
z &amp;= \text{LayerNorm}(x+ \text{SelfAttn}(x))\\
y &amp;= \text{LayerNorm}(z+ \text{FFNN}(z))
\end{align*}\]

<p>where we are outputting $y$</p>

<ul>
  <li>
    <p>we have <strong>Layer normalization</strong> (or layer norm) because it can be used to <strong>improve training performance</strong> in deep neural networks by keeping the values of a hidden layer in a range that <strong>facilitates gradient-based training</strong>:</p>

\[\text{LayerNorm}(x) = \gamma \hat{x} + \beta\]

    <p>for $\gamma, \beta$ are <strong>learnable parameters</strong> representing gain and offset, and $\hat{x}$ is the <strong>normalized version of $x$</strong>:</p>

\[\hat{x} = \frac{x-\mu}{\sigma}\]
  </li>
  <li>
    <p><strong>residual connection</strong>, as mentioned before, allows information from the activation going forward and the gradient going backwards to skip a layer <strong>improves learning</strong> and gives higher level layers <strong>direct access to information from the past</strong></p>
  </li>
  <li>
    <p><strong>input and output dimensions</strong> of these blocks are <strong>matched</strong>/the same so they can be stacked just as was the case for stacked RNNs.</p>
  </li>
</ul>

<p>(but what are the inputs $x$? You will soon see that the vector $x$ would come from $x+p$ which is the embedding of the input + positional embedding of the input)</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Now you may wonder what is the embedding used for tokens, i.e. $x$?</p>

  <ul>
    <li>e.g. BERT uses Wordpiece embeddings for tokens.</li>
    <li>In fact, the full input embedding for a token is a <strong>sum</strong> of the <strong>token embeddings</strong>, the <strong>segmentation embeddings</strong>, and the <strong>position embeddings</strong>.</li>
  </ul>
</blockquote>

<h3 id="multihead-attention">Multihead Attention</h3>

<p>Why are we not satisfied with single self-attention? A single word in a sentence can relate to each other in <strong>many different ways</strong> simultaneously!</p>

<ul>
  <li>It would be difficult for a single transformer block to learn to capture all of the different kinds of parallel relations among its inputs. (e.g. syntactic, semantic, and discourse relationships)</li>
  <li>Transformers address this issue with <strong>multihead self-attention</strong> layers.</li>
</ul>

<p>Therefore, the idea is that we have sets of <strong>self-attention layers</strong>, called ==heads==, that <strong>reside in a parallel fashion</strong>. The aim is that we want <strong>each self-attention layer/head</strong> capture <strong>different</strong> aspects of the relationships that exists among inputs:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209010338126.png" alt="image-20220209010338126" style="zoom: 67%;" /></p>

<p>where essentially, in the end we need input and output <strong>both</strong> of <strong>dimension $d$</strong>:</p>

<ul>
  <li>
    <p>here we have four heads, $h=4$</p>
  </li>
  <li>
    <p>since we want to capture different relationships, <strong>each head</strong> consist of its <strong>own set of key, query, value embedding matrix</strong> $W_i^K,W_i^Q,W_i^V$ for head $i$</p>
  </li>
  <li>
    <p>then, what we do inside each head/self-attention is the same as what we have covered before</p>
  </li>
  <li>
    <p>remember that embeddings does not need to have the same dimension as input (which is $d$). Also recall that we <strong>only needed key and query to be of the same dimension</strong>, therefore, here we have:</p>

\[W_i^Q,W_i^K \in \mathbb{R}^{d \times d_k};\quad W_i^V \in \mathbb{R}^{d \times d_v}\]

    <p>Then, if we pack them with inputs:</p>

\[Q_i = XW_i^Q \in \mathbb{R}^{N \times d_k}; \quad K_i = XW_i^K \in \mathbb{R}^{N \times d_k};\quad  V_i=XW_i^V\in \mathbb{R}^{N \times d_v}\]

    <p>for <strong>each</strong> head $i$. Then the final output <strong>for each head</strong> is essentially <strong>the same as mentioned before</strong>:</p>

\[A_{i}(Q_i, K_i, V_i) = \text{SelfAttn}_i(Q_i,K_i,V_i) = \left[\text{Softmax}\left(\frac{Q_iK_i^T}{\sqrt{d_k}}\right)\right]V_i\]

    <p>which is of shape $N \times d_v$</p>
  </li>
  <li>
    <p>Remember that input of <strong>$N$ tokens</strong> of dimension $d$ has size $X \in \mathbb{R}^{N \times d}$. Now we have essentially $h \times N \times d_v$. Therefore, we perform:</p>

    <ol>
      <li>concatenating the outputs $A_i$ from each head</li>
      <li>using a linear projection $W^O \in \mathbb{R}^{hd_v \times d}$ for the concatenated outputs.</li>
    </ol>

    <p>Therefore we have:</p>

\[\text{MultiHeadAttn}(X) = [\text{head}_1 \oplus ... \oplus \text{head}_h] W^O\\
\text{head}_i = \text{SelfAttn}_i(Q_i,K_i,V_i)\]

    <p>so that the final output is of size $N \times d$, which is the same as input.</p>
  </li>
</ul>

<p>Then, since output size is the same as input size, we can stack them easily like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209012338218.png" alt="image-20220209012338218" style="zoom: 67%;" /></p>

<p>where in this case the next layer is also a multihead attention layer.</p>

<ul>
  <li>notice that the input is now $X+P$, which is input embedding + positional embedding, which we will cover next</li>
</ul>

<h3 id="positional-embedding">Positional Embedding</h3>

<p>How does a transformer <strong>model the position of each token</strong> in the input sequence? With <strong>RNNs</strong>, information about the order of the inputs was ==built into== the structure of the model</p>

<ul>
  <li>we want to learn the relative, or absolute, positions of the tokens in the input</li>
</ul>

<p><strong>Solution</strong>: modify input embedding by $X:= X+P$ for $P$ being ==positional embedding==.</p>

<p>For example, we can do the following being the <strong>absolute positional embedding</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209013143904.png" alt="image-20220209013143904" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>A potential problem with the simple absolute position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits</li>
  <li>here we are mapping each input embedding vector $x_i$ to a <strong>scalar $p_i$</strong>, which is absolute positional encoding as it is <strong>relative to a global origin</strong> of $0$.
    <ul>
      <li>problems of this include that numbers/encoding will be big for long sequences, which could cause exploding gradients</li>
      <li>if we simply divide by sequence length, the problem is $4/5=8/10=12/15$ all equals $0.8$ but signifies different position.</li>
    </ul>
  </li>
</ul>

<p>It turns out that what we use is this</p>

\[P_{pos,2i} = \sin\left(\frac{pos}{10000^{2i/d}}\right),\quad P_{pos,2i+1} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\]

<p>where we essentially use a <strong>combination of sine and cosine</strong>, $\text{pos}$ is the position of word in the sentence</p>

<ul>
  <li>(resource on explaining how we got there: https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)</li>
</ul>

<h2 id="encoder-and-decoder">Encoder and Decoder</h2>

<blockquote>
  <p><strong>Encoder-decoder networks</strong>, or sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences.</p>

  <ul>
    <li>an <strong>encoder</strong> network that takes an input sequence and <strong>creates a contextualized representation</strong>, $c$</li>
    <li>a <strong>decoder</strong> then takes that context $c$ which generates a <strong>task specific output sequence</strong></li>
  </ul>
</blockquote>

<p>To be more specific</p>

<ul>
  <li>An <strong>encoder</strong> that accepts an input sequence, $x_1,…x_n$, and generates a corresponding sequence of contextualized representations, $h_1,…h_n$</li>
  <li>A <strong>context vector</strong>, $c$, which is a function of $h_1,…,h_n$, conveys the essence of the input to the decoder.</li>
  <li>A <strong>decoder</strong>, which accepts $c$ as input (first hidden state) and generates an arbitrary length sequence of hidden states $h_1,…,h_m$, from which a corresponding sequence of output states $y_1,…,y_m$, can be obtained</li>
</ul>

<blockquote>
  <p>The important thing of this idea is that LSTMs, convolutional networks, and Transformers <strong>can all be employed as encoders or decoders</strong>.</p>
</blockquote>

<p>Therefore, you will soon see that there are three main types of transformers:</p>

<ul>
  <li>encoder only</li>
  <li>decoder only</li>
  <li>encoder + decoder</li>
</ul>

<hr />

<p><em>For Example</em>, as you might have seen before, consider the simple RNN encoder-decoder architecture for MT:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210183154677.png" alt="image-20220210183154677" style="zoom:67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>
    <p>the <strong>context vector</strong> is simply the <strong>last state of encoder</strong> (you will see how useful Attention is soon)</p>
  </li>
  <li>The <strong>decoder</strong> autoregressively generates a sequence of outputs (i.e. output of previous state becomes input of next state, like a regression), an element at a time, until an end-of-sequence marker is generated.</li>
  <li>Each hidden state of decoder is conditioned on the previous hidden state and the output generated in the previous state</li>
</ul>

<hr />

<h3 id="attention">Attention</h3>

<p>This is also referred to as <strong>encoder-decoder attention</strong></p>

<hr />

<p><em>Recall</em>: this is useful when we have a encoder and decoder of RNN/LSTM/GRU like the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220209015006640.png" alt="image-20220209015006640" style="zoom:80%;" /></p>

<p>where:</p>

<ul>
  <li><strong>without attention</strong>, the input to decoder would <strong>only be the last state</strong> of encoder. So attention mechanism is a solution to the bottleneck problem, a way of allowing the <strong>decoder</strong> to get information from <strong>all the hidden states of the encoder</strong> (essentially by a weighted sum over all the states)</li>
  <li>then, the <strong>weights $\alpha_{ij}$</strong> will focus on (‘attend to’) a particular part of the source text that is <strong>relevant for the token the decoder is currently producing</strong>.
    <ul>
      <li>this means that $c_i$ will be different for each token in decoding</li>
    </ul>
  </li>
</ul>

<hr />

<blockquote>
  <p><strong>Goal</strong></p>

  <p>We want to find a way that allows the <strong>decoder</strong> to get information from <strong>all the hidden states of the encoder</strong>, not just the last hidden state.</p>

  <ul>
    <li>we can’t use the entire tensor of encoder hidden state vectors directly as the context for the decoder, as the number of hidden states <strong>varies</strong> with the size of the input</li>
  </ul>
</blockquote>

<p>Essentially, the idea of attention is similar to that of ==self-attention==, but the difference is that the <strong>query</strong> we care about is the <strong>decoding state $h^d_t$</strong>, whereas in self-attention both query and keys are the input.</p>

<ul>
  <li>
    <p>i.e. here we care about: <strong>how relevant is each information from encoder $h^e_i$ to the current decoding step $h^d_t$?</strong></p>
  </li>
  <li>
    <p>therefore, the idea is the same: we are doing some kind of <strong>weighted average of keys $h^e_i$</strong>.</p>
  </li>
</ul>

<blockquote>
  <p><em>Recall</em></p>

  <p>Given some query $x$, and we want the prediction $y$:</p>

\[y = \sum_{i=1}^n a(x,x_i)y_i\]

  <p><strong>Attention</strong> is essentially weights $\alpha(x,x_i)$, a measure of <strong>how relevant keys $x_i$ are to a query $x$</strong></p>
</blockquote>

<p>Therefore, the idea for <strong>encoder-decoder attention</strong> is to consider relevant between $h^e_j,\forall j$ and the current query $h_{i-1}^d$ (we want $h_i^d$, which is not computed yet)</p>

<ol>
  <li>the <strong>decoder</strong> receives as input the <strong>encoder encoder output sequence</strong> $c_i=c_i(h^e_1, …,h^e_t)$
    <ul>
      <li>essentially everything the <strong>encoder</strong> has got</li>
    </ul>
  </li>
  <li>different parts of the output sequence <strong>pay attention</strong> to <strong>different parts of the input sequence</strong>
    <ul>
      <li>essentially, given that we want to compute $h^d_t$, how much weight should be give to each input/stuff we know $h^e_{j}, \forall j$</li>
    </ul>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Simple Encoder-Decoder</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210185934352.png" alt="image-20220210185934352" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially:</p>

<ul>
  <li>
    <p>This context vector, $c_i$, is generated anew with <strong>each decoding step $i$</strong> and takes all of the encoder hidden states into account by a weight computed as:</p>

\[\alpha_{ij}  = \frac{\exp \left( \text{score}(h^d_{i-1},h^d_j ) \right)}{\sum_{k=1}^i \text{exp}( \text{score}(h^d_{i-1},h^d_j))}, \quad \forall j\]

    <p>which obviously <strong>depends on which output stage $i=t$ decoder is at</strong>. Then, simply do:</p>

\[c_i = \sum_{j} \alpha_{ij}h_j^e\]

    <p>being a weighted sum of all the encoder hidden states.</p>
  </li>
  <li>
    <p>then, obviously it is left for us to design a <strong>score function</strong> (e.g. dot product) that reflects the <strong>relevance between $h_{i-1}^d,h^e_{j}$</strong>, so that we could “pay more attention” to only specific $h_j^e$</p>
  </li>
</ul>

<p>Another more complicated example would be:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Encoder-Decoder with Bidirectional</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207210550496.png" alt="image-20220207210550496" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li>
    <p>the context vector is a function $c_i=c_i(o_1, …,o_t)$ instead, for $o_t= V\begin{bmatrix}h_t\\bar{h}_t\end{bmatrix}$ for a bidirectional layer shown below, or sometimes just $o_i = [ h_j;\bar{h}_j]^T$.</p>
  </li>
  <li>
    <p>specifically, in this <strong>bidirectional RNN</strong> as encoder, the context vector is computed as:</p>

\[c_i = \sum_j \alpha_{ij} [ h_j;\bar{h}_j]^T\]

    <p>which is a <strong>weighted sum</strong> over <strong>all hidden states in the encoder</strong> (but will have a larger emphasis on $h_i$). And the weightings $\alpha_{ij}$ are computed by:</p>

\[\alpha_{ij}  = \frac{\exp ( \text{score}(x_i,x_j) )}{\sum_{k=1}^i \text{exp}( \text{score}(x_i , x_k))}, \quad \forall j\]

    <p>which basically represents the <strong>amount of attention</strong> output $o_i$ will give to input word $x_j$, for some score function.</p>
  </li>
</ul>

<h2 id="word-embeddings">Word Embeddings</h2>

<p>For more details, please refer to the <strong>NLP notes</strong> on the section “Vector Semantics”.</p>

<p>The general idea is that:</p>

<ul>
  <li>we want to replace 1-hot word representation with lower dimensional feature vector for each word</li>
  <li>we can do this by learning word embedding from <strong>large unsupervised text corpus</strong>
    <ul>
      <li>e.g. distribution models, logistic regression models (e.g. Word2Vec), etc.</li>
    </ul>
  </li>
</ul>

<p>The goal is to find <strong>an embedding matrix $E$</strong> that takes a one-hot encoded word and returns an embedding. The general approach <strong>machine learning</strong> takes is to do the following:</p>

<ol>
  <li>input: each word in a vocabulary being a one-hot encoded vector</li>
  <li>initialize an embedding matrix $E$ randomly.</li>
  <li>for each sentence/window of words, <strong>mask one word out</strong></li>
  <li>now, given the context words $w_c$, our job is to output <strong>vector of probability</strong> $y$ for the masked word, so that $y[i]$ corresponds to the probability that word $i$ in the dictionary is the masked word
    <ol>
      <li>convert all words to $e_c = Ew_c$</li>
      <li>put it in a neural network, e.g. FFNN, to learn the weights and <strong>update the embedding matrix $E$</strong></li>
    </ol>
  </li>
  <li>After minimization, output $E$</li>
</ol>

<p>For instance, consider the following sentence in our training data:</p>

\[\text{Mary had a little lamb whose \textbf{fleece} was white as snow}\]

<p>We would like to mask the word <em>fleece</em> out. Then, we construct the following network:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210135056641.png" alt="image-20220210135056641" style="zoom:50%;" /></p>

<p>The idea is to <strong>learn by predicting the masked word</strong>, e.g. <em>fleece</em></p>

<ul>
  <li>what is the probability that the masked word (i.e. <em>fleece</em> is masked) given the current context words/sentences in the window</li>
  <li>$\theta$ will be the weights in the FFNN, which we don’t care about in the end</li>
  <li>since we are just masking words in a sentence, we are creating a supervised training set from <strong>unsupervised set</strong>!</li>
</ul>

<p>Once you trained your embedding matrix, you can even <strong>fine tune that to your specific task/vocabulary</strong>. For example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210135530591.png" alt="image-20220210135530591" style="zoom: 50%;" /></p>

<p>where input will use an embedding matrix $E$ that we have trained before</p>

<ul>
  <li>
    <p>then use the pre-trained embedding and update that during gradient descent as well</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ENCODER</span><span class="p">.</span><span class="n">get_vocabulary</span><span class="p">())</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="o">=</span><span class="n">LSTM_param</span><span class="p">[</span><span class="s">'units'</span><span class="p">],</span>
    <span class="n">embeddings_initializer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">EMBEDDING_MATRIX</span><span class="p">),</span> <span class="c1"># pretrained embedding
</span>    <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># whether if to fine tune
</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="beam-search">Beam Search</h2>

<p>The decoding algorithm we gave above for generating translations has a <strong>problem</strong> (as does the autoregressive generation we introduced in Chapter 9 for generating from a conditional language model:</p>

<ol>
  <li>at each time step in decoding, the output $y_t$ is chosen by computing a softmax over the set of possible outputs</li>
  <li>then, <strong>only</strong> the vocabulary with the <strong>highest probability</strong> is picked
    <ul>
      <li>this is also called <strong>greedy decoding</strong></li>
    </ul>
  </li>
</ol>

<p>Indeed, greedy search is not optimal, and <strong>may not find the highest probability translation</strong> as in the end we have a list of tokens.</p>

<p>For instance, consider the goal of <strong>generating a sentence</strong> from the decoder:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210200927842.png" alt="image-20220210200927842" style="zoom:67%;" /></p>

<p>where we see that:</p>

<ul>
  <li>the <strong>most probable sentence</strong> should be “ok ok <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>” with probability $0.4 \times 0.7 \times 1$</li>
  <li>the <strong>greedy</strong> algorithm would have picked <em>yes</em> as the first word, which is suboptimal</li>
</ul>

<p>Now, since this tree is growing as $t$ grows, this turns out to be an <strong>exhaustive search</strong> (dynamic programming will not work). This is obviously not good, so we instead consider the method called <strong>beam search</strong>:</p>

<ol>
  <li>instead of choosing the best token to generate at each timestep, we <strong>keep $k$ possible tokens at each step</strong></li>
  <li>At subsequent steps, <strong>each</strong> of the $k$ best hypotheses is extended incrementally by being passed to <strong>distinct decoders</strong>
    <ul>
      <li>i.e. if we have $k=2$ for the first level, then we will have <strong>2 distinct decoders</strong> for the next, as shown below</li>
    </ul>
  </li>
  <li>Each of these hypotheses is scored by $P(y_i \vert  x,y_{&lt;i})$, basically just multiplying the probability.</li>
  <li>Then, we <strong>prune the hypothesis</strong> down to the best $k$, so there are <strong>always only $k$ decoders</strong>.</li>
</ol>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210201708295.png" alt="image-20220210201708295" style="zoom:67%;" /></p>

<p>where we chose $k=2$ as an example:</p>

<ul>
  <li>at $t=2$, we have four hypothesis initially, but then we pruned down to $2$ again. Therefore, we always have only $k=2$ decoders at a time.</li>
</ul>

<p>More details on how we compute the probability score:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202257019.png" alt="image-20220210202257019" style="zoom:67%;" /></p>

<p>Then, we if use <strong>log probability</strong>, at each step, to compute the probability of a partial translation, we simply <strong>add</strong> the log probability of the prefix translation so far to the log probability of generating the next token:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202356694.png" alt="image-20220210202356694" style="zoom: 67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>here we <strong>terminated with 2 sentences of different length</strong>. This maybe problematic as sentences with shorter length will have a higher probability</li>
</ul>

<p>Therefore, we would consider some <strong>normalization</strong> against the length of sentences:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202626578.png" alt="image-20220210202626578" style="zoom: 80%;" /></p>

<p>for a sentence of length $T$.</p>

<p>This ends up with the following algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210202738922.png" alt="image-20220210202738922" style="zoom:80%;" /></p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>What do we do with the resulting $k$ hypotheses?</p>

  <ul>
    <li>In some cases, all we need from our MT algorithm is the single best hypothesis, so we can return that.</li>
    <li>In other cases our downstream application might want to look at all $k$ hypotheses, so we can pass them all (or a subset) to the downstream application with their respective scores.</li>
  </ul>
</blockquote>

<h2 id="encoder-decoder-with-transformers">Encoder-Decoder with Transformers</h2>

<p>The encoder-decoder architecture can also be <strong>implemented using transformers</strong> (rather than RNN/LSTMs) as the component modules</p>

<ul>
  <li>An <strong>encoder</strong> essentially consist of <strong>stacks of transformer blocks</strong>, typically $6$ layers</li>
  <li>A <strong>decoder</strong> as well consists of stacks of transformer blocks</li>
</ul>

<p>For instance, it could look like the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210212353425.png" style="zoom:67%;" /></p>

<p>where we notice that:</p>

<ul>
  <li>the only thing we don’t know is <strong>cross-attention</strong>, which is also very <strong>similar to the encoder-decoder attention</strong>, which we have covered before.</li>
  <li>the <strong>input of decoder</strong> is the auto-regressive output of itself generated</li>
</ul>

<hr />

<p><strong>A more detailed visualization at decoding</strong>:</p>

<p>Essentially for decode, we usually do it <strong>auto-regressively</strong> so that suppose our ==decoder== has <code class="language-plaintext highlighter-rouge">Attention</code> with <code class="language-plaintext highlighter-rouge">MAX_SEQ_LEN=32</code>. Then:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/transformer_decoding_1.gif" style="zoom: 50%;" /></p>

<p>the input to decoder done <strong>auto-regressively</strong>:</p>

<ul>
  <li>at $t=0$, there is no input/or we have <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;</code> for filling the <code class="language-plaintext highlighter-rouge">32</code> sequence length and a positional embedding</li>
  <li>get cross attention from encoder output</li>
  <li>generate an output “<code class="language-plaintext highlighter-rouge">I</code>” and feed back as input at $t=1$. So we <strong>get <code class="language-plaintext highlighter-rouge">&lt;pad&gt;&lt;pad&gt;...&lt;s&gt;I</code> as the input of decoder</strong></li>
  <li>repeat until <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> is generated</li>
</ul>

<blockquote>
  <p><strong>Note</strong>: Comparing with using a RNN based decoder, when we are generating output $o_{t+1}$, the <em>difference</em> is:</p>

  <ul>
    <li><strong>transformer</strong> based can <strong>only condition/read in</strong> $o_{t-31},…,o_t$ for a max sequence length of <code class="language-plaintext highlighter-rouge">32</code> for the attention layer</li>
    <li><strong>RNN</strong> based can use $h_t$ which ==encodes all previous information==. However, it has ==no attention/is sequential==.</li>
  </ul>
</blockquote>

<hr />

<p>To zoom in a bit, we see that</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210212608845.png" alt="image-20220210212608845" style="zoom: 67%;" /></p>

<p>Now, we have:</p>

<ul>
  <li>
    <p>an <strong>encoder</strong> that takes the source language input words $X=x_1, …., x_t$ and maps them to an output representation $H_{enc}=h_1,…,h_t$, as with the encoders discussed before</p>
  </li>
  <li>
    <p>a <strong>decoder</strong> then attends to the <strong>encoder representation</strong> and generates the target words one by one</p>

    <ul>
      <li>here, at each timestep conditioning on the source sentence (weighted by attention) and the previously generated target language words (see previous figure, essentially output $h^d_{t-1}$ is piped in as input)</li>
    </ul>
  </li>
  <li>
    <p><strong>Cross-attention</strong> has the same form as the <strong>multi-headed self-attention</strong> in a normal transformer block, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder. (i.e. the input/output is similar to the <strong>encoder-decoder attention</strong> discussed in <a href="#Attention">Attention</a>)</p>
  </li>
  <li>
    <p><strong>Self-attention</strong> in <strong>encoder</strong> is allowed to look ahead at the entire source language text, i.e. weights will be distributed on the entire input sequence</p>
  </li>
  <li>
    <p><strong>Casual Self-attention</strong> in <strong>decoder</strong> is what we have covered in the section <a href="#Self-Attention">Self-Attention</a>, so that we only have <strong>previous label inputs available</strong>:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220207193040566.png" alt="image-20220207193040566" style="zoom: 50%;" /></p>

    <p>(otherwise we will be cheating, as we are predicting the next word yet the next word is already available to us). This is sometimes also referred to as <strong>left-to-right attention</strong>, as it attends to input sequentially left to right.</p>
  </li>
</ul>

<p>Since the only thing “new” here is the cross attention, we focus on that. Essentially we need to consider what is $x,x_i,y_i$ analogue from the equation $\sum \alpha(x,x_i)\cdot y_i$ in this case.</p>

<ul>
  <li>
    <p>the <strong>query</strong>, i.e. $x$, will be sequence of <strong>the output of previous decoder layer</strong> $H^{dec[i-1]}$. We want to know the “answer” to this query</p>
  </li>
  <li>
    <p>the <strong>key, value</strong> will be the <strong>final output of encoder</strong> $H^{enc}=h_1 ,…,h_t$ (stuff we want to pay attention to)</p>
  </li>
</ul>

<p>Then, since it is similar to self-attention, we will have <strong>weights</strong> $W^Q,W^K,W^V$:</p>

\[Q=W^QH^{dec[i-1]};\quad K=W^KH^{enc};\quad V=W^VH^{enc}\]

<p>Then the rest is the same as the weights we calculated in self-attention:</p>

\[\text{CrossAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

<p>The cross attention thus allows the decoder to attend to each of the source language words with weights that depend on current focus (query).</p>

<h2 id="transformer-models">Transformer Models</h2>

<p>The big thing to distinguish between models would be:</p>

<ul>
  <li>encoder/decoder or both?</li>
  <li><strong>what is the optimization objective</strong>?
    <ul>
      <li>e.g. masked word to predict next, then likely a <strong>decoder only transformer</strong> (e.g. GPT)</li>
      <li>e.g. masked word/fill in blank prediction and next sentence prediction, then likely a <strong>encoder only transformer</strong> (e.g. BERT)</li>
    </ul>
  </li>
  <li><strong>what is the dataset</strong> that it trained on?</li>
</ul>

<h3 id="bert">BERT</h3>

<p>Let’s begin by introducing the <strong>bidirectional transformer</strong> encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020).</p>

<blockquote>
  <p>When applied to <strong>sequence classification</strong> and <strong>labeling problems</strong> causal models have obvious shortcomings since they are based on an incremental, <strong>left-to-right processing</strong> of their inputs.</p>

  <ul>
    <li>Bidirectional encoders overcome this limitation by allowing the self-attention mechanism to range over the entire input, as shown below</li>
    <li>hence, it is not suitable for tasks such as predict the next word</li>
  </ul>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Causal Transformer</th>
      <th style="text-align: center">Bidirectional Transformer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210220737829.png" alt="image-20220210220737829" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210220745062.png" alt="image-20220210220745062" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially BERT is doing the one on the right.</p>

<ul>
  <li>as you shall soon notice, BERT is a <strong>encoder only transformer</strong></li>
</ul>

<p>Essentially, inside BERT we have essentially stacks of transformer blocks for encoder/decoder:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221236252.png" alt="image-20220210221236252" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p><strong>bidirectional</strong> encoders use self-attention to map sequences of <strong>input</strong> embeddings $x_1,…,x_n$ to sequences of <strong>output</strong> embeddings $y_1,…,y_n$ of the same length. Again, the aim is to produce <strong>contextualized vectors</strong> that takes in information from the entire input sequence</p>
  </li>
  <li>
    <p>the contextualization in <strong>self-attention</strong> is one in the same manner covered in section <a href="#Self-Attention">Self-Attention</a>, so the formulas are still:</p>

\[\text{SelfAttention}(Q,K,V) = \left[\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\right]V\]

    <p>for input $X$, and <strong>embedding matrix</strong> $W^K,W^Q,W^V$ will be learned.</p>

\[Q = XW^Q; K = XW^K; V=XW^V\]

    <p>However, the difference is that for <strong>left-to-right</strong> models, we had the matrix $QK^T$ being:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Left-To-Right</th>
          <th style="text-align: center">Bidirectional</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221709470.png" alt="image-20220210221709470" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210221644852.png" alt="image-20220210221644852" /></td>
        </tr>
      </tbody>
    </table>

    <p>so basically we removed the mask</p>
  </li>
  <li>
    <p>Beyond this simple change, all of the other elements of the transformer architecture remain the same for bidirectional encoder models</p>
  </li>
</ul>

<p>However, this is very useful because it is a multi-purpose model. best results in <strong>many different tasks!</strong></p>

<ul>
  <li><strong>Inputs</strong> to the model are segmented using <strong>subword tokenization</strong> and are combined with <strong>positional embeddings</strong>. To be more precise, recall that for transformers, ==input dimension and output dimension are the same==:
    <ul>
      <li>The input embeddings are the sum of the token embeddings, the segmentation embeddings, and the position embeddings.</li>
      <li>BERT uses ==Wordpiece embeddings== for tokens</li>
    </ul>
  </li>
  <li>always remember, essentially what those model learn are <strong>weights</strong> $W$.</li>
</ul>

<h4 id="training-bert">Training BERT</h4>

<p>Since the entire context is available instead of trying to predict the next word, the model learns to perform a <strong>fill-in-the-blank</strong> task, technically called the <strong>cloze task</strong>.</p>

<ul>
  <li>in fact, a lot of the models are trained in this format. Vision Transformer also does it by masking patches in the image and predicting the patch.</li>
</ul>

<p>For instance, ==instead of== doing:</p>

\[\text{Please turn your homework \_\_\_\_}.\]

<p>For bidirectional transformers:</p>

\[\text{Please turn \_\_\_\_ homework in}.\]

<p>Therefore, the idea is that:</p>

<ul>
  <li>during training the model is <strong>deprived of one or more elements of an input sequence</strong> and must generate/<strong>output a probability distribution</strong> over the vocabulary for <strong>each of the missing items</strong></li>
  <li>then, it is essentially again a supervised training from self-supervised data, where loss would simply be <strong>cross entropy.</strong></li>
</ul>

<p>For example, an example architecture would look like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210222426656.png" alt="image-20220210222426656" style="zoom:67%;" /></p>

<p>so we see that:</p>

<ul>
  <li>
    <p>a token part of a training sequence is used in one of three ways:</p>

    <ul>
      <li>It is replaced with the unique vocabulary token <code class="language-plaintext highlighter-rouge">[MASK]</code>.</li>
      <li>It is replaced with another token from the vocabulary, randomly sampled based on token unigram probabilities.</li>
      <li>It is left unchanged.</li>
    </ul>

    <p>For BERT, 80% are masked, 10% replaced with randomly selected tokens, and 10% are unchanged</p>
  </li>
  <li>
    <p>training objective is to <strong>predict the original inputs</strong> for each of the masked tokens using a bidirectional encoder</p>
  </li>
  <li>
    <p>To produce a probability distribution over the vocabulary for each of the masked tokens, the <strong>output</strong> vector from the final transformer layer for <strong>each of the masked tokens</strong> is:</p>

\[y_i = \text{Softmax}(W_Vh_i)\]

    <p>for some weights $W_V$ which will be learnt.</p>
  </li>
</ul>

<h4 id="transfer-learning-through-fine-tuning">Transfer Learning through Fine-Tuning</h4>

<p>The idea of pretrained model is simple. Consider a pretrained BERT (so that weights such as $W^K,W^V,W^Q$ are already learnt), and you want to use it to do tasks such as <strong>sequence classification</strong></p>

<ul>
  <li>e.g. sentiment analysis</li>
</ul>

<p>The kind of question you need to think about is:</p>

<ul>
  <li>transformer based encoders have <strong>output shape same as input shape</strong>. Therefore, essentially we can get embeddings for each <strong>word</strong> in the context. But here we would like to obtain an <strong>embedding of the entire sequence</strong> for classification. Where do we get that?</li>
  <li>Given an embedding of entire sequence, what should we do next?</li>
  <li>Can we <strong>fine tune weights from pretrained models</strong> as well?</li>
</ul>

<p>In essence, this is what happens:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210224802368.png" alt="image-20220210224802368" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>We need an additional vector is added to the model to stand for the <strong>entire sentence sequence</strong>. This vector is sometimes called the <strong>sentence embedding</strong>. In BERT, the <code class="language-plaintext highlighter-rouge">[CLS]</code> token plays the role of this embedding. (i.e. the embedding of this token is what we want)
    <ul>
      <li>This unique token is added to the vocabulary and is prepended to the start of all input sequences, both during pretraining and encoding</li>
    </ul>
  </li>
  <li>
    <p>onece we have this <strong>sentence embedding</strong>, we can pipe this into our own <strong>neural network/logistic regression classifier</strong>, which basically learns a weight $W_c$. Then, our output for <strong>classification</strong> essentially comes from:</p>

\[y = \text{Softmax}(W_C y_{cls})\]
  </li>
  <li>we can also <strong>update the pretrained weights</strong> during our fine-tuning as well: In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often <strong>limited to updates over the final few layers</strong> of the transformer.</li>
</ul>

<hr />

<p><em>For Example</em>: Sequence Labeling</p>

<p>To demonstrate the multipurposeness of BERT, consider the task of <strong>part-of-speech tagging</strong> or <strong>BIO-based named entity recognition</strong>. Since we know that given a sequence of words, BERT produces a sequence of <strong>embeddings</strong>:</p>

<ul>
  <li>the final output vector corresponding to <strong>each input token embedding</strong> is passed to a classifier (since each token will be tagged)</li>
  <li>then, we just need to <strong>learn the weights in the classifier</strong> $W_K$</li>
</ul>

<p>So essentially:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210225529070.png" alt="image-20220210225529070" style="zoom:67%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>notice that the embedding for <code class="language-plaintext highlighter-rouge">CLS</code> is useless in this case</p>
  </li>
  <li>
    <p>our output tag for each token will come from:</p>

\[y_i = \text{Softmax}(W_K z_i)\]

    <p>for output from BERT will be $z_i$, and then we can <strong>greedily assign tags as</strong>:</p>

\[t_i = \arg\max_k(y_i)\]

    <p>for a tag consist of $k$ classes.</p>
  </li>
</ul>

<h3 id="gpt-2">GPT-2</h3>

<p>The GPT-2 is built using <strong>transformer decoder blocks</strong>. BERT, on the other hand, uses <strong>transformer encoder blocks</strong>.</p>

<p>We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, <strong>outputs one token at a time</strong>:</p>

<p><img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif" alt="img" style="zoom:50%;" /></p>

<p>notice that:</p>

<ul>
  <li>The way these models actually work is that after each token is produced, that <strong>token is added to the sequence of inputs</strong>. And that new sequence becomes the input to the model in its next step. This is an idea called “<strong>auto-regression</strong>”</li>
</ul>

<p>Therefore, essentially the model contains:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232055303.png" alt="image-20220210232055303" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>unlike BERT which uses bidirectional transformer as it wants to <strong>encode contextualized information</strong>, GPT cares more about <strong>next word prediction</strong>, hence it reverted to the <strong>masked/left-to-right self-attention</strong> during training:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232238810.png" alt="image-20220210232238810" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>alike encoders of course, we need some <strong>input to start with</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210232603576.png" alt="image-20220210232603576" style="zoom:50%;" /></p>

    <p>it turns out that essentially you would also have input <strong>embedded</strong> before putting into your model.</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>If you compare the architecture of encoder and decoder, they are kind of similar:</p>

  <table>
    <thead>
      <tr>
        <th style="text-align: center">Encoder Only</th>
        <th style="text-align: center">Decoder Only</th>
        <th style="text-align: center">Encoder-Decoder</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233157758.png" alt="image-20220210233157758" style="zoom: 67%;" /></td>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233050700.png" alt="image-20220210233050700" /></td>
        <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210233259239.png" alt="image-20220210233259239" style="zoom:67%;" /></td>
      </tr>
    </tbody>
  </table>

  <p>where:</p>

  <ul>
    <li>both encoder and decoder are transformer blocks, so <strong>output shape same as input shape</strong> (applications bascially only do something to the output of the last layer)</li>
    <li>encoder and decoder used alone have a slightly different attention mechanism, with the latter being usually a <strong>masked/left-to-right attention</strong></li>
    <li>when you have encoder+decoder, you then need a <strong>cross-attention</strong> to get stuff across from encoder to decoder</li>
    <li>the <strong>output embedding</strong> in the decoder is essentially the auto-regressive genreated output. i.e. at $t=0$, it is fed with <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code> and suppose it generated <code class="language-plaintext highlighter-rouge">hello</code>. Then at $t=1$, fed with <code class="language-plaintext highlighter-rouge">&lt;s&gt;, hello</code>.</li>
  </ul>
</blockquote>

<h3 id="efficient-transformers">Efficient Transformers</h3>

<p>One problem now is that <strong>attention computation</strong> is quadratic in the length of sentences.</p>

<p><strong>Solution</strong>: linear time transformer by approximations, transfer learning</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220210234034616.png" alt="image-20220210234034616" style="zoom:67%;" /></p>

<h1 id="generative-adversarial-network">Generative Adversarial Network</h1>

<p>Generative adversarial networks (GANs) are an <strong>unsupervised generative model</strong> used for ==generating samples== that are similar to training examples. GANs have also been used for generating images from text, generating audio from images, and generating images from audio, etc.</p>

<ul>
  <li>we are having a generator that <strong>learns the structure of the data</strong></li>
</ul>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>Essentially you have <strong>two models</strong>, a generator and a discriminator:</p>

  <ul>
    <li><strong>generator</strong> is trained to produce fake examples that fool the discriminator, hence its loss is based on the success of  discriminator</li>
    <li><strong>discriminator</strong> learns to distinguish between the generator’s fake synthesized samples and real data</li>
  </ul>

  <p>Therefore, you will have essentially a <strong>minimax optization problem</strong> that looks like:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]

  <p>where essentially:</p>

  <ul>
    <li>discriminator outputs a <strong>probability</strong> (that it is real) given some data $x$, and generator <strong>generates</strong> some data $G(z)$ from <strong>noise</strong> $z$ from some distribution (e.g. Gaussian)</li>
    <li>the loss says discriminator $D(x)$ should be as high as possible, so that discriminator is doing well, and $D(G(z))$ should be as high as possible as well, so that generator is doing well</li>
    <li>The generator and the discriminator are trained alternately, for $G(z)$ being descent first, and then $D(x)$.</li>
  </ul>

  <p>Then, essentially we update the two models with the two losses:</p>

  <ul>
    <li>
      <p>For <strong>generator</strong>, the the gradient step is:</p>

\[\nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}\]

      <p>in some other algorithm, we could maximize $\log D(G(z))$ instead</p>
    </li>
    <li>
      <p>For <strong>discriminator</strong>, we will have the same as maximization problem as before, so the gradient step is:</p>

\[\nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}\]
    </li>
  </ul>

  <p>In practice</p>

  <ul>
    <li>
      <p>once we finished training, we will take the generator to deploy in our various appications.</p>
    </li>
    <li>
      <p>relevant problems include <strong>saturation problem</strong>, <strong>mode collapse</strong> and <strong>convergence problem</strong>, which will be discussed in more detail later.</p>
    </li>
  </ul>
</blockquote>

<p>Note that:</p>

<ul>
  <li>The idea of minimax is not new. In game theory, we think about <em>minimizing</em> the loss involved when the <strong>opponent</strong> selects the strategy that gives <em>max</em>imum loss</li>
  <li>from a biological point of view, this is essentially co-evolution</li>
  <li>The discriminator and generator form two dueling networks with opposite objectives. This <strong>unsupervised</strong> setting eliminates the need for labels, since the <strong>label is whether the sample is real or not</strong>.</li>
</ul>

<p>On a bigger context, since we are trying to learn the given data distribution:</p>

<ul>
  <li>we can learn it <strong>explicity</strong>
    <ul>
      <li>VAE</li>
      <li>Markvo Chain</li>
    </ul>
  </li>
  <li><strong>implicitly</strong>
    <ul>
      <li>GAN</li>
    </ul>
  </li>
</ul>

<h2 id="minimax-optimization">Minimax Optimization</h2>

<p>Graphically, this is what the architecture looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 67%;" /></p>

<p>the joint optimization problem for the two agents is:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]

<p>where we are taking <strong>expected value</strong> over real data, but in <strong>practice</strong> we can only do empirical mean</p>

<ul>
  <li>The term $D(x)$ is the discriminator’s estimated probability that <strong>(real) data $x$ is real</strong>
    <ul>
      <li>The goal of the discriminator is to classify correctly real and generated data.</li>
    </ul>
  </li>
  <li>$G(z)$ is the output of the generator given random noise $z$.
    <ul>
      <li>the goal is to generate a signal from random noise $z \sim P(z)$ in a way that it will be difficult for the discriminator to distinguish</li>
    </ul>
  </li>
  <li>$D(G(z))$ is the discriminator’s estimated probability that a <strong>synthesized sample is real</strong>.
    <ul>
      <li>the goal of geneartor is to make $D(G(z)) \to 1$</li>
    </ul>
  </li>
</ul>

<p>Since we care about the <strong>network parameters</strong> in the end:</p>

\[\min_\phi\max_\theta V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]\]

<p>then <strong>how do we optimize this</strong>?</p>

<ul>
  <li>Since both the generator and discriminator will be represented by neural networks, problem is non-convex and non-concave</li>
  <li>hence we can only do <strong>gradient descent type solution</strong></li>
  <li>The first term of $\log D(x)$ is independent of the generator and therefore the <strong>generator only minimizes</strong> the function $\log(1 − D(G(z)))$.</li>
</ul>

<p>However, there is a problem with this loss. In practice, it saturates for the generator, meaning that the generator quite frequently <strong>stops training</strong> if it doesn’t catch up with the discriminator, so that:</p>

\[\nabla_\phi \log(1-D_\phi(G_\theta(z))) \to 0\]

<p>when the <strong>discriminator becomes optimal</strong> ($D$ is close to $D^*$). So gradient vanishes and <strong>generator</strong> cannot evolve!</p>

<h3 id="mode-collapse">Mode Collapse</h3>

<p>This issue is on the unpredictable side of things. It wasn’t foreseen until someone noticed that the generator model could only generate one or <strong>a small subset of different outcomes</strong> or modes, instead of a range of outputs.</p>

<blockquote>
  <p>If at each iteration the generator optimizes for the specific discriminator and the <strong>discriminator is not able to correctly classify the synthesized samples</strong> as fake, the generator will synthesize a small set of samples, not diverse samples, which is known as <strong>mode collapse</strong></p>
</blockquote>

<p>This may happen when:</p>

<ol>
  <li>In the process of training, the <strong>generator is always trying to find the one output</strong> that seems most plausible to the discriminator.</li>
  <li>if the <strong>next generation of discriminator gets stuck</strong> in a local minimum and doesn’t find its way out by getting its weights even more optimized, it’d get easy for the next generator iteration to find the most plausible output for the current discriminator.</li>
  <li>This way, it will keep on repeating the same output and refrain from any further training.</li>
</ol>

<p>When the discriminator using <strong>Wasserstein loss</strong> does not get stuck in local minima it learns to reject the generator’s repeated synthesized samples, encouraging the generator to synthesize new samples and diversify.</p>

<p>One solution is that:</p>

<ul>
  <li>In order to void mode collapse, encouraging the generator to diversify the synthesized samples and not optimize for a constant discriminator, the <strong>generator</strong> loss function may be modified to include <strong>multiple subsequent discriminators</strong></li>
</ul>

<h3 id="convergence-problem">Convergence Problem</h3>

<p>The utopian <strong>situation where both networks stabilize and produce a consistent result is hard to achieve in most cases.</strong></p>

<ul>
  <li>If the generator succeeds all the time, the discriminator has a 50% accuracy, similar to that of flipping a coin. This poses a threat to the convergence of the GAN as a whole.</li>
</ul>

<p>If discriminator is just randomly guessing, then as the <strong>discriminator’s feedback loses its meaning</strong> over subsequent epochs by giving outputs with equal probability, the <strong>generator may deteriorate its own quality</strong> if it continues to train on these junk training signals.</p>

<h2 id="divergence-between-distributions">Divergence between Distributions</h2>

<blockquote>
  <p><strong>Key Question</strong>:</p>

  <p><strong>Why</strong> did we choose a loss function in the form of:</p>

\[\begin{align*}
V(D_\phi,G_\theta)
&amp;=\mathbb{E}_{x \sim P_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{z \sim P_{z}(z)}[\log (1-D_\phi(G_\theta(z)))]\\
&amp;=\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]
\end{align*}\]

  <p>specifically, why using $\log$ probability?</p>

  <p>The short <strong>answer</strong> is:</p>

  <ul>
    <li>loss function here represents the <strong>distance</strong> between the <strong>distribution</strong> of the synthesized samples and the distribution of the real data</li>
    <li>so essentially we want to <strong>minimize the difference</strong> between our approximate distribution $P_G$ from generator and the true distribution $P_{data}$</li>
  </ul>

  <p>In fact, if $D_\theta(x)$ can be any function, it can be shown that the above is <strong>equivalent to minimizing the JS divergence</strong> between <strong>distribution $P_{data}$ and $P_{G}$</strong>.</p>

  <ul>
    <li>for more detail, checkout https://colinraffel.com/blog/gans-and-divergence-minimization.html</li>
  </ul>
</blockquote>

<p>Essentially, our goal is to <strong>minimize the distribution divergence</strong> between true distribution $P_{data}$ and our generated one $P_G$. To do this, we need a metric to measure <strong>“distance/difference”</strong> between two distributions, and the simplest one would be the KL divergence.</p>

<h3 id="kl-divergence">KL Divergence</h3>

<p>KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data, i.e. how many <strong>bits do we need to specify the data</strong>. This idea is captured in Entropy:</p>

\[H = -\sum_{i=1}^N p(x_i) \cdot \log p(x_i)\]

<p>where:</p>

<ul>
  <li>if $\log_2$, then we can interpret entropy as “the minimum number of bits it would take us to encode our information”.</li>
  <li>just looking at extreme values, if $p(x_i)=0$ or $p(x_i)=1$, then there is nothing random and entropy is $0$. For $p(x_i)=0.5$, entropy is large.</li>
</ul>

<p>Then, KL divergence is basically a measure of “<strong>how many bits of information we expect to lose</strong>” if we use $q(x_i)$ to approximate $p(x_i)$:</p>

\[D_{KL}(p||q) = \sum_{i=1}^N p(x_i)\cdot (\log p(x_i) - \log q(x_i))=\sum_{i=1}^N p(x_i)\cdot \log\frac{p(x_i)}{q(x_i)}\]

<p>Then:</p>

<ul>
  <li>
    <p>this is essentially the same as saying:</p>

\[D_{KL}(p||q)=\mathbb{E}[\log p(x) - \log q(x)]\]

    <p>which reminds us of the <strong>original GAN loss</strong>.</p>
  </li>
  <li>
    <p>for a <strong>continuous distribution</strong>:</p>

\[D_{KL}(p||q)=\int p(x)\cdot \log\frac{p(x)}{q(x)}\,dx\]
  </li>
  <li>
    <p>a <strong>reversed KL divergence</strong> is then:</p>

\[D_{KL}(q||p)=\int q(x)\cdot \log\frac{q(x)}{p(x)}\,dx\]

    <p>for $q$ being our approximate distribution.</p>
  </li>
  <li>
    <p>therefore, this means KL divergence is <strong>non-negative and asymmetric</strong></p>
  </li>
</ul>

<p>The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$</p>

<h3 id="jensen-shannon-divergence">Jensen-Shannon Divergence</h3>

<p>The Jensen-Shannon (JS) divergence $D_{JS}$ is a symmetric smooth version of the KL divergence defined by:</p>

\[D_{JS}(p||q) = \frac{1}{2}D_{KL}(p||m)+\frac{1}{2}D_{KL}(q||m),\quad m=\frac{1}{2}(p+q)\]

<p>The analogy in our loss function for GAN is to consider $p = P_{data}$ and $q=P_{G}$. In fact, this formulation of divergence metric leads to the <strong>formulation of loss</strong>:</p>

\[\mathbb{E}_{x \sim P_{data}}[\log D_\phi(x)]+\mathbb{E}_{\hat{x} \sim P_{G}}[\log (1-D_\phi(\hat{x})]\]

<h3 id="bregman-divergence">Bregman divergence</h3>

<p>The KL divergence and JS divergence are both <strong>special cases of the Bregman divergence</strong>.</p>

<p>The Bregman divergence is defined by a convex function F and is a <strong>measure of distance between two points $p$ and $q$</strong> defined by:</p>

\[D_F(p,q)= F(p)-F(q)-\lang \nabla F(q), p-q \rang\]

<p>where then, many <strong>divergence metrics</strong> come from defining some <strong>convex function $F$</strong>:</p>

<ul>
  <li>
    <p>if $F(p)=p\log p$, then we recover the <strong>generalized KL divergence</strong>:</p>

\[\begin{align*}
D_F(p,q)
&amp;= p\log p - q \log q - (\log q + 1)(p-q)\\
&amp;= p \log(\frac{p}{q}) + (q-p)
\end{align*}\]
  </li>
  <li>
    <p>if $F(p)=p\log p - (p+1)\log(p+1)$, we get <strong>JS divergence</strong>.</p>
  </li>
  <li>
    <p>if $F=(1-p)^2$, we get a <strong>Pearson $\chi^2$ divergence</strong>, which leads to ==loss in least square GAN==.</p>

    <p>The loss for discriminator being:</p>

\[\mathbb{E}_{x \sim P_{data}}[D_\phi(x)^2]-\mathbb{E}_{z \sim p_{z}(z)}[D_\phi(G_\theta(z))^2]\]

    <p>and the loss for geneator:</p>

\[\mathbb{E}_{z \sim p_{z}(z)}[(D_\phi(G_\theta(z))-1)^2]\]

    <p>providing a smoother loss</p>
  </li>
</ul>

<h3 id="optimal-objective-value">Optimal Objective Value</h3>

<p>Now, we come back to our discussion of the original loss function. Since the original loss is essentially <strong>minimizing JS divergence</strong> between $P_{data}$ and $P_{G}$, we obviously want the optimal solution to be $P_G=P_{data}$.</p>

<ul>
  <li>in practice, this is <strong>not computable</strong> since it involves knowing the true distribution. Therefore, in practice we just do <strong>gradient descent type updates</strong>.</li>
</ul>

<p>Here, we show that the above statement is true. For <strong>discriminator</strong>, we want:</p>

\[\max_\phi V(D_\phi,G_\theta)=\mathbb{E}_{x \sim p_{data}(x)}[\log D_\phi(x)]+\mathbb{E}_{\hat{x}\sim p_G}[\log (1-D(\hat{x}))]\]

<p>Hence:</p>

\[\mathcal{L}(x;\theta) = P_{data}\cdot \log D(x) + P_G \cdot \log(1-D(\hat{x}))\]

<p>taking derivative and setting it to zero, we obtain:</p>

\[D^*(x)=\frac{P_{data}}{P_{data}+P_G}\]

<p>Then plugging this back in to solve for <strong>generator</strong>:</p>

\[\min_G  V(G,D^*) = 2D_{JS}(P_{data}||P_G)-2\log 2\]

<p>Then if you plugin to the equation for JS divergence, you will realize that this is <strong>minimized</strong> if $P_{data}=P_G$.</p>

<h2 id="gradient-descent-ascent">Gradient Descent Ascent</h2>

<p>Since in reality we don’t know the true distribution, we only have samples from it, we can only <strong>update our guesses</strong> in a gradient descent type of manner as the overall loss is non-convex and non-concave.</p>

<p>In our setting we use a stochastic variant of GDA with mini-batches, in which the ==descent== update for the <strong>generator</strong> neural network is:</p>

\[\nabla_{\theta}\left\{ \frac{1}{m} \sum_{i=1}^m \log \left(1-D_\phi(G_\theta(z^{(i)})) \right)  \right\}\]

<p>so that we have performed the <strong>$\min_G$</strong> step, and then our <strong>discriminator</strong> will have an ==ascent== update:</p>

\[\nabla_{\phi}\left\{ \frac{1}{m} \sum_{i=1}^m \left[\log D(x^{(i)}) + \log(1-D(G(z^{(i)})))\right]  \right\}\]

<p>note that:</p>

<ul>
  <li>
    <p>If $V$ were convex-concave then playing the game simultaneously or in a sequential order would not matter; however, in our case $V$ is non-convex non-concave and the <strong>order matters</strong></p>
  </li>
  <li>
    <p>Unfortunately, GDA may converge to points that are not local minimax or fail to converge to a local minimax. A modification of GDA (Wang, Zhang &amp; Ba 2020) which partially addresses this issue is:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184103651.png" alt="image-20220211184103651" style="zoom:67%;" /></p>

    <p>which is a modified gradient ascent rule for $f=V$​. This converges and ==only converges to local minimax points==, driving the gradient quickly to zero and improving GAN convergence</p>
  </li>
</ul>

<h3 id="optimistic-gradient-descent-ascent">Optimistic Gradient Descent Ascent</h3>

<p>When introduced, GANs were implemented using momentum. However, later on the implementations did not use momentum, and using a <strong>negative momentum made the saturating GAN work</strong>. An algorithm which solves the minimax optimization problem by using negative momentum is <strong>optimistic gradient descent ascent (OGDA)</strong> (Daskalakis et al. 2017).</p>

<ul>
  <li>this is fonud in empirical experiences</li>
</ul>

<p>The negative momentum update is:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184326881.png" alt="image-20220211184326881" style="zoom: 67%;" /></p>

<p>where $f=V$ in our case.</p>

<ul>
  <li>the one with $x$ is for gradient descent, the one with $y$ is for gradient ascent</li>
  <li>OGDA yields better empirical results than GDA, and can be interpreted as an approximation of the proximal point method</li>
</ul>

<h2 id="gan-training">GAN Training</h2>

<p>When the generator training is successful the discriminator cannot distinguish between real data and fake samples synthesized
by the generator. So baically the architecture is as follows:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211162705009.png" alt="image-20220211162705009" style="zoom: 50%;" /></p>

<p>where both generator and discriminator are represented by neural networks and are both trained by backpropagation.</p>

<p>The algorithm for training is as follows, essentially we train them ==alternatingly==</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211184827501.png" alt="image-20220211184827501" style="zoom:50%;" /></p>

<p>where the end goal is <strong>we get good quality images from the generator</strong> and discriminator won’t be able to differentiate between real and fake images.</p>

<ul>
  <li>Training the <strong>discriminator</strong> uses real data as positive examples and samples synthesized by the generator as negative examples
    <ul>
      <li>When the discriminator is trained, the ==generator is not trained== and its parameters are held fixed</li>
      <li>discriminator loss serves as a signal to the generator for updating its parameters by backpropagation</li>
    </ul>
  </li>
  <li>the <strong>generator</strong> learns to synthesize realistic samples by the feedback it receives from the discriminator
    <ul>
      <li>During generator training the ==discriminator parameters are held fixed==.</li>
    </ul>
  </li>
  <li>we could have also reversed the training to train generator first, but the result will be difference as order matters.</li>
</ul>

<h2 id="gan-losses">GAN Losses</h2>

<p>As described, different Bregman divergences and loss functions have been explored with the goals of improving GAN training stability and diversity. Here we describe a few that is important.</p>

<h3 id="wasserstein-gan">Wasserstein GAN</h3>

<p>One problem with JS divergence is that, if the real data distribution and generator distribution do not overlap then the JS divergence is zero $D_{JS} = 0$</p>

<ul>
  <li>when distributions have non-overlapping support</li>
  <li>(support of a function is a <strong>closure</strong> $S={x \in \mathbb{R}^n:f(x) \neq 0}$)</li>
</ul>

<p>Graphically:</p>

<p><img src="https://i.stack.imgur.com/NbLrO.png" alt="enter image description here" style="zoom: 40%;" /></p>

<p>Fortunately, this issue has been <strong>resolved</strong> by using the Earth Mover’s Distance (EMD) or <strong>Wasserstein-1 distance</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211200143137.png" alt="image-20220211200143137" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>where $\gamma$ denotes how much mass, or earth, must be moved from $x$ to $y$ in order to transform distribution $P$ into distribution $Q$,</li>
  <li>$\Pi (P,Q)$ denotes the set of all disjoint distributions with marginals $P$ and $Q$.</li>
</ul>

<p>Graphically, we are doing:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215142125264.png" alt="image-20220215142125264" style="zoom:33%;" /></p>

<p>where $P_r$ is the real distribution, and the distance between $P_r,P_\theta$ is to measure how many “piles of dirt” dow we need to shovel from $P_r\to P_\theta$ so that the distributions are the same.</p>

<p>Computing $W(P,Q)$ is <strong>intractable</strong> since it requires considering all possible combinations of pairs of points between the two distributions, computing the mean distance of all pairs in each combination, and taking the minimum mean distance across all combinations.</p>

<p>Fortunately, an alternative is to <strong>solve a dual maximization problem</strong> that is tractable, which results in the ==Wasserstein loss==:</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[ D(x)]-\mathbb{E}_{z \sim p_{z}(z)}[D(G(z))]\]

<p>if you use WGAN, which is Wasserstein loss:</p>

<ul>
  <li>this loss outputs <strong>a real value</strong> that is <strong>larger for real data</strong> than <strong>synthesized</strong> samples
    <ul>
      <li>as compared to the original GAN uses the minimax loss in which the discriminator outputs a probability in $[0, 1]$ of a sample being real or synthesized</li>
      <li>therefore, the WGAN discriminator is called a critic since it does not output values in $[0, 1]$ for performing classification</li>
    </ul>
  </li>
  <li>There is no sigmoid in the final layer of the discriminator and the range is $[−\infty,\infty]$.</li>
</ul>

<p>Then, this means that the <strong>generator</strong> loss functions is:</p>

\[\min_G -\mathbb{E}_{z \sim p_{z}(z)}[D(G(z)]\]

<p>The <strong>discriminator</strong> loss function is the entire loss.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Recall that mode collapse happens when a <strong>discriminator hits a local minium</strong>. Then, if it fails to reject the fake sample from generator, <strong>geneartor would only generate a subset of the distribution</strong>, which is mode collapse.</p>

  <p>However, Wasserstein loss ==prevents discriminator from hitting a local minimum==, hence allowing it to learn to escape.</p>
</blockquote>

<h3 id="loss-function-summaries">Loss Function Summaries</h3>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215140041605.png" alt="image-20220215140041605" style="zoom: 50%;" /></p>

<h2 id="gan-architectures">GAN Architectures</h2>

<p>Firs, an example of the choice of generator/discriminator</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215140610509.png" alt="image-20220215140610509" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>this would be an example of generator (left to right). Discriminator <em>could</em> be the same architecture but left to right</li>
  <li>therefore, the generator above is also called a <strong>transposed CNN</strong></li>
</ul>

<hr />

<p>Below are some nowadays application of the GAN idea</p>

<ul>
  <li>
    <p><strong>Progressive GAN</strong>: A coarse-to-fine approach for training allows generating images at increasing resolutions.</p>

    <ul>
      <li>training the generator and discriminator using low-resolution images and incrementally add layers of higher-resolution images during training.</li>
    </ul>
  </li>
  <li>
    <p><strong>Deep Convolutional GAN</strong>: A GAN that uses convolutional neural networks (CNNs) as the generator and discriminator.</p>

    <ul>
      <li>using a CNN as the discriminator network and a deconvolution neural network as the generator</li>
    </ul>
  </li>
  <li>
    <p><strong>Semi-Supervised GAN</strong>: Instead of having the discriminator be a binary classifier for real or fake samples, in a semi-supervised GAN (SGAN) the discriminator is a multi-class classifier</p>

    <ul>
      <li>recall that all the GANs are normally unsupervised, as labels is just synthetic images or real ones</li>
      <li>The discriminator outputs the likelihood of sample to be synthesized or real, and if the sample is <em>classified as real</em> then the discriminator outputs the <em>probability of the $k$ classes</em>, estimating to which class the sample belongs</li>
    </ul>
  </li>
  <li>
    <p><strong>Conditional GAN</strong>: models the conditional probability distribution $P(x\vert y)$ by training the generator and discriminator on <em>labeled</em> data, with labels being $y$.</p>

    <ul>
      <li>
        <p>Replacing $D(x)$ with $D(x\vert y)$ and $G(z)$ with $G(z\vert y)$ for the loss, you get conditional GAN</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z|y)))]\]
      </li>
      <li>
        <p>Providing labels allows us to ==synthesize samples in a specific class== or with a specific attribute, providing a level of control over synthesis</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Pix2Pix:</strong> Image-to-Image Translation</p>

    <ul>
      <li>
        <p>Given a training set of unfiltered and filtered image pairs $A : A’$ and a new unfiltered image $B$ the output is a filtered image $B’$ such that the analogy $A:A’::B : B’$ is <strong>maintained</strong></p>
      </li>
      <li>
        <p>An input image is mapped to a synthesized image with different properties. The loss function is a <strong>combination of the conditional GAN loss</strong> with an additional loss term which is a <strong>pixel-wise loss</strong> (i.e. sum of loss per pixel on the image) that encourages the generator to match the source image:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211203343602.png" alt="image-20220211203343602" style="zoom:67%;" /></p>

        <p>with weight $\lambda$</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Cycle Consistent GAN</strong>: learns unpaired image-to image translation using GANs <em>without pixel-wise correspondence</em></p>

    <ul>
      <li>
        <p>The training data are image sets $X \in A$ and $Y \in A’$ from two different domains $A$ and $A’$ <strong>without pixel-wise correspondence</strong> between the images in $X$ and $Y$.</p>
      </li>
      <li>
        <p>CycleGAN consists of</p>

        <ul>
          <li>two generators $G(X) = \hat{Y}$ and $F(Y ) = \hat{X}$</li>
          <li>two discriminators $D_Y$ and $D_X$.</li>
        </ul>

        <p>The generator $F$ maps a real image $X$ to a synthesized sample $\hat{Y}$ and the discriminator $D_Y$ compares between them</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204326281.png" alt="image-20220211204326281" style="zoom:50%;" /></p>
      </li>
      <li>
        <p>CycleGAN maintains two approximate cycle consistencies:</p>

        <ul>
          <li>The first cycle consistency  $F(G(X)) \approx X$ approximately maintains that mapping a real image $X$ to a synthesized image $\hat{Y}$ and back is similar to $X$</li>
          <li>the second cycle consistency $G(F(Y )) \approx Y$ approximately maintains that mapping a real image $Y$ to a synthesized image $\hat{X}$ and back is similar to $Y$.</li>
        </ul>

        <p>Graphically, this is one of the consistency loop</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215143023657.png" alt="image-20220215143023657" style="zoom: 50%;" /></p>
      </li>
      <li>
        <p>The overall loss function is defined by (Zhu et al. 2017):</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204534646.png" alt="image-20220211204534646" style="zoom:67%;" /></p>

        <p>where the cycle consistency loss is defined by:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220211204602542.png" alt="image-20220211204602542" style="zoom: 67%;" /></p>

        <p>which is weighted by $\lambda$</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="evaluation-and-application">Evaluation and Application</h2>

<p>After training the generator so that discriminator cannot do any better, there is still the chance that <strong>generated samples are garbage</strong> if discriminator is garbage. Therefore, we have some metrics to <strong>evaluate how well generator’s end product</strong> is doing.</p>

<p>The Inception Score (IS) and Frechet Inception Distance (FID) measure the <strong>quality of synthesized examples</strong> using <strong>pre-trained</strong> neural network classifiers</p>

<ul>
  <li><strong>Inception Score</strong>: The Inception Score (IS) (Salimans et al. 2016) automatically evaluates the quality of images synthesized by the generator by using the <strong>pre-trained Inception v3 model</strong> (Szegedy et al. 2016) for classification.
    <ul>
      <li>A <strong>higher</strong> Inception Score is <strong>better</strong>, which corresponds to a larger KL divergence between the distributions.</li>
    </ul>
  </li>
  <li><strong>Frechet Inception Distance</strong>: The Frechet Inception Distance (FID) is also based on the Inception v3 modally.
    <ul>
      <li>The FID uses the feature vectors of the last layer for real and synthesized images to generate multivariate Gaussians that model the real and synthesized distributions</li>
      <li>A <strong>lower</strong> FID is <strong>better</strong>, which corresponds to similar real and synthesized distributions.</li>
    </ul>
  </li>
</ul>

<p>Finally, GAN architectures can be applied in numerous fields such as:</p>

<ul>
  <li>Image Completion</li>
  <li>Super Resolution and Restoration</li>
  <li>Style Synthesis</li>
  <li>De-Raining</li>
  <li>Text-to-Image Synthesis</li>
  <li>Music Synthesis</li>
  <li>etc.</li>
</ul>

<h1 id="deep-variational-inference">Deep Variational Inference</h1>

<p>This chapter begins with a review of variational inference (VI) as a fast approximation alternative to Markov Chain Monte Carlo (MCMC) methods, solving an optimization problem for <strong>approximating the posterior</strong></p>

<ul>
  <li>Amortized VI leads to the <strong>variational autoencoder (VAE)</strong> framework which is introduced using deep neural networks and graphical models and used for learning representations and generative modeling.</li>
</ul>

<p>The setup is as follows. Consider we are given some <strong>data $x$</strong>, so that we can visualize this as a probability distribution $p(x)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215194410664.png" alt="image-20220215194410664" style="zoom:50%;" /></p>

<p>Now, suppose that in reality, this is the actual data:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215194455483.png" alt="image-20220215194455483" style="zoom:50%;" /></p>

<p>where essentially:</p>

<ul>
  <li>$z \in [1,2,3]$ is a ==hidden variable== that signifies the three cluster within this data distribution</li>
  <li>Essentially, the real distribution $p(x)$ is actually coming from both $x$ and $z$. But only $x$ is observed, and we <strong>do not know what $z$ is in advance</strong></li>
</ul>

<hr />

<p>More formally, the problem task is as follows.</p>

<ul>
  <li>our target model is $p_\theta (x)$</li>
  <li>
    <p>our data is $D = {x_1, x_2, …,x_N}$</p>
  </li>
  <li>we <strong>want to model hidden variables as well</strong></li>
</ul>

<p>Therefore, our <strong>maximum likelihood fit becomes</strong>:</p>

\[\arg\max_\theta \frac{1}{N}\sum_i \log p_\theta (x_i) \to \arg\max_\theta \frac{1}{N}\sum_i \log \left( \int p_\theta(x_i|z)p(z) dz\right)\]

<p>which is <strong>problematic</strong> we would have needed to compute $\int p_\theta(x_i\vert z)p(z) dz$ for every data.</p>

<p>Therefore, the idea is:</p>

<ol>
  <li>observe some data $x$, and take a guess <strong>on the underlying $z$</strong> (e.g. this pile of data belongs to this cluster)</li>
  <li>construct “fake labels $z$” for your each of your data $x_i$
    <ul>
      <li>technically you would guess a $p(z\vert x_i)$ over it</li>
    </ul>
  </li>
  <li>do maximum likelihood on that $x_i$ and $z$ associated</li>
</ol>

<p>Then, the kind of objective you want to do would be:</p>

\[\arg\max_\theta \frac{1}{N} \sum_i  \mathbb{E}_{z \sim p(z|x_i)}[\log p_\theta (x_i,z)]\]

<p>so you are <strong>maximizing over the joint</strong>, so that once done:</p>

<ul>
  <li>obtain a model $\theta$, but also get information on $p(z\vert x_i)$.</li>
  <li>but we are taking an average/expected value over $p(z\vert x_i)$, ==how do we calculate this==?</li>
</ul>

<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>

<p>The idea is simple. Our task is to compute some <strong>posterior $p(z\vert x_i)$</strong> for some observation $x_i$. For this section, imagine that you are observing a <strong>coin</strong>, and you want to model $p(\theta \vert x)$ for $\theta$ being the probability of getting a head.</p>

<ul>
  <li>essentially $\theta=z$ is some <strong>hidden information from us</strong>, but carries essential information on how data is generated</li>
  <li>the only observables are $x$</li>
</ul>

<p>Using Bayes, we know that:</p>

\[\underbrace{p(\theta |x)}_{\text{posterior}} = \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}\,\,\frac{1}{p(x)}\]

<blockquote>
  <p><em>Intuition</em></p>

  <p>We can take a <strong>guess on the prior distribution $p(\theta)$</strong>, and with observables available, we can <strong>compute $p(x\vert \theta)$</strong></p>

  <ul>
    <li>e.g. for a coin toss, guess that $p(\theta) = \text{Unif}[0,1]$, and $p(x\vert \theta)=p_\theta(x) \sim \text{Bern}(\theta)$</li>
    <li>essentially, both quantities are now known</li>
    <li>however we often <strong>cannot compute $p(x)$</strong>, since would involve computing some nasty integrals. Hence, we need some technique to estimate $p(\theta \vert  x)$ when we only know $p(x\vert \theta)p(\theta)$</li>
  </ul>
</blockquote>

<hr />

<p><em>For Example</em>: Coin Toss</p>

<p>The likelihood $p(x=4\vert \theta)$=probability of getting four heads with $\text{Bern}(10,\theta)$, mewing that our traying data has four heads in total. The priors are beta distributions which we guessed. Then, the posterior is the product of the two:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Less Data (for likelihood)</th>
      <th style="text-align: center">More data (for likelihood)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217152847817.png" alt="image-20220217152847817" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217152855335.png" alt="image-20220217152855335" /></td>
    </tr>
  </tbody>
</table>

<p>where the key observation here is that:</p>

<ul>
  <li>if <strong>prior is informative</strong>, then we don’t need very good likelihood data to obtain a reasonable estimate for posterior</li>
  <li>if <strong>likelihood is informative</strong>, i.e. we have lots of data, then we don’t need to have a very informative prior</li>
</ul>

<hr />

<p>Now, lets get back to the task of finding out the <strong>posterior $p(\theta\vert x)$</strong>, or in general $p(z\vert x)$.</p>

<blockquote>
  <p><em>Intuition</em>:</p>

  <p>We do know that</p>

\[\underbrace{p(\theta |x)}_{\text{posterior}} \propto \underbrace{p(x | \theta)}_{\text{likelihood}}  \,\,\underbrace{p(\theta)}_{\text{prior}}\]

  <p>So the idea is that:</p>

  <ol>
    <li>pick some proposal distribution $q(\theta)$
      <ul>
        <li>e.g. a normal distribution</li>
      </ul>
    </li>
    <li>sample a data point from the proposal distribution $\theta^*$</li>
    <li>==save this data point $\theta^<em>$ if $p(x\vert \theta^</em>)p(\theta^*)$ is very likely==, reject it otherwise</li>
    <li>repeat step 2 to 3</li>
  </ol>

  <p>Then, you end up with lots of $\theta_i$, and it should <strong>resemble the posterior distribution $p(\theta\vert x)$</strong> due to the save/rejection step.</p>
</blockquote>

<p>In more details, consider the following <strong>MCMC Metropolis Hastings</strong> algorithm</p>

<ol>
  <li>
    <p>select some initial value $\theta_0$ to start with</p>
  </li>
  <li>
    <p>for $i=1,…,m$ do:</p>

    <ol>
      <li>
        <p><strong>Markov</strong>: update the proposal distribution $q(\theta^*\vert \theta_{i-1})$, based on the previous sample</p>
      </li>
      <li>
        <p><strong>Monte Carlo</strong>: pick a candidate $\theta^* \sim q(\theta^* \vert  \theta_{i-1})$</p>
      </li>
      <li>
        <p>consider whether or not to accept the candidate by considering:</p>

\[\alpha = \underbrace{\frac{p(\theta^*|x)}{p(\theta_{t-1}|x)}}_{\text{can't compute}} = \underbrace{\frac{p(x|\theta^*)p(\theta^*)}{p(x|\theta_{i-1})p(\theta_{i-1})}}_{\text{unknown}}\]

        <p>if $\alpha \ge 1$, <em>*accept $\theta^</em>$** so that $\theta_i \leftarrow \theta^*$</p>

        <p>if $0 &lt; \alpha &lt; 1$, accept $\theta^*$ with <strong>probability $\alpha$</strong>, else <strong>reject</strong> it.</p>
      </li>
    </ol>
  </li>
</ol>

<p>The resulting drawn distribution of $\theta^*$s will assemble the posterior distribution $p(\theta\vert x)$.</p>

<ul>
  <li>notice that if our proposal $q$ is <strong>very close to $p(\theta\vert x)$</strong>, then <strong>many samples will be accepted</strong> and we converge fast</li>
  <li>otherwise, we may need a lot of time.</li>
</ul>

<p>Graphically, if we picked</p>

<ul>
  <li>proposal $q(\theta^*\theta_{i-1}) = \mathcal{N}(\theta_{i-1},1)=\text{Beta}(1,1,\theta_{i-1})$,</li>
  <li>likelihood $\text{Bin}(10,\theta)$ and we <strong>observed 4 heads</strong> in our data</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">5 Iterations</th>
      <th style="text-align: center">5000 Iterations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217190211945.png" alt="image-20220217190211945" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217190327902.png" alt="image-20220217190327902" /></td>
    </tr>
  </tbody>
</table>

<p>However, the problem with this approach is that <strong>sampling and adjusting takes time</strong>, and for large number of sampes, it becomes very slow.</p>

<ul>
  <li>this introduces us with the following sections on using ML models such as VAE to solve the <strong>inference problem of $p(\theta\vert x)=p(z\vert x)$</strong> in general.</li>
</ul>

<h2 id="variational-inference">Variational Inference</h2>

<p>We begin with:</p>

<ul>
  <li><strong>observed data $x$</strong>, continuous or discrete
    <ul>
      <li>e.g. $x$ may be an image of a face</li>
    </ul>
  </li>
  <li>suppose that the process <strong>generating the data</strong> involved <strong>hidden latent variables $z$</strong>
    <ul>
      <li>e.g. $z$ a hidden vector describing latent variables such as pose, illumination, gender, or emotion</li>
      <li>e.g. $z$ could represent cluster information</li>
    </ul>
  </li>
</ul>

<p>And you see that constructing a model to model both $x_i,z$ requires knowledge over:</p>

\[p(z|x_i)\]

<p>which is what this method is <strong>attempting to solve</strong>.</p>

<blockquote>
  <p><strong>Summary</strong></p>

  <p>Our aim is to <strong>estimate</strong> $p(z\vert x_i)$: for example, answering the question what are the hidden latent variables (poses, gender, emotion) $z$ for a given observation (image) $x_i$.</p>

  <p>However, this involves computing $p(x_i)$ in the following expression</p>

\[p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}\]

  <p>which is intractable as the denominator cannot be computed.</p>

  <p>Hence, the idea is to <strong>approximate $p(z\vert x_i)$</strong> using $q_{i}(z) \in Q$, and <strong>minimize the distance</strong> using a metric such as KL-divergence. Therefore, this becomes an <strong>optimization problem</strong>, for ==each $x_i :=x$:==</p>

\[q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz\]

  <p>which can be simplied using Bayes to (since $p(z\vert x)$ we don’t know either)</p>

\[\arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]\]

  <p>where</p>

  <ul>
    <li>$z$ would be sampled from $z \sim q_\phi(z)$ as $q_\phi$ would be picked by us. Hence we can compute $p(x,z)=p(x\vert z)p(z)$ and $q_\phi(z)$</li>
    <li>essentially $p(x,z)$ would involve a likelihood and a prior, which we know.</li>
    <li>the first term is MAP and the second term encourages diffusion (entropy), or spreading of variational distribution.</li>
  </ul>
</blockquote>

<p>First, let us <em>revise</em> Bayes theorem. For <strong>each data point $x_i:=x$</strong></p>

\[p(z,x) = p(z|x)p(x)=p(x|z)p(z)\]

<p>where:</p>

<ul>
  <li>$p(z,x)$ is the joint</li>
  <li>$p(z\vert x)$ is called the <strong>posterior</strong> (as you observed $x$)</li>
  <li>$p(x)$ is call the <strong>evidence/marginal density</strong></li>
  <li>$p(z)$ is the <strong>prior density</strong> (before you observe $x$)</li>
  <li>$p(x\vert z)$ is the <strong>likelihood</strong> (given $z$, probability of seeing $x$)</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>For using deep generative models, we may also want to say:</p>

\[p(x|z)=p(x|z_1)p(z_1|z_2)...p(z_{l-1}|z_l)p(z_l)\]

  <p>for $l$ layers of hidden variables. This will be <strong>discussed later</strong>. Here we think of the simple case where we have only <strong>one hidden variable</strong>.</p>
</blockquote>

<p>Our aim is to estimate $p(z\vert x_i)$:</p>

\[p(z|x_i) = \frac{p(x_i|z)p(z)}{p(x_i)}\]

<p>which is intractable to compute because:</p>

<ul>
  <li>
    <p>for most models because the denominator is:</p>

\[p(x_i) = \int p(x_i|z)p(z)dz\]

    <p>this is high-dimensional intractable integral which requires integrating over an exponential number of terms for $z$.</p>
  </li>
  <li>
    <p>in the end, the posterior $p(z\vert x_i)$ is often intractable to compute analytically. For example, if $z$ is a vector of length $d$ (i.e. hidden state with $d$ “features”), <strong>then $p(z\vert x_i)$ is a $d \times d$ matrix</strong>, and the posterior is a function of the parameters of the model $p(z\vert x, \theta)$.</p>
  </li>
</ul>

<p>Therefore, the key idea is to ==estimate $p(z_i\vert x)$ by some variational distribution $q_\phi(z)\equiv q_i(z)$== from a family of distributions $Q$ and parameters $\phi$ such that $q_\phi \in Q$. Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215171509021.png" alt="image-20220215171509021" style="zoom:33%;" /></p>

<p>A typical choice for $Q$ is the <strong>exponential family of distribution</strong>:</p>

<blockquote>
  <p><strong>Exponential Family Distribution</strong></p>

  <p>We say that a class of distributions is <strong>in the exponential family</strong> if it can be written in the form:</p>

\[p(y; \eta) = b(y) \exp(\eta \, T(y) - a(\eta))\]

  <p>where:</p>

  <ul>
    <li>$y$ means the labels/target in your dataset</li>
    <li>$\eta$ is the <strong>natural parameter</strong> (also called the canonical parameter) of the <strong>distribution</strong></li>
    <li>$b(y)$ is the <strong>base measure</strong></li>
    <li>$T(y)$ is the <strong>sufficient statistic</strong> (see later examples, you often see $T(y)=y$)</li>
    <li>$a(\eta)$ is the <strong>log partition function</strong>, which basically has $e^{-a(\eta)}$ playing the role of <em>normalization constant</em></li>
  </ul>

  <p>so basically you can expression some distribution with the above form with any choice of $b(y), T(y), a(\eta)$, then that expression is in the exponential family.</p>
</blockquote>

<p>And a choice of <strong>closeness/distance</strong> would be the (reverse) ==KL divergence==:</p>

\[KL(q(x) || p(x)) = \int q(x) \log \frac{q(x)}{p(x)}dx\]

<p>Therefore, our ==objective== is then (<strong>recall that here $x_i := x$ represents a single data point</strong>)</p>

\[q_{\phi^*}(z) = \arg\min_{q_\phi(z)} KL(q_\phi(z)||p(z|x))=\arg\min_{q_\phi(z)} \int q(z)\log \frac{q(z)}{p(z|x)}dz\]

<p>which is intractable since $p(z\vert x)$ is what we want to estimate. Hence we use bayes to simplify:</p>

\[\begin{align*}
\int q(z)\log \frac{q(z)}{p(z|x)}dz 
&amp;= \int q(z)\log \frac{q(z)p(x)}{p(z,x)}dz \\
&amp;= \int q(z)\log \frac{q(z)}{p(z,x)}+q(z) \log (p(x))dz \\
&amp;= \log p(x) + \int q(z)\log \frac{q(z)}{p(z,x)}dz \\
&amp;= \log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz
\end{align*}\]

<p>Now, notice that KL divergence is <strong>non-negative</strong>. Hence we know:</p>

\[\begin{align*}
\log p(x) -\int q(z)\log \frac{p(z,x)}{q(z)}dz 
&amp; \ge  0\\
\log p(x) &amp; \ge \int q(z)\log \frac{p(z,x)}{q(z)}dz \equiv \mathcal{L}
\end{align*}\]

<p>where the integral on the right denoted by $\mathcal{L}$ is known as the <strong>evidence lower bound</strong> (ELBO).</p>

<ul>
  <li>therefore, minimizing KL divergence is the same as <strong>maximizing ELBO</strong></li>
  <li>The ELBO is a <strong>lower bound</strong> on the log-likelihood of the data $x$ given the latent variable $z$. It is a lower bound because it is ==not possible to compute the exact log-likelihood== of the data $x$ given the latent variable $z$</li>
  <li>essentially $p(x,z)$ would involve a likelihood and a prior, which we know. See the example algorithm below to see how it works.</li>
</ul>

<p>This finally can be written as:</p>

\[\arg\max_{q_\phi(z)} \int q(z)\log \frac{q(z,x)}{p(z)}dz=\arg\max_{q_\phi(z)}\,\, \mathbb{E}_{z \sim q_\phi(z)}[\log p(x,z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_\phi(z)]\]

<p>where we see that there is a tradeoff between these two terms.</p>

<ul>
  <li>the first term places mass on the MAP estimate;</li>
  <li>whereas the second term encourages diffusion, or spreading the variational distribution.</li>
  <li>note that since we would have specified $q_\phi$, we could <strong>sample $z \sim q_\phi$</strong> to compute for $p(x,z)$</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Compared with this formulation, many other methods have short-comings:</p>

  <ul>
    <li>mean-field variational inference (MFVI) <strong>assumes a full factorization of variables</strong> which is inaccurate</li>
    <li>MCMC sampling methods (Brooks, Gelman, Jones &amp; Meng 2011), such as the Metropolis Hastings algorithm, <strong>may not be scalable to very large datasets</strong> and may require <strong>manually specifying a proposal distribution</strong></li>
  </ul>
</blockquote>

<hr />

<p>Recall that:</p>

<p>In the end we also want to recover our $\theta$ for model $p_\theta(x)$. Hence essentially our loss can be rephrased as (rephrasig $p(x,z)=p(x\vert z)p(z)$)</p>

\[\mathcal{L}_i(p,q_i ) =\mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)]-\mathbb{E}_{z \sim q_\phi(z)}[\log q_i(z)]\]

<p>Therefore, our algorithm would be, for each $x_i$:</p>

<ol>
  <li>
    <p>calculate $\nabla_\theta \mathcal{L}_i(p,q_i )$</p>

    <ul>
      <li>
        <p>sample $z \sim q_i(z)$</p>
      </li>
      <li>
        <p>compute the gradient, which is only on the first term:</p>

\[\nabla_\theta \mathcal{L}_i(p,q_i) \approx \log p_\theta(x_i|z)\]

        <p>this we <strong>know because $x_i ,z$</strong> are now “observed”</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Update $\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}_i(p,q_i)$</p>
  </li>
  <li>
    <p>update <strong>$q_i$ to maximize $\mathcal{L}_i(p,q_i)$</strong></p>

    <ul>
      <li>e.g. if you picked $q_i(z) = N(\mu_i, \sigma_i)$, then essentially $\phi = (\mu_i, \sigma_i)$</li>
      <li>hence yuo would try to compute $\nabla_{\mu_i} \mathcal{L}<em>i$ and $\nabla</em>{\sigma_i} \mathcal{L}_i$ to optimize</li>
      <li>but that will be <strong>a lot of parameters</strong>, if we have many data points.</li>
    </ul>
  </li>
</ol>

<p>Therefore, this introduces us to other alternatives such as VAE.</p>

<hr />

<h3 id="optimizing-for-q_phi">Optimizing for $q_\phi$</h3>

<p>In practice, there has been <strong>many methods</strong> in which one can use to update the estimate distribution $q_\phi$. Here we will discuss the approach using</p>

<ul>
  <li><strong>Score function gradient</strong> - generic and has <strong>no requirement on $p_\theta(x)$</strong>, but has ==large variance==;</li>
  <li><strong>Reparameterization gradient</strong>. - less general purpose, <strong>$p_\theta(x)$ needs to  be continuous</strong>, much ==smaller variance==.</li>
</ul>

<h3 id="score-function-gradient">Score Function Gradient</h3>

<p>To find the best $p_\phi(z\vert x_i)$, we consider finding $\phi$:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p_\theta(x_i|z) + \log p(z)-\log q_{\phi_i}(z)]= \nabla \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z)]\]

<p>let us denote $\log p(x_i,z)-\log q_{\phi_i}(z) \equiv f_\phi(z)$, then we have:</p>

\[\begin{align*}
\nabla_\phi \mathbb{E}_{z \sim q_\phi(z)}[f_\phi(z)]
&amp;= \nabla_\phi \int f_\phi(z)q_\phi(z)dz \\
&amp;= \phi \int (\nabla_\phi f_\phi(z))q_\phi(z)+ (\nabla_\phi q_\phi(z))f_\phi(z)dz \\
\end{align*}\]

<p>Now, it turns out that we can <strong>compute</strong> using the following ==trick==</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215205915090.png" alt="image-20220215205915090" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the ==score function== is:</p>

\[\nabla_\phi \log q_\phi(z) = \frac{\nabla_\phi q_\phi(z)}{q_\phi(z)}\]
  </li>
</ul>

<p>This therefore means that:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) = \mathbb{E}_{z \sim q_\phi(z)}[\log p(x_i,z)-\log q_{\phi_i}(z) \nabla _\phi \log q_\phi(z)]\]

<p>which we can then <strong>estimate by sampling from the distribution $q_\phi(z)$ that we guessed</strong>:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k[\log p(x_i,z_i)-\log q_{\phi_i}(z_i) \nabla _\phi \log q_\phi(z_i)]\]

<p>which is called <strong>Monte Carlo sampling</strong></p>

<h3 id="reparameterization-gradient">Reparameterization Gradient</h3>

<p>The reparameterization trick utilizes the property that for <strong>continuous</strong> distribution $p_\theta(x)$, the following sampling processes are equivalent:</p>

\[\begin{align*} \hat{x} \sim p(x;\theta) \quad \equiv \quad \hat{x}=g(\hat{\epsilon},\theta) , \hat{\epsilon} \sim p(\epsilon) \end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>instead of directly sampling from the posterior, we typically take random sample from a standard Normal distribution $\hat{\epsilon} \sim \mathcal{N}(0,1)$ and multiply it by the mean and variance</p>
  </li>
  <li>
    <p>e.g. in our case, we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:</p>

\[z = \mu + \sigma \cdot \epsilon\]

    <p>where $\epsilon \sim \mathcal{N}(0,1)$</p>
  </li>
</ul>

<p>Then, this <strong>reparametrization can</strong>:</p>

<ol>
  <li>express the gradient of the expectation</li>
  <li>achieve a ==lower variance== than the score function estimator, and</li>
  <li>differentiate through the latent variable $z$ to optimize by backpropagation</li>
</ol>

<p>Then</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215211858216.png" alt="image-20220215211858216" style="zoom: 50%;" /></p>

<p>This finally gives:</p>

\[\nabla_{\phi_i} \mathcal{L}_i(p,q_i ) =\frac{1}{k}\sum_{i=1}^k\{\nabla_\phi [\log p(x, g(\epsilon_i,\phi))-\log q_\phi(g(\epsilon_i,\phi))]\}\]

<p>being the update equation.</p>

<h2 id="autoencoders">Autoencoders</h2>

<p>Instead of optimizing a separate parameter $\phi_i$ for <strong>each example</strong>, amortized variational inference (AVI) approximates the posterior across all examples together</p>

<p>The task of our model essentially involves finding <strong>two sets of parameters $\theta, \phi$</strong>. This makes it natural to consider an architecture using <strong>encoder + decoder</strong>, where essentially:</p>

<ul>
  <li><strong>encoder</strong> will perform $q_\phi(z\vert x)$ mapping. In other words, given an <strong>input $x$</strong>, <strong>output a hidden state $z$</strong></li>
  <li><strong>decoder</strong> will perform $p_\theta(\hat{x}\vert z)$, in an attempt to <strong>reconstruct $x$</strong> hence finding $p(x\vert z)$ with <strong>input from hidden state $z$</strong></li>
</ul>

<hr />

<p><em>Recap</em>: Autoencoder</p>

<p>Essentially an autoencoder is doing:</p>

\[F(x) = \text{decode}(\text{encoder}(x))\]

<p>where <strong>making sure that</strong> the dimension $z = \text{encoder}(x)$ is ==smaller than $x$==, hence we are “<strong>extracting important features</strong>” from $x$.</p>

<p>Therefore, the architecture for autoencoder looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215220325253.png" alt="image-20220215220325253" style="zoom:67%;" /></p>

<p>where we can say that there are <strong>two networks</strong> (e.g. with two weights):</p>

\[\min_{W_e,W_d} \sum_{i=1}^m ||x_i - (W_d)^T f( (W_e)^T x_i )||^2\]

<p>for each data $x_i \in \mathbb{R}^d$</p>

<ul>
  <li>we want $(W_e)^Tx_i$ to have a <strong>smaller dimension than $d$</strong> (otherwise reconstructing is trivial)</li>
  <li>if $f$ is an <strong>identity matrix</strong>, then this is equivalent of doing ==PCA==</li>
</ul>

<hr />

<p>This architecture can be used for:</p>

<ul>
  <li>
    <p><strong>denoising</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134248515.png" alt="image-20220217134248515" style="zoom:33%;" /></p>
  </li>
  <li>
    <p><strong>completion</strong></p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134306323.png" alt="image-20220217134306323" style="zoom:33%;" /></p>

    <p>for example:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134333967.png" alt="image-20220217134333967" style="zoom:33%;" /></p>
  </li>
</ul>

<h3 id="variational-autoencoder">Variational Autoencoder</h3>

<p>Then, for Variational Autoencoder, we are basically using autoencoder to learn $\theta, \phi$ by <strong>maximizing ELBO</strong>:</p>

\[\begin{align*}
\mathcal{L} 
&amp;= \int q(z)\log \frac{p(z,x)}{q(z)}dz \\
&amp;= \int q(z)\log p(x|z) dz - \int q(z) \log \frac{p(z)}{q(z)}dz\\
&amp;= \mathbb{E}_{z \sim q(z)}[\log p(x|z)] - KL(q(z)||p(z))
\end{align*}\]

<p>Therefore, we can think of the following architecture to <strong>solve this optimization problem</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Network</th>
      <th style="text-align: center">Abstraction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://miro.medium.com/max/1838/1*Q5dogodt3wzKKktE0v3dMQ@2x.png" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220215220914001.png" alt="image-20220215220914001" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>$\mathbb{E}_{z \sim q(z)}[\log p(x\vert z)]$ would correspond to <strong>decoder</strong></p>

    <ul>
      <li>given some <strong>sampled $z \sim q(z)$</strong>, this is the log-likelihood of the observed data $x$ (i.e. $x_i := x$).</li>
      <li>Therefore, this measures how well the samples from $q(z)$ explain the data $x$, which can be seen as the <strong>reconstruction error</strong> to get $x$ back from an encoded latent variable $z$</li>
    </ul>
  </li>
  <li>
    <p>$KL(q(z)\vert \vert p(z))$ would correspond to the <strong>encoder</strong>, because we are trying to find out $q(z)$ that is close to $p(z)$</p>

    <ul>
      <li>represents <strong>encoding data from $x$ to latent variable $z$</strong></li>
      <li>hence, if going well, this means that the explanation of the data ($z \sim q(z)$) does not deviate from the prior beliefs $p(z)$ and
is called the <strong>regularization term</strong></li>
    </ul>
  </li>
</ul>

<p><strong>In summary</strong>:</p>

<ul>
  <li>encoder neural network infers a hidden variable $z$ from an observation $x$.</li>
  <li>decoder neural network which reconstructs an observation $\hat{x}$ from a hidden variable $z$.</li>
  <li>The encoder $q_\phi$ and decoder $p_\theta$ are trained end-to-end, optimizing for <strong>both</strong> the <strong>encoder parameters $\phi$</strong> and <strong>decoder parameters $\theta$</strong> by backpropagation</li>
</ul>

<blockquote>
  <p>Now there is a problem, because in the above algorithm, we would need to <strong>sample $z$</strong> in the encoder (not a differentiable operation). However, for backpropagation, we need each operation to be ==differentiable==.</p>
</blockquote>

<p>Therefore, we need to <strong>reparametrize</strong> the sampling procedure ot the following:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217134853802.png" alt="image-20220217134853802" style="zoom:33%;" /></p>

<p>therefore, essentially:</p>

<ul>
  <li>
    <p>recall that we can have $z \sim q_\phi(z) = \mathcal{N}(\mu, \sigma)$ by:</p>

\[z = \mu + \sigma \cdot \epsilon\]

    <p>where $\epsilon \sim \mathcal{N}(0,1)$</p>
  </li>
  <li>we are <strong>predicting the mean/variance</strong>, instead of predicting $z$. Therefore, all we need to do is to <strong>update mean/variance</strong> of the distribution instead of update the “sampling procedure”</li>
  <li>the same goes on to decoder.</li>
</ul>

<p>Then together, the model is really <strong>learning</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135020612.png" alt="image-20220217135020612" style="zoom: 50%;" /></p>

<ul>
  <li>which can be seen as a probabilistic model as it involves calculation from samplings using mean and variance</li>
</ul>

<h2 id="probabilistic-programming">Probabilistic Programming</h2>

<p>Essentially you can compute <strong>distributions, conditional distributions, etc</strong> using a program.</p>

<ul>
  <li>e.g. you can infer <strong>conditional distribution $p(a+b+c\vert a+b=1)$</strong> from only knowing the individual probabilities $p(a),p(b),p(c)$</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135414796.png" alt="image-20220217135414796" style="zoom:50%;" /></p>

<p>which is a powerful idea (e.g. used in MCMC and etc)</p>

<h1 id="reinforcement-learning">Reinforcement Learning</h1>

<p>Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning.</p>

<ul>
  <li>In <strong>supervised</strong> learning we are given input-output pairs</li>
  <li>in <strong>unsupervised</strong> learning we are given only input examples</li>
  <li>In <strong>reinforcement</strong> learning we learn from interaction with an environment to achieve a goal</li>
</ul>

<blockquote>
  <p>We have an <strong>agent</strong>, a learner, that makes decisions under uncertainty.</p>

  <p>In this setting there is an <strong>environment</strong> which is what the agent interacts with. The agents selects <strong>actions</strong> and the environment responds to those actions with a new <strong>state</strong> and <strong>reward</strong> .</p>

  <p>The agent goal is to ==maximize reward over time== as shown below:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217201444646.png" alt="image-20220217201444646" style="zoom:50%;" /></p>
</blockquote>

<blockquote>
  <p><strong>Resource</strong>: https://web.stanford.edu/class/cs234/index.html</p>
</blockquote>

<h2 id="multi-armed-bandit">Multi-Armed Bandit</h2>

<p>Before considering reinforcement learning, we will consider the <strong>stateless</strong> (policy at time $t$ is identical) setting of a multi-armed bandit.</p>

<p>Given $k$ slot machines:</p>

<ul>
  <li>an <strong>action</strong> is to pull an arm of one of the machines</li>
  <li>at each <strong>time step</strong> $t$ the agent chooses an <strong>action</strong> at among the $k$ actions</li>
  <li>taking action a is pulling arm $i$ which gives a <strong>reward $r(a)$</strong> with probability $p_i$, which of course you don’t know
    <ul>
      <li>i.e. Behind each machine there is a probability distribution, and by pulling an arm we get a <em>sample</em> from that distribution</li>
    </ul>
  </li>
</ul>

<p>Our goal is to <strong>maximize the total expected return</strong>. To do this, consider:</p>

<ul>
  <li>
    <p>each action has an expected or mean reward given that that action is selected; we can it <strong>value</strong> of that action</p>

    <ul>
      <li>
        <p>we denote the <strong>true value of action $a$</strong> as $q(a)$</p>
      </li>
      <li>
        <p>the <strong>estimated value of action $a$ at time $t$ as $Q_t(a)$</strong></p>

\[Q_t(a) =\text{Sample Mean}(a)\]

        <p>which we can update per iteration/action taken</p>
      </li>
    </ul>
  </li>
</ul>

<p>Then a simple idea is, at any time step, <strong>pick the action</strong> whose ==estimated value is greatest==</p>

\[a_t = \arg\max_{a}Q_t(a)\]

<p>where if we do this, we are doing a <strong>greedy algorithm</strong></p>

<ul>
  <li>If you select a greedy action, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions</li>
  <li>If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong> (could lead to better results in the long run)</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider you are given</p>

<ul>
  <li>two possible actions, picking red or blue (door, pill, etc).</li>
</ul>

<p>At $t=0$, we can randomly pick an action. For instance we picked the red one, and receives a reward $0$:</p>

<table>
  <thead>
    <tr>
      <th>$t=0$</th>
      <th style="text-align: center">$t=1$</th>
      <th style="text-align: center">$t=2$</th>
      <th style="text-align: center">$t=3$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205400203.png" alt="image-20220217205400203" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205412649.png" alt="image-20220217205412649" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205426486.png" alt="image-20220217205426486" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217205443361.png" alt="image-20220217205443361" /></td>
    </tr>
  </tbody>
</table>

<p>notice that</p>

<ul>
  <li>at each time step we have <strong>updated our estimate for value of an action $Q_t(a)$</strong></li>
  <li>we are endlessly choosing blue, but it could have been the case that the value we received for the red door of $0$ was simply bad luck, and that value was sampled from the tail of the distribution behind the red door
    <ul>
      <li>hence we need some <strong>balance</strong> between exploration and expoitation</li>
    </ul>
  </li>
</ul>

<h3 id="varepsilon-greedy-approach">$\varepsilon$-greedy Approach</h3>

<blockquote>
  <p>If instead of taking a greedy action, we behave greedily most of the time, for example</p>

  <ul>
    <li>with a small probability $\varepsilon$ we choose a <strong>random</strong> action</li>
    <li>with probability $1-\varepsilon$ we take the <strong>greedy</strong> action then we are acting $\varepsilon$-greedy</li>
  </ul>
</blockquote>

<p>Then the algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217210047233.png" alt="image-20220217210047233" style="zoom: 50%;" /></p>

<p>notice that:</p>

<ul>
  <li>$r(a)-Q(a)$ is like an <strong>error</strong> of current estimate</li>
  <li>as an exercise, try to show that this is the same as calculating the runnning mean $Q_{t+1} = (1/k)\sum_i^k R_i$</li>
</ul>

<h3 id="upper-confidence">Upper Confidence</h3>

<p>We can choose to be optimistic under uncertainty by <strong>using both the mean and variance of the (estimated) reward</strong>, taking the action using the upper confidence bound (UCB) criteria:</p>

\[a_t = \arg\max_a(\mu(r(a)) + \epsilon \sigma(r(a)))\]

<p>then in this case, you would also need to keep track of $\sigma(r(a))$ estimate.</p>

<h2 id="state-machines">State Machines</h2>

<p>The algorithms before are <strong>stateless</strong>. Now we consider adding a state, and the problem can be formalized as a state machine.</p>

<blockquote>
  <p>The tuple $(S,X, f, Y, g, s_0)$ define the <strong>state machine</strong>.</p>

  <ul>
    <li>$S$ is the set of possible states</li>
    <li>$X$ is the set of possible inputs</li>
    <li>$f:S\times X\to S$ transition function</li>
    <li>$Y$ is the set of possible outputs</li>
    <li>$g:S \to Y$ mapping from state to output</li>
    <li>$s_0$ initial state</li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218170329457.png" alt="image-20220218170329457" style="zoom: 50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>$S = { \text{standing, moving} }$, $X = { \text{slow, fast} }$,</li>
  <li>$f$ shown in orange and purple arrows, e.g. $s_1 = f(s_0 , \text{fast}) = \text{moving}$</li>
  <li>$s_0 = \text{standing}$ being the initial state</li>
  <li>$y=g(\text{standing}) = \text{standing}$ is the output in this case</li>
</ul>

<p>Notice when we use a state machine, at each time step $t$ it is essentially resembling <strong>RNN</strong>:</p>

\[\begin{align*}
s_t &amp;= f(s_{t-1},x_t)\\
y_t &amp;= g(s_t)	
\end{align*}\]

<p>notice that</p>

<ul>
  <li>this is the same as RNN if we use hidden state $h_{t-1}$ instead of $s_{t-1}$ here.</li>
  <li>everything is <strong>deterministic</strong>, i.e. given a state and an action, you know for certain what will be the next state</li>
</ul>

<h3 id="markov-processes">Markov Processes</h3>

<blockquote>
  <p>In a <strong>Markov model</strong>, we assume that the probability of a state $s_{t+1}$ is dependent <strong>only on the previous state $s_t$ and an action $a_t$</strong>.</p>

  <p>Formally, we consider</p>

  <ul>
    <li>$S$ a set of possible states</li>
    <li>$A$ a set of possible actions</li>
    <li>$T:S\times A \times S \to \mathbb{R}$ is the transition model with probabilities</li>
    <li>$g$ is the mapping from state to output
      <ul>
        <li>in the following examples they will just be identity operation</li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>(notice that we haven’t included reward $R$ in this model. Adding this information essentially makes the model to become a Markov Decision Process)</p>

<p>For instance:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218172310634.png" alt="image-20220218172310634" style="zoom:50%;" /></p>

<p>where you see</p>

<ul>
  <li>$A = {\text{slow}, \text{fast}}$ denoted by orange and pick arc</li>
  <li>e.g. if the robot is $\text{fallen}$ and takes $\text{slow}$ action then with probability $p=3/5$ the robot will stay fallen, <strong>but with $p=2/5$</strong> the robot will stand up
    <ul>
      <li>notice that this means an action can potentially lead <strong>any number of states</strong></li>
      <li>this is no longer deterministic!</li>
    </ul>
  </li>
</ul>

<p>Notice that since we have <strong>three states and two actions</strong>, we have <strong>two probability matrices</strong> of $3\times 3$ in size:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218221548845.png" alt="image-20220218221548845" style="zoom:67%;" /></p>

<p>which corresponds to the figure above, and notice that:</p>

<ul>
  <li>the $i$-th row denote transition from state $s_i$ to $s_j$. Therefore probability per row <strong>adds up to $1$</strong></li>
</ul>

<blockquote>
  <p>Then, a <strong>policy $\pi_t(a\vert s)=P(a_t=a\vert s_t=s)$</strong> maps a <strong>state $s$ to action $a$</strong>, allowing agent to decide which action to take given the current state at time $t$.</p>

  <ul>
    <li>Reinforcement learning methods specify how the agent changes its policy as a result of its experience</li>
  </ul>
</blockquote>

<p>Graphically, this looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222135047939.png" alt="image-20220222135047939" style="zoom: 33%;" /></p>

<p>This idea can be easily extended when we have observations $o_i \neq s_i$ at each state, and we make decisions <strong>base on the observations $\pi(a_i \vert  o_i)$</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222135329258.png" alt="image-20220222135329258" style="zoom: 50%;" /></p>

<h3 id="markov-decision-processes">Markov Decision Processes</h3>

<p>This is the more relevant introduction to RL.</p>

<blockquote>
  <p>The tuple $(S,A,T,R,\gamma)$ define the <strong>Markov Decision Process</strong>.</p>

  <ul>
    <li>$S$ is the set of possible states</li>
    <li>$A$ is the set of possible actions</li>
    <li>$T:S\times A \times S\to \mathbb{R}$ is the transition model with <em>probabilities</em>
      <ul>
        <li>this is assumed to be known in advance in MDP. It will be unknown in RL.</li>
      </ul>
    </li>
    <li>$R: S \times A \to \mathbb{R}$ is reward function, given a state and action
      <ul>
        <li>not $r(s,a,s’)$ here because we need to “estimate” the reward before knowing what $s’$ is, which is probabalistic, so that we can define our policy based only on $s$ so that $\pi = \pi(a\vert s)$</li>
      </ul>
    </li>
    <li>$\gamma$ is the discount factor</li>
  </ul>
</blockquote>

<p>The idea is simple:</p>

<ol>
  <li>At every time step $t$ the agent finds itself in state $s \in S$ and selects an action $a \in A$.</li>
  <li>the agent then moves to a new state $s \leftarrow s’$ in a <strong>probabalistic manner</strong> and receives a <strong>reward</strong>
    <ul>
      <li>the reward is a function of previous state and action $s,a$ (technically the expected value of $r(s,a,s’)$)</li>
    </ul>
  </li>
  <li>repeat step 1</li>
</ol>

<p>For instance, consider the following example, where we have a robot <strong>collecting cans</strong>. It can either search actively for a can (depletes battery), wait for someone to give a can, or recharge:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" /></p>

<p>where we have provided:</p>

<ul>
  <li>
    <p>state $S$ being the larger nodes, actions $A$ being the smaller nodes</p>
  </li>
  <li>
    <p>transition $T(s,a,s’)$ being the probability on the arrow, and $r(s,a,s’)$ being the reward on the arrow.</p>

    <p>Reward function technically is:</p>

\[R(s,a) = \sum_{s'} r(s,a,s') p(s'|s,a)\]
  </li>
</ul>

<p>They can also be summarized in the table:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" /></p>

<p>notice that the above table essentially provides a <strong>probability distribution for each possible state-action pair</strong></p>

\[p(s',r|s,a) \equiv P(S_{t+1}=s,R_{t+1}=r| S_t = s,A_t=a)\]

<p>since each transition essentially is a two tuple. Then, we can use this to compute useful quantities such as:</p>

<ul>
  <li>
    <p><strong>expected rewards for state-action</strong> pair</p>

\[r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)\]

    <p>or alternatively, if we have $r(s,a,s’)$:</p>

\[r(s,a) = \mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\sum_{s'} r(s,a,s')p(s'|s,a)\]

    <p>for instance, we can compute $r(s=\text{low}, a=\text{search})$ as:</p>

\[\begin{align*}
r(s=\text{low}, a=\text{search})
&amp;= -3 \cdot \sum_{s'} p(s',r=-3|s,a) + r_{search}\cdot \sum_{s'}p(s',r=r_{search}|s,a)\\
&amp;= -3 \cdot (1-\beta) + r_{search}(\beta )
\end{align*}\]

    <p>which is essentially ==weighting the reward on the arrow with probability== (see graph above). We can compute this for <strong>every state-action pair</strong> and get a matrix of size $\vert S\vert  \times \vert A\vert$:</p>

\[R(s,a) = \begin{bmatrix}
r(s_0,a_0) &amp; r(s_0,a_1)\\
r(s_1,a_0) &amp; r(s_1,a_1)\\
r(s_2,a_0) &amp; r(s_2,a_1)
\end{bmatrix}\]

    <p>then we can take the action that maximizes the reward from that state per <strong>row</strong> (greedy)</p>
  </li>
  <li>
    <p><strong>state transition probability</strong></p>

\[p(s'|s,a) = P(S_{t+1}=s| S_t = s,A_t=a) = \sum_r p(s',r|s,a)\]
  </li>
  <li>
    <p><strong>expected rewards for state-action-next-state</strong>:</p>

\[r(s,a,s') = \mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s'] = \frac{\sum_r r\, p(s',r|s,a)}{p(s'|s,a)}\]
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>The latter two quantities will be <strong>the most important</strong>, which are the ones drawn on the graph/in the table, and dynamics will be expressed almost exclusively using those two in further chapters.</li>
    <li>In reality, the agent does not know $T(s,a,s’)=P(s’\vert s,a)$ and $R(s,a)$. We wil lneed to learn them by <strong>sampling</strong> the environment.</li>
  </ul>
</blockquote>

<h2 id="value-functions">Value Functions</h2>

<p>Almost all reinforcement learning algorithms involve estimating some kind of value functions:</p>

<ul>
  <li>either estimating <strong>functions of states</strong> that estimate how good it is for the agent to be in a given state</li>
  <li>or <strong>functions of state-action pairs</strong> that estimate how good it is to perform a given action in a given state</li>
</ul>

<blockquote>
  <p>The notion of “how good” here is defined in terms of <strong>future rewards</strong> that can be expected. To be specific, we want to maximize the <strong>cumulative reward it receives in the long run</strong>, which is called the ==expected return $G_t$==</p>
</blockquote>

<p>To formalize the above idea, we need to consider the <strong>sequence of rewards</strong> received ==at each step== after some time $t$:</p>

\[R_{t+1}, R_{t+2}, R_{t+3}, ...\]

<p>Then, we can define our ==goal being maximizing the expected return at time $t$==:</p>

\[G_t = f(R_{t+1},R_{t+2},...,R_{T})\]

<p>for $T$ being the final step. A simple example would be:</p>

\[G_t = R_{t+1} + R_{t+2} + .... + R_T\]

<p>and more commonly we can generalize this with <strong>discounting</strong>:</p>

\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+.... = \sum_{k=0}^T \gamma^k R_{t+k+1}\]

<p>where $\gamma \in [0,1]$ is a discount rate:</p>

<ul>
  <li>a reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately.</li>
  <li>if $\gamma=0$, then the agent is <strong>myopic</strong> as it only consider maximizing immediate reward as $G_t = R_{t+1}$</li>
  <li>if $\gamma \to1$, then the objective takes future rewards into account more strongly: the agent becomes more <strong>farsighted</strong>. (if $\gamma =1$ the sum might explode if $T \to \infty$ and your sequence is not bounded.)</li>
</ul>

<p>With this, we can define the following quantity that are crucial in making decisions.</p>

<blockquote>
  <p><strong>State Value Function</strong></p>

  <p>The value of a state $s$ under a policy $\pi$, denoted $V_\pi(s)$, is the <strong>expected return when starting in $s$</strong> and following $\pi$ thereafter.</p>

\[V_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\]

  <p>and we call $T\equiv h$ is the horizon, which can be seen as the <strong>number of time steps left</strong>. Additionally, we define the value of the last state $t=T=h$ being</p>

\[V_\pi^{0}(s) = 0\]

</blockquote>

<p>Of course, in reality we can only estimate this expected value. This means that we need to consider the <strong>most general case</strong> for a ==stochastic policy $\pi(a\vert s)$ being a distribution==:</p>

\[\begin{align*}
V_\pi (s)
= \mathbb{E}_\pi [G_t | S_t =s]
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\\\
&amp;= \mathbb{E}_\pi [R_{t+1}|s] + \gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi [R_{t+1}|s] + \sum_a \pi(a|s) \sum_{s'}\sum_r p(s',r|s,a)\gamma \mathbb{E}\left[\sum_{k=0}^T \gamma^k R_{t+k+2}|S_{t+1}=s\right]\\
&amp;= \sum _aR(s,a) \pi(a|s) + \gamma  \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)V_\pi(s')\\
\end{align*}\]

<p>is a essentially <strong>recursive formula</strong>, which we can <strong>dynamically update</strong>. Note that:</p>

<ul>
  <li>$s’$ is essentially the <strong>next state</strong> from $s,a$ tuple we chose</li>
  <li>
    <p>we have all the fuss/sums because the policy $\pi(a\vert s)$ is spitting out a <strong>probability for taking each action $a$</strong> when in a state $s$</p>
  </li>
  <li>
    <p>remember that:</p>

\[R(s,a)=\mathbb{E}[R_{t+1}|S_t=s,A_t=a] = \sum_r r \sum_{s'} p(s',r|s,a)\]

    <p>which</p>

    <ul>
      <li>do not forget that $R$ is basically the <strong>expected reward after taken $s,a$ tuple</strong>.</li>
      <li>the $r$ is a random variable, and $p(s’,r\vert s,a)$ would be the joint for all possible $s’,r$ output.</li>
    </ul>

    <p>Therefore, the above essentially becomes the <strong>Bellman equation</strong></p>
  </li>
</ul>

<blockquote>
  <p><strong>Bellman Equation for $V_\pi$</strong></p>

  <p>For a <strong>stochastic policy $\pi(a\vert s)$</strong>, the formula above can be rewritten as:</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

  <p>which can be interpreted as the <strong>expected value over $a,s’,r$</strong> for $r+\gamma V_\pi(s’)$, and we are <strong>weighting it by $\pi(a\vert s)p(s’,r\vert s,a)$</strong>. Hence, graphically, we are essentially considering <strong>all the possibilities from $s$</strong>:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222220640857.png" alt="image-20220222220640857" /></p>

  <p>and summing over <strong>all of them</strong>.</p>

  <p>Notice that this value function $V_\pi(s)$ is the <strong>unique solution to its Bellman equation, given a $\pi(a\vert s)$</strong>.</p>

  <ul>
    <li>
      <p>this <strong>yields $\vert S\vert$ equations to be solved</strong>, because it can be written as:</p>

\[V_\pi = r + TV_\pi\]

      <p>for $V_\pi(s)$ is a <strong>vector of values for each state $s$</strong>, $T$ being the transition matrix, $r$ being the reward for each state.</p>
    </li>
    <li>
      <p>We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn $V_\pi(s)$.</p>
    </li>
  </ul>
</blockquote>

<p>If $\pi(s) \to a \in A$ being some <strong>deterministic</strong> policy, then we can say that:</p>

\[V_\pi(s) =  R(s,\pi(s)) + \gamma  \sum_{s'}p(s'|s,\pi(s))V_\pi(s')\]

<p>so that essentially:</p>

<ul>
  <li>current reward + an expected value summing over all the possible next state $s’$ we could take</li>
  <li>again, a <strong>recursive formula</strong>, which we can dynamically update to solve for $V_\pi(s)$</li>
  <li>since this acts on $\pi(s)\to a \in A$ instead of $\pi(a\vert s)$ being a distribution, e.g. $\pi(s) = \arg\max_a \pi(a\vert s)$, we can use this formula to ==find a policy that maximizes the discounted return==.</li>
  <li>this <strong>yields $\vert S\vert$ equations to be solved</strong></li>
</ul>

<hr />

<p>Similarly, we can inductively compute $Q^h(s,a)$ which is the <strong>action value function</strong>.</p>

<blockquote>
  <p><strong>Action Value Function</strong></p>

  <p>We denote $Q_\pi(s,a)$ as the <strong>expected return</strong> starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>

\[Q_\pi(s,a)=\mathbb{E}_\pi [G_t | S_t = s,A_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]\]

  <p>Notice that obviously:</p>

\[V_\pi(s) = \sum_{a} \pi(a|s)Q_\pi(s,a)\]

  <p>are related quantities.</p>
</blockquote>

<p>Similar to the state value function, we can compute the $Q_\pi(s,a)$ using <strong>dynamic programming</strong> with the <strong>Bellman’s Equation</strong> again:</p>

\[\begin{align*}
Q_\pi (s,a)
= \mathbb{E}_\pi [G_t | S_t =s, A_t=a]
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]
\end{align*}\]

<p>which following a similar derivation with the state value function, we can arrive at:</p>

<blockquote>
  <p><strong>Bellman Equation for $Q_\pi$</strong></p>

  <p>For a <strong>stochastic policy $\pi(a\vert s)$</strong>, the formula above can be rewritten as:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

  <p>which can be interpreted as the <strong>expected value over $s’,r$</strong> whatever is in the bracket, meaning covering all possible $s’,r$ as the next step if we did $s,a$. Then we are <strong>weighting it by $p(s’,r\vert s,a)$</strong>. Hence, graphically, we are essentially considering <strong>all the possibilities from $s,a$</strong>:</p>

  <p>Graphically you are doing:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222221819799.png" alt="image-20220222221819799" style="zoom: 80%;" /></p>

  <p>And <strong>summing over all possibilities</strong> weighted by their probability.</p>
</blockquote>

<p>Again, this is not computable, so we usually consider estimating $V_\pi,Q_\pi$ from experience. For example:</p>

<ul>
  <li>if an agent follows policy $\pi$ and <strong>maintains an average</strong>, for each state encountered, <strong>of the actual returns that have followed that state</strong>, then the average will converge to the state’s value, $V_\pi(s)$.</li>
  <li>If <strong>separate averages are kept for each action</strong> taken <strong>in a state</strong>, then these averages will similarly converge to the action values, $Q_\pi(s,a)$.</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider a setup where each cells of the grid correspond to the states of the environment.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222232047833.png" alt="image-20220222232047833" /></p>

<p>where:</p>

<ul>
  <li>at each cell, <strong>four actions are possible</strong>: north, south, east, and west, and we assume the <strong>actions are deterministic</strong> in that $p(s \to s’\vert a)=1$.</li>
  <li>actions that would take the agent out of the grid leave its location unchanged, but also result in a <strong>reward of $-1$</strong></li>
  <li>from state $A$, <strong>all four actions yield a reward of $+10$</strong> and take the agent to $A’$.</li>
  <li>from state $B$, <strong>all actions yield a reward of $+5$</strong> and take the agent to $B’$.</li>
  <li>other actions result in <strong>a reward of $0$</strong></li>
</ul>

<p>Now, to compute the <strong>value function $V_\pi(s)$</strong>, we need to specify a policy: Suppose the agent selects <strong>all four actions with equal probability in all states</strong>. Then, using $\gamma=0.9$, this policy gives:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222232432754.png" alt="image-20220222232432754" /></p>

<p>which is the <strong>value functions $V_\pi(s)$ for all state $s$ given this policy</strong>.</p>

<ul>
  <li>since the policy is stochastic, verify it using the Bellman’s equation</li>
  <li>negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy</li>
  <li>$A$ is the best state to be in under this policy but its <strong>expected return is less than 10</strong> (which is its immediate reward), because from $A$ the agent is taken to $A’$, from which it is likely to run into the edge of the grid.</li>
  <li>State $B$, on the other hand, is <strong>valued more than $5$</strong> (which is its immediate reward), because from $B$ the agent is taken to $B’$, which has a positive value
    <ul>
      <li>because at $B’$ you have a possibility of bumping into $A$ or $B$</li>
    </ul>
  </li>
</ul>

<p>(At this point, you might want to improve the policy, i.e. to move towards $A$ more rather than randomly going in all direction. If you do that, then you will need to <strong>recompute $V_\pi(s)$</strong> because the above is only true for the stochastic policy)</p>

<h3 id="optimal-value-functions">Optimal Value Functions</h3>

<blockquote>
  <p><strong>Goal</strong></p>

  <p>Solving a reinforcement learning task means, roughly, <strong>finding a policy that achieves the most reward over the long run</strong>. Then, with this definition, we can order policies by their <strong>expected return</strong>:</p>

  <p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if</p>

\[\pi \ge \pi' \iff V_\pi(s) \ge V_{\pi'}(s),\quad \forall s\]

  <p>which means its expected return is greater than or equal to that of $\pi’$ <strong>for all states</strong>. And there is ==always at least one policy that is better== than or equal to all other policies</p>
</blockquote>

<p>Hence, we can define an <strong>optimal policy $\pi^*$</strong> that must satisfy the following:</p>

\[\pi^* \to \begin{cases}
V_{\pi^*}(s) = V^*(s) &amp;= \max_\pi V_\pi(s)\\
Q_{\pi^*}(s,a) = Q^*(s,a) &amp;= \max_\pi Q_\pi(s,a)
\end{cases}\]

<p>note that there may be more than one $\pi^*$, but then by definition they must share the same constraint above, i.e have the same state value function and action value function.</p>

<p>At this point, you might wonder <strong>why do we need both $V_\pi(s),Q_\pi(s,a)$</strong>? ==Technically, we only need one of them to find $\pi^*$==</p>

<ul>
  <li>
    <p>If you have the optimal value function, $V^*$, then the actions that appear <strong>best after a one-step search</strong> (i.e. the action that goes to the best valued next state) will be <strong>optimal actions</strong></p>

    <p>Aa greedy policy is actually optimal in the long term sense <strong>because $V^*$ already takes into account the reward consequences of all possible future behavior</strong>.</p>
  </li>
  <li>
    <p>With $Q^*$, the agent does not even have to do a one-step-ahead search: for any state $s$, it can simply <strong>pick any action that maximizes $Q^*(s\vert a)$</strong> by:</p>

\[\pi^*(s) = \arg\max_a Q^*(s,a)\]

    <p>again, a deterministic policy results.</p>

    <p>Hence, at the cost of representing a function of state{action pairs, instead of just of states, the optimal action-value function allows optimal actions to be selected without having to know anything about possible successor states and their values</p>
  </li>
</ul>

<p>Before we discuss <strong>how to solve for the optimal solution</strong>, consider the MDP case of the grid:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235445697.png" alt="image-20220222235445697" /></p>

<p>The <strong>optimal solution of state value function</strong> looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235513192.png" alt="image-20220222235513192" /></p>

<p>which, by greedily looking one step ahead, we have <strong>found the optimal policy $\pi(s):S \to a \in A$</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222235610203.png" alt="image-20220222235610203" /></p>

<hr />

<p>Now, consider <strong>solving for $V^*$</strong>. We know that:</p>

<ol>
  <li>
    <p>The solution must satisfy Bellman’s Equation</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]
  </li>
  <li>
    <p>But because it is <strong>optimal</strong> (i.e. corresponds to the optimal policy), <strong>value of a state</strong> under an optimal policy must <strong>equal the expected return for the best action from that state</strong>.</p>
  </li>
</ol>

<p>Therefore, we get:</p>

\[\begin{align*}
V^* (s)
= \max_{a \in A} Q^*(s,a)
&amp;= \max_a \mathbb{E}_{\pi^*}[G_t | S_t =s, A_t=a]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s,A_t=a\right]\\
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

<p>notice that the last two lines have <strong>no reference to the optimal policy</strong>. Those two lines are also called the ==optimality equation for $V^*$==.</p>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Bellman’s Optimality equation</th>
      <th style="text-align: center">Bellman’s Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223005457522.png" alt="image-20220223005457522" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223005536963.png" alt="image-20220223005536963" /></td>
    </tr>
  </tbody>
</table>

<p>(recall that once you have $V^<em>$, you can find $\pi^</em>$ easily.)</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>To solve for the optimal state value function, the Bellman optimality equation is actually <strong>a system of equations</strong>, ==one such equation for each state==. So if there are $N$ states, then there are $N$ equations in $N$ unknowns (see example at the end)</p>
</blockquote>

<p>The <strong>Bellman’s optimality equation for $Q^*$</strong> also has to satisfy</p>

<ol>
  <li>
    <p>Bellman’s equation:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]
  </li>
  <li>
    <p>Is <strong>optimal</strong>. Hence we consider $\gamma \max_{a \in A} Q^*(s’,a’)$ instead of $\gamma \sum_a \pi(a\vert s)\sum_{s’}  Q_\pi(s’,a’)$</p>
  </li>
</ol>

<p>This gives the following <strong>optimality equation</strong>:</p>

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&amp;= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}\]

<p>and graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Bellman’s Optimality equation</th>
      <th style="text-align: center">Bellman’s Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223010401381.png" alt="image-20220223010401381" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223010502964.png" alt=" but " /></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The runtime of solving the optimal action value function will take $O(\vert A\vert \times\vert S\vert )$ for having $\vert A\vert$ possible actions.</p>
</blockquote>

<hr />

<p><em>For Example</em>: Solving Optimal Solution for Robot Collection</p>

<p>Recall the setup being:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Transitions</th>
      <th style="text-align: center">Tabluated</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218231839711.png" alt="image-20220218231839711" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220218233039582.png" alt="image-20220218233039582" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>Essentially we know $p(s’,r\vert s,a)$, then since we have <strong>two states</strong>, let us encode the two states as $s_0=\text{high}=h,s_1=\text{low}=l$. Additinally:</p>

<ul>
  <li>$s,w,re$ represents the <strong>actions</strong> search, wait, recharge.</li>
  <li>parameters $\gamma, \beta, \alpha$ are assumed to be known</li>
</ul>

<p>Then, we have <strong>two equations because we have two states</strong>:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011058649.png" alt="image-20220223011058649" style="zoom:80%;" /></p>

<p>And</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011119229.png" alt="image-20220223011119229" style="zoom:80%;" /></p>

<p>Once we solved it, say $V^<em>(h)=21$, $V^</em>(l)=10$, we have <strong>solved the Bellman’s Optimality</strong></p>

<ul>
  <li>
    <p>there is exactly one pair of numbers, $V^<em>(h), V^</em>(l)$ that simultaneously satisfy these two nonlinear equations.</p>
  </li>
  <li>
    <p>essentially we can fill in the “cells” with values we found like in this example we discussed before</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223011350739.png" alt="image-20220223011350739" style="zoom:67%;" /></p>
  </li>
</ul>

<h2 id="iterative-dp-methods">Iterative DP Methods</h2>

<p>If one can solve the Bellman’s optimality equations, then an optimal policy can be easily found and there is no more work to do. However, in reality you need to face the following problem:</p>

<ul>
  <li>even if all the information is known, computing for the solution takes <strong>huge computation power</strong></li>
  <li>In most cases of practical interest there are <strong>far more states</strong> than could possibly be entries in a table/hence need huge memory, and <strong>approximations</strong> must be made</li>
  <li>in reality, certain aspects of the environment <strong>may not be known</strong>, such as transition probabilities</li>
</ul>

<blockquote>
  <p>In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be <strong>approximated</strong> in some way</p>
</blockquote>

<p>Recall that for the Bellman’s equation for <strong>any policy</strong></p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

<p>and the <strong>optimality equations</strong>:</p>

\[\begin{align*}
V^* (s)
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]\\
&amp;= \sum_{s'}\sum_r p(s',r|s,a)\left [r + \gamma \max_{a \in A} Q^*(s',a') \right]\\
\end{align*}\]

<blockquote>
  <p><strong>Essentially</strong> dynamic programming methods:</p>

  <ul>
    <li>
      <p>==uses Bellman’s equations as update rules== and improve policy by $\arg\max$ gives <a href="#Policy Iteration">Policy Iteration</a></p>
    </li>
    <li>
      <p>==uses the optimality constraint as update rules== for improving approximations of the desired value functions. This gives <a href="#Value Iteration">Value Iteration</a></p>
    </li>
  </ul>
</blockquote>

<h3 id="policy-evaluation">Policy Evaluation</h3>

<p>First we consider how to compute the state-value function $V_\pi$ for an arbitrary policy $\pi$, using Bellman’s equation.</p>

\[V_\pi (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\]

<p>Then we have $\vert S\vert$ simultaneous linear equations since we have $V_\pi(s):s \in S$. Then the idea is to consider a <strong>sequence of approximate solutions $V_0,V_1,V_2,…$</strong> such that they obey:</p>

\[V_{k+1} (s)
= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]\]

<p>Then, it can be shown that $\lim_{k \to \infty} {V_k} = V_\pi$, i.e. converges. Therefore our algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224173242299.png" alt="image-20220224173242299" style="zoom:67%;" /></p>

<p>where the Bellman’s equation essentially will iterate over all possible $s’ \in S$ while using the current $V$.</p>

<ul>
  <li>
    <p>this algorithm is also called <strong>iterative policy evaluation</strong></p>
  </li>
  <li>
    <p>To produce each successive approximation, $V_{k+1}$ from $V_k$, iterative policy evaluation applies the same operation to each state $s$ by looking at the old values of $s$ and <strong>all possible one-step transitions</strong>, which is called a ==full backup== since it looks at the entire tree:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220222220640857.png" alt="image-20220222220640857" style="zoom:50%;" /></p>
  </li>
</ul>

<p>In reality, Algorithm 11 will need <strong>two arrays</strong>, one to keep track of old $V_{k-1}(s)$ and the other to fill in the new $V_k(s)$:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224175028170.png" alt="image-20220224175028170" style="zoom:50%;" /></p>

<p>Another variant would be to use one array and <strong>update the values in place</strong>, that is, with each new backed-up value immediately overwriting the old one.  (it can be shown that this <strong>also converges to $V_\pi$</strong>)</p>

<ul>
  <li>Then, depending on the order in which the states are backed up, sometimes new values are used instead of old ones on the right-hand side of Bellman’s equation</li>
  <li>since it uses new data as soon as possible, it actually <strong>converges faster</strong>. Hence this will be used more often in reality.</li>
</ul>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the setup of the following environment, with each state/cell assigned a value for easier math definitions:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224175845046.png" alt="image-20220224175845046" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>non-terminal states are $S = {1,2…,14}$, and terminal states are the grey boxes</li>
  <li>We will assign a reward of $-1$ on all transitions until terminal state is reached. Therefore, the expected reward function becomes $r(s,a,s’)=-1$ for all states/actions.</li>
  <li>the movement will be deterministic, so that $p(6\vert 5,\text{Right})=1$, for example.</li>
</ul>

<p>We consider evaluating a policy $\pi$ being equi-probable for all actions. Then, we can <strong>iteratively compute $V_k$</strong> using the above algorithm, and at each $V_k$, we can compute the <strong>greedy policy w.r.t. $V_k$</strong>.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224180250255.png" alt="image-20220224180250255" style="zoom:67%;" /></p>

<p>notice that the greedy policy for $V_3,V_{10},V_\infty$ is actually an <strong>optimal policy in this setup</strong> (though it is obtained from the state value function from random policy). In general, it is only ==guaranteed== that the <strong>last policy</strong>/bottom right would be an ==improvement over the given policy $\pi$==.</p>

<ul>
  <li>this will hint at what we want to do in the next section</li>
</ul>

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Our reason for computing the value function for a policy is to help <strong>find better policies</strong>, just as what we have seen in the previous section. In general, it can be shown that</p>

<blockquote>
  <p><strong>Policy Improvement Theorem</strong></p>

  <p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s \in S$:</p>

\[Q_\pi(s, \pi'(s)) \ge V_\pi(s)\]

  <p>Then it ==must be that==</p>

\[V_{\pi'}(s) \ge V(s),\quad \forall s\]

  <p>Moreover, if there is a strict inequality in one of the state $Q_\pi(s, \pi’(s)) &gt; V_\pi(s)$, then there must be at least a strict inequality for $V_{\pi’}(s) &gt; V(s)$.</p>
</blockquote>

<p>A simple simple example to see how the theorem works is to consider a changed policy, $\pi’$, that is identical to $\pi$ except that $\pi(s’)=a \neq \pi(s)$ for <strong>only one $s \in S$.</strong> Then, if we know $Q_\pi(s,a) &gt; V_\pi(s)$ for that $s$, it follows that $V_{\pi’}(s) &gt; V_\pi(s)$.</p>

<p>Then, since it works for <strong>one $s \in S$</strong>, we can extend it to <strong>all $s \in S$</strong>: selecting at each state the action that appears best according to $Q_\pi(s,a)$ ==after you computed $V_\pi(s)$==:</p>

\[\begin{align*}
\pi^\prime (s)
= \arg\max_{a \in A} Q_\pi(s,a)
&amp;= \arg\max_a\mathbb{E}_{\pi} \left[R_{t+1}+\gamma V_\pi(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \arg\max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_\pi(s')]\\
\end{align*}\]

<p>notice that:</p>

<ul>
  <li>By construction, the greedy policy meets the conditions of the policy improvement theorem, so we know that it is as good as, or better than, the original policy: it <strong>guarantees improvement</strong> unless we are optimal already.</li>
  <li>it is essentially a function of $V_\pi(s)$, which depends on the original $\pi$ and a converged $V_\pi$. (this should provide enough hint how to design an algorithm to find out best $\pi^*$)</li>
</ul>

<blockquote>
  <p><strong>Extension to Stochastic Policy</strong></p>

  <p>Recall that for a stochastic policy $\pi(s)=\pi(a\vert s)$ spit out a probability distribution:</p>

\[Q_\pi(s,\pi^\prime(s)) = \sum_a \pi^\prime (a|s)Q_\pi(s,a)\]

  <p>Then the idea is to basically, if there are several actions which the maximum can be achieved, we assign <strong>each of those actions a portion of the probability</strong> in the new greedy policy. A simple example of this would be the case we discussed before:</p>

  <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224191414214.png" alt="image-20220224191414214" style="zoom:67%;" /></p>

  <p>where the LHS was the state function for equi-probable stochastic policy, whereas the RHS is the improved greedy policy, which is still stochastic.</p>
</blockquote>

<h3 id="policy-iteration">Policy Iteration</h3>

<p>At this point, you know how to:</p>

<ol>
  <li>evaluate $V_\pi$ of a policy $\pi$</li>
  <li>improve a policy $\pi \to \pi’$</li>
</ol>

<p>Then we can basically repeat the above loop again and again to improve our policy. We can thus obtain a sequence of monotonically improving policies and value functions:</p>

\[\pi_0 \to V_{\pi_0}\to \pi_1 \to V_{\pi_1}\to .... \pi_* \to V_{\pi_*}\]

<p>Hence the algorithm is simply:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224191804784.png" alt="image-20220224191804784" style="zoom:67%;" /></p>

<p>which works because a <strong>finite MDP has only a finite number of policies</strong>, this process must converge to an optimal policy and optimal value function in a finite number of iterations.</p>

<ul>
  <li>
    <p>While this looks like a painful computation heavy algorithm, it converges actually pretty fast, presumably because the value function changes little from one policy to the next are our policy is getting better</p>
  </li>
  <li>
    <p>but still, sometimes it takes a long time, in which case we can look at value iteration methods</p>
  </li>
</ul>

<h3 id="value-iteration">Value Iteration</h3>

<p>The key idea is that <strong>policy evaluation</strong> step of policy iteration can be <strong>truncated in several ways without losing the convergence</strong> guarantees of policy iteration. The idea is to <strong>combine policy improvement and evaluation</strong>:</p>

\[V_{k+1} (s)
= \max_a \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V_k(s')]\]

<p>which if you recall, was the optimality constraint:</p>

\[\begin{align*}
V^* (s)
&amp;= \max_a\mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma V^*(s_{t+1})|S_t=s,A_t=a\right]\\
&amp;= \max_{a \in A}\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma V^*(s')]\\
\end{align*}\]

<blockquote>
  <p>And it can be shown that this sequence $\lim_{k \to \infty }{V_k} \to V^*$ <strong>converges</strong> directly to optimal state value function.</p>
</blockquote>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224193336422.png" alt="image-20220224193336422" style="zoom:67%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement</p>
  </li>
  <li>
    <p>we can again use early stopping if it would run for a long time.</p>
  </li>
</ul>

<h2 id="monte-carlo-methods">Monte Carlo Methods</h2>

<p>In this chapter we consider our first learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we ==do not assume complete knowledge of the environment==. Therefore, this big change means that MC methods requires only <em>experience</em>, i.e. ==sampling state/action/rewards== from an environment.</p>

<blockquote>
  <p>Monte Carlo methods are ways of solving the reinforcement learning problem based on <strong>averaging sample returns</strong>. This places an assumption that it is defined for <strong>episodic tasks</strong>, instead of a continuous space.</p>

  <p>Specifically, features of this method include:</p>

  <ul>
    <li>
      <p>experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected</p>
    </li>
    <li>
      <p>only on the completion of an episode are value estimates and policies changed</p>
    </li>
  </ul>
</blockquote>

<p>Again, the key difference here is that before we <strong>computed value functions</strong> from knowledge of the MDP, here we <strong>learn value functions from sample returns</strong> with the MDP.</p>

<h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3>

<p>We begin by considering Monte Carlo methods for <strong>learning the state-value function</strong> for a given policy</p>

<blockquote>
  <p><strong>Key Idea</strong></p>

  <p>The value of a state $s$ is the <strong>expected return</strong> - expected cumulative future discounted reward - <strong>starting from that state</strong>. So an obvious idea to estimate it is simply to <strong>average the returns observed after visits to that state $s$</strong></p>
</blockquote>

<p>Suppose we wish to estimate $V_\pi(s)$, the value of a <strong>specific state $s$</strong>, given a set of episodes obtained by following $\pi$ and passing through $s$. Each occurrence of state s in an episode is called a <em>visit</em> to $s$, so that $s$ may be visited many times in an episode.</p>

<ul>
  <li>first-visit MC method estimates $V_\pi(s)$ as the ==average of the returns following first visits to $s$==</li>
  <li>every-visit MC method averages the returns following all visits to $s$</li>
</ul>

<p>Then the algorithm for First visit MC prediction is:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Abstraction</th>
      <th style="text-align: center">Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201733290.png" alt="image-20220224201733290" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201144309.png" alt="image-20220224201144309" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>the <code class="language-plaintext highlighter-rouge">if</code> statement in the highlight box basically checks if $S_t$ is the first time encountered in the sequence of states, action, reward pair.</li>
</ul>

<p>The every-visit versoin looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224201238212.png" alt="image-20220224201238212" style="zoom: 67%;" /></p>

<p><strong>Both first-visit MC and every-visit MC converge</strong> to $V_\pi(s)$ as the number of visits (or first visits) to s goes to infinity. However, they do have certain different theoretic properties, which is not covered here. In reality, the first visit version is used more often.</p>

<p>Last but not least, we can also leverage DP to make the averaging easier as:</p>

\[\mu_t = \mu_{t-1} + \frac{1}{t}(x_t - \mu_{t-1})\]

<p>for each new data $x_t$ that would contribute to $\mu_t$.</p>

<hr />

<p><em>For Example:</em> Black Jack</p>

<p>This would be a nice exercise for you to think of how to convert a real life game into <strong>state/action/reward</strong> where we can use the above method to optimize.</p>

<p>Assuming you are clear of the rules, then</p>

<ul>
  <li>Each game of blackjack is an episode.</li>
  <li>Rewards of $+1,-1,0$ are given for winning, losing, and drawing, respectively.</li>
  <li>All <strong>rewards</strong> within a game are zero, and we do not discount ($\gamma = 1$); therefore these terminal rewards are also the returns.</li>
  <li>The player’s <strong>actions</strong> are to hit or to stick.</li>
  <li>The <strong>states</strong> depend on the player’s cards and the dealer’s showing card.</li>
</ul>

<p>We further impose the assumption that the deck is infinite, so you don’t need to remember what cards are dealt, and you are competing independently against the dealer. This then restricts our states and actions to:</p>

<ul>
  <li>the player makes decisions on the basis of three variables: his current sum  $[12-21]$, the dealer’s one showing card ($A-10$, and whether or not he holds a <em>usable</em> ace (if the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable)</li>
  <li>if you have sum below $12$, then you definitely call hit so that we don’t need to compute.</li>
</ul>

<p>This makes for a total of 200 states, and we can use MC method to compute best policy.</p>

<h3 id="backup-diagram-for-mc-method">Backup Diagram for MC Method</h3>

<blockquote>
  <p>The general idea of a backup diagram is to show at the top the <strong>root node to be updated</strong> and to show below all the transitions and leaf nodes <strong>whose rewards and estimated values contribute to the update</strong></p>
</blockquote>

<p>Graphically, MC methods when given an episode does:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Iterative Method</th>
      <th style="text-align: center">MC Method</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205607347.png" alt="image-20220224205607347" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>so the difference with those DP based method is that:</p>

<ul>
  <li>DP diagram (LEFT) shows <strong>all possible transitions</strong>, the Monte Carlo diagram shows only those <strong>sampled</strong> <strong>on the one episode</strong>.</li>
  <li>DP diagram includes only <strong>one-step transitions</strong>, the Monte Carlo diagram goes <strong>all the way to the end of the episode</strong></li>
</ul>

<blockquote>
  <p>In particular, note that the computational expense of estimating the value $V_\pi(s)$ of a single state is <strong>independent of the number of states</strong>. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states.</p>
</blockquote>

<h3 id="mc-for-action-values">MC for Action Values</h3>

<p>Again, if the model is not available (i.e. no external device can tell you $p(s’,r\vert s,a)$ directly, or $r(s,a)$), you will need sampling to compute action value functions $Q_\pi$. Luckily, you soon realize the idea is the same as Prediction for state value function</p>

<blockquote>
  <p>The policy evaluation problem for action values is to estimate $Q_\pi(s,a)$, the <strong>expected return</strong> when starting in state $s$, taking action $a$, and thereafter following policy $\pi$.</p>

  <p>Therefore, now we talk about <strong>visits to a state-action pair</strong> (state $s$ is visited and action $a$ is taken in it) rather than to a state in an episode. We estimates the value of a state-action pair as the <strong>average of the returns</strong> that have followed from:</p>

  <ul>
    <li>the first time in each episode that the state was visited and the action was selected - First Time Method</li>
    <li>or doing every time method</li>
  </ul>
</blockquote>

<p>The only complication is that many state-action pairs <strong>may never be visited</strong>.</p>

<ul>
  <li>
    <p>If $\pi$ is a deterministic policy, then in following $\pi$ one will observe returns <strong>only for one of the actions from each state</strong>.</p>

    <p>One way to fix this is by <em>specifying</em> that the episodes start in a state-action pair, and that <em>every pair has a nonzero probability</em> of being selected as the start.</p>
  </li>
  <li>
    <p>if policy $\pi$ is stochastic with a nonzero probability of selecting all actions in each state, then there is <strong>no problem</strong>. This is more reliable and used more often.</p>
  </li>
</ul>

<h3 id="monte-carlo-control">Monte Carlo Control</h3>

<p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate <strong>optimal policies</strong> using what we have built up so far (<strong>methods for evaluating value functions</strong>)</p>

<p>To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of <strong>policy evaluation and policy improvement</strong>, beginning with an arbitrary policy $\pi_0$ and ending with the optimal policy and optimal action-value function\</p>

\[\pi_0 \to q_{\pi_0} \to \pi_1 \to q_{\pi_1} \to ...\pi_* \to q_{\pi_*}\]

<p>which is like the DP iterative algorithm, but  now:</p>

<ul>
  <li>
    <p>Policy evaluation step: <strong>Generating episodes</strong> and using MC method to compute $q_{\pi_i}$ following that $\pi_i$</p>
  </li>
  <li>
    <p>Policy improvement: <strong>Making the policy greedy</strong> with respect to the current value function</p>

\[\pi_{k+1}(s) = \arg \max_a q_{k}(s,a)\]
  </li>
</ul>

<p>Hence the full algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224213351523.png" alt="image-20220224213351523" style="zoom:67%;" /></p>

<h2 id="temporal-difference-learning">Temporal-Difference Learning</h2>

<p>TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas.</p>

<ul>
  <li>Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics</li>
  <li>Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).</li>
</ul>

<p>Again this is a model-free approach.</p>

<blockquote>
  <p><em>Recall</em>: Model Free vs Model-Based</p>

  <p><strong>Model-based</strong>: the agent, while learning, can ask the model for the <em>expected</em> next reward, or the full <em>distribution</em> of next states and next reward. i.e. it has <em>complete information for next state, next reward</em>.</p>

  <ul>
    <li>e.g. by computer code that understands the rules of a dice or board game</li>
  </ul>

  <p><strong>Model-free</strong>: have nothing, purely sample from experience.</p>

  <ul>
    <li>e.g. MC methods, TD learning, etc.</li>
  </ul>
</blockquote>

<h3 id="td-prediction">TD Prediction</h3>

<p>Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy $\pi$:</p>

<ul>
  <li>both methods update their estimate $V_\pi(s)$ for the nonterminal states $s_t$ occurring in that experience</li>
</ul>

<p>However, the difference is that:</p>

<ul>
  <li>
    <p>Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V_\pi(s_t)$, i.e.  estimates $V_\pi(s)$ as the average of the returns following first visits to $s$. The every-visit update rule can be generalized to:</p>

\[V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]\]

    <p>for $G_t$ is the actual return following time $t$, and $\alpha$ is a constant step-size parameter</p>
  </li>
  <li>
    <p>Whereas Monte Carlo methods must <strong>wait until the end of the episode</strong> to determine the increment to $V(s_t)$ (only then is $G_t$ known), TD methods need <strong>wait only until the next time step</strong>:</p>

\[V(s_t) \leftarrow V(s_t) + \alpha[R_{t+1} + \gamma V(s_{t+1}) - V(s_t)]\]
  </li>
</ul>

<p>Therefore, the big difference stems from the fact that the target value for MC update is $G_t$, but for TD update it is</p>

\[R_{t+1} + \gamma V(s_{t+1})\]

<p>The idea basically comes from the derivation that</p>

\[\begin{align*}
V_\pi (s)
&amp;= \mathbb{E}_\pi [G_t | S_t =s]\\
&amp;= \mathbb{E}_\pi \left[ \sum_{k=0}^T \gamma^k R_{t+k+1}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1} + \gamma \sum_{k=0}^T \gamma^k R_{t+k+2}|S_t=s\right]\\
&amp;= \mathbb{E}_\pi \left[ R_{t+1}+ \gamma V_\pi(s_{t+1})|S_t=s \right]\\
\end{align*}\]

<p>so that MC is using the first equality for estimating $V_\pi$, DP is using the last equality for estimating $V_\pi$.</p>

<ul>
  <li>Monte Carlo target is an estimate because the expected value in the first line is not known</li>
  <li>The DP target is an estimate because $V_{\pi}(s_{t+1})$ is not known</li>
</ul>

<p>Then TD is <strong>combining DP + MC</strong> by sampling form the environment like in the first equality, but uses DP for estimate $V_\pi(s)\approx V_\pi(s’)$ and converge for solution. Hence the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301170155096.png" alt="image-20220301170155096" style="zoom: 67%;" /></p>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Backoff Diagram for TD(0)</th>
      <th style="text-align: center">Backoff Diagram for MC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224223527026.png" alt="image-20220224223527026" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224205437456.png" alt="image-20220224205437456" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>On a larger scale, you are basically only looking ahead a single step</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224223446750.png" alt="image-20220224223446750" style="zoom: 50%;" /></p>

<hr />

<p><em>For Example</em></p>

<p>Consider predicting the time for you to return to your home from an office.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224225822737.png" alt="image-20220224225822737" style="zoom:67%;" /></p>

<p>then in this example:</p>

<ul>
  <li><strong>state</strong> is given as “leaving office”. “reach car”, etc.</li>
  <li><strong>return</strong> for each state is the actual time to go <em>from that state</em></li>
  <li><strong>value</strong> of each state $V_\pi(s)$ is the expected/actual time to go. Current <strong>estimated value</strong> for each state will there for be the “Predicted Time to Go”</li>
  <li>we pick $\gamma=1$</li>
</ul>

<p>Then, if we do MC method, then essentially updates are proportional to the difference between $G_t - V(s_t)$, where $G_t$ is the actual time in the end:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224230158369.png" alt="image-20220224230158369" /></p>

<p>essentially the error here would be the difference between “Actual Time to Go” and “Predicted Time to Go”</p>

<p>If we do TD method, then the update will be based on the <strong>next immediate observed value</strong> $R_{t+1}+\gamma V(s_{t+1})-V(s_t)$, hence it is ==Markov like== and looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224230353766.png" alt="image-20220224230353766" /></p>

<p>where the length of the arrow is proportional to the error, proportional to the <strong>temporal difference</strong>.</p>

<h3 id="td-vs-mc-methods">TD vs MC Methods</h3>

<p>Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\alpha$, as long as $\alpha$ is chosen to be sufficiently small. The constant-$\alpha$ MC method also converges deterministically under the same conditions, but to a <strong>different</strong> answer.</p>

<hr />

<p><em>For Example</em></p>

<p>Suppose you observe the following eight episodes:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301175321270.png" alt="image-20220301175321270" style="zoom:67%;" /></p>

<p>where we would like to estimate $V(A),V(B)$:</p>

<ul>
  <li>
    <p>the true value of $V(B)$ will be clearly $6/8$ because six out of the eight times in state $B$ the process terminated immediately with a return of $1$ and the other two $0$. This would also be the answer that both MC and TD will converge to.</p>
  </li>
  <li>
    <p>the value of $V(A)$ differs:</p>
    <ul>
      <li>
        <p>Observe that $100\%$ of the times the process was in state $A$ it traversed immediately to $B$ (with a reward of $0$); and since we have already decided that $B$ has value $V(B)=3/4$ , therefore $A$ must have value $3/4$ as well. This would be what TD converges to (in TD, the update depends on the difference between current reward and <strong>next expected reward</strong>).</p>

        <p>Graphically:</p>

        <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301175551863.png" alt="image-20220301175551863" style="zoom:67%;" /></p>

        <p>which is basically <strong>Markov</strong></p>
      </li>
      <li>
        <p>The other reasonable answer is simply to observe that we have seen $A$ once and the return that followed it was $0$; we therefore estimate $V(A)$ as $0$​. This is the answer that batch Monte Carlo methods give.</p>

        <p>(recall that in MC, the update depends on the difference between current reward and <strong>final reward in the episode</strong>.)</p>
      </li>
    </ul>
  </li>
</ul>

<p>Consider optimal estimates in the sense that they <strong>minimize the mean-squared error from the actual returns</strong> in the training set, then</p>

<ul>
  <li>MC would converge to the state value that gives minimum squared error on the training data.</li>
  <li>TD estimate would be exactly correct for the maximum-likelihood model of the <strong>Markov process</strong> (i.e. based on first
modeling the Markov process, then computing the correct estimates given the model)</li>
</ul>

<hr />

<p><strong>Advantages of TD Method</strong></p>

<ul>
  <li>
    <p>an advantage over DP methods in that they do not require a model of the environment</p>
  </li>
  <li>most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an <strong>on-line</strong>, fully incremental fashion.</li>
  <li>even as it is online, for any fixed policy $\pi$, the TD algorithm described above has been proved to <strong>converge</strong> to $V_\pi$,</li>
</ul>

<h3 id="td-lambda">TD lambda</h3>

<p>Notice that another view of the difference between TD and MC is essentially their backoff diagram:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301133529126.png" alt="image-20220301133529126" style="zoom: 67%;" /></p>

<p>where we notices that:</p>

<ul>
  <li>TD looks at one step ahead in backoff diagram and updates <strong>only depend on the next state</strong></li>
  <li>MC looks at a full height/depth in backoff diagram and updates depends on all the state until terminal</li>
</ul>

<p>Then in TD $\lambda$, we essentially combine the two:</p>

<p><img src="https://miro.medium.com/max/1838/1*L-LUOyW5W-0gBxx80GHdHQ.png" alt="Reinforcement Learning — TD(λ) Introduction(1) | by Jeremy Zhang | Towards  Data Science" style="zoom:33%;" /></p>

<p>so that basically $TD(0)$ with $\lambda = 0$ is the same as the TD prediction we discussed in the previous section.</p>

<h2 id="onoff-policy-learning">On/Off Policy Learning</h2>

<p>On policy methods:</p>

<ol>
  <li>Estimate the value of a policy while using it for control (i.e. generating episodes)</li>
  <li>Evaluate or improve the policy that is used to make decision (e.g. take greedy step)</li>
</ol>

<p>Off policy methods</p>

<ul>
  <li>Evaluate or improve a policy <strong>different from that used to generate the data</strong></li>
  <li>Separate these two functions</li>
  <li>Behavior policy: policy used to generate behaviour</li>
  <li>Target policy: policy that is imitated and improved</li>
  <li>Follow behavior policy while improving target policy</li>
  <li>Reuse experience generated from old policie</li>
</ul>

<h3 id="sarsa">SARSA</h3>

<p>SARSA is an <strong>on-policy method</strong> using TD methods for the evaluation of a <strong>action value function</strong>. In particular, this is done by:</p>

<ol>
  <li>for an on-policy method we must estimate $Q_\pi(s,a)$ for the current behavior policy $\pi$ and for all states $s$ and actions $a$.</li>
</ol>

<p>Recall that an episode essentially looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301202009996.png" alt="image-20220301202009996" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>In the previous section we considered transitions from state to state and <strong>learned the values of states</strong>. Now we consider transitions from state-action pair to state{action pair, and learn the <strong>value of state-action pairs</strong></p>
  </li>
  <li>
    <p>therefore, as it is TD learning, we consider the update rule being</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) ]\]

    <p>This update is done after every transition from a nonterminal state $s_t$.</p>

    <p>This rule uses every element of the quintuple of events, $(s,a,r’,s’,a’)$, hence it is called SARSA</p>
  </li>
</ul>

<p>As in all on-policy methods, we continually <strong>estimate $Q_\pi$ for the behavior policy $\pi$,</strong> and at the same time <strong>change $\pi$ toward greediness</strong> with respect to $Q_\pi$.</p>

<p>Algorithm:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301201435273.png" alt="image-20220301201435273" style="zoom: 50%;" /></p>

<p>where</p>

<ul>
  <li>in Sutton and Bartol, the “improve policy” step is also called “policy derived from $Q$ from $\epsilon$-greedy”</li>
  <li>it can be shown that this will converge to some <strong>optimal $Q_\pi$</strong>, hence returning optimal policy</li>
  <li>note that this update of $Q_\pi$ is <strong>updating  $Q_\pi$ for the $\pi$ it is currently following</strong>. Hence it is called ==on policy==.</li>
</ul>

<h3 id="q-learning">Q-Learning</h3>

<p>Recall that we know:</p>

\[Q_\pi (s,a)
= \sum_{s'}\sum_r p(s',r|s,a)\left[r + \gamma \sum_a \pi(a|s)\sum_{s'}  Q_\pi(s',a')\right]\]

<p>But we know that here we are model-less, hence we <em>don’t know $p(s’,r\vert s,a)$</em>. The idea is to essentially will learn the $Q$ function directly from experience, known as Q-learning:</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]\]

<p>comes from the <strong>optimality constraint</strong>, where the target is $R_{r+1} + \gamma \max_a Q(s_{t+1},a)$:</p>

\[\begin{align*}
Q^* (s,a)
&amp;= \mathbb{E}_{\pi^*} \left[R_{t+1}+\gamma \max_a Q^*(s_{t+1},a')|S_t=s,A_t=a\right]
\end{align*}\]

<blockquote>
  <p><strong>Therefore</strong>, Q-learning <strong>directly approximates $Q^*$</strong> using MC + DP like approach: TD update rule.</p>
</blockquote>

<p>Hence, the algorithm looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301203045085.png" alt="image-20220301203045085" style="zoom: 80%;" /></p>

<p>where in this case</p>

<ul>
  <li>the TD prediction task is to <strong>evaluate $V_\pi(s)$</strong> when given $\pi$, but here we can directly find $Q^<em>$ hence $\pi^</em>$!</li>
  <li>note that this update of $Q$ is <strong>updating  $Q$ for the $\pi^<em>$** by directly estimating $Q^</em>$, but it **behaves by $\pi_b$</strong>, which could be a policy with equal probability for every action. Hence it is <strong>not</strong> estimating $Q_\pi$ for the $\pi$ that generated the action like the one in SARSA. Hence it is called ==off policy==.
    <ul>
      <li>this way it is also continuously exploring</li>
    </ul>
  </li>
</ul>

<p>Since this is TD like rule, the backoff tree is also Markov like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220224232119132.png" alt="image-20220224232119132" /></p>

<p>where updating the root node only depends on nodes one step away.</p>

<hr />

<p><em>For Example</em>: Cliff Walking</p>

<p>This example aims to compare the difference of the optimal policy learnt by SARSA and Q-learning. Consider the game of cliff walking, where you will have a start state $S$, and an end state $G$. Additionally:</p>

<ul>
  <li>each state along any element will have reward of $-1$</li>
  <li>if you fall off the cliff (i.e. on the cliff cell), you have $-100$ and will be sent back to the start state</li>
  <li>the available actions are still left, right, up and down</li>
</ul>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301205614330.png" alt="image-20220301205614330" /></p>

<p>Interestingly, assuming our action will be $\epsilon$-greedy (i.e. optimal for $1-\epsilon$ of the time)</p>

<ul>
  <li>
    <p><strong>SARSA</strong> will converge to the <strong>safe path</strong> while <strong>Q-learning</strong> to the <strong>optimal</strong> path</p>

    <ul>
      <li>so Q-learning results in its occasionally falling off the cliff because of the $\epsilon$-greedy action selection</li>
      <li>SARSA takes the action selection into account and learns the longer but safer path through the upper part of the grid</li>
    </ul>

    <p>So in a sense Q-learning is <strong>optimistic</strong> about what happens when an action $a$ is taken, while SARSA is <strong>realistic</strong> about it.</p>
  </li>
  <li>
    <p>Of course, if $\epsilon$ were gradually reduced, then <strong>both</strong> methods would asymptotically converge to the optimal policy.</p>
  </li>
</ul>

<p>For those who are interested, the learning curve looks like:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301210416783.png" alt="image-20220301210416783" style="zoom:80%;" /></p>

<h2 id="maximum-entropy-rl">Maximum Entropy RL</h2>

<p>Before, our objective for those algorithms are to learn the <strong>optimal policy by maximizing the expected return</strong>.</p>

\[\pi^* = \arg\max_\pi \mathbb{E}[G_t]\]

<p>However, another interesting objective to consider is to <strong>also maximize the entropy of actions</strong>\pi^* = \arg\max_\pi \mathbb{E}[G_t]</p>

\[\pi^* = \arg\max_\pi \mathbb{E}[G_t] + H_\pi(a|s)\]

<p>where the entropy term is</p>

\[H_\pi(a|s) = \sum_t H_\pi(a_t|s_t) = \sum_t \mathbb{E}[-\log \pi(a_t|s_t)]\]

<p>Optimizing this objective will promote both high return and exploration. This then will lead to designing loss functions when doing NN based reinforcement learning.</p>

<h2 id="summary-of-rl-models">Summary of RL Models</h2>

<p>In general, we have covered RL methods that are</p>

<ul>
  <li><strong>Value-based</strong>
    <ul>
      <li>Estimate value function $Q^<em>(s,a)$ or $V^</em>(s)$</li>
      <li>then return policy by greedy</li>
    </ul>
  </li>
  <li><strong>Policy-based</strong>
    <ul>
      <li>Search directly for optimal policy $\pi$ Achieving maximum future reward</li>
    </ul>
  </li>
  <li><strong>Model-based</strong>
    <ul>
      <li>here it refers to you are either given the model/interaction environment</li>
      <li>or you are learning the model itself
        <ul>
          <li>Build transition model of environment</li>
          <li>Plan by lookahead using model (i.e. ask the model what the reward if action $a$ is taken, then choose best action)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Taxonomy:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301135343105.png" alt="image-20220301135343105" style="zoom: 50%;" /></p>

<hr />

<p><em>For Example</em>: World Model</p>

<blockquote>
  <p>Our <em>world model</em> can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment</p>
</blockquote>

<p>The architecture is basically as follows, where as the observation is the game</p>

<p><img src="https://miro.medium.com/proxy/1*9WsfCbPtdoKzL5IPThLWRg.png" alt="World models — a reinforcement learning story | by SmartLab AI | Medium" style="zoom:50%;" /></p>

<p>then, notice that the controller $C$ output also to the RNN model, which <strong>models the game</strong> (in-game graphics)</p>

<ul>
  <li>therefore, once trained, you can essentially play by just using RNN model, by feeding in actions to it and let it generate the output observaton/images</li>
  <li>since the policy it learns will then be <strong>based on how $M$ models the game</strong>, it is essentially learning the model.</li>
</ul>

<h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>

<p><em>Recall that</em></p>

<p>The previous chapter presents MDPs and reinforcement learning. A key difference between the two is that</p>

<ul>
  <li>when solving MDPs we know the transition function $T$ and reward function $R$</li>
  <li>in reinforcement learning we do not know the transition or reward functions. In reinforcement learning an agent samples the
environment and the previous chapter ends with the Q-learning algorithm which learns $Q^*(s, a)$ from experience.</li>
</ul>

<p>But regardless which one we use, they are all <strong>tabular methods</strong> in nature, i.e. we need to he value function in some table. (e.g. for a state function with $\vert S\vert$ states, your table will be $\vert S\vert$ in size)</p>

<hr />

<p>In many cases, storing the $Q$ values in a table may be infeasible when the state or action spaces are very large or when they are continuous. For example, the game of Go consists of $10^{170}$ states, or when the state or action spaces include continuous variables or complex sensations. A solution is to <strong>approximate the value function or approximate the policy</strong>.</p>

<blockquote>
  <p>In Deep RL, we essentially take examples from a desired function (e.g., a value function) and attempts to generalize from them
to construct an <strong>approximation of the entire function</strong> (e.g. in the entire continuous domain).</p>

  <ul>
    <li>e.g. instead of predicting image classes using CNN, we may <strong>predict</strong> the values of a state or the <strong>probabilities of actions $\pi(s\vert a)$ using a neural network</strong>, and based on these probabilities we can take action</li>
  </ul>
</blockquote>

<p>Then the idea for using NN as function approximation in RL can be shown below:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301135857210.png" alt="image-20220301135857210" style="zoom: 67%;" /></p>

<p>where essentially:</p>

<ul>
  <li>Given a state $s$ as input, such as an image of pixels</li>
  <li>neural network outputs an approximated vector of probabilities for each action given the state, i.e. $\pi_\theta(a\vert s)$</li>
  <li>finally we can pick $a_t$ be the action by, e.g. taking the highest probability</li>
</ul>

<p>To train the model in a supervised case, we have in general</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">General Supervised ML</th>
      <th style="text-align: center">Supervised RL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301134342206.png" alt="image-20220301134342206" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220301134356226.png" alt="image-20220301134356226" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>So that in a supervised case, we would need ground truth actions $a$ as the label to train the neural network to learn the stochastics policy $\pi_\theta(a\vert s)$ with weights $\theta$.</p>

<h2 id="function-approximations">Function Approximations</h2>

<p>As mentioned before, real-world problems often consists of large or continuous state and action spaces, so tabular methods introduced in the previous section will not work for finding $V_\pi(s)$ or $Q_\pi(a\vert s)$. Hence, we resort to use <strong>deep neural network</strong> to find <strong>approximations such as</strong></p>

<ul>
  <li>$V_\theta(s) \approx V_\pi(s)$ for state value</li>
  <li>or $Q_\theta(s,a)\approx Q_\pi(s,a)$ for action value</li>
  <li>or $\pi_\theta(a\vert s)$</li>
</ul>

<p>using only some finite number of weights usually much smaller than the total number of possible states.</p>

<p>However, the techniques learnt in previous sections aren’t useless, recall that all the <strong>back-off operations</strong> we were doing for estimating some value function at state $s$:</p>

<ul>
  <li>MC does $V(s_t) \to G_t$, shifting the estimated value towards $G_t$ (by some fraction), for $G_t$ being the <strong>target</strong></li>
  <li>TD(0) does $V(s_t) \to R_{t+1} + \gamma V(s’)$</li>
  <li>TD($\lambda$) does $V(s_t) \to G_t^\lambda$</li>
  <li>DP does $V(s_t) \to \mathbb{E}<em>\pi[R</em>{t+1} + \gamma V(s’)\vert S_t=s]$ which is exact since model would be given.</li>
</ul>

<p>Also, notice that the first three will backup the state $s_t$ only when encountered in some episode, whereas the last one will update every single $s$ as they are known.</p>

<blockquote>
  <p>Another way to view this would be to think of the backoff shift as an <strong>input-output pair</strong>: the input of the function will be $s_t$, and you <strong>want</strong> the <strong>output to be close to the RHS of the expression above</strong>. Hence this could be used as a supervised dataset!</p>
</blockquote>

<h3 id="state-value-function-approximation">State Value Function Approximation</h3>

<p>As we are training a NN to approximate $V_\pi(s)$, e.g. w.r.t. ==some policy $\pi$==, we first need some definition of <strong>loss function</strong>.</p>

<ul>
  <li>Most supervised learning methods seek to minimize the <strong>mean-squared error (MSE)</strong></li>
  <li>the label/<strong>ground truth</strong> will be $V_\pi(s)$ which is unknown, but can be any of the <strong>approximate target</strong> mentioned above, e.g. $R_{t+1} + \gamma \hat{V}_\theta(s’)$.</li>
</ul>

<p>Therefore, often our loss would be:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (V_\pi(s) - V_\theta(s))^2 = \frac{1}{2}\mathbb{E}_s[(V_\pi(s) - V_\theta(s))^2] \iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]\]

<p>where:</p>

<ul>
  <li>technically this will be a <strong>weighted version</strong> of MSE because $\sum_s \mu(s)=1$ gives relative importance to learn certain states. (because the number of parameters we learn will be much less than $\vert S\vert$, we cannot find the exact solution)</li>
  <li>$V_\theta(s)$ will be our approximate, and we want to learn $\theta$.</li>
  <li>in reality we use the sum term for loss, but for doing math using the $\mathbb{E}_s$ notation will be easier.</li>
  <li>so basically the <strong>label is $V_\pi(s)$</strong>, hint that we essentially have a ==supervised training==.</li>
</ul>

<p>This general form is differentiable, simply:</p>

\[\nabla_\theta J(\theta) = - \mathbb{E}_s[(V_\pi(s)-V_\theta(s))\cdot \nabla_\theta V_\theta(s)]\\
\theta_{i+1} = \theta_i - \alpha \nabla_\theta J(\theta_i)\]

<p>for the gradient descent update. If we use <strong>stochastic gradient descent of a single sample</strong>, then we throw away the expectation/weighted average of the samples and do:</p>

\[\theta_{i+1} = \theta_i + \alpha (V_\pi(s_i) - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]

<p>where now the only <strong>known is $V_\pi(s)$</strong>. Hence, we consider using an <strong>estimate such as the four targets mentioned before</strong></p>

<ul>
  <li>
    <p>MC learning with $V_\pi(s_i) \approx G_i$ being the return following state $s_i$:</p>

\[\theta_{i+1} = \theta_i + \alpha (G_i - V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]
  </li>
  <li>
    <p>TD(0) learning with</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma V_\theta(s_{i+1})- V_\theta(s_i)) \nabla_\theta V_\theta(s_i)\]
  </li>
</ul>

<blockquote>
  <p>Not all approximation targets are <strong>unbiased</strong>. By unbiased, we need:</p>

\[\mathbb{E}[V_i] = V_\pi(s_i)\]

  <p>where $V_i$ we inserted estimates such as $G_i$ in MC method.</p>

  <ul>
    <li>for MC, this is <strong>unbiased</strong> because $\mathbb{E}[G_i] = V_\pi(s_i)$​ by definition of value function. Therefore, this converges to a locally optimal approximation to $V_\pi(s_i)$.</li>
    <li>but for methods such as TD($\lambda$), it can be shown that $\lambda &lt; 1$​ has a <strong>biased estimate</strong>, hence does not actually converge to a local optimum. (Nevertheless, such bootstrapping methods can be quite effective, and other performance guarantees are available for important special cases)</li>
  </ul>
</blockquote>

<p>Essentially, though we do not know the ground truth $V_\pi(s)$, we can assume that every sample we got from reality <strong>is the ground truth</strong>, hence those update rules.</p>

<h3 id="action-value-function-approximation">Action-Value Function Approximation</h3>

<p>Here the neural network inputs are the states $s$ and actions $a$ and the network parameterized by $\theta$ outputs a value $Q_\theta(s, a)$.</p>

<p>Then, in the same line, our objective would be be minimizing the MSE between the approximate action-value function $Q_\theta(s, a)$ and $Q_\pi(s,a)$. The idea is basically the same as above:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q_\pi(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q_\pi(s,a) - Q_\theta(s,a))^2]\]

<p>Then computing the gradient and updating in SGD fashion:</p>

\[\theta_{i+1} = \theta_i + \alpha (Q_\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>where since we don’t know $Q_\pi(s,a)$, we use approximates such as:</p>

<ul>
  <li>
    <p>MC method:</p>

\[\theta_{i+1} = \theta_i + \alpha (G_i - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]
  </li>
  <li>
    <p>TD(0) learning</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1}+\gamma Q_\theta(s_{i+1},a_{i+1}) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Both of the function approximation methods only approximates $Q_\pi(s,a)$ or $V_\pi(s)$ for some $\pi$. It <strong>does not improve $\pi$</strong>, hence it is purely doing the evaluation step.</p>
</blockquote>

<h2 id="model-free-methods">Model-Free Methods</h2>

<p>Challenges of function approximation in the reinforcement learning setting include that</p>

<ul>
  <li>the agent’s experience is <strong>not independent and identically distributed</strong> (IID)</li>
  <li>the agent’s policy affects the future data it will sample</li>
  <li>the environment may change, i.e. our <strong>target is moving</strong> (hence certain NN should not be used as they assume stationary target)</li>
  <li>may not converge (e.g. biased approximations)</li>
</ul>

<p>And this section as well as the following are more models that help overcome those challenges. Here, we discuss model-free methods.</p>

<p>Model-free approaches may be divided into</p>

<ol>
  <li>value-based or Q-leaning methods such as NFQ (Riedmiller 2005) and DQN (Mnih et al. 2015)</li>
  <li>policy-based or policy optimization methods such as PPO (Schulman et al. 2017), and</li>
  <li>actor-critic methods such as DDPG (Lillicrap et al. 2016) which are a combination of both (i) and (ii).</li>
</ol>

<h3 id="experience-replay">Experience Replay</h3>

<p>In supervised learning the training examples may be sampled independently from an underlying distribution, but here data are corelated in time as we are taking actions.</p>

<blockquote>
  <p>A solution to this problem, known as experience replay, is to use a replay buffer that stores a collection of previous states, actions, and rewards, specifically storing tuples of $(s, a, r, s’)$</p>

  <p>Then each saved experience tuple may be <strong>sampled</strong> and used for updating the network weights.</p>
</blockquote>

<p>This means an experience tuple may be used multiple times, which is an efficient use of the data.</p>

<h3 id="neural-fitted-q-learning">Neural Fitted Q-Learning</h3>

<p>Recall that in value approximation we are only approximating $Q_\pi(s,a)$ for some $\pi$. In order to <em>*directly find $\pi^</em>$<strong>, we can use the idea from **Q-learning</strong> which directly finds $\pi^*$:</p>

\[Q(s_t, a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{r+1} + \gamma \max_a Q(s_{t+1},a)-Q(s_t,a_t) ]\]

<p>Therefore, the <strong>analogy</strong> here is to do, similarly:</p>

\[J(\theta) = \frac{1}{2}\sum_{s \in S} \mu(s)\cdot (Q^*(s,a) - Q_\theta(s,a))^2 = \frac{1}{2}\mathbb{E}_s[(Q^*(s,a) - Q_\theta(s,a))^2]\iff J(\theta) = \frac{1}{2} \mathbb{E}[(y - \hat{y})^2]\]

<p>where the update rule is the same using SGD:</p>

\[\theta_{i+1} = \theta_i + \alpha (Q^*\pi(s_i,a_i) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>finally <strong>approximating $Q^*$</strong> using the update from Q-learning:</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q(s_{i+1},a) - Q_\theta(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<blockquote>
  <p>Q-learning diverges when using a neural network since there are correlations between the samples and the target is non-stationary.</p>

  <p>Therefore, to remove the correlations between samples we may generate a data set from the agent’s experience and use it as a <strong>supervised training</strong></p>
</blockquote>

<p>Then the algorithm becomes:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220302001029747.png" alt="image-20220302001029747" style="zoom:67%;" /></p>

<p>where essentially:</p>

<ul>
  <li>the pocliy we used to explore the world is just using $Q_\theta$, which is just a <strong>forward pass of the NN</strong></li>
  <li>putting this in a ==supervised== fashion, labels become $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$.</li>
  <li>notice that states and rewards are generated by the environment and therefore the algorithm is <strong>model free</strong></li>
</ul>

<h3 id="deep-q-network">Deep Q-Network</h3>

<blockquote>
  <p>Deep Q-networks (DQN) (Mnih et al. 2015) build upon fitted Q-learning by incorporating a replay buffer and a <strong>second target neural network</strong>.</p>
</blockquote>

<p>In NFG we consider labels being $y_i = R_{i+1} + \gamma \max_a Q(s_{i+1},a)$, but here we consider <strong>another network with $\theta-$</strong> such that:</p>

\[y_i = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a)\]

<p>with $\theta-$ being the <strong>target network</strong>. We uses this by making it appear static as compared to the ever changing $Q_\theta$ so that:</p>

\[\mathcal{L}(\theta_i) = \mathbb{E}_{(s,a,r,s')\sim D}[(y_i - Q_{\theta_i}(s_i,a_i))^2] = \mathbb{E}[(R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i))^2]\]

<p>then doing SGD the gradient update step is:</p>

\[\theta_{i+1} = \theta_i + \alpha (R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) - Q_{\theta}(s_i,a_i)) \nabla_\theta Q_\theta(s_i,a_i)\]

<p>Then the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220302003105418.png" alt="" /></p>

<p>where notice that:</p>

<ul>
  <li>since we have a target $\hat{Q}$ that is updated much less frequently, this is helping the problem of chasing a non-stationary target as in NFQ. The label $y_i$ becomes dependent on $\hat{Q}<em>{\theta-}$ rather than $Q</em>\theta$</li>
  <li>notice that states and rewards are <strong>generated from $Q_\theta$</strong>, but policy learnt is <strong>based on $Q_{\theta-}$</strong>. Hence this is <strong>off-policy</strong> because the action value function learnt is towards a different policy that generated the data.</li>
</ul>

<h3 id="prioritized-replay">Prioritized Replay</h3>

<p>Before we are sampling from the replay buffer uniformly, prioritized experience replay (Schaul, Quan, Antonoglou &amp; Silver 2016) samples <strong>important transitions more frequently</strong> which results in more efficient learning.</p>

<blockquote>
  <p>We define more important transitions as the ones we <strong>made the large DQN error</strong></p>

\[\text{Err}(s_i,a_i) = R_{i+1} + \gamma \max_a Q_{\theta-}(s_{i+1},a) -Q_{\theta_i}(s_i,a_i)\]

</blockquote>

<p>Then, we prioritize them by:</p>

\[\text{Priority}(s_i,a_i)=\frac{p_i^\alpha}{\sum_j p_j^\alpha}\]

<p>where $p_i$ would be proportional to DQN error and $\alpha$ is hyper-parameter controlling the amount of prioritization</p>

<ul>
  <li>with $\alpha=0$ you have no prioritization and gets back uniform sampling</li>
</ul>

<h2 id="policy-based-methods">Policy-based Methods</h2>

<p>One problem with value based methods in the previous chapter such as NFQ and QDN cannot <em>easily deal with</em> <strong>continuous actions</strong>. Since we only know/approximated $Q^*(s,a)$, to convert that to action we need to do something like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Value Function</th>
      <th style="text-align: center">Policy/Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304205646509.png" alt="image-20220304205646509" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304205627824.png" alt="image-20220304205627824" /></td>
    </tr>
  </tbody>
</table>

<p>but imagining those cells being <strong>infinitesimally small and infinitesimally many</strong> so that the action space is continuous. This will be a headache to optimize over.</p>

<p>Then, the idea is why not <strong>approximate policy itself as a continuous function</strong>?</p>

<blockquote>
  <p><strong>Policy-based methods</strong> work well in continuous spaces for <strong>learning stochastic policies</strong>, the most common example would be having:</p>

\[a \sim \mathcal{N}(\mu_\theta(s), \sigma^2_\theta(s))\]

  <p>where we learn $\theta$ from NN.</p>
</blockquote>

<p>First, we present the general setup of obtaining the objective function and its gradients.</p>

<p>Consider starting with some policy $\pi_\theta$ (e.g. randomly initialized):</p>

<ol>
  <li>
    <p>an agent then can interact wit the environment to generate (probabilistically) a <strong>trajectory of state-action-reward episodes</strong>:</p>

\[\tau = s_0,a_0,r_0,....,s_t,a_t,r_t\]

    <p>and hence obtain a <strong>return $g(\tau)$</strong>:</p>

\[g(\tau) = \sum_t \gamma^t r_t\]
  </li>
  <li>
    <p>Then the <strong>value of this policy</strong> would be the the <strong>expected/average return</strong> over all trajectories (since the policy is probabilistic):</p>

\[\mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right]\]
  </li>
  <li>
    <p>Therefore, naturally we say that the <strong>best policy $\pi_\theta$ would have the highest value</strong>:</p>

\[\max_\theta \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \gamma^t r_t \right] \equiv \max_\theta J(\pi_\theta)\]
  </li>
</ol>

<p>Now, the we can basically do <strong>gradient ascent</strong> by considering:</p>

\[\theta = \theta + \alpha \nabla_\theta J(\pi_\theta)\]

<h3 id="policy-gradient">Policy Gradient</h3>

<p>The final step is to analytically find the gradient $\nabla_\theta J$. We will derive everything exactly and then replace the integral with the practical sum:</p>

\[J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau)] = \int g(\tau) p_\theta(\tau) d\tau \iff \sum_\tau p_\theta(\tau)g(\tau)\]

<p>but notice that since $\tau$ is a <strong>trajectory generated by $\pi_\theta$</strong>, we can further find:</p>

\[p_\theta(\tau) = \prod_t p(s_{t+1}|s_t,a_t)\pi_\theta(a_t|s_t)\]

<p>Now, consider taking the derivative:</p>

\[\nabla_\theta J = \int \nabla_\theta[g(\tau) p_\theta(\tau)]d\tau  =\int g(\tau)\nabla_\theta p_\theta(\tau) d\tau\]

<p>Now we <em>can</em> plugin the expression for $p_\theta(\tau)$, but we can <strong>use a trick to remove the product</strong> by considering:</p>

\[\nabla_\theta \log p_\theta(\tau) = \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)}\]

<p>hence:</p>

\[\nabla_\theta J =\int g(\tau)p_\theta(\tau)\nabla_\theta \log p_\theta(\tau) d\tau = \mathbb{E}_{\tau \sim \pi_\theta}[g(\tau) \nabla_\theta \log p_\theta(\tau)]\]

<p>Now we substitute in the $p_\theta(\tau)$:</p>

\[\nabla_\theta \log p_\theta(\tau) = \nabla_\theta \sum_t[\log [(s'|s,a) + \log \pi_\theta(a|s)]] =  \sum_t\nabla_\theta \log  \pi_\theta(a|s)\]

<p>so we get finally:</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]\]

<p>which we can then <strong>estimate expected value using sums</strong> in practice.</p>

<h3 id="reinforce-algorithm">REINFORCE Algorithm</h3>

<blockquote>
  <p>The REINFORCE algorithm estimates the policy gradient numerically by Monte- Carlo sampling: using <strong>random samples</strong> to <strong>approximate the policy gradient</strong>.</p>
</blockquote>

<p>The idea is simple:</p>

<ol>
  <li>set $\nabla_\theta J=0$ for the start. We are treating each episode IID.</li>
  <li>sample a trajectory $\tau$</li>
  <li>compute $\nabla_\theta J = \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a\vert s)$ in this case</li>
  <li>ascent and repeat</li>
</ol>

<p>Then the algorithm is</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304214257499.png" alt="image-20220304214257499" style="zoom:67%;" /></p>

<p>however, some problems with this algorithm is that:</p>

<ul>
  <li>updating $\theta$ ==will change $\pi_\theta$==, which will be needed to ==generate $\tau$, the experiences==. Hence there <strong>might be convergence problems</strong>.</li>
  <li>additionally, the above also means this policy gradient have <strong>high variance</strong></li>
</ul>

<h3 id="score-function-generality">Score Function Generality</h3>

<p>Additionally, the general form of:</p>

\[g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \to g_t(\tau)\cdot \text{score}(\pi_\theta)\]

<p>and the $\text{score}$ expression appears in <strong>many places when you do gradient</strong>. Example include:</p>

<ul>
  <li>Q actor-critic: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot V_t]$</li>
  <li>TD: $\nabla_\theta J = \mathbb{E}_\pi [\text{score}\cdot \delta]$</li>
  <li>etc</li>
</ul>

<h3 id="policy-gradient-baseline">Policy Gradient Baseline</h3>

<p>The estimate of a gradient over <strong>many sampled experiences $\tau$</strong> would be:</p>

\[\nabla_\theta J  \approx \hat{g} = \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)})\]

<p>but notice that if we consider a constant/variable $b$ that is <strong>independent on $\theta$</strong>:</p>

\[\frac{1}{n}\sum_{i=1}^n (g(\tau^{(i)}) - b) \nabla_\theta \log p_\theta(\tau^{(i)}) =  \frac{1}{n}\sum_{i=1}^n g(\tau^{(i)}) \nabla_\theta \log p_\theta(\tau^{(i)}) - \frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)})\]

<p>but notice that for $\lim n\to \infty$</p>

\[\frac{1}{n}\sum_{i=1}^n b\nabla_\theta \log p_\theta(\tau^{(i)}) \to \sum_\tau b \nabla_\theta \log p_\theta  (\tau)\cdot p_\theta(\tau)\]

<p>and we can show thiat this is zero:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304215546064.png" alt="image-20220304215546064" style="zoom:67%;" /></p>

<p>so to measure performance we will usually <strong>subtract of some baseline $b$</strong> in the model, which is still an unbiased model.</p>

<h2 id="actor-critic-methods">Actor-Critic Methods</h2>

<p>On problem with policy-based method is that it updates $\theta$ whose $\tau$ sampled next depends on. This means that there could be <strong>high variance</strong> in the gradient which might cause <strong>convergence problems</strong>. The idea of this chapter is to ==solve== this using ==value function approximation to reduce this variance==.</p>

<blockquote>
  <p>Actor-critic methods <strong>combine policy-based methods</strong> with <strong>value-based methods</strong> by using both the policy gradient and value function.</p>
</blockquote>

<p>The basic step up will be a GAN like model:</p>

<ul>
  <li><strong>actor</strong>: a policy network $\pi_\theta$ with parameters $\theta$</li>
  <li><strong>critic</strong>: a value network that contains $V_\phi(s)$ or $Q_\phi(s,a)$, or the advantage function $A_\phi(s,a)=g(\tau) - V_\phi(s)$</li>
  <li><strong>critic provides a loss function for the actor</strong> and the gradients backpropagate from the critic to the actor</li>
</ul>

<p>Since $g(\tau)$ changes every time we sampled. the idea is we <strong>swap out $g(\tau)$</strong> in the update rule:</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t g_t(\tau) \nabla_\theta \log  \pi_\theta(a|s) \right]\]

<p>for values such as:</p>

<ul>
  <li>$g(\tau) \to Q_\phi(s,a)$, which leads to <strong>Q-value actor-critic</strong></li>
  <li>$g(\tau) \to A_\phi(s,a) = g(\tau) - V_\phi(s)$ for advantage</li>
</ul>

<p>for some <strong>already fitted value functions</strong>.</p>

<p>Therefore, this means the algorithm looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304220900366.png" alt="image-20220304220900366" style="zoom:67%;" /></p>

<p>where the:</p>

<ul>
  <li>the yellow part updates the critic parameter, and the red updates actor</li>
</ul>

<h3 id="advantage-actor-critic">Advantage Actor-Critic</h3>

<p>Here we essentially consider swapping out $g(\tau)$ for $A_\phi(s,a)$ defined by:</p>

\[A_\phi(s,a) = \mathbb{E}_{r,s'}[r+ \gamma V_{\pi_\theta}(s')  - V_{\pi_\theta}(s)]\]

<p>which can be <strong>estimated by TD error</strong> $r+\gamma V_\phi(s’)-V_\phi(s)$, hence resulting in the objective being</p>

\[\nabla_\theta J = \mathbb{E}_{\tau \sim \pi_\theta}\left[ \sum_t \nabla_\theta \log  \pi_\theta(a|s) \cdot \gamma^{t-1}(r+\gamma V_\phi(s')-V_\phi(s)\right]\]

<h3 id="natural-policy-gradient">Natural Policy Gradient</h3>

<p>In reinforcement learning the data set collected depends on the policy, which has the following <strong>risk</strong>:</p>

<ul>
  <li>if updated parameters result in a poor policy</li>
  <li>will result in poor samples</li>
  <li>algorithm stuck at poor policy</li>
</ul>

<blockquote>
  <p>Therefore, when optimizing policy-based methods <strong>choosing a step size</strong>/learning rate for updating the policy parameters is (one of the) key.</p>
</blockquote>

<p>In general, we had:</p>

\[\theta ' =\theta \gets \theta + \alpha \nabla_\theta J(\pi_\theta) = \theta + \Delta \theta\]

<p>one idea is to <strong>constrain the size of this change of $J(\pi_\theta)$</strong> by considering Taylor expansion and <strong>restricting the latter term</strong></p>

\[J(\pi_{\theta'}) \approx J(\pi_\theta) + \nabla_\theta J(\pi_\theta)^T \Delta \theta\]

<p>with</p>

\[\max_{\theta'} \nabla_\theta J(\pi_\theta)^T \Delta \theta \quad \text{s.t.}\quad ||\Delta \theta||_2^2 \le \epsilon\]

<p>in our case $\Delta \theta = \alpha \nabla_\theta J(\pi_\theta)$. So this can be solved analytically with:\</p>

\[\Delta \theta = \sqrt{2 \epsilon} \frac{\nabla_\theta J(\pi_\theta)}{||\nabla_\theta J(\pi_\theta)||}\]

<p><strong>Alternatively</strong>, we can directly <strong>instead of constraining $\theta$, which constrains $\pi_\theta$</strong>, we can constrain the <strong>episode trajectory</strong> itself. Since the trajectory is essentially a probability distribution, we can use KL divergence can constraint that:</p>

\[D_{KL}[p(\tau|\pi_\theta)\, |\, p(\tau |  \pi_{\theta'})] \le \epsilon\]

<p>Then this result in</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304223202945.png" style="zoom: 67%;" /></p>

<p>which again can be solved analytically, and this called the <strong>natural gradient</strong></p>

\[\Delta \theta =F^{-1}_\theta \nabla_\theta J(\pi_\theta)\sqrt{\frac{2\epsilon}{||\nabla_\theta J(\pi_\theta)^T F_\theta^{-1}\nabla_\theta J(\pi_\theta)||}}\]

<p>and $\nabla_\theta J(\pi_\theta)$ and $F_\theta$ may be approximated by sampling trajectories using conjugate gradient descent</p>

<blockquote>
  <p><strong>Note</strong> that since we are only dealing with $\Delta \theta$ for the first term in approximation, we are essentially using <strong>first order methods</strong>.</p>
</blockquote>

<h3 id="trust-region-policy-optimization-trpo">Trust Region Policy Optimization (TRPO)</h3>

<p>Essentially a method based on the natural gradient, but here we are:</p>

<ul>
  <li>using second order method but <strong>approximating the hessian</strong> by using <strong>conjugate gradient</strong></li>
  <li>alike NPG, constrain the surrogate loss by the <strong>KL divergence</strong> between the new and old policy</li>
</ul>

<p>This results in the following algorithm</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135351156.png" alt="image-20220303135351156" style="zoom: 50%;" /></p>

<p>where surrogate loss is basically the loss that is caused by different policy trajectories.</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135530009.png" alt="image-20220303135530009" style="zoom:50%;" /></p>

<h3 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h3>

<p>This method is based on TRPO however:</p>

<ul>
  <li>is a first-order method that avoids computing the Hessian matrix</li>
  <li>also avoids line search (at the end of previous algo) by <strong>clipping</strong> the surrogate objective.</li>
</ul>

<p>This results in the constrained optimization or surrogate objective:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303135626037.png" alt="image-20220303135626037" style="zoom:50%;" /></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224444380.png" alt="image-20220304224444380" style="zoom:67%;" /></p>

<p>Then the algorithm looks like</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224501324.png" alt="image-20220304224501324" style="zoom:67%;" /></p>

<h3 id="deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)</h3>

<p>DDPG may be used in continuous action spaces and combines <strong>DQN</strong> with <strong>REINFORCE</strong>. Essentially the idea is again, the actor critic loop:</p>

<ul>
  <li>use DQN to estimate $Q_\phi(s,a)$ which will be a <strong>critic</strong></li>
  <li>use REINFORCE to estimate $\pi_\theta(s)$ which will be an <strong>actor</strong></li>
</ul>

<p>The loss and gradients are defined by:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Critic Loss</th>
      <th style="text-align: center">Actor Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224806600.png" alt="image-20220304224806600" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224852363.png" alt="image-20220304224852363" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>and</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224824803.png" alt="image-20220304224824803" style="zoom: 67%;" /></p>

<p>The algorithm being</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304224921793.png" alt="image-20220304224921793" style="zoom:67%;" /></p>

<h2 id="model-based-reinforcement-learning">Model-based Reinforcement Learning</h2>

<p>Here, model-based reinforcement learning means we are <strong>learning the environment model</strong>, e.g. the transitions, to give us optimal policies. This can further be split into to kinds:</p>

<ul>
  <li>given model/environment: AlphaZero</li>
  <li>learn model by sampling: World Model</li>
</ul>

<h3 id="monte-carlo-tree-search">Monte-Carlo Tree Search</h3>

<blockquote>
  <p>A tree search starts at the root and explores nodes from there, <strong>looking for one particular node</strong> that satisfies the conditions mentioned in the problem.</p>
</blockquote>

<p>Tree search has been used in cases such as board games, where we essentially want to do the best next move when we have a selection of possible moves $A$ and a selection of states $S$ to be in:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303140820378.png" alt="image-20220303140820378" style="zoom:50%;" /></p>

<p>However, solving this exactly <strong>would need time complexity</strong> of:</p>

\[O\left((|S||A|)^d\right)\]

<p>which is <strong>exponential in time</strong>. Hence, the idea for MC Tree Search is to <strong>sample some branch only</strong> to keep the exploration going.</p>

<blockquote>
  <p>Monte-Carlo Tree Search (MCTS) runs simulations from a given state and therefore has time complexity of $O(nd)$ where $n$ is the number of simulations and $d$ the tree depth.</p>
</blockquote>

<p>Then, once done, we <strong>select actions based on Upper Confidence Bound</strong>:</p>

\[Q(s,a) + c \sqrt{\frac{\log N(s)}{N(s,a)}}\]

<p>for $c$ being an exploration constant and</p>

<ul>
  <li>$N(s,a)$ being the <strong>number of action state pairs</strong></li>
  <li>$N(s) = \sum_a N(s,a)$ is the number of <strong>state visits</strong></li>
</ul>

<p>So we get the algorithm being:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220304230357312.png" alt="image-20220304230357312" style="zoom:67%;" /></p>

<h3 id="expert-iteration-and-alphazero">Expert Iteration and AlphaZero</h3>

<blockquote>
  <p>Expert iteration, or AlphaZero, uses</p>

  <ul>
    <li>a neural network to output a policy approximation $\pi_\theta(a\vert s)$ and state value function $V_\phi(s)$ approximation for guiding MCTS.</li>
    <li>use MCTS to output next best action</li>
  </ul>

  <p>Once dones, this is <strong>merged</strong> into a single network $f_\theta(s)$ that:</p>

  <ul>
    <li>receives a state representation as input $s$</li>
    <li>outputs a vector of probabilities <strong>$p_\theta = P(a\vert s)$ over all valid actions</strong> a and <strong>state values $V_\theta (s)$ over states $s$.</strong></li>
  </ul>

  <p>And AlphaZero learns these action probabilities and estimated values from <strong>games of self-play</strong></p>
</blockquote>

<p>The parameters $\theta$ are updated by stochastic gradient descent on the following loss function:</p>

\[L(\theta) = - \pi \log  p + (V- e)^2 + \alpha ||\theta||^2\]

<p>where:</p>

<ul>
  <li>the first term is a <strong>cross entropy loss</strong> between policy vector $p$ and search probabilities $\pi$ you have in reality</li>
  <li>the second term aims to <strong>minimize the difference</strong> between predicted performance $V$ and actual evaluation $e$</li>
  <li>the third is a <strong>regularization term</strong></li>
</ul>

<p>Then AlphaZero uses MCTS which is a stochastic search using upper confidence bound update rule of the action-value function</p>

\[U(s,a) = Q(s,a) + cP(a|s) \frac{\sqrt{N(s)}}{1+N(s,a)}\]

<p>where:</p>

<ul>
  <li>$N(s,a)$ is the number of times action $a$ was taken from state $s$</li>
  <li>$P(a\vert s)=p_\theta(a)$ is the NN output of probability taking action $a$ from state $s$</li>
</ul>

<p>Then at each step we take:</p>

<ol>
  <li>action $\arg\max_a U(s,a)$</li>
  <li>add this new state to a tree</li>
  <li>use MC Tree search on the new state and repeat</li>
</ol>

<p>Graphically we are doing</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303141302315.png" alt="image-20220303141302315" style="zoom: 67%;" /></p>

<h3 id="world-models">World Models</h3>

<blockquote>
  <p>A world model is a neural game simulator that uses a VAE and RNN to take action in an environment, which in turn <strong>can be used to model the world/game itself</strong>.</p>
</blockquote>

<p>The overall structure has been covered before</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220303142142395.png" alt="image-20220303142142395" style="zoom:50%;" /></p>

<p>where we essentially learnt $M$.</p>

<ul>
  <li>the VAE is trained on images from the environment learning a <strong>low dimension latent representation</strong> $z$ of state $s$.</li>
  <li>
    <p>RNN is trained on the VAE latent vectors $z_t$ through time <strong>predicting</strong> $p(z_{t+1}\vert a_t, z_t, h_t)$.</p>
  </li>
  <li>$z_t$ and RNN hidden vector $h_t$ are fed into a <strong>neural network controller</strong> which <strong>outputs an action</strong> that effects the environment</li>
</ul>

<p>In addition, the power of this model is that:</p>

<ul>
  <li>essentially the VAE addition could be used to generate and continue playing the game, because we can feed in from RNN and VAE can <strong>“reconstruct” the image</strong></li>
  <li>some difference between GAN and this model is that here we are <strong>learning/reconstructing the latent space $z$</strong>, instead of the real data itself directly.</li>
</ul>

<h2 id="imitation-learning">Imitation Learning</h2>

<p>Rather than learning from rewards, imitation learning <strong>learns from example demonstrations</strong> provided by an expert - ==behavior cloning==.</p>

<p>Therefore, we basically consider <strong>learning $\pi_\theta$</strong> that clones the <strong>expert demonstration $(s,a) \sim D$</strong>:</p>

\[\arg\max_\theta \sum_{(s,a) \in D} \log \pi_\theta(a|s)\]

<blockquote>
  <p>However, learning a policy like by imitation has a natural pitfall: if it <strong>encountered situations not well represented by the demonstrations</strong> it may perform poorly, and once encountered may not recover from cascading errors.</p>
</blockquote>

<p>Related models in this area include:</p>

<ul>
  <li>
    <p><strong>Dataset aggregation</strong> (DAgger) (Ross, Gordon &amp; Bagnell 2011) aims to <em>solve the problem of cascading errors by augmenting the data</em> with expert action labels of policy rollouts</p>
  </li>
  <li>
    <p><strong>Stochastic miximg iterative learning</strong> (SMILe) (Ross &amp; Bagnell 2010) trains a <em>new policy only on the augmented data</em> and then mixes the new policy with the previous policies</p>
  </li>
  <li>
    <p><strong>Generative adversarial imitation learning</strong> (GAIL) (Ho &amp; Ermon 2016) uses state and action examples $(s,a) \sim P_{real}$ from expert demonstrations as real samples for a discriminator in a GAN setting, and ask the generator to leanr $\pi_\theta(a\vert s)$:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308131653510.png" alt="image-20220308131653510" style="zoom: 67%;" /></p>

    <p>so that basically discriminators $D_\phi$ wants to distinguish the actions from expert demonstrations $(s,a) \sim P_{real}$ and the generated pair $(s,\pi_\theta(s))$.</p>
  </li>
  <li>
    <p><strong>Inverse reinforcement learning</strong> explicitly derives a <em>reward function from a set of expert demonstrations</em> and <em>uses that reward to learn an optimal policy</em></p>
  </li>
</ul>

<h2 id="maintaining-exploration">Maintaining Exploration</h2>

<p>As we know, if a model has $0$ exploration then it might <strong>not converge</strong> to the optimal solution as it will be stuck with greedy choices. Hence we have introduced $\epsilon$-greedy  approach that does random action with probability $\epsilon$.</p>

<p>Here, we introduce <strong>another way to promote exploration</strong>, which is to add some <strong>internal reward</strong> (i.e. not explicitly given from the environment) each time when a <strong>new state is explored</strong>.</p>

<ul>
  <li>this approach would also be useful for environments with <strong>sparse rewards</strong></li>
</ul>

<p>Related models include</p>

<ul>
  <li>
    <p><strong>Go-Explore</strong>: provide bonus rewards for novel states, you can encourage agents to explore more of the state space, even if they don’t receive any external reward from the environment.</p>

    <p>However,  one problem with these approaches is that they do a poor job at continuing to explore promising areas far away from the start state: <strong>Detachment</strong></p>

    <p><img src="https://www.alexirpan.com/public/go-explore/detachment.png" alt="Detachment diagram" style="zoom: 33%;" /></p>

    <p>And the proposed solution is to maintain a <strong>memory of previously visited novel states</strong>, so that:</p>

    <ol>
      <li>When learning, the agent first <em>randomly samples a previously visited state</em>, biased towards newer ones.</li>
      <li>It travels to that state, then <em>explores from that state</em>.</li>
    </ol>

    <p>Therefore, by chance we will <strong>eventually resample a state near the boundary of visited states</strong>, and from there it is easy to discover unvisited novel states</p>
  </li>
</ul>

<h2 id="multiagent-system">Multiagent System</h2>

<p>Here we consider essentially <strong>other agents/players</strong> in the system, which introduces the following changes:</p>

<ul>
  <li><strong>Environments</strong> that contain multiple agents
    <ul>
      <li>Agents act autonomously</li>
      <li>Outcome depends on actions of all agents</li>
      <li>Agents maximize their own reward</li>
    </ul>
  </li>
  <li><strong>Observation</strong>
    <ul>
      <li>Perfect information (i.e. your action decision can be made deterministically as you <em>already see/have everything you need</em>)
        <ul>
          <li>e.g. chess</li>
        </ul>
      </li>
      <li>Imperfect information (i.e. your action decision would be probabilistic because you <em>do not see/have opponent information yet</em>)
        <ul>
          <li>e.g. rock-paper-scissors</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Competitive - cooperative</strong>
    <ul>
      <li>Fully competitive zero-sum games: checkers, chess, Go</li>
      <li>Fully cooperative</li>
    </ul>
  </li>
</ul>

<h3 id="strategic-game">Strategic Game</h3>

<p>A matrix game (strategic game) can be described as follows</p>

<blockquote>
  <p>Formally, we have the following setup:</p>

  <ul>
    <li>$n$ agents, numbered from $i=1,…,n$</li>
    <li>at each step, we have some <strong>action profile $(a_1, …, a_n)$</strong> representing the action each agent $i$ took</li>
    <li>utility function for each agent $(u_1,…,u_n)$ measures the “utility” of their action $(a_1, …,a_n)$</li>
    <li>an outcome of the game is the joint action of all agents</li>
  </ul>

  <p>We <strong>assume</strong> that <strong>each agent maximizes its own utility over each outcome</strong></p>
</blockquote>

<p>Additionally, we can represent the games they play as a <strong>tree</strong>, for example:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308201011752.png" alt="image-20220308201011752" style="zoom:50%;" /></p>

<p>in this example:</p>

<ul>
  <li><strong>nodes</strong> are states, edges are actions, leaves are utilities, e.g. $(u_1,u_2)$</li>
  <li>each node is <strong>labeled with an agent</strong> that can control it, so edge from a node is the action the agent at the node takes</li>
  <li>each path to a leaf is a <strong>run</strong></li>
</ul>

<hr />

<p><em>Perfect Information Game Example</em></p>

<p>in a <strong>perfect information game</strong>, we can explore the state by <strong>not caring how you get to the state</strong></p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308133714063.png" alt="image-20220308133714063" style="zoom:50%;" /></p>

<p>therefore, the value of a state is based on the <strong>subtree</strong> where the current node is the root.</p>

<hr />

<p><em>Imperfect Information Game</em></p>

<p>Here, we do not have everything you need to determine your action to win 100%. An example would be <strong>rock-paper-scissor</strong>, which we can characterize by:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Matrix</th>
      <th style="text-align: center">Tree</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308204135128.png" alt="image-20220308204135128" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308204119834.png" alt="image-20220308204119834" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the situation is that, if you are player B on the right figure, you do not know what A chose and hence all three states look the same to you</p>

<ul>
  <li>
    <p>if either player has some deterministic strategy, the player would be <strong>exploited</strong>. Hence the optimal solution, or its ==Nash Equilibrium==, is to <strong>choose a random policy of $1/3$ for each action.</strong> This probabilistic policy is often referred to as <strong>mixed strategy</strong>, whereas the deterministic one is called <strong>pure strategy</strong>.</p>
  </li>
  <li>
    <p>This can also be seen as a <strong>zero-sum matrix game</strong>. When we say that it is a zero-sum game, we mean that one wins the same amount as the other loses.</p>
  </li>
</ul>

<h3 id="nash-equilibrium">Nash Equilibrium</h3>

<p>The idea of Nash Equilibrium in a game is a collection of <strong>strategies for all players</strong> such that <strong>no player can do better by changing its own strategy</strong> given that <strong>other players continue playing their NE strategies</strong></p>

<blockquote>
  <p>Formally, the Nash Equilibrium set of strategy for all $n$ players is $(\pi_1^<em>, …,\pi_n^</em>)$ such that</p>

\[V_i(\pi_1^*,...,\pi_i^*,...,\pi_n^*) \ge V_i(\pi_1^*,...,\pi_i,...,\pi_n^*) ,\quad i=1,...,n\]

  <p>where $V_i(\cdot)$ is player $i$’s value function which is player $i$’s <strong>expected reward/utility given all players’ strategies</strong></p>
</blockquote>

<p>It is important to know that:</p>

<ul>
  <li>
    <p>Clearly, the goal of converging to Nash equilibria is a good starting point for multi-agent learning algorithms. However, there commonly can be multiple Nash equilibria with different payouts, leading to greater difficulty in evaluating multiagent learning compared to single agent RL</p>
  </li>
  <li>
    <p>being in Nash Equilibrium therefore <strong>does not equal to maximal gain/utility</strong> attainable in a game, as it is <strong>assuming</strong> that all players want to maximize their <strong>own benefit</strong></p>

    <p>An example would be the following matrix game:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308205612393.png" alt="image-20220308205612393" style="zoom:67%;" /></p>

    <p>where the NE is for each player to <em>take</em>, as it maximizes their own benefit. However, the optimal solution is of course for both to <em>give</em>.</p>
  </li>
</ul>

<blockquote>
  <p>In fact:</p>

  <ul>
    <li>in <strong>Perfect information game</strong>, a <strong>unique Nash equilibrium/optimal</strong> is the value for player of current node in tree (as the value would be representative of all possible outcomes)</li>
    <li>in <strong>Imperfect information game</strong>, there may be <strong>multiple Nash equilibria</strong>, no deterministic strategy may be optimal.</li>
  </ul>
</blockquote>

<p><em>For Example</em>: Simple Nash Equilibrium</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308211944291.png" alt="image-20220308211944291" style="zoom:67%;" /></p>

<p>since the entire action space is shown, we see that:</p>

<ul>
  <li>“Top + Left” is a <strong>Nash Equilibrium</strong> because either can benefit themselves more if the other player stayed there. A can switch to Bottom, but this would reduce his payoff from $5$ to $1$. Player B can switch from left to right, but this would reduce his payoff from $4$ to $3$. “Bottom + Right” is <strong>also a Nash Equilibrium</strong> for the same reason</li>
  <li>other cells are not because one player will be gained more by shifting away.</li>
</ul>

<hr />

<p><em>For Example</em>: Mixed Nash Equilibrium Strategy</p>

<p>To produce mixed strategy, we <strong>must</strong> have at least one player going random policy, as otherwise the optimal solutoin will be determinstic as you can just exploit it.</p>

<p>Consider the example of <em>battle of sexes</em>, where the reward is shown in the table:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" /></p>

<p>We now <strong>suppose that</strong> woman go to the Baseball game with ==probability $p$,== and the Man go to the Baseball game with probability $q$. Then, we can look at the <strong>expected return</strong> for each party after choosing an action:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213426800.png" alt="image-20220308213426800" /></p>

<p>hence, notice that:</p>

<ul>
  <li>
    <p>A mixed strategy in the battle of the sexes game requires both parties to randomize</p>
  </li>
  <li>
    <p>for <strong>Man to randomize=indifference between going Baseball or Ballet</strong>, we need</p>

\[1+2p = 2-2p \quad \to \quad p=1/4\]

    <p>for woman</p>
  </li>
  <li>
    <p>for the <strong>Woman to randomize</strong>, the Woman must get equal payoffs from going to the Baseball game and going to the Ballet, which requires:</p>

\[2q = 3-2q \quad \to \quad q=3/4\]
  </li>
  <li>
    <p>notice that the <strong>above are independent decisions</strong>, hence we can pick $q=3/4$ ==and== $p=1/4$ to achieve ==Nash Equilibrium== so that the probability of man/woman going to each sport being given by:</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" /></p>

    <p>which is the result if man and woman following the NE.</p>
  </li>
</ul>

<p>Then, putting together the mixed strategy and the reward, we can compute the <strong>expected payoff</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Reward</th>
      <th style="text-align: center">Random Policy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213228803.png" alt="image-20220308213228803" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308213950680.png" alt="image-20220308213950680" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>basically we do element-wise multiplication for the two matrix and will get</p>

\[\mathbb{E}[\text{man reward}] = \frac{9}{16} + \frac{9}{16} + \frac{6}{16} + 0 = \frac{3}{2}\\
\mathbb{E}[\text{woman reward}] = \frac{6}{16} + \frac{9}{16} + \frac{9}{16} + 0 = \frac{3}{2}\\\]
  </li>
  <li>
    <p>the above Nash equilibrium (following $p,q$) is indeed <strong>smaller than if they had coordination</strong> to go both baseball or both ballet.</p>
  </li>
</ul>

<blockquote>
  <p>Here we have shown that <strong>randomization</strong> requires <strong>equality of expected payoffs</strong> (as otherwise it becomes deterministic), which would then result in Nash Equilibrium.</p>
</blockquote>

<hr />

<p><em>For Example</em>:</p>

<p>Consider the game of guessing coin flip:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308211127704.png" alt="image-20220308211127704" style="zoom: 50%;" /></p>

<p>essentially, if $P1$’s coin is guessed correctly by $P2$ then $P1$ receives a penalty. We want to **consider the Nash Equilibrium **</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Player 2 Guessing Head with $p=3/4$  and tail $p=1/4$</th>
      <th style="text-align: center">Player 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308232956782.png" alt="image-20220308232956782" /></td>
      <td style="text-align: center">Player 1 can always achieve 0 by always playing sell</td>
    </tr>
  </tbody>
</table>

<p>where the key observation for imperfect game is that:</p>

<ul>
  <li>for imperfect information game, you need to consider <strong>the full tree</strong> for possibility to determine your <strong>optimal strategy</strong></li>
</ul>

<h3 id="counterfactual-regret-minimization">Counterfactual Regret Minimization</h3>

<p>Key algorithm used in imperfect information games</p>

<ul>
  <li><strong>Counterfactual</strong>: If I had known</li>
  <li><strong>Regret</strong>: how much better would I have done if I did something else instead?</li>
  <li><strong>Minimization</strong>: what strategy minimizes my overall regret?</li>
</ul>

<p>Then a Monte Carlo CFR does:</p>

<ul>
  <li>Player 1 explores all options and player 2 samples actions</li>
  <li>Traverse entire tree</li>
  <li>Repeat switching roles: player 1 samples actions and player 2 explores all actions</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220308141226579.png" alt="image-20220308141226579" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>we know that player 1 played green, which gained 50. But the other options are to get 0 or 100. Hence the regret for them is -50 and 50.</li>
  <li>use neural network to approximate the tree, which <strong>guides the CFR search</strong></li>
</ul>

<h3 id="unification">Unification</h3>

<p>One idea is that perfect information can be seen as a special case of imperfect information:</p>

<ul>
  <li>realize that both basically includes a <strong>Tree Search + DNN</strong> to simulate the tree
    <ul>
      <li>perfect information does DNN + MC tree search</li>
      <li>imperfect information does DNN + MC-CFR</li>
    </ul>
  </li>
  <li>now a unified algorithm has come up which essentially can play both Chess, Go as well as games like Poker.</li>
</ul>

<h1 id="optional">Optional</h1>

<p>Some random stuff added in lecture notes</p>

<h2 id="adversarial-attacks">Adversarial Attacks</h2>

<p>The idea is that we <strong>add some noise</strong> in an image, such that the <strong>classification output</strong> would be ==different==</p>

<ul>
  <li>targeted</li>
  <li>untargeted</li>
</ul>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135858473.png" alt="image-20220217135858473" style="zoom:33%;" /></p>

<p>The idea is therefore to do</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217140014237.png" alt="image-20220217140014237" style="zoom:33%;" /></p>

<p>then</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220217135942981.png" alt="image-20220217135942981" style="zoom:33%;" /></p>

<p>which obviously requires you to know the network architecture in advance</p>

<h2 id="problem-solving-with-dl">Problem Solving with DL</h2>

<p>https://www.cs.columbia.edu/~idrori/drori2021math.pdf</p>

<blockquote>
  <p>We demonstrate that a neural network pre-trained on text <strong>and</strong> finetuned on code solves Mathematics problems by program synthesis.</p>

  <ul>
    <li>In this work we demonstrate that program synthesis is the key to solving math and STEM courses at scale, by turning questions into programming tasks</li>
  </ul>
</blockquote>

<p>Some of the previous interesting work</p>

<ul>
  <li>
    <p>When <strong>paired with graph neural networks (GNNs)</strong> to predict arithmetic expression trees, Transformers pre-trained on text have been used to solve university level problems in Machine Learning (14) with up to 95% accuracy.</p>

    <p>however this previous work is limited to numeric answers and isolated to a specific course and does not readily scale to other courses</p>
  </li>
  <li>
    <p>it works because <strong>humans solving problems by imaging them into computation trees</strong></p>
  </li>
</ul>

<hr />

<p>Here, the idea is to:</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220310205626517.png" alt="image-20220310205626517" /></p>

<ol>
  <li>turn question into programming task
    <ul>
      <li>add context (e.g. what course it is, e.g. use <code class="language-plaintext highlighter-rouge">numpy</code>)
        <ul>
          <li>assume that background knowledges from the available code repos on Github. e.g. if we are to solve a physics problem, it will assume physics knowledge from existing repos.</li>
        </ul>
      </li>
      <li>using codex to prompt codex (e.g. given a question, output a programming task)</li>
    </ul>
  </li>
  <li>automatically genreate program using a Transformer, OpenAI Codex, pre-trained on text and fine-tuned on code</li>
  <li>execute program to obtain and evluate answers</li>
  <li>automatically explain correct solution using codex</li>
</ol>

<blockquote>
  <p>The key takeaways is that:</p>

  <ul>
    <li>transformers pretrained on text does <strong>not directly work</strong> on solving math problems</li>
    <li>program synthesis using codex can assume “knowledge” of physics, math, etc. if there exists related code on Github</li>
  </ul>
</blockquote>

<p>But exactly what context is needed?</p>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220310210250349.png" alt="image-20220310210250349" /></p>

<p>In short we have mainly three kinds:</p>

<ul>
  <li><strong>Topic Context</strong>:  For example, without context, a question about networks may be about neural networks or communication networks, and therefore specifying the context is required.</li>
  <li><strong>Library Context</strong>: using the Python libraries sympy and streamplot are added for solving the question and plotting a visualization</li>
  <li><strong>Definitions Context</strong>: Often times, Codex doesn’t have real-world grounding for what certain terms are defined as. As an illustrative example, Codex lacks knowledge of what a “Full House” means in poker. Explicitly defining these in terms that Codex understands can better guide its program synthesis.</li>
</ul>

<p>Then, when transforming questions to Codex prompts, a key consideration is <strong>how semantically close the original question is to a prompt that produces a correct solution</strong>:</p>

<ul>
  <li>To measure the distance between the original questions and the successful prompts, we use cosine similarity between Sentence-BERT</li>
  <li>we can also run <strong>clusterings</strong> from thos embeddings to see how courses are interrelated.</li>
</ul>

<p>Finally, we can <strong>create new questions</strong> using Codex:</p>

<ul>
  <li>We use Codex to generate new questions for each course. This is done by creating a numbered list of questions from the dataset. This list is cut off after a random number of questions, and the result is used to <strong>prompt Codex to generate the next question</strong>.</li>
</ul>

<p><strong>More details on using Codex</strong></p>

<ul>
  <li>We use OpenAI’s davinci-codex engine for all of our generations. We fix all of Codex’s hyperparameters to be the same for all experiments: top-$p$ which is the portion p of the token probability mass a language model samples from at each step is set to 1, sampling temperature is set to 0 (i.e. argmax), and response length is set to 200 tokens.</li>
  <li>Both frequency and presence penalty are set to 0, and we do not halt on any stop sequences. Each prompt is structured as a Python documentation comment surrounded by triple quotations and line breaks.</li>
</ul>

<h1 id="tutorials">Tutorials</h1>

<p>Tutorials after classes</p>

<h2 id="remote-jupyter">Remote Jupyter</h2>

<p>Here are some notes on how to setup a remote jupyter server and connect to it locally using VSCode.</p>

<p>Firstly, on the <strong>remote</strong> (e.g. <code class="language-plaintext highlighter-rouge">ssh</code> into it)</p>

<ol>
  <li>
    <p>make sure you have already generated a <strong>settings file</strong> for you jupyter server. This should be located under <code class="language-plaintext highlighter-rouge">~/.jupyter/jupyter_notebook_config.py</code>. If not, you can generate it by:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook <span class="nt">--generate-config</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Make sure that you have the following two lines in your config file</p>

    <div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span>.<span class="n">NotebookApp</span>.<span class="n">allow_origin</span> = <span class="s1">'*'</span> <span class="c">#allow all origins
</span><span class="n">c</span>.<span class="n">NotebookApp</span>.<span class="n">ip</span> = <span class="s1">'0.0.0.0'</span> <span class="c"># listen on all IPs
</span></code></pre></div>    </div>
  </li>
  <li>
    <p>Start your jupyter server by doing (consider putting it into <code class="language-plaintext highlighter-rouge">tmux</code>):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>jupyter notebook <span class="nt">--no-browser</span> <span class="nt">--port</span><span class="o">=</span>9999
<span class="c"># some logs omitted</span>
<span class="o">[</span>I 13:00:56.602 NotebookApp] Jupyter Notebook 6.4.5 is running at:
<span class="o">[</span>I 13:00:56.602 NotebookApp] http://ip_of_your_server:9999/?token<span class="o">=</span>xxx
</code></pre></div>    </div>

    <p>take a note of the line <code class="language-plaintext highlighter-rouge">http://ip_of_your_server:9999/?token=xxx</code>, which will be used later</p>
  </li>
</ol>

<p>Then, on your <strong>local machine</strong>, open up VSCode and:</p>

<ol>
  <li>
    <p>connect to the remote jupyter server by putting in the url <code class="language-plaintext highlighter-rouge">http://ip_of_your_server:9999/?token=xxx</code> into the following popup</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223180806747.png" alt="image-20220223180806747" /></p>
  </li>
  <li>
    <p>Reload the window, and if successful, you will be able to see a <strong>remote kernel</strong> at the following pop up</p>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220223180957821.png" alt="image-20220223180957821" /></p>
  </li>
</ol>

<h2 id="pytorch">PyTorch</h2>

<p>In total there will be three parts of the tutorial.</p>

<ol>
  <li>Part I: introduction, compute derivatives, build a simple NN</li>
</ol>

<p>What is PyTorch? It’s a Python-based scientific computing package targeted at two sets of audiences:</p>

<ul>
  <li>A replacement for NumPy to use the power of GPUs</li>
  <li>a deep learning research platform that provides maximum flexibility and speed</li>
</ul>

<p>See the <code class="language-plaintext highlighter-rouge">tutorial/pytorch</code> for more details</p>

<h3 id="part-i">Part I</h3>

<p>Basically data structures are very similar to the <code class="language-plaintext highlighter-rouge">np.array</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span> <span class="c1"># again same as numpy
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="s">"""
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
"""</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>      <span class="c1"># new_* methods take in sizes
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="s">"""
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
"""</span>
<span class="c1"># Generates random values from **Normal distribution**
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>    <span class="c1"># override dtype!
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        <span class="c1"># result has the same size as x
</span><span class="s">"""
tensor([[-0.0351,  1.3674,  2.3571],
        [ 0.1267,  1.2764, -1.0151],
        [ 1.8572, -3.1128,  0.5551],
        [-0.6236,  0.6248,  0.1378],
        [-0.8379, -0.2349, -1.0931]])
"""</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># instead of x.shape
</span></code></pre></div></div>

<p>note that:</p>

<ul>
  <li>So, essentially, <code class="language-plaintext highlighter-rouge">new_ones</code> allows you to quickly create a ==new== <code class="language-plaintext highlighter-rouge">torch.Tensor</code> on the same device and data type ==from a <em>previously existing</em> tensor== (with ones), whereas <code class="language-plaintext highlighter-rouge">ones()</code> serves the purpose of creating a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> ==from scratch== (filled with ones).</li>
</ul>

<p>Some additional feature for arithmetic</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># normal way
</span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="n">q</span><span class="p">)</span>

<span class="c1"># method 2
</span><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> <span class="c1"># as a paramter
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># method 3
</span><span class="n">q</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="c1"># in-place, noitce the UNDERSCORE
</span><span class="k">print</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</code></pre></div></div>

<p>Common Operations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># slicing is the same as numpy
</span><span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># all rows, 1st column only
</span><span class="s">"""
tensor([[1.2144, 0.3950, 0.9391],
        [1.0756, 1.0725, 1.1850],
        [1.2121, 1.2862, 0.6270],
        [0.9586, 0.6927, 0.9081],
        [0.8100, 0.7620, 1.6299]])
tensor([0.3950, 1.0725, 1.2862, 0.6927, 0.7620])
"""</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="c1"># instead of reshape
</span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">z</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
<span class="s">"""
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
"""</span>

<span class="c1"># permute!
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">220</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 1th = 220, 2nd = 224, 0th = 3 
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<p>Conversion to list</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span> <span class="c1"># to a list
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">item</span><span class="p">())</span> <span class="c1"># gets the number, works for tensors with single value
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span> <span class="c1">#works for tensors with multiple values
# print(x.item()) # does not work with multiple dimension
</span></code></pre></div></div>

<h4 id="pytorch-and-numpy">PyTorch and Numpy</h4>

<p>PyTorch integrates very nice features with <code class="language-plaintext highlighter-rouge">numpy</code>. ==The Torch Tensor and NumPy array will share their underlying memory locations==, and changing one will change the other.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 1-d arrow
</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># converts to numpy
</span>
<span class="n">a</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># has to be inplace
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="s">""" both are changed
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
"""</span>
</code></pre></div></div>

<p>this works because they will be pointing to the <strong>same memory location</strong> (probably share a field).</p>

<p>The reverse ==also works==</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">a</span><span class="p">)</span> <span class="c1"># in-place
# below does not work because it returns a NEW ARRAY
# a = np.add(a,1) 
# a = a + 1
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="s">"""
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
"""</span>
</code></pre></div></div>

<h4 id="cuda-tensors">CUDA Tensors</h4>

<p>This basically will make your code runs faster</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
# Devices available are 'cuda' and 'cpu'
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>          <span class="c1"># a CUDA device object
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># directly create a tensor on GPU
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># or just use strings ``.to("cuda")``
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="part-2">Part 2</h3>

<p>Too tired, look at the notebook.</p>

<p>Basically you have <code class="language-plaintext highlighter-rouge">pytorch</code> <strong>automatically evaluated gradient</strong> by:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># gradient evaluated AT this value
# x.add_(1) # once requires_grad, you cannot do in-place operation
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span> <span class="c1"># only x will have `require_grad=True`
# y.retain_grad() # if you need the gradient of y as well
</span><span class="n">z</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="s">"""
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)
tensor(27., grad_fn=&lt;MeanBackward0&gt;)
"""</span>
</code></pre></div></div>

<p>Then, to evaluate gradient $\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1}$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Let's backprop now
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># not yet computed
</span><span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># computed
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="s">"""
None

tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
"""</span>
</code></pre></div></div>

<p>We have that $o = \frac{1}{4}\sum_i z_i= \frac{1}{4}\sum_i 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$. Therefore:</p>

\[\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)\]

<p>so evaluated at $x_i=1$:</p>

\[\quad \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5\]

<p>which is what we had.</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>Under the hood, the <code class="language-plaintext highlighter-rouge">grad</code> is done by keeping track of a computation graph behind all the operations.</p>
</blockquote>

<p>Then, to temporarily disable the gradient:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="s">"""
True
True
False
"""</span>
</code></pre></div></div>

<p>useful when the training is done.</p>

<h3 id="part-3">Part 3</h3>

<p>Now we create a NN for MNIST dataset.</p>

<p>First import a bunch:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Import dataset related API
</span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="c1"># Import common neural network API in pytorch
</span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># Import optimizer related API
</span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Check device, using gpu 0 if gpu exist else using cpu
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>We will be mainly dealing with MNIST:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Getting data
</span><span class="n">training_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># how dset works
</span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">cols</span><span class="p">,</span> <span class="n">rows</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)).</span><span class="n">item</span><span class="p">()</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span>
    <span class="n">figure</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span> <span class="c1">#.squeeze() to convert shape from (1, 28, 28) to (28, 28)
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>note that:</p>

<ul>
  <li>
    <p>the shape is <code class="language-plaintext highlighter-rouge">[28,28,1]</code> because <code class="language-plaintext highlighter-rouge">pytorch </code>set the first dimension to the <strong>channel dimension.</strong> We are using grey scale so it is $1$.</p>
  </li>
  <li>
    <p>squeeze can be quite dangerous if you do not know what you are doing:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div>    </div>

    <p>i.e. all dimension 1 will be removed if not specified</p>
  </li>
</ul>

<h4 id="preparing-data">Preparing Data</h4>

<p><strong>Preparing the data</strong></p>

<ul>
  <li>into batches and shuffle</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Important Library!!
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Sectioning dataset into batches using dataloaders
</span><span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="create-nn">Create NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining the neural network
</span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># define layers
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_fc_layers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_fc_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="c1"># W^1, notice that 784=28*28
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="c1"># W^2, input 128 dimension, output 64 dimension
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="c1"># W^3, input 64 dimension, output 10
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">layers</span>


    <span class="c1"># define forward function
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span> <span class="c1"># in this case, it will be (64, 784) where 64 is the batch_size you had
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># backward pass does not need to be specified because they are done automatically
</span>        

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span> <span class="c1"># Creating an instance of the Network class
</span><span class="n">net</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shifting to cuda if available*
</span>
<span class="c1"># Below is how the network looks
</span><span class="s">"""
Network(
  (layers): Sequential(
    (0): Linear(in_features=784, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=10, bias=True)
    (5): LogSoftmax(dim=1)
  )
)
"""</span>
<span class="c1"># Defining our Loss Function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Defining optimizer
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span> <span class="c1"># e.g. ADAM
</span></code></pre></div></div>

<h4 id="training-nn">Training NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Number of epochs
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

    <span class="c1"># Initialising statistics that we will be tracking across epochs
</span>    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="c1"># in batches
</span>        <span class="c1"># get the inputs; data is a list of [inputs, labels]
</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># loading onto cuda if available*
</span>        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># zero the parameter gradients: Clean the gradient caclulated in the previous iteration
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1">#reset all graidents to zero for each step as they accumulate over backprop
</span>
        <span class="c1"># forward + backward + optimize
</span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># reshapes [btach_size, 28, 28] to [batch_size, 784]
</span>
        <span class="c1"># 1. forward propagation
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># 2. backward propagation
</span>        <span class="c1"># Calculate gradient of matrix with requires_grad = True
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1">#computes dloss/dx for every parameter x which has requires_grad=True
</span>
        <span class="c1"># 3. Apply the gradient calculate from last step to the matrix
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># x += -lr * x.grad ie updates the weights of the parameters
</span>
        <span class="c1"># housekeeping
</span>        
        <span class="c1"># Adding loss to total loss
</span>        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Checking which output label has max probability
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Tracking number of correct predictions
</span>        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Calculating accuracy, epoch-time
</span>    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span> <span class="n">total_correct</span><span class="o">/</span><span class="n">total</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>    

    <span class="c1"># Printing out statistics
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Epoch no."</span><span class="p">,</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span> <span class="p">,</span><span class="s">"|accuracy: "</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span><span class="s">"%"</span><span class="p">,</span> <span class="s">"|total_loss: "</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="s">"| epoch_duration: "</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">end_time</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="s">"sec"</span><span class="p">)</span>
</code></pre></div></div>

<p>you basically have to specify the entire training steps.</p>

<h4 id="evaluating-nn">Evaluating NN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">correct</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Since, we're now evaluating, we don't have to track gradients now!
</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>

    <span class="c1"># everything here is similar to the train function, 
</span>    <span class="c1"># except we're not calculating loss and not preforming backprop
</span>    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># just forward propagatoin for computing Y_pred
</span>    <span class="c1"># no more backward updates
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">}</span><span class="s">%'</span><span class="p">)</span>    
</code></pre></div></div>

<p>notice that we asked <code class="language-plaintext highlighter-rouge">pytorch</code> not to track gradients any more.</p>

<h4 id="create-own-dset">Create Own Dset</h4>

<p>We can subclass <code class="language-plaintext highlighter-rouge">Dataset</code>. This gives us more control how the data is to be extracted from source (<strong>before training</strong>)</p>

<ul>
  <li>We can also perform some additional transformatios before returning the data</li>
  <li>will work natively with <code class="language-plaintext highlighter-rouge">DataLoader</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#CustomDataset is subclass of Dataset.
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="c1"># how many data points are there in your dset
</span>        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span> <span class="c1"># get one (x_train, y_train) sample
</span>        <span class="c1"># we can do transformation on the given input before it is returned
</span>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
    
<span class="c1">#Usage
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="tensorflow">Tensorflow</h2>

<p>Implemented in C++ in the back, so highly optimized in performance.</p>

<p>The basic components are <strong>tensors</strong>, which is a multi-dimensional arrays with a ==uniform type==. Basicallya generalized matrix.</p>

<ul>
  <li>
    <p>we also talk about ranks in a tensor: a rank is the same idea as “dimension”</p>

    <ul>
      <li>
        <p>rank 1 tensor for vector</p>
      </li>
      <li>
        <p>rank 2 for matrix</p>
      </li>
      <li>
        <p>rank 3 for rgb image</p>
      </li>
    </ul>
  </li>
  <li>
    <p>deep learning basically does <strong>computations using tensors</strong> that are very large.</p>

    <ul>
      <li>e.g. for NN, we can see the system as a graph and passing tensors across. Hence the name <strong>TensorFlow</strong></li>
    </ul>
  </li>
</ul>

<h3 id="basics">Basics</h3>

<p>In general, you should specify:</p>

<ul>
  <li>the size of the tensor</li>
  <li>the datatype</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">dtypes</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
    <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">dtypes</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="s">"""
(3, 4, 3)
&lt;dtype: 'float32'&gt;
"""</span>
</code></pre></div></div>

<p><strong>Integrated with <code class="language-plaintext highlighter-rouge">numpy</code></strong></p>

<ul>
  <li>yet there is no “in-place operation” in tensorflow.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">tf_tensor</span>
<span class="s">"""
&lt;tf.Tensor: shape=(3, 3), dtype=float64, numpy=
array([[42., 42., 42.],
       [42., 42., 42.],
       [42., 42., 42.]])&gt;
"""</span>

<span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf_tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="s">"""
array([[43., 43., 43.],
       [43., 43., 43.],
       [43., 43., 43.]])
"""</span>
</code></pre></div></div>

<p><strong>GPU Support</strong></p>

<ul>
  <li>TensorFlow is able to detect it automatically</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"GPU available and ready to use!"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"Unable to detect GPU"</span><span class="p">)</span>

        <span class="c1"># to run on some specific GPU
</span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"GPU"</span><span class="p">):</span> <span class="c1"># Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
        <span class="c1"># x.device specifies the device for operations on x 
</span>        <span class="k">assert</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">"GPU:0"</span><span class="p">)</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">"On GPU"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'GPU unavailable'</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="variables-and-constants">Variables and Constants</h4>

<p>Basically they are wrappers around Tensor object, but it is <strong>important because</strong></p>

<ul>
  <li>it allows ==in-place updates==</li>
  <li>they are automatically watched by <code class="language-plaintext highlighter-rouge">tf.GradientTape</code>, which does auto-differentiating
    <ul>
      <li>hence they are used for parameters such as <strong>weights in NN</strong></li>
    </ul>
  </li>
</ul>

<p>Then, to create variables and constants</p>

<ul>
  <li>variables are <strong>tensors</strong> that can be changed. They will be watched if you use <code class="language-plaintext highlighter-rouge">GradientTape</code></li>
  <li>constants are <strong>tensors</strong> that <em>cannot be updated</em>. They will <em>not be</em> watched automatically.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for creating a Variable, you need to provide an initial value
</span><span class="n">my_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]])</span>
<span class="n">my_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">)</span> <span class="c1"># so a wrapper
</span>
<span class="c1"># Variables can be all kinds of types, just like tensors
</span><span class="n">bool_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span>
<span class="n">complex_variable</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">5</span> <span class="o">+</span> <span class="mf">4j</span><span class="p">,</span> <span class="mi">6</span> <span class="o">+</span> <span class="mf">1j</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>In-place Updates</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="c1"># This will keep the same dtype, float32
</span><span class="n">a</span><span class="p">.</span><span class="n">assign</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"a:"</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="c1"># Not allowed as it resizes the variable:
</span><span class="n">a</span><span class="p">.</span><span class="n">assign</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Naming</strong></p>

<ul>
  <li>
    <p>helpful for you to know <strong>what this tensor is for</strong>, .e.g when reloading some model weights</p>
  </li>
  <li>
    <p>Variable names are preserved when saving and loading models.</p>

    <p>By default, variables in models will acquire unique variable names automatically, so you don’t need to assign them yourself unless you want to.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a and b; they will have the same name but will be backed by
# different tensors.
</span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"Tutorial"</span><span class="p">)</span>

<span class="c1"># A new variable with the same name, but different value
# Note that the scalar add is broadcast
</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">my_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"Tutorial"</span><span class="p">)</span>

<span class="c1"># These are elementwise-unequal, despite having the same name
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="gradients">Gradients</h4>

<p>Basically the same as PyTorch, so that you need to do something “special” to enable it to <strong>start memorizing the operations</strong></p>

<p>Consider the function we want to differentiate:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">z</span>
  
<span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">z</span><span class="o">=-</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># -12
</span></code></pre></div></div>

<p>then, suppose we want to compute:</p>

\[\left. \nabla f  \right|_{1,2,-4}\]

<p>Then:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># input needs to be tracked, hence using variables
</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">)]</span> 

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span> <span class="c1"># start tracking, i.e. a blank sheet of paper where each operatoin is recorded
</span>    <span class="n">r</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
  
<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="c1"># W.R.T. the params
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Gradients:  drdx: %0.2f, drdy: %0.2f, drdz: %0.2f"</span> <span class="o">%</span> 
      <span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grads</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">numpy</span><span class="p">()))</span>
<span class="s">"""
Gradients:  drdx: -4.00, drdy: -4.00, drdz: 3.00
"""</span>
</code></pre></div></div>

<p>notice that:</p>

<ul>
  <li>we used ` tape.gradient(r, params)<code class="language-plaintext highlighter-rouge"> to differentiate w.r.t. all the </code>params<code class="language-plaintext highlighter-rouge"> hence achieving the gradient. You can also compute $\partial f / \partial x$ by just passing in </code>tape.gradient(r, params[0])`</li>
  <li>you also <strong>cannot call it multiple times</strong></li>
</ul>

<p>Another example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>            <span class="c1"># track the weight      
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>            <span class="c1"># does not need if we do not diff w.r.t x
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">forward</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">x</span><span class="p">).</span><span class="n">numpy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="c1"># w.r.t the weight only
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Gradient with respect to w:  "</span><span class="p">,</span> <span class="n">grads</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="s">"""
Gradient with respect to w:   [[0.  0. ]
 [0.2 0.4]]
"""</span>
</code></pre></div></div>

<ul>
  <li>
    <p>note that if you have a <strong>constant Tensor</strong> that you want to watch:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
    <span class="n">g</span><span class="p">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># NEEDED!
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">dy_dx</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># Will compute to 6.0
</span><span class="k">print</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="training">Training</h3>

<p>The complete version.</p>

<p><strong>preparing data</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span> 
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="n">mpimg</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">timeit</span>


<span class="c1"># Download a dataset
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="s">"""
(60000, 28, 28)
(10000, 28, 28)
"""</span>

<span class="c1"># normalize the values to [0-1], hsuffle, and generates to batches
# returns an iterator
</span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)).</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1024</span><span class="p">).</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="n">test_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
    <span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)).</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># A quick example of iterating over a dataset object
</span><span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Batch Shape: "</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1">#  (32, 28, 28) where 32 is the batch size
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Training</strong></p>

<p>Now, in general there are two ways to establish and train a model</p>

<ul>
  <li>using <code class="language-plaintext highlighter-rouge">Sequential</code> - easier to use</li>
  <li>using <code class="language-plaintext highlighter-rouge">subclassing</code> API - more flexible</li>
</ul>

<p>Here we discuss the subclassing idea:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyCustomModel</span><span class="p">(</span><span class="n">Model</span><span class="p">):</span> <span class="c1"># inherits from tf.keras.Model
</span>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyCustomModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># define your network architecture
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()</span> <span class="c1"># Layer 1: flatten
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">dl_1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span> <span class="c1"># Layer 2: dense layer
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">dl_2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span> <span class="c1"># Layer 3: dense layer
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">d1_3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)</span> <span class="c1"># Layer 4: multiclass classification
</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># basically the FORWARD PASS
</span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">d1_3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyCustomModel</span><span class="p">()</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>  <span class="c1"># loss function
# what optimization method we use. i.e. HOW you want to update the weights
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">()</span>

<span class="c1"># For each epoch
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>

    <span class="c1"># For each batch of images and labels
</span>    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>

        <span class="c1"># Open a GradientTape.
</span>        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

            <span class="c1"># Forward pass
</span>            <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

            <span class="c1"># Calculate loss, done INSIDE the GradientTape since we want to do derivatives afterwards
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

        <span class="c1"># Backprop to calculate gradients
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>

        <span class="c1"># Gradient descent step, apply the `gradients` to `model.trainable_variables`
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="c1"># Calculate loss on the test data
</span>    <span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_ds</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss_on_batch</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_on_batch</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Epoch {}, Test loss: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)))</span>
    
<span class="s">"""
Epoch 1, Test loss: 0.320291668176651
Epoch 2, Test loss: 0.26357364654541016
Epoch 3, Test loss: 0.2302776277065277
Epoch 4, Test loss: 0.20258881151676178
Epoch 5, Test loss: 0.17977921664714813
"""</span>
</code></pre></div></div>

<h3 id="note">Note</h3>

<p>TensorFlow 2.x</p>

<ul>
  <li>uses <strong>eager execution by default</strong>: executes line by line, the same fashion as you wrote the code</li>
</ul>

<p>TensorFlow 1.x</p>

<ul>
  <li>
    <p>builds a graph from your code, and execute according to the graph</p>
  </li>
  <li>
    <p>Graph execution has the obvious advantage that, since you can track the dependency of operations, you can <strong>optimize the run by parallelization</strong></p>
    <ul>
      <li>this means Graph based execution can be faster when you do model training</li>
      <li>however, since the order of computation maybe different from your code, it would be ==hard to debug==</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model building
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">outputs</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s">"softmax"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> 

<span class="n">input_data</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>

<span class="c1"># Eager Execution, by default
</span><span class="n">eager_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Eager time:"</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">eager_model</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="mi">10000</span><span class="p">))</span>

<span class="c1">#Graph Execution 
</span><span class="n">graph_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">eager_model</span><span class="p">)</span> <span class="c1"># Wrap the model with tf.function 
</span><span class="k">print</span><span class="p">(</span><span class="s">"Graph time:"</span><span class="p">,</span> <span class="n">timeit</span><span class="p">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">graph_model</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">number</span><span class="o">=</span><span class="mi">10000</span><span class="p">))</span>
<span class="s">"""
Eager time: 21.913783847000104
Graph time: 6.057469765000064
"""</span>
</code></pre></div></div>

<h2 id="keras">Keras</h2>

<p>Keras has faster performance</p>

<ul>
  <li>backed by TensorFlow, so <code class="language-plaintext highlighter-rouge">Keras </code>is a <strong>high level API</strong></li>
  <li>easier to use than raw TensorFlow</li>
</ul>

<p>Since <code class="language-plaintext highlighter-rouge">keras</code> is like a subclass of <code class="language-plaintext highlighter-rouge">tensorflow</code>, there is no “basics” per se.</p>

<h3 id="sequential-model">Sequential Model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span> <span class="c1"># Dense layer is a normal NN layer
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>note that we imported <code class="language-plaintext highlighter-rouge">keras</code> from <code class="language-plaintext highlighter-rouge">tensorflow</code></p>

<p>We will use <code class="language-plaintext highlighter-rouge">Sequential</code>:</p>

<ul>
  <li>architecture is basically like a list</li>
  <li>output of previous will be input of next</li>
</ul>

<p>A Sequential model is <strong>not</strong> appropriate when:</p>

<ul>
  <li>Your model has multiple inputs or multiple outputs</li>
  <li>Any of your layers has multiple inputs or multiple outputs</li>
  <li>You need to do layer sharing</li>
  <li>You want non-linear topology (e.g. a residual connection, a multi-branch model)</li>
</ul>

<p>For dealing with these cases, Keras offers a Functional API as well. This handles non-linear topologies, shared layers and even multiple inputs or outputs.</p>

<ul>
  <li>Read more here: https://keras.io/guides/functional_api/</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">clear_session</span><span class="p">()</span>
</code></pre></div></div>

<blockquote>
  <p>More information on <code class="language-plaintext highlighter-rouge">Keras </code>layers: https://keras.io/api/layers/</p>

  <p>Look into <code class="language-plaintext highlighter-rouge">Keras </code>models: https://keras.io/api/models/</p>
</blockquote>

<p>Now let us <strong>prepare the data</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="s">"""
(60000, 28, 28)
(10000, 28, 28)
"""</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># "normalizing it"
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</code></pre></div></div>

<p>The four ==most important steps you will use==</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> <span class="c1"># define model
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)))</span> <span class="c1"># define architecture
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(...)</span> <span class="c1"># define optimizer
</span>
<span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(...)</span> <span class="c1"># fit
</span></code></pre></div></div>

<p>Let us build a 3 layer model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span> <span class="c1"># because input will be 28*28=724
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="c1"># add how we want to optimize the weights
</span><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="c1"># train
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                    <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="c1"># each weigth update occurs per 64 data
</span>                    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<p>where:</p>

<ul>
  <li>Keras optimizers: https://keras.io/api/optimizers/</li>
  <li>Keras Losses: https://keras.io/api/losses/</li>
</ul>

<p>the <code class="language-plaintext highlighter-rouge">model</code> and <code class="language-plaintext highlighter-rouge">history</code> variables are <strong>important</strong>.</p>

<ul>
  <li>
    <p>save/load/visualize the model</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'mnist_recognition.model'</span><span class="p">)</span> <span class="c1"># save
</span>   
<span class="n">loaded_model</span><span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">'mnist_recognition.model'</span><span class="p">)</span> <span class="c1"># load
</span>  
<span class="n">predictions</span> <span class="o">=</span> <span class="n">loaded_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span> <span class="c1"># predict
</span></code></pre></div>    </div>

    <p>visualize</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"mnist_model.png"</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>

    <p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211241339.png" alt="image-20220127211241339" style="zoom:67%;" /></p>
  </li>
  <li>
    <p>plotting model performance overtime</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Train'</span><span class="p">,</span> <span class="s">'Test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
  
<span class="c1"># Plot training &amp; validation loss values
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Train'</span><span class="p">,</span> <span class="s">'Test'</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Acc</th>
          <th style="text-align: center">Loss</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211406254.png" alt="image-20220127211406254" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211415578.png" alt="image-20220127211415578" /></td>
        </tr>
      </tbody>
    </table>

    <p>where it is clear that test performance has stopped improving pretty much after the 1st epoch</p>
  </li>
</ul>

<h3 id="callbacks">Callbacks</h3>

<p>Reading: https://keras.io/api/callbacks/</p>

<p>A callback is an object that can <strong>perform actions at various stages of training</strong> (e.g. at the start or end of an epoch, before or after a single batch, etc).</p>

<p>You can use callbacks to:</p>

<ul>
  <li>Write TensorBoard logs after every batch of training to monitor your metrics</li>
  <li>Periodically save your model to disk</li>
  <li>Do early stopping</li>
  <li>Get a view on internal states and statistics of a model during training</li>
  <li>…and more</li>
</ul>

<p>Common usages include:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s">'model.{epoch:02d}-{val_loss:.2f}.h5'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="s">'./logs'</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">my_callbacks</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="tensorboard">TensorBoard</h4>

<p>TensorBoard is a visualization tool provided with TensorFlow. This callback logs events for TensorBoard, including:</p>

<ul>
  <li>
    <p>Metrics summary plots</p>
  </li>
  <li>
    <p>Training graph visualization</p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>
      <p>The exclamation mark <code class="language-plaintext highlighter-rouge">!</code> is used for executing commands from the uderlying operating system; here is an example using WIndows <code class="language-plaintext highlighter-rouge">dir</code>:</p>

      <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="nb">dir</span>
<span class="c1"># result:
</span><span class="n">Volume</span> <span class="ow">in</span> <span class="n">drive</span> <span class="n">C</span> <span class="n">has</span> <span class="n">no</span> <span class="n">label</span><span class="p">.</span>
 <span class="n">Volume</span> <span class="n">Serial</span> <span class="n">Number</span> <span class="ow">is</span> <span class="mi">52</span><span class="n">EA</span><span class="o">-</span><span class="n">B90C</span>
  
 <span class="n">Directory</span> <span class="n">of</span> <span class="n">C</span><span class="p">:</span>\<span class="n">Users</span>\<span class="n">Root</span>
  
<span class="mi">27</span><span class="o">/</span><span class="mi">11</span><span class="o">/</span><span class="mi">2018</span>  <span class="mi">13</span><span class="p">:</span><span class="mi">08</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">.</span>
<span class="mi">27</span><span class="o">/</span><span class="mi">11</span><span class="o">/</span><span class="mi">2018</span>  <span class="mi">13</span><span class="p">:</span><span class="mi">08</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">..</span>
<span class="mi">23</span><span class="o">/</span><span class="mi">08</span><span class="o">/</span><span class="mi">2016</span>  <span class="mi">11</span><span class="p">:</span><span class="mi">00</span>             <span class="mi">2</span><span class="p">,</span><span class="mi">258</span> <span class="p">.</span><span class="n">adalcache</span>
<span class="mi">12</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">2016</span>  <span class="mi">18</span><span class="p">:</span><span class="mi">06</span>    <span class="o">&lt;</span><span class="n">DIR</span><span class="o">&gt;</span>          <span class="p">.</span><span class="n">anaconda</span>
</code></pre></div>      </div>

      <p>you can even do crazy things like</p>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">contents</span> <span class="o">=</span> <span class="err">!</span><span class="n">ls</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="k">print</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>
<span class="p">[</span><span class="s">'myproject.txt'</span><span class="p">]</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">directory</span> <span class="o">=</span> <span class="err">!</span><span class="n">pwd</span>
  
<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="k">print</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
<span class="p">[</span><span class="s">'/Users/jakevdp/notebooks/tmp/myproject'</span><span class="p">]</span>
</code></pre></div>      </div>
    </li>
    <li>
      <p>the <code class="language-plaintext highlighter-rouge">%</code> mark is for <strong>magic functions</strong></p>

      <ul>
        <li>
          <p>basically a way to provide richer output than simple CLI, but also allows for customization.</p>

          <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span> <span class="c1"># allows for magic functions in tensorboard
</span>    
<span class="c1"># do something
</span>    
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">logs</span><span class="o">/</span><span class="n">fit</span>
</code></pre></div>          </div>
        </li>
        <li>
          <p>Besides <code class="language-plaintext highlighter-rouge">%cd</code>, other available shell-like magic functions are <code class="language-plaintext highlighter-rouge">%cat</code>, <code class="language-plaintext highlighter-rouge">%cp</code>, <code class="language-plaintext highlighter-rouge">%env</code>, <code class="language-plaintext highlighter-rouge">%ls</code>, <code class="language-plaintext highlighter-rouge">%man</code>, <code class="language-plaintext highlighter-rouge">%mkdir</code>, <code class="language-plaintext highlighter-rouge">%more</code>, <code class="language-plaintext highlighter-rouge">%mv</code>, <code class="language-plaintext highlighter-rouge">%pwd</code>, <code class="language-plaintext highlighter-rouge">%rm</code>, and <code class="language-plaintext highlighter-rouge">%rmdir</code>, any of which can be used without the <code class="language-plaintext highlighter-rouge">%</code> sign if <code class="language-plaintext highlighter-rouge">automagic</code> is on. This makes it so that you can almost treat the IPython prompt as if it’s a normal shell:</p>

          <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">16</span><span class="p">]:</span> <span class="n">mkdir</span> <span class="n">tmp</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="n">ls</span>
<span class="n">myproject</span><span class="p">.</span><span class="n">txt</span>  <span class="n">tmp</span><span class="o">/</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">cp</span> <span class="n">myproject</span><span class="p">.</span><span class="n">txt</span> <span class="n">tmp</span><span class="o">/</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">ls</span> <span class="n">tmp</span>
<span class="n">myproject</span><span class="p">.</span><span class="n">txt</span>
                                                                                                                                                                                                                                                                                                                                                                                                                
<span class="n">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">rm</span> <span class="o">-</span><span class="n">r</span> <span class="n">tmp</span>
</code></pre></div>          </div>
        </li>
      </ul>
    </li>
  </ul>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
<span class="c1"># Clear any logs from previous runs
</span><span class="err">!</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="p">.</span><span class="o">/</span><span class="n">logs</span><span class="o">/</span> 
</code></pre></div></div>

<p>Then we configure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_dir</span><span class="o">=</span><span class="s">"logs/fit/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">)</span>
<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> 
          <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> 
          <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">])</span> <span class="c1"># specify callback
</span></code></pre></div></div>

<p>Finally:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">logs</span><span class="o">/</span><span class="n">fit</span>
</code></pre></div></div>

<p><img src="/lectures/images/2022-12-08-COMS4995_Deep_Learning_part1/image-20220127211824150.png" alt="image-20220127211824150" /></p>

<p>note that:</p>

<ul>
  <li>if you are using VSCode, you need to switch your cell output format to <code class="language-plaintext highlighter-rouge">text/html</code> so this thing renders correctly</li>
</ul>

<h4 id="early-stopping">Early Stopping</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">earlystopping_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="c1"># which field in `history` you are looking at
</span>    <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># if stays the same for 2 epochs, stop
</span>    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'auto'</span><span class="p">,</span> <span class="n">baseline</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Then using this callback</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>

<span class="n">history</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> 
            <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> 
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">,</span> <span class="n">earlystopping_callback</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="autokeras">AutoKeras</h3>

<p>An even <strong>higher level than <code class="language-plaintext highlighter-rouge">keras</code></strong></p>

<ul>
  <li>encapsulates even some basic preprocessing</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="kn">import</span> <span class="nn">autokeras</span> <span class="k">as</span> <span class="n">ak</span>
</code></pre></div></div>

<p>then the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 
</code></pre></div></div>

<p>finally, the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the image classifier.
# didn't mention what architecture or optimizer to use!
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">ak</span><span class="p">.</span><span class="n">ImageClassifier</span><span class="p">(</span><span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># max #models to try
</span>
<span class="c1"># Feed the image classifier with training data.
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Predict with the best model.
</span><span class="n">predicted_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">)</span>


<span class="c1"># Evaluate the best model with testing data.
</span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Validating</strong></p>

<ul>
  <li>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use <code class="language-plaintext highlighter-rouge">validation_split </code>to specify the percentage.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="c1"># Split the training data and use the last 15% as validation data.
</span>    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Predict with the best model.
</span><span class="n">predicted_y</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predicted_y</span><span class="p">)</span>


<span class="c1"># Evaluate the best model with testing data.
</span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="cnn-with-tf">CNN with TF</h2>

<p>For dense neural network it actually works on MNIST</p>

<ul>
  <li>because the features that are encoded in the images are <strong>easy representations</strong></li>
</ul>

<p>Then if we check this on CIFAR10</p>

<ul>
  <li>simple NN doesn’t work anymore as the feature “extraction from previous layers” is not very representative</li>
</ul>

<p>Remember to use transfer learning for using those models</p>

<ul>
  <li>i.e. train on the last layer</li>
  <li>e.g. use VGG16 on tumor classification</li>
</ul>

<h2 id="gnn-withdglai">GNN with<code class="language-plaintext highlighter-rouge">dgl.ai</code></h2>

<p><code class="language-plaintext highlighter-rouge">dgl.ai</code> (deep graph library), a framework that allows us to implement, experiment, and run graph machine learning techniques. The tutorial will explore how nodes and edges are represented as well as features of edges and nodes</p>

<h2 id="pyro">Pyro</h2>

<p>A framework with pytorch as backend to do probablistic programming.</p>

<p>The idea is that you can do <strong>probability computations</strong> such as $P(x\vert z)$ by sampling $z$ from some prior distirbution of your choice.</p>

<h2 id="gan">GAN</h2>

<p>Basically read the following link https://www.tensorflow.org/tutorials/generative/dcgan, and the real exapmles below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)):</span>
  <span class="s">'''INSERT YOUR CODE FOR THE MODEL DEFINITION BASED ON THE DESCRIPTION GIVEN ABOVE'''</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
  <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">))</span>

  <span class="c1">#Model compilation
</span>  <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">generator</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">):</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
	<span class="n">n_nodes</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Reshape</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">128</span><span class="p">)))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
	<span class="n">X</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span>

<span class="c1"># use the generator to generate n fake examples with class labels
</span><span class="k">def</span> <span class="nf">generate_fake_samples</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
	<span class="n">x_input</span> <span class="o">=</span> <span class="n">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="c1">#Predicting outputs
</span>	<span class="n">X</span> <span class="o">=</span> <span class="n">g_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>
	<span class="n">y</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">generate_real_samples</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
  <span class="c1">#Choosing random instances
</span>	<span class="n">ix</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">)</span>
	<span class="c1">#Retrieving Images
</span>	<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
	<span class="c1">#Generating class labels
</span>	<span class="n">y</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">define_gan</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
	<span class="n">d_model</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">g_model</span><span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
	<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>Then training</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">gan_model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_batch</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
	<span class="n">bat_per_epo</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_batch</span><span class="p">)</span>
	<span class="n">half_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_batch</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
	<span class="n">num_steps</span> <span class="o">=</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="n">bat_per_epo</span>
	<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bat_per_epo</span><span class="p">):</span>
			<span class="n">X_real</span><span class="p">,</span> <span class="n">y_real</span> <span class="o">=</span> <span class="n">generate_real_samples</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">half_batch</span><span class="p">)</span>
			<span class="n">X_fake</span><span class="p">,</span> <span class="n">y_fake</span> <span class="o">=</span> <span class="n">generate_fake_samples</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">half_batch</span><span class="p">)</span>
			<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">((</span><span class="n">X_real</span><span class="p">,</span> <span class="n">X_fake</span><span class="p">)),</span> <span class="n">vstack</span><span class="p">((</span><span class="n">y_real</span><span class="p">,</span> <span class="n">y_fake</span><span class="p">))</span>
			<span class="n">d_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">d_model</span><span class="p">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># first train discriminator
</span>			<span class="n">X_gan</span> <span class="o">=</span> <span class="n">generate_latent_points</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_batch</span><span class="p">)</span>
			<span class="n">y_gan</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">n_batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
			<span class="n">g_loss</span> <span class="o">=</span> <span class="n">gan_model</span><span class="p">.</span><span class="n">train_on_batch</span><span class="p">(</span><span class="n">X_gan</span><span class="p">,</span> <span class="n">y_gan</span><span class="p">)</span> <span class="c1"># fix discriminator and train generator
</span>			
			<span class="c1"># print('&gt;%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))
</span>			<span class="n">pbar</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
			<span class="n">pbar</span><span class="p">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"[Epoch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s">] Step </span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">bat_per_epo</span><span class="si">}</span><span class="s">: d_loss=</span><span class="si">{</span><span class="n">d_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> g_loss=</span><span class="si">{</span><span class="n">g_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">()</span>
<span class="n">g_model</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)</span>
<span class="n">gan_model</span> <span class="o">=</span> <span class="n">define_gan</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_real_samples</span><span class="p">()</span>
<span class="n">train</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">gan_model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="competitions">Competitions</h1>

<blockquote>
  <p><strong>Object Detection in Haze Link</strong></p>

  <ul>
    <li>http://cvpr2022.ug2challenge.org/track1.html</li>
  </ul>
</blockquote>

<p>basically we need to do:</p>

<ol>
  <li>dehazing an image</li>
  <li>object detection in those images
    <ul>
      <li>predict the bounding box for a vehicles</li>
    </ul>
  </li>
</ol>

<p>Some tipcs:</p>

<ul>
  <li>data augmentation likely needed</li>
</ul>

<hr />

<p>Ideas for <strong>dehazing</strong></p>

<ul>
  <li><strong>Pix2Pix:</strong> Image-to-Image Translation
    <ul>
      <li>Given a training set of unfiltered and filtered image pairs $A : A’$ and a new unfiltered image $B$ the output is a filtered image $B’$ such that the analogy $A:A’::B : B’$ is <strong>maintained</strong></li>
      <li>An input image is mapped to a synthesized image with different properties. The loss function is a combination of the conditional GAN loss with an additional loss term which is a pixel-wise loss that encourages the generator to match the source image</li>
    </ul>
  </li>
  <li>https://arxiv.org/pdf/1912.07015.pdf</li>
  <li>https://link.springer.com/article/10.1007/s11042-021-11442-6</li>
</ul>

<hr />

<p>Ideas for <strong>Object BB Drawing</strong></p>

<ul>
  <li>ViT based</li>
</ul>

<h2 id="detr">DETR</h2>

<p>Object detection (drawing bounding boxes).</p>

<p>Useful resources:</p>

<ul>
  <li>https://www.kaggle.com/code/tanulsingh077/end-to-end-object-detection-with-transformers-detr/notebook</li>
  <li>https://github.com/jasonyux/cvpr_2022 (checkout <code class="language-plaintext highlighter-rouge">models</code> folder)</li>
</ul>

<p><strong>code</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">pathlib</span><span class="p">.</span><span class="n">Path</span><span class="p">(</span><span class="n">__file__</span><span class="p">).</span><span class="n">parent</span><span class="p">.</span><span class="n">parent</span><span class="p">.</span><span class="n">resolve</span><span class="p">()</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">root</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetrFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetrConfig</span><span class="p">,</span> <span class="n">DetrForObjectDetection</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">utils.eval_util</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">utils.utils</span> <span class="kn">import</span> <span class="n">save_model_stats</span>
<span class="kn">from</span> <span class="nn">transformers.models.detr.modeling_detr</span> <span class="kn">import</span> <span class="n">ACT2FN</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>

<span class="n">PRETRAIN</span> <span class="o">=</span> <span class="s">"facebook/detr-resnet-50"</span>

<span class="k">class</span> <span class="nc">DetrAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="s">"""
	Multi-headed attention from 'Attention Is All You Need' paper.

	Here, we add position embeddings to the queries and keys (as explained in the DETR paper).
	"""</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
		<span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
		<span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
		<span class="n">is_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
		<span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
		<span class="k">assert</span> <span class="p">(</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span>
		<span class="p">),</span> <span class="sa">f</span><span class="s">"embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="si">}</span><span class="s"> and `num_heads`: </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s">)."</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bsz</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">tensor</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">with_pos_embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]):</span>
		<span class="k">return</span> <span class="n">tensor</span> <span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">tensor</span> <span class="o">+</span> <span class="n">position_embeddings</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">key_value_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">key_value_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
		<span class="s">"""Input shape: Batch x Time x Channel"""</span>

		<span class="c1"># if key_value_states are provided this layer is used as a cross-attention layer
</span>		<span class="c1"># for the decoder
</span>		<span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
		<span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>

		<span class="c1"># add position embeddings to the hidden states before projecting to queries and keys
</span>		<span class="k">if</span> <span class="n">position_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">hidden_states_original</span> <span class="o">=</span> <span class="n">hidden_states</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">with_pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">position_embeddings</span><span class="p">)</span>

		<span class="c1"># add key-value position embeddings to the key value states
</span>		<span class="k">if</span> <span class="n">key_value_position_embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">key_value_states_original</span> <span class="o">=</span> <span class="n">key_value_states</span>
			<span class="n">key_value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">with_pos_embed</span><span class="p">(</span><span class="n">key_value_states</span><span class="p">,</span> <span class="n">key_value_position_embeddings</span><span class="p">)</span>

		<span class="c1"># get query proj
</span>		<span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scaling</span>
		<span class="c1"># get key, value proj
</span>		<span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
			<span class="c1"># cross_attentions
</span>			<span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">key_value_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
			<span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">key_value_states_original</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="c1"># self_attention
</span>			<span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>
			<span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states_original</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">)</span>

		<span class="n">proj_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
		<span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shape</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
		<span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>
		<span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">proj_shape</span><span class="p">)</span>

		<span class="n">src_len</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

		<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

		<span class="k">if</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
			<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
				<span class="sa">f</span><span class="s">"Attention weights should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attn_weights</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
			<span class="p">)</span>

		<span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">):</span>
				<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
					<span class="sa">f</span><span class="s">"Attention mask should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
				<span class="p">)</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_mask</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

		<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="c1"># this operation is a bit awkward, but it's required to
</span>			<span class="c1"># make sure that attn_weights keeps its gradient.
</span>			<span class="c1"># In order to do so, attn_weights have to reshaped
</span>			<span class="c1"># twice and have to be reused in the following
</span>			<span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
			<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights_reshaped</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">attn_weights_reshaped</span> <span class="o">=</span> <span class="bp">None</span>

		<span class="n">attn_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">):</span>
			<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
				<span class="sa">f</span><span class="s">"`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s">, but is </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s">"</span>
			<span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
		<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

		<span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights_reshaped</span>

<span class="k">class</span> <span class="nc">DetrEncoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DetrConfig</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">encoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="p">.</span><span class="n">activation_function</span><span class="p">]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">activation_dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">encoder_ffn_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">encoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*): position embeddings, to be added to hidden_states.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""</span>
		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span><span class="p">(</span>
			<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
			<span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
			<span class="n">position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
			<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
		<span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
			<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">).</span><span class="nb">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">torch</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">).</span><span class="nb">any</span><span class="p">():</span>
				<span class="n">clamp_value</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">.</span><span class="n">dtype</span><span class="p">).</span><span class="nb">max</span> <span class="o">-</span> <span class="mi">1000</span>
				<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="n">clamp_value</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">clamp_value</span><span class="p">)</span>

		<span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>

		<span class="k">return</span> <span class="n">outputs</span>


<span class="k">class</span> <span class="nc">DetrDecoderLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DetrConfig</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
			<span class="n">is_decoder</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="p">.</span><span class="n">activation_function</span><span class="p">]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">activation_dropout</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn</span> <span class="o">=</span> <span class="n">DetrAttention</span><span class="p">(</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span>
			<span class="n">config</span><span class="p">.</span><span class="n">decoder_attention_heads</span><span class="p">,</span>
			<span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span>
			<span class="n">is_decoder</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">decoder_ffn_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">decoder_ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
		<span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">query_position_embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
		<span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
	<span class="p">):</span>
		<span class="s">"""
		Args:
			hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
			attention_mask (`torch.FloatTensor`): attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the cross-attention layer.
			query_position_embeddings (`torch.FloatTensor`, *optional*):
				position embeddings that are added to the queries and keys
			in the self-attention layer.
			encoder_hidden_states (`torch.FloatTensor`):
				cross attention input to the layer of shape `(seq_len, batch, embed_dim)`
			encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size
				`(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
			output_attentions (`bool`, *optional*):
				Whether or not to return the attentions tensors of all attention layers. See `attentions` under
				returned tensors for more detail.
		"""</span>
		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

		<span class="c1"># Self Attention
</span>		<span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn</span><span class="p">(</span>
			<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
			<span class="n">position_embeddings</span><span class="o">=</span><span class="n">query_position_embeddings</span><span class="p">,</span>
			<span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
			<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
		<span class="p">)</span>

		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">self_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="c1"># Cross-Attention Block
</span>		<span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

			<span class="n">hidden_states</span><span class="p">,</span> <span class="n">cross_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn</span><span class="p">(</span>
				<span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
				<span class="n">position_embeddings</span><span class="o">=</span><span class="n">query_position_embeddings</span><span class="p">,</span>
				<span class="n">key_value_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
				<span class="n">attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
				<span class="n">key_value_position_embeddings</span><span class="o">=</span><span class="n">position_embeddings</span><span class="p">,</span>
				<span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
			<span class="p">)</span>

			<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
			<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_attn_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="c1"># Fully Connected
</span>		<span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">activation_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>
		<span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

		<span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

		<span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
			<span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,</span> <span class="n">cross_attn_weights</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">outputs</span>

<span class="k">class</span> <span class="nc">FullDetr</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">lr_backbone</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
		<span class="c1"># replace COCO classification head with custom head
</span>		<span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">DetrForObjectDetection</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
			<span class="n">PRETRAIN</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
			<span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="bp">True</span>
		<span class="p">)</span>

		<span class="c1"># some customizations
</span>		<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">query_position_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="n">config</span> <span class="o">=</span> <span class="n">DetrConfig</span><span class="p">(</span>
			<span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
			<span class="n">activation_dropout</span><span class="o">=</span><span class="mf">0.1</span>
		<span class="p">)</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">)):</span> <span class="c1"># there are 6 encoders
</span>			<span class="n">pretrained_stat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">state_dict</span><span class="p">()</span>
			<span class="n">new_layer</span> <span class="o">=</span> <span class="n">DetrEncoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
			<span class="n">new_layer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained_stat</span><span class="p">)</span>
			<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_layer</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">)):</span>
			<span class="n">pretrained_stat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">state_dict</span><span class="p">()</span>
			<span class="n">new_layer</span> <span class="o">=</span> <span class="n">DetrDecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
			<span class="n">new_layer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained_stat</span><span class="p">)</span>
			<span class="n">pretrained_model</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_layer</span>
		
		<span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">pretrained_model</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">DetrFeatureExtractor</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">PRETRAIN</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

		<span class="c1"># see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">lr_backbone</span> <span class="o">=</span> <span class="n">lr_backbone</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>

		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">.</span><span class="n">pad_and_create_pixel_mask</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">enc_pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">pixel_mask</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="n">enc_pixel_values</span><span class="p">,</span> <span class="n">pixel_mask</span><span class="o">=</span><span class="n">pixel_mask</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span>

	<span class="k">def</span> <span class="nf">common_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>
		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">img</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
		<span class="n">labels</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span>

		<span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">feature_extractor</span><span class="p">.</span><span class="n">pad_and_create_pixel_mask</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">)</span>
		<span class="n">enc_pixel_values</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_values'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
		<span class="n">pixel_mask</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s">'pixel_mask'</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

		<span class="n">labels</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
		
		<span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">pixel_values</span><span class="o">=</span><span class="n">enc_pixel_values</span><span class="p">,</span>
							 <span class="n">pixel_mask</span><span class="o">=</span><span class="n">pixel_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
		
		<span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
		<span class="n">loss_dict</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss_dict</span>

		<span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span>

	<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		<span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">common_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
		<span class="c1"># logs metrics for each training_step,
</span>		<span class="c1"># and the average across the epoch
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"training_loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"train_"</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

		<span class="k">return</span> <span class="n">loss</span>

	<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
		<span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">common_step</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"validation_loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">loss_dict</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="s">"validation_"</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

		<span class="k">return</span> <span class="n">loss</span>

	<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">param_dicts</span> <span class="o">=</span> <span class="p">[</span>
			<span class="p">{</span><span class="s">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">(</span>
			<span class="p">)</span> <span class="k">if</span> <span class="s">"backbone"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">]},</span>
			<span class="p">{</span>
				<span class="s">"params"</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="s">"backbone"</span> <span class="ow">in</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">],</span>
				<span class="s">"lr"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr_backbone</span><span class="p">,</span>
			<span class="p">},</span>
		<span class="p">]</span>
		<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">param_dicts</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
									  <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">optimizer</span>

<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
	<span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
	<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
	<span class="n">batch</span> <span class="o">=</span> <span class="p">{}</span>
	<span class="n">batch</span><span class="p">[</span><span class="s">'imgs'</span><span class="p">]</span> <span class="o">=</span> <span class="n">imgs</span>
	<span class="n">batch</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
	<span class="k">return</span> <span class="n">batch</span>


<span class="k">class</span> <span class="nc">FullCocoDetection</span><span class="p">(</span><span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">CocoDetection</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_folder</span><span class="p">):</span>
		<span class="n">ann_file</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">img_folder</span><span class="p">):</span>
			<span class="k">if</span> <span class="nb">file</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">".json"</span><span class="p">):</span>
				<span class="n">ann_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">img_folder</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">FullCocoDetection</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">img_folder</span><span class="p">,</span> <span class="n">ann_file</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">ann_file</span> <span class="o">=</span> <span class="n">ann_file</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="o">=</span> <span class="bp">None</span>

	<span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
		<span class="c1"># read in PIL image and target in COCO format
</span>		<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">FullCocoDetection</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
		
		<span class="n">image_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
		<span class="n">target</span> <span class="o">=</span> <span class="p">{</span><span class="s">'image_id'</span><span class="p">:</span> <span class="n">image_id</span><span class="p">,</span> <span class="s">'annotations'</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
		<span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">target</span>

	<span class="k">def</span> <span class="nf">id_to_filename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span> <span class="o">=</span> <span class="p">{}</span>
			<span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ann_file</span><span class="p">,</span> <span class="s">"r"</span><span class="p">))</span>
			<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s">'images'</span><span class="p">]:</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'id'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s">'file_name'</span><span class="p">]</span>
		<span class="k">return</span>  <span class="bp">self</span><span class="p">.</span><span class="n">ids_to_fn</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
	<span class="n">root</span> <span class="o">=</span> <span class="s">"datasets"</span>
	<span class="n">model_name</span> <span class="o">=</span> <span class="s">"haze_full_detr_50"</span>

	<span class="n">info</span> <span class="o">=</span> <span class="p">{</span>
		<span class="s">"train_dset"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s">'full_train_haze_dset'</span><span class="p">)),</span>
		<span class="s">"test_dset"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s">'dry_test_dset'</span><span class="p">)),</span>
		<span class="s">"train_batch_size"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
		<span class="s">"test_batch_size"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
		<span class="s">"model_lr"</span><span class="p">:</span> <span class="mf">3e-5</span><span class="p">,</span>
		<span class="s">"model_lr_backbone"</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span>
		<span class="s">"model_weight_decay"</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
		<span class="s">"num_epochs"</span><span class="p">:</span> <span class="mi">50</span>
	<span class="p">}</span>
	<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"""
	Training with 
		</span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">
	"""</span><span class="p">)</span>

	<span class="c1">##### prepare data
</span>	<span class="n">train_images</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">'train_dset'</span><span class="p">]</span>
	<span class="n">test_images</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">'test_dset'</span><span class="p">]</span>

	<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">FullCocoDetection</span><span class="p">(</span><span class="n">img_folder</span><span class="o">=</span><span class="n">train_images</span><span class="p">)</span>
	<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">FullCocoDetection</span><span class="p">(</span><span class="n">img_folder</span><span class="o">=</span><span class="n">test_images</span><span class="p">)</span>

	<span class="k">print</span><span class="p">(</span><span class="s">"Number of training examples:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"Number of validation examples:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">))</span>

	<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
		<span class="n">train_dataset</span><span class="p">,</span> 
		<span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
		<span class="n">batch_size</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"train_batch_size"</span><span class="p">],</span> 
		<span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span>
	<span class="p">)</span>
	<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
		<span class="n">test_dataset</span><span class="p">,</span> 
		<span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> 
		<span class="n">batch_size</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"test_batch_size"</span><span class="p">]</span>
	<span class="p">)</span>

	<span class="c1">##### model
</span>	<span class="n">model</span> <span class="o">=</span> <span class="n">FullDetr</span><span class="p">(</span>
		<span class="n">lr</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_lr"</span><span class="p">],</span> 
		<span class="n">lr_backbone</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_lr_backbone"</span><span class="p">],</span> 
		<span class="n">weight_decay</span><span class="o">=</span><span class="n">info</span><span class="p">[</span><span class="s">"model_weight_decay"</span><span class="p">]</span>
	<span class="p">)</span>
	<span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

	<span class="c1">##### train
</span>	<span class="n">num_epochs</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s">"num_epochs"</span><span class="p">]</span>
	<span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>

	<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">))</span>

	<span class="n">optimizer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">configure_optimizers</span><span class="p">()</span>

	<span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
		<span class="s">"train_map"</span><span class="p">:[],</span>
		<span class="s">"val_map"</span><span class="p">:[]</span>
	<span class="p">}</span>
	<span class="n">train_base_ds</span> <span class="o">=</span> <span class="n">get_coco_api_from_dataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
	<span class="n">test_base_ds</span> <span class="o">=</span> <span class="n">get_coco_api_from_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>

	<span class="c1"># accelerator = Accelerator()
</span>
	<span class="n">best_vmap</span> <span class="o">=</span> <span class="mf">0.</span>
	<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
		<span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
			<span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
			<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
			<span class="c1"># accelerator.backward(loss)
</span>
			<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
			<span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
			<span class="n">progress_bar</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
		
		<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
		<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">feature_extractor</span>
		<span class="n">tmap</span> <span class="o">=</span> <span class="n">calculate_full_map</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_base_ds</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">vmap</span> <span class="o">=</span> <span class="n">calculate_full_map</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">test_base_ds</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">history</span><span class="p">[</span><span class="s">"train_map"</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">tmap</span><span class="p">)</span>
		<span class="n">history</span><span class="p">[</span><span class="s">"val_map"</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">vmap</span><span class="p">)</span>
		<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"train map=</span><span class="si">{</span><span class="n">tmap</span><span class="si">}</span><span class="s">, val map=</span><span class="si">{</span><span class="n">vmap</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">vmap</span> <span class="o">&gt;</span> <span class="n">best_vmap</span><span class="p">:</span>
			<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sa">f</span><span class="s">"models/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
			<span class="n">best_vmap</span> <span class="o">=</span> <span class="n">vmap</span>

	<span class="n">info</span><span class="p">[</span><span class="s">'model_saved_checkpoint'</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_vmap</span>
	<span class="n">save_model_stats</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">history</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="data-augmentations">Data Augmentations</h2>

<p>Using <code class="language-plaintext highlighter-rouge">albumentations</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">albumentations</span> <span class="k">as</span> <span class="n">A</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
		<span class="n">A</span><span class="p">.</span><span class="n">HorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
		<span class="n">A</span><span class="p">.</span><span class="n">VerticalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
		<span class="n">A</span><span class="p">.</span><span class="n">Affine</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rotate</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">180</span><span class="p">,</span> <span class="mi">180</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
	<span class="p">],</span> 
	<span class="n">bbox_params</span><span class="o">=</span><span class="n">A</span><span class="p">.</span><span class="n">BboxParams</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s">'coco'</span><span class="p">,</span> <span class="n">min_visibility</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="p">)</span>


<span class="n">sample_data</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># numpy array of an image
</span><span class="n">bboxes</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># bounding boxes
</span><span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">sample_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s">'annotations'</span><span class="p">]:</span>
	<span class="n">bboxes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="s">'bbox'</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s">'car'</span><span class="p">])</span>

<span class="n">transformed</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">bboxes</span><span class="o">=</span><span class="n">bboxes</span><span class="p">)</span>
<span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="s">'image'</span><span class="p">]</span>
<span class="n">transformed_bboxes</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="s">'bboxes'</span><span class="p">]</span>
</code></pre></div></div>


  </div><a class="u-url" href="/lectures/2022@columbia/COMS4995_Deep_Learning_part1.html/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
