<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>STFCS234 Reinforcement Learning | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="STFCS234 Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Stanford Reinforcement Learning:" />
<meta property="og:description" content="Stanford Reinforcement Learning:" />
<link rel="canonical" href="/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/" />
<meta property="og:url" content="/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-30T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="STFCS234 Reinforcement Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-09-30T00:00:00-04:00","datePublished":"2022-09-30T00:00:00-04:00","description":"Stanford Reinforcement Learning:","headline":"STFCS234 Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/"},"url":"/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
				<a class="page-link" href="/learning">Blog</a>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
  <script>
    MathJax = {
      // 
      loader: {
        load: ['[tex]/ams', '[tex]/textmacros', '[tex]/boldsymbol']
      },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']}
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">STFCS234 Reinforcement Learning</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-09-30T00:00:00-04:00" itemprop="datePublished">
        Sep 30, 2022
      </time></p>
  </header>

  <div class="section-nav" id="toc-all">
    <button type="button" id="toc-close" class="toc_collapsible hidden" title="collapse">
      <span><strong>Table of Contents</strong></span>
    </button>
    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" mirror-in-rtl="true" fill="#000000" style="width: 18px;" id="toc-reopen" class="toc_collapsible">
      <g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <circle fill="#494c4e" cx="2" cy="2" r="2"></circle> <circle fill="#494c4e" cx="2" cy="8" r="2"></circle> <circle fill="#494c4e" cx="2" cy="20" r="2"></circle> <circle fill="#494c4e" cx="2" cy="14" r="2"></circle> <path fill="#494c4e" d="M23.002 3H7.998C7.448 3 7 2.55 7 2.002v-.004c0-.55.45-.998.998-.998H23c.55 0 1 .45 1 .998V2c0 .55-.45 1-.998 1zM23.002 9H7.998C7.448 9 7 8.55 7 8.002v-.004c0-.55.45-.998.998-.998H23c.55 0 1 .45 1 .998V8c0 .55-.45 1-.998 1zM23.002 15H7.998c-.55 0-.998-.45-.998-.998V14c0-.55.45-1 .998-1H23c.55 0 1 .45 1 .998V14c0 .55-.45 1-.998 1zM23.002 21H7.998c-.55 0-.998-.45-.998-.998V20c0-.55.45-1 .998-1H23c.55 0 1 .45 1 .998V20c0 .55-.45 1-.998 1z"></path> </g>
    </svg>
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#introductions">Introductions</a></li>
<li class="toc-entry toc-h1"><a href="#intro-to-sequential-decision-making">Intro to Sequential Decision Making</a>
<ul>
<li class="toc-entry toc-h2"><a href="#markov-assumption-and-mdp">Markov Assumption and MDP</a></li>
<li class="toc-entry toc-h2"><a href="#types-of-sequential-decision-process">Types of Sequential Decision Process</a></li>
<li class="toc-entry toc-h2"><a href="#example-of-mdp">Example of MDP</a></li>
<li class="toc-entry toc-h2"><a href="#types-of-rl-agents">Types of RL Agents</a></li>
<li class="toc-entry toc-h2"><a href="#key-challenges-in-making-decisions">Key Challenges in Making Decisions</a></li>
<li class="toc-entry toc-h2"><a href="#important-components-in-rl">Important Components in RL</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#mdps-and-mrps">MDPs and MRPs</a>
<ul>
<li class="toc-entry toc-h2"><a href="#markov-processchain">Markov Process/Chain</a></li>
<li class="toc-entry toc-h2"><a href="#markov-reward-process">Markov Reward Process</a></li>
<li class="toc-entry toc-h2"><a href="#markov-decision-process">Markov Decision Process</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mdp-policy-evaluation">MDP Policy Evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#mdp-control">MDP Control</a></li>
<li class="toc-entry toc-h3"><a href="#mdp-policy-iteration">MDP Policy Iteration</a></li>
<li class="toc-entry toc-h3"><a href="#mdp-value-iteration">MDP Value Iteration</a>
<ul>
<li class="toc-entry toc-h4"><a href="#policy-iteration-as-bellman-operations">Policy Iteration as Bellman Operations</a></li>
<li class="toc-entry toc-h4"><a href="#contraction-operator">Contraction Operator</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#policy-evaluation-with-unknown-world">Policy Evaluation with Unknown World</a>
<ul>
<li class="toc-entry toc-h2"><a href="#monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#first-visit-mc-on-policy-evaluation">First Visit MC On Policy Evaluation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#bias-variance-and-mse-recap">Bias, Variance and MSE Recap</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#every-visit-mc-on-policy-evaluation">Every Visit MC On Policy Evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#incremental-mc-on-policy-evaluation">Incremental MC On Policy Evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#mc-on-policy-example">MC On Policy Example</a></li>
<li class="toc-entry toc-h3"><a href="#mc-on-policy-evaluation-key-limitations">MC On Policy Evaluation Key Limitations</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#temporal-difference-learning">Temporal Difference Learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#td0-learning">TD(0) Learning</a></li>
<li class="toc-entry toc-h3"><a href="#td0-learning-example">TD(0) Learning Example</a></li>
<li class="toc-entry toc-h3"><a href="#td-learning-key-limitations">TD Learning Key Limitations</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#comparison-between-dpmc-and-td">Comparison Between DP,MC and TD</a></li>
<li class="toc-entry toc-h2"><a href="#batch-mc-and-td">Batch MC and TD</a>
<ul>
<li class="toc-entry toc-h3"><a href="#properties-of-batch-md-and-td">Properties of Batch MD and TD</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#model-free-control">Model Free Control</a>
<ul>
<li class="toc-entry toc-h2"><a href="#mc-on-policy-policy-iteration">MC On-Policy Policy Iteration</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mc-for-on-policy-q-evaluation">MC for On Policy Q Evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#policy-evaluation-with-exploration">Policy Evaluation with Exploration</a></li>
<li class="toc-entry toc-h3"><a href="#epsilon-greedy-policy-improvement">$\epsilon$-greedy Policy Improvement</a></li>
<li class="toc-entry toc-h3"><a href="#mc-on-policy-improvementcontrol">MC On Policy Improvement/Control</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#td-on-policy-policy-iteration">TD On-Policy Policy Iteration</a>
<ul>
<li class="toc-entry toc-h3"><a href="#sarsa">SARSA</a></li>
<li class="toc-entry toc-h3"><a href="#convergence-properties-of-sarsa">Convergence Properties of SARSA</a></li>
<li class="toc-entry toc-h3"><a href="#q-learning">Q-Learning</a></li>
<li class="toc-entry toc-h3"><a href="#convergence-properties-of-q-learning">Convergence Properties of Q-Learning</a></li>
<li class="toc-entry toc-h3"><a href="#maximization-bias">Maximization Bias</a></li>
<li class="toc-entry toc-h3"><a href="#double-q-learning">Double Q-Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#value-function-approximation">Value Function Approximation</a>
<ul>
<li class="toc-entry toc-h2"><a href="#vfa-for-policy-evaluation">VFA for Policy Evaluation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#vfa-for-policy-evaluation-with-oracle">VFA for Policy Evaluation with Oracle</a></li>
<li class="toc-entry toc-h3"><a href="#model-free-vfa-policy-evaluation">Model Free VFA Policy Evaluation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#mc-value-function-approximation">MC Value Function Approximation</a></li>
<li class="toc-entry toc-h4"><a href="#convergence-for-mc-linear-value-function-approximation">Convergence for MC Linear Value Function Approximation</a></li>
<li class="toc-entry toc-h4"><a href="#batch-mc-value-function-approximation">Batch MC Value Function Approximation</a></li>
<li class="toc-entry toc-h4"><a href="#td-learning-with-value-function-approximation">TD Learning with Value Function Approximation</a></li>
<li class="toc-entry toc-h4"><a href="#convergence-for-td-linear-value-function-approximation">Convergence for TD Linear Value Function Approximation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#control-using-vfa">Control using VFA</a>
<ul>
<li class="toc-entry toc-h3"><a href="#action-value-function-approximation-with-an-oracle">Action-Value Function Approximation with an Oracle</a></li>
<li class="toc-entry toc-h3"><a href="#model-free-vfa-control">Model Free VFA Control</a>
<ul>
<li class="toc-entry toc-h4"><a href="#convergence-of-vfa-control">Convergence of VFA Control</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#deep-reinforcement-learning">Deep Reinforcement Learning</a>
<ul>
<li class="toc-entry toc-h2"><a href="#deep-q-learning">Deep Q Learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#dqns-experience-replay">DQNs: Experience Replay</a></li>
<li class="toc-entry toc-h3"><a href="#dqns-fixed-q-target">DQNs: Fixed Q-Target</a></li>
<li class="toc-entry toc-h3"><a href="#dqn-ablation-study">DQN Ablation Study</a></li>
<li class="toc-entry toc-h3"><a href="#dqn-summary">DQN Summary</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#double-dqn">Double DQN</a></li>
<li class="toc-entry toc-h2"><a href="#prioritized-replay">Prioritized Replay</a></li>
<li class="toc-entry toc-h2"><a href="#dueling-dqn-advantage-function">Dueling DQN: Advantage Function</a>
<ul>
<li class="toc-entry toc-h3"><a href="#identifiability-of-advantage-function">Identifiability of Advantage Function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#imitation-learning">Imitation Learning</a>
<ul>
<li class="toc-entry toc-h2"><a href="#learning-from-demonstrations">Learning from Demonstrations</a></li>
<li class="toc-entry toc-h2"><a href="#behavior-cloning">Behavior Cloning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#dagger-dataset-aggregation">DAGGER: Dataset Aggregation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#inverse-reinforcement-learning">Inverse Reinforcement Learning</a></li>
<li class="toc-entry toc-h2"><a href="#apprenticeship-learning">Apprenticeship Learning</a></li>
<li class="toc-entry toc-h2"><a href="#imitation-learning-summary">Imitation Learning Summary</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#policy-gradient">Policy Gradient</a>
<ul>
<li class="toc-entry toc-h2"><a href="#policy-objective-functions">Policy Objective Functions</a></li>
<li class="toc-entry toc-h2"><a href="#policy-gradient-methods">Policy Gradient Methods</a>
<ul>
<li class="toc-entry toc-h3"><a href="#gradients-by-finite-differences">Gradients by Finite Differences</a></li>
<li class="toc-entry toc-h3"><a href="#likelihood-ratioscore-function-policy-gradient">Likelihood Ratio/Score Function Policy Gradient</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#reducing-variance-in-policy-gradient">Reducing Variance in Policy Gradient</a>
<ul>
<li class="toc-entry toc-h3"><a href="#policy-gradient-use-temporal-structure">Policy Gradient: Use Temporal Structure</a></li>
<li class="toc-entry toc-h3"><a href="#reinforce-algorithm">REINFORCE Algorithm</a></li>
<li class="toc-entry toc-h3"><a href="#differentiable-policy-classes">Differentiable Policy Classes</a></li>
<li class="toc-entry toc-h3"><a href="#policy-gradient-introduce-baseline">Policy Gradient: Introduce Baseline</a>
<ul>
<li class="toc-entry toc-h4"><a href="#vanilla-policy-gradient-algorithm-and-auto-differentiation">Vanilla Policy Gradient Algorithm and Auto Differentiation</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#policy-gradient-actor-critic-methods">Policy Gradient: Actor-Critic Methods</a>
<ul>
<li class="toc-entry toc-h4"><a href="#n-step-estimators">N-step Estimators</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#automatic-step-size-tuning">Automatic Step Size Tuning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#evaluating-off-policy-value-function">Evaluating Off-Policy Value Function</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#mm-objective-and-trust-regions">MM Objective and Trust Regions</a>
<ul>
<li class="toc-entry toc-h3"><a href="#trust-region-constraints">Trust Region Constraints</a></li>
<li class="toc-entry toc-h3"><a href="#trust-region-policy-optimization">Trust Region Policy Optimization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#monte-carlo-tree-search">Monte Carlo Tree Search</a>
<ul>
<li class="toc-entry toc-h2"><a href="#model-learning">Model Learning</a></li>
<li class="toc-entry toc-h2"><a href="#planning-with-a-model">Planning with a Model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#forward-search">Forward Search</a></li>
<li class="toc-entry toc-h3"><a href="#simulation-based-search">Simulation-Based Search</a>
<ul>
<li class="toc-entry toc-h4"><a href="#simple-monte-carlo-search">Simple Monte-Carlo Search</a></li>
<li class="toc-entry toc-h4"><a href="#monte-carlo-tree-search-1">Monte-Carlo Tree Search</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#case-study-go">Case Study: GO</a></li>
<li class="toc-entry toc-h2"><a href="#off-policy-policy-evaluation">Off Policy Policy Evaluation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mc-off-policy-evaluation">MC Off Policy Evaluation</a></li>
<li class="toc-entry toc-h3"><a href="#importance-sampling-for-policy-evaluation">Importance Sampling for Policy Evaluation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#high-confidence-off-policy-policy-evaluation">High confidence Off-policy Policy Evaluation</a></li>
<li class="toc-entry toc-h2"><a href="#safe-policy-improvement">Safe Policy Improvement</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#general-offline-rl-algorithms">General Offline RL Algorithms</a>
<ul>
<li class="toc-entry toc-h2"><a href="#conservative-q-learning-methods">Conservative Q-Learning Methods</a></li>
<li class="toc-entry toc-h2"><a href="#model-based-offline-rl">Model-Based Offline RL</a>
<ul>
<li class="toc-entry toc-h3"><a href="#model-based-offline-policy-optimization">Model-based Offline Policy Optimization</a></li>
<li class="toc-entry toc-h3"><a href="#conservative-model-based-rl">Conservative Model-Based RL</a></li>
<li class="toc-entry toc-h3"><a href="#trajectory-transformer">Trajectory Transformer</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#summaries-and-directions-on-offline-rl">Summaries and Directions on Offline RL</a></li>
</ul>
</li>
</ul>
  </div>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Stanford Reinforcement Learning:</p>

<ul>
  <li>course video: https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&amp;index=1</li>
  <li>assignments: https://github.com/Huixxi/CS234-Reinforcement-Learning-Winter-2019</li>
  <li>slides: https://web.stanford.edu/class/cs234/CS234Win2019/schedule.html</li>
</ul>

<h1 id="introductions">Introductions</h1>

<p>Reinforcement Learning involves</p>

<ul>
  <li><strong>Optimization:</strong> find an optimal way to make decisions</li>
  <li><strong>Delayed consequences:</strong> Decisions now can impact things much later
    <ul>
      <li>When planning: decisions involve reasoning about not just immediate benefit of a decision but also its longer term ramifications</li>
      <li>When learning: temporal credit assignment is hard (what caused later high or low rewards?)</li>
    </ul>
  </li>
  <li><strong>Exploration:</strong> Learning about the world by making decisions and trying
    <ul>
      <li>censored data: Only get a reward (label) for decision made. i.e. only have one reality</li>
      <li>in a sense that we need to collect our own training data</li>
    </ul>
  </li>
  <li><strong>Generalization</strong>
    <ul>
      <li>learn a policy which can do interpolation/extrapolation, i.e. handle cases when not met in training data</li>
    </ul>
  </li>
</ul>

<p>Examples:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Example/Comment</th>
      <th style="text-align: center">Strategies Involved</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Go as RL</td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220516211946230.png" alt="image-20220516211946230" style="zoom:50%;" /></td>
      <td style="text-align: center">Optimization<br />Delayed Consequences<br />Generalization</td>
    </tr>
    <tr>
      <td>Supervised ML ad RL</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">Optimization<br />Generalization</td>
    </tr>
    <tr>
      <td>Imitation Learning as RL</td>
      <td style="text-align: center">Learns from experience…of others; <br />Assumes input demos of good policies<br />Reduces RL to supervised learning</td>
      <td style="text-align: center">Optimization<br />Delayed Consequences<br />Generalization</td>
    </tr>
  </tbody>
</table>

<h1 id="intro-to-sequential-decision-making">Intro to Sequential Decision Making</h1>

<p>The problem basically looks like</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220516213345977.png" alt="image-20220516213345977" style="zoom: 67%;" /></p>

<ul>
  <li>
    <p><strong>Goal</strong>: Select actions to maximize total ==expected== future ==reward==</p>
  </li>
  <li>
    <p>May require balancing immediate &amp; long term rewards</p>
  </li>
</ul>

<p>Note that it is critical to pick a good reward function. Consider the following RL system:</p>

<ul>
  <li>Agent: AI Teacher</li>
  <li>Action: pick a addition or subtraction problem for student to do</li>
  <li>World: some student doing the problem</li>
  <li>Observation: whether if student did it correctly or not</li>
  <li>Reward: +1 if correct, -1 if not</li>
</ul>

<p>What would happen in this case? The agent could learn to <strong>give easy problems</strong>.</p>

<hr />

<p>A more formal formulation of the task looks like</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220516214443813.png" alt="image-20220516214443813" style="zoom:67%;" /></p>

<p>where we have:</p>

<ul>
  <li>at each time $t$
    <ul>
      <li>agent takes some <strong>action</strong> $a_t$</li>
      <li>world updates given action $a_t$, emits <strong>observation</strong> $o_t$ and <strong>reward</strong> $r_t$</li>
      <li>agent receives observation $o_t$ and reward $r_t$</li>
    </ul>
  </li>
  <li>therefore, we essentially have a <strong>history</strong> $h_t=(a_1,o_1,r_1,…,a_t,o_t,r_t)$</li>
  <li><strong>State</strong> is information assumed to determine what happens next
    <ul>
      <li>usually the agent uses (some function of) the history to determine what happens next, so $s_t = f(h_t)$</li>
      <li>This is true state of the world is hidden from agent</li>
    </ul>
  </li>
</ul>

<h2 id="markov-assumption-and-mdp">Markov Assumption and MDP</h2>

<p>Why is Markov assumption so popular? It turns out that such an assumption can <strong>always be satisfied</strong> if we set state as history:</p>

\[p(s_{t+1}|s_t, a_t) = p(s_{t+1}|h_t,a_t)\]

<p>where RHS is like the true model, and LHS is what we are modelling.</p>

<p>However, in practice we often assume the <strong>most recent observation</strong> being sufficient to model the state</p>

\[p(s_{t+1}|s_t, a_t) = p(s_{t+1}|o_t,a_t)\]

<p>So that:</p>

<ul>
  <li>
    <p>If we consider most recent observation as your state, so $s_t = o_t$, then <strong>the agent is modelling the world as MDP</strong></p>
  </li>
  <li>
    <p>If agent state is not the same as the world state (e.g. Use history $s_t = h_t$ , or beliefs of world state, or RNN), then it is a <strong>Partially Observable MDP</strong>, or POMDP</p>
    <ul>
      <li>e.g. playing a poker game, where agent only sees its own card</li>
    </ul>
  </li>
</ul>

<p>Or other state representation (e.g. past 4 states). This choice affects <strong>how big our state space is</strong>, which has big implications for:</p>

<ul>
  <li>Computational complexity</li>
  <li>Data required</li>
  <li>Resulting performance</li>
</ul>

<hr />

<p>An example of this would be:</p>

<ul>
  <li>a state include all history, e.g. our entire life trajectory = have only a single data sample</li>
  <li>a state include only most recent life experience = a lot of data samples</li>
</ul>

<h2 id="types-of-sequential-decision-process">Types of Sequential Decision Process</h2>

<p>In reality, we may face different types of problems, each with different properties:</p>

<ul>
  <li><strong>Bandits</strong>: actions have no influence on next observations
    <ul>
      <li>e.g. whether if I clicked on the advertisement does not affect who the next customer (coming to the website) is</li>
    </ul>
  </li>
  <li><strong>MDP and POMDP</strong>: Actions influence future observations</li>
  <li><strong>Deterministic World</strong>: given the same state and action, we will have the same/only one possible observation and reward
    <ul>
      <li>Common assumption in robotics and controls</li>
    </ul>
  </li>
  <li><strong>Stochastic</strong>:  given the same state and action, we can have multiple possible observation and reward
    <ul>
      <li>Common assumption for customers, patients, hard to model domains</li>
      <li>can think of the case that we don’t have good enough model for deterministic coin flipping, hence we model it as stochastic</li>
    </ul>
  </li>
</ul>

<h2 id="example-of-mdp">Example of MDP</h2>

<p>Consider the case of having a Mars Rover exploring, which has seven possible states/locations to explore. We start at state $s_4$</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220516222302501.png" alt="image-20220516222302501" style="zoom:50%;" /></p>

<p>so we have</p>

<ul>
  <li><strong>States</strong>: Location of rover $(s_1, …,s_7)$</li>
  <li><strong>Actions</strong>: Left or Right</li>
  <li><strong>Rewards</strong>: $+1$ in state $s_1$,  $10$ in state $s_7$,  $0$ in all other states</li>
</ul>

<p>We want to find a program that tells the rover what to do next. In general RL problems can be modeled in three ways</p>

<ul>
  <li>
    <p><strong>Model</strong>: Representation of how the ==world== changes in response to agent’s action</p>

    <ul>
      <li>
        <p>in this case we need to model transition</p>

\[p(s_{t+1}=s' | s_t = s,a_t=a)\]

        <p>for any action state combination</p>
      </li>
      <li>
        <p>and the reward</p>

\[r(s_t=s,a_t=a) \equiv \mathbb{E}[r_t|s_t =s,a_t=a]\]

        <p>for any action state combination</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Policy</strong>: function mapping agent’s states to action (i.e. what to do next given current state)</p>

    <ul>
      <li>
        <p>hence we model, if deterministic</p>

\[\pi(s)=a\]

        <p>spits out an action given a state</p>
      </li>
      <li>
        <p>if stochastic</p>

\[\pi(a|s) = P(a_t=a|s_t=s)\]

        <p>being a probability distribution given a state.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Value function</strong>: ==Future rewards== from being in a state and/or action when following a particular policy</p>

    <ul>
      <li>
        <p>we consider the ==expected== discounted sum of future rewards <strong>under/given a particular policy $\pi$</strong></p>

\[V_\pi(s_t=s) = \mathbb{E}_\pi [r_t + \gamma r_{t+1} + \gamma^2 r_{t+2}+...|s_t=s]\]

        <p>for $\gamma \in [0,1)$ specifying how much we care about the future reward compared to current reward.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="types-of-rl-agents">Types of RL Agents</h2>

<p>Therefore, from the above discussion, we essentially have two types of RL algorithms:</p>

<ul>
  <li><strong>Model-based</strong>
    <ul>
      <li>explicitly having a model of the world (e.g. models the transition function, reward)</li>
      <li>may or may not have an explicit policy and/or value function (e.g. if needed compute from the model)</li>
      <li>no longer needs interaction/additonal experience</li>
    </ul>
  </li>
  <li><strong>Model-free</strong>
    <ul>
      <li>explicitly have a value function and/or policy function</li>
      <li>no model of the world</li>
    </ul>
  </li>
</ul>

<p>A more complete diagram would be</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220516224048782.png" alt="image-20220516224048782" style="zoom: 67%;" /></p>

<h2 id="key-challenges-in-making-decisions">Key Challenges in Making Decisions</h2>

<ul>
  <li><strong>Planning</strong> (Agent’s internal computation)
    <ul>
      <li>Given model of how the world works
        <ul>
          <li>i.e. you already know the transition and rewards</li>
        </ul>
      </li>
      <li>need an algorithm to compute how to act in order to maximize expected reward
        <ul>
          <li>With no interaction with real environment</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Reinforcement learning</strong> (we don’t even have the model)
    <ul>
      <li>Agent doesn’t know how world works</li>
      <li>Interacts with world to implicitly/explicitly learn how world works</li>
      <li>Agent improves policy (may involve planning)</li>
    </ul>
  </li>
</ul>

<p>so we see that RL deals with more problems: we also need to decide what action to do for a) getting the necessary information of the world, and b) achieve high future rewards</p>

<hr />

<p><em>For instance</em></p>

<ul>
  <li>Planning: Chess game
    <ul>
      <li>we already know all the possible moves, and rewards (who wins/losses)</li>
      <li>we need an algorithm to tell us what to do next based on this model
        <ul>
          <li>doing a tree search, etc.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Reinforcement Learning: Chess game with no rule book, i.e. don’t know the rule of chess
    <ul>
      <li>first we need to directly learn by taking actions and see what happens</li>
      <li>Try to find a good policy over time</li>
    </ul>
  </li>
</ul>

<h2 id="important-components-in-rl">Important Components in RL</h2>

<p>Agent <strong>only experiences</strong> what happens for the actions it tries. How should an RL agent balance its actions?</p>

<ul>
  <li><strong>Exploration</strong>: trying new things that might enable the agent to make better decisions in the future</li>
  <li><strong>Exploitation</strong>: choosing actions that are expected to yield good reward given past experience</li>
</ul>

<p>Often there may be an exploration-exploitation tradeoff, as you might not the correct model of the world (since you only have finite experience). May have to sacrifice reward in order to explore &amp; learn about potentially better policy</p>

<hr />

<p>Another two very important component you often see in RL is:</p>

<ul>
  <li>
    <p><strong>Evaluation</strong>: given some policy, we want to evaluate how good the policy is</p>
  </li>
  <li>
    <p><strong>Control</strong>: find the good policy</p>
    <ul>
      <li>e.g. do policy evaluation, and improve (iff the policy is stochastic)</li>
      <li>so does more than evaluation</li>
    </ul>
  </li>
</ul>

<h1 id="mdps-and-mrps">MDPs and MRPs</h1>

<p>Here we discuss how do we decide to take actions when <strong>given a world model</strong>. So here we will first discuss the problem of <strong>planning</strong>, instead of reinforcement learning (discussed later).</p>

<p>Specifically, we will cover</p>

<ul>
  <li>Markov Decision Processes (MDP)</li>
  <li>Markov Reward Processes (MRP)</li>
  <li>Evaluation and Control in MDPs</li>
</ul>

<p>Therefore, we basically consider</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517132606977.png" alt="image-20220517132606977" style="zoom: 50%;" /></p>

<p>so mathematically we consider</p>

\[p(s_{t+1}|s_t, a_t) = p(s_{t+1}|o_t, a_t)\]

<h2 id="markov-processchain">Markov Process/Chain</h2>

<p>Before we discuss MDP, we first consider what is MP.</p>

<blockquote>
  <p><strong>Markov Process</strong></p>

  <p>A Markov process has:</p>

  <ul>
    <li>
      <p>a set of finite states $s \in S$</p>
    </li>
    <li>
      <p>a dynamics/transition model $P$ that specifies $p(s_{t+1}=s’ \vert  s_t=s)$. For a finite number of states, this can be modelled as</p>

\[P = \begin{bmatrix}
P(s_1|s_1) &amp; P(s_2|s_1) &amp; \dots &amp; P(s_N|s_1)\\
P(s_1|s_2) &amp; P(s_2|s_2) &amp; \dots &amp; P(s_N|s_2)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
P(s_1|s_N) &amp; P(s_2|s_N) &amp; \dots &amp; P(s_N|s_N)\\
\end{bmatrix}\]
    </li>
  </ul>

  <p>notice that we have no action nor rewards: so we basically just observe this process being a <strong>memoryless random process</strong></p>
</blockquote>

<p>For instance, we can consider Mar’s Rover having the following transition dynamics</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Markov World</th>
      <th style="text-align: center">Transition Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517133727555.png" alt="image-20220517133727555" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517134058349.png" alt="image-20220517134058349" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where we see that $p(s_1\vert s_1)=0.6$, and $p(s_2\vert s_1)=0.4$, etc.</p>

<p>Using the transition model we can also mathematically compute the probability distribution of the next state given current state. For instance, if we are currently at $s_t=s_1$, then $p(s_{t+1}\vert s_t)$ is modelled by</p>

\[p(s_{t+1}|s_t=s_1) = [1, 0, 0,0,0,0,0]P = \begin{bmatrix}
0.6\\
0.4\\
0\\
\vdots\\
0
\end{bmatrix}\]

<p>Then, a sequence of states you would observe from the world would be sampled from the above distribution, so in the end you only see some deterministic observations/<strong>episodes</strong> such as</p>

<ul>
  <li>$s_4 \to s_5 \to s_6 \to s_7 \to s_7, …$</li>
  <li>$s_4 \to s_5 \to s_4 \to s_5 \to s_7, …$</li>
  <li>$s_4 \to s_3 \to s_2 \to s_1 \to s_2, …$</li>
  <li>etc.</li>
</ul>

<h2 id="markov-reward-process">Markov Reward Process</h2>

<p>Markov Reward Process is essentially Markov Chain + Reward</p>

<blockquote>
  <p><strong>Markov Reward Process</strong>:</p>

  <p>Markov reward process involves</p>

  <ul>
    <li>a set of finite states $s \in S$</li>
    <li>a dynamics/transition model $P$ that specifies $p(s_{t+1}=s’ \vert  s_t=s)$.
      <ul>
        <li>for a finite number of states, this can be expressed as a matrix.</li>
      </ul>
    </li>
    <li>a <strong>reward function</strong> $R(s_t=s) \equiv \mathbb{E}[r_t \vert  s_t=s]$, meaning the reward at a state is the expected reward of that state
      <ul>
        <li>for a finite number of states, this can be expressed as a vector.</li>
      </ul>
    </li>
    <li>allow for a discount factor $\gamma \in [0,1]$
      <ul>
        <li>mainly for mathematical convenient, which can avoid infinite returns and values/converges</li>
        <li>if episodes are finite, then $\gamma = 1$ works</li>
      </ul>
    </li>
  </ul>

  <p>note that we still have no actions.</p>
</blockquote>

<p>Once there is a reward, we can consider ideas such as returns and value functions</p>

<blockquote>
  <p><strong>Horizon</strong>: Number of time steps in each episode</p>

  <ul>
    <li>Can be infinite</li>
    <li>if not, it is called finite Markov reward process</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>(MRP) Return</strong>: discounted sum of rewards from time step $t$ to Horizon</p>

\[G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...\]

  <p>note that</p>

  <ul>
    <li>this is like a reward for a particular episode starting from time $t$</li>
    <li>why geometric series? This is usually used for its nice mathematical properties.</li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>(MRP) State Value Function $V(s)$</strong>: expected return from starting in state $s$</p>

\[V(s) = \mathbb{E}[G_t|s_t=s] = \mathbb{E}[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... | s_t=s]\]

</blockquote>

<p>note that</p>

<ul>
  <li>if the process is deterministic, then $\mathbb{E}[G_t\vert s_t=s]=G_t$</li>
  <li>if the process is stochastic, then it is usually different.</li>
</ul>

<hr />

<p><em>For instance</em>, consider the previous example of</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Markov World</th>
      <th style="text-align: center">Transition Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517133727555.png" alt="image-20220517133727555" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517134058349.png" alt="image-20220517134058349" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>but now we have:</p>

<ul>
  <li>$R(s=s_1)=+1$, and $R(s=s_7)=+10$. Zero otherwise</li>
</ul>

<p>Then consider sampling 2 episodes with 4-step length, using $\gamma=1/2$ we can compute the <strong>sample return</strong></p>

<ul>
  <li>$s_4,s_5,s_6,s_7$ having a return of $0+\frac{1}{2}\times 0+\frac{1}{4}\times 0+\frac{1}{8}\times 10=1.25$</li>
  <li>$s_4, s_4, s_5, s_4$ having a return of $0+\frac{1}{2}\times 0+\frac{1}{4}\times 0+\frac{1}{8}\times 0=0$</li>
</ul>

<p>Finally, we can consider the state value function by averaging over all the possible trajectories for each state, and we would get</p>

\[V= [1.53, 0.37, 0.13, 0.22, 0.85, 3.59, 15.31]\]

<p>in this particular example.</p>

<hr />

<p>How do we ==compute== the state function in reality?</p>

<ul>
  <li>
    <p>Could estimate by <strong>simulation</strong> (i.e. generate a large number of episodes and take average)</p>

    <ul>
      <li>notice that this method assumes no Markov process, as we are just sampling and averaging</li>
      <li>we have theoretical bounds as well on how many episodes we need</li>
    </ul>
  </li>
  <li>
    <p>Or we can <strong>utilize the Markov structure</strong> and know that MRP value function satisfies</p>

\[V(s) = \underbrace{R(s)}_{\text{imemdiate reward}}+ \quad \underbrace{\gamma \sum_{s'\in S} P(s'|s)V(s')}_{\text{discounted sum of future rewards}}\]
  </li>
</ul>

<p>Of course for computation we will use the latter case, which brings us to</p>

<blockquote>
  <p><strong>Bellman’s Equation for MRP</strong>: for fintie state MRP, we can express $V(s)$ for each state using a matrix equation</p>

\[\begin{bmatrix}
V(s_1)\\
\vdots\\
V(s_N)
\end{bmatrix} = \begin{bmatrix}
R(s_1)\\
\vdots\\
R(s_N)
\end{bmatrix} + \gamma \begin{bmatrix}
P(s_1|s_1) &amp; P(s_2|s_1) &amp; \dots &amp; P(s_N|s_1)\\
P(s_1|s_2) &amp; P(s_2|s_2) &amp; \dots &amp; P(s_N|s_2)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
P(s_1|s_N) &amp; P(s_2|s_N) &amp; \dots &amp; P(s_N|s_N)\\
\end{bmatrix}\begin{bmatrix}
V(s_1)\\
\vdots\\
V(s_N)
\end{bmatrix}\]

  <p>or more compactly</p>

\[V = R + \gamma PV\]

</blockquote>

<p>Note that both $R, P$ is known. All we need is to ==solve for $V$==. This can be solved in two ways:</p>

<ul>
  <li>directly with liner algebra</li>
  <li>iterative using DP</li>
</ul>

<p>First, solving it with linear algebra</p>

\[\begin{align*}
V &amp;= R + \gamma PV\\
V - \gamma PV &amp;= R\\
V &amp;= (I-\gamma P)^{-1}R
\end{align*}\]

<p>which requires solving matrix inverses, hence is $\sim O(N^3)$.</p>

<p>Another way to compute $V$ is by dynamic programming:</p>

<ol>
  <li>
    <p>initialize $V_0(s)=0$ for all state $s$</p>
  </li>
  <li>
    <p>For $k=1$ until convergence</p>

    <ul>
      <li>for all $s \in S$</li>
    </ul>

\[V_k(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V_{k-1}(s')\]
  </li>
</ol>

<p>As this is an iterative algorithm, the cost is $O(kN^2)$ for having $k$ iterations (so each iteration updates is only $O(N^2)$)</p>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>Finally we add in action as well, so essentially MDPs are Markov Reward Process + actions</p>

<blockquote>
  <p><strong>Markov Decision Process</strong>: MDP involves</p>

  <ul>
    <li>a set of finite states $s \in S$</li>
    <li>a finite set of actions $a \in A$</li>
    <li>a dynamics/transition model $P$ ==for each action== that specifies $p(s_{t+1}=s’ \vert  s_t=s, a_t=a)$.</li>
    <li>a reward function $R(s_t=s,a_t=a) \equiv \mathbb{E}[r_t \vert  s_t=s,a_t=a]$</li>
    <li>allow for a discount factor $\gamma \in [0,1]$</li>
  </ul>

  <p>note that know we mostly deal with the “joint probability” of $s_t,a_t$ together.</p>
</blockquote>

<p>For instance, for the Mars Rover MDP case</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">World Model</th>
      <th style="text-align: center">Transition Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517144202574.png" alt="image-20220517144202574" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517144210423.png" alt="image-20220517144210423" /></td>
    </tr>
  </tbody>
</table>

<p>where in this case we are having deterministic actions, which nonetheless can be modelled as a stochastic transition with probability of one.</p>

<p>Once we have actions in our model, we also have the following</p>

<blockquote>
  <p><strong>Policy</strong>: specifies what action to take in each state</p>

\[\pi (a|s) = P(a_t=a|s_t=s)\]

  <p>Basically a conditional distribution of actions on current state</p>
</blockquote>

<p>Note that once you ==specified a policy==, you can ==convert MDP + policy to a Markov Reward Process==, so that</p>

\[R^\pi (s) = \sum_{a \in A} \pi(a|s)R(s,a)\]

<p>is again independent of action in MRP, and similarly</p>

\[P^\pi(s'|s) = \sum_{a\in A}\pi(a|s) P(s'|s,a)\]

<p>which implies we can use <strong>same techniques to evaluate the value of a policy for a MDP</strong> as we could to compute the value of a MRP</p>

<h3 id="mdp-policy-evaluation">MDP Policy Evaluation</h3>

<p>For a <strong>deterministic policy $\pi(s)$</strong>, we can evaluate the value function $V^\pi(s)$ by, for example, the iterative algorithm mentioned before</p>

<ol>
  <li>
    <p>initialize $V_0(s)=0$ for all state $s$</p>
  </li>
  <li>
    <p>For $k=1$ until convergence</p>

    <ul>
      <li>for all $s \in S$</li>
    </ul>

\[V^\pi_k(s) = r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')\]

    <p>so essentially $V_k^\pi(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$, i.e. the value function if we are allowed to act for $k$ steps. Therefore, as $k$ increases we converge to infinite horizon.</p>
  </li>
</ol>

<blockquote>
  <p>This is also called the <strong>Bellman backup</strong> for a particular (deterministic) policy.</p>

  <ul>
    <li>note that of course we could have also computed $V^\pi$ analytically, or with simulation.</li>
  </ul>
</blockquote>

<p><em>For instance</em>, consider the setup of Mars Rover</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517144202574.png" alt="image-20220517144202574" style="zoom:50%;" /></p>

<p>And in this case we have:</p>

<ul>
  <li>Dynamics: $p(s_6 \vert  s_6,a_1)=0.5, p(s_7 \vert  s_6,a_1)=0.5, …$</li>
  <li>Reward: for all actions, +1 in state $s_1$, and +10 in state $s_7$. Zero otherwise</li>
  <li>Policy: $\pi(s)=a_1$ for all states.</li>
  <li>$\gamma$ set to $0.5$</li>
</ul>

<p>Let we initialize with $V^\pi_k=[1,0,0,0,0,0,0,10]$, we want to compute $V^\pi_{k+1}(s_6)$. From the iterative formula</p>

\[\begin{align*}
V^\pi_{k+1}(s_6) &amp;= r(s_6, a_1) + \gamma [0.5 * V_k(s_6) + 0.5 * V_k(s_7)]\\
&amp;= 0 + 0.5*[0.5 * 0 + 0.5 * 10]\\
&amp;= 2.5
\end{align*}\]

<blockquote>
  <p>Notice that we have <strong>propagated</strong> the reward information from $s_7$ to $s_6$ in this value function! If you do this for all states eventually such an information will be spread in all states.</p>
</blockquote>

<h3 id="mdp-control">MDP Control</h3>

<p>Ultimately we want our agent to <strong>find</strong> an optimal policy</p>

\[\pi^*(s) = \arg\max_\pi V^\pi(s)\]

<p>i.e. policy such that its value function is the maximum. Meaning that</p>

<blockquote>
  <p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if</p>

\[\pi \ge \pi' \iff V_\pi(s) \ge V_{\pi'}(s),\quad \forall s\]

  <p>which means its expected return is greater than or equal to that of $\pi’$ <strong>for all states</strong>. And there is ==always at least one policy that is better== than or equal to all other policies as a policy is essentially a mapping.</p>
</blockquote>

<p>Then, it turns out that</p>

<blockquote>
  <p><strong>Theorem</strong>:  MDP with infinite horizon:</p>

  <ul>
    <li>there exists a <em>unique optimal value function</em></li>
    <li>the <em>optimal policy</em> for a MDP in an infinite horizon problem is
      <ul>
        <li><em>deterministic</em></li>
        <li><em>stationary</em>: does not depend on time step. (intuition would be that for infinite horizon, you have essentially infinite time/visits for each state, hence the optimal policy does not depend on time)</li>
        <li><em>not necessarily unique</em></li>
      </ul>
    </li>
  </ul>
</blockquote>

<p>Therefore, it suffices for us to focus on (improving) deterministic policies as our final optimal policy is also deterministic</p>

<hr />

<p><em>For example</em>: Consider the Mars Rover case again</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517151127572.png" alt="image-20220517151127572" style="zoom: 50%;" /></p>

<p>which have 7 states and 2 actions, $a_1, a_2$. How many deterministic policies are there?</p>

<ul>
  <li>since a policy is essentially a mapping from states to actions, there are $2^7$ possible policies.</li>
</ul>

<hr />

<p>So how do we search for the ==best policy==?</p>

<ul>
  <li><strong>enumeration</strong>: compute for all $\vert A\vert ^{\vert S\vert }$ possible deterministic policy and pick best one</li>
  <li><strong>policy iteration</strong>: which is more efficient by doing policy evaluation + improvement</li>
</ul>

<h3 id="mdp-policy-iteration">MDP Policy Iteration</h3>

<p>The goal is to improve a policy iteratively so we end up with an optimal policy:</p>

<ol>
  <li>set $i=0$</li>
  <li>initialize $\pi_0(s)$ randomly for all $s$</li>
  <li>while $i==0$ or $\vert \vert \pi_i - \pi_{i-1}\vert \vert _1 &gt; 0$ being the L1-norm
    <ol>
      <li>$V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$</li>
      <li>$\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)</li>
      <li>$i = i+1$</li>
    </ol>
  </li>
</ol>

<p>How do we improve a policy? First need to consider some new definition</p>

<blockquote>
  <p><strong>State-Action Value Function</strong>: essentially a value function exploring what happens if you take some action $a$ (e.g. different than some given policy) in each state</p>

\[Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')\]

  <p>so essentially at each state $s$, we consider:</p>

  <ul>
    <li>take action $a$</li>
    <li>then follow policy $\pi$</li>
  </ul>
</blockquote>

<p>Therefore, using this we can essentially improve a policy $\pi_i$ by:</p>

<ol>
  <li>
    <p>compute the state-action value of a policy $\pi_i$</p>

    <ul>
      <li>for each state $s\in S$ and $a \in A$</li>
    </ul>

\[Q^{\pi_i}(s,a) = R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s')\]
  </li>
  <li>
    <p>compute the ==new policy $\pi_{i+1}$ by==</p>

\[\pi_{i+1}(s) = \arg\max_a Q^{\pi_i}(s,a),\quad \forall s\in S\]

    <p>to prove that this is a better policy, we need to show that</p>

\[V^{\pi_{i+1}}(s) \ge V^{\pi_i}(s)\]

    <p>which we will prove below. (inequality becomes strict if $V^{\pi_i}$ is suboptimal)</p>
  </li>
</ol>

<blockquote>
  <p>Using this approach, we are <strong>guaranteed</strong> to arrive at the global optimum of best policy.</p>
</blockquote>

<p><em>Proof</em>: Monotonic Improvement in Policy.</p>

<p>We first know that:</p>

\[\max_a Q^{\pi_i}(s,a) \ge V^{\pi_i}(s)=r(s,\pi_i(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi_i(s))V^{\pi_i}(s')\]

<p>by definition of choosing a $\max_a$. Then, notice that</p>

\[\begin{align*}
V^{\pi_i}(s) 
&amp;\le \max_a Q^{\pi_i}(s,a)\\
&amp;= \max_a \{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \}\\
&amp;= R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))V^{\pi_i}(s')\\
&amp;\le R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))\left( \max_{a'} Q^{\pi_i}(s',a') \right)\\\\
&amp;= R(s,\pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi_{i+1}(s))\left[ R(s',\pi_{i+1}(s')) + \gamma \sum_{s'' \in S} P(s''|s',\pi_{i+1}(s'))V^{\pi_i}(s'') \right]\\
&amp;\le \dots\\
&amp;= V^{\pi_{i+1}}(s)
\end{align*}\]

<p>where:</p>

<ul>
  <li>
    <p>notice that by definition of $Q^{\pi_{i}}$, we are <strong>using $\pi_i$ for future steps</strong> on the second and third equality</p>
  </li>
  <li>
    <p>the third equality comes from the fact that we know $\pi_{i+1}(s) = \arg\max_a Q^{\pi_i}(s,a)$ which spits out the action for maximizing $Q$</p>
  </li>
  <li>the fifth equality is basically doing the same as doing everything from the first to the third equality</li>
  <li>hence essentially we are expanding and pushing $\pi_{i+1}$ to a future step, ==until we are using $\pi_{i+1}$ for all steps==</li>
</ul>

<blockquote>
  <p>Note that: this is an <strong>monotonic improvement</strong></p>

  <ul>
    <li>this mean that if the policy didn’t change on one iteration, can it change in future iteration? No, because if taking the max cannot improve it, then it have reached global optimum.</li>
    <li>Is there are maximum number of policy iteration? Yes, because there is only $\vert A\vert ^{\vert S\vert }$ number of policies, and since improvement step is monotonic, each policy can only appear once (unless we reached optimal)</li>
  </ul>
</blockquote>

<h3 id="mdp-value-iteration">MDP Value Iteration</h3>

<p>Another approach to find an optimal policy is by <strong>value iteration</strong>.</p>

<ul>
  <li>policy iteration: for each policy $\pi_i$, we get the value $V^{\pi_i}$ for the infinite horizon and improve it, until obtained best $V^{\pi^*}$</li>
  <li>value iteration: maintain the best $V(s)$ up to $k$ number of steps left in the episode, until $k \to \infty$ or converges</li>
</ul>

<p>so we are computing a different thing here, but the final answer will be the same.</p>

<blockquote>
  <p><em>Recall</em> that value of a policy ==has to satisfy== the Bellman equation</p>

\[\begin{align*}
V^\pi(s) 
&amp;= R^\pi(s) + \gamma \sum_{s' \in S} P^\pi(s'|s)V^\pi(s')\\
&amp;=R(s,\pi(s)) + \gamma \sum_{s' \in S} P(s'|s,\pi(s))V^\pi(s')
\end{align*}\]

  <p>was the definition</p>
</blockquote>

<p>Then we can consider a Bellman backup operator which <strong>operates on a value function</strong>:</p>

\[\mathbb{B}V(s) \equiv \max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V(s') \}\]

<p>so basically:</p>

<ul>
  <li>$\mathbb{B}$ is like an operator, which will spit out a new value function</li>
  <li>it will improve the value if possible, as this is basically what policy iteration did</li>
</ul>

<hr />

<p>Then, with this, we define the algorithm for value iteration:</p>

<ol>
  <li>
    <p>set $k=1$</p>
  </li>
  <li>
    <p>Initialize $V_0(s)=0$ for all state $s$</p>
  </li>
  <li>
    <p>loop until [finite horizon, convergence]</p>

    <ul>
      <li>
        <p>for each state $s$</p>

\[V_{k+1}(s) = \max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V_k(s') \}\]

        <p>which can be views as just a Bellman backup operation on $V_k$</p>

\[V_{k+1} = \mathbb{B}V_k\]
      </li>
    </ul>
  </li>
  <li>
    <p>then policy for acting $k+1$ steps (best policy) can be easily derived by the action that leads to the best state given current state</p>

\[\pi_{k+1}(s) = \arg\max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V_k(s') \}\]

    <p>so basically considering best action if only act for $k=1$ step, the use this to compute $k=2$ steps, and etc.</p>
  </li>
</ol>

<p>How do we know that this converges? This is because Bellman’s backup operator is a <a href="#Contraction Operator">contraction operator</a> (if $\gamma \le 1$)</p>

<blockquote>
  <p>Difference between Policy and Value iteration</p>

  <ul>
    <li><strong>Value iteration</strong>
      <ul>
        <li>Compute optimal value as if horizon $= k$ steps
          <ul>
            <li>Note this can be used to compute optimal policy if horizon $= k$, i.e. finite horizon</li>
          </ul>
        </li>
        <li>Increment $k$</li>
      </ul>
    </li>
    <li><strong>Policy iteration</strong>
      <ul>
        <li>Compute infinite horizon value of a policy</li>
        <li>Use to select another (better) policy</li>
        <li>Closely related to a very popular method in RL: policy gradient</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h4 id="policy-iteration-as-bellman-operations">Policy Iteration as Bellman Operations</h4>

<p>Essentially policy iteration also derives from the Bellman’s constraint, so we can express the policy iteration as Bellman operations as well.</p>

<p>First, we consider Bellman backup operator $\mathbb{B}^\pi$ for a <strong>particular policy</strong> which operates <strong>on some value function</strong> (which could have a different policy):</p>

\[\mathbb{B}^\pi  V(s) = R^\pi(s) + \gamma \sum_{s'\in S} P^\pi(s'|s)V(s')\]

<p>Therefore, this means that:</p>

<ol>
  <li>
    <p>policy evaluation of $\pi_i$ is basically doing:</p>

\[V^{\pi_i} = B^{\pi_i}B^{\pi_i}...B^{\pi_i}V\]

    <p>for some randomly initialized $V$</p>
  </li>
  <li>
    <p>policy improvement</p>

\[\pi_{k+1}(s) = \arg\max_a \{ R(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)V^{\pi_k}(s') \}\]
  </li>
</ol>

<h4 id="contraction-operator">Contraction Operator</h4>

<blockquote>
  <p><strong>Contraction Operator</strong>:</p>

  <p>Let $O$ be an operator, and $\vert x\vert$ denote any norm of $x$. If</p>

\[|OV-OV'| \le |V-V'|\]

  <p>then $O$ is an contraction operator.</p>
</blockquote>

<p>so basically</p>

<ul>
  <li>
    <p>distance between two value functions after applying Bellman’s operator must be less than or equal to distance they had before.</p>
  </li>
  <li>
    <p>Given this property, it is straightforward to argue that distance between some value function $V$ to the optimal value function $V^*$ will decrease monotonically, hence convergence.</p>
  </li>
</ul>

<p><em>Proof</em></p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220517171411902.png" alt="image-20220517171411902" style="zoom: 80%;" /></p>

<h1 id="policy-evaluation-with-unknown-world">Policy Evaluation with Unknown World</h1>

<p>Before in the <a href="#MDPs and MRPs">MDPs and MRPs</a> section we discussed the problem of planning: <strong>if we know how the world works</strong>, i.e. have some transition/reward function known, <strong>how do we find out the best policy</strong>.</p>

<p>However, what if we <strong>do not have a world model</strong> to begin with? This is what we will discuss in this section, including:</p>

<ul>
  <li>Monte Carlo policy evaluation</li>
  <li>Temporal Difference (TD)</li>
</ul>

<h2 id="monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation</h2>

<p>First, recall that with a world model, our algorithm for <strong>policy evaluation</strong> is:</p>

<ol>
  <li>
    <p>initialize $V_0(s)=0$ for all state $s$</p>
  </li>
  <li>
    <p>For $k=1$ until convergence</p>

    <ul>
      <li>for all $s \in S$</li>
    </ul>

\[V^\pi_k(s) = r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')\]

    <p>so essentially $V_k^\pi(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$, i.e. the value function if we are allowed to act for $k$ steps. Therefore, as $k$ increases we converge to infinite horizon.</p>
  </li>
</ol>

<p>In other words, we are iteratively <strong>estimating $V^\pi(s)$</strong> by $V_k^\pi(s)$ as:</p>

\[V^\pi(s) = \mathbb{E}_\pi[G_t|s_t=s] \approx V_k^\pi(s) = \mathbb{E}_\pi[r_t + \gamma V_{k-1}|s_t=s]\]

<p>we can think update tule graphhically as:</p>

\[V^\pi(s) \leftarrow \mathbb{E}_\pi[r_t + \gamma V_{k-1}|s_t=s]\]

<table>
  <thead>
    <tr>
      <th style="text-align: center">Tree of Possible Trajectories Following a Stochastic $\pi(a\vert s)$</th>
      <th style="text-align: center">Dynamic Programming Algorithm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518174109832.png" alt="image-20220518174109832" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that:</p>

<ul>
  <li>
    <p>essentially we are taking $V_{k-1}^\pi(s)$ for each state as the “true value functions”, and then computing the highlighted expectation by</p>

\[V^\pi(s) \leftarrow r(s,\pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V_{k-1}(s')\]

    <p>to update the value functions. Notice that for this to make sense we need to ==assume infinite horizon==, since we are treating $V^\pi(s)$ being stationary for each state, i.e. not a function of time step.</p>
  </li>
  <li>
    <p>therefore we are bootstrapping as we are taking the value function from previous iteration.</p>
  </li>
  <li>
    <p>notice that to compute this update we needed $P(s’\vert s,a)$ to <strong>average over all possible futures</strong> (and $R(s,a)$ as well)</p>
  </li>
</ul>

<hr />

<p>Now, what happens if we ==do not know the world/model==?</p>

<p><strong>In the framework of MC Policy Evaluation</strong>, the idea is to notice that</p>

\[V^\pi(s) = \mathbb{E}_{T \sim \pi}[G_t | s_t=s]\]

<p>basically averaging the $G_t$ of each possible trajectory $T$ following $\pi$, i.e. <strong>average over the branches of the tree</strong></p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" /></p>

<p>therefore, if trajectories are all finite, <strong>sample set of trajectories &amp; average returns</strong> would give us some approximation of the value function. Hence, properties related to this approach include:</p>

<ul>
  <li>Does not require a known MDP dynamics/rewards, as we <strong>just need sample trajectories as its reward</strong></li>
  <li>Does not assume state is Markov (e.g. no notion of next state, etc.)</li>
  <li>Can only be applied to <strong>episodic MDPs</strong>
    <ul>
      <li>Requires each episode to terminate</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Aim</strong>: estimate $V^\pi(s)$ given sampled episodes $(s_1,a_1,r_1,s_2,a_2,r_2,…)$ generated under policy $\pi$</p>
</blockquote>

<p>Using this <strong>idea of MC policy evaluation</strong>, we have the following algorithms that can do this:</p>

<ul>
  <li>first visit MC on Policy Evaluation</li>
  <li>every visit MC on Policy Evaluation</li>
  <li>incremental MC on Policy Evaluation</li>
</ul>

<h3 id="first-visit-mc-on-policy-evaluation">First Visit MC On Policy Evaluation</h3>

<p>Our aim is to estimate $V^\pi(s)$ by sampling trajectories and taking their mean (as being the MC method). In this case we consider the following algorithm:</p>

<ol>
  <li>
    <p>Initialize $N(s)=0$, $G(s)=0, \forall s \in S$</p>
  </li>
  <li>
    <p>loop</p>

    <ol>
      <li>
        <p>sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},….,s_{i,T_i})$</p>
      </li>
      <li>
        <p>define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards</p>

\[G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}\]
      </li>
      <li>
        <p>for each state $s$ visited in the episode $i$</p>

        <ol>
          <li>for the ==first time== $t$ that state $s$ is visited
            <ol>
              <li>increment counter of total first visits for that state $N(s)=N(s)+1$</li>
              <li>increment total return $G(s)=G(s)+G_{i,t}$</li>
              <li><strong>Update estimate</strong> $V^\pi(s)=G(s)/N(s)$</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p>How does this algorithm work? How do we know that this way of estimating $V^\pi(s)$ being good (i.e. bias, variance, and consistent)?</p>

<blockquote>
  <p><strong>Theorem</strong>:</p>

  <ul>
    <li>$V^\pi$ estimator in this case is an <strong>unbiased</strong> estimator of the true $\mathbb{E}_\pi[G_t\vert s_t=s]$</li>
    <li>$V^\pi$ estimator in this case is also a <strong>consistent</strong> estimator so that as $N(s)\to \infty$, $V^\pi(s)\to \mathbb{E}_\pi[G_t\vert s_t=s]$</li>
  </ul>
</blockquote>

<p>However, those this is a good news, it might not be efficient as we are throwing away many other visits to the same state, which brings us to <a href="#Every Visit MC On Policy Evaluation">Every Visit MC On Policy Evaluation</a></p>

<h4 id="bias-variance-and-mse-recap">Bias, Variance and MSE Recap</h4>

<p>Recall that a way to see if certain estimators work is to <strong>compare against the ground truth</strong>. Let $\hat{\theta}=f(x)$ be our estimator function (a function of the observed data $x$) for some true parameter $\theta$.</p>

<ul>
  <li>so basically $x \sim P(x\vert \theta)$ being generated from the true parameter</li>
  <li>we are constructing an estimator $\hat{\theta}=f(x)$ to estimate such a parameter.
    <ul>
      <li>e.g. for $x \sim N(\mu, \sigma)$, we can estimate $\mu$ by $\hat{\mu} = \text{mean}(x)$ being our estimator</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Bias</strong>: The bias of an estimator $\hat{\theta}$ is defined by</p>

\[\text{Bias}_\theta(\hat{\theta}) \equiv  \mathbb{E}_{x|\theta}[\hat{\theta}] - \theta\]

  <p>so that over many sampled datasets, how far is the average $\hat{\theta}$ from the true $\theta$</p>
</blockquote>

<p>note that since we don’t know what $\theta$ is in reality, we usually just bound it.</p>

<blockquote>
  <p><strong>Variance</strong>: the variance of an estimator $\hat{\theta}$ is defined by:</p>

\[\text{Var}(\hat{\theta}) \equiv  \mathbb{E}_{x|\theta}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])]\]

  <p>which is basically the definition of variance itself and has nothing to do with $\theta$</p>
</blockquote>

<p>In general different algorithms will have different trade-off between bias and variance.</p>

<blockquote>
  <p><strong>MSE</strong>: Mean squared error of an estimator $\hat{\theta}$ is</p>

\[\text{MSE}_\theta(\hat{\theta}) \equiv \text{Var}(\hat{\theta})+\text{Bias}_\theta(\hat{\theta})^2\]

</blockquote>

<h3 id="every-visit-mc-on-policy-evaluation">Every Visit MC On Policy Evaluation</h3>

<p>Here, the change is small</p>

<ol>
  <li>
    <p>Initialize $N(s)=0$, $G(s)=0, \forall s \in S$</p>
  </li>
  <li>
    <p>loop</p>

    <ol>
      <li>
        <p>sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},….,s_{i,T_i})$</p>
      </li>
      <li>
        <p>define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards</p>

\[G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}\]
      </li>
      <li>
        <p>for each state $s$ visited in the episode $i$</p>

        <ol>
          <li>for the ==every time== $t$ that state $s$ is visited
            <ol>
              <li>increment counter of total first visits for that state $N(s)=N(s)+1$</li>
              <li>increment total return $G(s)=G(s)+G_{i,t}$</li>
              <li><strong>Update estimate</strong> $V^\pi(s)=G(s)/N(s)$</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p>Although this is more data efficient as we performed more updates</p>

<blockquote>
  <p><strong>Theorem</strong>:</p>

  <ul>
    <li>$V^\pi$ estimator in this case is an <strong>biased</strong> estimator of the true $\mathbb{E}_\pi[G_t\vert s_t=s]$
      <ul>
        <li>the intuition here is that because each state in an episode is <em>not IID</em>, the $G_{i,t}$ is <em>not IID</em> either. Therefore, we will get a biased estimator in this case as we use all encounters of state $s$</li>
      </ul>
    </li>
    <li>$V^\pi$ estimator in this case is a <strong>consistent</strong> estimator so that as $N(s)\to \infty$, $V^\pi(s)\to \mathbb{E}_\pi[G_t\vert s_t=s]$</li>
    <li>Empirically, $V^\pi$ has a lower variance</li>
  </ul>
</blockquote>

<h3 id="incremental-mc-on-policy-evaluation">Incremental MC On Policy Evaluation</h3>

<p>In both previous algorithms we had to update the mean incrementally by</p>

<ol>
  <li>
    <p>first doing $N(s)=N(s)+1$</p>
  </li>
  <li>
    <p>increment total return $G(s)=G(s)+G_{i,t}$</p>
  </li>
  <li>
    <p>then update</p>

\[V^\pi(s)=\frac{G(s)}{N(s)}\]
  </li>
  <li></li>
</ol>

<p>We can also perform the <strong>same update incrementally</strong> by:</p>

<ol>
  <li>
    <p>first doing $N(s)=N(s)+1$</p>
  </li>
  <li>
    <p>update</p>

\[V^\pi(s)=V^\pi(s)\frac{N(s)-1}{N(s)} + \frac{G_{i,t}}{N(s)}=V^\pi(s)+\frac{G_{i,t}-V^\pi(s)}{N(s)}\]

    <p>for basically ==adding a “correction term”== of $(G_{i,t}-V^\pi(s))/N(s)$</p>
  </li>
</ol>

<p>this idea of a correction term will be used later in TD algorithms, but in practice we can tweak this term so that, if we consider the following algorithm:</p>

<ol>
  <li>
    <p>Initialize $N(s)=0$, $G(s)=0, \forall s \in S$</p>
  </li>
  <li>
    <p>loop</p>

    <ol>
      <li>
        <p>sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},….,s_{i,T_i})$</p>
      </li>
      <li>
        <p>define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards</p>

\[G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}\]
      </li>
      <li>
        <p>for each state $s$ visited in the episode $i$</p>

        <ol>
          <li>
            <p>for the ==every time== $t$ that state $s$ is visited</p>

            <ol>
              <li>
                <p>increment counter of total first visits for that state $N(s)=N(s)+1$</p>
              </li>
              <li>
                <p>update</p>

\[V^\pi(s)=V^\pi(s)+\alpha\left( G_{i,t}-V^\pi(s) \right)\]
              </li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p>Then if we used:</p>

<ul>
  <li>$\alpha = 1/N(s)$ then it is the same as every visit MC</li>
  <li>$\alpha &gt; 1/N(s)$ then we are placing <strong>emphasis on more recent ones/updates $G_{i,t}-V^\pi(s)$</strong>.
    <ul>
      <li>this could be helpful if domains are non-stationary. For example in robotics some parts can be breaking down over time, so we want to focus more on more recently learnt data</li>
    </ul>
  </li>
</ul>

<h3 id="mc-on-policy-example">MC On Policy Example</h3>

<p>Consider a simple case of a Mars Rover again:</p>

<ul>
  <li>reward being $R=[1,0,0,0,0,0,0,10]$</li>
  <li>initialize $V^\pi(s)=0$ for all $s$</li>
  <li>Our policy is $\pi(s)=a_1,\forall s$.</li>
  <li>Any action from state $s_1$ or $s_7$ gives termination</li>
  <li>take $\gamma = 1$</li>
</ul>

<p>Suppose then we got a trajectory from this policy:</p>

<ul>
  <li>$T = (s_3,a_1,0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text)$</li>
</ul>

<p><strong>Question</strong>: What is the first and every visit MC estimate of $V^\pi$?</p>

<ul>
  <li>first visit of $V^\pi(s)=[1,1,1,0,0,0,0]$</li>
  <li>every visit is the same even though $s_2$ has two updates</li>
</ul>

<blockquote>
  <p>Notice that for all MC methods, we only was able to <strong>perform updates until the termination of the episode</strong>, since we need to know $G_{i,t}$.</p>
</blockquote>

<h3 id="mc-on-policy-evaluation-key-limitations">MC On Policy Evaluation Key Limitations</h3>

<p>Graphically, what MC algorithms are doing is</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Tree of Possible Trajectories Following a Stochastic $\pi(a\vert s)$</th>
      <th style="text-align: center">MC On Policy Algorithm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518173442550.png" alt="image-20220518173442550" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518215716060.png" alt="image-20220518215716060" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>So notice that we are averaging across all trajectories, meaning that:</p>

<ul>
  <li><strong>Generally high variance estimator</strong>. Reducing variance can require a lot of data</li>
  <li><strong>Requires episodic settings</strong> (since we can only update once episode terminated)</li>
</ul>

<h2 id="temporal-difference-learning">Temporal Difference Learning</h2>

<blockquote>
  <p>“If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.” – Sutton and Barto 2017</p>
</blockquote>

<p>Some key attributes of this algorithm is:</p>

<ul>
  <li><strong>Combination</strong> of Monte Carlo &amp; dynamic programming methods
    <ul>
      <li>therefore, it Bootstraps (dynamic programming, reusing $V_{k-1}^\pi(s)$) and samples (MC)</li>
    </ul>
  </li>
  <li><strong>Model-free</strong> (does not need to know the transition/reward functions)</li>
  <li>Can be used in episodic <strong>or infinite-horizon</strong> non-episodic settings</li>
  <li>Can <strong>immediately updates</strong> estimate of $V$ after each $(s,a,r,s’)$ tuple, i.e. we do not need to wait until the end-of-episode like MC methods.</li>
</ul>

<blockquote>
  <p><strong>Aim</strong>: estimate $V^\pi(s)$ given sampled episodes $(s_1,a_1,r_1,s_2,a_2,r_2,…)$ generated under policy $\pi$</p>
</blockquote>

<p>this is the ==same== as MC on Policy Evaluation. But recall that if we are having a MDP model, the <strong>Bellman Operator can be used to perform an update</strong> of the $V(s)$:</p>

\[\mathbb{B}^\pi  V(s) = R^\pi(s) + \gamma \sum_{s'\in S} P^\pi(s'|s)V(s')\]

<p>which basically ==takes the current reward== and ==averages over the value of next state==. From this, we consider</p>

<blockquote>
  <p><strong>Insight</strong>: given some current estimate of $V^\pi$, we can update using</p>

\[V^\pi(s_t) =V^\pi(s_t)+\alpha( \underbrace{[r_t+\gamma V^\pi(s_{t+1})]}_{\text{TD target}}-V^\pi(s_t))\]

  <p>for $r_t+\gamma V^\pi(s_{t+1})$ basically is our target (to approximate $r_t+\gamma \sum P^\pi(s’\vert s)V(s’)$) to improve $V^\pi(s)$, as compared to the MC On-policy method which used $G_{i,t}$.</p>

  <p>Notice that:</p>

  <ul>
    <li>
      <p>this target is basically also ==takes the current reward== and ==looks at the value of next state==.</p>
    </li>
    <li>
      <p>we are bootstrapping again, instead of waiting until end of episode, we use previous estimate $V^\pi(s)$ to compute our target $r_t+\gamma V^\pi(s_{t+1})$ and updates ==at each time step==. Therefore we also don’t need episodic requirement.</p>
    </li>
  </ul>
</blockquote>

<blockquote>
  <p><strong>TD Error</strong>: we can also view the above update rule as basically correcting $V^\pi(s)$ by the error term:</p>

\[\delta_t \equiv [r_t+\gamma V^\pi(s_{t+1})]-V^\pi(s_t)\]

</blockquote>

<p>Under this framework of TD learning, we also have some variations:</p>

<ul>
  <li>TD(0) Learning: only boostrap</li>
  <li>TD($\lambda$) Learning: MC update for $\lambda$ steps and then bootstrap</li>
</ul>

<p>hence technically we have a continuum of algorithm between using bootstrap and using MC algorithm.</p>

<h3 id="td0-learning">TD(0) Learning</h3>

<p>The simplest TD algorithm is TD(0):</p>

<ol>
  <li>
    <p>input $\alpha$</p>
  </li>
  <li>
    <p>initialize $V^\pi(s)=0$, $\forall s \in S$</p>
  </li>
  <li>
    <p>loop</p>

    <ol>
      <li>
        <p>sample tuple $(s_t,a_t,r_t,s_{t+1})$</p>
      </li>
      <li>
        <p>update the state:</p>

\[V^\pi(s_t) =V^\pi(s_t)+\alpha( \underbrace{[r_t+\gamma V^\pi(s_{t+1})]}_{\text{TD target}}-V^\pi(s_t))\]
      </li>
    </ol>
  </li>
  <li></li>
</ol>

<p>which is a <strong>combination</strong> of MC sampling and bootstrapping:</p>

<ul>
  <li>we needed sampling to get $s’$ for evaluating $V^\pi(s_{t+1})$</li>
  <li>we used the previous estimate of $V^\pi$ to calculate the values, which is boostrapping</li>
</ul>

<h3 id="td0-learning-example">TD(0) Learning Example</h3>

<p>Consider a simple case of a Mars Rover again:</p>

<ul>
  <li>reward being $R=[1,0,0,0,0,0,0,10]$</li>
  <li>initialize $V^\pi(s)=0$ for all $s$</li>
  <li>Our policy is $\pi(s)=a_1,\forall s$.</li>
  <li>Any action from state $s_1$ or $s_7$ gives termination</li>
  <li>take $\gamma = 1$</li>
</ul>

<p>Suppose then we got a trajectory from this policy:</p>

<ul>
  <li>$T = (s_3,a_1,0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text)$</li>
</ul>

<p><strong>Question</strong>: What is the first and every visit MC estimate of $V^\pi$?</p>

<ul>
  <li>first visit of $V^\pi(s)=[1,1,1,0,0,0,0]$</li>
  <li>every visit is the same even though $s_2$ has two updates</li>
</ul>

<p><strong>Question</strong>: What is the TD estimate of all states (init at 0, single pass) if we use $\alpha = 1$:</p>

<ul>
  <li>$[1,0,0,0,0,0,0]$. Notice that here we have forgotten the previous history/<strong>did not propagate</strong> the information of reward at $s_1$ to other states in a single episode.</li>
  <li>to propagate this information to $s_2$, we need <strong>another episode</strong> which had the tuple $s_2 \to s_1$. To propagate to $s_3$, we need yet another episode that contains $s_3 \to s_2$, etc. So this ==propagation is slow==</li>
</ul>

<h3 id="td-learning-key-limitations">TD Learning Key Limitations</h3>

<p>Graphically, temporal difference considers</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MC On Policy Algorithm</th>
      <th style="text-align: center">TD(0) On Policy Algorithm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518215716060.png" alt="image-20220518215716060" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518225606223.png" alt="image-20220518225606223" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>so we notice that TD(0):</p>

<ul>
  <li>sits between MC and DP because we are still sampling stuff, but we are updating in a DP fashion</li>
</ul>

<p>Although this algorithm allows us to update quickly:</p>

<ul>
  <li>it is a <strong>biased</strong> estimator, since our update rules are using bootstrapping (i.e. $V^\pi(s)$ we used are estimates of the true $V^\pi(s)$, hence it will just be biased)</li>
</ul>

<h2 id="comparison-between-dpmc-and-td">Comparison Between DP,MC and TD</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">DP</th>
      <th style="text-align: center">MC</th>
      <th style="text-align: center">TD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Usable when no models of current domain</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td>Handles continuing (non-episodic) domains</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td>Handles Non-Markovian domains</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td>Converges to true value in limit (assume Markov)</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td>Unbiased estimate of value</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<p>note that</p>

<ul>
  <li>for DP, it is the exact estimate.</li>
  <li>unbiased = for finite amount of data, on average it is $\theta$; consistent = for infinite amount of data it is $\theta$</li>
</ul>

<blockquote>
  <p>Note that here we are still living in the world of <strong>tabular state space</strong>, i.e. our state space is discrete. Once we move on to the case of having <strong>continuous state space</strong>, we will cover <strong>function approximation</strong> methods and a lot of them ==does not== guarantee convergence.</p>
</blockquote>

<p>In addition:</p>

<ul>
  <li><strong>MC: updates until end of episode</strong>
    <ul>
      <li>Unbiased</li>
      <li>High variance</li>
      <li>Consistent (converges to true) even with function approximation</li>
    </ul>
  </li>
  <li><strong>TD: updates per sample point $(s,a,r,s’)$ immediately</strong>
    <ul>
      <li>Some bias</li>
      <li>Lower variance</li>
      <li>TD(0) converges to true value with tabular representation</li>
      <li>TD(0) does not always converge with function approximation</li>
    </ul>
  </li>
</ul>

<h2 id="batch-mc-and-td">Batch MC and TD</h2>

<p>The aim is to use the data more efficiently, hence we might consider:</p>

<blockquote>
  <p><strong>Batch (Offline) solution</strong> for finite dataset:</p>

  <ul>
    <li>Given set of $K$ episodes</li>
    <li>Repeatedly sample an episode from $K$ (since we are offline, we can replay the episodes)</li>
    <li>Apply MC or TD(0) to the sampled episode</li>
  </ul>

  <p>The question is, ==what will MC and TD(0) converge to==?</p>
</blockquote>

<p>Consider the following example:</p>

<ul>
  <li>Two states $A, B$ with $\gamma = 1$ and a small $\alpha &lt; 1$</li>
  <li>Given 8 episodes of experience:
    <ul>
      <li>$A, 0, B, 0$</li>
      <li>$B, 1$ (observed 6 times)</li>
      <li>$B, 0$</li>
    </ul>
  </li>
</ul>

<p>Graphically we have</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220518232255338.png" alt="image-20220518232255338" style="zoom: 67%;" /></p>

<p><strong>Question</strong>: what is $V(A), V(B)$ under MC and TD(0) if we are sampling those data for infinite number of times until convergence?</p>

<ul>
  <li>$V(B)=6/8$ for MC: because in general for 6 out of 8 episodes we get $B\to 1$ in the trajectory</li>
  <li>$V(B)=6/8$ for TD(0): because in general 6 out of 8 times we get $B \to \text{Terminate}$ we will have a reward of 1</li>
  <li>$V(A)=0$ for MC: in MC setting because updates are done per episode, and the only episode that contained state $A$ had a reward of zero</li>
  <li>$V(A)\neq 0$ for TD(0): in TD settings reward propagate back across episodes. Hence once $V(B)\neq 0$ and we sampled a tuple $A\to B$, we will obtain a non-zero value for $V(A)$</li>
</ul>

<h3 id="properties-of-batch-md-and-td">Properties of Batch MD and TD</h3>

<ul>
  <li>Monte Carlo in batch setting
    <ul>
      <li>converges to min MSE (mean squared error) Minimize loss with respect to observed returns</li>
      <li>MC can be more data efficient than simple TD if markov assumption does not hold (i.e. we need to sample data in TD in order of the episode)</li>
    </ul>
  </li>
  <li>TD(0) in batch setting
    <ul>
      <li>converges to DP policy V π for the MDP with the maximum likelihood model estimates</li>
      <li>TD exploits Markov structure, if it holds then it is very data efficient</li>
    </ul>
  </li>
</ul>

<h1 id="model-free-control">Model Free Control</h1>

<p>How does an agent <strong>learn to act</strong> when it does not know how the world works, and does not aim to construct a model (model-free).</p>

<ul>
  <li>Previous section: ==Policy evaluation== with no knowledge of how the world works = how good is a specific policy?</li>
  <li>This section: ==Control (making decisions)== without a model of how the world works = how can we <strong>learn a good policy</strong>?</li>
</ul>

<p>Many applications can be modeled as a MDP:</p>

<ul>
  <li>Backgammon, Go, Robot locomation, Helicopter flight, Robocup soccer, Autonomous driving, etc.</li>
</ul>

<p>For many of these and other problems either:</p>

<ul>
  <li>MDP model is unknown but can be sampled</li>
  <li>MDP model is known but it is computationally infeasible to use directly, except through sampling (e.g. climate simulation)</li>
</ul>

<blockquote>
  <p><strong>Optimization Goal</strong>: identify a policy with high expected rewards (without model of the world). Certain features we will encounter like all RL algorithms:</p>

  <ul>
    <li><strong>Delayed consequences:</strong> May take many time steps to evaluate whether an earlier decision was good or not</li>
    <li><strong>Exploration:</strong> Necessary to try different actions to learn what actions can lead to high rewards</li>
  </ul>
</blockquote>

<p>In this section, we will discuss two types of algorithms:</p>

<ul>
  <li><strong>On-policy Learning</strong>
    <ul>
      <li>as we had for policy evaluation</li>
      <li>Direct experience</li>
      <li>Learn to estimate and evaluate a policy from experience obtained from following that policy</li>
    </ul>
  </li>
  <li><strong>Off-policy Learning</strong>
    <ul>
      <li>Learn to estimate and evaluate a policy using experience gathered from following a <strong>different</strong> policy</li>
      <li>e.g. given history $s_1,a_1,s_1,a_1$ and $s_1,a_2,s_1,a_2$, be able to <strong>extrapolate</strong> what happens if you do $s_1,a_1,s_1,a_2$</li>
    </ul>
  </li>
</ul>

<h2 id="mc-on-policy-policy-iteration">MC On-Policy Policy Iteration</h2>

<p>Recall that <strong>when we know the model</strong>, we had the following algorithm for control:</p>

<ol>
  <li>
    <p>set $i=0$</p>
  </li>
  <li>
    <p>initialize $\pi_0(s)$ randomly for all $s$</p>
  </li>
  <li>
    <p>while $i==0$ or $\vert \vert \pi_i - \pi_{i-1}\vert \vert _1 &gt; 0$ being the L1-norm</p>

    <ol>
      <li>
        <p>$V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$</p>
      </li>
      <li>
        <p>$\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)</p>

\[\pi_{i+1}(s) =  \arg\max_a \left\{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \right\} = \arg\max_a Q^{\pi_i}(s,a)\]
      </li>
    </ol>

    <p>which monotonically improves (strict inequality) the policy until optimal policy</p>
  </li>
</ol>

<blockquote>
  <p>Now, we want to do the above two steps without access to the true dynamics and reward models. Notice that essentially policy improvement does $\arg \max_a Q^\pi$, so we want to find a way to ==estimate $Q^\pi$ directly without knowing world==.</p>

  <ul>
    <li>previously we have only discussed how to do policy evaluation $V^\pi$ without the world, but ==not== how to improve it</li>
  </ul>
</blockquote>

<p>Therefore, we consider the following <strong>model-free policy iteration framework</strong>:</p>

<ol>
  <li>
    <p>initialize $\pi_0(s)$ randomly for all $s$</p>
  </li>
  <li>
    <p>repeat</p>

    <ol>
      <li>
        <p>policy evaluation: <strong>compute $Q^{\pi_i}(s,a)$</strong></p>
      </li>
      <li>
        <p>policy improvement: update $\pi_{i+1}$ by:</p>

\[\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)\]
      </li>
    </ol>
  </li>
</ol>

<p>But how do we estimate $Q^\pi$?</p>

<ul>
  <li>MC for On Policy Q Evaluation (first visit and every visit)</li>
</ul>

<h3 id="mc-for-on-policy-q-evaluation">MC for On Policy Q Evaluation</h3>

<p>The idea is basically the same as $V^\pi$ evaluation, where we consider:</p>

<ol>
  <li>
    <p>Initialize $N(s,a)=0$, $G(s,a)=0, Q^\pi(s,a)=0,\forall s \in S, \forall a \in A$</p>

    <ul>
      <li>in contrast to $V^\pi$ evaluation where we had $N(s)=0, G(s)=0$, etc.</li>
    </ul>
  </li>
  <li>
    <p>loop</p>

    <ol>
      <li>
        <p>sample an episode $i=(s_{i,1},a_{i,1},r_{i,1},s_{i,2},a_{i,2},r_{i,2},….,s_{i,T_i})$</p>
      </li>
      <li>
        <p>define $G_{i,t}$ being the return in this $i$-th episode from time $t$ onwards</p>

\[G_{i,t}=r_{i,t}+\gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + ... + \gamma^{T_i-1} r_{i,T_i}\]
      </li>
      <li>
        <p>for each <strong>state-action pair</strong> $(s,a)$ visited in the episode $i$</p>

        <ol>
          <li>for the ==first time or every time== $t$ that the pair $(s,a)$ is visited
            <ol>
              <li>increment counter of total first visits for that state $N(s,a)=N(s,a)+1$</li>
              <li>increment total return $G(s,a)=G(s,a)+G_{i,t}$</li>
              <li><strong>Update estimate</strong> $Q^\pi(s,a)=G(s,a)/N(s,a)$</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<blockquote>
  <p>Notice that a ==problem== with this algorithm: we can only evaluate $Q^\pi(s,a)$ for <strong>state-action pairs that we have experienced</strong>. And if $\pi$ is deterministic, we can’t compute $Q^\pi(s,a)$ for any $a\neq \pi(s)$. This means that we cannot say anything about new actions.</p>

  <p>The same problem goes with improvement, $\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)$ since we have initialized $Q^\pi(s,a)=0,\forall a \in A,\forall s \in S$.</p>

  <ul>
    <li>one solution to deal with this is optimistic initialization so we initialize a high $Q^\pi(s,a)$ to promote ==exploration==</li>
  </ul>
</blockquote>

<p>This means that our policy improvement will only look at actions we have taken, i.e. it ==will not explore new state-action pairs== so that</p>

\[\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)\]

<p>will only end up choosing actions visited by $s,\pi(s)$.</p>

<blockquote>
  <p>This means we may need to <strong>modify the policy evaluation algorithm</strong> to include non-deterministic state-action pairs for $Q^\pi(s,a)$</p>
</blockquote>

<h3 id="policy-evaluation-with-exploration">Policy Evaluation with Exploration</h3>

<p>Recall that the definition of $Q^\pi(s,a)$ was:</p>

\[Q^\pi(s,a) = R(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^\pi(s')\]

<p>so to <strong>find a good evaluation</strong> of $Q^\pi(s,a)$, we need to:</p>

<ul>
  <li>==explore== every possible action $a$ from state $s$, to record $R(s,a)$</li>
  <li>after that, follow policy $\pi$ to get $\gamma \sum_{s’ \in S}P(s’\vert s,a)V^\pi(s’)$</li>
</ul>

<blockquote>
  <p><strong>Insight</strong>: since we need to explore every possible action, consider the simple idea to balance exploration and exploitation</p>

\[\pi(a|s) = \begin{cases}
\pi(s),&amp; p=1-\epsilon\\
a \in A, &amp; p = \epsilon / |A|
\end{cases}\]

  <p>which is called the ==epsilon-greedy policy== <strong>w.r.t a deterministic policy $\pi(s)$.</strong></p>
</blockquote>

<p>This also means we can also define $\epsilon$-greedy policy <strong>w.r.t. a state-action value $Q(s,a)$</strong></p>

\[\pi(a|s) = \begin{cases}
\arg\max_a Q(s,a),&amp; p=1-\epsilon\\
a \in A, &amp; p = \epsilon / |A|
\end{cases}\]

<p>so that you will see we can use this to change the update rule we had, which is $\pi_{i+1} = \arg\max_a Q^{\pi_i}(s,a)$.</p>

<hr />

<p><em>For Example</em>: Mars Rovers again, with 7 states:</p>

<ul>
  <li>
    <p>Now we specify rewards for both actions as we need to compute $Q(s,a)$</p>

\[r(\cdot, a_1) = [1,0,0,0,0,0,10]\\
r(\cdot, a_2) = [0,0,0,0,0,0,5]\]
  </li>
  <li>assume current greedy policy $\pi(s)=a_1$</li>
  <li>take $\gamma =1, \epsilon = 0.5$</li>
</ul>

<p>Then, <strong>using $\epsilon$-greedy w.r.t to $\pi(s)$</strong>, we got a sampled trajectory of</p>

\[(s_3,a_1,0,s_2,a_2,0,s_3,a_1,0,   s_2,a_2,0,   s_1,a_1,1, \text{terminal})\]

<p><strong>Question</strong>: What is the first visit MC estimate of $Q$ of each $(s,a)$ pair?</p>

<ul>
  <li>$Q^{\epsilon-\pi}(\cdot,a_1)=[1,0,1,0,0,0,0]$ since we are doing MC, we propagates the end-of-episode reward to all states</li>
  <li>$Q^{\epsilon-\pi}(\cdot, a_2)=[0,1,0,0,0,0,0]$ same reason as above</li>
  <li>notice that without $\epsilon$-greedy, we would have never got $(s_2,a_2,0)$, hence we would have $Q^{\epsilon-\pi}(\cdot, a_2)=[0,0,0,0,0,0,0]$</li>
</ul>

<h3 id="epsilon-greedy-policy-improvement">$\epsilon$-greedy Policy Improvement</h3>

<p><em>Recall</em> that we previous thought of the following as the framework of Model free PI:</p>

<ol>
  <li>
    <p>set $i=0$</p>
  </li>
  <li>
    <p>initialize $\pi_0(s)$ randomly for all $s$</p>
  </li>
  <li>
    <p>while $i==0$ or $\vert \vert \pi_i - \pi_{i-1}\vert \vert _1 &gt; 0$ being the L1-norm</p>

    <ol>
      <li>
        <p>$V^{\pi_i}$ being the MDP policy evaluation of $\pi_i$</p>
      </li>
      <li>
        <p>$\pi_{i+1}$ being the policy improvement for $\pi_i$ (discuss next)</p>

\[\pi_{i+1}(s) =  \arg\max_a \left\{ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_i}(s') \right\} = \arg\max_a Q^{\pi_i}(s,a)\]
      </li>
    </ol>

    <p>which <strong>monotonically</strong> improves (strict inequality) the policy until optimal policy</p>
  </li>
</ol>

<p>And we had the <strong>problem</strong> of $Q^\pi(s,a)$ only containing updates for very small set of $(s,a)$ pairs that we followed a deterministic $\pi(s)$. This means that policy improvement will ==not try new actions==.</p>

<blockquote>
  <p>Now, the idea is to replace the update step <strong>w.r.t the stochastic $\epsilon$-greedy policy $\pi_i$,</strong> so that we consider from a $Q^{\pi_i}$ that:</p>

\[\pi_{i+1}(a|s) = \begin{cases}
\arg\max_a Q^{\pi_{i}}(s,a),&amp; p=1-\epsilon\\
a \in A, &amp; p = \epsilon / |A|
\end{cases}\]

  <p>being the ==new update rule==, which now can cover a wider range of $(s,a)$ pairs hence our updated $\pi_{i+1}$ will also contain ==new actions==.</p>
</blockquote>

<p>But does this actually provide a monotonic improvement as well (as in the determinstic case)?</p>

<blockquote>
  <p><strong>Theorem</strong>: For any $\epsilon$-greedy policy $\pi_i$, the $\epsilon$-greedy policy $\pi_{i+1}$ w.r.t $Q^{\pi_i}$ is a <strong>monotonic improvement</strong>, such that $V^{\pi_{i+1}} \ge V^{\pi_i}$</p>

  <ul>
    <li>with this theorem, we can have the <a href="#MC On Policy Improvement/Control">MC On Policy Improvement/Control</a> algorithm which uses this to provide a better estimate of $Q^\pi(s,a)$ and improve $\pi$</li>
  </ul>
</blockquote>

<p><em>Proof</em>: We just need to show that the value for taking this new policy is higher:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519162000682.png" alt="image-20220519162000682" style="zoom: 80%;" /></p>

<p>where basically:</p>

<ul>
  <li>on the fourth equality, we utilize the fact that $\sum_{a}\pi_i(a\vert s)=1$</li>
  <li>in the last equality, we basically cancelled out the $1-\epsilon$ again and obtained our result</li>
  <li>this of course <strong>assumes that you know exactly $Q^{\pi_i}$ for a policy $\pi_i$</strong>.
    <ul>
      <li>in case when you have an ==estimate== of $Q^{\pi_i}$, then there is ==no guarantee== of monotonic policy improvement. This will happen a lot in function approximation when we have a continuous state space.</li>
    </ul>
  </li>
</ul>

<p>This shows that by following $\pi_{i+1}$ the first step and then following $\pi_i$, we get a higher value. Then, we can show $V^{\pi_{i+1}} \ge V^{\pi_i}$ in the similar fashion as we proved the policy improvement by pushing out the $\pi_{i+1}$ policy to future terms.</p>

<hr />

<p>Finally, we need guarantees on convergence</p>

<blockquote>
  <p>**GLIE **(Greedy in the Limit of Infinite Exploration):</p>

  <ul>
    <li>
      <p>all state-action pairs are visitied an infintie number of times</p>

\[\lim_{i\to \infty} N_i(s,a) = \infty\]

      <p>(satisfied as we have stochastic policy)</p>
    </li>
    <li>
      <p>Behavior policy (the one we used to sample trajectory) <strong>converges to greedy policy</strong> (deterministic)</p>

\[\lim_{i \to \infty} \pi(a|s) \to \arg\max Q^\pi(s,a)\]
    </li>
  </ul>

</blockquote>

<p>A practice, instead of having $i \to \infty$, we can also reduce $\epsilon$ to have $\epsilon_i = 1/i$ to also satisfy GLIE (over time we have $0$ prob for exploration).</p>

<blockquote>
  <p><strong>Theorem</strong>: GLIE Monte-Carlo control converges to the optimal state-action value function</p>

\[Q(s,a) \to Q^*(s,a)\]

  <p>from which we can find the optimal policy easily by taking $\arg\max_a Q^*(s,a)$</p>
</blockquote>

<h3 id="mc-on-policy-improvementcontrol">MC On Policy Improvement/Control</h3>

<p>Finally, putting everything together, we have the MC On-policy Policy Improvement algorithm as:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519164409454.png" alt="image-20220519164409454" style="zoom: 60%;" /></p>

<p>which basically combines:</p>

<ul>
  <li>
    <p><a href="#MC for On Policy Q Evaluation">MC for On Policy Q Evaluation</a></p>
  </li>
  <li>
    <p><a href="#$\epsilon$-greedy Policy Improvement">$\epsilon$-greedy Policy Improvement</a></p>
  </li>
  <li>
    <p>and $\epsilon$-Greedy$(Q)$ means that</p>

\[\pi_{k}(a|s) = \begin{cases}
\arg\max_a Q(s,a),&amp; p=1-\epsilon\\
a \in A, &amp; p = \epsilon / |A|
\end{cases}\]
  </li>
</ul>

<hr />

<p><em>For Example</em>: Mars Rovers again, with 7 states:</p>

<ul>
  <li>
    <p>Now we specify rewards for both actions as we need to compute $Q(s,a)$</p>

\[r(\cdot, a_1) = [1,0,0,0,0,0,10]\\
r(\cdot, a_2) = [0,0,0,0,0,0,5]\]
  </li>
  <li>assume current greedy policy $\pi(s)=a_1$</li>
  <li>take $\gamma =1, \epsilon = 0.5$</li>
</ul>

<p>Then, <strong>using $\epsilon$-greedy w.r.t to $\pi(s)$</strong>, we got a sampled trajectory of</p>

\[(s_3,a_1,0,s_2,a_2,0,s_3,a_1,0,   s_2,a_2,0,   s_1,a_1,1, \text{terminal})\]

<p>So we know the first visit MC estimate of $Q$ of each $(s,a)$ pair gives:</p>

<ul>
  <li>$Q^{\epsilon-\pi}(\cdot,a_1)=[1,0,1,0,0,0,0]$ since we are doing MC, we propagates the end-of-episode reward to all states</li>
  <li>$Q^{\epsilon-\pi}(\cdot, a_2)=[0,1,0,0,0,0,0]$ same reason as above</li>
</ul>

<p>now, given the policy evaluation:</p>

<p><strong>Question</strong>: what is the greedy policy $\pi(s)$ w.r.t this $Q^{\epsilon - \pi}$?</p>

<ul>
  <li>simply $\pi(s) = [a_1,a_2,a_1,\text{tie},\text{tie},\text{tie},\text{tie}]$</li>
</ul>

<p><strong>Question</strong>: therefore, what is the new improved policy $\pi_{k+1}=\epsilon\text{-greedy}(Q)$ if $k=3$?</p>

<ul>
  <li>
    <p>this means $\epsilon=1/3$</p>
  </li>
  <li>
    <p>then, from the above result, this means</p>

\[\pi_{k+1}(a|s) = \begin{cases}
[a_1,a_2,a_1,\text{tie},\text{tie},\text{tie},\text{tie}], &amp; p=2/3\\
a \in A, &amp; p = 1/6
\end{cases}\]
  </li>
</ul>

<h2 id="td-on-policy-policy-iteration">TD On-Policy Policy Iteration</h2>

<p><em>Recall that</em> the general framework for policy iteration in a model-free case</p>

<ol>
  <li>initialize $\pi_0(a\vert s)$ randomly for all $s$</li>
  <li>repeat
    <ol>
      <li>policy <strong>evaluation</strong>: compute $Q^{\pi_i}(s,a)$</li>
      <li>policy <strong>improvement</strong>: update $\pi_{i+1}$ using $Q^{\pi_i}$</li>
    </ol>
  </li>
</ol>

<blockquote>
  <p>Recall that in the section <a href="#Policy Evaluation with Unknown World">Policy Evaluation with Unknown World</a>, we had <strong>two ways of doing policy evaluation</strong>, either doing MC or doing TD. We have come up with a way to compute $Q^\pi(s,a)$ using MC method, therefore, another variant is to use TD method.</p>
</blockquote>

<p>Hence, the general algorithm for TD based On-Policy Policy Iteration looks like</p>

<ol>
  <li>
    <p>initialize $\pi_0(a\vert s)$ randomly for all $s$</p>
  </li>
  <li>
    <p>repeat</p>

    <ol>
      <li>
        <p>policy <strong>evaluation ==using TD==</strong>: compute $Q^{\pi_i}(s,a)$</p>
      </li>
      <li>
        <p>policy <strong>improvement (same as MC)</strong>: update $\pi_{i+1}$ using $Q^{\pi_i}$ by</p>

\[\pi_{i+1} = \epsilon\text{-Greedy}(Q^{\pi_i})\]
      </li>
    </ol>
  </li>
</ol>

<h3 id="sarsa">SARSA</h3>

<p>Essentially using TD for policy evaluation:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519171606359.png" alt="image-20220519171606359" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>the key change is the update rule</p>

\[Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma Q(s_{t+1},a_{t+1})]}_{\text{TD target}}-Q(s_t,a_t))\]

    <p>which can perform update per sample instead of waiting until end-of-episode (in MC case).</p>
  </li>
  <li>
    <p>therefore, notice that the policy are updated much more frequently as well (as compared to the MC case)</p>
  </li>
  <li>
    <p>the $\alpha$ used is also often called the ==learning rate== in this context</p>
  </li>
</ul>

<blockquote>
  <p>Note that we discarded the notation of $Q^\pi$ here and used $Q$, as we are frequently changing the policy when computing this running estimate to converge to $Q^*(s,a)$</p>
</blockquote>

<h3 id="convergence-properties-of-sarsa">Convergence Properties of SARSA</h3>

<blockquote>
  <p><strong>Theorem</strong>: SARSA for finite-state and finite-action MDPs <strong>converges</strong> to the optimal action-value, $Q(s,a)\to Q^*(s,a)$ ,under the following conditions:</p>

  <ul>
    <li>
      <p>the policy sequence $\pi_t(a\vert s)$ satisfies GLIE</p>
    </li>
    <li>
      <p>the step size/learning rate $\alpha_t$ satisfy the Robbins-Munro sequence such that</p>

\[\sum_{t=1}^\infty \alpha_t =\infty\\
\sum_{t=1}^\infty \alpha_t^2 &lt; \infty\]

      <p>an example would be $\alpha_t = 1/t$</p>
    </li>
  </ul>
</blockquote>

<p>But as the above are conditions <strong>sufficient</strong> to guarantee convergence, in reality</p>

<ul>
  <li>we could also <strong>simply use $\alpha$ being some small constants</strong> and empirically they often converge as well</li>
  <li>there are also some domains that is <strong>hard to satisfy GLIE</strong> (e.g. helicopter crashed, can no longer visit some other states). Some research has been working on how to deal with that, but for now we will not worry about it.</li>
</ul>

<h3 id="q-learning">Q-Learning</h3>

<p>The only change will be in SARSA</p>

\[Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma Q(s_{t+1},a_{t+1})]}_{\text{TD target}}-Q(s_t,a_t))\]

<p>but <strong>for Q-learning we consider</strong> the update rule:</p>

\[Q(s_t,a_t) =Q(s_t,a_t)+\alpha( \underbrace{[r_t+\gamma \max_{a'} Q(s_{t+1},a')]}_{\text{TD target}}-Q(s_t,a_t))\]

<p>so that</p>

<ul>
  <li>
    <p>instead of taking $a_{t+1}\sim \pi(a\vert s_{t+1})$, which is being ==realistic== (in SARSA), we are being ==optimistic== so that we update our $Q(s_t,a_t)$ by the <strong>best value we could possibly get</strong> using $\max_{a’} Q(s_{t+1},a’)$</p>
  </li>
  <li>
    <p>this means that in cases where we have lots of negative reward/risks in <strong>early stages</strong> (e.g. cliff walking), SARSA would produce better policy whereas Q-Learning would produce more risky policy</p>
  </li>
</ul>

<hr />

<p>Hence the entire algorithm looks like</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519175251084.png" alt="image-20220519175251084" style="zoom:67%;" /></p>

<p>where notice that:</p>

<ul>
  <li>we no longer need the $a_{t+1}$ in the five tuple $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$ in SARSA, since we will be taking the next best action</li>
  <li>since we are being optimistic, does how Q is initialized matter? Asymptotically no, under mild conditions, but at the beginning, yes</li>
  <li>finally, like SARSA it is a policy based on TD, this means that the final reward will only <strong>propagate slowly</strong> to other states.</li>
</ul>

<h3 id="convergence-properties-of-q-learning">Convergence Properties of Q-Learning</h3>

<p>What conditions are sufficient to ensure that Q-learning with $\epsilon$-greedy exploration <strong>converges to optimal $Q^*$</strong> ?</p>

<ul>
  <li>Visit all $(s, a)$ pairs infinitely often</li>
  <li>and the step-sizes αt satisfy the Robbins-Munro sequence.</li>
  <li>Note: the algorithm does not have to be greedy in the limit of infinite exploration (GLIE) to satisfy this (could keep $\epsilon$ large).</li>
</ul>

<p>What conditions are sufficient to ensure that Q-learning with $\epsilon$-greedy exploration <strong>converges to optimal $\pi^*$ ?</strong></p>

<ul>
  <li>The algorithm is GLIE (i.e. policy being more and more greedy over time), along with the above requirement to ensure the Q value estimates converge to the optimal Q.</li>
</ul>

<h3 id="maximization-bias">Maximization Bias</h3>

<p>Because Q-Learning exploits the greedy action early on, we could have a <strong>maximization bias</strong> when estimating the value of a policy.</p>

<p>Consider the case of a single state MDP, so that $\vert S\vert =1$, with two actions. And suppose the two actions both have mean random rewards of zero: $\mathbb{E}[r\vert a=a_1]=\mathbb{E}[r\vert a=a_2]=0$</p>

<ul>
  <li>then this means that $Q(s,a_1)=Q(s,a_2)=0=V(s)$ is the true estimate and is optimal.</li>
</ul>

<p>In practice, we can only do finite samples taking action $a_1,a_2$. Let $\hat{Q}(s,a_1),\hat{Q}(s,a_2)$ be finite sample estimate of $Q$</p>

<ul>
  <li>
    <p>suppose we used an <strong>unbiased estimate of $\hat{Q}(s,a)$</strong> by taking the mean</p>

\[\hat{Q}(s,a_1) = \frac{1}{N(s,a_1)}\sum_{i=1}^{N(s,a_1)}r_i(s,a_1)\]
  </li>
  <li>
    <p>Let $\hat{\pi}=\arg\max \hat{Q}(s,a)$ be the greedy policy w.r.t the estimated $\hat{Q}$</p>
  </li>
  <li>
    <p>then we notice that:</p>

\[\begin{align*}
\hat{V}^{\hat{\pi}}
&amp;=\mathbb{E}[\max (\hat{Q}(s,a_1),\hat{Q}(s,a_2))]\\
&amp;\ge\max(\mathbb{E}[\hat{Q}(s,a_1)],\mathbb{E}[\hat{Q}(s,a_2)]\\
&amp;= \max[0,0]\\
&amp;= V^\pi
\end{align*}\]

    <p>meaning we have a <strong>biased estimator</strong> of $V^{\hat{\pi}}$ for the optimal policy.</p>
  </li>
</ul>

<p>To solve this, the idea is to instead split samples and use to create two independent unbiased estimates of $Q_1(s_1, a_i)$ and $Q_2(s_1, a_i),\forall a$.</p>

<ul>
  <li>Use one estimate to select max action: $a^* = \arg\max_a Q_1(s_1,a)$</li>
  <li>Use other estimate to estimate value of $a^<em>:Q_2(s,a^</em>)$</li>
  <li>Yields unbiased estimate: $\mathbb{E}[Q_2(s,a^<em>)]=Q(s,a^</em>)$</li>
</ul>

<p>This therefore gives birth to the double Q-Learning algorithm</p>

<h3 id="double-q-learning">Double Q-Learning</h3>

<p>So the only difference is now we have <strong>two $Q$ estimates:</strong></p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519181511825.png" alt="image-20220519181511825" style="zoom:67%;" /></p>

<p>where in practice compared to Q-Learning:</p>

<ul>
  <li>doubles the memory</li>
  <li>same computation requirements</li>
  <li>data requirements are subtle– might reduce amount of exploration needed due to lower bias</li>
</ul>

<p>Additionally</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220519181621394.png" alt="image-20220519181621394" style="zoom:67%;" /></p>

<h1 id="value-function-approximation">Value Function Approximation</h1>

<p>In the previous section, we discussed how to do <strong>control</strong> (make decisions) <strong>without a known model</strong> of how the world works:</p>

<ul>
  <li>learning a good policy from experience (sampled episodes)</li>
  <li>update $Q$ estimate using a ==tabular representation==: ==finite== number of state-action pair</li>
</ul>

<p>However, as you can imagine many real world problems have <strong>enormous state and/or action space</strong> so that we cannot really tabulate all possible values. So we need to somehow ==generalize== to those unknown state-actions.</p>

<blockquote>
  <p><strong>Aim</strong>: even if we encounter state-action pairs <em>not</em> met before, we want to make <em>good decisions</em> by past experience.</p>
</blockquote>

<blockquote>
  <p><strong>Value Function Approximation</strong>: represent a (state-action/state) value function <strong>with a parametrized function</strong> instead of a table, so that even if we met an inexperienced state/state-action, we can get some values.</p>
</blockquote>

<p>So we imagine</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521111857200.png" alt="image-20220521111857200" style="zoom: 50%;" /></p>

<p>so that essentially we are parameterized the function with weights $W$ as in a deep neural network.</p>

<p>Using such an approach has the following benefits:</p>

<ul>
  <li>Reduce memory needed to store $(P,R)/V/Q/\pi$</li>
  <li>Reduce computation needed to compute  $(P,R)/V/Q/\pi$</li>
  <li>Reduce experience/data needed to find a good  $(P,R)/V/Q/\pi$</li>
</ul>

<p>but this will <em>usually</em> be a tradeoff:</p>

<ul>
  <li><strong>representational capacity</strong> (of the DNN to represent states) v.s. memory/computation/data needed</li>
</ul>

<p>Then most importantly, we need to consider <strong>what class of functions</strong> do we consider for such an approximation? Many possible function approximators including</p>

<ul>
  <li>Linear combinations of features</li>
  <li>Neural networks</li>
  <li>Decision trees (useful for being highly interpretable)</li>
  <li>Nearest neighbors</li>
  <li>Fourier/ wavelet bases</li>
</ul>

<blockquote>
  <p>But here we will focus on function approximators that are <strong>differentiable</strong>, so that we can easily optimize for (e.g. gradient descent). Two popular classes of differentiable function approximators include:</p>

  <ul>
    <li><strong>linear feature representations</strong> (this section)</li>
    <li><strong>neural networks</strong> (next section)</li>
  </ul>

  <p>Also notice that since we are now doing gradient descent, our approximator will be local optimas instead of the global optimas.</p>
</blockquote>

<h2 id="vfa-for-policy-evaluation">VFA for Policy Evaluation</h2>

<blockquote>
  <p><strong>Aim</strong>: find the best approximate representation of $V^\pi$ using a parametrized function $\hat{V}$.</p>
</blockquote>

<h3 id="vfa-for-policy-evaluation-with-oracle">VFA for Policy Evaluation with Oracle</h3>

<p>In this case, we consider:</p>

<ul>
  <li>given a policy $\pi$ to evaluate</li>
  <li>assume that there is an ==oracle== that returns the <strong>true value for $V^\pi(s)$</strong></li>
</ul>

<p>so we basically want our function approximation to look like the true $V^\pi(s)$ in our space.</p>

<p>Then we can simply perform gradient descent type methods. For instance,</p>

<ul>
  <li>
    <p><strong>loss function</strong></p>

\[L(w) = \frac{1}{2} \mathbb{E}_\pi[(V^\pi(s) - \hat{V}(s;w))^2]\]

    <p>where $\mathbb{E}_\pi$ means expected value over the <strong>distribution of states</strong> under current policy $\pi$</p>
  </li>
  <li>
    <p><strong>then gradient update is</strong></p>

\[w:=w - \alpha \left( \nabla_w L(w) \right)\]

    <p>and</p>

\[\nabla_w L(w) = \mathbb{E}_\pi[(V^\pi(s)- \hat{V}(s;w))\nabla_w \hat{V}(s)]\]
  </li>
</ul>

<p>Then, for instance SGD considers using some batched/single sample to approximate the expected value:</p>

\[\nabla_w L(w) \approx (V^\pi(s)- \hat{V}(s;w))\nabla_w \hat{V}(s)\]

<p>for some sampled $s,V^\pi(s)$.</p>

<hr />

<p>If we are considering a <strong>Linear Value Function Approximation</strong> with an Oracle, then simply we consider</p>

\[\hat{V}(s;w) = \vec{x}(s)^T \vec{w}\]

<p>for $\vec{x}(s)$ being a representation of our state. Then we acn also show the gradient to be:</p>

\[\nabla_w L(w) = \mathbb{E}_\pi[(V^\pi(s)- \hat{V}(s;w)) \vec{x}(s)]\]

<p>since $\nabla_w \hat{V} = \vec{x}(s)$ in this case.</p>
<h3 id="model-free-vfa-policy-evaluation">Model Free VFA Policy Evaluation</h3>

<p>Obviously most of the time we do not have an oracle, which is like knowing the model of the world already.</p>

<blockquote>
  <p><strong>Aim</strong>: do model-free value function approximation for prediction/evaluation/policy evaluation <strong>without a model/oracle</strong></p>

  <ul>
    <li>recall that this means given a fixed policy $\pi$</li>
    <li>estimate its $V^\pi$ or $Q^\pi$</li>
  </ul>
</blockquote>

<p><em>Recall</em> that we have done model-free policy evaluation using tabular methods in:</p>

<ul>
  <li><a href="#Monte Carlo Policy Evaluation">Monte Carlo Policy Evaluation</a>: update estimate after each episode</li>
  <li><a href="#Temporal Difference Learning">Temporal Difference Learning</a>: update estimate after each step</li>
</ul>

<p>both of which essentially does:</p>

<ul>
  <li>maintain a look up table to store current estimates $V^\pi$ or $Q^\pi$</li>
  <li>Updated these estimates after each episode (Monte Carlo methods) or after each step (TD methods)</li>
</ul>

<hr />

<blockquote>
  <p>In VFA, we can basically ==change the update step== to be ==fitting the function approximation== (e.g. gradient descent)</p>
</blockquote>

<p>This means that we need to have prepared:</p>

<ul>
  <li>
    <p>a feature vector to <strong>represent a state $s$</strong></p>

\[x(s) = \begin{bmatrix}
x_1(s)\\
x_2(s)\\
\dots\\
x_n(s)
\end{bmatrix}\]

    <p>for instance, for robot navigation, it can be a 180-dimensional vector, with each cell representing the distance to the first detected obstacle. 
However, notice that this representation also means that it is ==not markov==, as in different hallways (true states) you could have the same feature vector. But it could be still a ==good representation to condition our decision on==.</p>
  </li>
  <li>
    <p>choose a class of function approximators to approximate the value function</p>
    <ul>
      <li>linear function</li>
      <li>neural networks</li>
    </ul>
  </li>
</ul>

<h4 id="mc-value-function-approximation">MC Value Function Approximation</h4>

<p>Recall that for MC methods, we used the following update rule for value function updates</p>

\[V^\pi(s)=V^\pi(s)+\alpha\left( G_{i,t}-V^\pi(s) \right)\]

<p>so that our ==target is $G_t$==, which is an <strong>unbiased</strong> but noisy estimate of the true value of $V^\pi(s_t)$.</p>
<blockquote>
  <p><strong>Idea</strong>: therere we treat $G_t$ being the “oracle” fo $V^\pi(s_t)$, from which we can get a loss and update our approximator.</p>
</blockquote>

<p>This then means our gradient update (for SGD is):</p>

\[\begin{align*}
\nabla_w L(w) 
&amp;=(V^\pi(s_t)- \hat{V}(s_t;w))\nabla_w \hat{V}(s_t)\\
&amp;\approx (G_t- \hat{V}(s_t;w))\nabla_w \hat{V}(s_t)
\end{align*}\]

<p>If we are using a linear function:</p>

\[\nabla_w L(w) \approx (G_t- \vec{x}(s_t)^T \vec{w}) \vec{x}(s_t)\]

<p>and then just use $\vec{w} := \vec{w} - \alpha \nabla_w L(w)$ to do descent.</p>

<blockquote>
  <p>This means that we essentially reduce MC VFA to doing ==supervised learning== on a set of (state,return) pairs: $(s_1, G_1), (s_2, G_2), …, (s_T, G_t)$</p>
</blockquote>

<p>Therefore our algorithm with MC updates looks like</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521130409230.png" alt="image-20220521130409230" style="zoom: 50%;" /></p>

<p>where notice that:</p>
<ul>
  <li>of course you can also have an every-visit version by changing line 5</li>
  <li>since we have a finite episode, we used $\gamma=1$ in line 6</li>
</ul>

<hr />

<p><em>For instance</em>: consider a 7 state space with the transition looking like:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521144341630.png" alt="image-20220521144341630" style="zoom:50%;" /></p>

<p>so that:</p>

<ul>
  <li>state $\vec{x}(s_1) = [2,0,0,0,0,0,1]$ so that $\vec{x}(s_1)^T \vec{w} = 2w_1+w_8$, etc.</li>
  <li>there are two states, $a_1$ being the solid lines and $a_2$ being the dashed lines.</li>
  <li>dashed transition means that here we have a uniform probability $1/6$ to be transitioned to any state $s_i$ for $i \in [1,6]$.</li>
  <li>we assume some small termination probability from state $s_7$, but not shown on the diagram</li>
  <li>all states have zero reward (so the optimal value function is $V^\pi = 0$)</li>
</ul>

<p>Suppose we have sampled the following episode:</p>
<ul>
  <li>$s_1, a_1, 0, s_7, a_1, 0, s_7, a_1, 0, \text{terminal}$</li>
  <li>we are using a <strong>linear function</strong></li>
  <li>we have initialized all weights to one $\vec{w}_0 = [1,1,1,1,1,1,1]$</li>
  <li>$\alpha = 0.5$, take $\gamma = 1$</li>
</ul>

<p><strong>Question</strong>: what is the MC estimate of $V^\pi(s_1)$? $\vec{w}$ after one update?</p>

<ul>
  <li>first we need $G_1$, which in this case is $0$</li>
  <li>our current estimate for $V^\pi(s_1)$ is $\vec{x}(s_1)^T \vec{w}_0 = 2+1=3$</li>
  <li>
    <p>therefore, our update for one step is:</p>

\[\Delta w = -\alpha \cdot (0 - 3) \cdot \vec{x}(s_1) =-1.5\vec{x}(s_1)=[-3,0,0,0,0,0,-1.5]\]
  </li>
  <li>
    <p>finally, our new weight is therefore $w + \Delta w$:</p>

\[\vec{w}_1 = [-2,1,1,1,1,1,-0.5]\]

    <p>so essentially we are doing SGD per state in the episode.</p>
  </li>
</ul>

<p>But does such an update ==converge to the right thing==?</p>

<h4 id="convergence-for-mc-linear-value-function-approximation">Convergence for MC Linear Value Function Approximation</h4>

<p>Recall that if provided a policy, then MDP problem is reduced to a Markov Reward Process (by following that policy). Therefore, if we eventually sample many episodes, we get a <strong>probability distribution</strong> over states $d(s)$, such that:</p>
<ul>
  <li>$d(s)$ is the stationary distribution over states following $\pi$</li>
  <li>then obviously $\sum_s d(s) = 1$</li>
</ul>

<p>Since it is stationary, this means that the distribution after a single transition gives the same $d(s)$:</p>

\[d(s') = \sum_s \sum_a \pi(a|s)p(s'|s,a)d(s)\]

<p>must hold, if the markov process has ran long enough.</p>

<p>Using this distribution, we can consider the ==mean square error== of our estimators for a particular policy $\pi$:</p>

\[MSVE(\vec{w}) = \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2\]

<p>for a linear function, we use $\hat{V}^\pi(s;\vec{w}) = \vec{x}(s)^T \vec{w}$.</p>

<blockquote>
  <p><strong>Theorem</strong>: MC policy evaluation with VFA converges to the weights $\vec{w}_{MC}$ which has the ==minimum mean squared error== possible:</p>

\[MSVE(\vec{w}_{MC}) = \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2\]

  <p>note that the error might not be zero, e.g. using a linear approximator has only a small capacity.</p>
</blockquote>

<h4 id="batch-mc-value-function-approximation">Batch MC Value Function Approximation</h4>

<p>The SGD version basically performs an update per sample, which is suitable for online scenario. However, often we could get a set of episodes already sampled from using a policy $\pi$. Then we can perform a better weight update by considering:</p>

\[\arg\min_\vec{w} \sum_{i=1}^N (G(s_i)- \vec{x}(s_i)^T \vec{w})^2\]

<p>to approximate the expected value version of the MSVE. Then the optimal weights can be solved <strong>directly</strong> to be:</p>

\[\vec{w} = (X^TX)^{-1} X^T \vec{G}\]

<p>for:</p>
<ul>
  <li>$\vec{G}$ is a vector of all $N$ returns</li>
  <li>$X$ is a matrix of the features of each of the $N$ states $\vec{x}(s_i)$</li>
  <li>but of course this would be memory intensive as we need to store all $N$ states and returns.</li>
  <li>finally, you can obviously have something in between SGD and this full batch.</li>
</ul>

<h4 id="td-learning-with-value-function-approximation">TD Learning with Value Function Approximation</h4>

<p>First recall that TD method considers bootstrapping and sampling to approximate $V^\pi$, so that the update rule is based on per sample:</p>

\[V^\pi(s) = V^\pi(s) + \alpha (r + \gamma V^\pi(s') - V^\pi(s))\]

<p>with the ==target== being $r+\gamma V^\pi(s’)$, which is a biased estimate of the true $V^\pi$.</p>

<blockquote>
  <p><strong>Idea</strong>: use function to represent $V^\pi$, and use boostrapping + sampling in TD method, with the same update rule shown above.</p>
</blockquote>

<p>(recall that we are still on-policy, we are evaluating the value of a given policy $\pi$ and all sampled data + estimation are on the same policy)</p>

<blockquote>
  <p>Therefore, essentially we now consider TD larning being a supervised learning on the set of data pairs:</p>

\[(s_1, r_1 + \hat{V}^\pi(s_2;\vec{w})), (s_2, r_2 + \hat{V}^\pi(s_3;\vec{w})), ...\]

  <p>then the MSE loss is simply:</p>

\[L(\vec{w}) = \mathbb{E}_\pi [ (r_j + \gamma \hat{V}^\pi(s_{j+1},\vec{w})) -\hat{V}(s_j; \vec{w})]\]

  <p>Hence our gradient step with a SGD update (replacing mean with a single sample) is:</p>

\[\Delta w = \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \nabla_w \hat{V}^\pi(s;\vec{w})\]

  <p>again, with target being $r+\gamma \hat{V}^\pi(s’;\vec{w})$</p>
</blockquote>

<p>In the case of a linear function, then we have:</p>

\[\begin{align*}
\Delta w 
&amp;= \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \nabla_w \hat{V}^\pi(s;\vec{w})\\
&amp;= \alpha \cdot (r + \hat{V}^\pi(s';\vec{w}) - V^\pi(s;\vec{w})) \cdot \vec{x}(s)\\
&amp;= \alpha \cdot (r + \vec{x}(s')^T \vec{w} - \vec{x}(s)^T \vec{w}) \cdot \vec{x}(s)\\
\end{align*}\]

<p>so we are boostrapping our target using our current estimate of $V^\pi(s)$.</p>

<p>Finally, the algorithm therefore looks like</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521160015879.png" alt="image-20220521160015879" style="zoom:50%;" /></p>

<hr />

<p><em>For instance</em>: we can consider the same example with MC case to compare the difference:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521144341630.png" alt="image-20220521144341630" style="zoom:50%;" /></p>

<p>so that:</p>

<ul>
  <li>state $\vec{x}(s_1) = [2,0,0,0,0,0,1]$ so that $\vec{x}(s_1)^T \vec{w} = 2w_1+w_8$, etc.</li>
  <li>there are two states, $a_1$ being the solid lines and $a_2$ being the dashed lines.</li>
  <li>dashed transition means that here we have a uniform probability $1/6$ to be transitioned to any state $s_i$ for $i \in [1,6]$.</li>
  <li>we assume some small termination probability from state $s_7$, but not shown on the diagram</li>
  <li>all states have zero reward (so the optimal value function is $V^\pi = 0$)</li>
</ul>

<p>Suppose we have sampled the following episode:</p>

<ul>
  <li>$s_1, a_1, 0, s_7, a_1, 0, s_7, a_1, 0, \text{terminal}$</li>
  <li>we are using a <strong>linear function</strong></li>
  <li>we have initialized all weights to one $\vec{w}_0 = [1,1,1,1,1,1,1]$</li>
  <li>$\alpha = 0.5$, take $\gamma = 0.9$</li>
</ul>

<p><strong>Question</strong>: what is the TD estimate of $V^\pi(s_1)$? $\vec{w}$ after one update of $(s_1,a_1,0,s_7)$?</p>

<ul>
  <li>
    <p>using the update formula:</p>

\[\Delta \vec{w} = \alpha (0+ 0.9 * 3 - 3)\vec{x}(s_1) = -0.3\alpha \vec{x}(s_1)\]

    <p>which is a much smaller weight update than the MC update.</p>
  </li>
</ul>

<h4 id="convergence-for-td-linear-value-function-approximation">Convergence for TD Linear Value Function Approximation</h4>

<p>As mentioned in the MC case, we consider the MSVE for our estimators for a particular policy $\pi$:</p>

\[MSVE(\vec{w}) = \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2\]

<p>for a linear function, we use $\hat{V}^\pi(s;\vec{w}) = \vec{x}(s)^T \vec{w}$.</p>

<blockquote>
  <p><strong>Theorem</strong>: TD policy evaluation with VFA converges to the weights $\vec{w}_{TD}$ is ==within a constant factor== of minmum mean squared error possible:</p>

\[MSVE(\vec{w}_{TD}) \le \frac{1}{1-\gamma} \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2\]

  <p>so it is slightly worse than MC method as it is biased, but it <strong>updates/converges much faster</strong>.</p>
</blockquote>

<p>As mentioned before, this happens also because we are using</p>
<ul>
  <li>some feature representation for a state which might be a <strong>subspace</strong> of the true space of states</li>
  <li>in TD we are <strong>bootstrapping</strong>, which gives rise to bias and error</li>
</ul>

<p>But this also means that if we have some one-hot encoded feature, <strong>one for each state</strong>, then:</p>

\[\min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2 = 0\]

<p>as you can have a unique value per state, even with a linear function. Therefore:</p>
<ul>
  <li>
    <p>if we used a MC method, then simply:</p>

\[MSVE(\vec{w}_{MC})= \min_w \sum_{s\in S} d(s)\cdot (V^\pi(s) - \hat{V}^\pi(s;\vec{w}))^2 = 0\]
  </li>
  <li>
    <p>if we used a TD method, then:</p>

\[MSVE(\vec{w}_{TD})\le 0\]

    <p>is also optimal, and MC v.s. TD has no difference.</p>
  </li>
</ul>

<p>(of course this also means we have enough data)</p>

<blockquote>
  <p>If our state representation is a <strong>subspace</strong> of the true state space, then using a TD might incur some error (due to bootstrapping error). But if the state representation is larger or equal to the true state space, then using a TD and MC has no difference.</p>
</blockquote>

<h2 id="control-using-vfa">Control using VFA</h2>

<p>Now for control, essentially we consider moving from policy evaluation (previous sections) to policy iteration. This is basically achieved by:</p>

<ol>
  <li><strong>estimate $Q^\pi(s,a)$</strong> instead, using MC or TD technique</li>
  <li>perform <strong>$\epsilon$-greedy policy improvement</strong></li>
</ol>

<p>However, this can get unstable because we had the folllowing components:</p>
<ul>
  <li>function approximation (has uncertainty)</li>
  <li>bootstrapping (has uncertainty)</li>
  <li><strong>off-policy</strong> learning for policy improvement (has the biggest uncertainty)
    <ul>
      <li>we are changing the policy over-time, so no longer have a good estimate of the stationary distribution over states of a particular policy</li>
    </ul>
  </li>
</ul>

<p>But again, lets first go through the algorithm.</p>

<h3 id="action-value-function-approximation-with-an-oracle">Action-Value Function Approximation with an Oracle</h3>

<blockquote>
  <p><strong>Aim</strong> approximate $Q^\pi(s,a)$ given a policy $\pi$, by $\hat{Q}^\pi(s,a)$</p>
</blockquote>

<p>The idea is the same as what we see in <a href="#Model Free VFA Policy Evaluation">Model Free VFA Policy Evaluation</a>. Given an oracle $Q^\pi(s,a)$ which spits out the true value, our loss is simply:</p>

\[L(\vec{w}) = \frac{1}{2} \mathbb{E}_\pi [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;\vec{w}))^2]\]

<p>Then for stochastic gradient descent method, the gradient looks like:</p>

\[\begin{align*}
\nabla_w L(w) 
&amp;= \mathbb{E}_w [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)] \\
&amp;\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
\end{align*}\]

<p>for some sampled $s,a$ pair.</p>

<p>Finally, the features would then should include both a state and action pair:</p>

\[\vec{x}(s,a) = \begin{bmatrix}
  x_1(s,a)\\
  x_2(s,a)\\
  \vdots\\
  x_n(s,a)
\end{bmatrix}\]

<hr />

<p>For a <strong>Linear State Action Value Function Approximation</strong>, we consider simply:</p>

\[\hat{Q}(s,a;\vec{w}) = \vec{x}^T(s,a)\vec{w}\]

<p>and the rest is trivial.</p>

<h3 id="model-free-vfa-control">Model Free VFA Control</h3>

<p>Similar to how we did <a href="#Model Free VFA Policy Evaluation">Model Free VFA Policy Evaluation</a>, the idea is to approximate the oracle with ==some target==.</p>
<ul>
  <li>
    <p>in MC methods, the target is simply $G_t$, so we consider:</p>

\[\begin{align*}
\nabla_w L(w) 
&amp;\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
&amp;\approx (G_t - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
\end{align*}\]
  </li>
  <li>
    <p>for SARSA, we used a TD target of $r + \gamma \hat{Q}^\pi(s’,a’;\vec{w})$ which leverages the current function approximation value to bootstrap:</p>

\[\begin{align*}
\nabla_w L(w) 
&amp;\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
&amp;\approx (r + \gamma \hat{Q}^\pi(s',a';\vec{w})- \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
\end{align*}\]

    <p>and you see for the above update we need $s,a,r,s’,a’$</p>
  </li>
  <li>
    <p>for Q-Learning, we use the TD target of $r + \gamma \max_{a’}\hat{Q}^\pi(s’,a’;\vec{w})$, which is optimistic and update gradient is:</p>

\[\begin{align*}
\nabla_w L(w) 
&amp;\approx (Q^\pi(s,a) - \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)\\
&amp;\approx (r + \gamma \max_{a'}\hat{Q}^\pi(s',a';\vec{w})- \hat{Q}^\pi(s,a;w) ) \nabla_w \hat{Q}^\pi(s,a;w)
\end{align*}\]

    <p>which we only needs $s,a,r,s’$</p>
  </li>
</ul>

<blockquote>
  <p>For all of the above essentially $\hat{Q}(s,a)$ is a neural network, approximating a function of the feature vectors $\vec{x}(s,a)$</p>
</blockquote>

<p>Graphically, the gradient descent algorithms are:</p>
<ul>
  <li>performing a Bellman backup update and</li>
  <li>then projecting back to the space of our approximator</li>
</ul>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521203039620.png" alt="image-20220521203039620" style="zoom:67%;" /></p>

<p>so that we can imagine our control algorithm doing:</p>

<ul>
  <li>the horizontal place is the space of our linear approximator</li>
  <li>first we perform one Bellman backup, and reaches $\mathbb{B}^\pi \vec{v}_w$, which is outside of our representation</li>
  <li>then, we project back to complete our gradient update $\Pi \mathbb{B}^\pi \vec{v}_w$</li>
</ul>

<p>However, notice that:</p>
<ul>
  <li>instead of iteratively performing Bellman’s backup outside the space (shown in gray above), which would make us reach the true value function (e.g. dynamic programming), we are constantly projecting back</li>
  <li>since we are <strong>projecting back at each step</strong>, then the final fixed point would be the point of vector-zero PBE</li>
  <li>the best approximation $\Pi v_\pi$ in the value error (VE) sense by projecting the true value function, the best approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal difference error (TDE) senses are <strong>all potentially different</strong></li>
</ul>

<h4 id="convergence-of-vfa-control">Convergence of VFA Control</h4>

<p>Consider the following example.</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220521201453082.png" alt="image-20220521201453082" style="zoom:67%;" /></p>

<p>Suppose we want to evaluate the policy of $\pi(\text{solid}\vert \cdot)=1$. Then as we are doing policy iteration, we consider $\epsilon$-greedy policy of:</p>

<ul>
  <li>$\mu(\text{dashed})\vert \cdot)= 6/7$</li>
  <li>$\mu(\text{solid})\vert \cdot)= 1/7$</li>
  <li>$\gamma = 0.99$</li>
</ul>

<p>as the behavioral policy used to sample data.</p>

<p>Suppose for simplicity that we then throw away all $(s,a,r,s’)$ tuples whose $a\neq \pi(s)$ which we wanted to evaluate. It turns out that <strong>TD-method will diverge</strong>:</p>

<ul>
  <li>the stationary distribution of states under the policy we want to approximate will contain a lot of $s_7$ states, so episodes look like $s_1,s_7,s_7,…$</li>
  <li>however, the data sampled using our behavior policy will contain episodes such as $s_1,s_7, \text{thrown away}, s_1, s_7, …$</li>
</ul>

<p>Therefore, the <strong>distribution of states</strong> we sampled will be very <strong>different</strong> from the distribution of state under $\pi(s)$.</p>

<p>Hence, in summary:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Tabular</th>
      <th>Linear VFA</th>
      <th>Nonlinear VFA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MC Control</td>
      <td>Converge</td>
      <td>Oscillates but Converge</td>
      <td>No guarantee</td>
    </tr>
    <tr>
      <td>SARSA</td>
      <td>Converge</td>
      <td>Oscillates but Converge</td>
      <td>No guarantee</td>
    </tr>
    <tr>
      <td>Q-Learning</td>
      <td>Converge</td>
      <td>No guarantee</td>
      <td>No guarantee</td>
    </tr>
  </tbody>
</table>

<p>but as VFA is very important, many research are done on this:</p>
<ul>
  <li>Extensive work in better TD-style algorithms with value function approximation, some with convergence guarantees: see Chp 11 S&amp;B</li>
  <li>Exciting recent work on batch RL that can converge with nonlinear VFA (Dai et al. ICML 2018): uses primal dual optimization</li>
</ul>

<h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>

<p>Before, we discussed value function approximation using NN and linear functions. Here, we will focus on <strong>RL with function approximation using deep NN</strong>:</p>

<ul>
  <li>need to deal with very large state spaces (e.g. for self-driving, video games, etc.)</li>
  <li>an intuition for using DNN in RL is to have a DNN to represent the features and the last linear layer represent the linear function approximator for $Q(s,a)$</li>
  <li>why is DNN so popular/useful? Universal function approximator theorem.</li>
</ul>

<p>(the following material assumes you know some basic DNN architectures, including CNNs, RNNs, etc.) Often the idea is to have:</p>

<ul>
  <li>game images as state</li>
  <li>game actions as actions (e.g. left, right, up, down, etc.)</li>
  <li>game score as rewards</li>
  <li>model the $Q$ value function using a DNN</li>
</ul>

<h2 id="deep-q-learning">Deep Q Learning</h2>

<p>Recall that since Q-Learning was based on TD updates, we had a concern that those off-policy control algorithms can fail to converge:</p>

<ul>
  <li>off-policy control + bootstrapping + function approximation, the deadly triad</li>
  <li>we have also seen some simple cases where the such an algorithm does diverge</li>
</ul>

<p>However, in 2014, DeepMind tried again to combine DNN with RL and achieved extraordinary results in Atari: <strong>in practice we can get reasonably good policies out</strong>. This is essentially the DQN network:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522155926513.png" alt="image-20220522155926513" /></p>

<p>where</p>

<ul>
  <li>since we somehow need to represent velocities in game state, 4-in game frames are taken to represent a state</li>
  <li>the network models $Q(s,a)$ for a total of 18 possible joystick actions</li>
  <li>reward is the game score</li>
</ul>

<hr />

<p>We know that Q-learning with VFA can diverge. Two of the issues are:</p>
<ul>
  <li><strong>correlations between samples</strong>: consider an episode of $s,a,r,s’,a’,r’…$. we notice that $V(s)$ would be highly correlated with $V(s’)$ in this case.</li>
  <li><strong>non-stationary targets due to bootstrapping</strong>: the update target is current approximation, which changes as we perform an update!</li>
</ul>

<p>DQN addresses those issues by using:</p>
<ul>
  <li><strong>Experience replay</strong>: help remove correlations</li>
  <li><strong>Fixed Q-targets</strong>: help convergence</li>
</ul>

<p>so that it is more likely to converge.</p>

<h3 id="dqns-experience-replay">DQNs: Experience Replay</h3>

<blockquote>
  <p>To help remove correlations, store dataset (called a ==replay buffer==) $D$ from prior experience. So that instead of having updates in the same order as the episode = high correlation, we sample from the buffer.</p>
</blockquote>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522161057736.png" alt="image-20220522161057736" style="zoom:80%;" /></p>

<p>note that this also means we need to decide <strong>which experience to put in the buffer</strong>.</p>
<ul>
  <li>Usually we can just place the most recent 1 million (for instance) samples in the buffer and sample from it.</li>
  <li>this means we also tend to set some fixed probability of how often we will want to sample new experiences</li>
</ul>

<p>With this, the update step of the algorithm would look like:</p>
<ol>
  <li>sample an experience tuple $(s,a,r,s’) \sim D$ from the replay buffer</li>
  <li>
    <p>compute the target value in a TD fashion as we are doing Q-learning:</p>

\[r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w})\]
  </li>
  <li>
    <p>use SGD to update:</p>

\[\Delta w = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}) - \hat{Q}(s,a|\vec{w}))\nabla_w \hat{Q}(s,a|\vec{w})\]
  </li>
</ol>

<p>now, as mentioned before another problem is that our update target is using the current approximation, which changes as we perform an update. We are chasing a <strong>non-stationary target</strong>.</p>

<h3 id="dqns-fixed-q-target">DQNs: Fixed Q-Target</h3>

<blockquote>
  <p>To help improve stability, fix the target weights used in the target calculation <strong>for multiple updates</strong>, instead of having it changed per update.</p>
</blockquote>

<p>Let parameters $\vec{w}^-$ and $\vec{w}$ be the weights of the target and current network. Essentially the idea is</p>
<ol>
  <li>sample an experience tuple $(s,a,r,s’) \sim D$ from the replay buffer</li>
  <li>
    <p>compute the target value <strong>using $\vec{w}^-$</strong></p>

\[r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}^-)\]
  </li>
  <li>
    <p>use SGD to update:</p>

\[\Delta w = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a'| \vec{w}^-) - \hat{Q}(s,a|\vec{w}))\nabla_w \hat{Q}(s,a|\vec{w})\]
  </li>
  <li>after a certain period, e.g. per 100 steps, update $\vec{w}^- := \vec{w}$.</li>
</ol>

<h3 id="dqn-ablation-study">DQN Ablation Study</h3>

<p>How well does the above improve performance?</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522164035968.png" alt="image-20220522164035968" style="zoom: 50%;" /></p>

<p>so we see that:</p>
<ul>
  <li>replay is <strong>hugely</strong> important</li>
</ul>

<h3 id="dqn-summary">DQN Summary</h3>

<ul>
  <li>DQN uses experience replay and fixed Q-targets</li>
  <li>Store transition $s_t, a_t, r_{t+1}, s_{t+1}$ in replay memory $D$</li>
  <li>Sample random mini-batch of transitions $(s,ta,r,s’)$ from $D$</li>
  <li>Compute Q-learning targets w.r.t. old, fixed parameters $w^−$</li>
  <li>Update using $\epsilon$-greedy exploration, so as before need some decaying exploration</li>
  <li>Optimizes MSE loss between Q-network and Q-learning targets</li>
</ul>

<p>After the success of DQNs, we then had many immediate improvements (many others!)</p>
<ul>
  <li><strong>Double DQN</strong> (Deep Reinforcement Learning with Double Q-Learning, Van Hasselt et al, AAAI 2016)</li>
  <li><strong>Prioritized Replay</strong> (Prioritized Experience Replay, Schaul et al, ICLR 2016)</li>
  <li><strong>Dueling DQN</strong> (best paper ICML 2016) (Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, ICML 2016)</li>
  <li>etc.</li>
</ul>

<h2 id="double-dqn">Double DQN</h2>

<p><em>Recall</em> that we had Double Q-Learning because we had the <strong>maximization bias challenge</strong>: Max of the estimated state-action values can be a biased estimate of the max. To solve this issue, there comes the Double Q-learning algorithm which looks like:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522164647254.png" alt="image-20220522164647254" style="zoom:33%;" /></p>

<p>so that we basically maintained two Q-networks.</p>

<blockquote>
  <p>Extend this idea to DQN using Deep network for the two $Q$ networks.</p>
</blockquote>

<p>Result:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522165031684.png" alt="image-20220522165031684" style="zoom:80%;" /></p>

<h2 id="prioritized-replay">Prioritized Replay</h2>

<p>The idea is that instead of sampling randomly from the replay buffer, maybe there are some better distributions to sample such that we can converge better.</p>

<p>Intuitively, consider the following example of the 7-state Mars Rovers again:</p>

<ul>
  <li>reward is $R=[1,0,0,0,0,0,10]$</li>
  <li>any action from $s_1,s_7$ are terminal</li>
  <li>we initialized with the greedy policy of $\pi(s)=a_1,\forall s$</li>
  <li>
    <p>we sampled a trajectory by having an $\epsilon$-greedy version of the policy $\pi$, and obtained</p>

\[(s_3, a_1, 0,s_2,a_1,0,s_2,a_1,0,s_1,a_1,1,\text{terminal})\]
  </li>
  <li>the TD estimate of all states after an in-order update with $\alpha=1$ is: $[1,0,0,0,0,0,0]$ which we had computed before.</li>
  <li>the MC estimate with $\gamma =1$ gives $V = [1,0,0,0,0,0,0]$</li>
</ul>

<p>Now, using experience buffer, we consider the four tuples:</p>

\[(s_3,a_1,0,s_2), (s_2,a_1,0,s_2), (s_2,a_1,0,s_1), (s_1,a_1,1,T)\]

<p><strong>Question</strong> if we get to choose three replay backups to do, which should we pick?</p>

<ul>
  <li>first pick the fourth tuple $(s_1,a_1,1,T)$ so that $V(s_1)=1+\gamma * 0 = 1$</li>
  <li>then pick the third tuple $(s_2,a_1,0,s_1)$, because we know that $V(s_1)=1$, so that $V(s_2)= 0 + \gamma * 1$ gets <strong>propagated</strong></li>
  <li>finally pick the first tuple, $(s_3,a_1,0,s_2)$, because we know that $V(s_2)=\gamma$, and so we can further propagate back to $s_3$.</li>
  <li>for $\gamma =1$, this results in $V = [1,0,0,0,0,0,0]$. Notice that it would be ==the same as MC update==!</li>
</ul>

<hr />

<p>In theory, such an order is important. There has been research on this and we basically found that if we know the correct order for TD updates, we can require exponentially less updates as sample grows:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522170804087.png" alt="image-20220522170804087" /></p>

<p>However, computing the exact ordering is intractable in practice.</p>

<blockquote>
  <p>Therefore, the idea is to consider some ordering based on how big the error is. Consider a sample being $(s_i,a_i,r_i,s_{i+1})$, then we consider the ==priority== of such tuple $i$ being:</p>

\[p_i \equiv \left|  \underbrace{r + \gamma \max_{a'}Q(s_{t+1},a';\vec{w}^-)}_{\text{TD target}&gt;  }- Q(s_i,a_i;\vec{w}) \right|\]

  <p>then the probability of sampling such a tuple is basically proportional to the priority:</p>

\[P(i) \equiv \frac{p_i^\alpha}{\sum_k p_k^\alpha}\]

  <p>notice that if we set $\alpha=0$ we get the normal experience replay with equal probability.</p>
</blockquote>

<h2 id="dueling-dqn-advantage-function">Dueling DQN: Advantage Function</h2>

<p>The intuition here is that:</p>

<ul>
  <li>Game score may be relevant to predicting $V(s)$</li>
  <li>But not necessarily in indicating <strong>relative benefit of taking different actions</strong> in a state</li>
</ul>

<p>Therefore, we do not care about the value of a state, but about ==which actions at state $s$ has a better value==, which can be done by looking at the advantage function</p>

\[A^\pi(s,a) \equiv Q^\pi(s,a) - V^\pi(s) = Q^{\pi}(s,a) - Q^{\pi}(s,\pi(s))\]

<p>which is basically the advantage of taking action $a$ at state $s$ compared to taking $\pi(s)$.</p>

<p>Then the architecture looks like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">DQN</th>
      <th style="text-align: center">Dueling DQN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522172421357.png" alt="image-20220522172421357" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522172429762.png" alt="image-20220522172429762" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially</p>
<ul>
  <li>Dueling DQN keeps one estimate of $V(s)$ from the network and another for the advantage function</li>
  <li>then, it is recombined back to $Q(s,a) = A^{\pi}(s,a) - V^{\pi}(s,a)$ to perform the control algorithm.</li>
  <li>notice that we are having ==another set of features== to represent $A(s,a)$, which reiterates the point that we might need different features for approximating the advantage than the value function $V\left(s,a\right)$.</li>
</ul>

<p>When compared to Double DQN with Prioritized Replay, this <strong>already has a big improvement</strong>:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220522173022329.png" alt="image-20220522173022329" /></p>

<p>also note that the performance for “Montezuma’s Revenge” was still ==not improved==, i.e. those families of improvements mainly aimed at improving reward propagation, but not for games that require <strong>a high exploration</strong>.</p>

<h3 id="identifiability-of-advantage-function">Identifiability of Advantage Function</h3>

<p>However, realize that by the definition of advantage function, this means:</p>

\[\hat{Q}^{\pi}(s,a) = V^{\pi}(s) + A^{\pi}(s,a)\]

<p>but it is <strong>not unique</strong> as we can have the same $\hat{Q}$ for different $\hat{A}$ and $\hat{V}$. This would be problematic as then our labels for training $A,V$ would be non-unique.</p>

<p>Therefore, the idea is to impose some fixed points:</p>

<ul>
  <li>
    <p>option 1: Force $A(s,a)=0$  for $a$ is the greedy action</p>

\[\hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \underbrace{\left( \hat{A}(s,a;\vec{w}) - \max_{a'} \hat{A}(s,a';\vec{w}) \right)}_{\text{approximate } A(s,a)}\]
  </li>
  <li>
    <p>option 2: Use mean to be zero as the baseline (more stable)</p>

\[\hat{Q}(s,a;\vec{w}) = \hat{V}(s;\vec{w}) + \underbrace{\left( \hat{A}(s,a;\vec{w}) - \frac{1}{\left|\mathcal{A}\right|} \sum\limits_{a'} \hat{A}(s,a';\vec{w}) \right)}_{\text{approximate } A(s,a)}\]
  </li>
</ul>

<h1 id="imitation-learning">Imitation Learning</h1>

<p>Previously we have seen approaches using DNN for learning value functions, and by performing policy iteration, we can improve policy.</p>

<p>However, there exists hardness results that if learning in a generic MDP, can require large number of samples to learn a good policy.</p>

<p>So it is important to remember that:</p>

<blockquote>
  <p>Reinforcement Learning: Learning policies guided by (often ==sparse==) rewards (e.g. win the game or not):</p>
  <ul>
    <li>Good: as a simple, cheap form of supervision</li>
    <li>Bad: requires high sample complexity for finding a good policy</li>
  </ul>
</blockquote>

<p>Therefore, it most successful when:</p>
<ul>
  <li>in simulation where data is cheap and parallelization is easy</li>
</ul>

<p>==Not== when:</p>
<ul>
  <li>Execution of actions is slow</li>
  <li>Very expensive or not tolerable to fail (e.g. learning to fly a helicopter will require crashing many times)</li>
  <li>Want to be safe</li>
</ul>

<blockquote>
  <p><strong>Alternative idea</strong>: instead of learning policies from scratch, we could use <em>structure and additional knowledge</em> (if there is) to help constrain and speed reinforcement learning:</p>
  <ul>
    <li>imitation learning: we have a fix structure of policy to learn (this section)</li>
    <li>policy search/gradient: we have a fixed class of policy to search from (next section)</li>
  </ul>
</blockquote>

<p>One application for this is to help improve the performance on “Montezoma’s Revenge” game:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220523170757631.png" alt="image-20220523170757631" style="zoom:67%;" /></p>

<p>where it is:</p>

<ul>
  <li>a game that you need to collect keys and explore many different rooms</li>
  <li>a very very long horizon game, and requires large exploration as well</li>
  <li>we see that DQN agents only reached the first two rooms (LHS), with exploration bonus (RHS), we can reach more rooms but still not finishing the game</li>
  <li>some of the early approaches to make this work is via imitation learning</li>
</ul>

<h2 id="learning-from-demonstrations">Learning from Demonstrations</h2>

<p>One of the challenge in Montezuma’s revenge is that long episodes means reward is very sparse. This means that you need to try many different things before getting some feedback of good or bad.</p>

<blockquote>
  <p>Rewards that are <strong>dense</strong> in time closely guide the agent:</p>
  <ul>
    <li>Manually design them: often brittle</li>
    <li>Implicitly specify them through demonstrations: <strong>imitation learning</strong>!</li>
  </ul>

  <p>Therefore, the idea is to have (optimal or pretty good) Expert provides a set of demonstration trajectories: sequences of states and actions</p>
</blockquote>

<p>This is useful when it is easier for the expert to demonstrate the desired behavior rather than come up with a reward that would generate such behavior, or coding up the desired policy directly.</p>

<hr />

<p>Hence in this section, we consider the following problem setup:</p>
<ul>
  <li>given some transition model $P(s’\vert s,a)$</li>
  <li>no reward function $R$</li>
  <li>given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , …)$  drawn from some teacher policy $\pi^{*}$</li>
</ul>

<blockquote>
  <p><strong>Aim</strong>:</p>
  <ul>
    <li>Behavior Cloning: learn the teacher’s policy $\pi^*$</li>
    <li>Inverse RL: find the reward function $R$ (e.g. to <em>understand</em> behavior of certain creatures)</li>
    <li>Apprenticeship Learning via Inverse RL: find the reward function $R$ and then generate a good policy</li>
  </ul>
</blockquote>

<h2 id="behavior-cloning">Behavior Cloning</h2>

<p>The idea of this is simple. Since we need to learn the teacher’s policy $\pi^{*}$, all we need to do is to ==learn the mapping from state to action==.</p>

<blockquote>
  <p>Therefore, this can be formulated as a <strong>supervised machine learning</strong> problem:</p>
  <ul>
    <li>Fix a policy class (e.g. neural network, decision tree, etc.)</li>
    <li>Estimate a policy (i.e. state to action mapping) from training examples $(s_{0} , a_{0}) , (s_{1}, a_{1} ), …$</li>
  </ul>
</blockquote>

<p>However, remember that supervised learning ==assumes iid. $(s, a)$ pairs==. This means that</p>
<ul>
  <li>supervised learning assumes the <strong>state distribution</strong> you will get in any future (e.g. after taking some actions) is the <strong>same</strong> as the ones in the past (iid)</li>
  <li>however, in MDP processes, the state distribution would change after you take some actions.</li>
</ul>

<blockquote>
  <p><strong>Problem: Compounding Errors</strong>. Therefore errors training in a supervised fashion leads to high generalization error, so that once we went into an unseen state:</p>
  <ul>
    <li>assumed same state distribution, hence gets errors</li>
    <li>takes the wrong action and continue getting in unseen states</li>
  </ul>

  <p>Therefore, we could often get a mismatch in training and test distribution as our data is limited:</p>
  <ul>
    <li>train have $s_t \sim D_{\pi^*}$ sampled from the teacher policy</li>
    <li>test will encounter $s_t \sim D_{\pi_{\theta}}$ sampled from the our estimated policy</li>
  </ul>
</blockquote>

<p>Graphically, this means that as soon as you made a mistake, you will continuously deviate from the path:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220523203357492.png" alt="image-20220523203357492" style="zoom: 50%;" /></p>

<p>theoretically, the generalization error in this case will be bounded by $\epsilon T^{2}$ instead of $\epsilon T$ bound in supervised learning.</p>

<h3 id="dagger-dataset-aggregation">DAGGER: Dataset Aggregation</h3>

<p>This is a simple idea to counter the compounding errors problem, so that during training we can <strong>ask the expert</strong> to provide the gold standard action:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220523205338781.png" alt="image-20220523205338781" style="zoom:67%;" /></p>

<p>therefore, we get:</p>

<ul>
  <li>in training we have $s_{t} \sim D_{\pi_\theta}$, which will be the same in test environment!</li>
  <li>note that we technically <em>don’t need to know the teacher’s policy $\pi^{</em>}$*, we just need to query something to get the gold standard action.</li>
</ul>

<p>So in this technique we need to have an expert, kind of like ==human in the loop==. Therefore in many cases this could be very label intensive.</p>

<h2 id="inverse-reinforcement-learning">Inverse Reinforcement Learning</h2>

<p><em>Recall</em> that our setup was:</p>
<ul>
  <li>given some transition model $P(s’\vert s,a)$
    <ul>
      <li>there are also some extensions on how to do it without transition model as well</li>
    </ul>
  </li>
  <li>no reward function $R$</li>
  <li>given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , …)$  drawn from some teacher policy $\pi^{*}$</li>
</ul>

<blockquote>
  <p>Goal: infer the reward function $R$.</p>
</blockquote>

<p>How does this work? Consider the following:</p>
<ul>
  <li>With <em>no assumptions</em> on the optimality of the teacher’s policy, what can be inferred about R?
    <ul>
      <li>nothing, because if the teacher is doing random things, then you have no idea what the reward/goal is.</li>
    </ul>
  </li>
  <li>Now <em>assume that the teacher’s policy is optimal</em>. What can be inferred about R?
    <ul>
      <li>given some optimal path, would the reward function be unique? No: consider the reward function $R=0$ for all states and actions. Then any policy/trajectory would be optimal hence there would be more than one unique answers.</li>
      <li>in fact, a reward function of any constant will have this behavior. Therefore, we need to <strong>impose additional constraints</strong> to solve this problem</li>
    </ul>
  </li>
</ul>

<p>Some of the key papers are:</p>
<ul>
  <li>Maximumum Entropy Inverse Reinforcement Learning (Ziebart et al. AAAI 2008)</li>
</ul>

<h2 id="apprenticeship-learning">Apprenticeship Learning</h2>

<p><em>Recall</em> that the setup is”</p>
<ul>
  <li>given some transition model $P(s’\vert s,a)$</li>
  <li>no reward function $R$</li>
  <li>given a set of teacher demonstrations $(s_{0,} a_{0}, s_{1,} a_{1} , …)$  drawn from some teacher policy $\pi^*$</li>
</ul>

<blockquote>
  <p><strong>Aim</strong>: find a policy that is as good as the expert policy</p>
</blockquote>

<p>(essentially this comes from trying to solve for the reward function, but realize that it is no longer needed.)</p>

<p>Like linear value function approximation, we can consider a model:</p>

\[R(s) = \vec{w}^{T} \vec{x}(s)\]

<p>for $\vec{x}(s)$  is a feature vector for states.</p>

<p>Then, given a set of demonstrations from $\pi$, we want to identify $\vec{w}$:</p>

\[\begin{align*}
  V^{\pi} 
  &amp;= \mathbb{E}_\pi\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R(s_t)\right]\\
  &amp;= \vec{w}^{T}\mathbb{E}_\pi\left[\sum\limits_{t=0}^{\infty} \gamma^{t}\vec{x}(s_t)\right]\\
  &amp;= \vec{w}^{T}\vec{\mu}(\pi)
\end{align*}\]

<p>for $\vec{\mu}(\pi) \equiv \mathbb{E}<em>\pi\left[\sum\limits</em>{t=0}^{\infty} \gamma^{t}\vec{x}(s_t)\right]$ can be interpreted as a distribution of states/features weighted by the discount factor.</p>

<p>But we know that an optimal policy will have by definition the largest value function:</p>

\[\begin{align*}
  V^{*}
  &amp;\ge V^{\pi}\\
  \mathbb{E}_{\pi^{*}}\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R^{*}(s_t)\right]
  &amp;\ge \mathbb{E}_{\pi}\left[\sum\limits_{t=0}^{\infty} \gamma^{t}R^{*}(s_t)\right]\\
  \vec{w}^{*^{T}} \vec{\mu}(\pi^{*})
  &amp;\ge \vec{w}^{*^{T}} \vec{\mu}(\pi)
\end{align*}\]

<blockquote>
  <p>From this, the idea is that:</p>
  <ul>
    <li>to find the optimal weights $w^{*}$, we need the expert policy to perform better than any other policy (under this reward function)</li>
    <li>to perform close to the optimal value $\vec{w}^{*^{T}}$, we just need to match the features under the expert policy</li>
  </ul>
</blockquote>

<p>Formally, if</p>

\[||\vec{\mu}(\pi) - \vec{\mu}(\pi^*)||_1 \le \epsilon\]

<p>then for ==all $\vec{w}$== with $\vert \vert \vec{w}_\infty \vert \vert  \le 1$ :</p>

\[||\vec{w}^T\vec{\mu}(\pi) - \vec{w}^{T}\vec{\mu}(\pi^*)||_1 \le \epsilon\]

<blockquote>
  <p>So that we can find the ==optimal policy== $\pi^{*}$ without needing to know the reward weight $\vec{w}$ by finding a policy $\pi$ such that:</p>

\[||\vec{\mu}(\pi) - \vec{\mu}(\pi^*)||_1 \le \epsilon\]

  <p>and for $\vec{\mu}(\pi)$, all we need to know is to compute the discounted sum of features $\vec{x}(s)$ following that policy $\pi$.</p>
</blockquote>

<p>Therefore, this gives the Feature Expectation Matching:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220523222433551.png" alt="image-20220523222433551" style="zoom: 67%;" /></p>

<p>where here we are picking the best policy and picking the corresponding reward function</p>
<ul>
  <li>still doesn’t deal with the problem that reward function is not unique</li>
  <li>if expert policy is suboptimal then the resulting policy is a mixture of somewhat arbitrary policies which have expert in the convex hull</li>
</ul>

<p>Nowadays we tend to use DNN versions, and some extensions include:</p>
<ul>
  <li>GAIL: Generative adversarial imitation learning (Ho and Ermon, NeurIPS 2016)</li>
</ul>

<h2 id="imitation-learning-summary">Imitation Learning Summary</h2>

<ul>
  <li>Imitation learning can greatly reduce the amount of data need to learn a good policy
    <ul>
      <li>i.e. think of the expert demonstration as an additional “constraint”/structure</li>
    </ul>
  </li>
  <li>Challenges remain and one exciting area is combining inverse RL / Learning from demonstration and online reinforcement learning</li>
</ul>

<h1 id="policy-gradient">Policy Gradient</h1>

<p>Similar to imitation learning, policy gradient/search methods imposes some <strong>constraints</strong> on the policy and hence makes learning more efficient. However, the changes are:</p>

<ul>
  <li>recall that previously our Policy Improvement algorithm mainly involved:
    <ul>
      <li>policy evaluation: update our estimate of the value function ($V_{\theta} \approx V^{\pi}(s)$ or $Q_{\theta} (s,a) \approx Q^\pi (s,a)$  )</li>
      <li>policy improvement: improve $\pi \gets \epsilon\mathrm{-Greedy}(Q_{\theta} (s,a))$</li>
    </ul>
  </li>
  <li>
    <p>however, in this section we will <strong>directly parametrize the policy</strong>, to consider the model:</p>

\[\pi_{\theta} (s,a) \equiv P_\theta[a|s]\]

    <p>and the ==aim is to find a policy $\pi$ with the highest value function $V^{\pi}$==.</p>
  </li>
  <li>of course we will do it in a model-free approach (i.e. without transition/reward function)</li>
</ul>

<p>Graphically, we are at:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605205922887.png" alt="image-20220605205922887" style="zoom:67%;" /></p>

<p>so essentially:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Learnt/Modelling Value Function</th>
      <th style="text-align: center">Learnt/Modelling Policy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Value Based (previous sections)</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Implicit (e.g. greedy)</td>
    </tr>
    <tr>
      <td>Policy Based (here)</td>
      <td style="text-align: center">No</td>
      <td style="text-align: center">Yes</td>
    </tr>
    <tr>
      <td>Actor-Critic (next sections)</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">Yes</td>
    </tr>
  </tbody>
</table>

<p>Why might we want to use policy search/gradient methods? Advantages:</p>
<ul>
  <li>
    <p>Better convergence properties (also depends on computation power)</p>
  </li>
  <li>
    <p>Effective in high-dimensional or <strong>continuous action spaces</strong></p>
  </li>
  <li>
    <p>Can learn <strong>stochastic</strong> policies.</p>
    <ul>
      <li>
        <p>Useful when: exists competitor and do no want to be exploited (e.g. rock-paper-scissor)</p>
      </li>
      <li>
        <p>Useful when: the problem is not Markov, e.g. your grey states cannot be distinguished</p>

        <p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605211409890.png" alt="image-20220605211409890" style="zoom:67%;" /></p>

        <p>the problem that states cannot be distinguished is also called ==aliasing==. In this case using stochastic policy in grey states is a good idea/has a higher value.</p>
      </li>
      <li>
        <p>Not very useful when: in a tabular MDP problem, optimal policy is deterministic</p>
      </li>
      <li>
        <p><strong>monotonic improvement</strong> of the policy (value-based methods, e.g. DQN, is noisy)</p>
      </li>
    </ul>
  </li>
</ul>

<p>Disadvantages:</p>
<ul>
  <li>Typically ==converge to a local rather than global optimum==</li>
  <li><strong>Evaluating</strong> a policy (from which we compute gradient) is typically sample inefficient and high variance</li>
</ul>

<h2 id="policy-objective-functions">Policy Objective Functions</h2>

<blockquote>
  <p><strong>Goal</strong>: given a policy $\pi_{\theta} (s,a)$  parameterized by $\theta$, we want to find $\theta$ that yields the <strong>highest value</strong> (not value function, but a single score)</p>
  <ul>
    <li>
      <p>in episodic setting (i.e. has terminal states), we can use the <strong>start state</strong> to compare policies:</p>

\[J_{1}(\theta) = V^{\pi_\theta}(s_1)\]

      <p>for $s_1$ is the start state of the entire environment</p>
    </li>
    <li>
      <p>in continuing/online environment where there is no terminal states, we can use the <strong>average value</strong> over all states:</p>

\[J_{avV}(\theta) = \sum_s d^{\pi_\theta} (s) V^{\pi_\theta}(s)\]

      <p>where $d^{\pi_\theta}(s)$ is the <strong>stationary distribution</strong> of states for $\pi_\theta$. A similar version of this is to look at the <strong>average reward per time-step</strong>:</p>

\[J_{avR}(\theta) = \sum_s d^{\pi_\theta}(s) \sum_a  \pi_\theta(s,a) R(a,s)\]
    </li>
  </ul>

</blockquote>

<p>Now, we have a goal function from which we can ==optimize==. Therefore, it becomes an optimization problem and we can consider:</p>

<ul>
  <li>gradient free optimization (useful when not differentiable, but not used often now)
    <ul>
      <li>Hill climbing</li>
      <li>Simplex / amoeba / Nelder Mead</li>
      <li>Genetic algorithms</li>
      <li>Cross-Entropy method (CEM)</li>
      <li>Covariance Matrix Adaptation (CMA)</li>
    </ul>
  </li>
  <li>gradient based optimization (this section)
    <ul>
      <li>Gradient descent</li>
      <li>Conjugate gradient</li>
      <li>Quasi-newton</li>
    </ul>
  </li>
</ul>

<h2 id="policy-gradient-methods">Policy Gradient Methods</h2>

<p>Consider the setup of:</p>
<ul>
  <li>Define $V(\theta) \equiv V^{\pi_{\theta}}$  to make explicit the dependence of the value on the policy parameters</li>
  <li>assume episodic MDPs</li>
</ul>

<p>Then policy search/gradient considers ==gradient ascent== w.r.t parameter $\theta$:</p>

\[\nabla \theta = +\alpha \nabla_{\theta} V(\theta)\]

<p>But the question is, ==what is $\nabla_{\theta} V(\theta)$==</p>

\[\nabla_{\theta} V(\theta) = \begin{bmatrix}
  \frac{\partial V(\theta)}{\partial \theta_1} \\
  \frac{\partial V(\theta)}{\partial \theta_2} \\
  \vdots\\
  \frac{\partial V(\theta)}{\partial \theta_n}
\end{bmatrix}\]

<p>==when our parameter is on $\pi_\theta$?==</p>

<h3 id="gradients-by-finite-differences">Gradients by Finite Differences</h3>

<p>The simplest approach to compute this by “trial and error”:</p>
<ol>
  <li>for each dimension $k \in [1, n]$ for $\theta \in \mathbb{R}^{n}$:
    <ol>
      <li>
        <p>perturb the parameter by a small amount:</p>

\[V(\theta + \epsilon u_{k})\]

        <p>for $u_k$ is a unit vector in the $k$-th dimension</p>
      </li>
      <li>
        <p>estimate the gradient w.r.t. $\theta_k$ by:</p>

\[\frac{\partial V(\theta)}{\partial \theta_k} \approx \frac{V(\theta + \epsilon u_{k}) - V(\theta)}{\epsilon}\]
      </li>
    </ol>
  </li>
  <li>repeat</li>
</ol>

<p>While this method seems simple, it has been used practically and was effective (required only few trials).</p>

<hr />

<p><em>For instance: AIBO Policy Experiment</em></p>

<p>The task was to train AIBO robots so that they can walk as fast as possible. Then there was research which had:</p>

<ul>
  <li>open-loop policy: find a sequence of actions, irrespective of the state</li>
  <li>12 parameters $\theta \in \mathbb{R}^{12}$ they wanted to search for:
    <ul>
      <li>The front locus (3 parameters: height, x-pos., y-pos.)</li>
      <li>The rear locus (3 parameters)</li>
      <li>Locus length</li>
      <li>Locus skew multiplier in the x-y plane (for turning)</li>
      <li>etc.</li>
    </ul>
  </li>
</ul>

<p>Then, essentially they needed to compute the gradient of the policy, which they can by:</p>
<ul>
  <li>Ran on 3 AIBOs at once</li>
  <li>Evaluated 15 policies per iteration.</li>
  <li>Each policy evaluated 3 times (to reduce noise) and averaged</li>
  <li>Used $\eta = 2$ (learning rate for their finite difference approach)</li>
</ul>

<p>And they were very successful in few iterations:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605224812515.png" alt="image-20220605224812515" style="zoom: 67%;" /></p>

<p>note that as we are converging to local optima, initialization for $\theta$  matters.</p>

<h3 id="likelihood-ratioscore-function-policy-gradient">Likelihood Ratio/Score Function Policy Gradient</h3>

<p>Suppose we <strong>can compute $\nabla_{\theta} \pi_\theta(s)$</strong>. We want to find a way to compute $\nabla_\theta V(\theta)$ by analytical methods, and one of which is the likelihood ratio policy gradient.</p>

<p>Consider the following setup:</p>

<ul>
  <li>denote a trajectory as $\tau = (s_{0}, a_{0} , r_{0} , … , s_{T-1},a_{T-1},r_{T-1},s_{T},a_{T},r_{T})$.</li>
  <li>denote the reward of the trajectory as $R(\tau) \equiv \sum\limits_{t=0}^{T} R(s_t, a_t)$</li>
  <li>
    <p>then define the ==policy value== as:</p>

\[V(\theta) \equiv \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau) = \mathbb{E}_{\pi_{\theta}} \left[ \sum\limits_{t=0}^{T} R(s_{t} , a_{t} ) \right]\]

    <p>for $P_{\pi_\theta}(\tau)$ being the probability over trajectories when executing policy $\pi_\theta$.</p>
  </li>
</ul>

<blockquote>
  <p>In this new notation, our goal is to find parameters $\theta$ such that:</p>

\[\arg\max_\theta V(\theta) = \arg\max_\theta \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau)\]

  <p>notice that only $P_{\pi_\theta}(\tau)$ changes if you change $\theta$.</p>
</blockquote>

<p>Therefore, we are interested in finding $\nabla_\theta V(\theta)$, which can be computed as:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605230842266.png" alt="image-20220605230842266" style="zoom: 67%;" /></p>

<p>which seems to have done nothing, but we will soon see that</p>

<ul>
  <li>since we are doing model free, ==this trick helps us to compute $\nabla_\theta \log P(\tau; \theta)$ without knowing the world==.</li>
  <li>to evaluate $\sum\limits_{\tau}$ and $R(\tau)$, we can just ==sample== a bunch of trajectories!</li>
</ul>

<p>Therefore, first we approximate this by:</p>

\[\nabla_{\theta} V(\theta) \approx \hat{g} = \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)\]

<p>for which $R(\tau^{(i)})$ are known already, and we are weighting each trajectory equally.</p>

<blockquote>
  <p>The above form of policy gradient can be interpreted as follows: consider the ==generic form of $R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)$ which can be made into:</p>

\[\hat{g}_i \equiv f(x_i) \nabla_\theta \log p(x_i;\theta)\]

  <p>which means that:</p>
  <ul>
    <li>$f(x)$ measure how good a sample $x$ is (e.g. $f(x_i) = R(\tau^{(i)})$ )</li>
    <li>performing a gradient ascent step means we are <strong>pushing up the log probability $p(x_i;\theta)$ of the sample, in proportion to how good it is $f(x)$</strong></li>
    <li>note that this form is valid even if $f(x)$ is discontinous</li>
  </ul>
</blockquote>

<p>Graphically:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605232345294.png" alt="image-20220605232345294" style="zoom: 50%;" /></p>

<p>Finally, we need to solve for the term $\nabla_\theta \log P(\tau^{(i)};\theta)$ to compute the gradient $\nabla_{\theta} V(\theta) \approx \hat{g}$:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220605232719619.png" alt="image-20220605232719619" style="zoom:67%;" /></p>

<p>notice that from this we ==only need to know $\nabla_{\theta} \log \pi_\theta$==! This term also gets a (relatively useless) name of ==score function==.</p>

<blockquote>
  <p><strong>Score Function Policy Gradient:</strong></p>

  <p>The goal is to find $\theta$ such that:</p>

\[\arg\max_\theta V(\theta) = \arg\max_\theta \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau)\]

  <p>which we can find local optimal using gradient ascent, by:</p>

\[\begin{align*}
  \nabla_{\theta} V(\theta) \approx \hat{g} 
    &amp;= \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)};\theta)\\
    &amp;= \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})
 \end{align*}\]

  <p>and the $m$ trajectories would be sampled under policy $\pi_\theta$. Notice that this is model free as we only need to evaluate the score function $\nabla_{\theta} \log \pi_\theta$.</p>
</blockquote>

<p>Finally, it turns out that this can be extended to many other objective functions $J$ than the one shown above $J = V(\theta) = \sum\limits_{\tau} P_{\pi_\theta}(\tau) R(\tau)$.</p>

<blockquote>
  <p><strong>Policy Gradient Theorem</strong>: for any differentiable policy $\pi_{\theta} (s,a)$, for any policy objective function:</p>
  <ul>
    <li>episodic $J = J_1$</li>
    <li>average reward per time step $J = J_{avR}$</li>
    <li>average value $J = (1 / (1 - \gamma)) J_{avV}$
the policy gradient is:</li>
  </ul>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ Q^{\pi_\theta}(s,a) * \nabla_{\theta} \log \pi_\theta (a|s)  ]\]

  <p>which can be derived in the same manner as we have shown here (also shown in Sutton and Barto chapter 13)</p>
</blockquote>

<h2 id="reducing-variance-in-policy-gradient">Reducing Variance in Policy Gradient</h2>

<p>Previously, we were able to compute policy gradient without a world model by:</p>

\[\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})\]

<p>for letting the objective function being $J = V(\theta)$. However, although this is an ==unbiased estimate of the gradient==, it is ==noisy (high variance) in practice==. Fixes include:</p>

<ul>
  <li>temporal structure/bootstrapping methods (reduces variance)</li>
  <li>baseline</li>
  <li>alternatives to using MC return $R(\tau^{(i)})$ as targets, e.g. Actor-Critic methods for TD and bootstrapping</li>
</ul>

<h3 id="policy-gradient-use-temporal-structure">Policy Gradient: Use Temporal Structure</h3>

<p>It turns out that we can manipulate the same expression we had to achieve another gradient expression. Consider the previous conclusion:</p>

\[\nabla_{\theta} \mathbb{E}_\tau [R] = \mathbb{E}_\tau \left[ R \left( \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \right]\]

<p>for $R = \left( \sum\limits_{t=0}^{T-1} r_t \right)$. This means that:</p>

\[\nabla_{\theta} \mathbb{E}_\tau [r_{t'}] = \mathbb{E}_\tau \left[ r_{t'} \left( \sum\limits_{t=0}^{t'} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \right]\]

<p>Hence we can recover the original form by summing over $t$:</p>

\[\begin{align*}
  \nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  &amp;= \mathbb{E}_\tau \left[ \sum\limits_{t'=0}^{T-1} r_{t'}  \sum\limits_{t=0}^{t'} \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\
  &amp;= \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum\limits_{t'=t}^{T-1} r_{t'} \right] \\
  &amp;= \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) G_{t'} \right]
\end{align*}\]

<p>where</p>
<ul>
  <li>the second last equality is due to the fact that, the $\nabla_\theta \log \pi_\theta(a_0\vert s_0)$ appears for all $r_{t’}$, and the $\nabla_\theta \log \pi_\theta(a_1\vert s_1)$ appears for all except $r_{t’ = 0}$, etc.</li>
  <li>the last equality is due to the fact that $G_{t’}$ is the expected return since time $t’$</li>
</ul>

<blockquote>
  <p>Therefore, this yields a slight lower variance estimate of the gradient:</p>

\[\nabla_{\theta} V(\theta) = \nabla_\theta \mathbb{E}_\tau [R] \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}\]

</blockquote>

<p>Notice that</p>
<ul>
  <li>as compared to the previous form $\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}\vert s_t^{(i)})$, we multiply each $\nabla_\theta \log \pi_\theta(a_t^{(i)}\vert s_t^{(i)})$ by its own expected reward $G_t^{(i)}$ instead of the constant episodic reward $R(\tau^{(u)})$. This ==reduces the variance== of the gradient estimate.</li>
  <li>since this uses $G_t$, it is a MC estimate.</li>
</ul>

<h3 id="reinforce-algorithm">REINFORCE Algorithm</h3>

<p>If we sample only one trajectory (doing SGD), then essentially the previous section says:</p>

\[\nabla_{\theta} V(\theta) \approx \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) G_t^{(i)}\]

<p>for an sampled trajectory $i$. And this is basically what REINFORCE algorithm does:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220606003925976.png" alt="image-20220606003925976" style="zoom:67%;" /></p>

<p>and of course if $\pi_{\theta} (a\vert s)$ is a neural network, then derivative can easily numerically evaluated as well (e.g. automatic differentiation in PyTorch/Tensorflow).</p>

<h3 id="differentiable-policy-classes">Differentiable Policy Classes</h3>

<p>Besides neural network, there are two classes of policy model that can be good to know:</p>

<ul>
  <li>
    <p><strong>Softmax Policy</strong>: weight action using linear combination of feature $\phi(s,a)$ so that:</p>

\[\pi_\theta(a|s)  = \mathrm{Softmax}\left( \phi(s,a)^{T} \theta \right)\]

    <p>which outputs a probability per ==discrete action==. The score function for this type of model is:</p>

\[\nabla_{\theta} \log \pi_{\theta} (a|s) = \phi(s,a) - \mathbb{E}_{\pi_{\theta}} [\phi(s, \cdot )]\]
  </li>
  <li>
    <p><strong>Gaussian Policy</strong>: very useful for ==continuos action space==, which we can parametrize by:</p>

\[\pi_\theta(a|s) \sim N( \mu(s), \sigma^{2})\]

    <p>for variance can be a fixed constant or parametrized as well. We model the mean by:</p>

\[\mu(s) = \phi(s)^{T} \theta\]

    <p>being a linear combination again. Then the score function for this is:</p>

\[\nabla_{\theta} \log \pi_{\theta} (a|s) = \frac{(a- \mu(s))\phi(s)}{\sigma^{2}}\]
  </li>
</ul>

<h3 id="policy-gradient-introduce-baseline">Policy Gradient: Introduce Baseline</h3>

<p>Another way to improve gradient estimates is to use a baseline $b(s_t)$. In the previous section we have derived:</p>

\[\nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \sum\limits_{t'=t}^{T-1} r_{t'} \right]\]

<p>It turns out that we can reduce variance by:</p>

\[\nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum\limits_{t'=t}^{T-1} r_{t'} - b(s_t) \right)\right]\]

<p>which is still an ==unbiased== estimate:</p>

<ul>
  <li>a near optimal choice is to consider $b(s_t)$ as the expected return $b(s_{t}) \approx \mathbb{E}[r_{t} + r_{t+1} + … + r_{T-1}]$.</li>
  <li>in that case, this means that we are increasing logprob of action at proportionally to how much returns $\sum\limits_{t’=t}^{T-1}  r_{t’}$ are better than expected.</li>
</ul>

<p>Why does this not introduce bias? We can prove this by showing:</p>

\[\mathbb{E}_\tau \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t) \right] = 0\]

<p>Proof:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220606171712.png" style="zoom:60%;display:block;margin:auto" /></p>

<p>note that this holds for any $b(s_t)$ that is a function of states, and:</p>

<ul>
  <li>the second equality holds because $b(s_t)$ only depends on current and past states</li>
  <li>the fourth equality holds because the probability of getting an action $a_t$ given $s_t$ is exactly specified by $\pi_{\theta} (a_{t} \vert  s_t)$</li>
</ul>

<p>Finally the question is what choice of $b(s_t)$ could reduce variance? There will be a number of choices, and some of which we will discuss next.</p>

<h4 id="vanilla-policy-gradient-algorithm-and-auto-differentiation">Vanilla Policy Gradient Algorithm and Auto Differentiation</h4>

<p>Using the baseline $b(s_t)$ derivation, the simplest algorithm would look like:</p>

<ol>
  <li>initialize policy parameter $\theta$, baseline $b$  (e.g. a NN, or a lookup table)</li>
  <li>for iteration $i=1,2, …$:
    <ol>
      <li>collect a set of trajectory using current policy $\pi_\theta$:</li>
      <li>(MC target) at each time step $t=1 , … , \vert \tau^{(i)}\vert$:
        <ol>
          <li>compute average return $G_t^{(i)} = \sum_{t’=t}^{T}r_t^{(i)}$</li>
          <li>compute <strong>advantage estimate</strong>: $A_t^{(i)} = G_t^{(i)} - b(s_t)$</li>
        </ol>
      </li>
      <li>refit the baseline by minimizing $\sum\limits_{i} \sum\limits_{t} \left\vert b(s_t^{(i)}) - G_t^{(i)} \right\vert ^{2}$
        <ul>
          <li>or instead of going over all trajectories (off-policy), we can only use the trajectories of the most recent iteration as well (on-policy)</li>
          <li>if only current iteration is used, then baseline is basically estimating $b(s_{t}) \approx V^{\pi_\theta}$ of the current policy</li>
        </ul>
      </li>
      <li>
        <p>(Policy Improvement) update the policy by policy gradient estimate by summing:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t\]

        <p>and plug this into SGD or ADAM, etc.</p>
      </li>
    </ol>
  </li>
</ol>

<blockquote>
  <p>Practically, many libraries have auto differentiation. Therefore, instead of calculating manually:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t\]

  <p>we can just define a ==surrogate loss function==:</p>

\[L(\theta) \equiv \sum_t \log \pi_\theta(a_t|s_t) \hat{A}_t\]

  <p>And then let ==auto differentiation== compute the gradient:</p>

\[\hat{g} = \nabla_\theta L(\theta)\]

</blockquote>

<h3 id="policy-gradient-actor-critic-methods">Policy Gradient: Actor-Critic Methods</h3>

<p>In the previous algorithm, we used MC targets which derives from the form:</p>

\[\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})\]

<p>having $R(\tau^{(i)})$ as a target we know leads to high variance but is unbiased (MC updates). But we can then reduce variance by introducing bias using <strong>bootstrapping</strong> and <strong>function approximation</strong> (e.g. TD vs MC)</p>

<blockquote>
  <p><strong>Actor-Critic Methods</strong>: Estimate the value function $V$ or $Q$, called ==critic==. Therefore, it maintains an explicit representation of ==both the policy and the value function==, and update both. (e.g. A3C model)</p>
</blockquote>

<p>Therefore, in the actor-critic case, we consider instead of:</p>

\[\nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum\limits_{t'=t}^{T-1} r_{t'} - b(s_t) \right)\right]\]

<p>Use:</p>

\[\nabla_{\theta} V(\theta) = \nabla_{\theta} \mathbb{E}_\tau [R] 
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( Q(s_t,a_t;w)- b(s_t) \right)\right]\]

<p>for $Q(s_t,a_t;w)$ being another function approximator using weights $w$. If we choose $b(s_t)$ to be the value function, then this becomes</p>

\[\nabla_{\theta} V(\theta)  
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}^{\pi} (s_{t}, a_t) \right], \quad \hat{A}^{\pi} (s_{t}, a_{t}) \equiv Q(s_{t}, a_{t} ) - V(s_t)\]

<h4 id="n-step-estimators">N-step Estimators</h4>

<p>It turns out that how exactly we model the $Q$ function can also be varied. Recall that</p>

\[\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})\]

<p>The critic can select any blend between TD and MC estimates for the true-state action value function:</p>

<ul>
  <li>
    <p>$\hat{R}<em>t^{(1)} = r</em>{t} + \gamma V(s_{t+1})$ being “TD(0)”, for which you bootstrap the value $V(s_{t+1})$ from your critic</p>
  </li>
  <li>$\hat{R}<em>t^{(2)} = r</em>{t} + \gamma r_{t+1} + \gamma^{2} V(s_{t+2})$ being “TD(1)”</li>
  <li>…</li>
  <li>$\hat{R}<em>t^{(\infty)} = r</em>{t} + \gamma r_{t+1} + \gamma^{2}r_{t+2} + … = G_t$ being MC</li>
</ul>

<p>From this , ths means hhat the ==critic can model $V$ and choose between==:</p>

<ul>
  <li>$\hat{A}<em>t^{(1)} = r</em>{t} + \gamma V(s_{t+1}) - V(s_t)$</li>
  <li>$\hat{A}<em>t^{(2)} = r</em>{t} + \gamma r_{t+1} + \gamma^{2} V(s_{t+1}) - V(s_t)$</li>
  <li>…</li>
  <li>$\hat{A}<em>t^{(\infty)} = r</em>{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + … - V(s_{t}) = G_{t} - V(s_t)$ becomes the MC update used in the (Vanilla Policy Gradient Algorithm)[#Vanilla Policy Gradient Algorithm and Auto Differentiation].</li>
</ul>

<p>to plug into the gradient equation:</p>

\[\nabla_{\theta} V(\theta)  
  = \mathbb{E}_\tau \left[ \sum\limits_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}^{\pi} (s_{t}, a_t) \right]\]

<p>and of course $\hat{A}_t^{(1)}$ would have <strong>low variance</strong> but high bias, and $\hat{A}_t^{(\infty)}$ would have high variance but low bias. Such models are also called ==Advantage Actor-Critic Methods== (A2C).</p>

<h2 id="automatic-step-size-tuning">Automatic Step Size Tuning</h2>

<p>In supervised learning case, we can overshoot the gradient if our step size is too big. However, it is not too bad because at least the ==data is fixed==, i.e. you might recover the local minima later.</p>

<p>However, in Reinforcement Learning cases, <strong>changing policy changes the data</strong>, this means that if you overshoot and obtain bad policies, then you might have ==no data that can learn towards optimas==.</p>

<ul>
  <li>previously we have been experimenting with different target values (MC vs TD target, etc)</li>
  <li>now, we care about how we use the gradient to update parameters, i.e. step size.</li>
</ul>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220606215929.png" style="zoom:70%;display:block;margin:auto" /></p>

<p>Therefore, what can we do to determine a good step size (and in order to <strong>achieve monotonic improvement</strong> we wanted before)?</p>

<ul>
  <li><strong>Line search</strong> in direction of gradient
    <ul>
      <li>Simple but expensive (perform evaluations along the line)</li>
      <li>Naive: ignores where the first order approximation is good or bad</li>
    </ul>
  </li>
  <li>
    <p>Given old policy weights $\theta_i$, we want to somehow find $\theta_{i+1}$ such that:</p>

\[\arg\max_{\theta_{i+1}} V^{\pi_{\theta_{i+1}}}(s)\]

    <p>so we ensure monotonic improvement and improvement is large. To do this we would need to do <strong>off-policy evaluation</strong>.</p>
  </li>
</ul>

<blockquote>
  <p>Central Problem: We have trajectories genreated from our old policy $\tau \sim \pi_{\theta_i}$, and we want to know the <strong>value of a new policy</strong> $V^{\pi_{\theta_{i+1}}}$ (e.g. results from a gradient update). Essentially this is off-policy evaluation!</p>
</blockquote>

<h3 id="evaluating-off-policy-value-function">Evaluating Off-Policy Value Function</h3>

<p>How do we estimate $V^{\pi_{\theta_{i+1}}}$ given only old data $V^{\pi_{\theta_{i}}}$ and trajectories $\tau \sim \pi_{\theta_i}$? Consider the following equality:</p>

\[V(\tilde{\theta}) = V(\theta) + \mathbb{E}_{\pi_{\tilde{\theta}}} \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]\]

<p>for the advantage being $A_{\pi_\theta}(s,a)$ we can compute using old weights (i.e. $A_{\pi_\theta}(s,a)= Q^{\pi_\theta}(s,a) - Q^{\pi_\theta}(s,\pi_\theta (s))$ ) <strong>but $(s,a) \sim \pi_{\tilde{\theta}}$ is from the new policy</strong>.</p>

<p>This can then be expressed as:</p>

\[\begin{align*}
  V(\tilde{\theta}) 
  &amp;= V(\theta) + \mathbb{E}_{\pi_{\tilde{\theta}}} \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]\\
  &amp;= V(\theta) + \sum\limits_{s} \mu_{\tilde{\theta}}(s) \left[ \sum\limits_{t=0}^{\infty}  \gamma^{t}A_{\pi_\theta} (s_{t}, a_t) \right]\\
  &amp;= V(\theta) + \sum\limits_{s} \mu_{\tilde{\theta}}(s) \sum\limits_{a} \pi_{\tilde{\theta}}(a|s) A_{\pi_\theta} (s_{t}, a_t) \\
\end{align*}\]

<p>therefore, the only unknown is $\mu_{\tilde{\theta}}(s)$ which is a ==stationary distribution of states under new policy $\pi_{\tilde{\theta}}$== as we don’t have new trajectories.</p>

<p>However, we do know $\mu_{\theta}(s)$ from the old policy $\pi_\theta$, and it turns out that the following <strong>approximation</strong> would provide a good lower bound estimate:</p>

\[L_{\pi_\theta} (\pi_{\tilde{\theta}}) \equiv V(\theta) + \sum\limits_{s} \mu_{\theta}(s) \sum\limits_{a} \pi_{\tilde{\theta}}(a|s) A_{\pi_\theta} (s_{t}, a_t)\]

<p>where we essentially substituted $\mu_{\tilde{\theta}}(s)$ for $\mu_{\theta}(s)$ . Notice that from this definition of objective function:</p>

\[L_{\pi_\theta} (\pi_\theta) = V(\theta)\]

<p>and of course we want $L_{\pi_\theta} (\pi_{\tilde{\theta}}) \approx V(\tilde{\theta})$, so there comes the theorem.</p>

<blockquote>
  <p><strong>Conservative Policy Iteration</strong>: if we take the new policy to be a mixture of old policy and a different policy:</p>

\[\pi_{new}(a|s) = (1-\alpha)\pi_{old}(a|s) + \alpha\pi_{old}'(a|s)\]

  <p>then this guarantees a lower bound:</p>

\[V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{2 \epsilon \gamma}{(1-\gamma)^2} \alpha^2\]

  <p>meaning the RHS is the lower bound of the true value we want $V^{\pi_{new}}$. And notice that all quantities on the RHS are computable/known!</p>
</blockquote>

<p>The above need a mixture of policy for the new policy. Can we make it even better? It turns out that we can have a lower bound of ==any stochastic policy== by:</p>

<blockquote>
  <p><strong>Lower Bound in General Stochastic Policy</strong>: for any new stochastic polict $\pi_{new}$, the following holds:</p>

\[V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} (D_{TV}^{\max} (\pi_{old}, \pi_{new}))^2\]

  <p>for $\epsilon = \max{s,a}\left\vert  A_{\pi} (s,a) \right\vert$ and the distance is defined as $D_{TV}^{\max} (\pi_{1}, \pi_{2})=\max_s D_{TV}(\pi_{1}, \pi_{2})$ for:</p>

\[D_{TV}(\pi_{1}(\cdot |s), \pi_{2}(\cdot|s)) \equiv \max_a | \pi_{1}(a|s) - \pi_{2}(a|s) |\]

  <p>essentially is a distance depending on the probability distribution.</p>
</blockquote>

<p>But of course since finding max will be difficult to work with, often we use the fact that:</p>

\[D_{TV}(p,q)^{2} \le D_{KL}(p,q)\]

<p>being the KL divergence between two distribution, which is more computable. Hence this means we can use:</p>

\[V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{old}, \pi_{new})\]

<p>Finally, how do we use this result to make sure we are having monotonic policy improvement?</p>

<blockquote>
  <p><strong>Guaranteed Improvement</strong>: recall that the goal is to make sure that the new policy $\pi_{i+1}$ (e.g. after gradient descent updates) have a better value than the old policy $\pi_i$. To achieve this, if we consider defineing a ==metric==:</p>

\[M_i(\pi) \equiv  L_{\pi_{i}} (\pi) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{i}, \pi)\]

  <p>Then, we realize that if we ==ensure improvement in this metric==:</p>

\[\begin{align*}
M_i(\pi_{i+1}) - M_i(\pi_i) &amp;&gt; 0\\
V^{\pi_{i+1}} - M_i(\pi_i) \ge M_i(\pi_{i+1}) - M_i(\pi_i) &amp;&gt; 0 \\
V^{\pi_{i+1}} - M_i(\pi_i) = V^{\pi_{i+1}} - V^{\pi_i} &amp;&gt; 0
\end{align*}\]

  <p>which achieved what we wanted, that $V^{\pi_{i+1}} - V^{\pi_i}$ is improved for all states (i.e. infinity norm), and where:</p>
  <ul>
    <li>
      <p>the second inequality due to the fact that we know by definition:</p>

\[V^{\pi_{new}} \ge L_{\pi_{old}} (\pi_{new}) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\pi_{old}, \pi_{new})\]
    </li>
    <li>
      <p>the last equality is because we know the KL divergence between the same distribution is zero:</p>

\[M_i(\pi_i) = L_{\pi_i}(\pi_i) - 0 = V^{\pi_i}\]

      <p>as the advantage function is zero for the same policy.</p>
    </li>
  </ul>
</blockquote>

<p>This means that if the ==new policy has a higher $M_i$, then my new policy has to be improving==.</p>

<ul>
  <li>
    <p>however, for cases when you have a large state-action space, evaluating the quantity:</p>

\[\epsilon = \max_{s,a}\left| A_{\pi} (s,a) \right|\]

    <p>would be difficult. So next we show some practical algorithms that “approximate” this.</p>
  </li>
  <li>
    <p>in general, algorithms that uses this type of objective will be called ==Minorization-Maximization (MM) algorithm==</p>
  </li>
</ul>

<h2 id="mm-objective-and-trust-regions">MM Objective and Trust Regions</h2>

<p>In the first part of this section, we were considering policy gradient with the objective being the value function itself, so that we were considering:</p>

\[\nabla_{\theta} V(\theta) \approx \frac{1}{m} \sum\limits_{n=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})\]

<p>But from the previous section, we found a way to guarantee the improvement of the policy by looking at the lower bound of $V^{\pi_{i+1}}$. This means that we can also ==directly optimize over the lower bound==:</p>

\[M_{\theta_i}(\theta) \equiv L_{\theta_{i}} (\theta) - \frac{4 \epsilon \gamma}{(1-\gamma)^2} D_{KL}^{\max} (\theta_{i}, \theta)\]

<p>But to make it practical as it may be hard to compute $\epsilon$, we consider:</p>

\[J_{\theta_i}(\theta) \equiv L_{\theta_{i}} (\theta) - c \cdot D_{KL}^{\max} (\theta_{i}, \theta)\]

<p>for $c$ is a <strong>penalty coefficient</strong> that we choose as a hyperparameter. Note that</p>

<ul>
  <li>if we used $c = \frac{4 \epsilon \gamma}{(1-\gamma)^2}$ is computable in your case, then you need to use a <strong>very small step size</strong>.</li>
  <li>or we can approximately find a <strong>step size</strong> in this case that will likely make the monotonic improvement hold: <strong>trust regions constraint</strong></li>
</ul>

<h3 id="trust-region-constraints">Trust Region Constraints</h3>

<p>Notice that we can reformulate the above objective into a constraint optimization task, and from which we can <strong>Use a trust region constraint on step sizes $\delta$ (Schulman, Levine, Abbeel, Jordan, &amp; Moritz ICML 2015)</strong>:</p>

\[\begin{align*}
  \max_{\theta} L_{\theta_{old}} (\theta &amp;) \\
  \mathrm{subject\,to}\, D_{KL}^{s\sim \mu_{\theta_{old}}} (\theta_{old},\theta)\le &amp;\delta
\end{align*}\]

<p>which uses the <strong>average KL divergence</strong> over states instead of max (max requires the KL is bounded at all states and yields an impractical number of constraints).</p>

<h3 id="trust-region-policy-optimization">Trust Region Policy Optimization</h3>

<p>The goal is to compute the policy improvement based on:</p>

\[\begin{align*}
  \max_{\theta} L_{\theta_{old}} (\theta &amp;) \\
  \mathrm{subject\,to}\, D_{KL}^{s\sim \mu_{\theta_{old}}} (\theta_{old},\theta)\le &amp;\delta
\end{align*}\]

<p>for</p>

\[L_{\theta_{old}} (\theta ) = V(\theta_{old}) + \sum\limits_{s} \mu_{\theta_{old}}(s) \sum\limits_{a} \pi_{\theta}(a|s) A_{\pi_{\theta_{old}}} (s, a)
$
we then make there further substitutions to make this more computable:

- instead of weigthing on the true stationary distribution:\]

<p>\sum\limits_{s} \mu_{\theta_{old}}\to \frac{1}{1-\gamma} \mathbb{E}<em>{s\sim \mu</em>{\theta_{old}}}</p>

\[- use ==importance sampling== to estimate the desired sum, which enables the use of an **alternate sampling distribution $q$** (other than the new policy $\pi_\theta$):\]

<table>
  <tbody>
    <tr>
      <td>\sum\limits_{a} \pi_{\theta}(a</td>
      <td>s) A_{\pi_{\theta_{old}}} (s, a)\to \mathbb{E}<em>{a\sim q}\left[ \frac{\pi</em>\theta(a</td>
      <td>s_n)}{q(a</td>
      <td>s_n)}  A_{\pi_{\theta_{old}}} (s_n, a)\right]</td>
    </tr>
  </tbody>
</table>

\[for $s_n$ is a particular sampled state. A simple choice of $q$ is to take the old policy $q(a\vert s)=\pi_{old}$.
- finally:\]

<p>A_{\theta_{old}}\to Q_{\theta_{old}}</p>

<p>$$</p>
<ul>
  <li>Note that these 3 substitutions ==do not change the solution== to the above optimization problem, but make them more computable.</li>
</ul>

<p>Therefore, this gives the following reformulation of the same objective:</p>

<blockquote>
  <p>Trust Region Objective Function: we want to optimize:
$$</p>

  <p>\begin{align<em>}
   \max_{\theta} \mathbb{E}_{a\sim q}\left[ \frac{\pi_\theta(a|s_n)}{q(a|s_n)}  Q_{\pi_{\theta_{old}}} (s_n, a )\right] <br />
   \mathrm{subject\,to}\, \mathbb{E}_{s\sim \mu_{\theta_{old}}} \left[ D_{KL} (\pi_{\theta_{old}}(\cdot,s),\pi_{\theta}(\cdot,s)) \right] \le &amp;\delta
\end{align</em>}</p>

  <p>$$
and often we take $q(a\vert s)=\pi_{old}$.</p>
</blockquote>

<p>Which gives rises to the TRPO algorithm:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220607164703.png" style="zoom:60%;" /></p>

<p>which has the following benefits and has been popular:</p>

<ul>
  <li>Policy gradient approach</li>
  <li>Uses surrogate optimization function</li>
  <li>Automatically constrains the weight update to a trusted region, to approximate where the first order approximation is valid</li>
  <li>Empirically consistently does well</li>
</ul>

<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>

<p>Essentially it is Model-Based Reinforcement Learning, which aims to do:</p>

<ul>
  <li>given experience, <strong>learn the model</strong> (i.e. transition and/or reward function)</li>
  <li>then use the model to plan and <strong>construct a value function or policy</strong>
    <ul>
      <li>since now you have that simulator model, you can also do model-free RL at this step</li>
    </ul>
  </li>
</ul>

<p>(whereas all the previous sections were directly learning the value function or policy from experience, which Model-Free RL)</p>

<p>Graphically, model-based RL is trying to answer the following questions:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608141633.png" style="zoom:50%;" /></p>

<p>where the trick of this against the Model-Free approach is that, when finding the next action, we ==do not have to compute the full value function/policy== (as in Model-Free approaches). We only need to find the ==next single action==, which is exploited by the MCTS algorithm.</p>

<p><strong>Advantages</strong>:</p>
<ul>
  <li>Can efficiently learn model by supervised learning methods
    <ul>
      <li>e.g. for modeling transition function we need to spit out a distribution, and for reward, it is a regression problem.</li>
    </ul>
  </li>
  <li>Can reason about model uncertainty
    <ul>
      <li>e.g.upper confidence bound methods for exploration/exploitation trade offs</li>
    </ul>
  </li>
  <li>can be good at transfer learning if only the reward function is changed.
    <ul>
      <li>i.e. if you have the model learnt, then you can zero-shot transfer</li>
    </ul>
  </li>
</ul>

<p><strong>Disadvantages</strong>:</p>
<ul>
  <li>First learn a model, then construct a value function. This means there are potentially two sources of approximation error
    <ul>
      <li>e.g. like imitation learning, we could have compounding errors</li>
    </ul>
  </li>
</ul>

<h2 id="model-learning">Model Learning</h2>

<p>Recall that a model is a representation of an MDP $(S,A,P,R)$, which can be <strong>parametrized by $\eta$</strong>:</p>

\[M = (P_{\eta}, R_{\eta} ), \quad \begin{cases}
  s_{t+1}\sim P_\eta(S_{t+1}|S_t,A_t) \\
  r_{t+1} = R_\eta(S_t,A_t) =R_\eta(R_{t+1}|S_t,A_t)
\end{cases}\]

<p>assuming that the state space and action space are already known. Typically we also assume that state transitions and rewards are independent:</p>

\[\mathbb{P}(S_{t+1},R_{t+1}|S_t,A_t) = P_{\eta}(S_{t+1}|S_t,A_t)R_{\eta}(R_{t+1}|S_t,A_t)\]

<blockquote>
  <p><strong>Goal</strong>: estimate a model $M_\eta$ from experience ${ S_1,A_1,R_1, …,S_T }$. This becomes a supervised problem to consider input of:
$$</p>

  <p>\begin{align<em>}
   S_1,A_1&amp;\to R_1,S_2<br />
   S_2,A_2&amp;\to R_2,S_3<br />
   &amp;…
\end{align</em>}</p>

  <p>$$
since there are two outputs, we can consider:</p>
  <ul>
    <li>learning $s,a\to r$ being a <strong>regression</strong> problem (output a scalar)</li>
    <li>learning $s,a\to s’$ being a <strong>density estimation</strong> problem (e.g. a vector of size $\left\vert S\right\vert$ with each slot being a probability)</li>
  </ul>

  <p>then the loss functions could be mean-square error and KL divergence, and etc.</p>
</blockquote>

<p>Example Models:</p>

<ul>
  <li>
    <p>Table Lookup Model, e.g. if state and actions are discrete, we can just count them:
$$</p>

    <table>
      <tbody>
        <tr>
          <td>\hat{P}(s’</td>
          <td>s,a) = \frac{1}{N(s,a)} \sum\limits_{t=1}^{T}\mathbb{1} {s’</td>
          <td>S_t=s,A_t=a}\</td>
        </tr>
        <tr>
          <td>\hat{R}(r</td>
          <td>s,a) = \frac{1}{N(s,a)} \sum\limits_{t=1}^{T}R(S_t=s,A_t=a)</td>
          <td> </td>
        </tr>
      </tbody>
    </table>

    <p>$$</p>
  </li>
  <li>Linear Expectation Model</li>
  <li>Linear Gaussian Model</li>
  <li>Gaussian Process Model</li>
  <li>Deep Belief Network Mode</li>
  <li>Bayseian DNN</li>
  <li>etc.</li>
</ul>

<hr />

<p><em>For instance</em>, consider using a table lookup model shown above and we have the following data:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Data</th>
      <th style="text-align: left">Table Lookup Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608151144.png" style="zoom:50%;" /></td>
      <td style="text-align: left"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608151159.png" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>for simplicity we assume there is only one action</li>
  <li>since we are storing averages and counts, essentially table look up decides:
    <ul>
      <li>$R(S=a)=0$ since all the time the immediate reward is zero</li>
      <li>$R(S=b)=0.75$ on average</li>
    </ul>
  </li>
</ul>

<p>What would happen if we used a MC/TD estimate of the value $V(A),V(B)$?</p>

<ul>
  <li>MC estimate considers full track, but since the track with $A$ is still zero:
    <ul>
      <li>$V(A)=0$</li>
      <li>$V(B)=0.75$</li>
    </ul>
  </li>
  <li>TD(0) estimate with infinite sampling propagates information by $V(A)=r+V(B)$ since $A\to B$ happens 100\%.
    <ul>
      <li>$V(B)=0.75$ still the same</li>
      <li>$V(A)=0+0.75=0.75$ is different</li>
    </ul>
  </li>
  <li>essentially the difference arises because TD assumes/factors in the MDP process, whereas the MC method does not.</li>
</ul>

<p>However, if we used our simulated model and ==sampled from it==, you could see the following data</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Data</th>
      <th style="text-align: center">Table Lookup Model</th>
      <th style="text-align: center">Sampled Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608151144.png" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608151159.png" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608152748.png" style="zoom:70%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that in this case, if you were to estimate a value function using MC method (e.g. for planning), you will get:</p>

<ul>
  <li>$V(A)=1$ instead</li>
  <li>$V(B)=0.75$ remains the same</li>
</ul>

<p>notice that value functions now becomes closer to our TD estimates.</p>

<h2 id="planning-with-a-model">Planning with a Model</h2>

<p>Once we have a learnt model, we can:</p>

<ol>
  <li><strong>sample experience from model</strong> $(S,A,P_\eta,R_\eta)$</li>
  <li>apply planning algorithms
    <ul>
      <li><strong>model-free</strong> RL to samples
        <ul>
          <li>e.g. MC control/policy iteration</li>
          <li>e.g. SARSA, Q-Learning</li>
        </ul>
      </li>
      <li><strong>Tree search</strong>!</li>
    </ul>
  </li>
</ol>

<p>However, if the model $(S,A,P_\eta,R_\eta)$ is imperfect, then obviously the performance of your planning will be limited by your model.</p>

<ul>
  <li>in those cases it is important to know <em>where it has gone wrong</em>, e.g. by reasoning about its uncertainty (see previous section), before switching back to use model-free RL</li>
</ul>

<p>But for here, we will focus on having a reasonbly good model, and how we can use tree search algorithms to compute the next step without explictly modelling the value functions.</p>

<h3 id="forward-search">Forward Search</h3>

<p>The idea here is simple. Since we have a model that can spit out next state and rewards, we can build a tree by <strong>trying out different actions</strong>:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608154536.png" style="zoom:50%;" /></p>

<p>But then the question is, what is the next action to take if we are currently at $s_t$? For that we need to construct a <strong>value of the next actions</strong>, and one way is to consider:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608154845.png" style="zoom:50%;" /></p>

<p>namely:</p>

<ul>
  <li>the value of the action is the <strong>mean</strong> of its future states</li>
  <li>the value of a state is the <strong>max</strong> of its actions</li>
</ul>

<blockquote>
  <p>Why might this be helpful? Notice that we don’t need to solve whole MDP, just ==sub-MDP starting from the current state==!</p>
</blockquote>

<p>However, to compute this at every step would be expensive. Do we have other approaches?</p>

<h3 id="simulation-based-search">Simulation-Based Search</h3>

<p>On the other hand, we can also utilize the <strong>model-free approaches</strong>, so that given a model $M=(S,A,P_\eta,R_\eta)$ we can:</p>

<ol>
  <li>given a policy $\pi$</li>
  <li>
    <p>sample experience ($K$ episodes) using the <strong>model</strong>:
 $$</p>

    <p>{ S_t^{k},A_t^{k},R_{t}^{k}, …,S_T^{k} }<em>{k=1}^{K}\sim M</em>\eta</p>

    <p>$$
 do model-free control algorithsm treating those simulated experience to be real:</p>
    <ul>
      <li>Monte-Carlo Control (e.g. policy iteration) $\to$ Monte-Carlo Search</li>
      <li>SARSA $\to$ TD-Search</li>
    </ul>
  </li>
</ol>

<h4 id="simple-monte-carlo-search">Simple Monte-Carlo Search</h4>

<p>Since we are considering MC methods, then the idea is:</p>

<ol>
  <li>Given a model $M_\eta$ and a simulation policy $\pi$</li>
  <li>for each action $a \in A$
    <ol>
      <li>
        <p>simulate $K$ episodes from current state $s_t$ by <em>considering all possible next actions, but then follows $\pi$</em>:
$$</p>

        <p>{s_t,a,R_{t+1}^{k},S^{k}<em>{t+1},\pi(S^{k}</em>{t+1}), …, S_T^{k}}<em>{k=1}^{K}\sim M</em>\eta,\pi</p>

        <p>$$</p>
      </li>
      <li>
        <p>from this, we can compute the $Q^{\pi}$ function for each possible next action:
$$</p>

        <p>Q^{\pi}(s_t,a) = \frac{1}{K} \sum\limits_{k=1}^{K} G_t^{k}(a)</p>

        <p>$$
for $G_t^{k}$ is the discounted reward of the $k$-th episode after following action $a$.</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Then the current best action is:
$$</p>

    <p>a_{t} = \arg\max_{a} Q^{\pi}(s_t,a)</p>

    <p>$$
which is like ==one-step of policy improvement==</p>
  </li>
</ol>

<p>But can we do better by finding an optimal policy instead of only improving by one-step?</p>

<h4 id="monte-carlo-tree-search-1">Monte-Carlo Tree Search</h4>

<p><em>Recall</em> that we can compute the optimal policy by considering the ==expectimax tree== that we have shown before:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608154845.png" style="zoom:50%;" /></p>

<p>however, the size of this tree scales as $O((\vert S\vert \vert A\vert )^{H})$ for a height of $H$. Therefore, doing this at each step would be computationally intractable.</p>

<p>Instead, we consider a more computationally efficent way to approximate this full tree.</p>

<blockquote>
  <p><strong>Heuristics</strong>: instead of simulting all possibilities, we can Iteratively construct and update tree by performing <strong>$K$ simulation episodes</strong> starting from the root state (i.e. $K$ rollout trajectories).</p>
  <ul>
    <li>note that for this to work we need to have a “good” policy while generating those trajectories, which we shall discuss next</li>
    <li>in fact, one key difference here against the Smple MC Search is that the <em>policy generating each rollouts would be improving as well</em></li>
  </ul>

  <p>Then, after the search is finished, simply consider:
$$</p>

  <p>a_t=\arg\max_a Q^\pi(s_t,a)</p>

  <p>$$
note that as the policy $\pi$ genearing those trajectories improves, eventually this will <strong>converge</strong> to $\pi \to \pi^{<em>}$ and $Q^{\pi} \to Q^{</em>}$.</p>
</blockquote>

<p>So we will consider two main changes as compared to the simple MC search:</p>

<ol>
  <li>rolling out $K$ trajecories will involve <em>changing policies</em> as well</li>
  <li>To evaluate the value of a tree node $i$ at state action pair $(s, a)$, <em>average over all rewards</em> received (instead of max and average) from that node onwards across simulated episodes in which this tree node was reached</li>
</ol>

<blockquote>
  <p>The key unasnwered question is: How to select what action to take during a simulated episode? <strong>Upper Confidence Tree Search</strong>!</p>
  <ul>
    <li>UCT: borrow idea from bandit literature and treat each node where can select actions as a multi-armed bandit (MAB) problem</li>
    <li>i.e. want to <strong>rollout more on actions that previously looked good</strong>, and spend less time on actions that previously looked bad</li>
  </ul>
</blockquote>

<p>Therefore, we consider maintaining an <strong>upper confidence bound</strong> over reward of each arm $s,a$ of an episode $i$:</p>

\[U(s,a,i) = \hat{Q}(s,a,i) + c\sqrt{\frac{\log(N(s))}{N(s,a)}}\]

<p>for $\hat{Q}(s,a,i)$ is just an average reward by counting:</p>

\[\hat{Q}(s,a,i)= \frac{1}{N(s,a,i)} \sum\limits_{k=1}^{K} \sum\limits_{u=t}^{T} \mathbb{1}(i\in \mathrm{episd.k})G_k(s,a,i)\]

<p>so essentially we are treating each state node as a separate MAB. Then, using this, while simulating episode $k$ at node $i$, our “policy” is to <strong>select action/arm with highest upper bound</strong> to expand (or evaluate) in the tree:</p>

\[a_{ik}=\arg\max U(s,a,i)\]

<p>This implies that the ==policy== used to simulate episodes with (and expand/update the tree) can ==change across each episode==.</p>

<p>All in all, we will sample new episodes by:</p>

<ul>
  <li><strong>Tree policy</strong>: pick actions for tree nodes to maximize $U(S, A)$, the upper confidence bound. e.g. use it until all next action has at least one sampled trajectory</li>
  <li><strong>Roll out policy</strong>: pick actions randomly, or another policy. This is used when we met a state with no data</li>
</ul>

<p>For a more detailed example of this, see the next section.</p>

<h2 id="case-study-go">Case Study: GO</h2>

<p>Go is 2500 years old and is considered as the Hardest classic board game:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608170059.png" style="zoom:60%;" /></p>

<p>However, in this case:</p>

<ul>
  <li>instead of a need to model the game, since all the rules are known, we <strong>do not need $M_\eta$</strong> in this case, but <strong>only do MCTS</strong>.</li>
  <li>since it is a two-player game, we consider a <strong>minimax tree</strong> (min value for opponent wining) instead of a expectimax tree</li>
</ul>

<p>For instance, we can consider a reward function of:
$$</p>

<p>R_{T} = \begin{cases}
1, &amp; \text{if black wins}<br />
0, &amp; \text{if white wins}
\end{cases}</p>

\[and our aim is to, say let black win. Then for $\pi=(\pi_B,\pi_W)$:\]

<p>V_\pi(s)=\mathbb{E}<em>\pi\left[R_T|S=s\right]=\mathbb{P}[\text{black wins}|S=s]<br />
V^*(s)=\max</em>{\pi_B}\min_{\pi_W}V_\pi(s)</p>

\[is a minimax problem. Then, essentially we consider sampling:

|$t=1$ | $t=2$ | $t=3$ | $t=4$ | $t=5$ |
|:----:|:----:|:----:|:----:|:----:|
|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608173657.png' style='zoom:50%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608172736.png' style='zoom:50%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608172750.png' style='zoom:50%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608172808.png' style='zoom:50%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608172827.png' style='zoom:50%;'/&gt;|

where essentially for the first 5 sampled episodes:

- the choice of opponent $\pi_W$ could be the agent at previous iteration (i.e. doing self-play)
- the default policy could just be a random policy
- we are performing the **default/rollout policy** for states that we have not seen before
- perform **tree policy** with picking the upper confidence bound for states when we have seen all actions.


&gt; **Importance of self-play**: essentially the idea is that in the beginning, both current agent and the opponent (e.g. previous checkpoint) would have a similar level. Then this makes the *reward signal being more dense* and the model learns faster.

And finally, for minimax tree, it looks like:

&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220608174045.png' style='zoom:60%;'/&gt;

where we are trying to minimize the probability of white winning and maximize the probability of black winning (as we are learning the policy for black).

**Advantages for MC Tree Search**:

- Highly **selective** best-first search
- Evaluates states dynamically (unlike e.g. DP)
- Uses **sampling** to break curse of dimensionality (model tree)
- Works for “black-box” models (only requires samples)
- Computationally efficient, anytime, parallelisable


# Safe Offline Reinforcement Learning

As we have known so far, RL algorithms updates/improves itself via interacting with the environment, which in some cases might not be feasible. (e.g. training conversation bot, but since its interaction is with people, it is often not possible).

&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821203131.png' style='zoom:50%;'/&gt;


&gt; Therefore, the aim of offline RL is to ==reuse previously collected datasets== (e.g. ImageNet) to create a data-driven RL framework.

Recall that some techniques we covered so far:

| Technique | Visual | Comments |
| :--:        |     :--: |   :--:    |
| on-policy RL|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821203859.png' style='zoom:50%;'/&gt; | continuously collects new data from environment
| on-policy RL|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821204030.png' style='zoom:50%;'/&gt;| collects new data after each policy update
| off-policy RL(sometimes called batch RL) | &lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821204141.png' style='zoom:100%;'/&gt; | we only have a buffer, no environment to interact with

Note that just as in off-policy RL, the policy we trained $\pi_\beta$ is different the policy that is used for data collection $\pi$.

---
In particular, consider solving the following problem:

&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821205626.png' style='zoom:80%;'/&gt;

where essentially:
- we cannot interact with the student/let them do more experiments
- hence, we only have two sets of data, which you can think of as:\]

<p>\begin{cases}
    (s_{1} =\text{A}, a_{1} = \text{add}, a_{2} = \text{multiply}, r=95)<br />
    (s_{1} =\text{B}, a_{1} = \text{multiply}, a_{2} = \text{add}, r=92)
  \end{cases}</p>

\[- so the task is to learn a policy **only from a fixed set of data** but can **generalize**/reason about unseen cases

In addition to produing a new policy:

&gt; ==Safe Offline/Batch RL== considers the following guarantee. Given past experience from some current policy $\pi$, we want to produce a new policy $\pi_\beta$ that
&gt; - with the probability at least $1 - \delta$, will not be worse than $\pi_\beta$
&gt; - guarantee should not contingent on tuning any hyperparamters
&gt; 
&gt; In general, we will be thinking of the **confidence** ($1-\delta$) we have on our predicted expected outcome using our policy $\pi_\beta$.

This is particularly useful in high stakes domain (e.g. medicine).

Finally, just to make the task clear:

&gt; **Task**: given some dataset $D$, we want to learn the ==best possible policy $\pi_e$== from it. Notice that sometimes learning a very good policy might not even be possible if $D$ contains no good data (e.g. only visiting the first few rooms in Montezuma's Revenge). In general, it depends on the coverage and will be discussed in section [Importance Sampling for Policy Evaluation](#Importance Sampling for Policy Evaluation).

## Safe Offline RL Formulation

We will consider the following notation for consistency:

- Our Policy $\pi$: $\pi(a) = P(a_t = a\vert s_t=s)$
- Trajectory: $T=(s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, ..., s_{L}, a_{L}, r_{L})$
- Historical/Training Data: $D= \{ T_1, T_2, ..., T_n \}$
- Behavioral policy used to generated the data $D$: $\pi_b$
- Objective:\]

<table>
  <tbody>
    <tr>
      <td>V^{\pi} = \mathbb{E} \left[ \sum\limits_{t=1}^{L} \gamma^{t} R_{t}</td>
      <td>\pi \right]</td>
    </tr>
  </tbody>
</table>

\[(note that in many cases we may not know what $\pi_b$ is, especially when it is generated by human. This will be discussed at the end of the section, **but for now we assume that it is known**.)

Then our aim is to have an algorithm $A$ learn a policy $A(D) \to \pi$ such that:\]

<p>P(V^{A(D)} \ge V^{\pi_{b}}) \ge 1 - \delta</p>

\[where the value of the behavioral policy $V^{\pi_{b}}$ can be estimated by MC estimation from the historical data, that $V^{\pi_b} (s) = (1/n)\sum\limits_{i=1}^{n} G_i(s)$ for $G_i(s)$ is the discounted return starting from state $s$ and ending at the end of the trajectory (and there are $n$ trajectories).

Or, more commonly, we could consider some external baselines $V_{\min}$ as well, that:\]

<p>P(V^{A(D)} \ge V_{\min}) \ge 1 - \delta</p>

<p>$$</p>

<p>Realize that to find out $V^{A(D)}$, we need to take our old data to <strong>compute how good our new policy is</strong>, which is intrinsically off-policy.</p>

<h2 id="off-policy-policy-evaluation">Off Policy Policy Evaluation</h2>

<blockquote>
  <p><strong>Off Policy Policy Evaluation (OPE)</strong></p>
  <ul>
    <li>for policy $\pi_e$ we want to evaluate (e.g. $A(D)\to \pi$), convert historical data $D$ into $n$ independent and unbiased estimates of $V^{\pi_e}$</li>
  </ul>
</blockquote>

<p>Essentially:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821220503.png" style="zoom:50%;" /></p>

<p>Therefore, our aim is:</p>

<blockquote>
  <p>Estimate value of policy $\pi_1$, $V^{\pi_1}(s)$, but <strong>given episodes $D= { T_1, T_2, …, T_n }$ generated under $\pi_2$</strong></p>
  <ul>
    <li>recall that $V^{pi}(s) = \mathbb{E}<em>\pi [G</em>{t} \vert s_t=s]$</li>
    <li>we want to learn an unbiased estimate of $V^{\pi_1}(s)$</li>
  </ul>
</blockquote>

<h3 id="mc-off-policy-evaluation">MC Off Policy Evaluation</h3>

<p>Once mentioned unbiased, we should think of MC methods (as opposed to TD methods, which does sampling + bootstrapping). Additionally, this ==does not require the world to be Markov==, which in realiy is often not the case!</p>

<p>First, we realize that because the policies are different $\pi_{1} \neq \pi_{2}$, we have <strong>two different distribution of states $\tau_{\pi_1}\neq  \tau_{\pi_2}$</strong> (otherwise the policies have the same value)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\tau_{\pi_1}$</th>
      <th style="text-align: center">$\tau_{\pi_2}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821231215.png" style="zoom:100%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821231318.png" style="zoom:100%;" /></td>
    </tr>
  </tbody>
</table>

<p>Suppose that $\pi_{1}, \tau_{\pi_1}$ is known/collected. The idea is: how do we change the distribution to that they look alike? ==Importance Sampling==!</p>
<ul>
  <li>essentially reweight the samples from $\tau_{\pi_2}$ so that samples from $\tau_{\pi_1}$</li>
</ul>

<h3 id="importance-sampling-for-policy-evaluation">Importance Sampling for Policy Evaluation</h3>

<blockquote>
  <p><strong>Importance Sampling</strong>: goal is to estimate the expected value of a function $f(x)$ under some probability distribution $q(x)$, without knowing the distribution $q(x)$.</p>

  <ul>
    <li>
      <p>we want to know:
$$</p>

      <p>\mathbb{E}_{x \sim q}[f(x)]</p>

      <p>$$</p>
    </li>
    <li>but we only have samples from $p(x)$, so that $x_1, x_2, …, x_n$ sampled form $p(x)$</li>
    <li>(just swap $x \to s$, and $f(x)\to G_(s)$ you should see why this could be helpful)</li>
  </ul>
</blockquote>

<p>Under few assumptions, we can use those samples to obtain an unbiased estimate of $\mathbb{E}_{x \sim q}[f(x)]$
$$</p>

<p>\begin{align<em>}
  \mathbb{E}_{x\sim q}[f(x)] 
  &amp;= \int_{x} q(x)f(x)dx<br />
  &amp;= \int_{x} \frac{p(x)}{p(x)}q(x) f(x) dx<br />
  &amp;= \int_{x} p(x) \left[ \frac{q(x)}{p(x)} f(x)\right] dx<br />
  &amp;\approx \frac{1}{n} \sum\limits_{i} \frac{q(x_i)}{p(x_i)}f(x_i),\quad x_i\sim p(x) 
\end{align</em>}</p>

\[so that essentially we sample data $x$ from $p(x)$, but ==reweigh== the probability so that it looks the same as if the came from $x\sim q(x)$

Therefore, back in the case of RL, let $T_j$ be the episode $j$ of states actions and rewards (that are collected/sampled by $p(x)$):\]

<p>T_{j} = (s_{j,1}, a_{j,1}, r_{j,1}, s_{j,2}, a_{j,2}, r_{j,2}, …, s_{j,L}, a_{j,L}, r_{j,L})</p>

\[We need to first compute the ratio of $p(T_j\vert \pi_e)$ (the $q(x)$) over $p(T_j\vert \pi_b)$ (the $p(x)$):\]

<table>
  <tbody>
    <tr>
      <td>p(T_j</td>
      <td>\pi_e) =p(s_{j,1}) \prod_{i=1}^{L}\underbrace{p(a_{j,i}</td>
      <td>s_{j,i})}<em>{\mathrm{policy}} \underbrace{p(r</em>{j,i}</td>
      <td>s_{j,i},a_{j,i})}<em>{\text{reward model}} \underbrace{p(s</em>{j,i+1}</td>
      <td>s_{j,i},a_{j,i})}_{\text{transition model}}</td>
    </tr>
  </tbody>
</table>

\[Then if we compute the ratio:\]

<p>\begin{align<em>}
  \frac{p(T_j|\pi_e)}{p(T_j|\pi_b)}
  &amp;= \frac{p_{\pi_e}(s_{j,1})}{p_{\pi_b}(s_{j,1})} \prod_{i=1}^{L} \frac{p_{\pi_e}(a_{j,i}|s_{j,i})}{p_{\pi_b}(a_{j,i}|s_{j,i})}<br />
  &amp;= \prod_{i=1}^{L} \frac{p_{\pi_e}(a_{j,i}|s_{j,i})}{p_{\pi_b}(a_{j,i}|s_{j,i})}<br />
  &amp;= \prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})}
\end{align</em>}</p>

\[where 
- in the first equality, the over terms cancel since they do not depend on the policy chosen. 
- in the second equality, we are assuming the start state is the same for both policies

Notice that the interpretation here is simple. Consider comparing the value of the policy using its start state, then:
- we have a bunch of trajectories and computed expected rewards $(T_1,G_1),(T_2,G_2),...,(T_n,G_n)$ sampled from following $\pi_b$
- then the value of this policy $\pi_b$ can be estimated by $(1/n)\sum\limits G_i$
- but to estimate the value of $\pi_e$, we ==reweigh the probabilities by thinking how likely those trajectories are to be generated by $\pi_e$==. So that if it is more likely, i.e. $p(T_j\vert \pi_e)&gt;p(T_j\vert \pi_b)$, we will increase the weight using the above ratio.

In general, $s_{j,1}$ can be any state as you can just rederive the above by considering $p(T_j\vert \pi_e,s=s_{j,1})$.

Finally, this gives us the estimate:\]

<p>\begin{align<em>}
  V^{\pi_e}(s)
  &amp;\approx \frac{1}{n} \sum\limits_{j=1}^{n} \frac{p(T_j|\pi_e,s)}{p(T_j|\pi_b,s)} G(T_j)<br />
  &amp;= \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})} \right) G(T_j)
\end{align</em>}</p>

\[Finally, since future cannot affect past rewards, we typically consider rewards only up to the timestep $t$ to reduce variance in our estimate:\]

<p>\begin{align<em>}
  V^{\pi_e}(s)
  &amp;\approx \frac{1}{n} \sum\limits_{j=1}^{n} \left(\prod_{i=1}^{L} \frac{\pi_e(a_{j,i}|s_{j,i})}{\pi_b(a_{j,i}|s_{j,i})} \right) \left( \sum_{t=1}^L \gamma^T R_t^{i} \right)
  &amp;\equiv \frac{1}{n} \sum\limits_{j=1}^{n} w_i \left( \sum_{t=1}^L \gamma^T R_t^{i} \right)
\end{align</em>}</p>

\[note than since sometimes the importance weights $w_i$ can become very small, there is a Weighted Importance Sampling (WIS) algorithm to deal with this tha basically switch $1/n$ ratio to $1/\sum w_i$ (however, this increase bias while reduces variance)

Now, what are the assumptions used to make IS work? You might notice the term $\frac{\pi_e(a_{j,i}\vert s_{j,i})}{\pi_b(a_{j,i}\vert s_{j,i})}$ could have gone badly, and it is extactly the case

&gt; **Importance Sampling Assumptions**: since we are reweighing samples from $\pi_b$, if we have distributions that are non-overlapping, then this will obviously not work.
&gt; - in particular, if we have any single case that $\pi_b(a\vert s)=0$ but $\pi_e(a\vert s)&gt;0$, then this will not work.
&gt; - therefore, for this to work, we want to have a large ==coverage==: so that for $\forall a,s$ such that $\pi_e(a\vert s)&gt;0$, you want $\pi_b(a\vert s)&gt;0$.

Intuively, this means that if $\pi_e$ is not too far off from $\pi_b$, then the importance sampling would work reasonably.

### Adding Controlling Variates

Given some random variable $X$, we want to estimate $\mu = \mathbb{E}[X]$.

- we have our estimator being $\hat{\mu}=X$
- unbiased estimator: $\mathbb{E}\left[\hat{\mu}\right]=\mathbb{E}\left[X\right]=\mu$
- variance of this estimator: $\mathrm{Var}\left[\hat{\mu}\right]=\mathrm{Var}\left[X\right]$

Now, we want to show that in many cases we can use some tricks to reduce the variance while still having an unbiased estimator. Consider another random variable $Y$:

- let our new estimator be $\hat{\mu}=X-Y+\mathbb{E}[Y]$
- it is still unbiased because:\]

<p>\mathbb{E}[\hat{\mu}] = \mathbb{E}[X-Y + \mathbb{E}[Y]] = \mathbb{E}[X] - \mathbb{E}[Y] + \mathbb{E}[Y] = \mathbb{E}[X]</p>

\[- it however can get a lower variance:\]

<p>\mathrm{Var}[\hat{\mu}] = \mathrm{Var}[X-Y+\mathbb{E}[Y]]=\mathrm{Var}[X-Y] = \mathrm{Var}[X] + \mathrm{Var}[Y] - 2 \mathrm{Cov}(X,Y)</p>

\[therefore, ==we just need $2\mathrm{Cov}(X,Y) &gt; \mathrm{Var}[Y]$== to lower the variance in total while not introducing bias. Note that this might sound like some free lunch, but it is not because we are using $Y$ that has some information about $X$ since $2\mathrm{Cov}(X,Y) &gt; \mathrm{Var}[Y]$.

---

Therefore, we can use this fact and use $X$ being the **importance sapmling estimator**, and $Y$ being a **control variate** build from an approxiamte model of the MDP (e.g. a Q value estimate). This gives a Doubly Robust Estimator (Jiang and Li, 2015)

&gt; **Doubly Robust Estimator**: robust to (1) poor approximate model/control variate, and (2) error in estimates of $\pi_b$ (importnace sampling) because:
&gt; - if the model is poor, the estimates $V^{\pi_b}$ is still unbiased
&gt; - if importance sampling could not see some $\pi_b$ state/actions, but the model/contorl variate is good, then MSE will still be low.

And just to briefly show the equation:\]

<table>
  <tbody>
    <tr>
      <td>DR(\pi_{e}</td>
      <td>D) = \frac{1}{n} \sum\limits_{i=1}^{n} \sum\limits_{t=0}^{L} \gamma^{t} \underbrace{w_i}<em>{\text{IS weights}} (R_t^{i} - \underbrace{\hat{q}^{\pi_e}(S_t^{i},A_t^{i})}</em>{\text{Y}})+\gamma^{t} \rho_{t-1}^{i}\underbrace{\hat{V}^{\pi_e}(S_t^{i})}_{\mathbb{E}[Y]}</td>
    </tr>
  </tbody>
</table>

<p>$$</p>

<hr />

<p>Finally, we can show the properties of those different algorithms empically by simulating a small grid world (e.g. 4x4 world), providing the algorithm with some data collected under polict $\pi_b$ and ask it to evaluate the value of some new policy $\pi_e$:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822195359.png" style="zoom:80%;" /></p>

<p>where:</p>
<ul>
  <li>the vertical axis is the MSE, which is $\hat{V}^{\pi_e}-V^{\pi_e}$ for $V^{\pi_e}$ is known beforehand since the world is simulated</li>
  <li>so we want to get low MSE with small amount of data</li>
</ul>

<h2 id="high-confidence-off-policy-policy-evaluation">High confidence Off-policy Policy Evaluation</h2>

<p>Now, we have had some methods of how to obtain an OPE of $\pi_e$. The question is how confidence we are of our estimates?</p>

<blockquote>
  <p><strong>High-confidence off-policy policy evaluation (HCOPE)</strong></p>
  <ul>
    <li>Use a concentration inequality to convert the $n$ independent and unbiased estimates of $V^{\pi_e}$ into a $1-\delta$ confidence lower bound on $V^{\pi_e}$</li>
  </ul>
</blockquote>

<p>You can often think of such as confidence as the confidence used for RL exploration: how confident are we in terms of the new actions we are taking?</p>

<p>Essentially:
<img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821222311.png" style="zoom:50%;" /></p>

<p>To be be able to know the confidence interval, we first need to revisit Hoeffding’s inequality:</p>

<blockquote>
  <p><strong>Hoeffding’s Inequality</strong>: let $X_1, X_2, …, X_n$ be $n$ IID random variables such that $X_i\in[0.b]$. THen with the probability at least $1-\delta$:
$$</p>

  <p>\mathbb{E}[X_i]\ge \frac{1}{n} \sum\limits_{i=1}^{n} X_i - b \sqrt{\frac{\ln(1/\delta)}{2n}}</p>

  <p>$$
where $X_i=\frac{1}{n}\sum\limits_{i=1}^{n} (w_{i} \sum\limits_{t=1}^{L} \gamma^{T} R_{t}^{i})$ in our case</p>
</blockquote>

<p>However, the problem with applying this directly is that we can get very high $b$ since the weights $w_i$ might become big for rare but successful events.</p>
<ul>
  <li>for instance, if we you have 10,000 trajectories/samples, we could get a 95% confidence lower bound of the policy’s vlaue being $-5,8310,000$</li>
  <li>whereas the true value would be $0.19$</li>
</ul>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">IS weighted Return</th>
      <th style="text-align: center">Idea</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822204110.png" style="zoom:60%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822204215.png" style="zoom:70%;" /></td>
    </tr>
  </tbody>
</table>

<p>where by cutting the large weights off, we will get a lower expected value but not have confidence interval exploding. Therefore, in practice, we can:</p>

<ul>
  <li>take 20% of the data to optimize the cutoff</li>
  <li>use 80% of the data to compute lower bound</li>
</ul>

<p>results for a certain policy on mountain climbing gives (true value $0.19$)</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822204717.png" style="zoom: 80%;" /></p>

<p>where the other columns are other concentration based methods.</p>

<h2 id="safe-policy-improvement">Safe Policy Improvement</h2>

<blockquote>
  <p>Safe policy improvement (SPI)</p>
  <ul>
    <li>
      <p>Use HCOPE method to create a safe batch reinforcement learning algorithm, by doing:
$$</p>

      <p>\pi = \arg\max_{\pi_e} V^{\pi_e}</p>

      <p>$$
with some confidence intervals</p>
    </li>
  </ul>
</blockquote>

<p>Essentially:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220821222520.png" style="zoom:50%;" /></p>

<p>for instance, if there is too little data, we should be able to say no that we can have some new policy/improved policy is safe.</p>

<p>Practically, some current approach involve</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822205037.png" style="zoom:80%;" /></p>

<p>Note that this is a algorithm that only <strong>improves the policy for a single step</strong>. Other future areas/goals involve improving the policy over multiple steps.</p>

<h1 id="general-offline-rl-algorithms">General Offline RL Algorithms</h1>

<blockquote>
  <p>A relevant content will be UCB CS285 on YouTube. <a href="https://www.youtube.com/watch?v=NV4oSWe1H9o&amp;list=PL_iWQOsE6TfXxKgI1GgyV1B_Xa0DxE5eH&amp;index=71">link</a></p>
</blockquote>

<p>Besides those safety constraints, there are also many other offline RL algorithms that are worth discussion. Before going to any details, we want to make certain high level ideas clear. Given this dataset $D$:</p>

<ul>
  <li>can offline RL perform <strong>better than the best behavior in $D$</strong>? Possibly yes, because of generalization: good behavior in one place may sufggst good behavior in another place</li>
  <li>can offline RL learn the <strong>best policy just from $D$</strong>? Often no, because we might have very few or no samples in $D$ where we see traces of the best policy, hence it is not even possible.</li>
</ul>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220822210525.png" style="zoom:60%;" /></p>

<p>But more generally, we consider:</p>
<ul>
  <li>$D = {(s_i, a_i, s_i^{\prime}, r_i}$ the same as before</li>
  <li>$\pi_b$ is the behavior policy creating the data $D$, so that $s \sim \mu_{\pi_b}(s)$ and $a \sim \pi_b(a\vert s)$</li>
  <li>however, now we generally assume that ==$\pi_b$ is NOT known==</li>
  <li>we want to find a new policy $\pi_{\mathrm{new}}$ that can combine good decisions</li>
</ul>

<p>And, as mentioned before, since $\pi_b \neq \pi_\mathrm{new}$ for $\pi_\mathrm{new}$ we want to learn, there is a problem of ==distribution shift==</p>

<blockquote>
  <p><strong>Distribution Shift for learning Value Functions in Offline RL</strong>: consider we are learning a value function $Q$. Idealy, if we want to learn $Q^{\pi_{b}}$, we consider the objective:
$$</p>

  <p>\min_Q \mathbb{E}<em>{(s,a,s’)\sim D} \left[  \left(\underbrace{r(s,a) + \mathbb{E}</em>{a’\sim \pi_b}[ Q(s’,a’)]}_{\mathrm{target}} - Q(s,a)\right)^2 \right]</p>

\[in practice, we can only do:
- using sample mean instead of expcted value (as usual)
- can only sample $a' \sim \pi_{e}$ since we do not have the behavioral policy, hence we are doing:\]

  <p>\min_Q \mathbb{E}<em>{(s,a,s’)\sim D} \left[  \left(\underbrace{r(s,a) + \mathbb{E}</em>{a’\sim \pi_\mathrm{new}}[ Q(s’,a’)]}_{\mathrm{target}} - Q(s,a)\right)^2 \right]</p>

  <p>$$
which we only expect to work if $\pi_\mathrm{new}(a\vert s) \approx \pi_b(a\vert s)$.</p>
</blockquote>

<p>However, in reality we usually consider having:
$$</p>

<table>
  <tbody>
    <tr>
      <td>\pi_{\mathrm{new}} = \arg \max_{\pi} \mathbb{E}_{a \sim \pi(a</td>
      <td>s)} [Q(s,a)]</td>
    </tr>
  </tbody>
</table>

\[so that the $\pi_\mathrm{new}$ learnt usually ends up fooling us by adverserially picking large values in the (inaccurate) Q function estiamte. Graphically:

| $\hat{Q}$ estimate of $\pi_\mathrm{new}$ | Actual Return of $\pi_\mathrm{new}$ |
| :--:                      | :--:                                |
| &lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826192651.png' style='zoom:60%;'/&gt; |&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826192630.png' style='zoom:60%;'/&gt; |

Therefore, what can we do this such a problem?

## Policy Constraint Methods

A simple and a bit naive way is to consider a policy $\pi_\mathrm{new}$ such that:\]

<table>
  <tbody>
    <tr>
      <td>\pi_\mathrm{new}(a</td>
      <td>s) = \arg\max_\pi \mathbb{E}_{a \sim \pi(a</td>
      <td>s)} [Q(s,a)],\quad \mathrm{s.t.} D_{KL}(\pi</td>
      <td> </td>
      <td>\pi_b) \le \epsilon</td>
    </tr>
  </tbody>
</table>

\[and then continue with the Bellman update:\]

<p>Q(s,a) \gets r(s,a) + \mathbb{E}<em>{a’ \sim \pi</em>\mathrm{new}} [Q(s’,a’)]</p>

\[while this seems to solve distribution shift, there are several issues:

1. usually we don't know what $\pi_b$ is (e.g. human collected data)
2. if you are too close to $\pi_b$, then you cannot improve your policy, even if your data $D$ already has a large support/coverage (e.g. $\pi_b$ is uniformly random)

But before we consider other methods, let us dive into more details about how/when this idea could work. Consider the case that you are at some state $s$ and given some rewards for some actions in your $D$:

| Data | Fitted Q Function | Constrained New Policy $\pi$ |
| :--: | :--: | :--:|
|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826211857.png' style='zoom:100%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826211906.png' style='zoom:80%;'/&gt;|&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826212034.png' style='zoom:80%;'/&gt;|

where:

- the orange dots are the given $Q$ values
- the middle fitted $Q$ function would give bad policy exploiting the tails
- the last one seems better, but it is still giving a bit too high Q values to non-optimal actions

Therefore, given this intution, we would prefer to have a support policy (instead of $\pi_b$) to look like:

&lt;img src='/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826212548.png' style='zoom:50%;'/&gt;

so that KL-divergence would work the best. This means we consider the support constraint:\]

<table>
  <tbody>
    <tr>
      <td>\pi(a</td>
      <td>s) \ge 0 \text{ only if } \pi_b(a</td>
      <td>s) \ge \epsilon</td>
    </tr>
  </tbody>
</table>

\[which we can approximate using methods such as MMD as we don't have $\pi_b$ directly, but in practice they are very difficult to implement.

---

Finally, here we show some examples of how to ==explicitly include policy constraints== in practice:

1. consider the case of using Actor Critic methods, where the actor $\pi_\theta(a\vert s)$ has the objective:\]

<table>
  <tbody>
    <tr>
      <td>\theta \gets \arg\max_\theta \mathbb{E}<em>{s \sim D} \left[ \mathbb{E}</em>{a\sim \pi_\theta(a</td>
      <td>s)} [Q(s,a)] \right]</td>
    </tr>
  </tbody>
</table>

\[but for KL divergence constraint, we want:\]

<table>
  <tbody>
    <tr>
      <td>D_{KL}(\pi_\theta</td>
      <td> </td>
      <td>\pi_b)  = \mathbb{E}<em>{\pi</em>\theta}[\log \pi_\theta(a</td>
      <td>s) - \log \pi_b(a</td>
      <td>s)] = \mathbb{E}<em>{\pi</em>\theta}[\log \pi_b(a</td>
      <td>s)] - \mathcal{H}(\pi_\theta)</td>
    </tr>
  </tbody>
</table>

\[for $\mathcal{H}(\pi_\theta)$ is the entropy. We can then easily convert constrained problems into unconstrained optimization using Legrange's Methods:\]

<table>
  <tbody>
    <tr>
      <td>\theta \gets \arg\max_\theta \mathbb{E}<em>{s \sim D} \left[ \mathbb{E}</em>{a\sim \pi_\theta(a</td>
      <td>s)} [Q(s,a)+ \lambda\log \pi_b(a</td>
      <td>s)] + \lambda \mathcal{H}(\pi_\theta(a</td>
      <td>s)\right]</td>
    </tr>
  </tbody>
</table>

\[for $\lambda &gt; 0$ is the Legrange Multiplier. But again, this means you need to know $\pi_b$, which unfortunatly you cannot avoid and commonly is done by doing behavioral cloning on the dataset $D$.
2. Another simple way is to change the reward function:\]

<table>
  <tbody>
    <tr>
      <td>\bar{r}(s,a) = r(s,a) - D_{KL}(\pi</td>
      <td> </td>
      <td>\pi_b)</td>
    </tr>
  </tbody>
</table>

\[which can not only makes current behavior more consistent, but also accounts for future behavior/divergence. This form of approach is called ==Behavior Regularized Offline RL==

But again, generally best offline RL algorithms today do not use these methods.

## Implicit Policy Constraint Methods

Recall the for policy constraint, we are considering the problem:\]

<table>
  <tbody>
    <tr>
      <td>\pi_\mathrm{new}(a</td>
      <td>s) = \arg\max_\pi \mathbb{E}_{a \sim \pi(a</td>
      <td>s)} [Q(s,a)],\quad \mathrm{s.t.} D_{KL}(\pi</td>
      <td> </td>
      <td>\pi_b) \le \epsilon</td>
    </tr>
  </tbody>
</table>

\[and by using Legrange Duality and solving for the optimal policy we get:\]

<table>
  <tbody>
    <tr>
      <td>\pi^{\star}(a</td>
      <td>s) = \frac{1}{Z(s)} \pi_b(a</td>
      <td>s) \exp\left( \frac{1}{\lambda}A^{\pi}(a</td>
      <td>s) \right)</td>
    </tr>
  </tbody>
</table>

\[which intuitively means that:

- suboptimal actions are exponentially less likely
- even for very high advantage $A$, if the action is rare in behavior policy $\pi_b$ then this is bad
- $\lambda$ can be often treated as a hyperparameter or tuned

Since this is the best policy, then it means we can approximate using weighted maximum likelihood:\]

<table>
  <tbody>
    <tr>
      <td>\pi_\mathrm{new}(a</td>
      <td>s) = \arg\max_\pi \mathbb{E}_{(s,a)\sim \pi_b(a</td>
      <td>s)}  \left[ \log\pi(a</td>
      <td>s)  \underbrace{\frac{1}{Z(s)}\exp\left(\frac{1}{\lambda}A^{\pi_{\mathrm{old} (s,a)}}\right)}_{\text{weights }w(s,a) } \right]</td>
    </tr>
  </tbody>
</table>

\[where the advantage function is just given by critic. In a sense this is a **weighted behavior cloning**, so that it is imitating the good actions more than the bad actions.

To implement this, all you need to do is to change slightly the actor-critic training:

1. train critic as usual with the loss:\]

<table>
  <tbody>
    <tr>
      <td>L_C(\phi) = \mathbb{E}<em>{(s,a,s’)\sim D} \left[ (Q</em>\phi(s,a) - ( r(s,a)+\gamma \mathbb{E}<em>{a’\sim \pi</em>\theta(a’</td>
      <td>s’)}[Q_\phi(s’,a’)] ))^{2}  \right]</td>
    </tr>
  </tbody>
</table>

\[2. train actor with our constrained loss:\]

<table>
  <tbody>
    <tr>
      <td>L_A(\theta) = - \mathbb{E}_{(s,a)\sim \pi_b(a</td>
      <td>s)}  \left[ \log\pi_\theta(a</td>
      <td>s)  \frac{1}{Z(s)}\exp\left(\frac{1}{\lambda}A^{\pi_{\mathrm{old} (s,a)}}\right) \right]</td>
    </tr>
  </tbody>
</table>

\[3. update the actor and critic loss via gradient descent alternatingly

More details see this paper [Nair, Dalal, Gupta, Levine. 2020](https://arxiv.org/pdf/2006.09359.pdf). Some more caustions of this method include the fact that you could still query OOD actions **during** training (constraints respected only at expectation), which could happen when computing the target value for updating critic or when estimating the advantage function.

### Implicit Q-Learning

Is there a way to avoid OOD actions while learning the Q function? This serves for the motivation of the work [Kostrikov, Nair, Levine. 2021](https://arxiv.org/pdf/2110.06169.pdf), with the idea being formulated as follows.

1. we consider instead of the following update rule\]

<p>Q(s,a) \gets r(s,a) + \mathbb{E}<em>{a’ \sim \pi</em>{\mathrm{new}}}[Q(s’,a’)]</p>

\[we view it as\]

<p>Q(s,a) \gets r(s,a) + \underbrace{V(s’)}_{\text{just another neural network}}</p>

\[2. next, we recall that a batch constrained TD updates considers:\]

<table>
  <tbody>
    <tr>
      <td>L(\theta) = \mathbb{E}<em>{(s,a,s’)\sim D} \left[ ( r(s,a) +\gamma \max</em>{\pi_b(a’</td>
      <td>s’)&gt;0}Q_{\hat{\theta}} - Q_\theta(s,a))^2 \right]</td>
    </tr>
  </tbody>
</table>

\[thefore, to mimic this behavior, we have two choices:
   - using expectile loss $L_2^{\tau}$ directly on $Q$, so that we have:\]

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  L(\theta) =  \mathbb{E}_{(s,a,s')\sim D} \left[ L_2^\tau( r(s,a) +\gamma \max_{\pi_b(a'|s')&gt;0}Q_{\hat{\theta}} - Q_\theta(s,a)) \right]
  
  $$
  (explaination of expectile loss seen in the next paragraph) but this have a drawback, and that is since each update sample contains $(s,a,s')$, it includes the stochasticity of a sample getting high reward because of *lucky* transition to $s'$ instead of doing the correct action $a$. This we do not want to have in our learned critic/policy.
- using expectile loss to first fit a value function $V$:
  $$
  
  L_V(\psi) = \mathbb{E}_{(s,a) \sim D} \left[ L_2^{\tau}(Q_{\hat{\theta}}(s,a) - V_\psi(s)) \right]
  
  $$
  which uses expectile because we want $V_\psi$ to learn the upper end in the distribution to approximate $\max Q_{\hat{\theta}}$, and then we simply use that and do:
  $$
  
  L_Q(\theta) = \mathbb{E}_{(s,a,s')\sim D} \left[ ( r(s,a) +\gamma V_\psi(s')- Q_\theta(s,a))^{2} \right]
  
  $$
  which notice that we do avoided the case of lucky transition while we are training the $V_\psi$ approximation function.
</code></pre></div></div>

<p>note that one concern could be: “before we were worried about $Q(s,a)$ giving unlikely large values, but now we are fitting $V$ to get large values?” The answer is no, because the <strong>previous problem is due to the fact that it comes from OOD actions</strong>, but here $V$ is trained only in data and hence have no overestimation problem.</p>

<p>Finally, we graphically provide interpretation of why this expectile and the $V_\psi$ function would work. Consider the case that you have multiple trajectories, and for a continous/large action space, you might see:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827004946.png" style="zoom:60%;" /></p>

<p>Suppose you are on the green circle state. Even though there is only one action, you can still consider the value of this state being a distribution because its neighbor states/trajectories has a different action. Therefore, this leads up to the following value function distribution from your data:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827005200.png" style="zoom:60%;" /></p>

<p>where the probability of each value is induced by actions (e.g. neighbor/similar states) only. Then, if we go back to the objective of fitting a value function:</p>

\[V = \arg\min_{V} \frac{1}{N} \sum\limits_{i=1}^{N} L(V(s_i), Q(s_i,a_i))\]

<p>taking:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MSE Loss</th>
      <th style="text-align: center">Expectile Loss, $x=V_\psi(s)-Q_{\hat{\theta}}(s,a)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$(Q_{\hat{\theta}}(s,a) - V_\psi(s))^{2}$</td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827010014.png" style="zoom:50%;" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827010118.png" style="zoom:60%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827010125.png" style="zoom:60%;" /></td>
    </tr>
  </tbody>
</table>

<p>This means that when fitting the $V$:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MSE Loss</th>
      <th style="text-align: center">Expectile Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827010746.png" style="zoom:60%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827010347.png" style="zoom:65%;" /></td>
    </tr>
  </tbody>
</table>

<p>Finally, when this $V$ learns from the good actions at each state, it can be effectively combined to our $Q$ network from which we can extract the policy, as indicated in the paper/algorithm overview above.</p>

<h2 id="conservative-q-learning-methods">Conservative Q-Learning Methods</h2>

<p>Instead of trying to fix actors by asking them not to sample OOD actions, CQL methods try to repair critic directly. Specically, recall that our previous problem is:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$\hat{Q}$ estimate of $\pi_\mathrm{new}$</th>
      <th style="text-align: center">Actual Return of $\pi_\mathrm{new}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826192651.png" style="zoom:60%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220826192630.png" style="zoom:60%;" /></td>
    </tr>
  </tbody>
</table>

<p>SO essentially, if the idea is to push down large Q values in general (as they are likely to be wrong), being rather pessimistic:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Fitted Q vs Real Q</th>
      <th style="text-align: center">Conservative Q-Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827115742.png" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827115829.png" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>a simple idea to achieve this is to consider the Q value objective:</p>

\[\hat{Q}^{\pi} = \arg\min_{Q} \,\,\max_\mu \underbrace{\alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]}_{\text{push down big Q-values}}
+ \underbrace{\mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]}_{\text{regular objective}}\]

<p>where notice that $\mu$ is picked so that $Q$ looks large, and then the final $Q$ is picked to minimize those. Hence in theory, it can be shown that if you pick a large enough $\alpha$, you can show that $\hat{Q}^\pi \le Q^{\pi}$ for the true $Q^\pi$.</p>

<p>But in reality, this generally pushes down all the Q values, which could be a little too pessimistic. Therefore, we could maybe do better by considering:</p>

\[\hat{Q}^{\pi} = \arg\min_{Q} \,\,\max_\mu \underbrace{\alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]}_{\text{always pushes down Q}}
- \underbrace{\alpha \mathbb{E}_{(s,a)\sim D}[Q(s,a)]}_{\text{push up (s,a) in data}}
+ \underbrace{\mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]}_{\text{regular objective}}\]

<p>where this is the CQL objective $L_{\mathrm{CQL}}$, such that:</p>

<ul>
  <li>for $(s,a)$ samples in the dataset, the push up and down term will cancel the effect out and have little net effect</li>
  <li>for $(s,a)$ samples not in the dataset, the first term will push them down but the second will push up actions in dataset. This means that for the next time, $\mu$ would tend to select actions <em>similar to the dataset</em> since they have large values.</li>
</ul>

<p>Therefore, the intuition is that :</p>
<ul>
  <li>if you meet OOD actions, the two terms will act to push them back to in-distribution actions.</li>
  <li>once they are in-distribution, the two terms more or less cancel out.</li>
</ul>

<p>As a result:</p>
<ul>
  <li>we no longer directly guarantee that $\hat{Q}^{\pi}(s,a) \le Q^\pi(s,a),\forall (s,a)$</li>
  <li>but we do guarantee that their expected value (i.e. value function) is bounded $\mathbb{E}<em>{\pi(a\vert s)}[\hat{Q}^{\pi}(s,a)] \le \mathbb{E}</em>{\pi(a\vert s)}[Q^{\pi}(s,a)], \forall s \in D$</li>
</ul>

<p>Then, as shown in the paper <a href="https://arxiv.org/abs/2006.04779">Kumar, Zhou, Tucker, Levine. 2020</a>, implement RL algorithm with CQL:</p>

<ol>
  <li>update $\hat{Q}^{\pi}$ w.r.t $L_{\mathrm{CQL}}$ using the dataset</li>
  <li>update your policy $\pi$:
    <ul>
      <li>
        <p>if actions are discrete then simply:
 $$</p>

        <p>\pi(a|s) = \begin{cases}
   1, \text{if } a=\arg\max_a \hat{Q}(s,a)<br />
   0 , \text{otherwise}
 \end{cases}</p>

        <p>$$</p>
      </li>
      <li>
        <p>if actions are continous, then you usually have to fit another policy network $\theta$:
 $$</p>

        <table>
          <tbody>
            <tr>
              <td>\theta \gets \theta + \alpha \nabla_\theta \sum_i \mathbb{E}<em>{a \sim \pi</em>\theta(a</td>
              <td>s)}[\hat{Q}(s_i,a)]</td>
            </tr>
          </tbody>
        </table>

        <p>$$
 which is basically like an actor-critic method.</p>
      </li>
    </ul>
  </li>
</ol>

<p>Finally, to pick a $\mu$, we can first consider adding a max-entropy regularization term in our objective:</p>

\[\begin{align*}
  \hat{Q}^{\pi} 
  &amp;= \arg\min_{Q} \,\,\max_\mu \alpha \mathbb{E}_{s\sim D, a \sim \mu(a|s)}[Q(s,a)]
- \alpha \mathbb{E}_{(s,a)\sim D}[Q(s,a)] - \overbrace{\mathcal{R}(\mu)}^{\text{regularizer}} \\
  &amp;\quad + \mathbb{E}_{(s,a,s')\sim D} \left[ (Q(s,a) - (r(s,a)+ \mathbb{E}_\pi[Q(s',a')]))^{2} \right]
\end{align*}\]

<p>then with the choice of taking $\mathcal{R}=\mathbb{E}_{s\sim D}[H(\mu(\cdot \vert s))]$, we will have an optimal choce with $\mu(a\vert s) \propto \exp(Q(s,a))$ and hence:
$$</p>

<table>
  <tbody>
    <tr>
      <td>\mathbb{E}_{a \sim \mu(a</td>
      <td>s)} [Q(s,a)] = \log \sum\limits_{a} \exp(Q(s,a))</td>
    </tr>
  </tbody>
</table>

<p>$$
so that:</p>
<ul>
  <li>for discrete actions, we can just caulcate directly $\log \sum\limits_{a} \exp(Q(s,a))$</li>
  <li>for contionus actions, we can estimate $\mathbb{E}_{a \sim \mu(a\vert s)} [Q(s,a)]$ by using importance sampling</li>
</ul>

<h2 id="model-based-offline-rl">Model-Based Offline RL</h2>

<p>All the previous mentioned methods are model-free, i.e. we do not have a world model getting us the transitions/rewards. In the context of offline RL, model-based methods can be pretty useful as you can:</p>
<ol>
  <li>train a wolrd model using your data $D$</li>
  <li>use that model to do planning directly or learn a policy from it</li>
</ol>

<p>Usually, model-based RL methods work as follows:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827161230.png" style="zoom:50%;" /></p>

<p>But since we cannot collect more data in offline RL, it means that:</p>

<p>| <img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827161524.png" style="zoom:50%;" /> | <img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827161544.png" style="zoom:50%;" /> |
| :—: | :—: |</p>

<p>so that essentially we are facing the same problem as those OOD actions in Q value estimates: now we are facing the threat that ==our policy could adversarially pick actions and transit to OOD states which the model could have some erroneously high values==.</p>

<p>Therefore, the general intuition for this is to apply some kind of penalty for policys that exploit too many OOD states/actions.</p>

<h3 id="model-based-offline-policy-optimization">Model-based Offline Policy Optimization</h3>

<p>There are two papers on this topic of thought, which is:</p>
<ul>
  <li>MOPO: Model-based Offline Policy Optimization (2020)</li>
  <li>MOReL: Model-based Offline Reinforcement Learning (2020)</li>
</ul>

<p>where both of which <strong>modifies the reward function</strong> to trick the policy to stick more of less to data we have.</p>

<blockquote>
  <p><strong>Uncertainty Penalty</strong>: we would like to modify the reward function to penalize the policy for going into states that might be incorrect (due to inaccuracies in the learned model). Therefore, we in general consider:
$$</p>

  <p>\tilde{r}(s,a) = r(s,a) + \lambda u(s,a)</p>

  <p>$$
for $u$ is an <em>uncertain penalty</em> term.</p>
</blockquote>

<p>While this sounds easy, it is usually hard to adjust the penalty term to make it work. Intuitvely, we would want:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Before Penalty</th>
      <th style="text-align: center">After Penalty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827162535.png" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827162718.png" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>This means that this penalty term needs to be <strong>at least as large as the error</strong> the model makes at those erroneous states (hence to provide incentive for the policy to not exploit them). So how do we quantify this?</p>

<ul>
  <li>one simple idea is to use <strong>ensemble disagreement</strong>: we train an ensemble of world models, and quantify the error by looking at the disagreement between the models.</li>
  <li>but of course there are other ways as well</li>
</ul>

<p>Consider we are learning a policy $\hat{\pi}$, then in the MOPO paper it is proven that:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827163651.png" style="zoom:80%;" /></p>

<p>where $M$ is the world model we trained, and $\nu$ measures the true return. This basically gave two important implications. As we can substitute the policy $\pi$ term:</p>

<ol>
  <li>
    <p>Our learned policy can improve upon the behaviorial policy (in terms of the true return):
$$</p>

    <p>\nu_M(\hat{\pi}) \ge \nu_M(\pi_b) - 2 \lambda \epsilon_u(\pi_b)</p>

    <p>$$</p>
  </li>
  <li>
    <p>We can also quantify the gap between the learned policy and the true optimal policy in terms of model error:
$$</p>

    <p>\nu_M(\hat{\pi}) \ge \nu_M(\pi^{\star}) - 2 \lambda \epsilon_u(\pi^{\star})</p>

    <p>$$
and notice that if our model $M$ is very accurate, then $\epsilon_u$ is small and we are close to the optimal policy.</p>
  </li>
</ol>

<h3 id="conservative-model-based-rl">Conservative Model-Based RL</h3>

<blockquote>
  <p>see the <a href="https://arxiv.org/pdf/2102.08363.pdf">COMBO paper</a> for more details</p>
</blockquote>

<p>Intuition: just like with CQL we can minimize the Q-value of OOD policy actions, we can minimize the <strong>Q-value of model</strong> for OOD state-action tuples.</p>

\[\begin{align*}
  \hat{Q}^{k+1}
  &amp;\gets \arg\min_{Q} \beta ( \overbrace{\mathbb{E}_{(s,a) \sim \rho(s,a)}[Q(s,a)]}^{\text{push down}} - \overbrace{\mathbb{E}_{(s,a) \sim D}[Q(s,a)]}^{\text{push up}} )\\
  &amp;+ \frac{1}{2} \mathbb{E}_{(s,a,s') \sim d_f}[ (Q(s,a) - \mathcal{B}^\pi \hat{Q}^{k}(s,a))^{2} ]
\end{align*}\]

<p>where $\rho(s,a)$ is from our model $M$, and $\mathcal{B}^\pi$ is a Bellman operator. This tries to make the Q-values from the models be worse, but Q-values from the dataset be better. Therefore,</p>

<ul>
  <li>if the model produces something that looks different from the dataset, then this objective can have $Q$ makes it look bad</li>
  <li>if the model produces something that is very close/indistinguishable from the dataset, then the first two terms cancel out</li>
</ul>

<h3 id="trajectory-transformer">Trajectory Transformer</h3>

<blockquote>
  <p>See <a href="https://arxiv.org/pdf/2106.02039.pdf">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a></p>
</blockquote>

<p>The basic idea is:</p>
<ol>
  <li>
    <p>consider trajectory as a whole, so we have a joint state-action model:
$$</p>

    <p>p_b(\tau) = p_b(s_1, a_1, …, s_{T}, a_T)</p>

    <p>$$
which is the probabiliy under the behavioral policy.</p>
  </li>
  <li>
    <p>then, we can use a transformer to model such a probability distribution.</p>
  </li>
</ol>

<p>Specifically, to deal with continuous states and actions, we discretize each dimension independently. Assuming N-dimensional states and M-dimensional actions, this turns $\tau$ into sequence of length $T(N +M +1)$:</p>

\[\tau = ( ... , s_t^1, s_t^{2}, ..., s_t^{N},a_t^{1},a_t^{2},...,a_t^{M},r_t, ...), \quad t=1,...,T\]

<p>so that subscripts on all tokens denote timestep and superscripts on states and actions denote dimension (i.e. $s_t^{i}$ is the $i$th dimension of the state at time $t$). While this choice may seem inefficient, it allows us to model the distribution over trajectories with more expressivity without simplifying assumptions such as Gaussian transition.</p>

<p>Then, we can model this as:</p>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827173339.png" style="zoom:60%;" /></p>

<p>where we can then <strong>decode next state/actions given previous ones</strong>, and note that both the model and search strategy are nearly identical to those common in natural language processing.</p>

<p>Finally, to do planning, you need to keep in mind to take the action probabilities into account, so that you avoid OOD actions. But otherwise, you can:</p>

<ul>
  <li>just use Beam search, but to promote non-myoptic chioces:
    <ol>
      <li>given the current sequence (e.g. upto time stamp 3), sample the next $K$ tokens from the model (which means they are sampled from $p_b$)</li>
      <li>store the top $K$ tokens with highest cumulative reward $\sum_{t} r(s_t,a_t)$ instead of probability</li>
      <li>continue</li>
    </ol>
  </li>
  <li>or you could even use MCTS</li>
</ul>

<p>Why does/should this work?</p>

<ul>
  <li>generating high-probability trajectories avoids OOD states and actions already, since we are sampling from $p_b$ when decoding.</li>
  <li>using a big model (e.g. transformers) tend to work well in offline mode</li>
</ul>

<h2 id="summaries-and-directions-on-offline-rl">Summaries and Directions on Offline RL</h2>

<p>If you want to to <strong>only train offline</strong>:</p>
<ul>
  <li>CQL: just one hyperparmeter and is well understood and widely tested</li>
  <li>Implicit Q-Learning: more flexible (offline+online), but has more hyperparameters</li>
</ul>

<p>If you want to <strong>train offline and finetune online</strong>:</p>

<ul>
  <li>not CQL because it tends to be too conservative</li>
  <li>Adavantage-weighted Actor-Critic (AWAC) is widely used and well tested</li>
  <li>Implicit Q-learning works well</li>
</ul>

<p>If you are confident that you <strong>can train a good world model</strong>:</p>

<ul>
  <li>COMBO: similar properties from CQL but also benefits from being model-based</li>
  <li>Trajectory Transformer: very powerful, but is extremly computationally expensive to train and evaluate</li>
</ul>

<p><img src="/lectures/images/2022-09-30-STFCS234_Reinforcement_Learning/image-20220827184217.png" style="zoom:60%;" /></p>

<p>Finally, worthy mentions and challenages include that:</p>
<ul>
  <li>as compared to supervised training where you can train/test offline, offline RL upto today still have to <strong>test online</strong> (costly and maybe even dangerous)</li>
  <li>statistically guarantees to help quantify <strong>distributional shift</strong> (currently pretty loose and incomplete)</li>
</ul>

  </div><a class="u-url" href="/lectures/2022@columbia/STFCS234_Reinforcement_Learning.html/" hidden></a>
  <script src="/lectures/assets/js/my_navigation.js"></script>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
