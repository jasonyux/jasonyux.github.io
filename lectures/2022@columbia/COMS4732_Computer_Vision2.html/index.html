<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>COMS4732 Computer Vision II | Lecture Notes</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="COMS4732 Computer Vision II" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="/lectures/2022@columbia/COMS4732_Computer_Vision2.html/" />
<meta property="og:url" content="/lectures/2022@columbia/COMS4732_Computer_Vision2.html/" />
<meta property="og:site_name" content="Lecture Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-04-28T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="COMS4732 Computer Vision II" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-04-28T00:00:00-04:00","datePublished":"2022-04-28T00:00:00-04:00","description":"Introduction","headline":"COMS4732 Computer Vision II","mainEntityOfPage":{"@type":"WebPage","@id":"/lectures/2022@columbia/COMS4732_Computer_Vision2.html/"},"url":"/lectures/2022@columbia/COMS4732_Computer_Vision2.html/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lectures/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/lectures/feed.xml" title="Lecture Notes" /></head>
<body><header class="site-header">

	<div class="wrapper"><a class="site-title" rel="author" href="/lectures/">Lecture Notes</a>

		<nav class="site-nav">
			<input type="checkbox" id="nav-trigger" class="nav-trigger" />
			<label for="nav-trigger">
			<span class="menu-icon">
				<svg viewBox="0 0 18 15" width="18px" height="15px">
				<path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
				</svg>
			</span>
			</label>

			<div class="trigger">
				<a class="page-link" href="/">Home</a>
				<a class="page-link" href="/projects">Projects</a>
				<a class="page-link" href="/learning">Blog</a>
				<a class="page-link" href="/research">Research</a>
				<span class="page-link" href="#">[Education]</span>
			</div>
		</nav>
	</div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <head>
  <script>
    MathJax = {
      // 
      loader: {
        load: ['[tex]/ams', '[tex]/textmacros', '[tex]/boldsymbol']
      },
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: {'[+]': ['ams', 'textmacros', 'boldsymbol']}
      }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>
  </head>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COMS4732 Computer Vision II</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-04-28T00:00:00-04:00" itemprop="datePublished">
        Apr 28, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>

<p>What is vision?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121101730762.png" alt="image-20220121101730762" style="zoom: 25%;" /></p>

<h2 id="applications">Applications</h2>

<p>One very important application is <strong>Biometrics</strong></p>

<ul>
  <li>how FaceID works!</li>
</ul>

<p>Another would be <strong>Optical Character Recognition</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121102247405.png" alt="image-20220121102247405" style="zoom: 33%;" /></p>

<p><strong>Gaming</strong> with VR: recognize your body poses:</p>

<ul>
  <li>recognize fine details about your movements</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121102437420.png" alt="image-20220121102437420" style="zoom: 33%;" /></p>

<p>Recently there has been application in <strong>shopping</strong></p>

<ul>
  <li>
    <p>as a customer, you can grab whatever you want, and you will be charged by Amazon</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121102746163.png" alt="image-20220121102746163" style="zoom: 33%;" /></p>
  </li>
</ul>

<p>Last but not least, <strong>self-driving cars</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121103524340.png" alt="image-20220121103524340" style="zoom: 33%;" /></p>

<h2 id="perceiving-images">Perceiving Images</h2>

<p>Basically the input of an image would be</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">What We See</th>
      <th style="text-align: center">What Computer Sees</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121104334539.png" alt="image-20220121104334539" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121104304895.png" alt="image-20220121104304895" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>which hints at the why computer vision is difficult.</p>

<ul>
  <li>
    <p>other factors that could make it more complicated is the <strong>lighting</strong>, which can change the picture</p>
  </li>
  <li>
    <p>object <strong>occlusion</strong>, an object will be partially blocked</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121104734416.png" alt="image-20220121104734416" style="zoom:50%;" /></p>
  </li>
  <li>
    <p><strong>class variation</strong>: objects can have various shapes. What <em>is</em> a chair?</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121105139330.png" alt="image-20220121105139330" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p><strong>clutter and camouflage</strong>: we are able to see through camouflage</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121105236653.png" alt="image-20220121105236653" style="zoom:50%;" /></p>

    <p>so that we can see there is an owl, but computer vision systems would struggle here.</p>
  </li>
</ul>

<p>In general, there is often <strong>no correct answer</strong> for computer vision!</p>

<h2 id="evolution-of-vision">Evolution of Vision</h2>

<p>Before the <strong>Cambrian explosions,</strong> there were only about 4 species (worm-like) on Earth. However, after the explosion:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121110259233.png" alt="image-20220121110259233" style="zoom: 67%;" /></p>

<p>some theories to</p>

<ul>
  <li>“In the blink of an eye”: The Cambrian Explosion is trigged by the sudden evolution of vision, which set off an <strong>evolutionary arms race</strong> where animals either evolved or <em>died</em>.</li>
  <li>our vision has evolved for more than 200 million years. Now let the computer do it.</li>
</ul>

<p>What don’t we just <strong>build a brain</strong>?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121112001855.png" alt="image-20220121112001855" style="zoom:50%;" /></p>

<p>where we start the loop from our retina:</p>

<ul>
  <li>starting from PFC it is related to other stuff.</li>
  <li>but even until today, we are still not sure how brain works.</li>
</ul>

<p>Additionally, there is a difference in datasets</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121123817657.png" alt="image-20220121123817657" style="zoom: 67%;" /></p>

<p>notice that what a 2 year-old child have seen would have been much more than the best dset we have now.</p>

<h2 id="syllabus">Syllabus</h2>

<p>Because the course is large, there will be <strong>no exceptions at all</strong></p>

<p><strong>Topics</strong>: we do NOT assume prior knowledge in computer vision and machine learning</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121114035339.png" alt="image-20220121114035339" style="zoom:50%;" /></p>

<p><strong>Format</strong>: Hybrid</p>

<ul>
  <li>so Zoom is allowed</li>
  <li>every lecture will be recorded</li>
</ul>

<p><strong>Grading</strong></p>

<ul>
  <li><em>Homework 0</em>: 5% (self-assessment, should be easy)</li>
  <li><em>Homework 1 through 5</em>: 10% each</li>
  <li><em>Final Quiz</em>: 45% (written)</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121115341756.png" alt="image-20220121115341756" style="zoom:50%;" /></p>

<p><strong>Homework</strong>: outlines</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121115530614.png" alt="image-20220121115530614" style="zoom:50%;" /></p>

<ul>
  <li>usually it will be 2 weeks for each homework</li>
  <li>probably hand in via Gradescope</li>
  <li>collaborations will be allowed, but need to disclose</li>
</ul>

<p><strong>Useful Resources</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121120415670.png" alt="image-20220121120415670" style="zoom:50%;" /></p>

<p><strong>OH</strong></p>

<ul>
  <li>will be online</li>
</ul>

<h2 id="optical-illusions">Optical Illusions</h2>

<p>Below are some interesting illusions</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Illusion</th>
      <th style="text-align: center">Your Brain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121121612265.png" alt="image-20220121121612265" style="zoom: 50%;" /></td>
      <td style="text-align: center">You brain “factors” out the fact that there is a shadow, which automatically made a block $B$ seem lighter than $A$. (How can your computer vision do this if they have the same RGB?)</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121121809867.png" alt="image-20220121121809867" style="zoom:50%;" /></td>
      <td style="text-align: center">Some explanation of this talks about that you see them “moving” because your neurons overloaded.</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121122029031.png" alt="image-20220121122029031" style="zoom:50%;" /></td>
      <td style="text-align: center">Ambiguities our brain resolve pretty fast: a big chair instead of a small person</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121122307953.png" alt="image-20220121122307953" style="zoom:50%;" /></td>
      <td style="text-align: center">Makes you think people shrunk in size. But actually this is how it happened<img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121122240165.png" alt="image-20220121122240165" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>in short:</p>

<ul>
  <li>our brain “automatically fill in things” that are not there - hard part of perception</li>
</ul>

<h1 id="convolution">Convolution</h1>

<blockquote>
  <p>The idea is that we want to de preprocessing of the image, such that:</p>

  <ul>
    <li>we can <strong>“denoise”</strong> an image.</li>
    <li><strong>highlight edges</strong> (taking gradient)</li>
    <li>etc</li>
  </ul>

  <p>using a linear kernel/filter, which essentially are using ==weighted sums of pixel values== using different patterns of weights to find different image patterns. Despite its simplicity, this process is extremely useful.</p>
</blockquote>

<p>For instance, when you take a photo at night, there is little light hence it would capture a lot of <strong>noise</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128102919459.png" alt="image-20220128102919459" /></p>

<blockquote>
  <p><em>Intuition</em></p>

  <p>One way to suppress noise would be to:</p>

  <ul>
    <li>take many photos and take average</li>
    <li>how do we “take an average” even if we only have <strong>one photo</strong>?</li>
  </ul>
</blockquote>

<p>One way to think about this, is that we can <strong>first treat each image as a “function”</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128103201774.png" alt="image-20220128103201774" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>as a ==function==, the image <strong>maps a coordinate $(x,y)$</strong> to <strong>intensity</strong> $[0,255]$</li>
  <li>(in some other cases, thinking of this as a <strong>matrix</strong> would work)</li>
</ul>

<p>Then, then, you can take a <strong>moving average</strong>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sliding Through</th>
      <th style="text-align: center">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128103942716.png" alt="image-20220128103942716" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128103957449.png" alt="image-20220128103957449" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>when we finish, notice that:</p>

<ul>
  <li>the next effect is that it “blurs” or “smooths” the image out</li>
  <li>the output has a <strong>smaller size</strong> than the input. This is because there are $(n-3+1)^2$ unique positions for putting the $3\times 3$ kernel.</li>
</ul>

<h2 id="linear-filter">Linear Filter</h2>

<p>The above can also be thought of as:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128104522974.png" alt="image-20220128104522974" style="zoom:50%;" /></p>

<p>In general, we will be looking at ==linear filters==, which has to satisfy the following</p>

<ol>
  <li>
    <p>$\text{filter}( \text{im},f_1 +f_2) = \text{filter}( \text{im},f_1) + \text{filter}( \text{im},f_2)$</p>

    <ul>
      <li>
        <p>$f_1,f_2$ are filters/kernels. The function is the process of applying them to the image.</p>
      </li>
      <li>output of sum of filters is the same as sum of output of filters $f((a+b)x)=f(ax)+f(bx)$</li>
      <li>since filters can also be seen as “images”: output of the sum of two images is the same as the sum of the outputs obtained for the images separately.</li>
    </ul>
  </li>
  <li>
    <p>$C\cdot \text{filter}( \text{im},f_1) = \text{filter}( \text{im},C\dot f_1)$</p>

    <ul>
      <li>multiplied by a constant</li>
    </ul>
  </li>
</ol>

<p>And you can think of this as <strong>linear algebra</strong></p>

<ul>
  <li>most of the convolutions operations are linear by construction</li>
</ul>

<h2 id="convolution-filter">Convolution Filter</h2>

<blockquote>
  <p><strong>Kernel/Filter</strong>: The pattern of weights used for a ==linear filter== is usually referred to as the kernel/the filter</p>
</blockquote>

<blockquote>
  <p>The process of <strong>applying</strong> the filter is usually referred to as ==convolution==.</p>
</blockquote>

<p>For instance, we can do a running average by the following convolution:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128104732695.png" alt="image-20220128104732695" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>$*$ is often a symbol used for <strong>convolving</strong></li>
  <li>essentially it is about <strong>taking $G$ , then taking sum of element-wise product</strong> with a $3\times 3$ region in $F$</li>
  <li>This is the <strong>same as moving average we had</strong>. But notice that we needed $1/9$ in front:
    <ul>
      <li>In reality, we also want to make sure that the <strong>output is still a valid image</strong>. Hence we need to be careful that the output intensity value <em>does not exceed $255$</em>, for instance.</li>
    </ul>
  </li>
</ul>

<p>Formally, convolution is defined as:</p>

\[(f * g)[x,y] = \sum_{i,j} f[x-i,y-j]g[i,j]\]

<p>where</p>

<ul>
  <li>
    <p>$(f * g)[x,y]$ means $f$ convolves with $g$, which is a function of coordinate $x,y$. Outputs the intensity at $x,y$.</p>
  </li>
  <li>
    <p>For a $3\times 3$ kernel, we would set $i \in [0,2], j \in [0,2]$ and output to the <strong>top right</strong> instead of center.</p>
  </li>
  <li>
    <p>notice that the ==minus sign is intended==, so that we are <strong>flipping the filter</strong>:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128105549387.png" alt="image-20220128105549387" style="zoom:50%;" /></p>

    <p>where:</p>

    <ul>
      <li>the only purpose of flipping is that it makes the math easier later on</li>
      <li>increasing index in $g$ but doing decreasing for $f$.
        <ul>
          <li>therefore, you need to ==flip the filter upside down==, and ==then right to left==</li>
        </ul>
      </li>
      <li>when you code it, however, often you will just have <code class="language-plaintext highlighter-rouge">+</code> sign.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Note that if the ==filter is symmetric==, then flipping doesn’t matter.</p>

  <ul>
    <li>However, if the filter is not symmetric, (most people) just don’t flip it either way. So it depends.</li>
  </ul>
</blockquote>

<blockquote>
  <p>If you use the $+$ instead, it is called a <strong>cross-correlation</strong> operation</p>

\[(f * g)[x,y] = \sum_{i,j} f[x+i,y+j]g[i,j]\]

  <p>which is also denoted as:</p>

\[f \otimes g\]

  <p>which <strong>does not have all the nice properties</strong> like convolution just due to that sign.</p>
</blockquote>

<hr />

<p><em>For instance</em>: convolution examples</p>

<p>Identity transformation:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128111418548.png" alt="image-20220128111418548" style="zoom:50%;" /></p>

<ul>
  <li>basically It will output the same image (but contracted by 1)</li>
</ul>

<p><strong>Translation</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128111601735.png" alt="image-20220128111601735" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>where it shifts to the right <strong>because we had the minus sign</strong>. In essence, we need to flip the convolutional kernel upside down and right to left, which becomes this:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128112643727.png" alt="image-20220128112643727" style="zoom:50%;" /></p>

    <p>hence it is in fact shifted to the right</p>
  </li>
</ul>

<p><strong>Nonlinear Kernel</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128111838656.png" alt="image-20220128111838656" style="zoom:50%;" /></p>

<p>where notice that <strong>no such convolution kernel exist</strong>, because:</p>

<ul>
  <li>this is not a linear operation!</li>
  <li>for convolution kernel to work, we needed to **treat everything/pixel identically (from its neighbors) **. However, a rotation doesn’t work like this (e.g. consider the treatment of the pixel in the center and the pixel far away from the center on the LHS image)</li>
</ul>

<p><strong>Sharpening</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128112835352.png" alt="image-20220128112835352" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>sharpening actually <strong>increases the noise</strong></p>

    <ul>
      <li>multiply by $2$ is like brightening</li>
      <li>subtracting a blurred image = subtracting removed noise</li>
    </ul>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128113123051.png" alt="image-20220128113123051" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>so it turns out that our <strong>eyes</strong> think “adding noise” makes the photo looks sharper</p>
  </li>
</ul>

<h3 id="convolution-properties">Convolution Properties</h3>

<p>The <strong>operation $*$</strong> has the following property:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128113707448.png" alt="image-20220128113707448" style="zoom: 25%;" /></p>

<p>those can be proved with the <strong>minus sign</strong> in our definition, which switching to plus sign might make things break. $F,G,H$ are all filters/kernels, so remember that $F * G$ means, .e.g having image $F$ convolve with filter $G$</p>

<ul>
  <li><strong>commutative/associative</strong>: order of convolution does not matter. You can apply $F$ then $G$, or $G$ then $F$</li>
  <li><strong>distributive</strong>: same as <strong>linearity of kernels</strong></li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>you kind of have to ignore the fact that different sizes of image/filter produces a different border</li>
    <li>those are useful because it makes your code runs faster</li>
  </ul>
</blockquote>

<p>Additionally, we also know that</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128114403261.png" alt="image-20220128114403261" style="zoom:50%;" /></p>

<p>which makes sense since a linear convolution treats each pixel the same/”same weights from neighbors”.</p>

<h3 id="gaussian-filter">Gaussian Filter</h3>

<p>Now, let us reconsider the task of blurring an image: we can <strong>blur</strong> the image by “<em>creating multiple copies of the image</em>”, dis-align them and add them up:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Box Filter</th>
      <th style="text-align: center">Gaussian Filter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128114716875.png" alt="image-20220128114716875" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128114847863.png" alt="image-20220128114847863" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where in both cases, we have blurred/smoothened the image</p>

<ul>
  <li>black means 0, white means 1, and this white box is <strong>larger</strong> than $1 \times 1$ in size.</li>
  <li>smoothing: suppresses noise by enforcing the requirement that ==pixels should look like their neighbors==</li>
  <li>the Gaussian one does indeed is more visually appealing</li>
</ul>

<p>More mathematically, the Gaussian is a <strong>multivariate Gaussian</strong> but having identity as covariance: i.e. the two variables are <strong>independent</strong>:</p>

\[G_\sigma = \frac{1}{2\pi \sigma^2} \exp({ - \frac{x^2 + y^2}{2\sigma^2}})\]

<p>where $x,y$ are coordinates, and an example output looks like:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128185007494.png" alt="image-20220128185007494" style="zoom:50%;" /></p>

<p>recall that Gaussian also has the nice property that they sum up to 1.</p>

<ul>
  <li>notice that it is <strong>symmetric</strong>. This is enforced.</li>
  <li>yet since it is a Gaussian, we can also <strong>control its parameters</strong> $\sigma$, which ==determines the extent of smoothing==</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128185111480.png" alt="image-20220128185111480" style="zoom:50%;" /></p>

<p>so that:</p>

<ul>
  <li>more spread out gives more blur</li>
</ul>

<p>For instance:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Slow Sigma</th>
      <th> </th>
      <th style="text-align: center">High Sigma</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128115547289.png" alt="image-20220128115547289" /></td>
      <td><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128115552940.png" alt="image-20220128115552940" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128115558560.png" alt="image-20220128115558560" /></td>
    </tr>
  </tbody>
</table>

<h3 id="computation-complexity">Computation Complexity</h3>

<p>For having an image of $n\times n$ doing a convolution of $m \times m$ kernel/filter:</p>

\[O(n^2 m^2)\]

<p>where we assumed that there are <strong>paddings</strong> done, so the output is the same size as input.</p>

<ul>
  <li>For each single pixel, we need to do $m \times m$ work</li>
  <li>Since we have $n \times n$ pixels, we needed to $n^2 m^2$</li>
  <li>this is <strong>very expensive</strong>!</li>
</ul>

<p>But we can speed this up in some cases. Consider <strong>separating the Gaussian filter into 2</strong>:</p>

\[G_\sigma = \frac{1}{2\pi \sigma^2} \exp({ - \frac{x^2 + y^2}{2\sigma^2}}) = \left[  \frac{1}{\sqrt{2\pi }\sigma} \exp({ - \frac{x^2 }{2\sigma^2}}) \right]\left[  \frac{1}{\sqrt{2\pi} \sigma} \exp({ - \frac{y^2}{2\sigma^2}}) \right]\]

<p>Therefore, since we know that if we have <strong>two filters $g,h$</strong>, and an image $f$, associativity says:</p>

\[f * (g * h) = (f*g)*h\]

<p>Therefore</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128120123085.png" alt="image-20220128120123085" style="zoom: 67%;" /></p>

<p>(technically, we are saying the following)</p>

\[f * g = f * (g_v \times g_h)= (f* g_v) * g_h\]

<p>Then, since $G_\sigma$ <strong>can be separated into two filters of smaller dimension</strong>:</p>

\[O(n^2 m)\]

<p>now for each pixel, we only needed to do $m$ work/look at $m$ neighbors.</p>

<ul>
  <li>technically you do it twice, so $2n^2m$, but $2$ is ignored.</li>
  <li>this only works in special cases.</li>
</ul>

<h2 id="human-visual-system">Human Visual System</h2>

<p>In fact, one stage our vision system <strong>also does convolution</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128120502683.png" alt="image-20220128120502683" style="zoom:50%;" /></p>

<ul>
  <li>$V1$ is doing convolution.</li>
</ul>

<p>Experiments have been done on cats, and show that the <strong>kernel they are using looks like the following</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128120700506.png" alt="image-20220128120700506" style="zoom:67%;" /></p>

<p>where:</p>

<ul>
  <li>to simulate the kernels in cat, we have those <strong>Gabor’s filter</strong></li>
</ul>

<h3 id="gabor-filters">Gabor Filters</h3>

<p>Gabor filters are defined by:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128120758009.png" alt="image-20220128120758009" style="zoom:50%;" /></p>

<p>interestingly:</p>

<ul>
  <li>it seems that convolutional NN also <strong>returned a similar filter</strong></li>
  <li>it turns out this can do <strong>edge detection</strong></li>
</ul>

<h2 id="image-gradients">Image Gradients</h2>

<p>Now, we want to consider the problem of <strong>identifying edges</strong> in a picture, which is part of an important process in <strong>identifying objects</strong>.</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128121101657.png" alt="image-20220128121101657" style="zoom:50%;" /></p>

<p>Consider looking at the red arrow. We are interested in <strong>how does the intensity change</strong></p>

<ul>
  <li>
    <p>when we moved across the pillar, it seems that intensity changed dramatically!</p>
  </li>
  <li>
    <p>so we want to compute the “derivatives”</p>
  </li>
</ul>

<p>We know that</p>

\[\frac{\partial f}{\partial x} = \lim_{\epsilon \to 0} \frac{f(x + \epsilon ,y)-f(x - \epsilon ,y)}{\epsilon}\]

<p>but since the <strong>smallest unit is a pixel</strong>:</p>

\[\frac{\partial f}{\partial x} \approx f(x+1,j) -  f(x-1,j)\]

<p>Therefore, we basically have the following:</p>

<ul>
  <li>$\partial f/ \partial x$: using $[-1,0,1]$ or $[-1,1]$ as kernel</li>
  <li>$\partial f/ \partial y$: using $[-1,0,1]^T$ or $[-1,1]^T$ as kernel</li>
</ul>

<p>Result:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128121536252.png" alt="image-20220128121536252" style="zoom:50%;" /></p>

<p>where we see:</p>

<ul>
  <li>
    <p>the $\partial x$ shows how images change when we <strong>move in $x$-direction</strong>. Hence we see the texture of the pillars on the RHS. But if we do $\partial y$, they disappear.</p>
  </li>
  <li>if we want to be more “exact”: $0.5[-1,0,1]$ since the step size is $2$ pixels</li>
  <li>technically the signs are “backwards” because we need to flip our kernel</li>
</ul>

<p>Similarly, we can also compute <strong>second derivative</strong> from using the <strong>first derivative as input</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128121910286.png" alt="image-20220128121910286" style="zoom:50%;" /></p>

<h3 id="edge-detection-idea">Edge Detection: Idea</h3>

<p>There is no strict “<strong>definition of what is edge</strong>”, so it is more like a practical trial and error:</p>

<ul>
  <li>detect edge such that first derivative has a <strong>largest change</strong> in some region, i.e. second derivative is $0$!</li>
</ul>

<p>We may care about second derivatives because, usually our image will be <strong>noisy</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128122058967.png" alt="image-20220128122058967" style="zoom:50%;" /></p>

<p>notice that <strong>derivatives is high everywhere</strong></p>

<ul>
  <li>hence we may need to smoothing it first</li>
  <li>then the edges <strong>has the larges derivative</strong> among them</li>
</ul>

<p>Therefore, we can do:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128195234301.png" alt="image-20220128195234301" style="zoom:50%;" /></p>

<p>again, we can combine them because:</p>

<ul>
  <li>convoving with filter 1, then convolve with filter 2 = covolve with (filter 1 convovle filter 2)</li>
  <li>notice that they are all <strong>linear filters</strong>!</li>
  <li>the <strong>Laplacian filter</strong> looks similar to the <strong>Gabor filter</strong>! Detecting the edge!</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>If you pad an image with $0$ outside (instead of reflection), then essentially you will be <strong>adding an extra edge</strong> to the image.</p>

  <ul>
    <li>though in a CNN, those could be learnt</li>
  </ul>
</blockquote>

<h4 id="laplacian-filter">Laplacian Filter</h4>

<p>The more exact definition of Laplacian filter is:</p>

\[\nabla^2 I = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\]

<p>For instance</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128195431819.png" alt="image-20220128195431819" style="zoom:67%;" /></p>

<p>where basically:</p>

<ul>
  <li>edges will get high intensity</li>
</ul>

<p>Another example, but now we <strong>threshold</strong> the second derivative:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128123029329.png" alt="image-20220128123029329" style="zoom:50%;" /></p>

<ul>
  <li>smaller than $\lambda$ so that <strong>changes in gradients are large</strong></li>
</ul>

<h3 id="object-detection-idea">Object Detection: Idea</h3>

<p>What if we “convolve Einstein with his own eye”: (with the aim of finding the eye)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128123416802.png" alt="image-20220128123416802" style="zoom:50%;" /></p>

<p>where we see that the results <strong>are not that good</strong>.</p>

<ul>
  <li>
    <p>in the end, this is where machine learning kicks in, let it figure out what</p>
  </li>
  <li>
    <p>note that the above does not work because, if you think of $f_{ij} * g$ as computing a <strong>cosine similarity between vectors</strong> as we are doing inner products anyway:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128123605159.png" alt="image-20220128123605159" style="zoom:50%;" /></p>

    <p>then obviously it does not work.</p>
  </li>
</ul>

<p>However, it turns out that we can <strong>do the following</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128123801263.png" alt="image-20220128123801263" style="zoom:50%;" /></p>

<ul>
  <li>
    <p>so the problem is more like <strong>how do we find the right filter</strong>.</p>
  </li>
  <li>
    <p>Finally, this task will be one reason why we will be using CNN to <strong>learn the filters</strong></p>
  </li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220128123854863.png" alt="image-20220128123854863" style="zoom:50%;" /></p>

<h1 id="fourier-transform">Fourier Transform</h1>

<p>The basic idea of Fourier Transform is that <strong>any univariate function</strong> an be rewritten as a weighted sum of sines and cosines of different frequencies. (recall in PDE)</p>

\[f(x) = a_0 + \sum_{n} a_n \sin (n \pi \frac{x}{L}) + \sum_{n} b_n \cos(n\pi \frac{x}{L})\]

<p>An example would be that we can:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original</th>
      <th style="text-align: center">Fourier Series</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204104240993.png" alt="image-20220204104240993" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204104255734.png" alt="image-20220204104255734" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>If this is true, we can also <strong>fourier transform</strong> the 2D images as sums:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original</th>
      <th style="text-align: center">Fourier Series</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204104506678.png" alt="image-20220204104506678" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204104459681.png" alt="image-20220204104459681" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li>we can use this for, e.g. <strong>compression</strong>, by removing some higher order terms to reduce data but still making the image look reasonbly good.</li>
  <li>now, since the source function is in 2D, fourier transform basically converts it to a <strong>sum of 2D waves</strong>
    <ul>
      <li>notice that the <em>frequency</em> of the “image” increases. This is basically what happens in higher order frequency terms in FT!</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>The key idea in this chapter is that images, which can be treated as function $g(x,y)$, can be thought of as a <strong>linear combination/sum of waves with different frequencies $u,v$</strong>. Such that, in the end it is found that:</p>

  <ul>
    <li>low frequency information usually encapsulates details of the image</li>
    <li>high frequency usually encapsulates noise</li>
  </ul>
</blockquote>

<h2 id="backgrounds">Backgrounds</h2>

<p>Recall that for a sinusoid, we have <strong>three key parameters</strong> to specify a wave</p>

\[g(t) = A \sin(2 \pi ft + \phi) = A \sin (\omega t + \phi)\]

<p>where:</p>

<ul>
  <li>
    <p>$A, \phi , f$ are amplitude, phase, freqency respectively.</p>
  </li>
  <li>
    <p>essentially, Fourier transform gets any function to a sum of those waves by telling us <strong>what would be the $A_i, \phi_i, f_i$</strong> for each component (technically, Fourier transform is a <strong>function</strong> when given frequency $f_i$, what will be the amplitude and phase $A_i, \phi_i$)</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204105122051.png" alt="image-20220204105122051" style="zoom:50%;" /></p>

    <p>where frequency is encoded in the $x$-axis</p>

    <ul>
      <li>for instance, according to the graph, the decomposition to $f=0$ has $A\approx 55$ and $\phi = 0$</li>
    </ul>
  </li>
</ul>

<p>Now, in <strong>2D</strong>,</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204105447791.png" alt="image-20220204105447791" style="zoom:50%;" /></p>

<p>where since our image is in 2D, we will have two axis/two waves: <strong>horizontal frequency and vertical frequency</strong>.</p>

<ul>
  <li>typically the coordinate $(0,0)$ will be in the center of the image</li>
  <li>for amplitude graph: black means $0$, white is large</li>
  <li>for phase graph: grey means 0, black means negative, and white means large</li>
  <li>note that fourier series by default generates an <strong>infinite amount of waves</strong>, yet here we do cut off at certain frequencies
    <ul>
      <li>all those waves are fully specified by $A_i , \phi_i, f_i$, which are all available on the two plots!</li>
    </ul>
  </li>
</ul>

<h2 id="fourier-transform-1">Fourier Transform</h2>

<blockquote>
  <p><strong>Aim</strong>: the goal of this is to find a procedure, that</p>

  <ul>
    <li><strong>given</strong> some signal wave $g(t)$, or $g(x,y)$ if you think of images, and a frequency $f$ of interest</li>
    <li><strong>return</strong> $A_f, \phi_f$ being the amplitude and phrase corresponding to that $f$</li>
  </ul>

  <p>so essentially tells you the $f$-th term in the fourier series.</p>
</blockquote>

<p>Recall that we can we know</p>

\[e^{ift} = \cos (ft) + i \sin (ft)\]

<p>Then, if we <strong>increase</strong> $t$, we will basically find a <strong>unit circle</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204110844923.png" alt="image-20220204110844923" style="zoom:33%;" /></p>

<p>where the vertical component will be $i$. So this could represent a wave!</p>

<ul>
  <li>e.g. increasing amplitude means a larger circle</li>
</ul>

<p>Then, we can consider $Ae^{ift}$ with different $A$ and frequency $f$:</p>

<p><img src="https://raw.githubusercontent.com/gist/amroamroamro/617305c05001caffc8d0/raw/a7c7fd8eceb2a89f89df09f50d299cecdaa1ed8a/out1_.gif" alt="Fourier series animation · GitHub" /></p>

<p>where:</p>

<ul>
  <li>essentially we can imagine the <strong>sinusoidal</strong> as unit circles but with <strong>different amplitude</strong> and different frequency (time taken to complete an entire revolution)</li>
</ul>

<p>Now, consider that we are <strong>modulating the amplitude by the signal</strong></p>

\[g(t) e^{-2\pi ift}\]

<p>then essentially:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204151041842.png" alt="image-20220204151041842" style="zoom:50%;" /></p>

<ul>
  <li>while you are revolving the circle, you are “wrapping the original wave/signal $g(t)$” around it</li>
</ul>

<p>Then, fourier transform does:</p>

\[G(f) = \int_{\infty}^{\infty}g(t)e^{-2\pi i ft}dt\]

<p>which is basically can be thought as <strong>calculating the average position of $g(t)$</strong>, when ==given some frequency $f$==</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204111631759.png" alt="image-20220204111631759" style="zoom:33%;" /></p>

<p>notice that:</p>

<ul>
  <li>the function output is in <strong>frequency domain</strong>, where as the original signal is in $t$ domain</li>
  <li>with <strong>different frequency</strong>, the final shape/average position might be different (see below)</li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Consider the following original signal:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204111840264.png" alt="image-20220204111840264" style="zoom: 33%;" /></p>

<p>Then:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Low Frequency</th>
      <th> </th>
      <th style="text-align: center">Slightly Higher Frequenct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204112008915.png" alt="image-20220204112008915" /></td>
      <td><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204112017757.png" alt="image-20220204112017757" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204112024240.png" alt="image-20220204112024240" /></td>
    </tr>
  </tbody>
</table>

<p>where we notice that we only plotted for a <strong>finite amount of time</strong>, instead of $t \in [-\infty, \infty]$</p>

<ul>
  <li>
    <p>since $g(t)=\cos(t)+1$, there are time when amplitude $g(t) \to 0$. Hence they go back to the origin on the graph.</p>
  </li>
  <li>
    <p>for a different frequency, we have a finite amount revolved as time is finite here</p>
  </li>
</ul>

<p>Then, if we consider the <strong>average</strong>, i.e. the center of mass, the following images</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original</th>
      <th style="text-align: center">Computing $G(f)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204151510481.png" alt="image-20220204151510481" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204151521558.png" alt="image-20220204151521558" /></td>
    </tr>
  </tbody>
</table>

<p>which then means $G[f=1 /(2\pi)]$ spits out approximately $1 + 0i$.</p>

<ul>
  <li>
    <p>notice the <strong>output is always a Complex number</strong>.</p>
  </li>
  <li>then, since we can do this for many different frequencies, we have a function of frequency $G(f)$</li>
  <li>it can be shown that the “angle” of the complex vector will <strong>always be $0$</strong> if there is <strong>no phase</strong>.</li>
</ul>

<p>This means that If I do a phase shift, then essentially I <strong>start the wave</strong> at another position. Hence this results in the following:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204113014791.png" alt="image-20220204113014791" style="zoom:33%;" /></p>

<p>where we have <strong>rotated the circle</strong></p>

<ul>
  <li>so the <strong>angle of the vector</strong> has information about the ==phase==</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204112513787.png" alt="image-20220204112513787" style="zoom: 33%;" /></p>

<p>where the circle is a bit bigger.</p>

<ul>
  <li>
    <p>so the <strong>magnitude of the vector</strong> has information about the ==amplitude==</p>
  </li>
  <li>
    <p>so if an amplitude of zero, this means that that frequency wave is <strong>not contributing</strong> to the $G(f)$</p>
  </li>
</ul>

<p>Then, the general formula would be:</p>

\[G(f) = \int_{\infty}^{\infty}g(t)e^{-2\pi i ft}dt =\mathbb{R}[G(f)] + i\cdot \mathbb{I}[G(f)]\]

<p>has a real and an imaginary part, hence:</p>

\[\begin{cases}
\sqrt{\mathbb{R}[G(f)]^2 + \mathbb{I}[G(f)]^2}, &amp; \text{amplitude}\\
\tan^{-1}(\mathbb{I}[G(f)] / \mathbb{R}[G(f)]), &amp; \text{phase}
\end{cases}\]

<p>so a single complex number output of $G(f)$ has <strong>all the information about amplitude and phase</strong>!</p>

<blockquote>
  <p><strong>Note</strong></p>

  <p>In reality, you will have $g(t)$ taking a discrete domain (as you will see, essentially $g(x)\to g(x)$ if we think about position in the image). The number of frequencies you need to describe it will be the same as the number “positions” you have in your discrete $g(t)$, i.e. size of the domain.</p>
</blockquote>

<p>Finally, for the 1D case:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204113627782.png" alt="image-20220204113627782" style="zoom: 33%;" /></p>

<p>Then for a ==higher dimension==, you will just be having multiple integrals over $dt_x dt_y$ for instance:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204152341182.png" alt="image-20220204152341182" style="zoom: 80%;" /></p>

<p>where:</p>

<ul>
  <li>$(x,y)$ would be the position in your image, and $u,v$ would be <strong>horizontal and vertical frequencies</strong></li>
</ul>

<hr />

<p><em>For Example</em></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204114026880.png" alt="image-20220204114026880" style="zoom: 50%;" /></p>

<p>where this means:</p>

<ul>
  <li>for the first column: the only waves that are “contributing” are the <strong>low frequency waves</strong> (because only those have non-zero amplitude/white dots). There is a tilt because the original wave in the image $g(x,y)$ <strong>has a phase</strong>.</li>
  <li>the higher the <strong>frequency</strong> in the image, we therefore have a <strong>larger magnitude</strong> of the vector of $G(f)$, hence farther away the activated points in the frequency domain</li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <p>For any signals that takes <strong>only takes real component</strong>, the amplitude will be symmetrical.</p>

  <ul>
    <li>an easy way to think about is that you will need to “cancel out” the imaginary component, as images are real</li>
  </ul>
</blockquote>

<p>Another real life example would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204114751815.png" alt="image-20220204114751815" style="zoom:33%;" /></p>

<p>where:</p>

<ul>
  <li>recall that horizontal and vertical component of the amplitude graph are frequencies</li>
  <li>in the image, <strong>horizontal</strong> sinusoids will have a <strong>low frequency component being more dominant</strong>, because the horizontal part of the image have rather <strong>slow</strong> “changes”. Hence, we have mostly <strong>low horizonal frequency activated</strong> in the $G(f)$</li>
  <li>in the image, <strong>vertical</strong> sinusoids will need <strong>high frequency component</strong>, since the change/sinusoids in the original image vertically is <strong>fast</strong>. Therefore, we see <strong>high vertical frequency activated</strong> in the $G(f)$</li>
</ul>

<p>In code, this is how it is done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cat_fft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">fftshift</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">cat</span><span class="p">))</span>
<span class="n">dog_fft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">fftshift</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">dog</span><span class="p">))</span>

<span class="c1"># Visualize the magnitude and phase of cat_fft. This is a complex number, so we visualize
# the magnitude and angle of the complex number.
# Curious fact: most of the information for natural images is stored in the phase (angle).
</span><span class="n">f</span><span class="p">,</span> <span class="n">axarr</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axarr</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cat_fft</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">axarr</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">angle</span><span class="p">(</span><span class="n">cat_fft</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220214000106672.png" alt="image-20220214000106672" style="zoom:67%;" /></p>

<hr />

<p><em>For Example</em>: Blurring and Edge detection</p>

<p>Originally, we would have the image as:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204115620330.png" alt="image-20220204115620330" style="zoom: 33%;" /></p>

<p>Then if we <strong>remove the high frequency</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204115744842.png" alt="image-20220204115744842" style="zoom: 33%;" /></p>

<p>notice that:</p>

<ul>
  <li>this is the <strong>same effect as blurring the photo</strong> (we see why convolving with Gaussian filter is the same as this soon)</li>
</ul>

<p>Then, if we <strong>remove low frequency</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204115853766.png" alt="image-20220204115853766" style="zoom:33%;" /></p>

<p>note that:</p>

<ul>
  <li>this is the same as ==edge== detection</li>
</ul>

<hr />

<p>In code, this is how it is done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we can create a low mask utlizing outer product
</span><span class="nb">filter</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cat_fft</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">.</span><span class="n">shape</span>
<span class="n">box_width</span> <span class="o">=</span> <span class="mi">10</span>
<span class="nb">filter</span><span class="p">[</span><span class="n">w</span><span class="o">//</span><span class="mi">2</span><span class="o">-</span><span class="n">box_width</span><span class="p">:</span><span class="n">w</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="n">box_width</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="o">//</span><span class="mi">2</span><span class="o">-</span><span class="n">box_width</span><span class="p">:</span><span class="n">h</span><span class="o">//</span><span class="mi">2</span><span class="o">+</span><span class="n">box_width</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># high and low mask filter
</span><span class="n">high_mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">filter</span>
<span class="n">low_mask</span> <span class="o">=</span> <span class="nb">filter</span>
</code></pre></div></div>

<p>Then applying the filter to <strong>FFT version of the image</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># filtering fft, elementwise dot
</span><span class="n">cat_fft_filtered</span> <span class="o">=</span> <span class="n">high_mask</span> <span class="o">*</span> <span class="n">cat_fft</span> <span class="c1"># cat_fft = np.fft.fftshift(np.fft.fft2(cat))
</span><span class="n">dog_fft_filtered</span> <span class="o">=</span> <span class="n">low_mask</span> <span class="o">*</span> <span class="n">dog_fft</span>

<span class="n">cat_filtered</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">ifft2</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">ifftshift</span><span class="p">(</span><span class="n">cat_fft_filtered</span><span class="p">)))</span> <span class="c1"># shift back and then transform
</span><span class="n">dog_filtered</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">ifft2</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="n">ifftshift</span><span class="p">(</span><span class="n">dog_fft_filtered</span><span class="p">)))</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axarr</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axarr</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">dog_filtered</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">axarr</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">cat_filtered</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220214000251012.png" alt="image-20220214000251012" /></p>

<h2 id="convolution-with-ft">Convolution with FT</h2>

<p>Now it turns out that:</p>

<blockquote>
  <p><strong>Theorem</strong></p>

  <p><strong>Convolution</strong> in $x,y$ space is <strong>element-wise multiplication</strong> in frequency space</p>

\[g(x) * h(x) = \mathcal{F}^{-1}[\mathcal{F}[g(x)] \cdot \mathcal{F}[h(x)]]\]

  <p>and <strong>convolution</strong> in frequency space is the same as <strong>element-wise multiplication</strong> in $x,y$ space:</p>

\[\mathcal{F}[g(x)] * \mathcal{F}[h(x)] = \mathcal{F}[g(x) \cdot h(x)]\]

  <p>where the 2D version of this is analogous.</p>
</blockquote>

<p>This means you <strong>could speed up convolution operation</strong> since <strong>element-wise multiplication</strong> can be done fast (technically this also depends on the speed you Fourier transforms)</p>
<ul>
  <li>if your filter is <strong>huge</strong>, then doing <strong>Fourier Transformation</strong> and element-wise dot product is fast
    <ul>
      <li>e.g. if your image is size $n \times m$, and filter size $n \times m$, with padding, you will get $O(n^2m^2)$ if doing convolution</li>
    </ul>
  </li>
  <li>if your filter is <strong>small</strong>, then <strong>convolution</strong> in space would be faster
    <ul>
      <li>as Fourier transform takes time</li>
    </ul>
  </li>
  <li>This is also why we mentioned to treat essentially an image/filter as <strong>a function!</strong> (i.e. $g(x), h(x)$ shown in the text)</li>
</ul>

<p>For instance:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204120346836.png" alt="image-20220204120346836" style="zoom:33%;" /></p>

<p>notice that:</p>

<ul>
  <li>
    <p>in reality, applying Fourier Transform returns your a ==matrix of complex numbers== (i.e. the vector of $G(f)$). So technically you are doing <strong>element-wise multiplication for those complex numbers</strong></p>
  </li>
  <li>
    <p>but for visualization, let us only consider the <strong>amplitude</strong> of the returned complex vectors in $G(f)$. (so if that is zero, than means the particular frequency wave is not useful) Then, element-wise multiplication with a Gaussian filter is basically <strong>removing high frequency details</strong>.</p>
    <ul>
      <li>note that FT of Gaussian is still a Gaussian</li>
    </ul>
  </li>
</ul>

<hr />

<p><em>For Example</em></p>

<p>Now, it makes sense that why box filters have the following effect</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204121440613.png" alt="image-20220204121440613" style="zoom:33%;" /></p>

<p>which is suboptimal as compared to Gaussian filter. This is because when we do Fourier transform for box wave:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204104255734.png" alt="image-20220204104255734" style="zoom:50%;" /></p>

<p>we had <strong>high frequency terms involved!</strong></p>

<p>Therefore, the FT of box filter looks like:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204121355423.png" alt="image-20220204121355423" style="zoom:33%;" /></p>

<p>which <strong>included some high frequency noises</strong>.</p>

<hr />

<p><em>For Example</em>: Laplacian Filter</p>

<p>In reality, we often use the following instead of $[-1,2,1]^T$ as Laplacian filter:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204153837782.png" alt="image-20220204153837782" style="zoom:50%;" /></p>

<p>This is because, if we consider the <strong>Fourier transform</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204121717008.png" alt="image-20220204121717008" style="zoom:33%;" /></p>

<p>where we notice that</p>

<ul>
  <li>
    <p>just using $[-1,2,1]^T$ would have <strong>included lots of high frequency noise</strong>, as shown on the bottom</p>
  </li>
  <li>
    <p>but we want to remove both details and those noise to leave edges. Hence:</p>

    <ol>
      <li>involve a Gaussian blurring = removing high frequency</li>
      <li>perform $[-1,2,1]^T$ filter to remove low frequency details</li>
    </ol>

    <p>The end product is what we see on the top, which is the commonly used <strong>Laplacian filter</strong></p>
  </li>
</ul>

<h2 id="hybrid-image">Hybrid Image</h2>

<p>This is more of a interesting application of Fourier transform. Consider the question: What frequency waves can we see from a monitor if you are <em>exactly 150cm</em> away?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204154150191.png" alt="image-20220204154150191" style="zoom:50%;" /></p>

<p>where the key idea is that you will <strong>not be able to perceive</strong> certain frequencies well.</p>

<p>The result shows that:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204123548985.png" alt="image-20220204123548985" style="zoom:33%;" /></p>

<p>hence, any wave with configuration above the red line, people cannot see the wave/they see just grey stuff</p>

<ul>
  <li>
    <p>contrast is brightness/amplitude</p>
  </li>
  <li>
    <p>then maybe you can <em>hide data</em> above the red line</p>
  </li>
</ul>

<p>For example:</p>

<p>Consider keeping only low frequency data of a man’s face with high frequency data of a women’s face:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204123628867.png" alt="image-20220204123628867" style="zoom:33%;" /></p>

<p>so that:</p>

<ul>
  <li>depend on how far away you are, the red line is at different position.</li>
  <li>when you are <strong>far</strong>, the <strong>high frequency</strong> details you will not be able to discern. But when you are close, you will be able to see the high frequency</li>
</ul>

<p>Then another example:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220204123905600.png" alt="image-20220204123905600" style="zoom:33%;" /></p>

<p>where Einstein will be encoded in the high frequency data.</p>

<ul>
  <li>here we scaled them so you can experience see the image “from afar”</li>
</ul>

<h1 id="machine-learning">Machine Learning</h1>

<p>If you take this class 10 years ago, you would be majorly doing <strong>maths to design filters</strong>, such that properties such as shift invariance is satified. However, it turns out that those <strong>filters/kernels</strong> can be <strong>learnt</strong> by ML/DL architectures.</p>

<ul>
  <li>specifty the constraints, such as Toeplitz matrix, then let the machine learn it</li>
</ul>

<h2 id="regression-review">Regression Review</h2>

<p>Checkout the ML notes on reviewing the basics of <strong>regression</strong></p>

\[\hat{y}=f(x;\theta)\]

<p>where:</p>

<ul>
  <li>$\theta$ willl be our parameters to learn</li>
  <li>the difference between regression/classification is basically the <strong>loss</strong> you are trying to assign</li>
</ul>

<p><strong>Objective</strong> function is essentially what <strong>drives the algorithm</strong> to update the parameters:</p>

\[\min \mathcal{L}(\hat{y},y)\]

<p>Some notes you should read on:</p>

<ul>
  <li><strong>Linear Regression</strong> and <strong>Logistic Regression</strong></li>
  <li>checkout how to prove that XOR problem is not solvable by linear models</li>
  <li><strong>Convolutional Neural Network</strong></li>
  <li><strong>Backpropagation</strong></li>
</ul>

<p>Some key take-aways:</p>

<ul>
  <li>
    <p>Essentially we are having <strong>computation graphs</strong></p>

    <p><img src="https://www.ruihan.org/courses/cs224n/fig/computation-graph-backprop.png" alt="CS224N Write-up - RUIHAN.ORG" style="zoom:50%;" /></p>

    <p>then your network architecture eventually is about <strong>what operation you want</strong> for each block.</p>

    <p>Then, essentially you will have a loss that is a nested function:</p>

\[\mathcal{L} = f(W^3f(W^2f(W^1x)))\]

    <p>then I ask you to compute $\partial L / \partial W^1$? You realize that computing this needs:</p>

    <ul>
      <li>$\partial L / \partial W^3$</li>
      <li>$\partial L / \partial W^2$</li>
    </ul>

    <p>Hence you realize that you can</p>

    <ul>
      <li>compute everything in <strong>one go</strong> by backpropagation.</li>
      <li>you have a dependency tree, where the latest layer $\partial L / \partial W^3$ will get used by all other children nodes. So it makes sense to do backpropagation.</li>
    </ul>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218111613137.png" alt="image-20220218111613137" style="zoom:50%;" /></p>
  </li>
</ul>

<hr />

<p><em>Note</em>:</p>

<p>A good trick you can use to compute derivative would be the following. Consider:</p>

\[y = W^{(2)}h+b^{(2)}\\
L = \frac{1}{2}||y-t||^2\]

<p>And we need $dL/dh$:</p>

<ol>
  <li>
    <p>consider scalar derivatives:</p>

\[\frac{dL}{dh} = \frac{dL}{dy}\frac{dy}{dh} = \frac{dL}{dy}W^{(2)}\]
  </li>
  <li>
    <p>Convert this to vector and check dimension:</p>

\[\frac{dL}{dh} \to \nabla_h L\]

    <p>hence:</p>

\[\nabla_hL = (\nabla_y L) W^{(2)},\quad \mathbb{R}^{|h| \times 1}=\mathbb{R}^{|y| \times 1}\times \mathbb{R}^{|y| \times h}\]
  </li>
  <li>
    <p>Correct the dimension to:</p>

\[\mathbb{R}^{|h| \times 1}=\mathbb{R}^{h \times|y|}\times \mathbb{R}^{|y| \times 1}\]

    <p>which means:</p>

\[\nabla_h L = W^{(2)^T}(\nabla_y L)\]
  </li>
</ol>

<h2 id="convolution-layer-review">Convolution Layer Review</h2>

<p>Review the CNN chapter of DL</p>

<ul>
  <li>
    <p>Instead of linear layers that does $W^Tx + b$, consider doing <strong>convolution operation $*$</strong>:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Separated</th>
          <th style="text-align: center">Compact Overview</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218112016128.png" alt="image-20220218112016128" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218112042383.png" alt="image-20220218112042383" /></td>
        </tr>
      </tbody>
    </table>

    <p>then question is then, <strong>what is the gradient of this operation?</strong></p>
  </li>
  <li>
    <p>another frequently used layer is <strong>max-pooling</strong>. For instance, $2 \times 2$ with stride $2$ does:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218112210127.png" alt="image-20220218112210127" style="zoom: 33%;" /></p>

    <p>why would you want to do this?</p>

    <ul>
      <li>e.g. when you are detecting cats in an image, and certain neurons get triggered, you can use max pooling to <strong>only focus on those activated values</strong> (easier for classification head as you ignore low value ones)</li>
      <li>cheap resize operation which can cut down the number of neurons/connections for further layers</li>
      <li>the gradient defined here would be:
        <ul>
          <li>$1$ for the pixel that is the max</li>
          <li>$0$ otherwise.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>batch normalization</strong> also very important</p>

\[x_{i+1} = a_i \frac{x_i - \mathbb{E}[x_i]}{\text{Var}(x_i)} + b_i\]

    <p>where:</p>

    <ul>
      <li>$a_i$, $b_i$ is the scaling and shift parameter</li>
      <li>this is called batch normalization as this operation will be applied the same way to the <strong>entire batch</strong>.</li>
    </ul>
  </li>
  <li>
    <p><strong>dropout</strong>: a layer where with some probability we output $0$</p>

\[x_{i+1}^j = \begin{cases}
x_{i+1}^j &amp; \text{with probability $p$}\\
0 &amp; \text{otherwise}
\end{cases}\]

    <p>which is pretty helpful for preventing overfitting.</p>
  </li>
  <li>
    <p><strong>Softmax</strong>: we are doing some kind of max, but also making sure we can compute the gradient</p>

\[x_{i+1}^j = \frac{\exp(x_i^j)}{\sum_k \exp(x_i^k)}\]

    <p>which can also be interpreted as a <strong>probability distribution</strong></p>
  </li>
</ul>

<p>Then an example CNN looks like</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218114929082.png" alt="image-20220218114929082" style="zoom:50%;" /></p>

<p>then in order to train your network/take gradient, you would need to define $\mathcal{L}$.</p>

<ul>
  <li>
    <p>typical loss function would be <strong>cross entropy loss</strong>: Average number of bits loss/needed to encode $y$ if the coding schema from $\hat{y}$ is used instead.</p>

\[\mathcal{L}(y,\hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)\]
  </li>
  <li>
    <p>once done, you can also look at the filters/weights learnt and visualize them</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218115515302.png" alt="image-20220218115515302" style="zoom:50%;" /></p>

    <p>where notice that:</p>

    <ul>
      <li>the top FFT means that we are concentrating on <strong>low frequency data</strong></li>
      <li>the bottom FFT shows that they look at <strong>top frequency data</strong></li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong>: Why ReLU?</p>

\[\text{ReLU}(a)=\max(0,a),\quad a = Wx+b\]

  <p>Then</p>

  <ul>
    <li>One major benefit is the <strong>reduced likelihood of the gradient to vanish</strong>. This arises when $a&gt;0$. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.</li>
    <li>The other benefit of ReLUs is <strong>sparsity</strong>. Sparsity arises when a≤0a≤0. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations.</li>
  </ul>

  <p>However, there is a <strong>Dying ReLU</strong> problem - if too many activations get below zero then most of the units(neurons) in network with ReLU will simply output zero, in other words, die and thereby prohibiting learning.</p>
</blockquote>

<h2 id="width-vs-depth">Width vs Depth</h2>

<p>We consider:</p>

<ul>
  <li><strong>width</strong> = how many neurons? (i.e. size of weight matrix $W$)</li>
  <li><strong>depth</strong> = how many layers? (i.e. how many of those weights to learn)</li>
</ul>

<p>In reality, there is a interesting theoretical result which is rarely used in reality</p>

<blockquote>
  <p><strong>Universal approximation theorem</strong>:  With ==sufficiently wide== network and just one (hidden) layer, you can approximate any continuous function with arbitrary approximation error.</p>
</blockquote>

<p>The problem is that</p>

<ul>
  <li>
    <p>it doesn’t specify “how wide we need”, which could be extremely wide hence not computational efficient.</p>
  </li>
  <li>
    <p>but if we go deep, we can backprop and it is in general quite fast</p>
  </li>
</ul>

<h1 id="object-recognition">Object Recognition</h1>

<p>Why is it so hard for a machine to do object recognitions?</p>

<blockquote>
  <p><strong>Canonical Perspective</strong>: the best and most easily recognized view of an object</p>

  <ul>
    <li>e.g. a perspective so that you can recognize this object very fast</li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218120050840.png" alt="image-20220218120050840" style="zoom: 50%;" /></p>

<p>where you should feel that the <strong>top row is easier to recognize</strong></p>

<ul>
  <li>
    <p>how can you train a network that works ==regardless of the perspective==?</p>
  </li>
  <li>
    <p>model will also <strong>learn the bias</strong></p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218121225784.png" alt="image-20220218121225784" style="zoom:33%;" /></p>

    <p>e.g. all handles are almost all on the right!</p>
  </li>
</ul>

<blockquote>
  <p><strong>Entry Level Categories</strong>: The first category a human pick when classifying an object, among potentially a tree of categories that corresponds to an object.</p>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218123633204.png" alt="image-20220218123633204" style="zoom:50%;" /></p>

<p>the question is, why did you think of this as a dolphin, but <strong>not</strong> saying it is “an animal”? A “living being”?</p>

<p><strong>Other problems</strong> involve:</p>

<ul>
  <li>scale problem</li>
  <li>
    <p>illumination problem</p>
  </li>
  <li>
    <p>within-class <strong>variation</strong></p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218120403775.png" alt="image-20220218120403775" style="zoom:33%;" /></p>
  </li>
</ul>

<blockquote>
  <p><strong>Note</strong></p>

  <ul>
    <li>In reality, many massive models are trained with <strong>data</strong> coming from <strong>crowdsourcing</strong>: paying people around the world to label data (e.g. Amazon Mechanical Turk)</li>
    <li>one large image dataset commonly used is <strong>ImageNet</strong> - often used as a <strong>benchmark</strong> for testing your model performance.</li>
  </ul>
</blockquote>

<h2 id="classical-view-of-categories">Classical View of Categories</h2>

<p>One big problem is “what is xxx”? Hot Dog or a Sandwich?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218122237352.png" alt="image-20220218122237352" style="zoom:25%;" /></p>

<p>Some natural ways a <strong>human think about categorizing an object</strong>:</p>

<ul>
  <li>A category is formed by defining properties</li>
  <li>Anything that matches all/enough of the properties is part of the category</li>
  <li>Anything else is outside of the category</li>
</ul>

<p>But even this idea could <strong>vary</strong>, in different people/culture.</p>

<ul>
  <li>
    <p>e.g. in some indigenous people in Australia, people have a single word for “Women, Fire, and Dangerous Things”</p>
  </li>
  <li>
    <p>e.g. in a culture, what are the words you use to <strong>represent colors</strong>?</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220218122940067.png" alt="image-20220218122940067" style="zoom:50%;" /></p>

    <p>where:</p>

    <ul>
      <li>if you only have two words for color, which colors will you pick? Black and White</li>
      <li>for three colors, most people gives Red</li>
      <li>the take-away message is that you can <strong>think of things even if you don’t have language for it</strong>. Yet for machine models, we are categorizing objects based on language (i.e. language label for category)</li>
    </ul>
  </li>
</ul>

<p>Another way to define category would be:</p>

<blockquote>
  <p><strong>Affordance</strong>: An object is defined by what action it affords</p>

  <ul>
    <li>e.g. what we can do with it</li>
    <li>e.g. a <em>laptop</em> is a laptop for us, but could be a <em>chair</em> for a pet</li>
  </ul>
</blockquote>

<p>A theory of him is that when we see an object, we <strong>automatically think about affordance of it</strong>, i.e. what we can do with it.</p>

<h2 id="two-extremes-of-vision">Two Extremes of Vision</h2>

<p>In reality, we are always dealing with either of the two occasions:</p>

<ul>
  <li>we don’t have much data, we need extrapolation to predict things</li>
  <li>we have much data, we need to interpolate and find differences between existing objects</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225230611973.png" alt="image-20220225230611973" style="zoom: 50%;" /></p>

<p>where:</p>

<ul>
  <li>the latter end of the spectrum would be captured more by NN types of model, which tends to be poor at <strong>generalization</strong>, so we care a lot of few-shot training/zero-shot training</li>
  <li>for huge training dataset, one reason for test accuracy to be high is that the training dataset distribution <strong>does model the true distribution</strong>, hence “overfitting” will not really damage performance.</li>
</ul>

<h3 id="exemplar-svm">Exemplar SVM</h3>

<p>In reality approaches that uses <strong>big data</strong> to do basically <em>lookup function</em> for classification.</p>

<p>One example is the <strong>Exemplar-SVM</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225232028053.png" alt="image-20220225232028053" style="zoom:67%;" /></p>

<p>this idea can be seen as a new way to do ==classification==. For example data in the training set, train a SVM where that single data point is a positive example, whereas all the others are negative. Graphically:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225233815650.png" alt="image-20220225233815650" style="zoom:50%;" /></p>

<p>Therefore, you learn $N$ SVM, if there are $N$ data points. With this, when you are classifying an input $x$, all you need to do is to ask is: to which of the $N$ data point is it <strong>most similar to w.r.t the SVM</strong>? (hence it is like a k-NN). Then, when giving an image, you do:</p>

<ol>
  <li>for each possible window in an image</li>
  <li>try all the $N$-SVM and pick out the SVM that fires the most (hence it is like a lookup table)</li>
  <li>Since each SVM trained corresponds to an object, this can be used for <strong>object recognition</strong></li>
</ol>

<p>Graphically</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225234156924.png" alt="image-20220225234156924" style="zoom: 50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>since SVM gives <em>some degree of extrapolation</em>/robustness, it works even if the bus has a different color.</li>
</ul>

<p>This works essentially based on the idea that, instead of definition <em>what is a car</em>, we consider <em>what is this object similar to</em> (something we already know)?</p>

<p>This setup in the end can also do <strong>segmentation and occlusion</strong>, just because there are <strong>many repetitions</strong> in our real world.</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225234744502.png" alt="image-20220225234744502" style="zoom:50%;" /></p>

<p>where the above would be an example of segmentation</p>

<blockquote>
  <p><strong>What might not work</strong>:</p>

  <ul>
    <li>there is a view-point bias for photos, so that technically if you change the view point, the SVM might not work. However, again, assuming we have huge data, there could be essentially many images taken from many viewpoints. Then it still works.</li>
  </ul>
</blockquote>

<h3 id="deformable-part-models">Deformable Part Models</h3>

<p>This idea is then to learn each <strong>component of the objects</strong> + learn the <strong>connections</strong>. This would work extremely well at detecting poses, for instance, where all we changed is the connection between components of the object (human).</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225235533615.png" alt="image-20220225235533615" style="zoom: 33%;" /></p>

<p>Specifically, you would <strong>build a tree</strong> that connects the</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225235648846.png" alt="image-20220225235648846" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>nodes encode the <strong>component</strong> we recognized, e.g. the <code class="language-plaintext highlighter-rouge">root</code> would be the torso, and etc.</li>
  <li>edges encode the <strong>relationship</strong> we found, e.g. relative relationship between leg and torso.</li>
</ul>

<blockquote>
  <p><strong>Therefore</strong>, as it can recognize individual parts + connections, it <strong>can work with different view points</strong>.</p>
</blockquote>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220225235843403.png" alt="image-20220225235843403" style="zoom:67%;" /></p>

<p>Specifically, this model does the following as the objective for similarity:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226000328888.png" alt="image-20220226000328888" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>$z_i$ is the location of the different parts/components</p>
  </li>
  <li>
    <p>part template refers the score for the position nodes w.r.t the large image</p>
  </li>
  <li>
    <p>deformation model refers to the score for the edges w.r.t the pair of node, e.g. answering the question: what is the score if a leg is below a torso?</p>
  </li>
</ul>

<h3 id="r-cnn">R-CNN</h3>

<p>Consider a task that tries to assign a <strong>category to each pixel</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226002250786.png" alt="image-20220226002250786" style="zoom: 50%;" /></p>

<p>The idea is basically to:</p>

<ol>
  <li>consider <strong>all possible windows</strong> (of various sizes) in an image</li>
  <li>for each window:
    <ol>
      <li>in each of the window, classify if we should continue processing it</li>
      <li>if yes, put it into CNN and classify the window</li>
    </ol>
  </li>
</ol>

<p>Graphically, we are doing</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226002400414.png" alt="image-20220226002400414" style="zoom: 50%;" /></p>

<p>and it works pretty well in reality. However, the problem is that it is <strong>slow</strong>. Therefore we also had models such as Faster CNN, by learning the <strong>window proposition step</strong>, i.e. which windows are plausible, hence reduce the time.</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226002834815.png" alt="image-20220226002834815" style="zoom: 50%;" /></p>

<p>then you basically just backpropagate to update the weights:</p>

<ul>
  <li>initially the convolutional layer at the bottom of right image would consider <strong>all possible windows</strong></li>
  <li>the Region of Interest feature vector would encode the proposed window, then you compute loss to the window proposed as you know the bounding box</li>
  <li>in faster RNN, the feature maps are used two fold: used for proposal <strong>and</strong> being passed on as encoding what is inside the window</li>
</ul>

<h3 id="segmentation-examples">Segmentation Examples</h3>

<p>Consider the task to assign each pixel of the image a label: either a category, or whether if it is a new instance. This task is commonly referred to as <strong>segmentation</strong>.</p>

<p>Some architecture that aims to solve this include <strong>Fully Convolutional Network</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226123504667.png" alt="image-20220226123504667" style="zoom:50%;" /></p>

<p>Essentially you can just <strong>keep doing convolution</strong>, so the output is still an image</p>

<hr />

<p><strong>Encoder-Decoder</strong> type.</p>

<p>Here the idea is that, in order to be able to recognize a “bed”, you need to somehow encode all the related pixels into a group and recognize this group of pixels is a bed.</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226123826560.png" alt="image-20220226123826560" style="zoom: 50%;" /></p>

<p>where essentially the latent feature space would be able to encode/compress pixels. However, this does mean resolution loss in the output image, hence we also have <strong>skip confections</strong> added.</p>

<h3 id="residual-networks">Residual Networks</h3>

<p>The observation comes from the abnormal behavior that, increasing the layers actually caused a decrease in performance for both train and test:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226130605234.png" alt="image-20220226130605234" style="zoom:50%;" /></p>

<p>this is abnormal because, if the 20-layer solution is optimal, then the other 36-layers should be able to learn to do <strong>nothing</strong>, or doing <strong>identity operation</strong>.</p>

<p>Then, the intuition is to make <strong>learning nothing</strong> an easy thing to do for the network. Hence:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226130825570.png" alt="image-20220226130825570" style="zoom:50%;" /></p>

<p>where essentially we can have $F(x)=0$ being pretty easy to do (v.s. $F(x)=x$ with nonlinear operation is pretty hard).</p>

<ul>
  <li>This is also helpful for solving <strong>vanishing gradient</strong></li>
  <li>essentially enabled us to train very deep networks!</li>
</ul>

<blockquote>
  <p><strong>Again</strong>, the key reason behind all the idea of training deeper network is that you have <strong>big data</strong> for training.</p>
</blockquote>

<h1 id="video-recognition">Video Recognition</h1>

<blockquote>
  <p><strong>Theory of mind</strong> refers to the capacity to understand other people by ascribing mental states to them. In terms of CS, it is like you have the same program, but different memory. Yet you still can more of less know what the program will do.</p>
</blockquote>

<p>First of all, we need to represent video as some kind of numbers. Consider videos as a <strong>series of pictures</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226141015412.png" alt="image-20220226141015412" style="zoom: 50%;" /></p>

<p>Then essentially you just have a <strong>cube of pictures/matrix</strong>.</p>

<p>Accordingly, convolution operation thus involve a third dimension</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">2D Convolution</th>
      <th style="text-align: center">3D Convolution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226141511338.png" alt="image-20220226141511338" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220226141517105.png" alt="image-20220226141517105" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where now essentially you have an <strong>increased dimension in kernel</strong> + another dimension of <strong>time</strong> for the kernel to move around (convolution).</p>

<ul>
  <li>
    <p>first imagine the video as a grey scale image, then essentially from image convolution (2D kernel) we now have video convolution (3D kernel)</p>
  </li>
  <li>
    <p>note that because the filters basically <strong>also have a time dimension</strong> (stacks of 2D kernel), so they can be represented as a video as well.</p>
  </li>
</ul>

<h2 id="human-behaviors">Human Behaviors</h2>

<p>Before we consider how machines should solve the problem, we should first understand and look around how <strong>human solve those problems</strong> such as:</p>

<ul>
  <li>action <strong>classification</strong>: what is he doing (given a video)? Is his action intentional/unintentional?</li>
  <li>action <strong>prediction</strong>: what will happen next?</li>
</ul>

<h3 id="behavior-continuum">Behavior Continuum</h3>

<p>Consider the case when a children goes to school, an continuous set of events that he/she would do involve:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304102055893.png" alt="image-20220304102055893" style="zoom: 50%;" /></p>

<p>for example, doing $A-G$ would have included doing $A-B$, etc.</p>

<ul>
  <li>this poses the question of <strong>how to quantitatively represent an action</strong> hard, as it’s no longer discrete</li>
  <li>this then relates to how we perhaps want to design video recognition</li>
</ul>

<h3 id="human-brain-video-recognition">Human Brain Video Recognition</h3>

<p>Essentially a video is a stack of images, such that if flipped through fast enough, we have the <strong>illusion</strong> that things are moving. How does a human brain understand videos?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304103208354.png" alt="image-20220304103208354" style="zoom: 50%;" /></p>

<p>where essentially:</p>

<ul>
  <li>
    <p>we are doing two <strong>separate systems</strong>: one that performs object recognition and the other recognizes motion/location.</p>
  </li>
  <li>
    <p>an example would be the <strong>stepping feet illusion</strong>: our dorsal stream regonizes dots moving around as <em>a person walking</em></p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304103827186.png" alt="image-20220304103827186" style="zoom:33%;" /></p>
  </li>
</ul>

<p>Therefore, one idea is to build a network also with <strong>two visual passways</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304104417529.png" alt="image-20220304104417529" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>the spatial stream is basically the normal convolutional net</li>
  <li>the temporal stream basically is the convolutional net but the input is <strong>optical flows</strong>, how each pixel in an image moves</li>
</ul>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<p>Another way to <strong>represent time</strong> would be naturally the recurrent neural networks. When unrolled, basically does:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304104842518.png" alt="image-20220304104842518" style="zoom:50%;" /></p>

<p>where the “forward” formulas becomes:</p>

\[h_i = f(w_x^T x_i + w_h^t h_{i-1})\\
y_i = g(w_y^T h_i)\]

<p>where interestingly:</p>

<ul>
  <li>
    <p>with the additional of time, another way to see this is that we now can do <strong>loops</strong> in FFNN.</p>
  </li>
  <li>
    <p>basically now we have a <strong>state machine</strong>:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304105310518.png" alt="image-20220304105310518" style="zoom: 33%;" /></p>
  </li>
</ul>

<p>Though this network is sound, the problem is that it has a problem of ==vanishing/exploding gradient==. Because when you backpropagation, you would be doing <strong>backpropagation through time</strong>: (==TODO replace $z_i$ with $h_i$==)</p>

<p>At time $i$, we have the <strong>forward pass</strong> being</p>

\[z_i = h_i = f(w_x^T x_i + w_h^T h_{i-1})\]

<p>then the gradient being:</p>

\[\frac{d\mathcal{L}(\hat{y} , y)}{dw} =\frac{d\mathcal{L}}{dz_{i+1}} \frac{dz_{i+1}}{dz_i}\frac{dz_{i}}{dw} = \frac{d \mathcal{L}}{dz_T}\left( \prod_{j=1}^{T-1}\frac{dz_{j+1}}{dz_j} \right)\frac{dz_i}{dw}\]

<p>being the general form.</p>

<ul>
  <li>e.g. let $w = w_h$. (recall that only <strong>three weights</strong>). Then the update/gradient at the end of the sequence at time $T$ will be <strong>products of gradients</strong>, which would either explode or vanish if it is large or small.</li>
  <li>to solve those problem, we have GRU/LSTMs.</li>
</ul>

<h3 id="gru-and-lstm">GRU and LSTM</h3>

<p>Schematically, GRU does the following change:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">RNN Encapsulation</th>
      <th style="text-align: center">GRU Encapsulation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304112058291.png" alt="image-20220304112058291" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304112105007.png" alt="image-20220304112105007" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>Specifically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">GRU Schematic</th>
      <th style="text-align: center">Equations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304111657781.png" alt="image-20220304111657781" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304111642314.png" alt="image-20220304111642314" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Similarly, the LSTM architecture looks like:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304111935706.png" alt="image-20220304111935706" style="zoom:33%;" /></p>

<p>note that you have an <strong>additional memory cell</strong>, $C_{t}$, as compared to the GRU and RNN we had.</p>

<p>Each unit of LSTM look like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">LSTM Schematic</th>
      <th style="text-align: center">Another View</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304111859220.png" alt="image-20220304111859220" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304111908282.png" alt="image-20220304111908282" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the highlighted part is clear, same as RNN.</p>

<p>(a good blog that discuss LSTM would be: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)</p>

<blockquote>
  <p><strong>In both cases</strong>, the backpropagation through time would now involve <strong>addition instead of products</strong>. Hence this aims to solve the <strong>exploding/vanishing gradient</strong> problem.</p>
</blockquote>

<h2 id="action-classification">Action Classification</h2>

<blockquote>
  <p>The basic approach used here is to learn <strong>motion Features</strong></p>

  <ul>
    <li>e.g. elapsed time feature</li>
  </ul>
</blockquote>

<p>Key aspects of motion/video that we seem to care about:</p>

<ul>
  <li>how long does <strong>each action take</strong>? i.e. normally, what would be the elapsed time for a <em>normal motion</em>.</li>
  <li>what are the <strong>main objects</strong>/what will happen <strong>next</strong>?</li>
</ul>

<p>One way to learn this in NN is that we can <strong>resample a video</strong>, and then ask the NN to predict elapsed time:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304115322967.png" alt="image-20220304115322967" style="zoom: 50%;" /></p>

<p>This <strong>feature</strong> can be helpful for:</p>

<ul>
  <li>deciding whether if an action is intentional/unintentional: ==speed of action alters perception==</li>
</ul>

<h2 id="action-prediction">Action Prediction</h2>

<blockquote>
  <p>It turns out that all our mind cares is about the <strong>future/actions</strong>, i.e. for things that seem irrelevant in the future, we kind of just ignores it.</p>

  <ul>
    <li>correlates to the idea before that categorization of an object is related to <em>intention/action</em> we can do with it</li>
  </ul>
</blockquote>

<p>An example to stress how to <strong>predict the future</strong> would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304120205976.png" alt="image-20220304120205976" style="zoom: 33%;" /></p>

<p>this will be called <strong>future geneation</strong>:</p>

<ul>
  <li>given data up to $x_t$</li>
  <li>predict $x_{t+1}$</li>
</ul>

<p>Then for <strong>each video</strong> you collected in your dataset:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304134017805.png" alt="image-20220304134017805" style="zoom: 50%;" /></p>

<p>with loss being</p>

\[\min_w \sum_i ||f(x^i_t; w) - x_{t+1}^i||_2^2\]

<p>which basically is a Euclidean loss:</p>

<ul>
  <li>each vector $x^{i}_t$ represents the <strong>flattened vector representation</strong> of video at time $t$ (hence an image), for the <strong>$i$-th video</strong> in your dataset</li>
</ul>

<blockquote>
  <p>But consider $x_{t+1}^i$ being the $i$-th <strong>possible future</strong> of the video up to $x_{t}$. Now you want to output, say, <strong>all possible futures</strong>, and perhaps among them, <strong>pick the most probable future</strong>.</p>

  <ul>
    <li>note that our brain can do this pretty easily!</li>
  </ul>
</blockquote>

<p>Then, we see a problem that with this is that you can let:</p>

\[f^*(x_t;w) = \frac{1}{m}\sum_i x^i_{t+1}\]

<p>to <strong>regress to the mean</strong>, i.e. your predicted future would be a <strong>mean of possible futures</strong>. This is bad! But how do we build models that is capable of <strong>predicting possible/likely future</strong>?</p>

<p>One problem is that there are <strong>multiple possible outcomes</strong> (i.e. we have uncertainties in what will happen next), but the reality we have in the video has ==only one future==. How do we build this?</p>

<blockquote>
  <p><em>Intuition</em>:</p>

  <p>When a child gets near a candy store, and right before he/she goes inside, what will he/she predict to happen inside?</p>

  <ul>
    <li>instead of saying how many candies, and their color, he/she might predict <strong>his/her own sensation</strong>: they are going to taste like xxx, smell like xxx, and etc.</li>
  </ul>
</blockquote>

<p>Therefore, the idea here is to build a NN with:</p>

<ul>
  <li>input $x_t$, e.g. a picture</li>
  <li>predict the <strong>features</strong> of the future picture $x_{t+1}$. (the feature could come from an encoder that  encodes $x_{t+1}$ for example)</li>
</ul>

<p>Graphically, we are doing:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304172808864.png" alt="image-20220304172808864" style="zoom: 33%;" /></p>

<p>which is an <strong>easier prediction problem</strong>, because the output space is much smaller.</p>

<p>Then, since there are <strong>multiple possible futures</strong>, we could have each <strong>multiple predictions of the feature</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304173101359.png" alt="image-20220304173101359" style="zoom:33%;" /></p>

<p>which we can do by basically having $k$-learnable activation functions/NN attached after. But then, to <strong>train this multiple prediction</strong> model, notice that we <strong>only have one output/future in the video data</strong>, hence only <strong>“labeled feature”</strong> $g(x_{t+1})$:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304121536803.png" alt="image-20220304121536803" style="zoom:33%;" /></p>

<p>so then the problem is <strong>how to figure out the whole distribution</strong> ${f_1(x_1),f_2(x_1),…,f_k(x_1)}$ while you only have <strong>one label/ground truth</strong> $g(x_{t+1})$. Then, the idea is:</p>

<ul>
  <li>
    <p>We know that if we only have one prediction, then we can do:</p>

\[\min_f \sum_i ||f(x_t^i) - g(x_{t+1}^i)||_2^2\]

    <p>for the $i$ data points you have in your training set.</p>
  </li>
  <li>
    <p>If we have only one of them correct, but I do not know which one, then it means we have some <strong>latent variable</strong> to estimate.</p>

    <p>For a <strong>single data point $x_t$</strong>, the loss would be:</p>

\[\sum_k \delta_k ||f_k(x_t) - g(x_{t+1})|| _2^2\]

    <p>for $\delta_k \in {0,1}$ being a <strong>latent variable</strong>, so that $\vert \vert \delta\vert \vert _1 =1$.</p>

    <p>Then for all those data points, we have a different $\delta_k$ to learn:</p>

\[\min_{f,\delta} \sum_i \sum_k^K \delta_k^i ||f_k(x^i_t) - g(x^i_{t+1})|| _2^2,\quad \text{s.t. } ||\delta^i||_1=1\]

    <p>for basically $\delta^i$ being like a one-hot vector to learn.</p>
  </li>
</ul>

<p>Now we have the entire problem setup, lastly we need to train this.</p>

<ul>
  <li>this using backprop <strong>does not work</strong>, because $\vert \vert \delta^i\vert \vert _1=1$ makes this a discrete variable, which we cannot take derivative of.</li>
  <li>but since it is a latent variable, use <strong>EM algorithm</strong></li>
</ul>

<ol>
  <li><strong>E-step:</strong> Fill in the missing variable ($\delta$) by hallucinating (if at initialization) or <strong>estimating</strong> it by MLE (when you have some $f$)</li>
  <li><strong>M-step:</strong> Fit the model with known latent variable ($\delta$), and do backpropagation on $f$ to <strong>maximize</strong> the parameters for $f$.</li>
  <li>repeat</li>
</ol>

<p>where essentially it solves the loop by “hallucinating”:</p>

<ul>
  <li>to solve/optimize for $f$, we need $\delta$; but to solve/optimize for $\delta$, we need $f$.</li>
  <li>therefore, we just assume/hallucinate some $\delta$ to start with, then iteratively update</li>
</ul>

<hr />

<p>Examples: Then we can use this to do <strong>action prediction</strong>, with $k=3$ and predicting <strong>four features</strong> (handshake, high five, hug, kiss):</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304122557762.png" alt="image-20220304122557762" style="zoom:50%;" /></p>

<p>For prediction, we then use $\delta^i$ to tell which future is taking place, and then spit out the feature that has the highest score as the prediction.</p>

<hr />

<p>Another idea is that, since someimtes we have <strong>uncertainty</strong> in actions (even if we do it by ourselves)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311102448304.png" alt="image-20220311102448304" style="zoom:50%;" /></p>

<h2 id="predicting-in-eucliean-space">Predicting in Eucliean Space</h2>

<p>Last time we saw that the objective we used results in the problem of <strong>regression to the mean</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311102641366.png" alt="image-20220311102641366" style="zoom:50%;" /></p>

<p>where basically</p>

<ul>
  <li>you imagine the four possible futures, indicated by the three black points and the blue point
    <ul>
      <li>the “possible futures” are obtained by having <em>similar videos</em> and claiming their “past” are the same even though there are some variations</li>
    </ul>
  </li>
  <li>one idea of how we “fix” this is to represent this perhaps <strong>not in the input feature space</strong></li>
</ul>

<p>First, we need to recap what properties eucliean geometry have.</p>

<h3 id="hyperbolic-geometry">Hyperbolic Geometry</h3>

<p><strong>Axioms of Eucliean Geometry</strong>: (i.e. we can derive all euclidean stuff from those five axioms)</p>

<ol>
  <li>
    <p>There is one and only one line segment between any two given points.</p>
  </li>
  <li>
    <p>Any line segment can be extended continuously to a line.</p>
  </li>
  <li>
    <p>There is one and only one circle with any given center and any given radius.</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311103030751.png" alt="image-20220311103030751" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>All right angles are congruent to one another.</p>
  </li>
  <li>
    <p>Given any straight line and a point not on it, there exists one and only one straight line which passes through that point and never intersects the first line.</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311103146597.png" alt="image-20220311103146597" style="zoom:33%;" /></p>

    <p>basically related to what it means being parallel.</p>
  </li>
</ol>

<p>For <strong>hyperbolic geometry</strong>, we only chage the fifth rule and we will have a different geometry:</p>

<ul>
  <li>Given any straight line and a point not on it, there exists <del>one and only one</del> <strong>infinitely many</strong> straight line which passes through that point and never intersects the first line.</li>
</ul>

<p>Some graphical comparision would be</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311103658412.png" alt="image-20220311103658412" style="zoom: 33%;" /></p>

<p>where</p>

<ul>
  <li>the plus sign represents the origin.</li>
  <li>for hyperbolic space, the <strong>infinity of the space is the circular boundary</strong></li>
  <li>the straight line in hyperbolic space is drawn by doing the <strong>shortest path in the manifold</strong> (see below).
    <ul>
      <li>This line is also called the <strong>geodesic line</strong>, which in cartesian would be a straight line.</li>
      <li>one intition here is that the <em>density of space</em> is high near the boundary of the hyperbolic space.</li>
    </ul>
  </li>
</ul>

<p>All the points live oin a manifold, where the manifold is the <strong>hyperbolic surface</strong> in this case (the blue region above, generated by rotating a hyperbole)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311104354996.png" alt="image-20220311104354996" style="zoom:50%;" /></p>

<p>Then the formula for <strong>distance</strong> between points on hyperole (the blue surface), becomes:</p>

\[d(a,b) = \cosh^{-1}\left( 1+ 2 \frac{||a-b||^2}{(1-||a||^2)(1-||b||^2)} \right)\]

<p>for $a,b$ being vectors to the points. Some other properties of space include:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Shapes in Hyperbolic Space</th>
      <th style="text-align: center">Center of Circles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311105312495.png" alt="image-20220311105312495" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311105335099.png" alt="image-20220311105335099" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>on the left, it is significant as the area of triangle will be <strong>solely determined</strong> from angles. And the shape of “square” does not exist (though there exists four sides shapes)</li>
  <li>on the right, the <strong>center of circle</strong> shears more towrads the boundart, because the density is higher near boundary (i.e. the red curves, technically it sohuld be, should have the same length!)</li>
</ul>

<p>Additionally, you can also <strong>find the mean</strong> (which now relates to regression!)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311105058830.png" alt="image-20220311105058830" style="zoom: 33%;" /></p>

<h3 id="distortoin-of-space">Distortoin of Space</h3>

<p>Why do we want to use eucliean space? We want to <strong>embed a hierarchy tree in to the space</strong>.</p>

<blockquote>
  <p>I want <strong>distance defined by a line joining the nodes</strong> should be the <strong>sum</strong> of distancce between between node-node in the tree.</p>
</blockquote>

<p>Consider doing this in eucliean space, this does not work and we have <strong>distortion</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311105908378.png" alt="image-20220311105908378" style="zoom: 50%;" /></p>

<p>where this comes from $2=1+1$ is the correct distance we want, and $\sqrt{3}$ is the actual distance we got.</p>

<p>Yet, <strong>hyperbolic spaces can naturally embed trees</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Trees in Hyperbolic Space</th>
      <th style="text-align: center">Example</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311110044551.png" alt="image-20220311110044551" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311110231218.png" alt="image-20220311110231218" style="zoom:50%;" /></td>
      <td><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311110353370.png" alt="image-20220311110353370" style="zoom: 67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where the</p>

<ul>
  <li>second figure shows an example of “straight line”/shortest path that defines the distance between the two nodes.</li>
  <li>third figure shows bats that have the same area in hyperbolic space</li>
</ul>

<h3 id="predicting-in-hyperbolic-space">Predicting in Hyperbolic Space</h3>

<p>Then we consider <strong>4 possible futures</strong>, shown as the three black points and a single blue point. Our task is to predict $\hat{z}$ given the three past images, and the 4 true labels such as $f_\theta(\text{past images}) = \hat{z}$ represents the <strong>mean</strong> of the future = minimize the distance to the all the possible futures:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Regressoin Task</th>
      <th style="text-align: center">Interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311110641694.png" alt="image-20220311110641694" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311110716560.png" alt="image-20220311110716560" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>regression to mean in hyperbolic space means having the point $\hat{z}$ which is <strong>closer to origin</strong>, which corresponds to <strong>uncertainty in or prediction</strong> being in higher parts in the hierarchy tree!</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311102448304.png" alt="image-20220311102448304" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>Then, the objective function would be defined by <strong>regression using hyperbolic distance</strong></p>

\[\min \sum_i\left[ d^2 (\hat{z}_i,z_i) + \log \sum_j \exp (-d^2 (\hat{z}_i,z_j)) \right]\]

    <p>such that we essentially have <strong>two neurnets</strong>, $z_i$ from the blue neural net and the $\hat{z}_i$ from red for future prediction:</p>

    <ul>
      <li>the first term minimizes the distance between $z_i$ and $\hat{z}_i$, for $z_i$ being the one past, and $\hat{z}_i$ being its future
        <ul>
          <li>technically we are predicting one $\hat{z}_i$ per past, but eventually we converge to the same future $\hat{z}$ if the past are similar</li>
        </ul>
      </li>
      <li>the second term wants $\hat{z}_i$ to be far away from other <strong>non-related</strong> examples $z_j$ in the dataset (without this term $z,\hat{z}$ collapse to origin)</li>
    </ul>

    <p>Graphically:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311111152350.png" alt="image-20220311111152350" style="zoom:50%;" /></p>

    <p>where the blue latent point can be interpreted as “what features in the future image”</p>
  </li>
</ul>

<p>Last but not least, given those points in the latent space, you finally <strong>map it back to features</strong> such as “probability of hugging”, and etc:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311113534102.png" alt="image-20220311113534102" style="zoom:50%;" /></p>

<p>where the classifer you attached from the output of latent space vector $z$ could be a linear one in hyperbolic space.</p>

<p><strong>Predicing Action</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311114520145.png" alt="image-20220311114520145" style="zoom:30%;" /></p>

<p>notice that:</p>

<ul>
  <li>essentially as more future is revealed, the less uncertainty you have by <strong>moving down the action hierarchy tree</strong></li>
  <li>the purple dash lines would represent the levels of the tree you are at</li>
</ul>

<h2 id="action-regression">Action Regression</h2>

<p>Other related applications include <strong>regression on actions</strong> to predict a score.</p>

<p>For example: <strong>How well are they diving</strong>?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311122117279.png" alt="image-20220311122117279" style="zoom: 33%;" /></p>

<ol>
  <li>Track and compute human pose</li>
  <li>Extract temporal features
    <ul>
      <li>normalize pose</li>
      <li>convert to frequency space</li>
      <li>use histogram as descriptor</li>
    </ul>
  </li>
  <li>Train regression model to predict expert quality score</li>
</ol>

<p>Additionally, this can also be applied <strong>reversely</strong> by answering the question: how should the post change <strong>to get a higher score</strong>?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220304123521508.png" alt="image-20220304123521508" style="zoom: 33%;" /></p>

<p>where</p>

<ul>
  <li>essentially compute gradients</li>
</ul>

<h1 id="object-tracking">Object Tracking</h1>

<p>The first and foremost <strong>useful representation of motion</strong> is the <strong>optical flow</strong>.</p>

<blockquote>
  <p><strong>Optical flow field</strong>: assign a flow vector to each pixel</p>
</blockquote>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311122322202.png" alt="image-20220311122322202" style="zoom:50%;" /></p>

<p>However, there is a problem with computing optical flow, e.g:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Start</th>
      <th style="text-align: center">End</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311122741107.png" alt="image-20220311122741107" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311122736581.png" alt="image-20220311122736581" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>which is <strong>ambiguous</strong> how the line moved, as it could have go up/right/top right, all yielding the same result.</p>

<ul>
  <li>another example would bte the barber pole illusion, where</li>
  <li>e.g. if you put an aperture near the car, then how it moves become ambiguous. Hence this where machine learning becomes useful, which <strong>can learn the priors</strong>. But the problem is where can we get the correct labels if we have those ambiguities?</li>
</ul>

<h2 id="learning-optic-flow">Learning Optic Flow</h2>

<p>The idea is to training use <strong>game engines</strong>, so that we can:</p>

<ul>
  <li>generate dataset with labelled/ground truth optic flow using game engines</li>
</ul>

<p>An example dataset that comes out for this is the <strong>falling chairs</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311123133511.png" alt="image-20220311123133511" style="zoom: 33%;" /></p>

<p>And one model that worded well is the <strong>EpicFlow</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311123516050.png" alt="image-20220311123516050" style="zoom: 50%;" /></p>

<p>The general setup would looklike:</p>

<ul>
  <li>input image pairs, output <strong>which pixel moves to where</strong> (i.e. flow vector for each pixel)</li>
  <li>sample architecture with CNN looks like</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325143331339.png" alt="image-20220325143331339" style="zoom:67%;" /></p>

<p>Then this can be used to <strong>to predict motion</strong> by using the motion field</p>

<ul>
  <li>
    <p><strong>Motion Magnification</strong>: since machines can see more subtle motions, we can create videos with those magnified</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220311123753885.png" alt="image-20220311123753885" style="zoom: 67%;" /></p>

    <ol>
      <li>find the motion field</li>
      <li>cluster similar trajectores</li>
      <li>magnify the motion</li>
    </ol>
  </li>
</ul>

<h2 id="tracking-dynamics">Tracking Dynamics</h2>

<blockquote>
  <p>Moving from knowing how each pixel is moving, we would like to consider <strong>how each object is moving</strong>. Hence we end up in the task of how to track an object.</p>
</blockquote>

<p>When tracking an object, we generally consider how to answer the following two questions:</p>

<ul>
  <li><strong>common fate:</strong> connected parts so that they should move together</li>
  <li><strong>correpondance:</strong> how do you know those are the same thing after some time?</li>
</ul>

<p>Example:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325115735421.png" alt="image-20220325115735421" style="zoom:50%;" /></p>

<p>The common approach is to solve this by learning an optical flow field using supervised approach. Similar to how we learnt optimal flow:</p>

<ul>
  <li>given some input video with ground truth labelled object trajectory, for instance</li>
  <li>learnt the tracking</li>
</ul>

<p>Then you would end up using similar architecture for learning optical flow. For instance:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325143542294.png" alt="image-20220325143542294" style="zoom:67%;" /></p>

<p>while this does work great, but the problem would be <strong>collecting those labeled data</strong>, and that:</p>

<blockquote>
  <p>Is there an approach where we can solve this <strong>without</strong> having a <strong>supervised</strong> approach? It feels that every living being in existence should be able to track without a “teacher”.</p>

  <ul>
    <li>for most problems, if you have a big enough dataset, then they can usually be solved by many architectures</li>
    <li>can we come up with a <strong>unsupervised problem</strong> that <strong>tricks</strong> the machine and actually <strong>solves the actual problem</strong>?</li>
  </ul>
</blockquote>

<p>An example would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325120351845.png" alt="image-20220325120351845" style="zoom:33%;" /></p>

<p>where notice that to answer this question, you ==would have logically tracked the image==!</p>

<p>Then we can have a system such that, we are given a colored video:</p>

<ul>
  <li>only take the first image as colored</li>
  <li>the rest we process to grey scale and feed into network to <strong>predict color for each pixel</strong></li>
  <li>notice that we have all the labels already!</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325120556167.png" alt="image-20220325120556167" style="zoom: 50%;" /></p>

<p>note that this won’t solve the tracking problem conpletely, but is a good approach.</p>

<ul>
  <li>exceptions inclued an object changing color over time, perhaps due to lighting, e.g. at party house</li>
</ul>

<h3 id="human-perception-of-color">Human Perception of Color</h3>

<p>Recall that colors we perceive essentially is determined by <strong>wavelength in light</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325120951791.png" alt="image-20220325120951791" style="zoom:33%;" /></p>

<p>And we have in brain <strong>rods</strong> that perceive brightness and <strong>cones</strong> that perceive those colors</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Cones and Rods in Human</th>
      <th style="text-align: center">Absorbance Spectrum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325121214108.png" alt="image-20220325121214108" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325121307736.png" alt="image-20220325121307736" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where in human,</p>

<ul>
  <li>we have only <strong>three types of cones</strong>: one for <strong>blue</strong>, one for <strong>green</strong>, and one for <strong>red</strong>. But combinatinos of the three gives us perception of a spectrum of colors. This is also why we have RGB scale in computer images.</li>
  <li>we have only few cones in periphery, so we are actually not that good at detecting colors at periphery</li>
</ul>

<p>Then from this, you also get modern applications in how to arrive at different colors:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Additive</th>
      <th style="text-align: center">Subtractive</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325144147081.png" alt="image-20220325144147081" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325144152475.png" alt="image-20220325144152475" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>additive color mixing</strong>: adding RGB to get more colors</li>
  <li><strong>subtractive color mixing</strong>: multiplying/intersection of color</li>
</ul>

<p>And we have different representation of <strong>color spaces</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">RGB</th>
      <th style="text-align: center">HSV</th>
      <th style="text-align: center">Lab Space</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122019275.png" alt="image-20220325122019275" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122027420.png" alt="image-20220325122027420" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122105806.png" alt="image-20220325122105806" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>HSV: hue saturation value
    <ul>
      <li>notice that we get an illusino of magenta which comes from mixing of red and blue, which if you look at the wavelength scale, it should not happen</li>
    </ul>
  </li>
  <li>$L$ in lab space means intensity. This is a non-Euclidean space that seems to correspond the best with human vision (the idea is color spectrum could be a function of intensity as well)
    <ul>
      <li>so essentially $L,a,b$ would be the values for color</li>
      <li>in practice $L$ is often represented as the pixel value when in <strong>grey</strong> scale</li>
    </ul>
  </li>
</ul>

<p>Then using Lab space could be used very commonly in for the task of <strong>image colorization</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122443381.png" alt="image-20220325122443381" style="zoom:50%;" /></p>

<p>where the:</p>

<ul>
  <li>
    <p>the grey scale image could ==already be the $L$ values==</p>
  </li>
  <li>
    <p>then the task is just to predict $a,b$ values of the lab</p>
  </li>
</ul>

<p>We can also only look at the predicted $a,b$ values:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122625600.png" alt="image-20220325122625600" style="zoom:50%;" /></p>

<p>But since we are learning via regression, we could have <strong>averaging problem</strong> where if we have red/blue/green birds, then</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Given Data</th>
      <th style="text-align: center">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122712369.png" alt="image-20220325122712369" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122653940.png" alt="image-20220325122653940" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>One way to deal with it is to <strong>predict a distribution of discrete colors</strong>, so that we ==allow for more than one answer!==</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325122758695.png" alt="image-20220325122758695" style="zoom:50%;" /></p>

<p>then basically we can output a distribution of possible for color for each pixel.</p>

<p>But still this type of model still have problems in biases:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Training Data</th>
      <th style="text-align: center">Input</th>
      <th style="text-align: center">Color Prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325144440486.png" alt="image-20220325144440486" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325144450354.png" alt="image-20220325144450354" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325144458924.png" alt="image-20220325144458924" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>because many training data had dogs sticking tongues out, it paints a tongue as well on the input</li>
</ul>

<h3 id="color-mapping-for-tracking">Color Mapping for Tracking</h3>

<p>For image colorization, we ask the question:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325163554005.png" alt="image-20220325163554005" style="zoom:33%;" /></p>

<p>However, in video, recall that we would want to consider <strong>coloring for the hidden purpose of tracking</strong>. Hence your question would be:</p>

<blockquote>
  <p>Where should I <strong>copy</strong> this color <strong>from</strong>?</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Learning Task</th>
      <th style="text-align: center">Label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325123300413.png" alt="image-20220325123300413" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325123321431.png" alt="image-20220325123321431" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where notice that the solution to this colorization problem is tracking (hence we achieve our goal)</p>

<ul>
  <li>we do not want to say that all objects of the same color are the same object, which is kind of what image colorization do</li>
  <li>here we learn color for tracking, hence this reformulation.</li>
</ul>

<blockquote>
  <p>How do we color the video <strong>such that it learns where to map</strong>?</p>

  <ul>
    <li>essentially what the NN <strong>learn</strong> is a <strong>pointer</strong>, but the <strong>loss</strong> is on the <strong>color</strong></li>
  </ul>
</blockquote>

<p>For <strong>each pixel, we have some embedding</strong>.</p>

<ul>
  <li>$i,j$ would represent the location of the pixel in each image</li>
  <li>for every pixel $i$ in frame 1, we want to know how <strong>similar is it (i.e. if same object)</strong> to pixel $j$ in frame 2, e.g. at a later time.</li>
  <li>Hence we get a matrix $A_{ij}$ for measuring similarity between every pair of pixel</li>
  <li>then, we want to <strong>assign same color to “similar” pixels</strong> by having a weighted sum</li>
</ul>

<blockquote>
  <p>Therefore, the <strong>whether if a pointer exist</strong> between pixel $i$ and $j$ would be represented by <strong>similarity between $f_i$ and $f_j$</strong>.</p>
</blockquote>

<p>Graphically, we are doing:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325123849784.png" alt="image-20220325123849784" style="zoom:67%;" /></p>

<p>In more details: given color $c_i$ from reference and (learnt) embedding $f_i$ from refernce, and a input to predict, what is the color at each position $j$? We do this by:</p>

\[\hat{c}_j = \sum_i A_{ij}c_i,\quad A_{ij} = \frac{\exp(f_i^T f_j)}{\sum_{k} \exp(f_k^T f_j)}\]

<p>essentially a ==weighted sum based on similarity== of the embedding of each pixel. (note the <strong>analogy to self-attention mechanisms</strong>)</p>

<p>Then since we have the label already:</p>

\[\min_\theta \mathcal{L}\left( c_j, \hat{c}_j | f_\theta \right) = \min_\theta \mathcal{L}\left( c_j, \sum_{i}A_{ij}c_i \,\, |f_\theta \right)\]

<p>so that</p>

<ul>
  <li>for a particular video, our NN would be able to produce a pixel-wise embedding $f$ from its learnt parameters $\theta$</li>
  <li>once we have the embedding, we can color the image <strong>or we find object correspondance</strong> hence ==tracking== by measuing similarity between $f_i,f_j$ between any two locations of between two frames!</li>
</ul>

<hr />

<p><em>Example</em>: using it to predict color</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325124856500.png" alt="image-20220325124856500" style="zoom:50%;" /></p>

<p>which <strong>implicitly learns object tracking</strong>. Therefore, if you need tracking information, you just keep a pointer by:</p>

<ul>
  <li>
    <p>compute the $\arg\max_{i} f_i^T f_j$ so we know which pixel $i$ the pixel $j$ corresponded to</p>
  </li>
  <li>
    <p>then convert an entire group of it as a <strong>mask</strong></p>
  </li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401102559353.png" alt="image-20220401102559353" style="zoom: 50%;" /></p>

<p>and let the mask propagate in your network to do other things. Some more result examples</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Tracking Segments</th>
      <th style="text-align: center">Tracking Poses</th>
      <th style="text-align: center">Visualization of Embeddings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401103008266.png" alt="image-20220401103008266" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401103104761.png" alt="image-20220401103104761" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401103257846.png" alt="image-20220401103257846" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li>embedding in the third example refers to the $f_i$ for each pixel. Since $f_i$ is high dimensional, we needed to use PCA to reduce it to 3 dimension to superimpose on the original image. Note that this could also be useful for drawing a segmentation for objects in a video.</li>
  <li>note that the above notion of $\arg\max_{i} f_i^T f_j$ makes sense as the colors we found is dependent on the <em>similarity</em> between $f_i$ in input/reference image and $f_j$ of another frame</li>
</ul>

<h1 id="interpretability">Interpretability</h1>

<p>How to interpret deep learning architectures? Consider the simple example of</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325102602815.png" alt="image-20220325102602815" style="zoom:50%;" /></p>

<blockquote>
  <p><strong>What</strong> are neurons in the network learning? What should it learn?</p>

  <ul>
    <li>those techniques below could also be useful for debugging your model.</li>
  </ul>
</blockquote>

<p>This is an important chapter that covers many common technique used in real life to visualize what is happening in your model.</p>

<h2 id="grandmother-neurons-in-human">Grandmother Neurons in Human</h2>

<p>It turns out that research shows there are <strong>specific neuron</strong> in your brain that <strong>represents your grandmother</strong>, a neuron in your brain that represents your friends, etc.</p>

<ul>
  <li>done by inserting electrodes into brain and letting patients look at certain images. Hence recording neuron activities.</li>
  <li>recall that brain sends electrical signal around. Here it is sticked in visual system, so it responds to what people see and activates certain neurons.</li>
</ul>

<p>When flashing pictures of celebrities, there are neurons that would <strong>only fire for them:</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325103223030.png" alt="image-20220325103223030" style="zoom:50%;" /></p>

<p>where we see there are high activations for only a few neurons.</p>

<p>More interestingly, they are firing for the <strong>concept of a person</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325103310014.png" alt="image-20220325103310014" style="zoom:50%;" /></p>

<p>so that it also fires for things like “sketches” Halle Barry.</p>

<ul>
  <li>but the question is, if I take out that neuron, would I forget about Helle Barry? It is highly plausible that there would be redundancies in brain so that we don’t forget easily.</li>
  <li>but still the concept of a <strong>few/specific neurons being able to <em>fire/activate</em> for a certain class</strong> is important.</li>
</ul>

<blockquote>
  <p><strong>A grandmother neuron</strong> is a <em>neuron</em> that responds selectively to an activation of a high level concept corresponding with an idea in the mind.</p>
</blockquote>

<h2 id="deep-net-electrophysiology">Deep Net Electrophysiology</h2>

<p>Following from the above search, this <strong>hint on one way how we can interpret deep learning networks</strong>, by looking at what kind of image patch would cause the neuron to fire.</p>

<ul>
  <li>other interpretation methods include <a href="#Similarity Analysis">Similarity Analysis</a>, <a href="#Saliency by Occlusion">Saliency by Occlusion</a>, etc.</li>
</ul>

<p>First, we consider the activation values for each neuron:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325104128815.png" alt="image-20220325104128815" style="zoom:50%;" /></p>

<p>then you can also get a graph like the above for a certain layer.</p>

<p>A more detailed example is <strong>visualizing the CNNs</strong>. Here we have each layer being a bunch of Convolutions, and we treat the <strong>kernel</strong> as neurons.</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325104337993.png" alt="image-20220325104337993" style="zoom: 33%;" /></p>

<p>where essentially we ==record what image batches activate the first layer most strongly==, and it seems that we are detecting <strong>edges</strong>. If you also do it for layer 2 in the network:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325104507636.png" alt="image-20220325104507636" style="zoom: 33%;" /></p>

<p>where it seems that those neurons are firing for <strong>patterns/shapes</strong>, and finally at layer 3:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325104607757.png" alt="image-20220325104607757" style="zoom: 33%;" /></p>

<p>where here we seem to be able to put shapes together and <strong>detect objects</strong>!</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325104851838.png" alt="image-20220325104851838" style="zoom:33%;" /></p>

<p>and etc. But notice that the image activated are axis aligned</p>

<blockquote>
  <p>Since <strong>rotations are linear transformation</strong>, then we should imagine that to not change any information hence learnt representation should have an <strong>arbitrary aligned axis</strong>?</p>

  <ul>
    <li>rotation can be performed by a linear transformation, so then a NN could have rotated and those representations. Then why are we still have the vertical alignment for maximal activation? i.e. the activation is <strong>lower if we rotated the image</strong>, which shouldn’t happen.</li>
  </ul>

  <p>Therefore, this also motivates another view that instead of having a grandmother neuron specialized for a concept, could it be that we have a ==distributed view of a concept across neurons==, so that the combination gives us the classification?</p>

  <ul>
    <li>then we can perhaps recover the extra degree of freedom carried in by transformation such as rotation?</li>
  </ul>
</blockquote>

<p>In summary, it seems that CNNs <em>learned</em> the classical visual recognition pipeline</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325105106707.png" alt="image-20220325105106707" style="zoom: 40%;" /></p>

<p>We can also quantity this at each level:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325105921032.png" alt="image-20220325105921032" style="zoom:50%;" /></p>

<p>where vertical is percent of neurons that activated when pictures described in the title is fed in. So here we see that:</p>

<ul>
  <li>the deep layer we are in the model, the more higher layer concepts we are leanring.</li>
</ul>

<h2 id="similarity-analysis">Similarity Analysis</h2>

<p>Then if we take the embedding vector/hidden state of those images, we can also <strong>compare those vectors</strong> between images of different classes:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325110351678.png" alt="image-20220325110351678" style="zoom:33%;" /></p>

<p>where we expect that similar images should have similar representations. Then we can use this to conpare compare thi</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325110456297.png" alt="image-20220325110456297" style="zoom:50%;" /></p>

<p>where here we can see what <strong>DNN</strong> thinks are similar or different objects. The correspondence (left is from people) is high!</p>

<ul>
  <li>in some ways, this is surprising that machine is learning a similar way as human does</li>
  <li>but it could be reasoned that as <strong>humans are labelling</strong> those images. of course machines learnt a similar way.</li>
</ul>

<h2 id="saliency-by-occlusion">Saliency by Occlusion</h2>

<blockquote>
  <p><strong>What part of the image</strong> does the neural net make decisions on? Which part of the elephant did the neural net use to determine?</p>
</blockquote>

<p>One simple idea is to <strong>blocking of several regions in the image</strong>, and consider <strong>how much does the score go down</strong> when each region is blocked</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325110738672.png" alt="image-20220325110738672" style="zoom:50%;" /></p>

<p>Then doing it over all regions:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325110804959.png" alt="image-20220325110804959" style="zoom:50%;" /></p>

<p>where we can basically identify:</p>

<ul>
  <li>which part of the image blocked out, still has high confidence</li>
  <li>then the <strong>inverse</strong> of the number would represent <strong>importance</strong></li>
</ul>

<p>Another intuitive approach would be to answer the following question.</p>

<blockquote>
  <p>What is the <strong>maximum number of pixels I can mask out</strong> so that the machine can <strong>still classify t</strong>he image?</p>
</blockquote>

<p>An example of answering the above question would be:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325111031913.png" alt="image-20220325111031913" style="zoom:50%;" /></p>

<p>so in this case the neural net is not learning the correct thing.</p>

<h2 id="guided-backpropagation">Guided Backpropagation</h2>

<blockquote>
  <p>What pixles can I <strong>twiddle such that the resulting clasification is no longer correct</strong>?</p>
</blockquote>

<p>Then this results in</p>

<blockquote>
  <p><strong>Guided backprop</strong>: Only propagate pixel if it has a positive gradient backwards, i.e. activation increases if this pixel changed. Truncate negative gradients to zero.</p>

  <ul>
    <li>the reason why we truncate negative gradients is because we want to find which regions cause the object/find <strong>causation</strong> relationship, not the regions that do not cause it.</li>
  </ul>
</blockquote>

<p>Visual examples of what we are doing:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325111510971.png" alt="image-20220325111510971" style="zoom: 50%;" /></p>

<p>Results:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325111810980.png" alt="image-20220325111810980" style="zoom: 50%;" /></p>

<p>where in this result, we are doing:</p>

<ol>
  <li>patches found using the “Grandmother” neuron procedure, i.e. maximum activating patches</li>
  <li>from those patches, we perform a guided backpropagation to know <strong>what aspects of those patches</strong> that caused the maximum activation</li>
</ol>

<p>You could also do only a guided backprop on the whole picture.</p>

<h2 id="gradient-ascentadversarial-attack">Gradient Ascent/Adversarial Attack</h2>

<blockquote>
  <p>Given a trained model, <strong>what image</strong> does the network think is the <strong>most representative/likely</strong> of class $k$?</p>
</blockquote>

<p>Then we consider:</p>

\[\max_x f_{\text{neuorn}_i}(x) - \lambda ||x||^2_2\]

<p>where $f$ would be the activation function for each neuron</p>

<ul>
  <li>
    <p>$x$ would be input to each neuron, which <strong>corresponds to certain pixles of the image</strong></p>
  </li>
  <li>
    <p>the regularizatoin is needed so that $x$ would be at least in the visible range, as otherwise we can go towards infinity</p>
  </li>
</ul>

<p>Then eventually we do a gradient ascent to find the “best representation for each class”. Results look like:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325112333762.png" alt="image-20220325112333762" style="zoom: 67%;" /></p>

<p>Then the “fun” things people could do is that we can try to modify an image such that <strong>some class $k$</strong> would be activated for a neuron:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Original Image</th>
      <th style="text-align: center">Modified Image using Gradient Ascent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325112749164.png" alt="image-20220325112749164" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220325112739789.png" alt="image-20220325112739789" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where in the right we are modifying images so that the model would have triggered activations of many classes you like.</p>

<h1 id="self-supervised-learning">Self-Supervised Learning</h1>

<p>One example we have seen before would be how to use color for tracking, which turned the task into a self-supervised/unsupervised task. Here we see some other <strong>generic unsupervised methods</strong> used for downstream tasks.</p>

<ul>
  <li>
    <p>such as unsupervised segmentation $\to$ object detection.</p>
  </li>
  <li>
    <p>e.g. representations learnt can then be used for <strong>clustering</strong>. We can use the learnt $h=z$ hidden vector for k-means</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401105617067.png" alt="image-20220401105617067" style="zoom:33%;" /></p>
  </li>
</ul>

<p>One simple architecture used would be similar to the process of <em>fine-tune a pretrained model</em>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401170549357.png" alt="image-20220401170549357" style="zoom: 80%;" /></p>

<p>where the key point is that <strong>finetuning</strong> starts with <strong>some representation learnt from a previous task</strong> hence:</p>

<ul>
  <li>we aim to construct a network that can ==learn useful representation $h$== of images $x$ in an ==unsupervised way==</li>
  <li>then use that representation $h$ as a “pretrained network” for fine-tuning on other tasks</li>
</ul>

<p>hence here we are mostly concerned with:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">General Self-Supervised</th>
      <th style="text-align: center">Self-Supervised Representation Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401105335455.png" alt="image-20220401105335455" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401105402483.png" alt="image-20220401105402483" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<hr />

<blockquote>
  <p><em>Why is having some representation $h$ useful</em>?</p>
</blockquote>

<p>Consider the example of remembering the observed image and then drawing from scratch</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401105209243.png" alt="image-20220401105209243" style="zoom:33%;" /></p>

<p>notice that:</p>

<ul>
  <li>when most people draw it, we automatically <strong>extrapolated</strong>: we drew the entire rubbish bin when we only observed part of it</li>
  <li>the same happened for videos, when we are only show part of a video and were asked to describe it, we extrapolate unseen scenes.</li>
</ul>

<blockquote>
  <p>Our mind is constantly predicting and extrapolating. Self-supervised learning aim is to be able to <strong>extrapolate information/representation from the given data</strong>.</p>
</blockquote>

<h2 id="common-self-supervised-tasks">Common Self-Supervised Tasks</h2>

<p>How do we get that representation $z$ or $h$? Here we will present a few:</p>

<ul>
  <li>find a low dimension $h$ such that <strong>reconstruction</strong> is the best: <strong>autoencoder</strong></li>
  <li>find a network $f_\theta$ that outputs representation of both <strong>image and audio</strong> of the same video, and maximize <strong>correlation</strong></li>
  <li>find a network $f_\theta$ that outputs representation for <strong>context prediction</strong>, i.e. predicting relative location of patches of an image</li>
  <li>find a network $f_\theta$ that outputs representation that <strong>can be added</strong>, i.e. sum of representation of parts of an image = representation of an image</li>
  <li>find a network $f_\theta$ that outputs representation such that <strong>similar objects in a video have a similar representation</strong></li>
</ul>

<h3 id="using-autoencoder">Using Autoencoder</h3>

<p>One self-supervised task is to use an autoencoder to learnt $z$ for <strong>reconstruction</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401105723442.png" alt="image-20220401105723442" style="zoom: 33%;" /></p>

<p>where the loss would be reconstruction loss $\mathbb{E}[f_\theta(x)-x]$. of course you want to make the dimension of $z$ much smaller than the dimension of the image. So you <strong>want</strong> the representation to be reflective of the object</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401184538778.png" alt="image-20220401184538778" style="zoom: 33%;" /></p>

<p>e.g. we hope that $z$ can compress and learn “<em>face of fish is yellow</em>”, etc, but in reality is just learnt a <strong>down-sampled version</strong> of the image</p>

<h3 id="using-audio-data">Using Audio Data</h3>

<p>Another idea is <strong>correlate different views of the data</strong>, and hence predict “what sound it can produce” (this is actually one of the first self-supervised approach).</p>

<ul>
  <li>hence, rather than <em>compression</em>, this is about <em>prediction/extrapolation</em></li>
  <li>i.e. I know what a “<em>cow</em>” is because it can make a “<em>moo</em>” sound. Hence the representation should reflect the two</li>
</ul>

<p>For example, given an input video, it will have ==both sound and image== in the video:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401110730523.png" alt="image-20220401110730523" style="zoom:50%;" /></p>

<p>where our aim is to let the model be able to say that “<em>cow</em>” produced the sound “<em>moo</em>”. In fact, this idea itself to use <strong>different modality of the same data</strong> is common in self-supervised learning in CV (e.g. colorized image vs grey scale)</p>

<h3 id="using-context-prediction">Using Context Prediction</h3>

<p>we want to improve the autoencoder so that it does not just learn a down-sampled version. Consider solving the following problem</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401185242157.png" alt="image-20220401185242157" style="zoom:33%;" /></p>

<p>notice that to solve it, we <strong>needed to know how a cat looks like</strong>.</p>

<p>Hence, we want to <strong>predict the spatial layout</strong> between the patches, which ==depends on learning some good representation $z$ of the object==:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401111011200.png" alt="image-20220401111011200" style="zoom:50%;" /></p>

<p>where remember that instead of just compressing the image. it should manage to learn “<em>what a cat looks like</em>” so be able to correctly place the missing patch:</p>

<ul>
  <li>note that then context prediction, the $Y$ we already know as we have the entire image. Then, since we only have 8 choices of where to place it, the loss would be cross-entropy loss</li>
  <li>similar for image colorization, the by-product of this would be a neural net that <strong>produced $n=4096$ vector $h$ which should be representative of the image patch</strong></li>
</ul>

<p>How do we visualize the embeddings $h$? One way is to do:</p>

<ol>
  <li>given an input patch $i$, produce an embedding $h$</li>
  <li>find <strong>nearest neighbors</strong> $j$ of the that embedding amongst the training data</li>
  <li>return that original image patch $j$</li>
</ol>

<p>Some examples:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401111523743.png" alt="image-20220401111523743" style="zoom:50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>the AlexNet representation also learnt the <em>color</em> of the wheels, but the new version learnt only the <em>wheel</em></li>
</ul>

<h3 id="using-counts">Using Counts</h3>

<p>Another idea is inspired from counting: i.e. the <strong>sum of the representations</strong> should recover the <strong>total representation</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401111929437.png" alt="image-20220401111929437" style="zoom:50%;" /></p>

<p>the intuition is we want to ==leverage recursive structure to images==:</p>

<ul>
  <li>consider the question being how many eyes there are in the last image</li>
  <li>it should be the same as the sum of number of eyes in those 4 patches</li>
</ul>

<p>Hence the architecture is</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401112043702.png" alt="image-20220401112043702" style="zoom: 67%;" /></p>

<p>where</p>

<ul>
  <li>
    <p>first we concatenate the four pieces into $t$</p>
  </li>
  <li>
    <p>you want the sum to be <strong>close to the original image</strong> $\vert d-t\vert ^2$ but <strong>far away from some other random image $-\vert e-t\vert ^2$</strong></p>
  </li>
</ul>

<p>This is very valuable in videos as we want tracking objects which can <em>easily deform</em>, e.g. a human doing parkour.</p>

<h3 id="using-tracked-videos">Using Tracked Videos</h3>

<p>We can use this idea of the <strong>same objects in a video over time</strong> should be <strong>close to each other in the embedding space</strong>, even if its shape could have deformed:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401112256329.png" alt="image-20220401112256329" style="zoom: 67%;" /></p>

<p>where</p>

<ul>
  <li>this of course requires an already labeled/tracked video</li>
  <li>again, we want same objects being close but different objects being far away</li>
</ul>

<h3 id="learnt-kernels-from-unsupervised-learning">Learnt Kernels from Unsupervised Learning</h3>

<p>In many of the above applications, we can <strong>visualize the kernels learnt</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401112531648.png" alt="image-20220401112531648" style="zoom:50%;" /></p>

<p>where notice that they ended up learning a <strong>very similar kernel than fine-tuned tasks which is a supervised version.</strong></p>

<h2 id="contrastive-learning">Contrastive Learning</h2>

<blockquote>
  <p>This is probably the most successful unsupervised learning method in CV to date.</p>
</blockquote>

<p>The idea is the following:</p>

<ul>
  <li>given an image, we can create its similar pair by transformation of itself such as rotation/cropping</li>
  <li>given an image, we can create negative pair/different pairs by transformation of other images</li>
  <li>given any of the two images above, we also want to encode it $h_i = f(x_i)$ using an encoder</li>
</ul>

<p>Then, we want to minimize the following loss:</p>

\[l(i,j) = -\log \frac{\exp(\text{sim}(z_i,z_j) / \tau)}{\sum_{k=1,k\neq j}^{2N}\exp(\text{sim}(z_i,z_k) / \tau)}\]

<p>essentially making sure that <strong>similar pairs score high</strong> (e.g. same labelled pair). Of course this can be extended to learn negative pairs as well (SupCon). Graphically:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401112712356.png" alt="image-20220401112712356" style="zoom:50%;" /></p>

<p>where:</p>

<ul>
  <li>
    <p>we want the network to <strong>agree</strong> that they are the same object/representation if it is just rotation/cropping of the object</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401192713906.png" alt="image-20220401192713906" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>the loss have $z_i,z_j$ being the same image, $z_k$ is iterating over all images. Hence we want the top to be as small as possible/close together, while the denominator we want to be large/far away</p>
  </li>
</ul>

<blockquote>
  <p>Notice that the <strong>loss</strong> is on <strong>another representation</strong> $z$ rather than $h$ which is the representation we use. The idea is that $z$ vector might only be storing the minimal sufficient part of the image for maximal agreement.</p>

  <p>However, this does perturn the objective of “ensuring $h$ representation is good” as ensuring $z$ matching might not be enough. But empirically it works.</p>
</blockquote>

<p>Finally, when training is done, we can take that $h_i$ for each input image $x_i$ and plot them (not on this dataset, just for example)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">SimCLR</th>
      <th style="text-align: center">SupCon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401192628389.png" alt="image-20220401192628389" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401192650353.png" alt="image-20220401192650353" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>so that essentially close together pairs seems to be clustered.</p>

<p>This is very as it even beats some of the supervised version:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401113024811.png" alt="image-20220401113024811" style="zoom: 33%;" /></p>

<h2 id="causal-interpretation">Causal Interpretation</h2>

<p>see https://arxiv.org/pdf/2005.01856.pdf</p>

<h2 id="learning-visual-shortcuts">Learning Visual Shortcuts</h2>

<p>Whether if this is a good phenomenon or bad depends on how you use/see it. Consider the task of <strong>recovering the layout of an image</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401201219308.png" alt="image-20220401201219308" style="zoom:67%;" /></p>

<p>which works well as an unsupervised task. However, if we convert the image into grey scale, the <strong>same training image failed</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401201249403.png" alt="image-20220401201249403" style="zoom:67%;" /></p>

<p>why did that happen?</p>

<ul>
  <li>this is because there is chromatic aberration and vignetting in the inage, where <strong>neural network picked up</strong> as clues.</li>
  <li>however, those are ==hidden information relative to human eyes==</li>
</ul>

<h3 id="chromatic-aberration-and-vignetting">Chromatic Aberration and Vignetting</h3>

<p>Those two are come from the <strong>physics of lenses</strong>, essentially its property that refraction depends on the wavelength of the light.</p>

<blockquote>
  <p><strong>Chromatic aberration</strong>, also known as color fringing, is <strong>a color distortion that creates an outline of unwanted color along the edges of objects in a photograph</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Vignetting</strong> is a <strong>reduction of an image’s brightness</strong> or saturation <strong>toward the periphery</strong> compared to the image center.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Chromatic Aberration (exaggerated)</th>
      <th style="text-align: center">Vignetting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://upload.wikimedia.org/wikipedia/commons/6/66/Chromatic_aberration_%28comparison%29.jpg" alt="Chromatic aberration - Wikipedia" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401201852226.png" alt="image-20220401201852226" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Why did they happen?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401120052782.png" alt="image-20220401120052782" style="zoom:50%;" /></p>

<ul>
  <li>the fundamental problem is it is difficult to focus on <strong>all wavelength</strong> in the same manner using a lens</li>
  <li>so that some wavelength, e.g. purple, got focused better than other color, such as green, causing <strong>chromatic aberration</strong></li>
  <li>on the other hand, more light is going through on the center, hence in general you have brighter regions in the center -  cuasing <strong>vignetting</strong></li>
</ul>

<p>Graphically:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Causing Chromatic Aberration</th>
      <th style="text-align: center">Causing Vignetting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401202408887.png" alt="image-20220401202408887" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="https://s3.amazonaws.com/red_3/uploads/asset_image/image/520e788b26465a66a80000f5/lens-elements-v2.png" alt="Understanding Lens Vignetting" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="using-shortcuts">Using Shortcuts</h3>

<p>Hence NN utilizes those to do patch re-localization. Some plots of how those effects affect performance:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401120418806.png" alt="image-20220401120418806" style="zoom: 67%;" /></p>

<p>But you can also <strong>turn this as a good “feature” of the program</strong>, as chromatic aberration and vignetting is always present in photos, we can use it to ==detect if the photo is cropped/edited/etc==</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401120541177.png" alt="image-20220401120541177" style="zoom:50%;" /></p>

<p>where, for instance, if the photo is cropped, then the vignetting/chromatic aberration <em>center will be shifted</em>.</p>

<h2 id="to-reinforcement-learning">To Reinforcement Learning</h2>

<p>Self-supervised learning sounds like the “next stage” for supervised learning. But there could be more</p>

<p><strong>Kitten Carousel</strong>:</p>

<p>Consider the following experiment:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401120848391.png" alt="image-20220401120848391" style="zoom:50%;" /></p>

<ul>
  <li>take two cats born in darkness, and let them grow up in darkness</li>
  <li>the first cat $A$ can move around and see scenes</li>
  <li>the second cat $P$ can not move, but they are <em>seeing the same thing</em> as the contraption is symmetric</li>
  <li>then they did some IQ test on both cats afterwards, and it turns out that cat $A$ is smarter (though this result is very controversial)</li>
</ul>

<p>The upshot of this is that ML algorithms is essentially cat $P$, it is <strong>not interacting</strong> with the world, only <strong>learning from observations</strong>.</p>

<p>Hence then we get the field <strong>Reinforcement Learning</strong> becoming a very important field for building intelligence.</p>

<hr />

<p><strong>Yann LeCun’s cake</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401121518887.png" alt="image-20220401121518887" style="zoom:50%;" /></p>

<ol>
  <li>Cake is unsupervised representation learning (i.e. most of the math)</li>
  <li>Frosting is supervised transfer learning (we need a little bit of it to be interesting)</li>
  <li>Cherry on top is reinforcement learning (model-based RL)</li>
</ol>

<p>so that AI would work with just 1 and 2, but more intelligence needs interactions hence 3. But of course, the third step is <strong>expensive as it could have high stakes</strong>, i.e. if you make a mistake, people might get hurt.</p>

<h1 id="synthesis">Synthesis</h1>

<p>Before, all the tasks we had could be generalized to “how to process an image”.</p>

<blockquote>
  <p>The goal of synthesis is “<strong>how to create an image</strong>” (either generate from scratch/random noise or manipulating existing ones)</p>
</blockquote>

<p><em>Some history of photographs:</em></p>

<p>In 1888, when the first camera is created, <strong>photos were proofs</strong>. However, that only lasted 100 years:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220401123028005.png" alt="image-20220401123028005" style="zoom:50%;" /></p>

<p>which comes to the topic of <strong>how to synthesize images</strong>, e.g. DeepFakes.</p>

<hr />

<p>Since we need to create an image, we are moving from <strong>discriminative models to generative models</strong></p>

<ul>
  <li>
    <p><strong>Discriminative Models</strong> (what we had before, e.g. CNNs)</p>

    <ul>
      <li>Learn the linear/nonlinear boundary between classes</li>
      <li>==Estimates: $P(Y \vert  X)$ the posterior== (which essentially learns boundaries)</li>
    </ul>
  </li>
  <li>
    <p><strong>Generative Models</strong> (what we want now)</p>

    <ul>
      <li>Learn the distribution of the data</li>
      <li>Often you can sample from it</li>
      <li>==Estimates: $P(Y, X)$ the joint== (i.e. learn the data distribution, hence generate more data)</li>
    </ul>
  </li>
</ul>

<p>Examples of generative ones we will discuss include</p>

<ul>
  <li>GAN networks, e.g. trained on many dog samples, and synthesize a new dog, and OpenAI DallE 2</li>
  <li>Variational AutoEncoder</li>
  <li>etc.</li>
</ul>

<h2 id="view-of-generative-models">View of Generative Models</h2>

<p>Of course the aim of genenerative models is to directly learn the distribution $P(X,Y)$. But along with this goal, we need to make sure:</p>

<ul>
  <li>model being able to <em>inferencce</em>/genereate data within the distribution but outside of training data</li>
  <li>we also want to able to draw samples from it</li>
</ul>

<p>Hence this results in the following idea for generative models:</p>

<blockquote>
  <p>Given some known <strong>prior</strong> distribution $p(z)$, e.g. a Gaussian, ==learn a mapping== (e.g. done from neural network $G$) from $p(z)$ to the <strong>target</strong> joint distribution you want to learn.</p>
</blockquote>

<p>Visually, if we need to find a model $G$ such that it learns:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408103649368.png" alt="image-20220408103649368" style="zoom:33%;" /></p>

<p>so that for generation, then we just need to give a random input $z \in p(z)$ to $G$, and it will give us a synthesized output.</p>

<p>Then, in eventually you use a NN to model $G$, hence basically genreative models are doing the following:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408103821223.png" alt="image-20220408103821223" style="zoom:33%;" /></p>

<p>where after you learnt the parameters in $G$</p>

<ul>
  <li>$z$ input would be input from the prior $p(z)$ you specified in training</li>
  <li>$x$ output could be an image, for instance.</li>
</ul>

<blockquote>
  <p>But how do you know that the networks is <strong>not memorizing photos</strong>? i.e. memorizing $z \to x$ being an identity map?</p>

  <p>This is often resolved by the fact that you can move in latent space $p(z)$ and <strong>change attributes</strong> of a given $G(z)\to x$ such as camera view point. This shows that it can <strong>interpolate</strong> unseen images, hence not merely memorizing!</p>
</blockquote>

<p>Visually, again the aim of learning $p(x)$ from a <strong>finite set of training data</strong> is to that we can <strong>interpolate</strong> unseen images:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408110341483.png" alt="image-20220408110341483" style="zoom:33%;" /></p>

<p>where you can imagine the two black dots being the given <strong>training</strong> set, data along the line are <strong>interpolated</strong>.</p>

<p>Additionally, some research shows that, given a $z,G(x)$ pair, you can <strong>move along some specific direction</strong> (a basis for the Gassian) of the latent space $p(z)$, and obtain samples of data corresponds to <strong>changing in camera viewpoint</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Angle 1</th>
      <th style="text-align: center">Angle 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408184948123.png" alt="image-20220408184948123" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408185003734.png" alt="image-20220408185003734" /></td>
    </tr>
  </tbody>
</table>

<p>In fact, there is a class of GAN network that aims to find ways to, say, change the color only, change the view point only, and etc, which is called <strong>StyleGAN</strong>.</p>

<h2 id="generation-with-vae">Generation with VAE</h2>

<p>Recall that the classical autoencoder does compression:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408173539656.png" alt="image-20220408173539656" style="zoom:33%;" /></p>

<p>where your model $M$ would learn</p>

<ul>
  <li>an encoder that goes from $E(x) \to z$ being compressed</li>
  <li>a decoder that goes $G(z) \to x$ <em>seems to learn a mapping</em> from the red space to blue space</li>
</ul>

<p>But why does pure autoencoder not work? Because we technically still <strong>don’t know the red latent space</strong>, hence we cannot <strong>sample $z$</strong> from the latent space to ==generate some new data==. Therefore, instead of mapping to some random latent space, we can enforce it to map to a known, given prior distribution:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408104249814.png" alt="image-20220408104249814" style="zoom: 40%;" /></p>

<p>Therefore we consider <strong>Variational AutoEncoder</strong>:</p>

<ul>
  <li>we are learning a mapping from prior ==distribution== in red (e.g. Gaussian), which is <strong>parameterized distribution</strong> (so that we know how to sample from it once we know the parameters) to the target distribution</li>
  <li>then we can construct this problem as an autoencoder like problem, but $p(z)$ would now be parametrized</li>
  <li>with this learnt, we can sample from $p(z)$ and when $G(z)$ to output a new image/sample!</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Learning Time</th>
      <th style="text-align: center">Generation Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408181613086.png" alt="image-20220408181613086" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408181620927.png" alt="image-20220408181620927" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<hr />

<p>Formally, this is how it works. <strong>First</strong> we consider learing $G_\theta$ that maps from $z\sim p(z)$ to $x \sim p(x)$. Given $p(z)$ which is known:</p>

\[p_\theta(x) = \int p(x|z;\theta)p(z)dz\]

<p>then we need to ==figure out $p(x\vert z;\theta)$==, which is essentially given a $z$, how can we map it to a distribution?</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408180335516.png" alt="image-20220408180335516" style="zoom:50%;" /></p>

<p>Suppose we can <strong>approximate the target distribution $p$</strong> by assuming it to be a collection of priors so that</p>

\[p(x|z;\theta) \approx q(x|z;\theta)\sim \mathcal{N}(x;G_\theta^\mu(z),G_\theta^\sigma(z))\]

<p>so that eventually all $x \sim p(x)$ is <strong>approximated by</strong></p>

\[x = G_\theta^\mu(z)+G_\theta^\sigma(z)\cdot \epsilon\]

<p>for $\epsilon \sim \mathcal{N}(0,1)$.</p>

<blockquote>
  <p>So essentially, the network $G$ decoder has to learn only $\mu(z) =G_\theta^\mu(z),\sigma(z)=G_\theta^\sigma(z)$ when given some $z$.</p>
</blockquote>

<p>Then, together with the encoder, the architecture looks like:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Network</th>
      <th style="text-align: center">Abstraction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://miro.medium.com/max/1838/1*Q5dogodt3wzKKktE0v3dMQ@2x.png" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408174645583.png" alt="image-20220408174645583" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>
    <p>$KL(q(z)\vert \vert p(z))$ would correspond to the <strong>encoder</strong>, because we are trying to find out $q(z)$ that is close to $p(z)$</p>

    <ul>
      <li>represents <strong>encoding data from $x$ to latent variable $z$</strong></li>
      <li>hence, if going well, this means that the explanation of the data ($z \sim q(z)$) does not deviate from the prior beliefs $p(z)$ and is called the <strong>regularization term</strong></li>
    </ul>
  </li>
  <li>
    <p>$\mathbb{E}_{z \sim q(z)}[\log p(x\vert z)]$ would correspond to <strong>decoder</strong></p>

    <ul>
      <li>given some <strong>sampled $z \sim q(z)$</strong>, this is the log-likelihood of the observed data $x$ (i.e. $x_i := x$).</li>
      <li>Therefore, this measures how well the samples from $q(z)$ explain the data $x$, which can be seen as the <strong>reconstruction error</strong> to get $x$ back from an encoded latent variable $z$</li>
    </ul>
  </li>
</ul>

<p>Then the total task becomes learning $\theta, \phi$ by <strong>maximizing ELBO</strong>:</p>

\[\begin{align*}
\mathcal{L} 
&amp;= \int q(z)\log \frac{p(z,x)}{q(z)}dz \\
&amp;= \int q(z)\log p(x|z) dz - \int q(z) \log \frac{p(z)}{q(z)}dz\\
&amp;= \mathbb{E}_{z \sim q(z)}[\log p(x|z)] - KL(q(z)||p(z))
\end{align*}\]

<h2 id="generation-with-gan">Generation with GAN</h2>

<p>The basic idea is that you again, learn some mapping from $z \to x$ by $G(z)\approx x \sim p(x)$. However, do it in the following way</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Generator</th>
      <th style="text-align: center">Full Network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408182020803.png" alt="image-20220408182020803" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408182028786.png" alt="image-20220408182028786" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where $G(z)$ again <strong>learns a mapping</strong>, but we train this by the architecture on the right, so that the entire forward pipeline looks like:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408104514700.png" alt="image-20220408104514700" style="zoom: 50%;" /></p>

<ul>
  <li>A <strong>generator</strong> tries to <strong>learn the mapping</strong> from prior $p(z)$, e.g. a Gaussian, to the image distributions</li>
  <li>A <strong>discriminator</strong> tries to ==provide feedback== on how close $G(z)$ is to real sample $x$ it learnt</li>
  <li>then, if the discriminator $D$ learnt some feature (e.g. human have 2 eyes) and used this to tell $G(z)$ is fake, it can <strong>backpropagate this information to $G$</strong> so that $G$ can update and learn about the distribution $x \sim p(x)$</li>
</ul>

<p>So formally we want:</p>

<ul>
  <li>
    <p><strong>generator</strong> fool discriminator to say $D(G(z)) \to 1$ being real</p>

\[\min_G \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z))]\]
  </li>
  <li>
    <p><strong>discriminator</strong> being able to tell the difference and learn from $p_{data}$ so that $D(x)\to 1$ <strong>and</strong> $D(G(z)) \to 0$</p>

\[\max_D\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]
  </li>
</ul>

<p>Together, the loss for the whole network is therefore</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]

<p>where this is if $D(x)=1$ telling that it is real is a good thing. If you want $D(x)=1$ meaning $x$ is fake, then you would <strong>swapped</strong> to have</p>

\[\min_G\max_DV(D,G)=\mathbb{E}_{x \sim p_{data}(x)}[\log D(G(z))]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(x))]\]

<p>which is a <strong>minimax optimization</strong>.</p>

<hr />

<p>Common observation during training:</p>

<ul>
  <li><strong>at the start</strong>, you would initialize $D$ with random weights and its would have 0.5 performance.  And $G$ would initially generates random noise</li>
  <li><strong>then</strong> $D$ realize you have random noise, hence can discriminate. But this provides a <strong>gradient</strong>/signal to $G$
    <ul>
      <li>in other words, whatever rule $D$ learnt can be undone/backpropagated to $G$!</li>
      <li>e.g. $G$ generates person with 3 eyes, $D$ realize and learns how to count number of eyes in real images (2), and then $G$ realizes and updates.</li>
    </ul>
  </li>
  <li>then, $G$ fix that bug, and $D$ will need to <strong>learn a new features/rule from the $x \sim p(x)$</strong> that could discriminate.</li>
</ul>

<blockquote>
  <p><strong>Note that</strong> if $D$ is really good (e.g. pretrained on large task), then it ==might not work== to train a $G$. This is because:</p>

  <ul>
    <li>since $D$ needs to produce a probability, typically we have as a sigmoid behind it</li>
    <li>then, if $D$ is really good, it will always output values very close to $0,1$ as it is very confident.</li>
    <li>But this means that the <strong>gradient will vanish</strong> as gradients near the tails of sigmoid are minimal.</li>
  </ul>

  <p>Therefore, the above architecture/training only <strong>work</strong> if we have $D$ <strong>developing knowledge along</strong> with $G$.</p>
</blockquote>

<hr />

<p><em>Samples from BigGAN</em></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408110943956.png" alt="image-20220408110943956" style="zoom:50%;" /></p>

<h3 id="mode-collapse">Mode Collapse</h3>

<p>Common practical problems with training GANs include Mode Collapse and Convergence issues. Here we discuss <strong>mode collapse</strong>.</p>

<p>Recall that we wanted</p>

<ul>
  <li>
    <p><strong>generator</strong> fool discriminator to say $D(G(z)) \to 1$ being real</p>

\[\min_G \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z))]\]
  </li>
  <li>
    <p><strong>discriminator</strong> being able to tell the difference and learn from $p_{data}$ so that $D(x)\to 1$ <strong>and</strong> $D(G(z)) \to 0$</p>

\[\max_D\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\]
  </li>
</ul>

<p>But suppose ==$G(z)$ can generated a subset of $x$ being realistic==. So that it learnt</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408185418486.png" alt="image-20220408185418486" style="zoom: 33%;" /></p>

<p>e.g. suppose $p(x)$ are pictures of animals, but if $G$ generates <em>cats</em> that $D$ cannot tell, then it has no motivation to learn another mode</p>

<ul>
  <li>
    <p>i.e. there is no guarantee that the mapping covers the entire image space</p>
  </li>
  <li>
    <p>in theory, this should not happen because if $G$ only learns a subspace of the real images, then $D$ <strong>could be able to learn by memory</strong> the small set of images $G(z)$ returned, and hence get out of the pitfall. However, it still does happen and it is under active research.</p>
  </li>
</ul>

<h3 id="cycle-gan">Cycle GAN</h3>

<p>The idea of CycleGAN is to do <strong>style/domain/etc transfer</strong> between two classes using a GAN network:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Object 1 Domain $X$</th>
      <th style="text-align: center">Object 1 Domain $Y$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408200231919.png" alt="image-20220408200231919" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408200238650.png" alt="image-20220408200238650" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where essentially you want to learn:</p>

<ul>
  <li>a mapping from $X \to Y$</li>
  <li>a mapping from $Y \to X$</li>
</ul>

<p>So then you can consider having <strong>model being</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408200411713.png" alt="image-20220408200411713" style="zoom: 50%;" /></p>

<p>where you want:</p>

<ul>
  <li>
    <p>$G$ learns a mapping from $G(X) \to Y$</p>
  </li>
  <li>$F$ learns a mapping from $F(Y) \to X$</li>
  <li>$D_X$ discriminates between real $X$ and $F(Y)$</li>
  <li>$D_Y$ discriminates between real $Y$ and $F(X)$</li>
</ul>

<p>Then ==naively== you might write down:</p>

\[\mathcal{L}=\mathcal{L}_{\text{GAN}}(G,D_Y) + \mathcal{L}_{\text{GAN}}(F,D_X)\]

<p>where</p>

\[\mathcal{L}_{\text{GAN}}(F,D_X) =\mathbb{E}_{x \sim p_{data}(x)}[\log D_X(x)]+\mathbb{E}_{y \sim p_{data}(y)}[\log (1-D_X(F(y)))]\\
\mathcal{L}_{\text{GAN}}(G,D_Y) =\mathbb{E}_{y \sim p_{data}(y)}[\log D_Y(y)]+\mathbb{E}_{x \sim p_{data}(x)}[\log (1-D_Y(G(x)))]\]

<p>being the losses for normal GANs. However, this would not work as, for example, it <strong>does not require $G(x),x$ to be the same object</strong>, i.e. it only needs to learn realistic $F(y),G(x)$, but it could be of entire unrelated objects from $y,x$.</p>

<p>Therefore, the solution is to enforce ==cycle consistency== to ensure the transfer is done on the same object</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408201225287.png" alt="image-20220408201225287" style="zoom: 50%;" /></p>

<p>so that you have</p>

\[\mathcal{L}_{\text{cycle}} = \mathbb{E}_{x \sim p_{data}(x)}[||F(G(x))-x||_1] + \mathbb{E}_{y \sim p_{data}(y)}[||G(F(y))-y||_1]\]

<p>so that the final objective is</p>

\[\mathcal{L}=\mathcal{L}_{\text{GAN}}(G,D_Y) + \mathcal{L}_{\text{GAN}}(F,D_X) + \mathcal{L}_{\text{cycle}}(G,F)\]

<hr />

<p>Then for <strong>training</strong>, you would need to prepare paired $X,Y$ ready for transfer</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408201513984.png" alt="image-20220408201513984" style="zoom:50%;" /></p>

<p>and the trained network could be used for <strong>style transfers on test/new images</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408201611416.png" alt="image-20220408201611416" style="zoom: 50%;" /></p>

<h1 id="ethics-and-bias">Ethics and Bias</h1>

<blockquote>
  <p>This section will cover a series of real life scenarios where we caused problem due engineers not paying attention to <strong>bias/ethics</strong></p>
</blockquote>

<p>Note that a lot of the ideas/interpretations written in this section will be ==subjective==.</p>

<h2 id="unconscious-understanding">Unconscious Understanding</h2>

<blockquote>
  <p>It can be said that often those bias/ethical issues crawls in because we <strong>didn’t realize</strong> that it could be a problem. We often think we understand something, but in reality we might not.</p>
</blockquote>

<p>Consider the question of  understanding <strong>how we see</strong>. In the past, people believed that vision is accomplished by emitting beams from eyeballs:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Past Theory</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408195205756.png" alt="image-20220408195205756" /></td>
      <td><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408195213418.png" alt="image-20220408195213418" /></td>
    </tr>
  </tbody>
</table>

<p>But they had some interesting evidences to back up:</p>

<ol>
  <li>In near darkness, cat <strong>eyes are still visible</strong>, deer in headlights, also red eye
    <ul>
      <li>e.g. in cartoon, you only see people’s eyes but not there body!</li>
    </ul>
  </li>
  <li>Taping the eye causes short flashes (don’t try it)</li>
  <li>Evil eye, <strong>feel when somebody is looking</strong> at you
    <ul>
      <li>how did you feel that? in the emission theory it seems to make sense</li>
    </ul>
  </li>
  <li>Elegance: similar to touch</li>
</ol>

<p>But today, with careful experimenting we found that:</p>

<ol>
  <li>in reality, your retina is just very reflective even of minimal light. (and people in the past cannot make full darkness anyway)</li>
  <li>its blood</li>
  <li>in reality a study is done that people only had $50.001\%$  of the time being able to tell</li>
</ol>

<p>The upshot is that bias/ethical issues could crawl in in things you believed was right!</p>

<h2 id="racism-with-motion-tracking">Racism with Motion Tracking</h2>

<p>In reality, we have a lot of examples with products having ethical and bias issues:</p>

<ul>
  <li>
    <p>2008 HP webcam <strong>cannot track black people</strong> but white (engineers explained as the training set had only white people)</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408114255069.png" alt="image-20220408114255069" style="zoom:33%;" /></p>
  </li>
  <li>
    <p>cameras tries to <strong>make your face “whiter”</strong> when <strong>auto-enhance</strong> is enabled (with the aim of making your photo looks good). But again it is racist!</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408114230367.png" alt="image-20220408114230367" style="zoom:25%;" /></p>
  </li>
  <li>
    <p>ML encoding definition of beauty, which is completely biased!</p>
  </li>
</ul>

<blockquote>
  <p>Is the <strong>source</strong> of the bias/ethics problem the <strong>training data</strong>?</p>
</blockquote>

<h2 id="film-and-beauty-cards">Film and Beauty Cards</h2>

<blockquote>
  <p>The below story will show you how some <strong>bias/ethical issue</strong> can be embedded in <strong>everyday objects/tools</strong> we used!</p>
</blockquote>

<p>Consider how films in the past works.</p>

<ul>
  <li>
    <p>For a black and white film: photons comes through lens and hits the film (which looks like a sandwhich)</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408114645668.png" alt="image-20220408114645668" style="zoom:33%;" /></p>

    <p>basically depending on the intensity of light, the light sensitive material in the middle picks up.</p>

    <p>Then, for black and white files, Film development wipes away undeveloped silver halides, resulting in the negative</p>
  </li>
  <li>
    <p>However, for color films, you have:</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408114833793.png" alt="image-20220408114833793" style="zoom:33%;" /></p>

    <p>e.g. anything that is not blue nor green pass all the way to red.</p>

    <p>Then, when you ship this to the lab, we need to take the channels, find a way to mix them, and resemble them into a colored photo. This means you need to a come up with a <strong>chemical process to mix</strong> those back. But what is the ==standard for a good mix==?</p>
  </li>
</ul>

<p>How do you reassemble a photo that makes you happy?</p>

<blockquote>
  <p>Film companies distributed <strong>reference cards</strong> so labs could test their color reproduction.</p>
</blockquote>

<p>Then this gives the first Shirley Card:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">First Shirley Card</th>
      <th style="text-align: center">Other Shirley Card</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408115225280.png" alt="image-20220408115225280" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408115312397.png" alt="image-20220408115312397" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>so that people <strong>calibrates machines</strong>/color mixings according to those cards. Apparently we see the definition of beauty in colored photos is ==bias towards white==!</p>

<p>As a result, if you take a colored photo of the blacks at that time:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408115344395.png" alt="image-20220408115344395" style="zoom:33%;" /></p>

<p>so that the exposure is so bad that yuo cannot see the facial details of those black people.</p>

<p>What is the solution? We should fix the training data to <strong>include diversity</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408115514697.png" alt="image-20220408115514697" style="zoom:33%;" /></p>

<p>(but what actually caused the change are <strong>advertisings</strong>, e.g. chocolate does not look good, not due to complaints from people)</p>

<blockquote>
  <p>However, the source of the problem are people themselves.</p>

  <ul>
    <li>whatever <strong>reference set</strong> we defined is subjective, which is in the end defined by <strong>people</strong></li>
    <li>it is sometimes all those tiny decisions you (carelessly) make that propagates to the society and becomes a bias!</li>
  </ul>
</blockquote>

<p>Many other examples show-casing how <strong>people themselves produces bias</strong> include training models based off <strong>internet</strong>:</p>

<ul>
  <li>learns to tag black people as terrorists, because there are patterns/statistics that does have this correlation</li>
  <li>Tay chatbot on twitter</li>
</ul>

<h2 id="image-processing-and-lena">Image Processing and Lena</h2>

<p>Early reference image used in <strong>image processing</strong> commonly included the following image:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408120330102.png" alt="image-20220408120330102" style="zoom:33%;" /></p>

<p>where for image processing, examples include:</p>

<ul>
  <li>
    <p>find a compression technique such that <strong>this photo</strong> looks <strong>good</strong>.</p>
  </li>
  <li>
    <p>etc.</p>
  </li>
</ul>

<p>But why this picture? Do you know where it comes from when you are using it?</p>

<ul>
  <li>this image is a crop from a photo originally in an <em>adult magazine</em> (Playboy)</li>
</ul>

<p><em>even today, we compare results, papers on this picture!</em></p>

<ul>
  <li>again, these tiny. careless decisions people make could have impact! (those temporary decisions could be stuck in the entire industry)</li>
  <li>but people would just use those biased dataset, for example, <strong>without realizing that this could be biased</strong>. However, the motivation would sound justified: we do not want privacy issues, hence we end up using those public images which are usually celebrities</li>
</ul>

<p>And the only way to break it is to have people along the line of using it to be <strong>aware of every decisions</strong> that you make.</p>

<h2 id="tay-chatbot">Tay Chatbot</h2>

<p>Consider producing a bot that <strong>maximizes likes</strong> on twitter</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408121246930.png" alt="image-20220408121246930" style="zoom:33%;" /></p>

<p>Then after 16 hours of deployment, this chatbot</p>

<ul>
  <li>Learned to retweet offensive messages</li>
  <li>then was shut down</li>
</ul>

<p>Again, problem is the training data. Yet who creates the training data? It is us ourselves!</p>

<h2 id="facial-bias">Facial Bias</h2>

<p>In the past and even today, you might hear:</p>

<blockquote>
  <p>“Facial recognition is Accurate, if you’re a white guy”</p>
</blockquote>

<p>In 2018, some results show that <strong>commercial facial recognition</strong> on gender identification when given a photo:</p>

<ul>
  <li>white man has 1 percent of error</li>
  <li>black man has up to 12 percent of error</li>
  <li>white woman has 7 percent of error</li>
  <li>black woman has 35 percent of error</li>
</ul>

<p><em>“One widely used facial-recognition data set was estimated to be more than 75 percent male and more than 80 percent white, according to another research study.”</em></p>

<p>And you also have a lot of bugs as well in the system:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408121845023.png" alt="image-20220408121845023" style="zoom:33%;" /></p>

<p>which are all examples where you think vision is easy but <strong>how do you build a system that avoids that error</strong>.</p>

<h3 id="criminality-and-sexual-orientation">Criminality and Sexual Orientation</h3>

<p>Some very controversial research were even published on the topic of:</p>

<ul>
  <li>
    <p>Given a <strong>face</strong>, inference the likelihood of <strong>crime</strong>.</p>

    <ul>
      <li>https://arxiv.org/abs/1611.04135</li>
    </ul>
  </li>
  <li>
    <p>Attempting to predict <strong>sexual orientation</strong> from <strong>facial</strong> photograph</p>

    <ul>
      <li>
        <p>first we need to collect dataset. One way they did is to download photographs from a dating website (another problem of data privacy)</p>

        <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220408122810135.png" alt="image-20220408122810135" style="zoom:33%;" /></p>

        <p>where the first column would be heterosexual, the second being homosexual.</p>
      </li>
      <li>
        <p>Found some correlation, and claimed it was due to facial structure (i.e. had a non-chance performance)</p>
      </li>
    </ul>

    <p>However, in reality</p>

    <ul>
      <li>there is a <strong>superficial bias</strong> that is hidden, such as angle of taking a photograph, whether if there are makeups or wore glasses, etc.</li>
      <li>so if you control on those variables, the performance becomes pure chance.</li>
    </ul>
  </li>
</ul>

<p>Again, many work comes from good desire but the approach is pure fallacy if you <strong>do not take care of bias/ethics</strong></p>

<blockquote>
  <p>Career-wise advice: join a <strong>diverse</strong> team! We engineers have to start insisting on preventing those biases.</p>
</blockquote>

<h2 id="fairness-and-ml">Fairness and ML</h2>

<p>fairmlbook.org</p>

<h1 id="vision-and-sound">Vision and Sound</h1>

<p>In a video, we not only have the visuals, but also the sound! In general, we have many <strong>multimodal data</strong> in reality to deal with</p>

<blockquote>
  <p>When dealing with those data, keep in mind that there are <strong>rich interaction between modalities</strong></p>
</blockquote>

<p>A famous example will be the McGurk effect</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415101635928.png" alt="image-20220415101635928" style="zoom:33%;" /></p>

<p>where even if the audio is the same:</p>

<ul>
  <li>if the mouth movement changed, you might hear “Ba” or “Fa”</li>
  <li>when you have conflicting data in modalities, how does your mind resolve/combine those information? which one is your brain listen to?
    <ul>
      <li>If you have <strong>conflicting</strong> perception, you will trust your <strong>eyes</strong>. (one possible explanation is because your vision system works faster)</li>
    </ul>
  </li>
</ul>

<p>But most of the time, <strong>normal/natural data</strong> will have ==correspondence== between modalities. In this chapter, we aims to build models that ==exploit those interactions==.</p>

<ul>
  <li>an example application would be to train a model that can locate the source of a sound (e.g. an instrument) from a video</li>
  <li>denoising algorithms (has nothing to do with vision)</li>
</ul>

<h2 id="human-ear">Human Ear</h2>

<p>Before we look at how to build systems on solving the above mentioned problems, first we can look at how human ear works</p>

<p>Essentially how we hear is by having sound waves hitting your <strong>ear drum</strong>, so that</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415102306227.png" alt="image-20220415102306227" style="zoom:33%;" /></p>

<ul>
  <li>
    <p>vibration of air causes some bones in your <strong>eardrum</strong> to vibrate as well</p>
  </li>
  <li>
    <p>then the vibration transfers to <strong>cochlea</strong>: which essentially does a “FT” (vibration in fluid) by activating on different frequencies</p>
    <ul>
      <li>when you get old, some parts of your cochlea breaks down and you cannot hear high frequency sounds</li>
    </ul>
  </li>
</ul>

<hr />

<p>Additionally, if you loses your sight, the system that processes your vision will <strong>switch</strong> to hearing, so that</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220121112001855.png" alt="image-20220121112001855" style="zoom: 33%;" /></p>

<p>approximately all the vision related regions in brain will <strong>adapt to hearings</strong>, hence:</p>

<ul>
  <li>
    <p>you end up having acute hearing</p>
  </li>
  <li>
    <p>the adaptation happens in about 6 month</p>
  </li>
</ul>

<p>An interesting experiment would be that, if you spend times with only touching/hearing things, but then given sight back, can you recognize the same object you touched?</p>

<h2 id="sound-in-computer">Sound in Computer</h2>

<p>Now, to deal with sound information, first we have to know how to represent sound into “numbers”.</p>

<blockquote>
  <p>How do we <strong>represent sound</strong> in computer?</p>
</blockquote>

<p>Computer <strong>represents sound</strong> by resenting its wave: by variation of amplitude (air pressure) over time. But more often we do a FT of the waveform to get a <strong>frequency domain</strong>: ==spectrogram==/<strong>sonographs</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Amplitude</th>
      <th style="text-align: center">Frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415102454187.png" alt="image-20220415102454187" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415102803313.png" alt="image-20220415102803313" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>which is basically doing two things: a) break the amplitude-time graph into several windows/partitions; b) for each partition get a Fourier transform $G(f)\to (A,\phi)$ for the wave inside that partition; c) concatenate them back with time where now the <strong>color/brightness</strong> represents the amplitude</p>

<ul>
  <li>
    <p>recall that a fourier transform of a Amplitude-Time graph gives Frequency-Amplitude:</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center">Wave within a Window</th>
          <th style="text-align: center">FT</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220427235819474.png" alt="image-20220427235819474" style="zoom:33%;" /></td>
          <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220427235838190.png" alt="image-20220427235838190" style="zoom:33%;" /></td>
        </tr>
      </tbody>
    </table>

    <p>note that with only a FT, you <strong>lose the time information</strong>/ordering (which is an important feature if you want to use this as input to a model). This is why we have spectrogram as a representation.</p>
  </li>
  <li>
    <p>in the example example, we see that at time $t=0$, we have few high frequencies, but many low frequencies. Therefore, we also get only a <strong>few high frequency</strong> waves (i.e. many have $A=0$ for high $f$, less dense/bright on the right figure) but <strong>dense low frequency</strong> waves.</p>
  </li>
  <li>
    <p>for humans, we can only hear up to 22,000 hertz. So if it gets high frequency regions in the chart, we might not be able to hear it.</p>
  </li>
</ul>

<p>Some more examples include:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415102909887.png" alt="image-20220415102909887" style="zoom:50%;" /></p>

<p>where notice that:</p>

<ul>
  <li><strong>white noise</strong> has a “uniform distribution”: all frequencies everywhere hence the brightness/density is approximately the same.</li>
  <li><strong>for party with laughter</strong>, we see many high frequencies data present as compared to the other ones</li>
  <li>this also means that if your hardware ended up adding/manipulating the spectrogram, then you will hear a slightly different sound</li>
</ul>

<h2 id="learning-to-hear">Learning to Hear</h2>

<p>Essentially all techniques mentioned in video works in audio.</p>

<blockquote>
  <p>Our aim is to:</p>

  <ul>
    <li>
      <p>given a <strong>spectrogram</strong> $x_s(w)$, i.e. sound data</p>
    </li>
    <li>
      <p>learn some task-related information from it (e.g. what object does it correspond to)</p>

\[f_\theta(x_s(\omega)) \to \text{objects}\]

      <p>or you can learn other things such as the location of the image that produced the sound</p>
    </li>
  </ul>
</blockquote>

<p>Many architecture for sound, which is <strong>essentially 2D data</strong>, can be basically made similar to a ImageNet (note that the only difference is that you would expect sound data to have a high width-dimension as you typically have a high sampling rate for sound)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415104843699.png" alt="image-20220415104843699" style="zoom:50%;" /></p>

<p>most of the hard stuff is ==how do we get training data== (for free). Usually this is done by utilizing the synchronization of videos which have <strong>both a sound and vision</strong> perspective.</p>

<hr />

<p>Then consider the task of <strong>associating an object</strong> from a given sound</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input Sound (of a Lion)</th>
      <th style="text-align: center">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415200533010.png" alt="image-20220415200533010" /></td>
      <td style="text-align: center">Lion</td>
    </tr>
  </tbody>
</table>

<p>We can Use natural synchronization of sound and video to “<em>label</em>” the sounds. We consider that, from a <strong>video</strong>:</p>

<ul>
  <li>use a network $F$ that learn the objects and scenes in the picture $F(x_v)$. Use this as a <strong>teacher</strong></li>
  <li>
    <p>use a network $f$ that deals with sound input $f(x_s)$. This will be a student</p>
  </li>
  <li>use KL divergence to match the distribution between by  and student model</li>
</ul>

<p>Hence this is basically what SoundNet does:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415105231087.png" alt="image-20220415105231087" style="zoom:50%;" /></p>

<p>then it learns that, given a sound, what are the <strong>objects and scenes</strong> in the sounds:</p>

<ul>
  <li>
    <p>we usually first train the model $F$ alone (or take a pretrained model), so that it is treated as the <strong>teacher</strong></p>
  </li>
  <li>
    <p>then, the <strong>student</strong> network $f$ tries to learn a <strong>mapping</strong> from its own data to the output of the teacher network</p>
  </li>
  <li>
    <p>as a result, it can learn that the above particular sound should correlate with the object of lion</p>
  </li>
</ul>

<p>However, there are “problem” cases. Consider the example of</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Example Input: Dog barking</th>
      <th style="text-align: center">Example Input: Birthday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415105522778.png" alt="image-20220415105522778" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415105821609.png" alt="image-20220415105821609" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>notice that both cases above where output of $f(x_s)$ when <strong>only given sound</strong></p>

<ul>
  <li>on the left figure, it even found the breed of the dog (extra information learnt due to the vision mapping)</li>
  <li>on the left figure, can also tell some background sceneries, e.g. on a pasture. (extra information learnt due to the vision mapping)</li>
  <li>on the right figure, it predicts that there are candles when only given a sound of happy birthday (extra information learnt due to vision mapping)</li>
  <li>technically the above are forms of “<strong>spurious correlation</strong>”. But whether if it is good or bad depends.</li>
</ul>

<hr />

<p>Finally, for completeness, below is performance of SoundNet for classification:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415110126974.png" alt="image-20220415110126974" style="zoom:33%;" /></p>

<h3 id="cocktail-party-problem">Cocktail Party Problem</h3>

<blockquote>
  <p>The cocktail party effect is the phenomenon of the brain’s ability to <strong>focus</strong> one’s auditory attention <strong>on a particular stimulus</strong> while <strong>filtering out a range of other stimuli</strong>.</p>
</blockquote>

<p>With this ability we can easily/fast <strong>switch attention</strong> to people mentioning your names in the <strong>noisy</strong> background</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415110433098.png" alt="image-20220415110433098" style="zoom:50%;" /></p>

<p>A related task in DL would be:</p>

<blockquote>
  <p>Given a sound/spectrogram that is a <strong>composition of sounds</strong> (e.g. two musicians playing), can we build a network so that we can choose to attend to one player’s sound while filtering out the other?</p>

  <ul>
    <li>essentially the problem of ==unmixing sounds==</li>
  </ul>
</blockquote>

<p>The aim would be to build a program so that:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415110640248.png" alt="image-20220415110640248" style="zoom:33%;" /></p>

<p>if you <strong>click on a person/instrument</strong>, you will attend to a specific person/instrument’s sound (i.e. only hear its sound)</p>

<blockquote>
  <p>To make the above application, we need a network that figure out <strong>which regions</strong> of the video are making <strong>which sounds</strong>.</p>

  <ul>
    <li>the final hidden aim is to unmix the sound in the video</li>
  </ul>
</blockquote>

<p>So essentially:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415110759986.png" alt="image-20220415110759986" style="zoom: 67%;" /></p>

<p>but again how do we even get training data?</p>

<ul>
  <li>Again the trick is to utilize the fact that videos have synchronized audio and vision information</li>
  <li>sppose we have $N$ videos with a single player producing some sound. Then we can compose $2^N$ video by:
    <ul>
      <li>choose a combination of the videos (with their sounds)</li>
      <li>concatenate the video and <strong>add the sound</strong> (assume each mixed sounds are <strong>sums of spectrograms</strong>)</li>
    </ul>
  </li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415110923935.png" alt="image-20220415110923935" style="zoom:50%;" /></p>

<p>Then we automatically have labelled data. But recall that we need our network to do two things:</p>

<ol>
  <li><strong>unmix</strong> the audio</li>
  <li><strong>assign</strong> which region in the video each unmixed audio comes from</li>
</ol>

<blockquote>
  <p>The biggest problem is that there are <strong>infinitely many solutions</strong> for un-mixing + assigning:</p>

  <ul>
    <li>there are infinitely many ways to unmix the audio</li>
    <li>even after unmixed, how do we let it learn which location it comes from?</li>
  </ul>

  <p>To solve this problem, consider formulate the problem to be <strong>reconstruction task</strong>:</p>

  <ul>
    <li><strong>given</strong> a mixed video + audio input</li>
    <li>find some $k$ video embeddings $i_k$ and $k$ audio embeddings $s_k$ (i.e. learning unmixing/separation)</li>
    <li>let the video embedding choose which audio embedding it has (e.g. similarity)</li>
    <li><strong>reconstruct</strong> the sound from using those $2k$ embeddings</li>
  </ul>
</blockquote>

<p>High level architecture</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">General Architecture</th>
      <th style="text-align: center">Detailed Architecture</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415111126396.png" alt="image-20220415111126396" style="zoom: 67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415203706398.png" alt="image-20220415203706398" style="zoom: 50%;" /></td>
    </tr>
  </tbody>
</table>

<p>so that essentially</p>

<ul>
  <li>we want the network to get from one spectrogram to $k$ spectrograms</li>
  <li>then the key idea is that the audio NN should learn ==what are the $k$-channels that can best reconstruct the original sound==!</li>
</ul>

<p>Then if we have <strong>large enough data</strong>:</p>

<ul>
  <li>each $s_i$ (e.g sound of a dog) would correspond to the vision of $v_i$</li>
  <li>technically you can choose a $k$ that is large, so that even if there are less objects than $k$, we can have the vision “fragmented” $v_i$ and $s_i$ so that when we “click” on the object, we just sum the fragmented sounds and get back the sound of the object</li>
  <li>so technically it learns by ==separation by category==, so that if you have multiple instances of the same instrument, then it won’t work</li>
</ul>

<hr />

<p>Once trained, this system can</p>

<ul>
  <li>
    <p>manipulate volumes of of each individual instrument as now we have it seperated!</p>
  </li>
  <li>
    <p><strong>also create a heat map</strong> knowing where the sounds come from</p>
  </li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415113001280.png" alt="image-20220415113001280" style="zoom:50%;" /></p>

<h3 id="interpreting-soundnet">Interpreting SoundNet</h3>

<p>We mentioned that the same architecture of CNN can be used for sound. Then <strong>what does the kernel learns to do</strong>?</p>

<p>For instance, recall that the firs layer in CNN for vision learns to detect edges:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Layer</th>
      <th style="text-align: center">Kernel Visualization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415113230884.png" alt="image-20220415113230884" style="zoom: 50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415113246443.png" alt="image-20220415113246443" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>it turns out that the kernel for sound at layer one:</p>

<ul>
  <li>detects low frequencies and high frequencies</li>
  <li>the “edge detectors” parallel for sound</li>
</ul>

<p>Moving on, for middle layer kernels  <strong>activates specifically</strong> for an “<strong>object</strong>” of sound. E.g. smacking/chime sound (i.e. only hear those = only those got activated after some neuron)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Layer</th>
      <th style="text-align: center">Activation “Map”</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415113403698.png" alt="image-20220415113403698" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415204439906.png" alt="image-20220415204439906" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>Then in an even higher level, there is a unit activates on an even higher level “object” of sound (e.g. one parent talking to kids)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Layer</th>
      <th style="text-align: center">Activation “Map”</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415204640787.png" alt="image-20220415204640787" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415204611636.png" alt="image-20220415204611636" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<h3 id="clustering-sound">Clustering sound</h3>

<p>Once we have embeddings of sound data, we can cluster them based on distance (e.g. below uses MDS)</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415204731052.png" alt="image-20220415204731052" style="zoom:50%;" /></p>

<p>where here we compare how vision and sound data relates to each other</p>

<ul>
  <li>green/nature cluster: images/<strong>vision</strong> about nature seems to be close together in “semantics” as they have similar embedding</li>
  <li>yellow/sport cluster: for <strong>sound</strong>, they are spread all over the space</li>
</ul>

<p>Additionally for sound data specifically:</p>

<ul>
  <li>music are being close together, forming a cluster</li>
  <li>
    <p>urban and nature seems to be close in sound representation</p>
  </li>
  <li>clustering becomes more diverse</li>
</ul>

<h3 id="denoising">Denoising</h3>

<p>How do we suppress the noise in a video conference call? Solving this task essentially leads to</p>

<blockquote>
  <p>How do you figure out which part is <strong>signal</strong> (our speech), and which part is <strong>noise</strong>? (Hence do noise removal)</p>

  <ul>
    <li>note that this is purely an application of hearings, no vision related techniques are applied</li>
  </ul>
</blockquote>

<p>One key observation/property is that human needs to breathe, hence we get <strong>silent intervals</strong>. Then ==during those silent intervals, estimate the noise distribution==:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415114058030.png" alt="image-20220415114058030" style="zoom:50%;" /></p>

<p>then we want to estimate the noise using those intervals, and then subtract it to get denoised input.</p>

<p>Then the architecture looks like</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415114223221.png" alt="image-20220415114223221" style="zoom:50%;" /></p>

<h1 id="vision-and-language">Vision and Language</h1>

<p>What is an ideal AI system? Ideally, we would want it to be able to do:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415120059530.png" alt="image-20220415120059530" style="zoom:50%;" /></p>

<blockquote>
  <p>Notice that to answer those, we not only need vision, we also needed NL understanding, as well as:</p>

  <ul>
    <li>
      <p>how to combine two two information/query and data</p>
    </li>
    <li>
      <p>some common sense (see below)</p>
    </li>
  </ul>
</blockquote>

<p>. More examples:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415120329111.png" alt="image-20220415120329111" style="zoom:50%;" /></p>

<p>where notice that:</p>

<ul>
  <li>we need encode the given data which is <strong>both in vision and text</strong></li>
  <li>we also need some <strong>common sense encoded</strong> in the system, so that we can answer the bottom row questions</li>
</ul>

<h2 id="trial-architecture">Trial Architecture</h2>

<p>If this solved, then it is real AI! But it is not yet solved, and some simple approaches just brute force combining embeddings of everything:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415120440819.png" alt="image-20220415120440819" style="zoom:50%;" /></p>

<p>where the question is how do we fill in the black box.</p>

<hr />

<p><em>Recall how we can representing Words</em>: One NLP task is to find similar words given a word</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415120554123.png" alt="image-20220415120554123" style="zoom:50%;" /></p>

<p>which essentially is to find <strong>embeddings</strong> given a word and hence produce similarity scores.</p>

<ul>
  <li>word2vec embedding</li>
  <li>Glove embedding</li>
  <li>BERT</li>
  <li>etc.</li>
</ul>

<hr />

<p>Then a <strong>sample architecture</strong> comes out as</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415120854585.png" alt="image-20220415120854585" style="zoom: 50%;" /></p>

<p>Then if we have enough data, we hope to encode common sense in the system as well:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415121047247.png" alt="image-20220415121047247" style="zoom:67%;" /></p>

<p>where</p>

<ul>
  <li>green is when we gave both text and image as input</li>
  <li>blue is when we only have text as input</li>
  <li>notice that there is a great potential of the network just learning by <strong>memorization</strong>/overfitting</li>
</ul>

<p>But some times it works. Some applications that comes out from this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Examples</th>
      <th style="text-align: center">Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415121255815.png" alt="image-20220415121255815" style="zoom:40%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415205625522.png" alt="image-20220415205625522" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>however, there are problems:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Inconsistencies</th>
      <th style="text-align: center">Inconsistencies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415205731372.png" alt="image-20220415205731372" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415205748971.png" alt="image-20220415205748971" /></td>
    </tr>
  </tbody>
</table>

<p>where we see that</p>

<ul>
  <li>there is no self consistency (yellow frisbee)</li>
  <li>it is perhaps <strong>not how a person answers the questions</strong> (its common sense is not the same as ours)
    <ul>
      <li>overfit, biases comes in, etc.</li>
    </ul>
  </li>
</ul>

<h2 id="compositional-vqa">Compositional VQA</h2>

<blockquote>
  <p>Instead of finding a model to answer the question, let the model learn the <strong>logics</strong> to reach the answer</p>

  <ul>
    <li>that way, we can perhaps control the bias that would be learnt from the NN</li>
  </ul>
</blockquote>

<p>Consider the following questions on the compositions you have in the image:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415121827182.png" alt="image-20220415121827182" style="zoom:50%;" /></p>

<p>We want a NN to synthesize a <strong>program that outputs the answer</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415122153203.png" alt="image-20220415122153203" style="zoom: 33%;" /></p>

<p>where:</p>

<ul>
  <li>right column includes the <strong>operation</strong> you can have for the program to do</li>
  <li>then, the network learns to <strong>assemble</strong> pieces to <strong>output a program</strong></li>
  <li>so that when you run the program, you get answer to the question</li>
</ul>

<p>Therefore, your architecture looks like</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415122407528.png" alt="image-20220415122407528" style="zoom:50%;" /></p>

<p>where essentially the</p>

<ul>
  <li>program generated from the text only, so there might be efficiency issues.</li>
  <li>the objective is to produce the same program given the same question, so that we have predictability (even if we swap the image input)</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415122735026.png" alt="image-20220415122735026" style="zoom:67%;" /></p>

<h2 id="relational-network">Relational Network</h2>

<blockquote>
  <p>To answer the question, we need to first learn some mapping/<strong>correspondence</strong> between parts of the image and words in the question. Then answer the question.</p>
</blockquote>

<p>Google came up with a this network that solve the following types with 99.9% performance</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415122900586.png" alt="image-20220415122900586" style="zoom:50%;" /></p>

<p>where the big difference from the previous is:</p>

<ul>
  <li>if one component of the program is wrong, then the performance is bad (i.e. structure ==assumptions== we are making, which works only if right)</li>
  <li>but for black box approaches with NN, the risk is much smaller</li>
</ul>

<p>The idea is to basically</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415123351957.png" alt="image-20220415123351957" style="zoom:50%;" /></p>

<p>so that we view image as <strong>patches</strong> of pixels, and sentence as a <strong>patches</strong> of words</p>

<ul>
  <li>then each patch of image would correspond to each word/phrase</li>
  <li>$O$ is a set of objects, where an object could be a pathc of image or a word</li>
  <li>basically consider all possible pairs, and produce a <strong>feature representing those pairings</strong></li>
</ul>

<p>Then there is very little assumptions made</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220415123336660.png" alt="image-20220415123336660" style="zoom:67%;" /></p>

<h1 id="3d-vision">3D Vision</h1>

<p>Why you have two eye balls instead of one? It might seem redundant that you have two eyes looking at the same direction = why do we need this extra redundancy? Why did evolution not have our second eye at the back of our head?</p>

<blockquote>
  <p>It is with such “redundancy” that we can perceive depths.</p>

  <p>Binocular stereopsis, or <strong>stereo vision</strong>, is the ability to derive information about how far away objects are, based solely on the relative positions of the object in the two eyes.</p>

  <p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Feye.2014.279/MediaObjects/41433_2015_Article_BFeye2014279_Fig1_HTML.jpg" alt="Stereo vision and strabismus | Eye" style="zoom:50%;" /></p>
</blockquote>

<h2 id="human-vision-and-applications">Human Vision and Applications</h2>

<p>Many animals also have <strong>stereo vision</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422102314309.png" alt="image-20220422102314309" style="zoom:33%;" /></p>

<p>and the further the separation is, the better you are at seeing depth (e.g. for very far away objects)</p>

<ul>
  <li>
    <p>This is very important for <strong>predator</strong>: how far away are you from the prey?</p>
  </li>
  <li>
    <p>On the other hand, prey sometimes doesn’t need this. For instance. for pigeon, it is more about seeing 360 vision instead of depth</p>
    <ul>
      <li>Therefore their eyes <strong>don’t</strong> need to “overlap” but goes “sideways”</li>
      <li>however, they still can achieve some stereo vision by moving your head in some particular away</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Stereoscopes</strong>: in the 19th Century we already had goggles that you can wear to see 3D pictures</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">In the past</th>
      <th style="text-align: center">Today</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://payload.cargocollective.com/1/7/247643/9514132/_2160520_1000.jpg" alt="TwinScope Viewer - Colleen Woolpert" style="zoom: 25%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422171809789.png" alt="image-20220422171809789" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>but they are typically very expensive.</p>

<ul>
  <li>today we often have a cheaper way: we construct the visuals in a way that pairs up with the goggles on the right</li>
  <li>it works by having only red light passes through the red lens, and same for blue, to create an illusion of 3D. Essentially it controls which eye sees which view to render the entire scene 3D!</li>
</ul>

<hr />

<p><strong>Mars Rovers</strong>: Very expensive, so we want our rover not to hit/crash into any obstacles!</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422102955141.png" alt="image-20220422102955141" style="zoom:50%;" /></p>

<p>so we see that it used many cameras for stereo vision and hence navigation.</p>

<h3 id="depth-without-objects-recognition">Depth without Objects Recognition</h3>

<p>How does your brain merge the two views you see in two eyes to get a 3D perception?</p>

<p>Some interesting experiments done in the past considers whether if we performed object recognition first and then depth:</p>

<ul>
  <li>e.g. could it be that because I know it is a table, I know the depth? i.e. we know the <strong>priors</strong> and use for depth perception</li>
  <li>turns out to be not the case! we directly see depth without object recognition, as shown in the experiment below</li>
</ul>

<p><strong>Random dot stereograms</strong>: consider concentrating on the two squares highlghted on the le</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Random Dot Stereograms</th>
      <th style="text-align: center">Human Perception</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422174834302.png" alt="image-20220422174834302" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422174839436.png" alt="image-20220422174839436" style="zoom: 80%;" /></td>
    </tr>
  </tbody>
</table>

<p>the idea is simple:</p>

<ul>
  <li>take two noises, and take the same two squares in the noise as shown on the left</li>
  <li>if you can cross your eyes to put the two squares on top of each other, it will seem that the square is closer to you than the background</li>
  <li>this shows that depth has nothing to do with objects recognition. You <strong>directly perceives depth</strong> somehow!</li>
</ul>

<h3 id="important-of-depth-information">Important of Depth Information</h3>

<p>Consider the following examples:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Left Light Source</th>
      <th style="text-align: center">Right Light Source</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422182621636.png" alt="image-20220422182621636" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422182617090.png" alt="image-20220422182617090" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where:</p>

<ul>
  <li>on the left you should perceived that the holes are sticking in, whereas on the right the holes appears to be popping out</li>
  <li>but we have the <strong>same image</strong>, hence whether if the holes are sticking in/popping out is <strong>ambiguous if we don’t know the location of the light source</strong> (or resolved if we know the depth!)</li>
</ul>

<p>Similarly, which vertex of the square is sticking out/which face is in the front is ambiguous:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422182916696.png" alt="image-20220422182916696" style="zoom: 33%;" /></p>

<p>it all comes down to putting a 3D object 2D loses information:</p>

<ul>
  <li>there can be infinite many 3D configurations that have landed in the same 2D picture shown above</li>
  <li><strong>ambiguous</strong> what the original 3D object is without depth information</li>
</ul>

<h2 id="perceive-depth-with-machines">Perceive Depth with Machines</h2>

<blockquote>
  <p>How do we use algorithms based on geometry to see depth?</p>

  <ul>
    <li>e.g. given an <strong>object</strong> and some <strong>cameras</strong>, how do you construct the depth information of the objects?</li>
    <li>once we understand how this works, we can maybe <em>inference</em> some new view points and construct 3D scenes you never saw before!</li>
  </ul>
</blockquote>

<p>There are two common approaches to calculate depth (given some view point), and to construct 3D visuals:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Binocular Stereo</th>
      <th style="text-align: center">Photometric Stereo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422175318428.png" alt="image-20220422175318428" style="zoom:67%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422175829670.png" alt="image-20220422175829670" style="zoom:67%;" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li><strong>Binocular Stereo</strong>: like our eyes, we have a second camera</li>
  <li>based on how far away the pixels move when we “move” our camera, we can estimate depth
    <ul>
      <li>i.e. things are <strong>far away</strong> will have almost no movement when we shifted the camera, however for <strong>close</strong> objects it will have some movement that is related to how far it is from the camera</li>
    </ul>
  </li>
  <li><strong>Photometric Stereo:</strong> only one camera but lights move around</li>
  <li>essentially computed based on changes in pixel brightness
    <ul>
      <li>actually works very well in practice</li>
    </ul>
  </li>
</ul>

<p>If this works well, why do we need ML on this?</p>

<blockquote>
  <p>For estimating the depth of a point, we need to figure out <strong>changes of a pixel</strong> when we changed the camera position/light. However, this means that we first need to know which pixel are <strong>corresponding</strong> to which pixel in the different images we took.</p>
</blockquote>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422104121903.png" alt="image-20220422104121903" style="zoom: 50%;" /></p>

<p>Given two matched pixels, we can of course figure out how far they moved, and therefore depth:</p>

<ul>
  <li>but how do we find the matching pixels on the first hand?</li>
  <li>then, how far did they move/how many pixels did it move/change?</li>
</ul>

<h3 id="applications-of-using-depth">Applications of Using Depth</h3>

<p>More examples <strong>using ML to find out depth information</strong> could be useful</p>

<hr />

<p><strong>Necker Cube</strong></p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422183256979.png" alt="image-20220422183256979" style="zoom:25%;" /></p>

<p>with many data samples of the same cube taken from different angles, we can use ML to estimate depth and hence reconstruct the original 3D object!</p>

<hr />

<p><strong>Facial Recognition</strong></p>

<p>For face recognition, we need to build a 3D model of your face</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Facial Recognition</th>
      <th style="text-align: center">Modeling</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422105005423.png" alt="image-20220422105005423" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422183533946.png" alt="image-20220422183533946" style="zoom:25%;" /></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>how it works is that it shines infrared light on your face and you that to estimate depth</li>
  <li>our phones have many 3D sensors/streo cameras already!</li>
</ul>

<hr />

<p><strong>LiDAR</strong>:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422105055269.png" alt="image-20220422105055269" style="zoom:33%;" /></p>

<p>not using machine learning to compute depth, but uses laser to calculate depth.</p>

<ul>
  <li>
    <p>essentially works by calculating how long the wave returns.</p>
  </li>
  <li>basically the best sensor we have for outdoor depth estimation.
    <ul>
      <li>it can see depth VERY far</li>
    </ul>
  </li>
  <li>but many car manufactors uses camera instead of LiDAR. because
    <ul>
      <li>it is too <strong>expensive</strong>!</li>
      <li>also there are cases when LiDAR <strong>doesn’t work</strong>. Since it is based on reflection time for wave, if you have rain and fog then it could reflect of from rain drops.</li>
      <li>last but not least, you still need cameras as it does not tell you <strong>what</strong> is there, but only depth</li>
    </ul>
  </li>
</ul>

<h2 id="representation-of-3d-information">Representation of 3D Information</h2>

<p>How do we represent this in machine?</p>

<ul>
  <li>Images: Pixels</li>
  <li>Videos: Stop</li>
  <li>Motion Sound: Wave form</li>
  <li>3D: how do we do it?</li>
</ul>

<blockquote>
  <p>Essentially some ways to represent 3D information are:</p>

  <ul>
    <li><strong>Voxel</strong> (volume Element): representing 3D scenes with many small 3D cubes</li>
    <li><strong>Point Cloud</strong>: representing only object surfaces with a discrete number of points</li>
    <li><strong>Mesh</strong>: the above but with surfaces connecting them, hence no holes</li>
    <li><strong>Implicit Surface</strong>: by using a function $F(x,y,z)$ that given a coordinate gives you $0$ if you are in/on the object!</li>
  </ul>
</blockquote>

<h3 id="voxel-representation">Voxel Representation</h3>

<p>Recall how pixel representtaoin works</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">2D Images</th>
      <th style="text-align: center">3D Info</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422110437087.png" alt="image-20220422110437087" style="zoom: 33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422110511364.png" alt="image-20220422110511364" style="zoom: 33%;" /></td>
    </tr>
  </tbody>
</table>

<p>where if we have an object somewhere, we have a corresponding volume there.</p>

<ul>
  <li>
    <p>basically like encapsulating the real world into the 3D box, so that if an object will be combination of volume elements in the space (like putting together a Lego)</p>
  </li>
  <li>
    <p>but this is not used because</p>
    <ul>
      <li>it is <strong>too memory intensive</strong> . If we increase our “world”, it scales with $O(N^3)$</li>
      <li>it is often very <strong>sparse</strong>!</li>
      <li>there is also a trade of of resolution (i.e. the smallest volume component of an object) and details
        <ul>
          <li>e.g. if your smallest Lego piece is very small, then you can have complex shapes. But if that is large, then you can only have simple shapes.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="point-cloud-representation">Point Cloud Representation</h3>

<p>Consider to only represent objects (but not empty space), by using <strong>a collection of points</strong> on its surface</p>

<ul>
  <li>resolves the sparsity problem as we only have objects represented</li>
  <li>and it also scales if we have a large scene!</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Representation</th>
      <th style="text-align: center">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422111043844.png" alt="image-20220422111043844" style="zoom:33%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422111057087.png" alt="image-20220422111057087" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<p>but there are problems:</p>

<ul>
  <li>there are holes, which means you will need to interpolate in between if you need to model some interaction (e.g. robot picking up)</li>
  <li>the above worked because we have so many points, hence an illusion. If we zoom in, you see holes!</li>
</ul>

<h3 id="mesh-representation">Mesh Representation</h3>

<p>Instead of a collection of points, having them connected to form a mesh would resolve the “hole” problems</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422111306824.png" alt="image-20220422111306824" style="zoom:33%;" /></p>

<p>however, the problem is to integrade with neural nets</p>

<ul>
  <li>it is easy to turn a lidar scan into a point cloud, but not a mesh</li>
  <li>why can we not just combine nearby points from a point cloud to a surface to get mesh?
    <ul>
      <li>difficult to deal with noisy points. We need to determine whether if it is a noise and then decide to include it or not</li>
      <li>then we also need a merge algorithm, to merge the small surfaces into a larger smoother mesh</li>
    </ul>
  </li>
</ul>

<h3 id="implicit-surface-representation">Implicit Surface Representation</h3>

<p>Instead of modelling what we see, we can <strong>model a 3D shape by a function</strong>:</p>

\[F(x,y,z) = 0 \iff \text{on surface}\]

<p>if it is not zero, it can represent the distance away form the object</p>

<ul>
  <li>a <strong>very compact representation</strong>. You only need to store the parametre to the function.</li>
  <li>there is no resolution trade-off as everything is now continous! We can query <strong>any point</strong> we want (hence infinite resolution)</li>
  <li>but to get this function, e.g. we can train a NN to represent $F$. but it could be expensive to train</li>
</ul>

<blockquote>
  <p>Essentially you can imagine this $F$ <strong>models the real world</strong>! (like the model-based method in RL algorithms)</p>
</blockquote>

<h2 id="learnig-with-3d-represnetation">Learnig with 3D Represnetation</h2>

<p>Now, given a representation in either of the four, how do we perform tasks such as:</p>

<ul>
  <li>classification of 3D objects</li>
  <li>segmentation of 3D objects by parts</li>
  <li>segmentation of a 3D scene</li>
</ul>

<p>For example, if the input is point cloud representation:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422112004603.png" alt="image-20220422112004603" style="zoom:50%;" /></p>

<h3 id="learning-with-point-clouds">Learning with Point Clouds</h3>

<p>Given some point clouds input, our task is to solve the following problems using ML:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422112004603.png" alt="image-20220422112004603" style="zoom:50%;" /></p>

<p>Note that, as mentioned before, this representation have holes in the object, which we need to find some way to interpolate and know it is not empty space.</p>

<p>Since point cloud is essentially a list of coordinates:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422191400191.png" alt="image-20220422191400191" /></p>

<p>where consider we want to do object classification given this data:</p>

<ul>
  <li><strong>no-order</strong> in the data.
    <ul>
      <li>for images we know which pixel is next to which pixel</li>
      <li>the above essentially enables convolution, we know neighbors of each pixel</li>
      <li>however, we do not have this information here!</li>
    </ul>
  </li>
  <li>to recognize this as an object, we need to somehow <strong>learn interaction between the points</strong></li>
  <li>we want our algorithm to be <strong>invariant under transformation</strong>
    <ul>
      <li>if I shift everything over by 10, I would have not changed anything (because the center can be arbitrary).</li>
      <li>How do we make a NN learn those invariant transformation (e.g. still work produce the right classification)?</li>
    </ul>
  </li>
</ul>

<hr />

<p>Some ideas you may have:</p>

<ul>
  <li>first <strong>sort</strong> the data (deterministically), and then feed the results into a CNN. This is invariant to order!
    <ul>
      <li>problem: will be disturbed by noise a lot</li>
      <li>problem: not invariant to rotation</li>
    </ul>
  </li>
  <li>treat the order as <strong>data augmentation</strong>, then train a RNN
    <ul>
      <li>problem: there are too many possible configurations to go through</li>
      <li>problem: still isn’t solving the invariance to order problem as we will only sample a limited orderings</li>
    </ul>
  </li>
  <li>render this to a <strong>picture</strong> and then use CNN
    <ul>
      <li>problem:  lost all the 3D information such as viewpoints, occlusion.</li>
    </ul>
  </li>
</ul>

<h4 id="pointnet-architecture">PointNet Architecture</h4>

<blockquote>
  <p>Idea: for each pooint $x_i$, we can extract some feature $h(x_i)$ by the <strong>same neural net</strong>, ant then have another function $g$ that <strong>is invariant to input</strong> so that we have:</p>

\[f(x_1,...,x_n) = g(h(x_1),...,h(x_n))\]

  <p>which is invariant to order by construction!</p>
</blockquote>

<p>So essentially:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422113148111.png" alt="image-20220422113148111" style="zoom: 50%;" /></p>

<ol>
  <li>each point goes through some NN $h$ to give a feature</li>
  <li>then the features goes into $g$, but this has to be order-invariant (e.g. sum/max)
    <ul>
      <li>this this can be passed into some further NN as we are already order-invariant</li>
    </ul>
  </li>
  <li>then our final resultant function is order invariant as well!
    <ul>
      <li>however it is <strong>not invariant to transformation</strong> by construction</li>
    </ul>
  </li>
</ol>

<p>Then architecture looks like</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422113254350.png" alt="image-20220422113254350" style="zoom: 67%;" /></p>

<p>where:</p>

<ul>
  <li>the global feature vector is essentially the output of the function $g$. Everything before that does some embedding of the input points are the function $h$, being marked as “shared”</li>
  <li>for segmentatoin, we needed to classify <strong>each point</strong>. Therefore it concatenates all the point features with the output hence giving a $n \times 1088$ matrix for classification.</li>
  <li>note that one limitation is that we need to feed in ALL points for input.</li>
</ul>

<p>Some results of this architecture on classification and segmentation:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422113405304.png" alt="image-20220422113405304" style="zoom: 50%;" /></p>

<p>where we see that it works fine even if we have only partial point clouds</p>

<hr />

<p><strong>Critical Points</strong></p>

<p>We see that in the network, a global feature is selected for classification:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422114120507.png" alt="image-20220422114120507" style="zoom: 50%;" /></p>

<p>this means that</p>

<ul>
  <li>there are some points that are useless (i.e. whether if we had them doens’t matter) for the current task (e.g. classification)</li>
  <li>therefore, if only a few points are useful for classification, we can visualize this by</li>
</ul>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422114235330.png" alt="image-20220422114235330" style="zoom:50%;" /></p>

<p>where we see that the bottom row are the kept points that the network used to <strong>do classification</strong>.</p>

<h3 id="learning-with-implicit-surface">Learning with Implicit Surface</h3>

<p>Recall that we want to learn a $F(x,y,z)$ that essentially models the scene we are given.</p>

<blockquote>
  <p>Idea: since this is a model, we can try to use this to do <strong>reconstruction</strong> of the original 3D scene and see if it matches</p>

  <ul>
    <li>note that since it only requires $x,y,z$,  this means we already <strong>specified a camera view</strong>/hence coordinate space</li>
    <li>if we want to render scenes from a new coordinate space, this will not work</li>
  </ul>
</blockquote>

<p>So essentially our overall architecture for training a NN to do $F(x,y,z)$ looks like</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422193356807.png" alt="image-20220422193356807" style="zoom: 67%;" /></p>

<p>(note that the representation is conditioned on a certain camara view)</p>

<h4 id="training-scene-representation">Training Scene Representation</h4>

<p>Here we go into the details of how such algorithm works.</p>

<p>Essentially we want the scene representation network $F(x,y,z)$ to produce <strong>some information</strong> of the object/what to render when we give a coordinate. So essentially given some space</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Scene to Learn</th>
      <th style="text-align: center">Scene Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422193658867.png" alt="image-20220422193658867" style="zoom:50%;" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422193823388.png" alt="image-20220422193823388" style="zoom:50%;" /></td>
    </tr>
  </tbody>
</table>

<p>where</p>

<ul>
  <li>grey means free space.</li>
  <li>Essentially we want to turn them into parameters such that, if we input coordinate of a blue triangle, output a feature vector that encodes the shape being triangle and color blue</li>
</ul>

<p>How do we make sure we are modelling the scenes corectly?</p>

<blockquote>
  <p>Idea: have a decoder that queries this network $F$ and <strong>renders</strong> the scene accordingly. Then if $F$ does its job correctly, we will get a good reconstruction of the original 3D scene.</p>
</blockquote>

<p>So basically the render (given a view point) iteratively computes the new scene by querying the network $F$</p>

<ol>
  <li>
    <p>Basically it is a procedure of intersection testing. First it pick some point $x_0$ to render</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422194806865.png" alt="image-20220422194806865" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>then it will query the network $F$ to know what is there to render</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422121342518.png" alt="image-20220422121342518" style="zoom:50%;" /></p>
  </li>
  <li>
    <p>After rendered something based on $v_i$, consider what is the <strong>next point to query</strong></p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422121551658.png" alt="image-20220422121551658" style="zoom: 50%;" /></p>

    <p>which is determined by outputing the step length $\delta$</p>

    <ul>
      <li>we used LSTM here because it needs to know the history of past queries for optimizing on what is the next step to pick</li>
      <li>this is useful as we have only a <strong>limited</strong> sampling time/iteration steps to render a scene</li>
    </ul>

    <p>So then the next step looks like</p>

    <p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422121721054.png" alt="image-20220422121721054" style="zoom: 50%;" /></p>
  </li>
  <li>
    <p>Finally, repeatedly do this until finish. The loss will be reconstruction loss.</p>
  </li>
</ol>

<p>Some results:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422123414034.png" alt="image-20220422123414034" style="zoom:50%;" /></p>

<p>where in practice we might not have a 3D scene to start with. But we can start with a number of images taken on the same thing but <strong>different view points</strong></p>

<ul>
  <li>input will be a few samples of the same object, then output synthesized views from any camera views</li>
</ul>

<h4 id="neural-radiance-fields---nerf">Neural Radiance Fields - NERF</h4>

<p>the model worked by specifying some coordinate system/camera view to begin with. What if we want to also produce a model $F$ such that it can render <strong>different camera views?</strong></p>

<p>So essentially we will have our model being</p>

\[F(x,y,z,\theta,\phi)\]

<p>for $\theta,\phi$ specified our view point. Hence our network becomes:</p>

<p><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422123706785.png" alt="image-20220422123706785" style="zoom:50%;" /></p>

<p>then results look like</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Input Scene</th>
      <th style="text-align: center">Output View 1</th>
      <th style="text-align: center">Output View 2</th>
      <th style="text-align: center">Output View …</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422195833818.png" alt="image-20220422195833818" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422195852618.png" alt="image-20220422195852618" /></td>
      <td style="text-align: center"><img src="/lectures/images/2022-04-28-COMS4732_Computer_Vision2/image-20220422195934960.png" alt="image-20220422195934960" /></td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<p>so that basically the “world model” then also spits out lighting information/reflection now!</p>

<h1 id="final-exam">Final Exam</h1>

<p>Final exam next class:</p>

<ul>
  <li>grade scope exam releaieas at 10:15</li>
  <li>need to joing zoom and have camera on <strong>without virtual backgrounds</strong></li>
  <li>length is 90 minutes</li>
  <li>open notes, open slides, etc.</li>
</ul>

<p>Some topics:</p>

<ul>
  <li>fourier transform</li>
  <li>back propagation</li>
  <li>object recognition</li>
  <li>motion and hyperbolic geometry</li>
</ul>

  </div><a class="u-url" href="/lectures/2022@columbia/COMS4732_Computer_Vision2.html/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lectures/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Lecture Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Lecture Notes</li><li><a class="u-email" href="mailto:jasonyux17@gmail.com">jasonyux17@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jasonyux"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jasonyux</span></a></li><li><a href="https://www.linkedin.com/in/xiao-yu2437"><svg class="svg-icon"><use xlink:href="/lectures/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">xiao-yu2437</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An inexhaustive collection of markdown/latex(PDF) notes that I took since college. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
